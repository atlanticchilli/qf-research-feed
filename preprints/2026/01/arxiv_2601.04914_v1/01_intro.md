---
authors:
- Jean-Gabriel Attali
doc_id: arxiv:2601.04914v1
family_id: arxiv:2601.04914
is_current: true
taxonomy:
  alpha_families: []
  asset_classes: []
  horizons: []
  themes: []
title: Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow
  Networks
url_abs: http://arxiv.org/abs/2601.04914v1
url_html: https://arxiv.org/html/2601.04914v1
venue: arXiv q-fin
version: 1
year: 2026
---


Jean-Gabriel Attali
  
De Vinci Higher Education, De Vinci Research Center, Paris, France
  
jean-gabriel.attali@devinci.fr

###### Abstract

We study approximation limits of single-hidden-layer neural networks with analytic
activation functions under global coefficient constraints.
Under uniform â„“1\ell^{1} bounds, or more generally sub-exponential growth of the
coefficients, we show that such networks generate model classes with strong quantitative
regularity, leading to uniform analyticity of the realized functions.
As a consequence, up to an exponentially small residual term, the error of best network
approximation on generic target functions is bounded from below by the error of best
polynomial approximation.
In particular, networks with analytic
activation functions with controlled coefficients cannot outperform
classical polynomial approximation rates on non-analytic targets.
The underlying rigidity phenomenon extends to smoother, non-analytic activations
satisfying Gevrey-type regularity assumptions, yielding sub-exponential variants of the
approximation barrier.
The analysis is entirely deterministic and relies on a comparison argument combined with
classical Bernstein-type estimates; extensions to higher dimensions are also discussed.

Keywords:
neural networks, analytic activation functions, polynomial approximation,
approximation barriers, Gevrey regularity.

MSC 2020:
41A10, 41A25, 68T07.

## 1 Introduction

Single-hidden-layer neural networks are classical nonlinear approximation tools whose theoretical properties have been extensively studied over the past decades. Early results established their universal approximation capabilities under mild assumptions on the activation function. More refined analyses later showed that, under additional structural assumptions on the target function, such networks may achieve remarkably fast approximation rates.
A prominent example is provided by the theory initiated by Barron [[3](https://arxiv.org/html/2601.04914v1#bib.bib1 "Universal approximation bounds for superpositions of a sigmoidal function")], and earlier related contributions such as Attali and Pagâ€˜es [[1](https://arxiv.org/html/2601.04914v1#bib.bib3 "Approximations of functions by a multilayer perceptron: the random case"), [2](https://arxiv.org/html/2601.04914v1#bib.bib2 "Approximation of functions by a multilayer perceptron: a probabilistic approach")], which show that neural networks can overcome the curse of dimensionality when the target function belongs to a suitable analytic or spectral class. These results crucially rely on global constraints on the network coefficients and on strong regularity properties of the target function itself.
In parallel, classical approximation theory provides sharp minimax bounds for generic smoothness classes such as Lipschitz or Sobolev spaces. These bounds show that, in the absence of additional structure, no approximation method can outperform polynomial rates [[6](https://arxiv.org/html/2601.04914v1#bib.bib4 "Constructive approximation"), [7](https://arxiv.org/html/2601.04914v1#bib.bib5 "Nonlinear approximation")]. This fundamental limitation naturally raises the following question:
*can analyticity of the activation function alone improve approximation rates on generic target functions?*
Before addressing this question, it is important to note that analyticity-based approximation properties depend not only on the activation function but also on the effective control of the network parameters. In particular, uniform analyticity on a fixed complex neighborhood is obtained when the inner parameters remain suitably bounded; more generally, increasing inner parameters lead to shrinking analytic neighborhoods and exponentially small residual terms, as discussed later.
In this work, we provide a negative answer to the above question. We show that analyticity of the activation function, even when combined with uniform â„“1\ell^{1} bounds on the network coefficients, does not suffice to overcome the classical polynomial approximation barrier on non-analytic target functions. More precisely, we consider single-hidden-layer networks with real-analytic activation functions and uniformly bounded coefficient sums, and we show that the error of best network approximation is bounded from below by the error of best polynomial approximation, up to an exponentially small term. This result should be understood as a rigidity phenomenon for analytic model classes, rather than as a minimax lower bound.
Our results reveal a structural limitation of networks with analytic
activation functions. While analyticity plays a central role in positive approximation results for structured target classes, it also imposes intrinsic constraints on the class of realizable functions. As a consequence, networks with analytic
activation functions cannot adapt to non-analytic features of generic target functions and cannot achieve approximation rates faster than those dictated by classical polynomial approximation theory.
The proof is entirely deterministic and relies on a comparison between best network approximation and best polynomial approximation, combined with classical Bernstein-type estimates for analytic functions. In contrast with probabilistic constructions or random sampling arguments, our approach provides a transparent explanation of why analyticity of the activation function alone cannot bypass minimax approximation barriers. Although the main results are presented for analytic activation functions, the underlying rigidity mechanism extends to smoother, non-analytic activations under quantitative regularity assumptions, such as Gevrey smoothness, leading to sub-exponential variants of the approximation barrier.

###### Remark 1.1.

Throughout the paper, we restrict attention to functions defined on the interval [âˆ’1,1][-1,1].
This choice is made for notational convenience only: any compact interval can be reduced to
this setting by an affine change of variables, without affecting approximation rates or
analyticity properties.
Working on [âˆ’1,1][-1,1] allows us to use standard Bernstein ellipses and simplifies the presentation.

The paper is organized as follows.
SectionÂ 2 introduces the class of analytic activation networks considered throughout
the paper and establishes their basic analytic properties.
SectionÂ 3 recalls classical results on polynomial approximation of analytic functions.
The main lower bound is stated and proved in SectionÂ 4.
Finally, SectionÂ 5 discusses the implications of our results and their relation to
existing approximation theories for neural networks.

## 2 Analytic Activation Networks

In this section, we introduce the class of neural networks considered throughout the paper and establish their basic analytic properties. The framework is entirely deterministic and relies on analyticity of the activation function together with global bounds on the network coefficients.
A key point is that the approximation properties derived below hold in a regime where the analytic regularity of the resulting model class can be quantitatively controlled. In particular, uniform analyticity on a fixed complex neighborhood is obtained under suitable control of the inner parameters; more general parameter growth leads to shrinking analytic neighborhoods, which will be accounted for explicitly in the subsequent estimates.

### 2.1 Model definition

Let Ï†:â„â†’â„\varphi:\mathbb{R}\to\mathbb{R} be a real-analytic function.
We consider single-hidden-layer neural networks of the form

|  |  |  |
| --- | --- | --- |
|  | gâ€‹(x)=âˆ‘k=1mÎ»kâ€‹Ï†â€‹(Î±kâ€‹x),xâˆˆ[âˆ’1,1],g(x)=\sum\_{k=1}^{m}\lambda\_{k}\,\varphi(\alpha\_{k}x),\qquad x\in[-1,1], |  |

where mâ‰¥1m\geq 1 and Î»k,Î±kâˆˆâ„\lambda\_{k},\alpha\_{k}\in\mathbb{R}.
For a fixed constant C>0C>0, we denote by ğ’©mâ€‹(C)\mathcal{N}\_{m}(C) the class of network functions
whose output weights satisfy the uniform â„“1\ell^{1} constraint

|  |  |  |
| --- | --- | --- |
|  | âˆ‘k=1m|Î»k|â‰¤C.\sum\_{k=1}^{m}|\lambda\_{k}|\leq C. |  |

We omit bias terms for simplicity.
Their inclusion in expressions of the form Ï†â€‹(Î±â€‹x+b)\varphi(\alpha x+b) would not affect any of the
arguments below, as long as the biases remain uniformly bounded.

The analysis relies on the fact that the resulting model class enjoys a controlled analytic
structure.
More precisely, we assume that for each mm there exists a complex neighborhood
UmâŠ‚â„‚U\_{m}\subset\mathbb{C} containing the interval [âˆ’1,1][-1,1] and a constant Cm>0C\_{m}>0 such that every
function gâˆˆğ’©mâ€‹(C)g\in\mathcal{N}\_{m}(C) admits a holomorphic extension to UmU\_{m} satisfying

|  |  |  |
| --- | --- | --- |
|  | supzâˆˆUm|gâ€‹(z)|â‰¤Cm.\sup\_{z\in U\_{m}}|g(z)|\leq C\_{m}. |  |

This assumption captures the uniform analyticity properties induced by the activation function
together with the imposed parameter constraints, and constitutes the only structural hypothesis
used in the comparison arguments developed below.

The â„“1\ell^{1} constraint on the output weights plays a central role in controlling the complexity
of the model.
Such constraints are classical in approximation theory and arise naturally in several contexts,
including Barron-type representations and earlier works on multilayer perceptrons
[[3](https://arxiv.org/html/2601.04914v1#bib.bib1 "Universal approximation bounds for superpositions of a sigmoidal function"), [2](https://arxiv.org/html/2601.04914v1#bib.bib2 "Approximation of functions by a multilayer perceptron: a probabilistic approach")].
Throughout the paper, the constraint is imposed deterministically and uniformly with respect to
the network widthÂ mm.

### 2.2 Analytic extension and uniform bounds

A key consequence of analyticity of the activation function is that it imposes strong
regularity constraints on the functions generated by the network.

###### Assumption 2.1 (Analytic extension).

Fix Ï>1\rho>1 and Lâ‰¥1L\geq 1.
The activation function Ï†\varphi admits a holomorphic extension to an open neighborhood of
the dilated Bernstein ellipse Lâ€‹EÏ:={Lâ€‹z:zâˆˆEÏ}LE\_{\rho}:=\{Lz:z\in E\_{\rho}\}, and we set

|  |  |  |
| --- | --- | --- |
|  | MÏ,Lâ€‹(Ï†):=supzâˆˆLâ€‹EÏ|Ï†â€‹(z)|<âˆ.M\_{\rho,L}(\varphi):=\sup\_{z\in LE\_{\rho}}|\varphi(z)|<\infty. |  |

Under this assumption, the analytic structure of network outputs can be controlled
uniformly.

###### Proposition 2.1 (Uniform analytic control).

Assume Ï†\varphi satisfies AssumptionÂ 2.1.
Let Lâ‰¥1L\geq 1 and consider network functions

|  |  |  |
| --- | --- | --- |
|  | gâ€‹(x)=âˆ‘k=1mÎ»kâ€‹Ï†â€‹(Î±kâ€‹x),xâˆˆ[âˆ’1,1],g(x)=\sum\_{k=1}^{m}\lambda\_{k}\,\varphi(\alpha\_{k}x),\qquad x\in[-1,1], |  |

with âˆ‘k=1m|Î»k|â‰¤C\sum\_{k=1}^{m}|\lambda\_{k}|\leq C and |Î±k|â‰¤L|\alpha\_{k}|\leq L for all kk.
Then gg admits a holomorphic extension to EÏE\_{\rho} and satisfies

|  |  |  |
| --- | --- | --- |
|  | supzâˆˆEÏ|gâ€‹(z)|â‰¤Câ€‹MÏ,Lâ€‹(Ï†).\sup\_{z\in E\_{\rho}}|g(z)|\leq C\,M\_{\rho,L}(\varphi). |  |

###### Proof.

Let gâ€‹(x)=âˆ‘k=1mÎ»kâ€‹Ï†â€‹(Î±kâ€‹x)g(x)=\sum\_{k=1}^{m}\lambda\_{k}\,\varphi(\alpha\_{k}x).
Fix zâˆˆEÏz\in E\_{\rho}. Since |Î±k|â‰¤L|\alpha\_{k}|\leq L, we have Î±kâ€‹zâˆˆLâ€‹EÏ\alpha\_{k}z\in LE\_{\rho}.
By AssumptionÂ 2.1, Ï†\varphi is holomorphic on a neighborhood of Lâ€‹EÏLE\_{\rho}; hence
zâ†¦Ï†â€‹(Î±kâ€‹z)z\mapsto\varphi(\alpha\_{k}z) is holomorphic on EÏE\_{\rho}, and

|  |  |  |
| --- | --- | --- |
|  | |Ï†â€‹(Î±kâ€‹z)|â‰¤supwâˆˆLâ€‹EÏ|Ï†â€‹(w)|=MÏ,Lâ€‹(Ï†).|\varphi(\alpha\_{k}z)|\leq\sup\_{w\in LE\_{\rho}}|\varphi(w)|=M\_{\rho,L}(\varphi). |  |

Therefore, by linearity and the â„“1\ell^{1} bound on the coefficients,

|  |  |  |
| --- | --- | --- |
|  | supzâˆˆEÏ|gâ€‹(z)|â‰¤âˆ‘k=1m|Î»k|â€‹supzâˆˆEÏ|Ï†â€‹(Î±kâ€‹z)|â‰¤(âˆ‘k=1m|Î»k|)â€‹MÏ,Lâ€‹(Ï†)â‰¤Câ€‹MÏ,Lâ€‹(Ï†),\sup\_{z\in E\_{\rho}}|g(z)|\leq\sum\_{k=1}^{m}|\lambda\_{k}|\,\sup\_{z\in E\_{\rho}}|\varphi(\alpha\_{k}z)|\leq\Big(\sum\_{k=1}^{m}|\lambda\_{k}|\Big)\,M\_{\rho,L}(\varphi)\leq C\,M\_{\rho,L}(\varphi), |  |

which proves the claim.
âˆ

### 2.3 Smooth non-analytic activations: a Gevrey relaxation

The analyticity assumption imposed in the previous sections can be relaxed to a broader
class of infinitely differentiable activation functions, provided that their smoothness
is controlled in a quantitative manner.
In this subsection, we introduce a canonical relaxation based on Gevrey regularity,
which allows one to retain a weakened but still effective form of the polynomial
approximation barrier established for analytic activations.

#### Gevrey regularity.

Let sâ‰¥1s\geq 1. A function Ï†âˆˆCâˆâ€‹(â„)\varphi\in C^{\infty}(\mathbb{R}) is said to belong to the
Gevrey class Gsâ€‹(â„)G^{s}(\mathbb{R}) if there exist constants CÏ†>0C\_{\varphi}>0 and RÏ†>0R\_{\varphi}>0 such that

|  |  |  |  |
| --- | --- | --- | --- |
|  | suptâˆˆâ„|Ï†(n)â€‹(t)|â‰¤CÏ†â€‹RÏ†nâ€‹(n!)s,âˆ€nâ‰¥0.\sup\_{t\in\mathbb{R}}|\varphi^{(n)}(t)|\leq C\_{\varphi}\,R\_{\varphi}^{\,n}\,(n!)^{s},\qquad\forall n\geq 0. |  | (1) |

The case s=1s=1 corresponds, up to constants, to real-analytic functions, while s>1s>1
describes a large family of CâˆC^{\infty} but non-analytic functions, including standard
compactly supported mollifiers and smooth activations obtained by regularizing
piecewise linear functions.

Gevrey classes provide a classical quantitative relaxation of analyticity, interpolating between
real-analytic and general CâˆC^{\infty} regularity. They play a central role in approximation theory
and the study of sub-exponential approximation rates; see, e.g., RodinoÂ [[9](https://arxiv.org/html/2601.04914v1#bib.bib13 "Linear partial differential operators in gevrey spaces")] or
DeVore and LorentzÂ [[6](https://arxiv.org/html/2601.04914v1#bib.bib4 "Constructive approximation")].

#### Uniform Gevrey control of network outputs.

We consider single-hidden-layer networks of the form

|  |  |  |  |
| --- | --- | --- | --- |
|  | gmâ€‹(x)=âˆ‘k=1mÎ»kâ€‹Ï†â€‹(Î±kâ€‹x),xâˆˆ[âˆ’1,1],g\_{m}(x)=\sum\_{k=1}^{m}\lambda\_{k}\,\varphi(\alpha\_{k}x),\qquad x\in[-1,1], |  | (2) |

under the coefficient constraints

|  |  |  |  |
| --- | --- | --- | --- |
|  | âˆ‘k=1m|Î»k|â‰¤Bm,|Î±k|â‰¤Lm.\sum\_{k=1}^{m}|\lambda\_{k}|\leq B\_{m},\qquad|\alpha\_{k}|\leq L\_{m}. |  | (3) |

###### Proposition 2.2 (Uniform Gevrey control).

Assume that Ï†âˆˆGsâ€‹(â„)\varphi\in G^{s}(\mathbb{R}) satisfies ([1](https://arxiv.org/html/2601.04914v1#S2.E1 "In Gevrey regularity. â€£ 2.3 Smooth non-analytic activations: a Gevrey relaxation â€£ 2 Analytic Activation Networks â€£ Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow Networks")).
Then every function gmg\_{m} of the form ([2](https://arxiv.org/html/2601.04914v1#S2.E2 "In Uniform Gevrey control of network outputs. â€£ 2.3 Smooth non-analytic activations: a Gevrey relaxation â€£ 2 Analytic Activation Networks â€£ Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow Networks")) satisfies

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–gm(n)â€–Lâˆâ€‹([âˆ’1,1])â‰¤CÏ†â€‹Bmâ€‹(RÏ†â€‹Lm)nâ€‹(n!)s,âˆ€nâ‰¥0.\|g\_{m}^{(n)}\|\_{L^{\infty}([-1,1])}\leq C\_{\varphi}\,B\_{m}\,(R\_{\varphi}L\_{m})^{n}\,(n!)^{s},\qquad\forall n\geq 0. |  | (4) |

In particular, the family of network outputs is uniformly bounded in the Gevrey class
Gsâ€‹([âˆ’1,1])G^{s}([-1,1]), with constants depending only on BmB\_{m} and LmL\_{m}.

###### Proof.

Differentiating ([2](https://arxiv.org/html/2601.04914v1#S2.E2 "In Uniform Gevrey control of network outputs. â€£ 2.3 Smooth non-analytic activations: a Gevrey relaxation â€£ 2 Analytic Activation Networks â€£ Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow Networks")) nn times yields

|  |  |  |
| --- | --- | --- |
|  | gm(n)â€‹(x)=âˆ‘k=1mÎ»kâ€‹Î±knâ€‹Ï†(n)â€‹(Î±kâ€‹x).g\_{m}^{(n)}(x)=\sum\_{k=1}^{m}\lambda\_{k}\,\alpha\_{k}^{\,n}\,\varphi^{(n)}(\alpha\_{k}x). |  |

Using the triangle inequality, the coefficient constraints
([3](https://arxiv.org/html/2601.04914v1#S2.E3 "In Uniform Gevrey control of network outputs. â€£ 2.3 Smooth non-analytic activations: a Gevrey relaxation â€£ 2 Analytic Activation Networks â€£ Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow Networks")), and the Gevrey bound ([1](https://arxiv.org/html/2601.04914v1#S2.E1 "In Gevrey regularity. â€£ 2.3 Smooth non-analytic activations: a Gevrey relaxation â€£ 2 Analytic Activation Networks â€£ Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow Networks")), we obtain

|  |  |  |
| --- | --- | --- |
|  | |gm(n)â€‹(x)|â‰¤âˆ‘k=1m|Î»k|â€‹|Î±k|nâ€‹suptâˆˆâ„|Ï†(n)â€‹(t)|â‰¤CÏ†â€‹Bmâ€‹(RÏ†â€‹Lm)nâ€‹(n!)s,|g\_{m}^{(n)}(x)|\leq\sum\_{k=1}^{m}|\lambda\_{k}|\,|\alpha\_{k}|^{n}\sup\_{t\in\mathbb{R}}|\varphi^{(n)}(t)|\leq C\_{\varphi}\,B\_{m}\,(R\_{\varphi}L\_{m})^{n}\,(n!)^{s}, |  |

uniformly for xâˆˆ[âˆ’1,1]x\in[-1,1], which proves the claim.
âˆ

### 2.4 Discussion and relation to analytic and Gevrey frameworks

PropositionÂ [2.1](https://arxiv.org/html/2601.04914v1#S2.Thmtheorem1 "Proposition 2.1 (Uniform analytic control). â€£ 2.2 Analytic extension and uniform bounds â€£ 2 Analytic Activation Networks â€£ Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow Networks") and PropositionÂ [2.2](https://arxiv.org/html/2601.04914v1#S2.Thmtheorem2 "Proposition 2.2 (Uniform Gevrey control). â€£ Uniform Gevrey control of network outputs. â€£ 2.3 Smooth non-analytic activations: a Gevrey relaxation â€£ 2 Analytic Activation Networks â€£ Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow Networks") show that, under global â„“1\ell^{1} constraints,
single-hidden-layer networks generate model classes with strong quantitative
regularity, ranging from uniform analyticity to Gevrey smoothness.
This property is central to the comparison arguments developed in the subsequent
sections.

Related analytic regularity phenomena have already been observed in earlier works on
neural network approximation.
In particular, Attali and PagÃ¨s [[2](https://arxiv.org/html/2601.04914v1#bib.bib2 "Approximation of functions by a multilayer perceptron: a probabilistic approach")] studied approximation by
multilayer perceptrons with analytic activation functions and emphasized the strong
regularity induced by analyticity.
Barron-type results [[3](https://arxiv.org/html/2601.04914v1#bib.bib1 "Universal approximation bounds for superpositions of a sigmoidal function")] also rely on global coefficient constraints, but
their conclusions concern approximation on structured target classes rather than
generic functions.

The present work adopts a different perspective.
Rather than exploiting analyticity to derive positive approximation rates, we use it
to identify intrinsic limitations of analytic activation networks on generic target
functions.
The uniform analytic bounds established above will serve as the starting point for the
polynomial comparison arguments developed in SectionsÂ 3 andÂ 4.

#### Historical context.

Explicit low-dimensional instances of the rigidity induced by analyticity were already observed
in early works by Attali and PagÃ¨s [[1](https://arxiv.org/html/2601.04914v1#bib.bib3 "Approximations of functions by a multilayer perceptron: the random case"), [2](https://arxiv.org/html/2601.04914v1#bib.bib2 "Approximation of functions by a multilayer perceptron: a probabilistic approach")].
In particular, these authors provided an explicit bivariate proof of the convergence of partial
derivatives for Bernstein-type approximants, placing the result in an appendix as a classical
technical ingredient in their analysis of multilayer perceptrons.
From a modern perspective, this argument can be viewed as an early manifestation of the
general rigidity phenomena associated with globally parametrized analytic approximation schemes.

## 3 Polynomial Approximation of Analytic Functions

In this section, we recall classical results on the approximation of analytic functions
by algebraic polynomials.
These results form a cornerstone of approximation theory and will be used as a key
ingredient in the proof of the main lower bound.
Standard references include the works of Bernstein, Timan, DeVore and Lorentz, and
more recent expositions such as [[4](https://arxiv.org/html/2601.04914v1#bib.bib7 "LeÃ§ons sur les propriÃ©tÃ©s extrÃ©males et la meilleure approximation des fonctions analytiques dâ€™une variable rÃ©elle"), [10](https://arxiv.org/html/2601.04914v1#bib.bib8 "Theory of approximation of functions of a real variable"), [6](https://arxiv.org/html/2601.04914v1#bib.bib4 "Constructive approximation"), [11](https://arxiv.org/html/2601.04914v1#bib.bib10 "Approximation theory and approximation practice")].

### 3.1 Bernstein ellipses and analytic norms

Let K=[âˆ’1,1]K=[-1,1].
For Ï>1\rho>1, denote by EÏE\_{\rho} the Bernstein ellipse with foci at Â±1\pm 1 and parameter
Ï\rho, defined as the image of the circle {wâˆˆâ„‚:|w|=Ï}\{w\in\mathbb{C}:\ |w|=\rho\} under the
Joukowski map

|  |  |  |
| --- | --- | --- |
|  | wâ†¦12â€‹(w+wâˆ’1).w\mapsto\tfrac{1}{2}\bigl(w+w^{-1}\bigr). |  |

The ellipse EÏE\_{\rho} is a compact subset of â„‚\mathbb{C} containing [âˆ’1,1][-1,1].

For a function hh holomorphic on EÏE\_{\rho}, we define the associated analytic norm by

|  |  |  |
| --- | --- | --- |
|  | MÏâ€‹(h)=supzâˆˆEÏ|hâ€‹(z)|.M\_{\rho}(h)=\sup\_{z\in E\_{\rho}}|h(z)|. |  |

This quantity provides a convenient measure of the strength of analyticity of hh in a
neighborhood of the real interval.

### 3.2 Best polynomial approximation

For a continuous function hâˆˆCâ€‹([âˆ’1,1])h\in C([-1,1]), we denote by

|  |  |  |
| --- | --- | --- |
|  | Em(h)=inf{âˆ¥hâˆ’pâˆ¥âˆ:pÂ is a polynomial of degree at mostÂ m}E\_{m}(h)=\inf\bigl\{\|h-p\|\_{\infty}:\ p\text{ is a polynomial of degree at most }m\bigr\} |  |

the error of best uniform polynomial approximation of degree at most mm.

If hh is merely continuous, the decay of Emâ€‹(h)E\_{m}(h) is typically polynomial and governed
by the smoothness of hh.
In contrast, analyticity of hh implies exponentially fast decay of the approximation
error.

###### Proposition 3.1 (Bernstein-type inequality).

Let Ï>1\rho>1 and let hh be holomorphic on EÏE\_{\rho}.
Then there exists a constant AÏ>0A\_{\rho}>0, depending only on Ï\rho, such that

|  |  |  |
| --- | --- | --- |
|  | Emâ€‹(h)â‰¤AÏâ€‹MÏâ€‹(h)â€‹Ïâˆ’m,mâ‰¥1.E\_{m}(h)\leq A\_{\rho}\,M\_{\rho}(h)\,\rho^{-m},\qquad m\geq 1. |  |

This result is classical and can be found in many textbooks on approximation theory;
see for instance [[4](https://arxiv.org/html/2601.04914v1#bib.bib7 "LeÃ§ons sur les propriÃ©tÃ©s extrÃ©males et la meilleure approximation des fonctions analytiques dâ€™une variable rÃ©elle"), [10](https://arxiv.org/html/2601.04914v1#bib.bib8 "Theory of approximation of functions of a real variable"), [6](https://arxiv.org/html/2601.04914v1#bib.bib4 "Constructive approximation"), [11](https://arxiv.org/html/2601.04914v1#bib.bib10 "Approximation theory and approximation practice")].

### 3.3 Consequences for analytic model classes

PropositionÂ [4.1](https://arxiv.org/html/2601.04914v1#S4.Thmtheorem1 "Proposition 4.1 (Bernstein inequality). â€£ 4.1 Bernstein ellipses and polynomial approximation â€£ 4 Polynomial approximation barrier â€£ Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow Networks") shows that any family of functions admitting a uniform
analytic extension to a fixed Bernstein ellipse necessarily enjoys exponentially fast
polynomial approximation.

More precisely, if a class â„±âŠ‚Câ€‹([âˆ’1,1])\mathcal{F}\subset C([-1,1]) satisfies

|  |  |  |
| --- | --- | --- |
|  | suphâˆˆâ„±MÏâ€‹(h)<âˆ\sup\_{h\in\mathcal{F}}M\_{\rho}(h)<\infty |  |

for some Ï>1\rho>1, then every function in â„±\mathcal{F} can be approximated uniformly by
polynomials at an exponential rate.

As shown in SectionÂ 2, networks with analytic
activation functions with uniformly â„“1\ell^{1}-bounded
coefficients generate families of functions that satisfy exactly such uniform analytic
bounds.
This observation provides the crucial link between neural network approximation and
classical polynomial approximation and forms the basis of the comparison argument
developed in the next section. From the viewpoint of classical approximation theory, networks with analytic
activation functions with
global coefficient constraints form a precompact family in C([-1,1]).
This observation explains why approximation rates beyond those achieved by polynomial
methods cannot be expected on generic target functions.

#### Remark (Gevrey extension).

Although the results of this section are stated for analytic functions,
classical approximation theory provides analogous estimates for functions
with Gevrey regularity.
In that case, exponential polynomial approximation rates are replaced by
sub-exponential bounds depending on the Gevrey index.
Since our arguments rely only on quantitative polynomial approximation
estimates, the comparison results of SectionÂ 4 extend verbatim to the
Gevrey setting introduced in SectionÂ 2.3.

## 4 Polynomial approximation barrier

This section establishes a lower bound showing that single-hidden-layer neural networks with
analytic or Gevrey activation functions and globally controlled coefficients remain confined to
classical polynomial approximation regimes on generic target functions. The argument relies on
quantitative regularity of the model class and a direct comparison with best polynomial
approximation.

### 4.1 Bernstein ellipses and polynomial approximation

For Ï>1\rho>1, let EÏE\_{\rho} denote the Bernstein ellipse with foci Â±1\pm 1. For a function hh
holomorphic on EÏE\_{\rho}, define

|  |  |  |
| --- | --- | --- |
|  | MÏâ€‹(h):=supzâˆˆEÏ|hâ€‹(z)|.M\_{\rho}(h):=\sup\_{z\in E\_{\rho}}|h(z)|. |  |

The error of best uniform polynomial approximation of degree at most mm is

|  |  |  |
| --- | --- | --- |
|  | Emâ€‹(h):=infdegâ¡pâ‰¤mâ€–hâˆ’pâ€–Lâˆâ€‹([âˆ’1,1]).E\_{m}(h):=\inf\_{\deg p\leq m}\|h-p\|\_{L^{\infty}([-1,1])}. |  |

###### Proposition 4.1 (Bernstein inequality).

For every Ï>1\rho>1 there exists a constant Câ€‹(Ï)>0C(\rho)>0 such that, for all functions hh
holomorphic on EÏE\_{\rho},

|  |  |  |
| --- | --- | --- |
|  | Emâ€‹(h)â‰¤Câ€‹(Ï)â€‹MÏâ€‹(h)â€‹Ïâˆ’m,mâ‰¥1.E\_{m}(h)\leq C(\rho)\,M\_{\rho}(h)\,\rho^{-m},\qquad m\geq 1. |  |

### 4.2 Analytic activation functions

Let Ï†:â„â†’â„\varphi:\mathbb{R}\to\mathbb{R} be a real-analytic activation function.
For mâ‰¥1m\geq 1, consider single-hidden-layer networks of the form

|  |  |  |
| --- | --- | --- |
|  | gmâ€‹(x)=âˆ‘k=1mÎ»kâ€‹Ï†â€‹(Î±kâ€‹x),xâˆˆ[âˆ’1,1],g\_{m}(x)=\sum\_{k=1}^{m}\lambda\_{k}\varphi(\alpha\_{k}x),\qquad x\in[-1,1], |  |

under the constraints

|  |  |  |
| --- | --- | --- |
|  | âˆ‘k=1m|Î»k|â‰¤Bm,|Î±k|â‰¤Lm.\sum\_{k=1}^{m}|\lambda\_{k}|\leq B\_{m},\qquad|\alpha\_{k}|\leq L\_{m}. |  |

###### Assumption 4.1 (Analytic extension on a Bernstein ellipse).

There exist Ï>1\rho>1 and Lâ‰¥1L\geq 1 such that Ï†\varphi admits a holomorphic extension to a
neighborhood of the dilated ellipse Lâ€‹EÏ:={Lâ€‹z:zâˆˆEÏ}LE\_{\rho}:=\{Lz:\ z\in E\_{\rho}\}, and

|  |  |  |
| --- | --- | --- |
|  | MÏ,Lâ€‹(Ï†):=supwâˆˆLâ€‹EÏ|Ï†â€‹(w)|<âˆ.M\_{\rho,L}(\varphi):=\sup\_{w\in LE\_{\rho}}|\varphi(w)|<\infty. |  |

###### Lemma 4.1 (Uniform analytic control).

Assume that Ï†\varphi satisfies AssumptionÂ [4.1](https://arxiv.org/html/2601.04914v1#S4.Thmassumption1 "Assumption 4.1 (Analytic extension on a Bernstein ellipse). â€£ 4.2 Analytic activation functions â€£ 4 Polynomial approximation barrier â€£ Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow Networks") and that Lmâ‰¤LL\_{m}\leq L.
Then gmg\_{m} admits a holomorphic extension to EÏE\_{\rho} and

|  |  |  |
| --- | --- | --- |
|  | MÏâ€‹(gm)â‰¤Bmâ€‹MÏ,Lâ€‹(Ï†).M\_{\rho}(g\_{m})\leq B\_{m}\,M\_{\rho,L}(\varphi). |  |

###### Proof.

For zâˆˆEÏz\in E\_{\rho} and 1â‰¤kâ‰¤m1\leq k\leq m, the bound |Î±k|â‰¤L|\alpha\_{k}|\leq L implies Î±kâ€‹zâˆˆLâ€‹EÏ\alpha\_{k}z\in LE\_{\rho}.
Thus

|  |  |  |
| --- | --- | --- |
|  | |Ï†â€‹(Î±kâ€‹z)|â‰¤MÏ,Lâ€‹(Ï†),|\varphi(\alpha\_{k}z)|\leq M\_{\rho,L}(\varphi), |  |

and

|  |  |  |
| --- | --- | --- |
|  | |gmâ€‹(z)|â‰¤âˆ‘k=1m|Î»k|â€‹|Ï†â€‹(Î±kâ€‹z)|â‰¤Bmâ€‹MÏ,Lâ€‹(Ï†).|g\_{m}(z)|\leq\sum\_{k=1}^{m}|\lambda\_{k}|\,|\varphi(\alpha\_{k}z)|\leq B\_{m}\,M\_{\rho,L}(\varphi). |  |

âˆ

###### Proposition 4.2 (Polynomial approximation of analytic network outputs).

Under the assumptions of LemmaÂ [4.1](https://arxiv.org/html/2601.04914v1#S4.Thmlemma1 "Lemma 4.1 (Uniform analytic control). â€£ 4.2 Analytic activation functions â€£ 4 Polynomial approximation barrier â€£ Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow Networks"), there exists Câ€‹(Ï)>0C(\rho)>0 such that

|  |  |  |
| --- | --- | --- |
|  | Emâ€‹(gm)â‰¤Câ€‹(Ï)â€‹Bmâ€‹MÏ,Lâ€‹(Ï†)â€‹Ïâˆ’m.E\_{m}(g\_{m})\leq C(\rho)\,B\_{m}\,M\_{\rho,L}(\varphi)\,\rho^{-m}. |  |

###### Theorem 4.3 (Polynomial approximation barrier: analytic case).

Let Ï†\varphi satisfy AssumptionÂ [4.1](https://arxiv.org/html/2601.04914v1#S4.Thmassumption1 "Assumption 4.1 (Analytic extension on a Bernstein ellipse). â€£ 4.2 Analytic activation functions â€£ 4 Polynomial approximation barrier â€£ Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow Networks"). Then for all
fâˆˆCâ€‹([âˆ’1,1])f\in C([-1,1]) and all mâ‰¥1m\geq 1,

|  |  |  |
| --- | --- | --- |
|  | infgmâ€–fâˆ’gmâ€–Lâˆâ€‹([âˆ’1,1])â‰¥Emâ€‹(f)âˆ’Câ€‹(Ï)â€‹Bmâ€‹MÏ,Lâ€‹(Ï†)â€‹Ïâˆ’m,\inf\_{g\_{m}}\|f-g\_{m}\|\_{L^{\infty}([-1,1])}\geq E\_{m}(f)-C(\rho)\,B\_{m}\,M\_{\rho,L}(\varphi)\,\rho^{-m}, |  |

where the infimum is taken over all networks satisfying the above constraints.

###### Proof.

Let pmp\_{m} be a best polynomial approximant of degree at most mm to gmg\_{m}. Then

|  |  |  |
| --- | --- | --- |
|  | â€–fâˆ’gmâ€–âˆâ‰¥â€–fâˆ’pmâ€–âˆâˆ’â€–gmâˆ’pmâ€–âˆâ‰¥Emâ€‹(f)âˆ’Emâ€‹(gm).\|f-g\_{m}\|\_{\infty}\geq\|f-p\_{m}\|\_{\infty}-\|g\_{m}-p\_{m}\|\_{\infty}\geq E\_{m}(f)-E\_{m}(g\_{m}). |  |

Apply PropositionÂ [4.2](https://arxiv.org/html/2601.04914v1#S4.Thmtheorem2 "Proposition 4.2 (Polynomial approximation of analytic network outputs). â€£ 4.2 Analytic activation functions â€£ 4 Polynomial approximation barrier â€£ Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow Networks").
âˆ

###### Remark 4.4 (Optimized scaling).

If Ï†\varphi is holomorphic and bounded only on a strip |â„‘â¡z|<Î´|\Im z|<\delta, then gmg\_{m} is
holomorphic on |â„‘â¡z|<Î´/Lm|\Im z|<\delta/L\_{m}. Choosing Ïm>1\rho\_{m}>1 such that
EÏmâŠ‚{|â„‘â¡z|<Î´/Lm}E\_{\rho\_{m}}\subset\{|\Im z|<\delta/L\_{m}\} with Ïm=1+câ€‹Î´/Lm\rho\_{m}=1+c\,\delta/L\_{m} yields

|  |  |  |
| --- | --- | --- |
|  | Emâ€‹(gm)â‰¤Aâ€‹Bmâ€‹expâ¡(âˆ’câ€‹mLm),E\_{m}(g\_{m})\leq A\,B\_{m}\,\exp\!\Big(-c\,\frac{m}{L\_{m}}\Big), |  |

and the lower bound of TheoremÂ [4.3](https://arxiv.org/html/2601.04914v1#S4.Thmtheorem3 "Theorem 4.3 (Polynomial approximation barrier: analytic case). â€£ 4.2 Analytic activation functions â€£ 4 Polynomial approximation barrier â€£ Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow Networks") holds with this optimized residual.

### 4.3 Extension to Gevrey activation functions

Let sâ‰¥1s\geq 1. A function ÏˆâˆˆCâˆâ€‹([âˆ’1,1])\psi\in C^{\infty}([-1,1]) belongs to the Gevrey class Gsâ€‹([âˆ’1,1])G^{s}([-1,1])
if there exist constants CÏˆ,RÏˆ>0C\_{\psi},R\_{\psi}>0 such that

|  |  |  |
| --- | --- | --- |
|  | â€–Ïˆ(n)â€–Lâˆâ€‹([âˆ’1,1])â‰¤CÏˆâ€‹RÏˆnâ€‹(n!)s,nâ‰¥0.\|\psi^{(n)}\|\_{L^{\infty}([-1,1])}\leq C\_{\psi}\,R\_{\psi}^{\,n}\,(n!)^{s},\qquad n\geq 0. |  |

###### Proposition 4.5 (Polynomial approximation of Gevrey functions).

If hâˆˆGsâ€‹([âˆ’1,1])h\in G^{s}([-1,1]), then there exist constants A,c>0A,c>0 such that

|  |  |  |
| --- | --- | --- |
|  | Emâ€‹(h)â‰¤Aâ€‹expâ¡(âˆ’câ€‹m1/s),mâ‰¥1.E\_{m}(h)\leq A\,\exp\!\big(-c\,m^{1/s}\big),\qquad m\geq 1. |  |

Let Ï†âˆˆGsâ€‹(â„)\varphi\in G^{s}(\mathbb{R}) and consider networks gmg\_{m} as above.

###### Lemma 4.2 (Uniform Gevrey bounds for network outputs).

There exist constants CÏ†,RÏ†>0C\_{\varphi},R\_{\varphi}>0 such that

|  |  |  |
| --- | --- | --- |
|  | â€–gm(n)â€–Lâˆâ€‹([âˆ’1,1])â‰¤CÏ†â€‹Bmâ€‹(RÏ†â€‹Lm)nâ€‹(n!)s,nâ‰¥0.\|g\_{m}^{(n)}\|\_{L^{\infty}([-1,1])}\leq C\_{\varphi}\,B\_{m}\,(R\_{\varphi}L\_{m})^{n}\,(n!)^{s},\qquad n\geq 0. |  |

###### Proof.

Differentiation yields

|  |  |  |
| --- | --- | --- |
|  | gm(n)â€‹(x)=âˆ‘k=1mÎ»kâ€‹Î±knâ€‹Ï†(n)â€‹(Î±kâ€‹x).g\_{m}^{(n)}(x)=\sum\_{k=1}^{m}\lambda\_{k}\alpha\_{k}^{n}\varphi^{(n)}(\alpha\_{k}x). |  |

Using the coefficient bounds and the Gevrey estimate on Ï†\varphi gives the result.
âˆ

###### Theorem 4.6 (Polynomial approximation barrier: Gevrey case).

Let Ï†âˆˆGsâ€‹(â„)\varphi\in G^{s}(\mathbb{R}) with s>1s>1. Then there exist constants A,c>0A,c>0 such that for all
fâˆˆCâ€‹([âˆ’1,1])f\in C([-1,1]) and all mâ‰¥1m\geq 1,

|  |  |  |
| --- | --- | --- |
|  | infgmâ€–fâˆ’gmâ€–Lâˆâ€‹([âˆ’1,1])â‰¥Emâ€‹(f)âˆ’Aâ€‹Bmâ€‹expâ¡(âˆ’câ€‹(m/Lm)1/s).\inf\_{g\_{m}}\|f-g\_{m}\|\_{L^{\infty}([-1,1])}\geq E\_{m}(f)-A\,B\_{m}\,\exp\!\Big(-c\,(m/L\_{m})^{1/s}\Big). |  |

###### Proof.

Proceed as in the analytic case:

|  |  |  |
| --- | --- | --- |
|  | â€–fâˆ’gmâ€–âˆâ‰¥Emâ€‹(f)âˆ’Emâ€‹(gm).\|f-g\_{m}\|\_{\infty}\geq E\_{m}(f)-E\_{m}(g\_{m}). |  |

Apply PropositionÂ [4.5](https://arxiv.org/html/2601.04914v1#S4.Thmtheorem5 "Proposition 4.5 (Polynomial approximation of Gevrey functions). â€£ 4.3 Extension to Gevrey activation functions â€£ 4 Polynomial approximation barrier â€£ Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow Networks") to gmg\_{m}, using LemmaÂ [4.2](https://arxiv.org/html/2601.04914v1#S4.Thmlemma2 "Lemma 4.2 (Uniform Gevrey bounds for network outputs). â€£ 4.3 Extension to Gevrey activation functions â€£ 4 Polynomial approximation barrier â€£ Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow Networks").
âˆ

###### Remark 4.7 (Extended regularization paradox).

The above results show that the effectiveness of neural network approximation is governed by
the *combined growth* of the output weights and the inner parameters.
In the analytic setting, the residual term in
TheoremÂ [4.3](https://arxiv.org/html/2601.04914v1#S4.Thmtheorem3 "Theorem 4.3 (Polynomial approximation barrier: analytic case). â€£ 4.2 Analytic activation functions â€£ 4 Polynomial approximation barrier â€£ Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow Networks") tends to zero whenever

|  |  |  |
| --- | --- | --- |
|  | logâ¡Bm=oâ€‹(mLm),\log B\_{m}=o\!\left(\frac{m}{L\_{m}}\right), |  |

in which case the network class remains confined to the same approximation regime as
classical polynomial methods on generic target functions.

More generally, under the Gevrey regularity assumptions of
TheoremÂ [4.6](https://arxiv.org/html/2601.04914v1#S4.Thmtheorem6 "Theorem 4.6 (Polynomial approximation barrier: Gevrey case). â€£ 4.3 Extension to Gevrey activation functions â€£ 4 Polynomial approximation barrier â€£ Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow Networks") with index s>1s>1, the corresponding condition becomes

|  |  |  |
| --- | --- | --- |
|  | logâ¡Bm=oâ€‹((mLm)1/s),\log B\_{m}=o\!\left(\left(\frac{m}{L\_{m}}\right)^{1/s}\right), |  |

and the exponential residual term is replaced by a sub-exponential one.
In both cases, regularization mechanisms that control only the magnitude of the output
weights are insufficient to guarantee improved approximation power.
As long as the combined growth condition holds, the model class remains locked into an
approximation regime dictated by its quantitative regularity, and cannot adapt efficiently
to non-analytic or non-Gevrey features of generic target functions.

###### Remark 4.8 (Relation with Barron-type weighted constraints).

Barron-type approximation results are based on a weighted variation control of the form

|  |  |  |  |
| --- | --- | --- | --- |
|  | âˆ‘k=1m|Î»k|â€‹â€–Î±kâ€–â‰¤CB,\sum\_{k=1}^{m}|\lambda\_{k}|\,\|\alpha\_{k}\|\leq C\_{\mathrm{B}}, |  | (5) |

which naturally arises when discretizing an integral representation of the target function.
This constraint differs fundamentally from the uniform bounds considered in the present work,
as it does not impose any *a priori* restriction on the magnitude of the inner parameters
Î±k\alpha\_{k}.

*Derivative control.*
UnderÂ ([5](https://arxiv.org/html/2601.04914v1#S4.E5 "In Remark 4.8 (Relation with Barron-type weighted constraints). â€£ 4.3 Extension to Gevrey activation functions â€£ 4 Polynomial approximation barrier â€£ Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow Networks")), and assuming that Ï†\varphi is continuously differentiable
with locally bounded derivative, the associated network function

|  |  |  |
| --- | --- | --- |
|  | gâ€‹(x)=âˆ‘k=1mÎ»kâ€‹Ï†â€‹(âŸ¨Î±k,xâŸ©)g(x)=\sum\_{k=1}^{m}\lambda\_{k}\,\varphi(\langle\alpha\_{k},x\rangle) |  |

admits a uniformly bounded gradient on [âˆ’1,1]d[-1,1]^{d}, namely

|  |  |  |
| --- | --- | --- |
|  | â€–âˆ‡gâ€–Lâˆâ€‹([âˆ’1,1]d)â‰¤sup|t|â‰¤Lâ€‹d|Ï†â€²â€‹(t)|â€‹âˆ‘k=1m|Î»k|â€‹â€–Î±kâ€–,L:=maxkâ¡â€–Î±kâ€–.\|\nabla g\|\_{L^{\infty}([-1,1]^{d})}\;\leq\;\sup\_{|t|\leq L\sqrt{d}}|\varphi^{\prime}(t)|\,\sum\_{k=1}^{m}|\lambda\_{k}|\,\|\alpha\_{k}\|,\qquad L:=\max\_{k}\|\alpha\_{k}\|. |  |

In particular, if Ï†â€²\varphi^{\prime} is bounded on â„\mathbb{R}, the Lipschitz constant of gg is
uniformly controlled by CBC\_{\mathrm{B}}, independently of the network width.
This shows that the Barron-type weighted constraint enforces a strong geometric regularity of
the model class.

*Frequency truncation and effective analyticity.*
More generally, the weighted constraintÂ ([5](https://arxiv.org/html/2601.04914v1#S4.E5 "In Remark 4.8 (Relation with Barron-type weighted constraints). â€£ 4.3 Extension to Gevrey activation functions â€£ 4 Polynomial approximation barrier â€£ Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow Networks")) allows one to decompose
the network function as

|  |  |  |
| --- | --- | --- |
|  | g=gâ‰¤L+r>L,gâ‰¤Lâ€‹(x):=âˆ‘â€–Î±kâ€–â‰¤LÎ»kâ€‹Ï†â€‹(âŸ¨Î±k,xâŸ©),g=g\_{\leq L}+r\_{>L},\qquad g\_{\leq L}(x):=\sum\_{\|\alpha\_{k}\|\leq L}\lambda\_{k}\,\varphi(\langle\alpha\_{k},x\rangle), |  |

for any threshold L>0L>0.
The high-frequency remainder satisfies the uniform bound

|  |  |  |
| --- | --- | --- |
|  | â€–r>Lâ€–âˆâ‰¤âˆ‘â€–Î±kâ€–>L|Î»k|â€‹sup|Ï†|â‰¤CBLâ€‹sup|Ï†|.\|r\_{>L}\|\_{\infty}\leq\sum\_{\|\alpha\_{k}\|>L}|\lambda\_{k}|\,\sup|\varphi|\leq\frac{C\_{\mathrm{B}}}{L}\,\sup|\varphi|. |  |

On the other hand, the truncated part gâ‰¤Lg\_{\leq L} admits a holomorphic extension to a complex
neighborhood of [âˆ’1,1]d[-1,1]^{d} whose size scales as 1/L1/L.
Classical Bernstein-type estimates for multivariate analytic functions then imply that
gâ‰¤Lg\_{\leq L} can be approximated by multivariate polynomials of total degree at most mm with
an error decaying as expâ¡(âˆ’câ€‹m/L)\exp(-c\,m/L).

*Interpretation.*
This decomposition shows that Barron-type constructions escape the polynomial approximation
barrier identified in this work by allowing the effective analytic neighborhood to shrink with
the network width through the presence of increasingly large inner parameters.
In this regime, uniform analyticity is lost, and the comparison argument with polynomial
approximation no longer applies.
From this perspective, Barron-type approximation results operate precisely at the boundary of
the analyticity-controlled regime considered here.

### 4.4 Extension to higher dimension

We now extend the previous results to functions defined on the hypercube [âˆ’1,1]d[-1,1]^{d}.
Under the same assumptions on the activation function and the coefficient constraints,
network outputs admit holomorphic extensions to suitable polyelliptic neighborhoods of the
domain.

Classical Bernstein-type inequalities for multivariate analytic functions on polyelliptic
domains yield exponential bounds on the best polynomial approximation error; see, e.g.,
[[5](https://arxiv.org/html/2601.04914v1#bib.bib12 "Polynomials and polynomial inequalities"), [8](https://arxiv.org/html/2601.04914v1#bib.bib11 "Approximation theory of the mlp model in neural networks")].

For fâˆˆCâ€‹([âˆ’1,1]d)f\in C([-1,1]^{d}), we denote by Em(d)â€‹(f)E\_{m}^{(d)}(f) the error of best uniform approximation of
ff by multivariate polynomials of total degree at most mm.

###### Theorem 4.9 (Polynomial approximation barrier in dimension dd).

Let dâ‰¥1d\geq 1 and let Ï†:â„â†’â„\varphi:\mathbb{R}\to\mathbb{R} be a real-analytic activation function
admitting a holomorphic extension to a complex neighborhood of the real axis.
For each mâ‰¥1m\geq 1, let Bm>0B\_{m}>0 and Lm>0L\_{m}>0, and consider the class

|  |  |  |
| --- | --- | --- |
|  | ğ’©m(d)â€‹(Bm,Lm)={gâ€‹(x)=âˆ‘k=1mÎ»kâ€‹Ï†â€‹(âŸ¨Î±k,xâŸ©):âˆ‘k=1m|Î»k|â‰¤Bm,â€–Î±kâ€–â‰¤Lm}.\mathcal{N}\_{m}^{(d)}(B\_{m},L\_{m})=\Bigl\{g(x)=\sum\_{k=1}^{m}\lambda\_{k}\,\varphi(\langle\alpha\_{k},x\rangle):\sum\_{k=1}^{m}|\lambda\_{k}|\leq B\_{m},\ \|\alpha\_{k}\|\leq L\_{m}\Bigr\}. |  |

Then there exist constants A,c>0A,c>0, depending only on Ï†\varphi and dd, such that for all
fâˆˆCâ€‹([âˆ’1,1]d)f\in C([-1,1]^{d}) and all mâ‰¥1m\geq 1,

|  |  |  |
| --- | --- | --- |
|  | infgâˆˆğ’©m(d)â€‹(Bm,Lm)â€–fâˆ’gâ€–Lâˆâ€‹([âˆ’1,1]d)â‰¥Em(d)â€‹(f)âˆ’Aâ€‹Bmâ€‹expâ¡(âˆ’câ€‹mLm).\inf\_{g\in\mathcal{N}\_{m}^{(d)}(B\_{m},L\_{m})}\|f-g\|\_{L^{\infty}([-1,1]^{d})}\;\geq\;E\_{m}^{(d)}(f)-A\,B\_{m}\,\exp\!\Big(-c\,\frac{m}{L\_{m}}\Big). |  |

###### Proof.

The proof follows the same strategy as in the one-dimensional case.
Each ridge function xâ†¦Ï†â€‹(âŸ¨Î±k,xâŸ©)x\mapsto\varphi(\langle\alpha\_{k},x\rangle) admits a holomorphic extension
to a complex neighborhood of [âˆ’1,1]d[-1,1]^{d} whose size scales as 1/â€–Î±kâ€–1/\|\alpha\_{k}\|.
As a consequence, every function in ğ’©m(d)â€‹(Bm,Lm)\mathcal{N}\_{m}^{(d)}(B\_{m},L\_{m}) admits a holomorphic
extension to a polyelliptic neighborhood of width proportional to 1/Lm1/L\_{m}, with a uniformly
bounded analytic norm of order BmB\_{m}.

Applying multivariate Bernstein-type inequalities yields an approximation error bounded by
Aâ€‹Bmâ€‹expâ¡(âˆ’câ€‹m/Lm)A\,B\_{m}\exp(-c\,m/L\_{m}).
The conclusion follows by comparison with the best polynomial approximation of ff.
âˆ

###### Remark 4.10 (Comparison with CkC^{k} approximation rates).

For functions in Ckâ€‹([âˆ’1,1]d)C^{k}([-1,1]^{d}) and not smoother, the error of best uniform approximation by
multivariate polynomials of total degree at most mm cannot decay faster than a polynomial
rate, typically of order mâˆ’k/dm^{-k/d}.
If logâ¡Bm=oâ€‹(m/Lm)\log B\_{m}=o(m/L\_{m}), TheoremÂ [4.9](https://arxiv.org/html/2601.04914v1#S4.Thmtheorem9 "Theorem 4.9 (Polynomial approximation barrier in dimension ğ‘‘). â€£ 4.4 Extension to higher dimension â€£ 4 Polynomial approximation barrier â€£ Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow Networks") shows that analytic activation networks remain
confined to these classical rates on CkC^{k} target functions in dimension dd.

###### Remark 4.11 (On the necessity of coefficient growth).

Escaping the polynomial approximation regime requires either exponential growth of the output
weights or sufficiently fast growth of the inner parameters.
This observation is consistent with the constructive approximation results of
Attali and PagÃ¨sÂ [[2](https://arxiv.org/html/2601.04914v1#bib.bib2 "Approximation of functions by a multilayer perceptron: a probabilistic approach")], where the approximation of high-degree polynomials by
single-hidden-layer networks with analytic activations involves coefficients that become
arbitrarily large as the target degree increases.

## 5 Discussion and Perspectives

The main result of this paper identifies a fundamental limitation of single-hidden-layer
neural networks with analytic activation functions and uniformly â„“1\ell^{1}-bounded
coefficients.
Up to an exponentially small term, the best achievable approximation error on generic
target functions is lower bounded by the error of best polynomial approximation.
This shows that analyticity of the activation function, while central to several
positive approximation results, also induces intrinsic rigidity that prevents
adaptation to non-analytic features.

Our result should be interpreted as complementary to classical Barron-type
approximation theorems.
When the target function itself belongs to a structured analytic class, neural networks
can achieve fast, dimension-independent approximation rates.
In contrast, when no such regularity is assumed on the target, the analytic structure
of the model class becomes a limitation rather than an advantage.
In this sense, analyticity does not provide a universal mechanism to bypass minimax
approximation barriers on generic smoothness classes.

###### Remark 5.1 (Relation with the constructions of Attaliâ€“PagÃ¨s).

In [[2](https://arxiv.org/html/2601.04914v1#bib.bib2 "Approximation of functions by a multilayer perceptron: a probabilistic approach")], fast (in fact arbitrarily fast) approximation of polynomials by
single-hidden-layer neural networks is achieved through a singular parameter regime.
More precisely, the inner parameters scale as Î±k=Oâ€‹(h)\alpha\_{k}=O(h) while the output
coefficients behave as Î»k=Oâ€‹(hâˆ’p)\lambda\_{k}=O(h^{-p}) when approximating polynomials of degree pp.
As a consequence, the Barron-type quantity
âˆ‘k|Î»k|â€‹â€–Î±kâ€–\sum\_{k}|\lambda\_{k}|\,\|\alpha\_{k}\| diverges as hâ†’0h\to 0. These constructions therefore fall outside the coefficient-constrained
framework considered in the present work and do not contradict the polynomial approximation
barriers established here.

The analysis developed in this work is entirely deterministic and relies on a direct
comparison between best network approximation and best polynomial approximation.
This approach differs from probabilistic constructions or random feature methods and
highlights the role of analytic continuation and Bernstein-type estimates in
understanding the expressive power of neural networks.
It provides a transparent explanation of why analytic activation functions alone cannot
improve approximation rates on non-analytic targets.

###### Remark 5.2 (Relation with Barron-type assumptions).

Barron-type approximation results control complexity through a *variation* (or weighted
â„“1\ell^{1}) norm on an integral representation of the network.
In discretized models this corresponds to â„“1\ell^{1}-type constraints on the output weights,
possibly coupled with weighted controls involving the inner parameters.
For a more detailed discussion and a decomposition explaining how Barron-type constructions
escape the uniform analyticity regime considered here, see RemarkÂ [4.8](https://arxiv.org/html/2601.04914v1#S4.Thmtheorem8 "Remark 4.8 (Relation with Barron-type weighted constraints). â€£ 4.3 Extension to Gevrey activation functions â€£ 4 Polynomial approximation barrier â€£ Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow Networks").

###### Remark 5.3 (Sharpness of the lower bound).

The lower bound established in TheoremÂ 4.3 is sharp for a large class of target functions.
Indeed, whenever the best polynomial approximation error satisfies

|  |  |  |
| --- | --- | --- |
|  | Emâ€‹(f)â‰«rm,E\_{m}(f)\gg r\_{m}, |  |

where rmr\_{m} denotes the residual term appearing in TheoremÂ 4.3 (exponentially small in the
analytic case, or sub-exponentially small in the Gevrey case), one has

|  |  |  |
| --- | --- | --- |
|  | infgmâ€–fâˆ’gmâ€–Lâˆâ€‹([âˆ’1,1])=Emâ€‹(f)â€‹(1+oâ€‹(1))(mâ†’âˆ).\inf\_{g\_{m}}\|f-g\_{m}\|\_{L^{\infty}([-1,1])}\;=\;E\_{m}(f)\,(1+o(1))\qquad(m\to\infty). |  |

This includes, in particular, all non-analytic functions in classical smoothness classes
Ckâ€‹([âˆ’1,1])C^{k}([-1,1]) for which Emâ€‹(f)E\_{m}(f) decays at most polynomially.
A canonical example is fâ€‹(x)=|x|f(x)=|x|, for which it is well known that

|  |  |  |
| --- | --- | --- |
|  | Emâ€‹(f)â‰mâˆ’1.E\_{m}(f)\asymp m^{-1}. |  |

For such functions, the residual term is negligible and networks with analytic
activation functions under
the coefficient constraints considered here cannot outperform polynomial approximation.

A natural question is whether a similar rigidity phenomenon persists when the activation
function is no longer analytic but still infinitely differentiable.
In general, such an extension is not possible without additional assumptions, since
CâˆC^{\infty} functions need not admit any holomorphic extension and may exhibit arbitrarily
slow rates of polynomial approximation.
As a consequence, no uniform approximation barrier comparable to the analytic case can
hold for general CâˆC^{\infty} activation functions.

Nevertheless, analogous rigidity effects can be established under stronger quantitative
regularity assumptions, such as Gevrey-type bounds on the growth of derivatives.
In that case, the approximation barrier reflects the corresponding regularity class and
leads to sub-exponential, but still non-adaptive, approximation rates.
This highlights that the phenomenon identified in this work is intrinsically tied to the
analytic structure of the activation function.

Several extensions of the present work can be considered.
First, the comparison argument extends naturally to higher-dimensional settings, where
Bernstein-type estimates for multivariate analytic functions yield analogous polynomial
barriers.
Second, it would be of interest to investigate whether similar limitations persist for
deeper architectures with analytic activations under global coefficient constraints.
Finally, the present results suggest that improved approximation on generic targets
requires either non-analytic activation functions or adaptive mechanisms that allow the
model to escape global analytic regularity constraints.

Overall, our findings contribute to a clearer understanding of the interplay between
activation regularity, model constraints, and approximation power, and help delineate
the precise scope of applicability of analytic neural network models in approximation
theory.

## References

* [1]
  J. Attali and G. PagÃ¨s (1995)
  Approximations of functions by a multilayer perceptron: the random case.
  Neural Processing Letters 2 (3),  pp.Â 20â€“24.
  Cited by: [Â§1](https://arxiv.org/html/2601.04914v1#S1.p1.1 "1 Introduction â€£ Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow Networks"),
  [Â§2.4](https://arxiv.org/html/2601.04914v1#S2.SS4.SSS0.Px1.p1.1 "Historical context. â€£ 2.4 Discussion and relation to analytic and Gevrey frameworks â€£ 2 Analytic Activation Networks â€£ Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow Networks").
* [2]
  J. Attali and G. PagÃ¨s (1997)
  Approximation of functions by a multilayer perceptron: a probabilistic approach.
  Neural Networks 10 (6),  pp.Â 1069â€“1080.
  Cited by: [Â§1](https://arxiv.org/html/2601.04914v1#S1.p1.1 "1 Introduction â€£ Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow Networks"),
  [Â§2.1](https://arxiv.org/html/2601.04914v1#S2.SS1.p4.2 "2.1 Model definition â€£ 2 Analytic Activation Networks â€£ Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow Networks"),
  [Â§2.4](https://arxiv.org/html/2601.04914v1#S2.SS4.SSS0.Px1.p1.1 "Historical context. â€£ 2.4 Discussion and relation to analytic and Gevrey frameworks â€£ 2 Analytic Activation Networks â€£ Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow Networks"),
  [Â§2.4](https://arxiv.org/html/2601.04914v1#S2.SS4.p2.1 "2.4 Discussion and relation to analytic and Gevrey frameworks â€£ 2 Analytic Activation Networks â€£ Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow Networks"),
  [Remark 4.11](https://arxiv.org/html/2601.04914v1#S4.Thmtheorem11.p1.1.1 "Remark 4.11 (On the necessity of coefficient growth). â€£ 4.4 Extension to higher dimension â€£ 4 Polynomial approximation barrier â€£ Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow Networks"),
  [Remark 5.1](https://arxiv.org/html/2601.04914v1#S5.Thmtheorem1.p1.5.5 "Remark 5.1 (Relation with the constructions of Attaliâ€“PagÃ¨s). â€£ 5 Discussion and Perspectives â€£ Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow Networks").
* [3]
  A. R. Barron (1993)
  Universal approximation bounds for superpositions of a sigmoidal function.
  IEEE Transactions on Information Theory 39 (3),  pp.Â 930â€“945.
  Cited by: [Â§1](https://arxiv.org/html/2601.04914v1#S1.p1.1 "1 Introduction â€£ Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow Networks"),
  [Â§2.1](https://arxiv.org/html/2601.04914v1#S2.SS1.p4.2 "2.1 Model definition â€£ 2 Analytic Activation Networks â€£ Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow Networks"),
  [Â§2.4](https://arxiv.org/html/2601.04914v1#S2.SS4.p2.1 "2.4 Discussion and relation to analytic and Gevrey frameworks â€£ 2 Analytic Activation Networks â€£ Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow Networks").
* [4]
  S. N. Bernstein (1912)
  LeÃ§ons sur les propriÃ©tÃ©s extrÃ©males et la meilleure approximation des fonctions analytiques dâ€™une variable rÃ©elle.
   Gauthier-Villars.
  Cited by: [Â§3.2](https://arxiv.org/html/2601.04914v1#S3.SS2.p3.1 "3.2 Best polynomial approximation â€£ 3 Polynomial Approximation of Analytic Functions â€£ Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow Networks"),
  [Â§3](https://arxiv.org/html/2601.04914v1#S3.p1.1 "3 Polynomial Approximation of Analytic Functions â€£ Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow Networks").
* [5]
  P. Borwein and T. ErdÃ©lyi (1995)
  Polynomials and polynomial inequalities.
   Springer.
  Cited by: [Â§4.4](https://arxiv.org/html/2601.04914v1#S4.SS4.p2.1 "4.4 Extension to higher dimension â€£ 4 Polynomial approximation barrier â€£ Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow Networks").
* [6]
  R. A. DeVore and G. G. Lorentz (1993)
  Constructive approximation.
   Springer.
  Cited by: [Â§1](https://arxiv.org/html/2601.04914v1#S1.p1.1 "1 Introduction â€£ Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow Networks"),
  [Â§2.3](https://arxiv.org/html/2601.04914v1#S2.SS3.SSS0.Px1.p2.1 "Gevrey regularity. â€£ 2.3 Smooth non-analytic activations: a Gevrey relaxation â€£ 2 Analytic Activation Networks â€£ Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow Networks"),
  [Â§3.2](https://arxiv.org/html/2601.04914v1#S3.SS2.p3.1 "3.2 Best polynomial approximation â€£ 3 Polynomial Approximation of Analytic Functions â€£ Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow Networks"),
  [Â§3](https://arxiv.org/html/2601.04914v1#S3.p1.1 "3 Polynomial Approximation of Analytic Functions â€£ Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow Networks").
* [7]
  R. A. DeVore (1998)
  Nonlinear approximation.
  Acta Numerica 7,  pp.Â 51â€“150.
  Cited by: [Â§1](https://arxiv.org/html/2601.04914v1#S1.p1.1 "1 Introduction â€£ Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow Networks").
* [8]
  A. Pinkus (1999)
  Approximation theory of the mlp model in neural networks.
  Acta Numerica 8,  pp.Â 143â€“195.
  Cited by: [Â§4.4](https://arxiv.org/html/2601.04914v1#S4.SS4.p2.1 "4.4 Extension to higher dimension â€£ 4 Polynomial approximation barrier â€£ Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow Networks").
* [9]
  L. Rodino (1993)
  Linear partial differential operators in gevrey spaces.
   World Scientific.
  Cited by: [Â§2.3](https://arxiv.org/html/2601.04914v1#S2.SS3.SSS0.Px1.p2.1 "Gevrey regularity. â€£ 2.3 Smooth non-analytic activations: a Gevrey relaxation â€£ 2 Analytic Activation Networks â€£ Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow Networks").
* [10]
  A. F. Timan (1963)
  Theory of approximation of functions of a real variable.
   Pergamon Press.
  Cited by: [Â§3.2](https://arxiv.org/html/2601.04914v1#S3.SS2.p3.1 "3.2 Best polynomial approximation â€£ 3 Polynomial Approximation of Analytic Functions â€£ Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow Networks"),
  [Â§3](https://arxiv.org/html/2601.04914v1#S3.p1.1 "3 Polynomial Approximation of Analytic Functions â€£ Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow Networks").
* [11]
  L. N. Trefethen (2013)
  Approximation theory and approximation practice.
   SIAM.
  Cited by: [Â§3.2](https://arxiv.org/html/2601.04914v1#S3.SS2.p3.1 "3.2 Best polynomial approximation â€£ 3 Polynomial Approximation of Analytic Functions â€£ Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow Networks"),
  [Â§3](https://arxiv.org/html/2601.04914v1#S3.p1.1 "3 Polynomial Approximation of Analytic Functions â€£ Analytic Regularity and Approximation Limits of Coefficient-Constrained Shallow Networks").