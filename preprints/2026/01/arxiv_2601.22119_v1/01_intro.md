---
authors:
- Han Yang
- Dong Hao
- Zhuohan Wang
- Qi Shi
- Xingtong Li
doc_id: arxiv:2601.22119v1
family_id: arxiv:2601.22119
is_current: true
taxonomy:
  alpha_families: []
  asset_classes: []
  horizons: []
  themes: []
title: Alpha Discovery via Grammar-Guided Learning and Search
url_abs: http://arxiv.org/abs/2601.22119v1
url_html: https://arxiv.org/html/2601.22119v1
venue: arXiv q-fin
version: 1
year: 2026
---


Han Yang
â€ƒâ€ƒ
Dong Hao
â€ƒâ€ƒ
Zhuohan Wang
â€ƒâ€ƒ
Qi Shi
â€ƒâ€ƒ
Xingtong Li

###### Abstract

Automatically discovering formulaic alpha factors is a central problem in quantitative finance. Existing methods often ignore syntactic and semantic constraints, relying on exhaustive search over unstructured and unbounded spaces. We present AlphaCFG, a grammar-based framework for defining and discovering alpha factors that are syntactically valid, financially interpretable, and computationally efficient. AlphaCFG uses an alpha-oriented context-free grammar to define a tree-structured, size-controlled search space, and formulates alpha discovery as a tree-structured linguistic Markov decision process, which is then solved using a grammar-aware Monte Carlo Tree Search guided by syntax-sensitive value and policy networks. Experiments on Chinese and U.S. stock market datasets show that AlphaCFG outperforms state-of-the-art baselines in both search efficiency and trading profitability. Beyond trading strategies, AlphaCFG serves as a general framework for symbolic factor discovery and refinement across quantitative finance, including asset pricing and portfolio construction.

Machine Learning, ICML

## 1 Introduction

### 1.1 Alpha discovery

In quantitative finance, alpha factors play a central role in asset management, quantitative trading, and investment decision-making. An alpha factor is an explicit function that maps historical market features, such as prices and volumes, to predictions of future returns. Alpha discovery refers to the systematic identification of such predictive functions from historical data and remains a core challenge due to the vast and complex space of possible functional forms. Beyond their practical importance, alpha discovery poses a fundamental machine-learning challenge: identifying symbolic functions that are both predictive and interpretable under severe combinatorial constraints.

Existing approaches to alpha discovery can be broadly classified into three categories. Heuristic or expert-driven methods rely on financial intuition, such as value factors (e.g., price-to-earnings ratios (Fama and French, [1992](https://arxiv.org/html/2601.22119v1#bib.bib15 "The cross-section of expected stock returns"))) and momentum factors (e.g., past 12-month returns (Carhart, [1997](https://arxiv.org/html/2601.22119v1#bib.bib14 "On persistence in mutual fund performance"))), but lack scalability and are quickly arbitraged once widely adopted, reducing predictive accuracy over time. Data-driven learning methods, including regression (Bhandari et al., [2022](https://arxiv.org/html/2601.22119v1#bib.bib43 "Predicting stock market index using lstm"); Qin et al., [2017](https://arxiv.org/html/2601.22119v1#bib.bib44 "A dual-stage attention-based recurrent neural network for time series prediction"); Dai et al., [2022](https://arxiv.org/html/2601.22119v1#bib.bib45 "Price change prediction of ultra high frequency financial data based on temporal convolutional network"); Mozaffari and Zhang, [2024](https://arxiv.org/html/2601.22119v1#bib.bib46 "Predictive modeling of stock prices using transformer model")), tree-based ensembles (Wang et al., [2023](https://arxiv.org/html/2601.22119v1#bib.bib42 "An xgboost-based multivariate deep learning framework for stock index futures price forecasting"); Bisdoulis, [2024](https://arxiv.org/html/2601.22119v1#bib.bib41 "Assets forecasting with feature engineering and transformation methods for lightgbm")), unsupervised learning (Xu, [2025](https://arxiv.org/html/2601.22119v1#bib.bib47 "Unsupervised temporal encoding for stock price prediction through dual-phase learning")), and reinforcement learning (Lee, [2001](https://arxiv.org/html/2601.22119v1#bib.bib35 "Stock price prediction using reinforcement learning")), can capture complex nonlinear patterns, yet often suffer from limited interpretability and overfitting due to their black-box nature. Formulaic alpha methods (Zhang et al., [2020](https://arxiv.org/html/2601.22119v1#bib.bib3 "Autoalpha: an efficient hierarchical evolutionary algorithm for mining alpha factors in quantitative investment")) emphasize human-readable mathematical expressions composed of predefined operators, offering transparency and interpretability, and have therefore regained recent attention.

Table 1: Comparison of Alpha Discovery Methods

| Category | Pros | Cons |
| --- | --- | --- |
| Heuristic / Expert | Intuitive, easy to use | Limited, quickly arbitraged |
| Data-driven Learning | Captures complex patterns | Black-box, less interpretable |
| Formulaic Alpha | Interpretable, transparent | Computationally expensive |

Our work lies at the intersection of data-driven learning and formulaic alpha methods, aiming at the automatic discovery of explainable alpha factors. This problem can be viewed as symbolic regression (Makke and Chawla, [2024](https://arxiv.org/html/2601.22119v1#bib.bib17 "Interpretable scientific discovery with symbolic regression: a review")), which seeks explicit mathematical expressions that fit data while remaining interpretable, but is difficult due to its combinatorial search space and semantic equivalence among expressions. Early approaches such as genetic programming (GP) (Zhang et al., [2020](https://arxiv.org/html/2601.22119v1#bib.bib3 "Autoalpha: an efficient hierarchical evolutionary algorithm for mining alpha factors in quantitative investment")) evolve expression trees to optimize information coefficients. More recent methods, including AlphaGen (Yu et al., [2023](https://arxiv.org/html/2601.22119v1#bib.bib30 "Generating synergistic formulaic alpha collections via reinforcement learning")) and AlphaQCM (Zhu and Zhu, [2025](https://arxiv.org/html/2601.22119v1#bib.bib29 "AlphaQCM: alpha discovery in finance with distributional reinforcement learning")), adopt reinforcement learning to improve scalability.
Existing methods face the following fundamental challenges.

(1) Lack of linguistic characterization leads to inefficient search in an unbounded space.
Automated discovery of formulaic alphas is fundamentally a problem of searching over mathematical languages, yet existing methods lack an explicit linguistic framework to organize and constrain this search. In the absence of formal grammatical structure, current approaches must explore vast, and often effectively infinite, combinatorial spaces of expressions, relying on ad hoc syntactic checks to ensure validity. This unstructured exploration severely limits sample efficiency, degrades model performance, and incurs substantial computational cost.

(2) Semantic redundancy causes systematic waste in learning and search.
Many syntactically distinct mathematical sequences correspond to the same underlying semantics, but existing methods mostly encode expressions as linear sequences and treat the variants as independent. As a result, semantically equivalent expressions are repeatedly explored and evaluated, leading to significant redundancy in representation learning and search, and greatly reducing efficiency.

### 1.2 Our Work

We propose AlphaCFG,111Our source code is available at <https://github.com/HanYang544/AlphaCFG> a general linguisticâ€“learning framework for the automatic discovery of interpretable alpha factors. The central idea is to treat alpha discovery as a structured language generation and learning problem, rather than an unstructured search over mathematical expressions. By combining formal grammar with learning and search, AlphaCFG provides a principled way to generate, validate, and optimize human-readable alpha factors. In this way, grammar serves as an explicit inductive bias that shapes both the search space and the learning dynamics.

(1) Grammar-Constrained Alpha Factors.
From a language-theoretic perspective, we first formalize the space of alpha factors as a structured mathematical language.
We propose two formal languages, Î±\alpha-Syn and Î±\alpha-Sem, that integrate context-free grammar (CFG) with finance domainâ€“specific knowledge of alpha factors. Î±\alpha-Syn enforces grammatical correctness, while Î±\alpha-Sem further ensures financial semantic validity. These languages generate alpha expressions recursively in a tree-structured form, making tree-structureâ€“based learning and optimization possible. To control complexity and reduce redundancy, we further enforce (i) length constraints to bound the search space, and (ii) expression-tree pruning to remove syntactically distinct but semantically equivalent factors.

(2) Structure Characterization of Alpha Space.
Building on this grammar-based language, we cast alpha discovery as a large Tree-Structured Linguistic Markov Decision Process (TSL-MDP), where each state is a partial expression, terminal states represent complete alpha factors, and rewards are given by the information coefficient (IC) on real market data. This formulation transforms alpha discovery from unstructured trial-and-error into a principled sequential decision process over the space of formulaic alpha factors.

(3) Reinforcing MCTS with Syntax-Aware Learning.
Finally, we design a learning and search algorithm that exploits the grammar-induced structure of the TSL-MDP. We employ a grammar-aware Monte Carlo Tree Search (MCTS), in which action selection is guided by a syntax-aware Upper Confidence Bound (UCB) rule. To generalize across the large state space, each partial expression tree is encoded using a Tree-LSTM, yielding structure-aware representations shared by a value network, which estimates expected performance from historical market data, and a policy network, which predicts promising alpha expansions. Through reinforced interaction between MCTS and these learned models, AlphaCFG progressively refines its search strategy and discovers high-quality alpha factors efficiently.

AlphaCFG is not limited to trading strategies and naturally extends to other quantitative finance tasks, by allowing flexible customization of operators, grammatical structures, and objective functions. We use trading as a representative testbed to demonstrate the effectiveness of AlphaCFG.
We evaluate AlphaCFG on CSIÂ 300 and S&PÂ 500 stocks, where it consistently outperforms strong baselines across multiple metrics, including returns, information coefficient (IC), Sharpe ratio, and maximum drawdown. Our results show that improved grammar design yields faster convergence and higher-quality factors. Moreover, AlphaCFG effectively refines existing factors and improves their predictive performance, highlighting its utility as a general tool for factor refinement and augmentation. Ablation studies further confirm the critical roles of grammar design and syntax-based representation learning in effective factor discovery.

## 2 Problem Formulation

Consider a market with nn stocks over TT trading days.
On each day tâˆˆ{1,2,â€¦,T}t\in\{1,2,\dots,T\}, stock ii is associated with a feature matrix
ğ±t,iâˆˆâ„mÃ—Ï„â€²\mathbf{x}\_{t,i}\in\mathbb{R}^{m\times\tau^{\prime}}, which records mm raw market features
(e.g., opening and closing prices, volumes) over Ï„â€²\tau^{\prime} days.
We denote by
ğ—t=(ğ±t,1,ğ±t,2,â€¦,ğ±t,n)\mathbf{X}\_{t}=(\mathbf{x}\_{t,1},\mathbf{x}\_{t,2},\dots,\mathbf{x}\_{t,n})
the collection of features for all stocks on day tt.

An *alpha factor* is a function
f:â„mÃ—Ï„â€²â†’â„f:\mathbb{R}^{m\times\tau^{\prime}}\rightarrow\mathbb{R}
that maps the historical features of a single stock to a scalar score.
Applying ff cross-sectionally to all stocks on day tt yields a factor vector
ğ²t=(yt,1,â€¦,yt,n)âˆˆâ„n\mathbf{y}\_{t}=(y\_{t,1},\dots,y\_{t,n})\in\mathbb{R}^{n},
where yt,i=fâ€‹(ğ±t,i)y\_{t,i}=f(\mathbf{x}\_{t,i}) (illustrated in [FigureÂ 6(a)](https://arxiv.org/html/2601.22119v1#A3.F6.sf1 "In Figure 6 â€£ Appendix C Supplement to Problem Formulation â€£ Alpha Discovery via Grammar-Guided Learning and Search")).
These scores are subsequently used to rank stocks or construct portfolios.

We focus on *formulaic* alpha factors, which are explicit mathematical expressions constructed from a predefined set of input features ([TableÂ 4](https://arxiv.org/html/2601.22119v1#A1.T4 "In Appendix A Tables â€£ Alpha Discovery via Grammar-Guided Learning and Search")), constants ([TableÂ 5](https://arxiv.org/html/2601.22119v1#A1.T5 "In Appendix A Tables â€£ Alpha Discovery via Grammar-Guided Learning and Search")), and operators ([TableÂ 6](https://arxiv.org/html/2601.22119v1#A1.T6 "In Appendix A Tables â€£ Alpha Discovery via Grammar-Guided Learning and Search")).
These operators and operands are commonly used in quantitative finance (Yang et al., [2020](https://arxiv.org/html/2601.22119v1#bib.bib23 "Qlib: an ai-oriented quantitative investment platform")).
Representative examples are shown in [FigureÂ 6(b)](https://arxiv.org/html/2601.22119v1#A3.F6.sf2 "In Figure 6 â€£ Appendix C Supplement to Problem Formulation â€£ Alpha Discovery via Grammar-Guided Learning and Search").

#### Evaluation via Information Coefficient.

The predictive quality of an alpha factor is evaluated using the *Information Coefficient* (IC), a standard metric in asset management (Grinold and Kahn, [2000](https://arxiv.org/html/2601.22119v1#bib.bib32 "Active portfolio management")).
For a given prediction horizon Ï„\tau, the realized Ï„\tau-day return of stock ii observed at day tt is

|  |  |  |  |
| --- | --- | --- | --- |
|  | rt,i(Ï„)=Closet+Ï„,iCloset,iâˆ’1,r\_{t,i}^{(\tau)}=\frac{\mathrm{Close}\_{t+\tau,i}}{\mathrm{Close}\_{t,i}}-1, |  | (1) |

where Closet,i\mathrm{Close}\_{t,i} is the closing price of stock ii on day tt.
Let ğ«t(Ï„)=(rt,1(Ï„),â€¦,rt,n(Ï„))\mathbf{r}\_{t}^{(\tau)}=(r\_{t,1}^{(\tau)},\dots,r\_{t,n}^{(\tau)}) denote the cross-sectional return vector.
The daily IC at day tt is defined as the Pearson correlation between factor scores and subsequent returns:

|  |  |  |
| --- | --- | --- |
|  | ICtâ€‹(ğ²t,ğ«t(Ï„))=âˆ‘i=1n(yt,iâˆ’yÂ¯t)â€‹(rt,i(Ï„)âˆ’rÂ¯t(Ï„))âˆ‘i=1n(yt,iâˆ’yÂ¯t)2â€‹âˆ‘i=1n(rt,i(Ï„)âˆ’rÂ¯t(Ï„))2,\mathrm{IC}\_{t}(\mathbf{y}\_{t},\mathbf{r}\_{t}^{(\tau)})=\frac{\sum\_{i=1}^{n}(y\_{t,i}-\bar{y}\_{t})(r\_{t,i}^{(\tau)}-\overline{r}\_{t}^{(\tau)})}{\sqrt{\sum\_{i=1}^{n}(y\_{t,i}-\bar{y}\_{t})^{2}}\sqrt{\sum\_{i=1}^{n}(r\_{t,i}^{(\tau)}-\overline{r}\_{t}^{(\tau)})^{2}}}, |  |

where
yÂ¯t=1nâ€‹âˆ‘i=1nyt,i\bar{y}\_{t}=\frac{1}{n}\sum\_{i=1}^{n}y\_{t,i} and
rÂ¯t(Ï„)=1nâ€‹âˆ‘i=1nrt,i(Ï„)\overline{r}\_{t}^{(\tau)}=\frac{1}{n}\sum\_{i=1}^{n}r\_{t,i}^{(\tau)}.

To assess factor performance over the entire period, we use average daily IC of alpha factor ff:

|  |  |  |  |
| --- | --- | --- | --- |
|  | ICâ€‹(f)=1Tâ€‹âˆ‘t=1TICtâ€‹(ğ²t,ğ«t(Ï„)).\mathrm{IC}(f)=\frac{1}{T}\sum\_{t=1}^{T}\mathrm{IC}\_{t}\bigl(\mathbf{y}\_{t},\mathbf{r}\_{t}^{(\tau)}\bigr). |  | (2) |

A higher ICâ€‹(f)\mathrm{IC}(f) indicates stronger predictive power.
Accordingly, the goal of *alpha discovery* is to identify formulaic factors that maximize ICâ€‹(f)\mathrm{IC}(f).

In practice, a common and effective strategy is to linearly combine multiple factors.
Following AlphaGen (Yu et al., [2023](https://arxiv.org/html/2601.22119v1#bib.bib30 "Generating synergistic formulaic alpha collections via reinforcement learning")), we optimize the IC of such linear combinations (referred to as a *factor pool*).
The detailed combination procedure is provided in [AlgorithmÂ 1](https://arxiv.org/html/2601.22119v1#alg1 "In B.1 Linear combination alpha factor algorithm â€£ Appendix B Algorithms â€£ Alpha Discovery via Grammar-Guided Learning and Search") in AppendixÂ [B.1](https://arxiv.org/html/2601.22119v1#A2.SS1 "B.1 Linear combination alpha factor algorithm â€£ Appendix B Algorithms â€£ Alpha Discovery via Grammar-Guided Learning and Search").

## 3 Design Language of Interpretable Alphas

The space of formulaic alpha factors grows combinatorially with expression length, rendering brute-force search inefficient.
Moreover, a large fraction of candidate expressions are either *syntactically invalid* (i.e., ill-formed operator compositions) or *semantically nonsensical* (i.e., violating financial or temporal constraints), which severely hampers both efficiency and interpretability.

From a machine learning perspective, automated alpha discovery is therefore not merely an optimization problem, but fundamentally a *language design problem*: one must define a hypothesis space that is expressive enough to capture meaningful financial signals, while being sufficiently structured to admit efficient search and learning.
In the absence of such structure, existing methods are forced to explore an effectively unbounded symbolic space, leading to severe combinatorial explosion and redundant evaluations.

To address these challenges, we introduce a formal *linguistic characterization* of alpha factors based on Context-Free Grammar (CFG) (Chomsky and SchÃ¼tzenberger, [1959](https://arxiv.org/html/2601.22119v1#bib.bib1 "The algebraic theory of context-free languages"); Hopcroft and Ullman, [1979](https://arxiv.org/html/2601.22119v1#bib.bib22 "Automata theory, languages, and computation")).
By explicitly specifying the syntactic rules that govern valid alpha expressions, we restrict the search space to well-formed expressions, enforce operator-operand consistency, and enable tree-based search and learning.
This linguistic view allows us to systematically decompose the alpha search space into nested levels of validity, as illustrated in [FigureÂ 1](https://arxiv.org/html/2601.22119v1#S3.F1 "In 3 Design Language of Interpretable Alphas â€£ Alpha Discovery via Grammar-Guided Learning and Search").

Î£âˆ—\Sigma^{\*}â„’sâ€‹yâ€‹n\mathcal{L}\_{syn}â„’sâ€‹eâ€‹m\mathcal{L}\_{sem}â„’semâ‰¤k\mathcal{L}\_{\mathrm{sem}}^{\leq k}


Figure 1: 
Nested spaces of alpha expressions:
Î£âˆ—\Sigma^{\*} (all symbol sequences),
â„’syn\mathcal{L}\_{\mathrm{syn}} (syntactically valid),
â„’sem\mathcal{L}\_{\mathrm{sem}} (semantically valid),
and â„’semâ‰¤K\mathcal{L}\_{\mathrm{sem}}^{\leq K} (length-bounded semantic alphas).

### 3.1 Syntactically-Valid Alpha Language

We begin by defining a grammar that ensures *syntactic validity*, which serves as the foundation for the following sections of semantic constraints and learning algorithms.

Syntactic validity requires that every generated alpha expression be a well-formed and evaluable symbolic program.
It entails two conditions:
(i) a well-defined hierarchical structure enforced by prefix notation and recursive nonterminal expansion; and
(ii) strictly follow operator arity, so that each operator receives the correct number of operands.
These are captured by the following generation rule:

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ–¤ğ—‘ğ—‰ğ—‹â†’ğ–®ğ—‰â€‹(ğ–¤ğ—‘ğ—‰ğ—‹,â€¦)âˆ£ğ–³ğ–¾ğ—‹ğ—†ğ–²ğ—’ğ–»,\mathsf{Expr}\;\to\;\mathsf{Op}(\mathsf{Expr},\dots)\;\mid\;\mathsf{TermSyb}, |  | (3) |

where ğ–¤ğ—‘ğ—‰ğ—‹âˆˆğ’©\mathsf{Expr}\in\mathcal{N} denotes a recursively expandable nonterminal symbol,
ğ–®ğ—‰âˆˆğ’¯\mathsf{Op}\in\mathcal{T} denotes prefix-notation operators,
and ğ–³ğ–¾ğ—‹ğ—†ğ–²ğ—’ğ–»âˆˆğ’¯\mathsf{TermSyb}\in\mathcal{T} denotes terminal symbols which are features and constants.

#### Structural Well-Formedness.

FormulaÂ ([3](https://arxiv.org/html/2601.22119v1#S3.E3 "Equation 3 â€£ 3.1 Syntactically-Valid Alpha Language â€£ 3 Design Language of Interpretable Alphas â€£ Alpha Discovery via Grammar-Guided Learning and Search")) enforces a prefix-notation structure in which each operator ğ–®ğ—‰\mathsf{Op} precedes its operands, eliminating ambiguity in operator precedence and evaluation order.
Recursive expansion of ğ–¤ğ—‘ğ—‰ğ—‹\mathsf{Expr} enables the construction of complex expressions, while termination is ensured by substituting terminal symbols.
Therefore, each valid derivation admits a unique hierarchical representation which we call *Abstract Syntax Representation (ASR)*. 222In formal language (Hopcroft and Ullman, [1979](https://arxiv.org/html/2601.22119v1#bib.bib22 "Automata theory, languages, and computation")), an expression corresponds to an abstract syntax tree (AST); we use the term ASR to distinguish it from the large search tree introduced later.

###### Definition 1.

An Abstract Syntax Representation (ASR) is a rooted, ordered tree encoding a single alpha expression, whose internal nodes are operators with arity-matched children and whose leaves are features, constants, or (in partial derivations) nonterminal symbols.

#### Operator Arity Constraints.

Syntactic validity also requires that all operators be applied with the correct number of operands.
We instantiate ğ–®ğ—‰\mathsf{Op} using operator families with fixed arity, reflecting common primitives in quantitative finance.
These include unary operators (ğ–´ğ—‡ğ–ºğ—‹ğ—’ğ–®ğ—‰\mathsf{UnaryOp}), binary operators (ğ–¡ğ—‚ğ—‡ğ–ºğ—‹ğ—’ğ–®ğ—‰\mathsf{BinaryOp}),
rolling operators (ğ–±ğ—ˆğ—…ğ—…ğ—‚ğ—‡ğ—€ğ–®ğ—‰\mathsf{RollingOp}),
paired rolling operators (ğ–¯ğ–ºğ—‚ğ—‹ğ–¾ğ–½ğ–±ğ—ˆğ—…ğ—…ğ—‚ğ—‡ğ—€ğ–®ğ—‰\mathsf{PairedRollingOp}),
and nullary terminal symbols (ğ–³ğ–¾ğ—‹ğ—†ğ–²ğ—’ğ–»\mathsf{TermSyb}) representing constants and raw features.
The resulting production rules are given by:

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ–¤ğ—‘ğ—‰ğ—‹â†’ğ–´ğ—‡ğ–ºğ—‹ğ—’ğ–®ğ—‰â€‹(ğ–¤ğ—‘ğ—‰ğ—‹)âˆ£ğ–¡ğ—‚ğ—‡ğ–ºğ—‹ğ—’ğ–®ğ—‰â€‹(ğ–¤ğ—‘ğ—‰ğ—‹,ğ–¤ğ—‘ğ—‰ğ—‹)âˆ£ğ–±ğ—ˆğ—…ğ—…ğ—‚ğ—‡ğ—€ğ–®ğ—‰(ğ–¤ğ—‘ğ—‰ğ—‹,ğ–¤ğ—‘ğ—‰ğ—‹)âˆ£ğ–¯ğ–ºğ—‚ğ—‹ğ–¾ğ–½ğ–±ğ—ˆğ—…ğ—…ğ—‚ğ—‡ğ—€ğ–®ğ—‰(ğ–¤ğ—‘ğ—‰ğ—‹,ğ–¤ğ—‘ğ—‰ğ—‹,ğ–¤ğ—‘ğ—‰ğ—‹)âˆ£ğ–³ğ–¾ğ—‹ğ—†ğ–²ğ—’ğ–».\begin{aligned} \mathsf{Expr}\;\to\;&\mathsf{UnaryOp}(\mathsf{Expr})\;\mid\;\mathsf{BinaryOp}(\mathsf{Expr},\mathsf{Expr})\\ &\mid\;\mathsf{RollingOp}(\mathsf{Expr},\mathsf{Expr})\\ &\mid\;\mathsf{PairedRollingOp}(\mathsf{Expr},\mathsf{Expr},\mathsf{Expr})\\ &\mid\;\mathsf{TermSyb}.\end{aligned} |  | (4) |

All feature symbols and constants are listed in [TableÂ 4](https://arxiv.org/html/2601.22119v1#A1.T4 "In Appendix A Tables â€£ Alpha Discovery via Grammar-Guided Learning and Search") and [TableÂ 5](https://arxiv.org/html/2601.22119v1#A1.T5 "In Appendix A Tables â€£ Alpha Discovery via Grammar-Guided Learning and Search"), respectively,
while [TableÂ 6](https://arxiv.org/html/2601.22119v1#A1.T6 "In Appendix A Tables â€£ Alpha Discovery via Grammar-Guided Learning and Search") lists all operators together with their arity categories.
These rules fully specify the admissible syntactic forms of alpha expressions.

We now formally define the context-free grammar that characterizes syntactically valid alpha expressions.

###### Definition 2 (Î±\alpha-Syn).

The context-free grammar for a syntactically valid alpha language Î±\alpha-Syn is defined as G=(ğ’©,ğ’¯,ğ’«,ğ’®)G=(\mathcal{N},\mathcal{T},\mathcal{P},\mathcal{S}) where
ğ’©\mathcal{N} is the recursively expandable nonterminal symbols,
ğ’¯\mathcal{T} is the terminal symbols including stock features ([TableÂ 4](https://arxiv.org/html/2601.22119v1#A1.T4 "In Appendix A Tables â€£ Alpha Discovery via Grammar-Guided Learning and Search")),
numerical constants ([TableÂ 5](https://arxiv.org/html/2601.22119v1#A1.T5 "In Appendix A Tables â€£ Alpha Discovery via Grammar-Guided Learning and Search")), and operators with fixed arity ([TableÂ 6](https://arxiv.org/html/2601.22119v1#A1.T6 "In Appendix A Tables â€£ Alpha Discovery via Grammar-Guided Learning and Search")),
ğ’«\mathcal{P} is the production rules given in Formula [4](https://arxiv.org/html/2601.22119v1#S3.E4 "Equation 4 â€£ Operator Arity Constraints. â€£ 3.1 Syntactically-Valid Alpha Language â€£ 3 Design Language of Interpretable Alphas â€£ Alpha Discovery via Grammar-Guided Learning and Search"),
which enforce prefix-notation and strict operator-arity consistency, and ğ’®\mathcal{S} is the start symbol.

[DefinitionÂ 2](https://arxiv.org/html/2601.22119v1#Thmdefinition2 "Definition 2 (ğ›¼-Syn). â€£ Operator Arity Constraints. â€£ 3.1 Syntactically-Valid Alpha Language â€£ 3 Design Language of Interpretable Alphas â€£ Alpha Discovery via Grammar-Guided Learning and Search") generates the language â„’syn\mathcal{L}\_{\mathrm{syn}} of syntactically valid alpha expressions, as illustrated in [FigureÂ 1](https://arxiv.org/html/2601.22119v1#S3.F1 "In 3 Design Language of Interpretable Alphas â€£ Alpha Discovery via Grammar-Guided Learning and Search").

### 3.2 Semantically-Interpretable Alpha Language

While Î±\alpha-Syn guarantees syntactic validity, it does not ensure semantic soundness in quantitative trading, as syntax alone cannot capture domain-specific financial constraints such as temporal coherence, numerical admissibility, or economically meaningful operator interactions.
Now we refine Î±\alpha-Syn in [DefinitionÂ 2](https://arxiv.org/html/2601.22119v1#Thmdefinition2 "Definition 2 (ğ›¼-Syn). â€£ Operator Arity Constraints. â€£ 3.1 Syntactically-Valid Alpha Language â€£ 3 Design Language of Interpretable Alphas â€£ Alpha Discovery via Grammar-Guided Learning and Search") by embedding domain-informed semantic constraints directly into the grammar, thereby defining a semantically interpretable alpha language.

#### Semantic Constraints.

We enforce a set of minimal and widely accepted financial semantic constraints:
(i) Rolling window constraint: the window-size operand of ğ–±ğ—ˆğ—…ğ—…ğ—‚ğ—‡ğ—€ğ–®ğ—‰\mathsf{RollingOp} and ğ–¯ğ–ºğ—‚ğ—‹ğ–¾ğ–½ğ–±ğ—ˆğ—…ğ—…ğ—‚ğ—‡ğ—€ğ–®ğ—‰\mathsf{PairedRollingOp} is integer constant;
(ii) Non-triviality: expressions cannot consist solely of constants and operators;
(iii) Numerical validity: operands must lie within domains consistent with their operators;
(iv) Time-series consistency: ğ–¯ğ–ºğ—‚ğ—‹ğ–¾ğ–½ğ–±ğ—ˆğ—…ğ—…ğ—‚ğ—‡ğ—€ğ–®ğ—‰\mathsf{PairedRollingOp} must operate on two time-varying features; constant operands are disallowed.

###### Definition 3 (Î±\alpha-Sem).

A semantic refinement of Î±\alpha-Syn is a context-free grammar
G=(ğ’©â€²,ğ’¯â€²,ğ’«â€²,ğ’®)G=(\mathcal{N}^{\prime},\mathcal{T}^{\prime},\mathcal{P}^{\prime},\mathcal{S}) that shares the same start symbol ğ’®\mathcal{S} as Î±\alpha-Syn. The nonterminal symbols ğ’¯â€²\mathcal{T}^{\prime} add Num and Constant.
The Terminal symbols ğ’¯\mathcal{T} is refined to containing features in [TableÂ 4](https://arxiv.org/html/2601.22119v1#A1.T4 "In Appendix A Tables â€£ Alpha Discovery via Grammar-Guided Learning and Search"), constant in [TableÂ 5](https://arxiv.org/html/2601.22119v1#A1.T5 "In Appendix A Tables â€£ Alpha Discovery via Grammar-Guided Learning and Search") and operators in [TableÂ 6](https://arxiv.org/html/2601.22119v1#A1.T6 "In Appendix A Tables â€£ Alpha Discovery via Grammar-Guided Learning and Search").
The production rules ğ’«â€²\mathcal{P}^{\prime} distinguishes the type of operands by:

|  |  |  |
| --- | --- | --- |
|  | ğ–¤ğ—‘ğ—‰ğ—‹â†’ğ–¥ğ–¾ğ–ºğ—ğ—ğ—‹ğ–¾âˆ£ğ–´ğ—‡ğ–ºğ—‹ğ—’ğ–®ğ—‰â€‹(ğ–¤ğ—‘ğ—‰ğ—‹)âˆ£ğ–¡ğ—‚ğ—‡ğ–ºğ—‹ğ—’ğ–®ğ—‰â€‹(ğ–¤ğ—‘ğ—‰ğ—‹,ğ–¤ğ—‘ğ—‰ğ—‹)âˆ£â€‹ğ–¡ğ—‚ğ—‡ğ–ºğ—‹ğ—’ğ–®ğ—‰â€‹(ğ–¤ğ—‘ğ—‰ğ—‹,ğ–¢ğ—ˆğ—‡ğ—Œğ—ğ–ºğ—‡ğ—)âˆ£ğ–¡ğ—‚ğ—‡ğ–ºğ—‹ğ—’ğ–®ğ—‰\_ğ– ğ—Œğ—’ğ—†(ğ–¢ğ—ˆğ—‡ğ—Œğ—ğ–ºğ—‡ğ—,ğ–¤ğ—‘ğ—‰ğ—‹)âˆ£ğ–±ğ—ˆğ—…ğ—…ğ—‚ğ—‡ğ—€ğ–®ğ—‰(ğ–¤ğ—‘ğ—‰ğ—‹,ğ–­ğ—ğ—†)âˆ£ğ–¯ğ–ºğ—‚ğ—‹ğ–¾ğ–½ğ–±ğ—ˆğ—…ğ—…ğ—‚ğ—‡ğ—€ğ–®ğ—‰(ğ–¤ğ—‘ğ—‰ğ—‹,ğ–¤ğ—‘ğ—‰ğ—‹,ğ–­ğ—ğ—†),ğ–­ğ—ğ—†â†’â€„20âˆ£â€¦,ğ–¢ğ—ˆğ—‡ğ—Œğ—ğ–ºğ—‡ğ—â†’âˆ’0.01âˆ£â€¦\begin{aligned} \mathsf{Expr}\;\to\;&\;\mathsf{Feature}\;\mid\;\mathsf{UnaryOp}(\mathsf{Expr})\\ &\mid\;\mathsf{BinaryOp}(\mathsf{Expr},\mathsf{Expr})\mid\mathsf{BinaryOp}(\mathsf{Expr},\mathsf{Constant})\\ &\mid\;\mathsf{BinaryOp\\_Asym}(\mathsf{Constant},\mathsf{Expr})\\ &\mid\;\mathsf{RollingOp}(\mathsf{Expr},\mathsf{Num})\\ &\mid\;\mathsf{PairedRollingOp}(\mathsf{Expr},\mathsf{Expr},\mathsf{Num}),\\ \mathsf{Num}\;\to\;&\;20\mid\dots,\qquad\mathsf{Constant}\;\to\;-0.01\mid\dots\end{aligned} |  |

The terminal symbols and operators of Î±\alpha-Sem can be revised or extended beyond [TableÂ 4](https://arxiv.org/html/2601.22119v1#A1.T4 "In Appendix A Tables â€£ Alpha Discovery via Grammar-Guided Learning and Search"), [TableÂ 5](https://arxiv.org/html/2601.22119v1#A1.T5 "In Appendix A Tables â€£ Alpha Discovery via Grammar-Guided Learning and Search"), and [TableÂ 6](https://arxiv.org/html/2601.22119v1#A1.T6 "In Appendix A Tables â€£ Alpha Discovery via Grammar-Guided Learning and Search") to support other domains or tasks in quantitative finance.

#### Length Bounded Grammar Î±\alpha-Sem-kk.

Although Î±\alpha-Sem enforces both syntactic and semantic validity, its recursive production rules can still generate expressions of unbounded depth, leading to an intractable search space.
We apply *kk-bounded constraint*, which assign each alpha expression with a length counter kk capped at KK. Each production rule incurs an incremental cost Î”â€‹k\Delta k ([TableÂ 7](https://arxiv.org/html/2601.22119v1#A1.T7 "In Appendix A Tables â€£ Alpha Discovery via Grammar-Guided Learning and Search")), and a rule may be applied only if k+Î”â€‹kâ‰¤Kk+\Delta k\leq K.
This constraint yields a bounded semantic grammar Î±\alpha-Sem-kk ([AlgorithmÂ 2](https://arxiv.org/html/2601.22119v1#alg2 "In B.2 Length control of semantic interpretable alpha factor generator â€£ Appendix B Algorithms â€£ Alpha Discovery via Grammar-Guided Learning and Search")).

### 3.3 Alpha Space Structure

Each grammar above (Î±\alpha-Syn, Î±\alpha-Sem, and Î±\alpha-Sem-kk) generates a space of many alpha expressions, corresponding to a *formal alpha language*, denoted by
â„’syn\mathcal{L}\_{\mathrm{syn}}, â„’sem\mathcal{L}\_{\mathrm{sem}}, and â„’semâ‰¤K\mathcal{L}\_{\mathrm{sem}}^{\leq K}, respectively.
These languages are naturally nested, as illustrated in [FigureÂ 1](https://arxiv.org/html/2601.22119v1#S3.F1 "In 3 Design Language of Interpretable Alphas â€£ Alpha Discovery via Grammar-Guided Learning and Search"), with each successive layer imposing additional constraints and yielding a progressively smaller and more structured hypothesis space for alpha discovery.

Among them, the kk-bounded semantic grammar Î±\alpha-Sem-kk plays a central role in this work.
Bounding the derivation depth while enforcing both syntactic and semantic validity yields a finite yet expressive language â„’semâ‰¤K\mathcal{L}\_{\mathrm{sem}}^{\leq K}, enabling systematic search.
A detailed analysis of the space complexity of â„’semâ‰¤K\mathcal{L}\_{\mathrm{sem}}^{\leq K} is provided in AppendixÂ [E](https://arxiv.org/html/2601.22119v1#A5 "Appendix E Search Space Complexity â€£ Alpha Discovery via Grammar-Guided Learning and Search").

###### Definition 4 (Search Space Structure).

Given a grammar Î±\alpha-Syn, Î±\alpha-Sem, or Î±\alpha-Sem-kk, the corresponding alpha language can be represented as a rooted tree.
The root node corresponds to the start symbol, each edge corresponds to the application of a production rule,
intermediate nodes represent partially derived expressions, and leaf nodes correspond to fully derived alpha factors.

This formulation of [DefinitionÂ 4](https://arxiv.org/html/2601.22119v1#Thmdefinition4 "Definition 4 (Search Space Structure). â€£ 3.3 Alpha Space Structure â€£ 3 Design Language of Interpretable Alphas â€£ Alpha Discovery via Grammar-Guided Learning and Search") fundamentally changes the nature of alpha discovery.
Rather than exhaustive searching over the unstructured and infinite symbol space Î£âˆ—\Sigma^{\*}, alpha discovery can now be viewed as exploring a tree-structured language space
â„’semâ‰¤K\mathcal{L}\_{\mathrm{sem}}^{\leq K} induced by Î±\alpha-Sem-kk.
In this view, our alpha discovery reduces to identifying high-quality leaf nodes within a large but well-organized derivation tree.

![Refer to caption](Search-Tree-0.png)


Figure 2: The tree-structured search space.

[FigureÂ 2](https://arxiv.org/html/2601.22119v1#S3.F2 "In 3.3 Alpha Space Structure â€£ 3 Design Language of Interpretable Alphas â€£ Alpha Discovery via Grammar-Guided Learning and Search") illustrates the structure of the space
of all alpha expressions under Î±\alpha-Sem-kk. In this tree, each rounded-box node corresponds to an alpha expression, which is equivalent to an Abstract Syntax Representation (ASR) shown in the middle of the figure. Within each ASR, grey nodes denote nonterminal symbols, colored nodes denote terminal symbols, and edges represent grammar-driven expansion steps. This tree-structured perspective naturally supports tree-based search and learning algorithms.

## 4 Reinforced Alpha Language Tree Search

In the previous section, Î±\alpha-Sem-kk induces a large yet well-structured tree of candidate alpha factors, where each leaf corresponds to a complete, evaluable expression.
Unlike conventional tree search problems, this space combines (i) explosive early branching, (ii) sharp contraction near a depth bound, and (iii) grammar-driven and formula-dependent actions, resulting in highly non-uniform search dynamics.
Moreover, the predictive performance of an alpha is revealed only at terminal nodes, which yields long-horizon dependencies and sparse rewards.

These challenges make unguided search ineffective and motivate a language-principled decision-making formulation.
Accordingly, we cast alpha discovery as a Tree-Structured Linguistic Markov Decision Process (TSL-MDP) and develop a reinforcement learningâ€“guided, grammar-aware MCTS framework, supported by syntax-aware representation learning for efficient policy and value estimation.

### 4.1 Decision-Making on Large Tree

With [DefinitionÂ 4](https://arxiv.org/html/2601.22119v1#Thmdefinition4 "Definition 4 (Search Space Structure). â€£ 3.3 Alpha Space Structure â€£ 3 Design Language of Interpretable Alphas â€£ Alpha Discovery via Grammar-Guided Learning and Search"), alpha discovery can be viewed as a sequential decision process over a large derivation tree. Equivalently, the task reduces to: (i) selecting a high-quality root-to-leaf path that yields a strong alpha, or (ii) expanding an intermediate node (e.g., a partially specified or masked factor) into a more predictive expression.

In this search tree, each complete alpha factor (leaf node) is evaluated by the average IC in [EquationÂ 2](https://arxiv.org/html/2601.22119v1#S2.E2 "In Evaluation via Information Coefficient. â€£ 2 Problem Formulation â€£ Alpha Discovery via Grammar-Guided Learning and Search"), computed from historical market data ([FigureÂ 2](https://arxiv.org/html/2601.22119v1#S3.F2 "In 3.3 Alpha Space Structure â€£ 3 Design Language of Interpretable Alphas â€£ Alpha Discovery via Grammar-Guided Learning and Search"), [AlgorithmÂ 1](https://arxiv.org/html/2601.22119v1#alg1 "In B.1 Linear combination alpha factor algorithm â€£ Appendix B Algorithms â€£ Alpha Discovery via Grammar-Guided Learning and Search")).
This IC serves as the reward signal and can be propagated backward along the derivation path, assigning value estimates to intermediate nodes.
Consequently, partial expressions naturally correspond to states, grammar production rules to actions, and derivation steps to state transitions.

This perspective leads to a principled formulation of grammar-guided alpha discovery as a Markov Decision Process, which we term the *Tree-Structured Linguistic Markov Decision Process (TSL-MDP)*.

###### Definition 5 (TSL-MDP).

Alpha discovery under Î±\alpha-Sem-kk is a Tree-Structured Linguistic Markov Decision Process
TSL-MDP=âŸ¨S,A,P,R,Î³âŸ©\text{TSL-MDP}=\langle S,A,P,R,\gamma\rangle, where
SS is the set of partial or complete alpha expressions;
AA is the set of grammar production rules in [DefinitionÂ 3](https://arxiv.org/html/2601.22119v1#Thmdefinition3 "Definition 3 (ğ›¼-Sem). â€£ Semantic Constraints. â€£ 3.2 Semantically-Interpretable Alpha Language â€£ 3 Design Language of Interpretable Alphas â€£ Alpha Discovery via Grammar-Guided Learning and Search");
Pâ€‹(sâ€²âˆ£s,a)P(s^{\prime}\mid s,a) deterministically applies rule aa to expand the leftmost nonterminal in ss, yielding a longer alpha expression sâ€²s^{\prime};
and reward Râ€‹(s,a)R(s,a) is nonzero only when sâ€²s^{\prime} is a complete alpha expression, equal to its IC evaluated on market data.

![Refer to caption](new-rl2.png)


Figure 3: Grammar-aware reinforcement learning and MCTS, based on alpha representation and value and policy networks.

### 4.2 Reinforcement Learning Guided MCTS

While the tree structure of TSL-MDP makes it amenable to search, classical MCTS becomes ineffective at this scale due to long-horizon dependencies, highly irregular branching, and the absence of intermediate rewards.
We embed MCTS into a reinforcement learning framework that is explicitly tailored to grammar-based alpha generation.

Specifically, two neural networks are introduced: a policy network that predicts promising grammar production rules conditioned on a partial expression, and a value network that estimates the expected predictive quality of an incomplete alpha.
Both networks are driven by a Tree-LSTM encoder (Tai et al., [2015](https://arxiv.org/html/2601.22119v1#bib.bib7 "Improved semantic representations from tree-structured long short-term memory networks")) that consumes the Abstract Syntax Representation ([DefinitionÂ 1](https://arxiv.org/html/2601.22119v1#Thmdefinition1 "Definition 1. â€£ Structural Well-Formedness. â€£ 3.1 Syntactically-Valid Alpha Language â€£ 3 Design Language of Interpretable Alphas â€£ Alpha Discovery via Grammar-Guided Learning and Search")) of the current alpha expression, enabling structure-aware generalization across the vast TSL-MDP state space. Our framework is illustrated in [FigureÂ 7](https://arxiv.org/html/2601.22119v1#A4.F7 "In Appendix D Reinforcement Learning Framework â€£ Alpha Discovery via Grammar-Guided Learning and Search").

#### Overall Interaction Between RL and MCTS.

Starting from the start symbol of Î±\alpha-Sem, alpha generation proceeds iteratively.
At each iteration jj, we perform II rounds of grammar-aware MCTS guided by the current policy and value networks.
The resulting search statistics induce a distribution over different production rules at the root, from which an action is sampled to expand to a node in the next layer of the search tree, which increases the current alpha expression.
The expanded node in this new layer then becomes the new root, and the process repeats until a complete alpha expression is generated.
Each completed alpha yields an IC reward, forming a trajectory of grammar decisions.
By collecting such trajectories, we iteratively update the policy and value networks via reinforcement learning, resulting in an effective *searchâ€“learnâ€“search* loop.
An overview of this interaction is illustrated in [FigureÂ 7](https://arxiv.org/html/2601.22119v1#A4.F7 "In Appendix D Reinforcement Learning Framework â€£ Alpha Discovery via Grammar-Guided Learning and Search"), with the corresponding pseudocode provided in [AlgorithmÂ 4](https://arxiv.org/html/2601.22119v1#alg4 "In Appendix D Reinforcement Learning Framework â€£ Alpha Discovery via Grammar-Guided Learning and Search").

#### MCTS Components.

At a given root state jj, the MCTS agent incrementally explores a subtree of the TSL-MDP through repeated simulations. Then it executes the following components (see [FigureÂ 3](https://arxiv.org/html/2601.22119v1#S4.F3 "In 4.1 Decision-Making on Large Tree â€£ 4 Reinforced Alpha Language Tree Search â€£ Alpha Discovery via Grammar-Guided Learning and Search") (a) and Appendix [B.3](https://arxiv.org/html/2601.22119v1#A2.SS3 "B.3 Algorithm of Four Stages of MCTS â€£ Appendix B Algorithms â€£ Alpha Discovery via Grammar-Guided Learning and Search") for details).

Selection. From the root, the MCTS agent repeatedly applies an Î±\alpha-CFG production rule to the leftmost nonterminal symbol until reaching a frontier node which has not yet been expanded.
The TSL-MDP exhibits highly irregular branching, with depth-dependent numbers of applicable production rules.
We therefore introduce an adaptive branching factor in the PUCT formulation, where bb denotes the number of valid actions at the current state and brefb\_{\text{ref}} is a normalization constant given by the maximum branching factor.
The ratio bbref\sqrt{\frac{b}{b\_{\text{ref}}}} modulates the exploration term, emphasizing exploitation for small branching factors and promoting broader exploration for larger ones. Accordingly, we use the adapted PUCT-style selection ruleÂ (Silver et al., [2017](https://arxiv.org/html/2601.22119v1#bib.bib5 "Mastering the game of go without human knowledge")):

|  |  |  |
| --- | --- | --- |
|  | aâˆ—=argâ¡maxaâ¡(Qâ€‹(s,a)+cpuctâ€‹bbrefâ€‹Pâ€‹(s,a)â€‹âˆ‘bNâ€‹(s,b)1+Nâ€‹(s,a))a^{\*}=\arg\max\_{a}\left(Q(s,a)+c\_{\text{puct}}\sqrt{\tfrac{b}{b\_{\text{ref}}}}\,P(s,a)\tfrac{\sqrt{\sum\_{b}N(s,b)}}{1+N(s,a)}\right) |  |

Expansion and Evaluation.
Upon reaching a frontier node, all valid Î±\alpha-CFG production rules are applied to generate its child states. The resulting node is evaluated using a Tree-LSTMâ€“based value network Vâ€‹(s)V(s), which estimates the expected terminal reward of the corresponding partial expression. Meanwhile, a policy network produces a distribution Pâ€‹(s,a)P(s,a) over valid production rules, providing prior guidance for future selections.

Backpropagation. The evaluation result Vâ€‹(s)V(s) is backpropagated along the selection path, updating Qâ€‹(s,a)Q(s,a) and visit counts Nâ€‹(s,a)N(s,a). Iterating these steps allow MCTS agent progressively expands its explored subtree and refines the search statistics over the TSL-MDP ([AlgorithmÂ 3](https://arxiv.org/html/2601.22119v1#alg3 "In B.3 Algorithm of Four Stages of MCTS â€£ Appendix B Algorithms â€£ Alpha Discovery via Grammar-Guided Learning and Search")).

Table 2: Evaluation metrics comparison of different methods (5 random seeds).

| CSI300 | | | | | | |
| --- | --- | --- | --- | --- | --- | --- |
| Method | Rank IC | IC | Rank ICIR | ICIR | Sharpe | Max Drawdown |
| XGBoost | 0.0288 (0.0000) | 0.0326 (0.0000) | 0.2895 (0.0000) | 0.2818 (0.0000) | 0.2853 (0.0000) | -0.2777 (0.0000) |
| LightGBM | 0.0539 (0.0029) | 0.0296 (0.0014) | 0.3963 (0.0247) | 0.2649 (0.0395) | 0.2680 (0.0666) | -0.3271 (0.0177) |
| LSTM | 0.0128 (0.0260) | 0.0127 (0.0136) | 0.0896 (0.2064) | 0.1041 (0.1060) | 0.1268 (0.0425) | -0.3542 (0.0240) |
| TCN | 0.0303 (0.0236) | 0.0085 (0.0133) | 0.2726 (0.1855) | 0.0871 (0.1557) | 0.0908 (0.0754) | -0.2988 (0.0191) |
| ALSTM | 0.0138 (0.0076) | 0.0105 (0.0067) | 0.1194 (0.0540) | 0.0950 (0.0550) | 0.1372 (0.1113) | -0.3475 (0.0501) |
| Transformer | 0.0423 (0.0133) | 0.0248 (0.0132) | 0.3759 (0.0697) | 0.2457 (0.0971) | 0.1699 (0.1105) | -0.3365 (0.0377) |
| gplearn | 0.0706 (0.0119) | 0.0440 (0.0139) | 0.4695 (0.1164) | 0.3478 (0.1397) | 0.2062 (0.2346) | -0.3854 (0.0324) |
| AlphaQCM | 0.0811 (0.0046) | 0.0525 (0.0048) | 0.5334 (0.0296) | 0.3874 (0.0121) | 0.4363 (0.0610) | -0.3605 (0.0339) |
| RPN+PPO(AlphaGen) | 0.0837 (0.0070) | 0.0477 (0.0086) | 0.5724 (0.0343) | 0.3531 (0.0574) | 0.4978 (0.1478) | -0.3497 (0.0423) |
| Ablation Studies | | | | | | |
| RPN+MCTS | 0.0710 (0.0031) | 0.0500 (0.0026) | 0.5577 (0.0292) | 0.4285 (0.0293) | 0.5639 (0.1050) | -0.3201 (0.0613) |
| Î±\alpha-Syn+MCTS | 0.0745 (0.0052) | 0.0487 (0.0036) | 0.5125 (0.0467) | 0.3974 (0.0367) | 0.4852 (0.1320) | -0.3475 (0.0414) |
| Î±\alpha-Sem+MCTS | 0.0770 (0.0044) | 0.0512 (0.0015) | 0.5593 (0.0340) | 0.4369 (0.0301) | 0.5801 (0.1169) | -0.3039 (0.0206) |
| Î±\alpha-Sem-kk+MCTS(AlphaCFG) | 0.0865 (0.0060) | 0.0577 (0.0029) | 0.6036 (0.0537) | 0.4505 (0.0249) | 0.6459 (0.0612) | -0.2963 (0.0289) |
| S&P500 | | | | | | |
| Method | Rank IC | IC | Rank ICIR | ICIR | Sharpe | Max Drawdown |
| XGBoost | 0.0140 (0.0000) | 0.0104 (0.0000) | 0.1535 (0.0000) | 0.1456 (0.0000) | 0.5883 (0.0000) | -0.2543 (0.0000) |
| LightGBM | 0.0078 (0.0021) | 0.0220 (0.0032) | 0.0860 (0.0269) | 0.2072 (0.0229) | 0.5852 (0.0547) | -0.2047 (0.0128) |
| LSTM | 0.0131 (0.0077) | 0.0219 (0.0040) | 0.1157 (0.0786) | 0.1847 (0.0419) | 0.5601 (0.0546) | -0.2345 (0.0142) |
| TCN | 0.0198 (0.0040) | 0.0166 (0.0020) | 0.1358 (0.0190) | 0.1340 (0.0133) | 0.4973 (0.0271) | -0.2396 (0.0175) |
| ALSTM | 0.0202 (0.0028) | 0.0268 (0.0039) | 0.1569 (0.0344) | 0.1993 (0.0391) | 0.4441 (0.0397) | -0.2418 (0.0109) |
| Transformer | 0.0106 (0.0049) | 0.0185 (0.0036) | 0.0828 (0.0433) | 0.1806 (0.0361) | 0.5979 (0.1163) | -0.2512 (0.0070) |
| gplearn | 0.0130 (0.0122) | 0.0322 (0.0110) | 0.0812 (0.0643) | 0.1877 (0.0437) | 0.8241 (0.1814) | -0.2456 (0.0434) |
| AlphaQCM | 0.0178 (0.0055) | 0.0384 (0.0056) | 0.1149 (0.0381) | 0.2527 (0.0336) | 1.0566 (0.0756) | -0.2105 (0.0273) |
| RPN+PPO(AlphaGen) | 0.0149 (0.0055) | 0.0342 (0.0050) | 0.1045 (0.0364) | 0.2420 (0.0296) | 0.8271 (0.1421) | -0.2559 (0.0242) |
| Ablation Studies | | | | | | |
| RPN+MCTS | 0.0309 (0.0054) | 0.0385 (0.0031) | 0.2447 (0.0234) | 0.3308 (0.0344) | 0.7992 (0.0854) | -0.1957 (0.0140) |
| Î±\alpha-Syn+MCTS | 0.0111 (0.0017) | 0.0272 (0.0047) | 0.0913 (0.0087) | 0.2335 (0.0356) | 0.8046 (0.0322) | -0.2286 (0.0186) |
| Î±\alpha-Sem+MCTS | 0.0265 (0.0011) | 0.0413 (0.0030) | 0.2075 (0.0108) | 0.3360 (0.0162) | 0.8315 (0.0855) | -0.2243 (0.0225) |
| Î±\alpha-Sem-kk+MCTS(AlphaCFG) | 0.0354 (0.0026) | 0.04573 (0.0034) | 0.2958(0.0154) | 0.4099 (0.0230) | 0.8473 (0.0483) | -0.1942 (0.0126) |

### 4.3 Syntax Representation Learning

Network Design.
The main challenge in TSL-MDP is its vast state space, which requires evaluating both partial and complete alpha expressions as well as policies for expanding them.
Since each state is naturally represented by an ASR ([DefinitionÂ 1](https://arxiv.org/html/2601.22119v1#Thmdefinition1 "Definition 1. â€£ Structural Well-Formedness. â€£ 3.1 Syntactically-Valid Alpha Language â€£ 3 Design Language of Interpretable Alphas â€£ Alpha Discovery via Grammar-Guided Learning and Search")), we employ syntax-aware representation learning that directly encodes structure and semantics, which avoids costly rollout-based evaluations in classical MCTS.
Moreover, due to the symmetry of some operators (e.g., commutative operands), there are a large number of isomorphic factor expressions (defined in [DefinitionÂ 6](https://arxiv.org/html/2601.22119v1#Thmdefinition6 "Definition 6 (Isomorphism of ASR(Tree)). â€£ Appendix G Calculation of Tree Similarity â€£ Alpha Discovery via Grammar-Guided Learning and Search")) in TSL-MDP. Syntax-aware representation learning is suitable for addressing these redundancies as it operates directly on ASRs rather than linear sequence.

Accordingly, we use a Tree-LSTM encoder (Tai et al., [2015](https://arxiv.org/html/2601.22119v1#bib.bib7 "Improved semantic representations from tree-structured long short-term memory networks")) with two heads: a policy head for predicting production-rule distributions and a value head for estimating terminal rewards (details are provided in AppendixÂ [F](https://arxiv.org/html/2601.22119v1#A6 "Appendix F Details of Tree-LSTM â€£ Alpha Discovery via Grammar-Guided Learning and Search")).
As shown in [FigureÂ 3](https://arxiv.org/html/2601.22119v1#S4.F3 "In 4.1 Decision-Making on Large Tree â€£ 4 Reinforced Alpha Language Tree Search â€£ Alpha Discovery via Grammar-Guided Learning and Search") (b), the Tree-LSTM recursively aggregates information, producing a fixed-dimensional state embedding for each ASR. This embedding is shared by both policy network for production-rule prediction and value network for state-value estimation in MCTS.

Train and Sampling Procedure. The policy and value networks are trained jointly using Tree-LSTM representations of TSL-MDP states.
Initially, both networks are randomly initialized and used to guide MCTS expansion and evaluation.
The resulting search statistics define an initial policy for alpha generation, which is then used to: (i) supervise the policy network via imitation of the MCTS-derived action distribution, and (ii) sample complete alpha expressions whose IC values (from market data) provide supervision for the value network.
In subsequent iterations, the updated networks guide new MCTS constructions, and the process repeats until enough alphas have been sampled.

Diversity-Aware Value Target.
Since the ultimate objective is to construct a composite factor ICâ„±\mathrm{IC}\_{\mathcal{F}} (See [AlgorithmÂ 1](https://arxiv.org/html/2601.22119v1#alg1 "In B.1 Linear combination alpha factor algorithm â€£ Appendix B Algorithms â€£ Alpha Discovery via Grammar-Guided Learning and Search")), generating expressions that are structurally similar to existing factors can reduce pool diversity and degrade overall performance. To mitigate this, we incorporate a diversity-aware adjustment into the value target.
Specifically, we define a normalized structural similarity measure simâ€‹(â‹…,â‹…)\mathrm{sim}(\cdot,\cdot), based on maximum common subtree matching (Sager et al., [2006](https://arxiv.org/html/2601.22119v1#bib.bib11 "Detecting similar java classes using tree algorithms")) between the newly generated ASR fjf\_{j} corresponding to state sjs\_{j} and any existing ftâˆˆâ„±f\_{t}\in\mathcal{F}. This similarity penalizes states whose grammatical structures overlap with â„±\mathcal{F}. The resulting value target is defined as

|  |  |  |  |
| --- | --- | --- | --- |
|  | zâ€‹(sj)=(1âˆ’maxâ¡(0,maxftâˆˆâ„±â¡simâ€‹(ft,fj)))â‹…ICâ„±.z(s\_{j})=\bigl(1-\max(0,\max\_{f\_{t}\in\mathcal{F}}\mathrm{sim}(f\_{t},f\_{j}))\bigr)\cdot\mathrm{IC}\_{\mathcal{F}}. |  | (5) |

More details about tree similarity can be seen in [AppendixÂ G](https://arxiv.org/html/2601.22119v1#A7 "Appendix G Calculation of Tree Similarity â€£ Alpha Discovery via Grammar-Guided Learning and Search").

## 5 Experiments

Detailed experimental settings, including datasets, comparison methods, evaluation metrics, and hyperparameters, are provided in the Appendix
([SectionÂ I.1](https://arxiv.org/html/2601.22119v1#A9.SS1 "I.1 Data â€£ Appendix I More Results of Experiment â€£ Alpha Discovery via Grammar-Guided Learning and Search"), [SectionÂ I.2](https://arxiv.org/html/2601.22119v1#A9.SS2 "I.2 Comparison Methods â€£ Appendix I More Results of Experiment â€£ Alpha Discovery via Grammar-Guided Learning and Search"), [SectionÂ I.3](https://arxiv.org/html/2601.22119v1#A9.SS3 "I.3 Evaluation Metrics â€£ Appendix I More Results of Experiment â€£ Alpha Discovery via Grammar-Guided Learning and Search"), and [AppendixÂ H](https://arxiv.org/html/2601.22119v1#A8 "Appendix H AlphaCFG Framework Parameter Setting for Experiment â€£ Alpha Discovery via Grammar-Guided Learning and Search")).
Analysis on network architectures and mined factor examples with interpretability discussions are presented in
[SectionÂ I.4](https://arxiv.org/html/2601.22119v1#A9.SS4 "I.4 Comparison of Different Network Architectures â€£ Appendix I More Results of Experiment â€£ Alpha Discovery via Grammar-Guided Learning and Search") and [SectionÂ I.6](https://arxiv.org/html/2601.22119v1#A9.SS6 "I.6 Case Study of the interpretability of formulaic factors â€£ Appendix I More Results of Experiment â€£ Alpha Discovery via Grammar-Guided Learning and Search"), respectively.

Comparison of Generation Spaces. We first compare different factor generation spaces ([FigureÂ 1](https://arxiv.org/html/2601.22119v1#S3.F1 "In 3 Design Language of Interpretable Alphas â€£ Alpha Discovery via Grammar-Guided Learning and Search")) to evaluate the impact of language constraints on factor discovery. Specifically, we compare three CFG levels with Reverse Polish Notation (RPN) (Krtolica and StanimiroviÄ‡, [2004](https://arxiv.org/html/2601.22119v1#bib.bib33 "Reverse polish notation method")), a computation and verification formalism with a non-recursive structure, on the CSIÂ 300 and S&PÂ 500 training datasets.
With a pool size of 10 and max length 5, [FigureÂ 4](https://arxiv.org/html/2601.22119v1#S5.F4 "In 5 Experiments â€£ Alpha Discovery via Grammar-Guided Learning and Search") shows the training IC across epochs.
Results confirm our analysis in [SectionÂ 3.3](https://arxiv.org/html/2601.22119v1#S3.SS3 "3.3 Alpha Space Structure â€£ 3 Design Language of Interpretable Alphas â€£ Alpha Discovery via Grammar-Guided Learning and Search"), where more constrained, grammar-defined spaces yield faster convergence and higher-quality factors.
Although RPN converges to a performance level close to Î±\alpha-Sem, its convergence is noticeably slower.
This behavior reflects the limited semantic and length constraints of RPN, whose non-recursive structure restricts its effectiveness for structured factor generation compared to Î±\alpha-Sem.

![Refer to caption](x1.png)


Figure 4: Comparison of training curves of generation methods.

Comparison with Existing Alpha Mining Methods. To create a fair comparison environment, we use the optimized hyperparameters from the validation dataset experiments (see details in Appendix [I.5](https://arxiv.org/html/2601.22119v1#A9.SS5 "I.5 Optimization of Combined Factor Parameters on the Validation Set â€£ Appendix I More Results of Experiment â€£ Alpha Discovery via Grammar-Guided Learning and Search")) for each method, including our MCTS-based methods (Î±\alpha-Syn, Î±\alpha-Sem, Î±\alpha-Sem-kk and RPN) against existing factor mining methods or prediction models (formulaic: Alphagen, AlphaQCM, GPlearn; ML-based: XGBoost, LightGBM, LSTM, ALSTM, TCN, Transformer). The experiments were conducted separately on the CSIÂ 300 index and the S&PÂ 500 constituents testing data, evaluating both correlation-based metrics and backtesting performance. The backtesting results are obtained using a single top-kk/drop-nn strategy to conduct simulated trading based on real stock data (detailed in Appendix [I.3](https://arxiv.org/html/2601.22119v1#A9.SS3 "I.3 Evaluation Metrics â€£ Appendix I More Results of Experiment â€£ Alpha Discovery via Grammar-Guided Learning and Search")).
Quantitative results are summarized in [TableÂ 2](https://arxiv.org/html/2601.22119v1#S4.T2 "In MCTS Components. â€£ 4.2 Reinforcement Learning Guided MCTS â€£ 4 Reinforced Alpha Language Tree Search â€£ Alpha Discovery via Grammar-Guided Learning and Search"), and cumulative return curves are shown in [FigureÂ 5](https://arxiv.org/html/2601.22119v1#S5.F5 "In 5 Experiments â€£ Alpha Discovery via Grammar-Guided Learning and Search").

![Refer to caption](x2.png)


(a) CSI 300

![Refer to caption](x3.png)


(b) S&P 500

Figure 5: Cumulative return comparison in simulated trading

Overall, our method achieves the best performance across all correlation metrics directly related to the optimization target IC.
Ablation studies further demonstrate the indispensable roles of syntactic constraints, semantic constraints, and length control.
While formulaic factor mining methods generally outperform machine-learning models that directly predict returns in correlation metrics, our approach also achieves strong backtesting performance.
Despite not directly optimizing for backtesting objectives, our method consistently attains superior Sharpe ratios and lower maximum drawdowns, and achieves the highest overall profitability among all compared methods.

Improving Traditional Alpha Factors.
Beyond directly mining composite factors, our Î±\alpha-Sem-kk+MCTS framework can be used to refine existing interpretable alpha factors.
We select a set of classic but recently ineffective factors from the GTJA 191 Factor Library and the Alpha101 Factor LibraryÂ (Kakushadze, [2016](https://arxiv.org/html/2601.22119v1#bib.bib12 "101 formulaic alphas")).
Factors from the GTJA 191 library are refined using the CSIÂ 300 dataset, while Alpha101 factors are refined using the S&PÂ 500 dataset.
By partially masking operators and operands while preserving the left-side structure within half of the original expression length, we optimize these factors using a single-factor reward objective (illustrated by the blue path in [FigureÂ 2](https://arxiv.org/html/2601.22119v1#S3.F2 "In 3.3 Alpha Space Structure â€£ 3 Design Language of Interpretable Alphas â€£ Alpha Discovery via Grammar-Guided Learning and Search")).
As shown in [TableÂ 3](https://arxiv.org/html/2601.22119v1#S5.T3 "In 5 Experiments â€£ Alpha Discovery via Grammar-Guided Learning and Search"), our framework consistently improves the absolute IC values of many classic factors on the test datasets, demonstrating its effectiveness in strengthening existing alpha signals.

Table 3: Refinement Results: Test Set IC Before and After Applying Î±\alpha-Sem-kk+MCTS framework.

|  |  |
| --- | --- |
| GTJA191 | |
| Original: open/Ref(close,1)-1 | 0.00185 |
| Improved: open/0.1-Cov(volume,high,20) | 0.04279 |
| Original: Mean(close,6)-close | 0.00482 |
| Improved: Mean(Cov(vwap,volume,20)/(-0.01),20)/0.05 | 0.04262 |
| Original: close-Ref(close,5) | 0.00495 |
| Improved: close-Greater(-0.1,Cov(volume,|vwap|,30)) | 0.03872 |
| Alpha101 | |
| Original: -Corr(open,volume,10) | 0.00271 |
| Improved: Corr(open,Log(|open|),40)Â·CSRank(high) | 0.02934 |
| Original: -Rank(CSRank(low),9) | 0.01031 |
| Improved: Rank(CSRank(CSRank(Sign(vwap))),30)Â·CSRank(high) | 0.02944 |
| Original: Pow(highÂ· low,0.5)-vwap) | 0.00112 |
| Improved: Pow(CSRank(|open|)Â·open,CSRank(close))-vwap | 0.03126 |

## 6 Conclusion

AlphaCFG formulates alpha factor discovery as a grammar-guided, syntax-treeâ€“structured search problem that enforces interpretability while enabling efficient integration of reinforcement learning with neural Monte Carlo Tree Search.
Beyond trading, the framework naturally extends to other factor-based quantitative finance tasks.
More broadly, AlphaCFG exemplifies grammar-guided symbolic regression, where domain knowledge is encoded directly in the search space rather than learned implicitly from data.
A promising direction for future work is to integrate AlphaCFG with large-scale learned priors, such as foundation models over programs or syntax trees, to further accelerate search and improve generalization in structured reasoning problems.

## References

* H. N. Bhandari, B. Rimal, N. R. Pokhrel, R. Rimal, K. R. Dahal, and R. K. Khatri (2022)
  Predicting stock market index using lstm.
  Machine Learning with Applications 9,  pp.Â 100320.
  Cited by: [Â§I.2](https://arxiv.org/html/2601.22119v1#A9.SS2.p3.1 "I.2 Comparison Methods â€£ Appendix I More Results of Experiment â€£ Alpha Discovery via Grammar-Guided Learning and Search"),
  [Â§1.1](https://arxiv.org/html/2601.22119v1#S1.SS1.p2.1 "1.1 Alpha discovery â€£ 1 Introduction â€£ Alpha Discovery via Grammar-Guided Learning and Search").
* K. Bisdoulis (2024)
  Assets forecasting with feature engineering and transformation methods for lightgbm.
  arXiv preprint arXiv:2501.07580.
  Cited by: [Â§I.2](https://arxiv.org/html/2601.22119v1#A9.SS2.p3.1 "I.2 Comparison Methods â€£ Appendix I More Results of Experiment â€£ Alpha Discovery via Grammar-Guided Learning and Search"),
  [Â§1.1](https://arxiv.org/html/2601.22119v1#S1.SS1.p2.1 "1.1 Alpha discovery â€£ 1 Introduction â€£ Alpha Discovery via Grammar-Guided Learning and Search").
* M. M. Carhart (1997)
  On persistence in mutual fund performance.
  The Journal of Finance 52 (1),  pp.Â 57â€“82.
  Cited by: [Â§1.1](https://arxiv.org/html/2601.22119v1#S1.SS1.p2.1 "1.1 Alpha discovery â€£ 1 Introduction â€£ Alpha Discovery via Grammar-Guided Learning and Search").
* N. Chomsky and M. P. SchÃ¼tzenberger (1959)
  The algebraic theory of context-free languages.
  In Studies in Logic and the Foundations of Mathematics,
  Vol. 26,  pp.Â 118â€“161.
  Cited by: [Â§3](https://arxiv.org/html/2601.22119v1#S3.p3.1 "3 Design Language of Interpretable Alphas â€£ Alpha Discovery via Grammar-Guided Learning and Search").
* W. Dai, Y. An, and W. Long (2022)
  Price change prediction of ultra high frequency financial data based on temporal convolutional network.
  Procedia Computer Science 199,  pp.Â 1177â€“1183.
  Cited by: [Â§I.2](https://arxiv.org/html/2601.22119v1#A9.SS2.p3.1 "I.2 Comparison Methods â€£ Appendix I More Results of Experiment â€£ Alpha Discovery via Grammar-Guided Learning and Search"),
  [Â§1.1](https://arxiv.org/html/2601.22119v1#S1.SS1.p2.1 "1.1 Alpha discovery â€£ 1 Introduction â€£ Alpha Discovery via Grammar-Guided Learning and Search").
* E. F. Fama and K. R. French (1992)
  The cross-section of expected stock returns.
  The Journal of Finance 47 (2),  pp.Â 427â€“465.
  Cited by: [Â§1.1](https://arxiv.org/html/2601.22119v1#S1.SS1.p2.1 "1.1 Alpha discovery â€£ 1 Introduction â€£ Alpha Discovery via Grammar-Guided Learning and Search").
* R. C. Grinold and R. N. Kahn (2000)
  Active portfolio management.
  Cited by: [Â§2](https://arxiv.org/html/2601.22119v1#S2.SS0.SSS0.Px1.p1.4 "Evaluation via Information Coefficient. â€£ 2 Problem Formulation â€£ Alpha Discovery via Grammar-Guided Learning and Search").
* J. E. Hopcroft and J. D. Ullman (1979)
  Automata theory, languages, and computation.
   Addison-Wesley.
  Cited by: [Â§3](https://arxiv.org/html/2601.22119v1#S3.p3.1 "3 Design Language of Interpretable Alphas â€£ Alpha Discovery via Grammar-Guided Learning and Search"),
  [footnote 2](https://arxiv.org/html/2601.22119v1#footnote2 "In Structural Well-Formedness. â€£ 3.1 Syntactically-Valid Alpha Language â€£ 3 Design Language of Interpretable Alphas â€£ Alpha Discovery via Grammar-Guided Learning and Search").
* L. Jin, F. Doshi-Velez, T. Miller, W. Schuler, and L. Schwartz (2018)
  Unsupervised grammar induction with depth-bounded pcfg.
  Transactions of the Association for Computational Linguistics 6,  pp.Â 211â€“224.
  Cited by: [Â§B.2](https://arxiv.org/html/2601.22119v1#A2.SS2.p1.7 "B.2 Length control of semantic interpretable alpha factor generator â€£ Appendix B Algorithms â€£ Alpha Discovery via Grammar-Guided Learning and Search").
* Z. Kakushadze (2016)
  101 formulaic alphas.
  Wilmott (84),  pp.Â 72â€“81.
  Cited by: [Â§5](https://arxiv.org/html/2601.22119v1#S5.p5.2 "5 Experiments â€£ Alpha Discovery via Grammar-Guided Learning and Search").
* P. V. Krtolica and P. S. StanimiroviÄ‡ (2004)
  Reverse polish notation method.
  International Journal of Computer Mathematics 81 (3),  pp.Â 273â€“284.
  Cited by: [Â§5](https://arxiv.org/html/2601.22119v1#S5.p2.2 "5 Experiments â€£ Alpha Discovery via Grammar-Guided Learning and Search").
* J. W. Lee (2001)
  Stock price prediction using reinforcement learning.
  In ISIE 2001. 2001 IEEE International Symposium on Industrial Electronics Proceedings (Cat. No. 01TH8570),
  Vol. 1,  pp.Â 690â€“695.
  Cited by: [Â§1.1](https://arxiv.org/html/2601.22119v1#S1.SS1.p2.1 "1.1 Alpha discovery â€£ 1 Introduction â€£ Alpha Discovery via Grammar-Guided Learning and Search").
* N. Makke and S. Chawla (2024)
  Interpretable scientific discovery with symbolic regression: a review.
  Artificial Intelligence Review 57 (1),  pp.Â 2.
  Cited by: [Â§1.1](https://arxiv.org/html/2601.22119v1#S1.SS1.p3.1 "1.1 Alpha discovery â€£ 1 Introduction â€£ Alpha Discovery via Grammar-Guided Learning and Search").
* L. Mozaffari and J. Zhang (2024)
  Predictive modeling of stock prices using transformer model.
  In Proceedings of the 2024 9th International Conference on Machine Learning Technologies,
   pp.Â 41â€“48.
  Cited by: [Â§I.2](https://arxiv.org/html/2601.22119v1#A9.SS2.p3.1 "I.2 Comparison Methods â€£ Appendix I More Results of Experiment â€£ Alpha Discovery via Grammar-Guided Learning and Search"),
  [Â§1.1](https://arxiv.org/html/2601.22119v1#S1.SS1.p2.1 "1.1 Alpha discovery â€£ 1 Introduction â€£ Alpha Discovery via Grammar-Guided Learning and Search").
* Y. Qin, D. Song, H. Chen, W. Cheng, G. Jiang, and G. Cottrell (2017)
  A dual-stage attention-based recurrent neural network for time series prediction.
  arXiv preprint arXiv:1704.02971.
  Cited by: [Â§I.2](https://arxiv.org/html/2601.22119v1#A9.SS2.p3.1 "I.2 Comparison Methods â€£ Appendix I More Results of Experiment â€£ Alpha Discovery via Grammar-Guided Learning and Search"),
  [Â§1.1](https://arxiv.org/html/2601.22119v1#S1.SS1.p2.1 "1.1 Alpha discovery â€£ 1 Introduction â€£ Alpha Discovery via Grammar-Guided Learning and Search").
* T. Sager, H. C. Gall, M. Pinzger, and A. Bernstein (2006)
  Detecting similar java classes using tree algorithms.
  In Proceedings of the 2006 ACM Symposium on Applied Computing,
   pp.Â 654â€“661.
  Cited by: [Â§4.3](https://arxiv.org/html/2601.22119v1#S4.SS3.p4.6 "4.3 Syntax Representation Learning â€£ 4 Reinforced Alpha Language Tree Search â€£ Alpha Discovery via Grammar-Guided Learning and Search").
* D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, et al. (2017)
  Mastering the game of go without human knowledge.
  nature 550 (7676),  pp.Â 354â€“359.
  Cited by: [Â§B.3](https://arxiv.org/html/2601.22119v1#A2.SS3.p3.10 "B.3 Algorithm of Four Stages of MCTS â€£ Appendix B Algorithms â€£ Alpha Discovery via Grammar-Guided Learning and Search"),
  [Â§4.2](https://arxiv.org/html/2601.22119v1#S4.SS2.SSS0.Px2.p2.4 "MCTS Components. â€£ 4.2 Reinforcement Learning Guided MCTS â€£ 4 Reinforced Alpha Language Tree Search â€£ Alpha Discovery via Grammar-Guided Learning and Search").
* K. S. Tai, R. Socher, and C. D. Manning (2015)
  Improved semantic representations from tree-structured long short-term memory networks.
  arXiv preprint arXiv:1503.00075.
  Cited by: [Â§4.2](https://arxiv.org/html/2601.22119v1#S4.SS2.p2.1 "4.2 Reinforcement Learning Guided MCTS â€£ 4 Reinforced Alpha Language Tree Search â€£ Alpha Discovery via Grammar-Guided Learning and Search"),
  [Â§4.3](https://arxiv.org/html/2601.22119v1#S4.SS3.p2.1 "4.3 Syntax Representation Learning â€£ 4 Reinforced Alpha Language Tree Search â€£ Alpha Discovery via Grammar-Guided Learning and Search").
* J. Wang, Q. Cheng, and Y. Dong (2023)
  An xgboost-based multivariate deep learning framework for stock index futures price forecasting.
  Kybernetes 52 (10),  pp.Â 4158â€“4177.
  Cited by: [Â§I.2](https://arxiv.org/html/2601.22119v1#A9.SS2.p3.1 "I.2 Comparison Methods â€£ Appendix I More Results of Experiment â€£ Alpha Discovery via Grammar-Guided Learning and Search"),
  [Â§1.1](https://arxiv.org/html/2601.22119v1#S1.SS1.p2.1 "1.1 Alpha discovery â€£ 1 Introduction â€£ Alpha Discovery via Grammar-Guided Learning and Search").
* Q. Xu (2025)
  Unsupervised temporal encoding for stock price prediction through dual-phase learning.
  In Proceedings of the 2025 International Conference on Economic Management and Big Data Application,
   pp.Â 778â€“784.
  Cited by: [Â§1.1](https://arxiv.org/html/2601.22119v1#S1.SS1.p2.1 "1.1 Alpha discovery â€£ 1 Introduction â€£ Alpha Discovery via Grammar-Guided Learning and Search").
* X. Yang, W. Liu, D. Zhou, J. Bian, and T. Liu (2020)
  Qlib: an ai-oriented quantitative investment platform.
  arXiv preprint arXiv:2009.11189.
  Cited by: [Â§I.2](https://arxiv.org/html/2601.22119v1#A9.SS2.p3.1 "I.2 Comparison Methods â€£ Appendix I More Results of Experiment â€£ Alpha Discovery via Grammar-Guided Learning and Search"),
  [Â§2](https://arxiv.org/html/2601.22119v1#S2.p3.1 "2 Problem Formulation â€£ Alpha Discovery via Grammar-Guided Learning and Search").
* S. Yu, H. Xue, X. Ao, F. Pan, J. He, D. Tu, and Q. He (2023)
  Generating synergistic formulaic alpha collections via reinforcement learning.
  In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,
   pp.Â 5476â€“5486.
  Cited by: [Â§I.2](https://arxiv.org/html/2601.22119v1#A9.SS2.p2.1 "I.2 Comparison Methods â€£ Appendix I More Results of Experiment â€£ Alpha Discovery via Grammar-Guided Learning and Search"),
  [Â§1.1](https://arxiv.org/html/2601.22119v1#S1.SS1.p3.1 "1.1 Alpha discovery â€£ 1 Introduction â€£ Alpha Discovery via Grammar-Guided Learning and Search"),
  [Â§2](https://arxiv.org/html/2601.22119v1#S2.SS0.SSS0.Px1.p3.1 "Evaluation via Information Coefficient. â€£ 2 Problem Formulation â€£ Alpha Discovery via Grammar-Guided Learning and Search").
* T. Zhang, Y. Li, Y. Jin, and J. Li (2020)
  Autoalpha: an efficient hierarchical evolutionary algorithm for mining alpha factors in quantitative investment.
  arXiv preprint arXiv:2002.08245.
  Cited by: [Â§I.2](https://arxiv.org/html/2601.22119v1#A9.SS2.p2.1 "I.2 Comparison Methods â€£ Appendix I More Results of Experiment â€£ Alpha Discovery via Grammar-Guided Learning and Search"),
  [Â§1.1](https://arxiv.org/html/2601.22119v1#S1.SS1.p2.1 "1.1 Alpha discovery â€£ 1 Introduction â€£ Alpha Discovery via Grammar-Guided Learning and Search"),
  [Â§1.1](https://arxiv.org/html/2601.22119v1#S1.SS1.p3.1 "1.1 Alpha discovery â€£ 1 Introduction â€£ Alpha Discovery via Grammar-Guided Learning and Search").
* Z. Zhu and K. Zhu (2025)
  AlphaQCM: alpha discovery in finance with distributional reinforcement learning.
  In Forty-second International Conference on Machine Learning,
  Cited by: [Â§I.2](https://arxiv.org/html/2601.22119v1#A9.SS2.p2.1 "I.2 Comparison Methods â€£ Appendix I More Results of Experiment â€£ Alpha Discovery via Grammar-Guided Learning and Search"),
  [Â§1.1](https://arxiv.org/html/2601.22119v1#S1.SS1.p3.1 "1.1 Alpha discovery â€£ 1 Introduction â€£ Alpha Discovery via Grammar-Guided Learning and Search").

## Appendix A Tables

Table 4: Stock Feature Variables

| Feature | Description |
| --- | --- |
| open | Opening price |
| high | Highest price |
| low | Lowest price |
| close | Closing price |
| volume | Trading volume |
| vwap | Volume Weighted Average Price (VWAP) |




Table 5: Constant Parameters

| Nonterminal | Values |
| --- | --- |
| Constant | âˆ’0.1,âˆ’0.05,âˆ’0.01,â€‰0.01,â€‰0.05,â€‰0.1-0.1,\,-0.05,\,-0.01,\,0.01,\,0.05,\,0.1 |
| Num | 20,â€‰30,â€‰4020,\,30,\,40 |




Table 6: Formulaic Alpha Factor Operators in Our Framework (the BinaryOp in FormulaÂ ([4](https://arxiv.org/html/2601.22119v1#S3.E4 "Equation 4 â€£ Operator Arity Constraints. â€£ 3.1 Syntactically-Valid Alpha Language â€£ 3 Design Language of Interpretable Alphas â€£ Alpha Discovery via Grammar-Guided Learning and Search")) does not distinguish whether it is symmetric)

| Operator | Type | Description |
| --- | --- | --- |
| Absâ€‹(x)\text{Abs}(x) | Unary | Absolute value, |x|\lvert x\rvert. |
| Signâ€‹(x)\text{Sign}(x) | Unary | Returns the sign of xx: 1 for positive, -1 for negative, 0 for zero. |
| Logâ€‹(x)\text{Log}(x) | Unary | Natural logarithm, logâ¡(x)\log(x). |
| Addâ€‹(x,y)\text{Add}(x,y) | Binary | Addition, x+yx+y. |
| Mulâ€‹(x,y)\text{Mul}(x,y) | Binary | Multiplication, xâ‹…yx\cdot y. |
| Greaterâ€‹(x,y)\text{Greater}(x,y) | Binary | Returns the larger of two values: maxâ¡(x,y)\max(x,y). |
| Lessâ€‹(x,y)\text{Less}(x,y) | Binary | Returns the smaller of two values: minâ¡(x,y)\min(x,y). |
| Divâ€‹(x,y)\text{Div}(x,y) | Binary-Asym | Division, x/yx/y. |
| Powâ€‹(x,y)\text{Pow}(x,y) | Binary-Asym | Exponentiation, xyx^{y}. |
| Subâ€‹(x,y)\text{Sub}(x,y) | Binary-Asym | Subtraction, xâˆ’yx-y. |
| CSRankâ€‹(x)\text{CSRank}(x) | Rolling | Cross-sectional ranking (normalizes the rank of xx across all stocks on the same day). |
| Rankâ€‹(x,t)\text{Rank}(x,t) | Rolling | Time-series ranking of xx over the past tt days. |
| WMAâ€‹(x,t)\text{WMA}(x,t) | Rolling | Weighted moving average with weights decaying over time. |
| EMAâ€‹(x,t)\text{EMA}(x,t) | Rolling | Exponential moving average with recursive smoothing. |
| Refâ€‹(x,t)\text{Ref}(x,t) | Rolling | Value of xx from tt days ago. |
| Meanâ€‹(x,t)\text{Mean}(x,t) | Rolling | Mean of xx over the past tt days, 1tâ€‹âˆ‘i=0tâˆ’1xâˆ’i\frac{1}{t}\sum\_{i=0}^{t-1}x\_{-i}. |
| Sumâ€‹(x,t)\text{Sum}(x,t) | Rolling | Sum of xx over the past tt days, âˆ‘i=0tâˆ’1xâˆ’i\sum\_{i=0}^{t-1}x\_{-i}. |
| Stdâ€‹(x,t)\text{Std}(x,t) | Rolling | Standard deviation of xx over the past tt days. |
| Varâ€‹(x,t)\text{Var}(x,t) | Rolling | Variance of xx over the past tt days. |
| Skewâ€‹(x,t)\text{Skew}(x,t) | Rolling | Skewness (measure of asymmetry) of xx over the past tt days. |
| Kurtâ€‹(x,t)\text{Kurt}(x,t) | Rolling | Kurtosis (measure of tail thickness) of xx over the past tt days. |
| Maxâ€‹(x,t)\text{Max}(x,t) | Rolling | Maximum value of xx over the past tt days. |
| Minâ€‹(x,t)\text{Min}(x,t) | Rolling | Minimum value of xx over the past tt days. |
| Medâ€‹(x,t)\text{Med}(x,t) | Rolling | Median of xx over the past tt days. |
| Madâ€‹(x,t)\text{Mad}(x,t) | Rolling | Mean absolute deviation, 1tâ€‹âˆ‘i=0tâˆ’1|xâˆ’iâˆ’xÂ¯|\frac{1}{t}\sum\_{i=0}^{t-1}\lvert x\_{-i}-\bar{x}\rvert. |
| Deltaâ€‹(x,t)\text{Delta}(x,t) | Rolling | Difference, xâˆ’Refâ€‹(x,t)x-\text{Ref}(x,t). |
| Covâ€‹(x,y,t)\text{Cov}(x,y,t) | PairedRolling | Covariance between xx and yy over the past tt days. |
| Corrâ€‹(x,y,t)\text{Corr}(x,y,t) | PairedRolling | Pearson correlation coefficient between xx and yy over the past tt days. |




Table 7: 
Length increments Î”â€‹k\Delta k for each production rule.

| Production Rules | Î”â€‹k\Delta k |
| --- | --- |
| ğ–¤ğ—‘ğ—‰ğ—‹â†’ğ–¥ğ–¾ğ–ºğ—ğ—ğ—‹ğ–¾\mathsf{Expr}\to\mathsf{Feature} | 0 |
| ğ–­ğ—ğ—†â†’20â€‹â€¦\mathsf{Num}\to 20\dots | 0 |
| ğ–¢ğ—ˆğ—‡ğ—Œğ—ğ–ºğ—‡ğ—â†’âˆ’0.01â€‹â€¦\mathsf{Constant}\to-0.01\dots | 0 |
| ğ–¤ğ—‘ğ—‰ğ—‹â†’ğ–´ğ—‡ğ–ºğ—‹ğ—’ğ–®ğ—‰â€‹(ğ–¤ğ—‘ğ—‰ğ—‹)\mathsf{Expr}\to\mathsf{UnaryOp}(\mathsf{Expr}) | 1 |
| ğ–¤ğ—‘ğ—‰ğ—‹â†’ğ–¡ğ—‚ğ—‡ğ–ºğ—‹ğ—’ğ–®ğ—‰â€‹(ğ–¤ğ—‘ğ—‰ğ—‹,ğ–¤ğ—‘ğ—‰ğ—‹)\mathsf{Expr}\to\mathsf{BinaryOp}(\mathsf{Expr},\mathsf{Expr}) | 2 |
| ğ–¤ğ—‘ğ—‰ğ—‹â†’ğ–¡ğ—‚ğ—‡ğ–ºğ—‹ğ—’ğ–®ğ—‰â€‹(ğ–¤ğ—‘ğ—‰ğ—‹,ğ–¢ğ—ˆğ—‡ğ—Œğ—ğ–ºğ—‡ğ—)\mathsf{Expr}\to\mathsf{BinaryOp}(\mathsf{Expr},\mathsf{Constant}) | 2 |
| ğ–¤ğ—‘ğ—‰ğ—‹â†’ğ–¡ğ—‚ğ—‡ğ–ºğ—‹ğ—’ğ–®ğ—‰â€‹\_â€‹ğ– ğ—Œğ—’ğ—†â€‹(ğ–¢ğ—ˆğ—‡ğ—Œğ—ğ–ºğ—‡ğ—,ğ–¤ğ—‘ğ—‰ğ—‹)\mathsf{Expr}\to\mathsf{BinaryOp\\_Asym}(\mathsf{Constant},\mathsf{Expr}) | 2 |
| ğ–¤ğ—‘ğ—‰ğ—‹â†’ğ–±ğ—ˆğ—…ğ—…ğ—‚ğ—‡ğ—€ğ–®ğ—‰â€‹(ğ–¤ğ—‘ğ—‰ğ—‹,ğ–­ğ—ğ—†)\mathsf{Expr}\to\mathsf{RollingOp}(\mathsf{Expr},\mathsf{Num}) | 2 |
| ğ–¤ğ—‘ğ—‰ğ—‹â†’ğ–¯ğ–ºğ—‚ğ—‹ğ–¾ğ–½ğ–±ğ—ˆğ—…ğ—…ğ—‚ğ—‡ğ—€ğ–®ğ—‰â€‹(ğ–¤ğ—‘ğ—‰ğ—‹,ğ–¤ğ—‘ğ—‰ğ—‹,ğ–­ğ—ğ—†)\mathsf{Expr}\to\mathsf{PairedRollingOp}(\mathsf{Expr},\mathsf{Expr},\mathsf{Num}) | 3 |

## Appendix B Algorithms

### B.1 Linear combination alpha factor algorithm

The linear combination factor model is defined as

|  |  |  |  |
| --- | --- | --- | --- |
|  | câ€‹(X;F,w)=âˆ‘j=1nwjâ€‹fjâ€‹(X)=y,c(X;F,w)=\sum\_{j=1}^{n}w\_{j}f\_{j}(X)=y, |  | (6) |

where F={f1,â€¦,fn}F=\{f\_{1},\dots,f\_{n}\} denotes the set of factors, w={w1,â€¦,wn}w=\{w\_{1},\dots,w\_{n}\} are the weights of factors in linear combination , XX represents the input stock feature data, and yy is the combined output. The optimization is conducted by minimizing the loss function

|  |  |  |  |
| --- | --- | --- | --- |
|  | Lâ€‹(w)=1Tâ€‹âˆ‘t=1Tâ€–ytâˆ’rtâ€–2L(w)=\frac{1}{T}\sum\_{t=1}^{T}\|y\_{t}-r\_{t}\|^{2} |  | (7) |

where rtr\_{t} is the actual stock return, and yty\_{t} is the alpha value of linear combination factor.

Algorithm 1  Incremental Combination Model Optimization

Input: alpha set F={f1,â€¦,fn}F=\{f\_{1},\ldots,f\_{n}\}, weights w={w1,â€¦,wn}w=\{w\_{1},\ldots,w\_{n}\}, new alpha fnewf\_{\text{new}}

Output: optimal alpha subset Fâˆ—F^{\*}, optimal weights wâˆ—w^{\*}, combination IC ICâ„±\mathrm{IC}\_{\mathcal{F}}

Fâ†Fâˆª{fnew}F\leftarrow F\cup\{f\_{\text{new}}\}

wâ†wâˆ¥rand()w\leftarrow w\,\|\,\text{rand()}

for i=1i=1 to num\_gradient\_steps do

Compute loss Lâ€‹(w)L(w) according to Eq.Â ([7](https://arxiv.org/html/2601.22119v1#A2.E7 "Equation 7 â€£ B.1 Linear combination alpha factor algorithm â€£ Appendix B Algorithms â€£ Alpha Discovery via Grammar-Guided Learning and Search"))

wâ†GradientDescentâ€‹(Lâ€‹(w))w\leftarrow\text{GradientDescent}(L(w))

end for

pâ†argâ¡miniâ¡|wi|p\leftarrow\arg\min\_{i}|w\_{i}|

Fâ†Fâˆ–{fp}F\leftarrow F\setminus\{f\_{p}\}; â€ƒwâ†wâˆ–{wp}w\leftarrow w\setminus\{w\_{p}\}

Compute the combination IC: ICâ„±â†ICâ€‹(F,w)\mathrm{IC}\_{\mathcal{F}}\leftarrow\text{IC}(F,w)

Return F,w,ICâ„±F,w,\mathrm{IC}\_{\mathcal{F}}

### B.2 Length control of semantic interpretable alpha factor generator

Following the intuition of grammar-constrained generationÂ (Jin et al., [2018](https://arxiv.org/html/2601.22119v1#bib.bib20 "Unsupervised grammar induction with depth-bounded pcfg")), we introduce a kk-bounded constraint to explicitly limit expression length.
The mechanism maintains a counter k\mathit{k} for the partial length of the expression and enforces a maximum threshold KK. Each production rule has a predefined increment Î”â€‹k\Delta k, representing its contribution to the expression length(see [TableÂ 7](https://arxiv.org/html/2601.22119v1#A1.T7 "In Appendix A Tables â€£ Alpha Discovery via Grammar-Guided Learning and Search") for details). A rule is applied only if
k+Î”â€‹kâ‰¤K,\mathit{k}+\Delta k\leq K,
thereby guaranteeing that each expansion step remains within the feasible bound.
By integrating this length-aware constraint into the derivation procedure, we obtain a bounded variant of Î±\alpha-Sem, denoted as Î±\alpha-Sem-k.
The procedure is described in [AlgorithmÂ 2](https://arxiv.org/html/2601.22119v1#alg2 "In B.2 Length control of semantic interpretable alpha factor generator â€£ Appendix B Algorithms â€£ Alpha Discovery via Grammar-Guided Learning and Search").

Algorithm 2  Î±\alpha-Sem-kk

Input: Grammar G=(ğ’©,ğ’¯,ğ’«,ğ’®)G=(\mathcal{N},\mathcal{T},\mathcal{P},\mathcal{S}), maximum length KK, rule increments Î”â€‹k:Î“â†’Î²\Delta k:\Gamma\to\beta

Output: Prefix expression tree TT

Initialize TT as a single-node tree with root SS

Set: kâ†0k\leftarrow 0

while TT contains a nonterminal node do

Let uu be the first nonterminal node in a pre-order traversal of TT

Compute the set of applicable rules:

|  |  |  |
| --- | --- | --- |
|  | ğ’œâ†{lâˆˆğ’«âˆ£lâ€‹Â is applicable toÂ â€‹uâ€‹Â andÂ â€‹k+Î”â€‹kâ€‹(l)â‰¤K}\mathcal{A}\leftarrow\{l\in\mathcal{P}\mid l\text{ is applicable to }u\text{ and }k+\Delta k(l)\leq K\} |  |

Choose rule l:Î“â†’Î²l:\Gamma\to\beta from ğ’œ\mathcal{A}

Replace node uu with children corresponding to Î²\beta

Update: kâ†k+Î”â€‹kâ€‹(l)k\leftarrow k+\Delta k(l)

end while

Return TT

### B.3 Algorithm of Four Stages of MCTS

Algorithm 3  Grammar-aware MCTS with Branch-adapted PUCT

Input: root state sroots\_{\mathrm{root}}, policy-value network fÎ¸f\_{\theta}, iteration count II

Output: improved policy Ï€â€‹(aâˆ£sroot)\pi(a\mid s\_{\mathrm{root}})

for i=1i=1 to II do

sâ†sroots\leftarrow s\_{\mathrm{root}}

Initialize empty list of traversed edges Eâ†[]E\leftarrow[\;]

while ss is not fully expanded do

bâ†b\leftarrow number of valid actions from ss

aâˆ—â†argâ¡maxaâ¡[Qâ€‹(s,a)+cpuctâ‹…bbrefâ‹…Pâ€‹(s,a)â‹…âˆ‘bNâ€‹(s,b)1+Nâ€‹(s,a)]a^{\*}\leftarrow\arg\max\_{a}\Bigg[Q(s,a)+c\_{\text{puct}}\cdot\sqrt{\tfrac{b}{b\_{\mathrm{ref}}}}\cdot P(s,a)\cdot\tfrac{\sqrt{\sum\_{b}N(s,b)}}{1+N(s,a)}\Bigg]

Append (s,aâˆ—)(s,a^{\*}) to EE

sâ†applyâ€‹(s,aâˆ—)s\leftarrow\text{apply}(s,a^{\*})

end while

sLâ†ss\_{L}\leftarrow s

(Pâ€‹(sL,â‹…),Vâ€‹(sL))â†fÎ¸â€‹(sL)(P(s\_{L},\cdot),V(s\_{L}))\leftarrow f\_{\theta}(s\_{L})

Expand sLs\_{L} using Pâ€‹(sL,â‹…)P(s\_{L},\cdot)

for all (s,a)âˆˆE(s,a)\in E do

Nâ€‹(s,a)â†Nâ€‹(s,a)+1N(s,a)\leftarrow N(s,a)+1

Qâ€‹(s,a)â†1Nâ€‹(s,a)â€‹âˆ‘sâ€²âˆ£s,aâ†’sâ€²Vâ€‹(sâ€²)Q(s,a)\leftarrow\frac{1}{N(s,a)}\sum\_{s^{\prime}\mid s,a\rightarrow s^{\prime}}V(s^{\prime})

end for

end for

Ï€â€‹(aâˆ£sroot)=Nâ€‹(sroot,a)1/Tâˆ‘bâˆˆAâ€‹(sroot)Nâ€‹(sroot,b)1/T\pi(a\mid s\_{\mathrm{root}})=\frac{N(s\_{\mathrm{root}},a)^{1/T}}{\sum\_{b\in A(s\_{\mathrm{root}})}N(s\_{\mathrm{root}},b)^{1/T}}

Return Ï€â€‹(aâˆ£sroot)\pi(a\mid s\_{\mathrm{root}})

Assume that at a certain iteration ii, our MCTS has already explored a portion of the TSL-MDP, denoted by an agent MiM\_{i}. This agent corresponds to a subtree of the large TSL-MDP, sharing the same root, and MiM\_{i} has obtained policy for this partial subtree. For example, at simulation MiM\_{i}, the subtree agent MiM\_{i} shown on the left in [FigureÂ 3](https://arxiv.org/html/2601.22119v1#S4.F3 "In 4.1 Decision-Making on Large Tree â€£ 4 Reinforced Alpha Language Tree Search â€£ Alpha Discovery via Grammar-Guided Learning and Search") has already been explored. This subtree starts as only a root when i=0i=0, and is intended to expand toward the full TSL-MDP tree as ii increases, eventually reaching iteration i=Ii=I.

Selection. First, within MiM\_{i}, starting from root of the subtree, the MCTS agent repeatedly selects an Î±\alpha-CFG production rule at each incomplete alpha expression (each round-box node), and replaces its leftmost nonterminal symbol (the dark black arrows in [FigureÂ 3](https://arxiv.org/html/2601.22119v1#S4.F3 "In 4.1 Decision-Making on Large Tree â€£ 4 Reinforced Alpha Language Tree Search â€£ Alpha Discovery via Grammar-Guided Learning and Search")), which goes to a new incomplete alpha expression (a child round-box node). This repeats until it reaches a â€œfrontierâ€ alpha expression that has a child not yet included in MiM\_{i} (e.g., node (1) in [FigureÂ 3](https://arxiv.org/html/2601.22119v1#S4.F3 "In 4.1 Decision-Making on Large Tree â€£ 4 Reinforced Alpha Language Tree Search â€£ Alpha Discovery via Grammar-Guided Learning and Search")).

The TSL-MDP has two key features: (1) different nonterminal symbols have different numbers of production rules, and (2) the number of valid production rules decreases sharply near the bottom of the search tree due to the length control in [B.2](https://arxiv.org/html/2601.22119v1#A2.SS2 "B.2 Length control of semantic interpretable alpha factor generator â€£ Appendix B Algorithms â€£ Alpha Discovery via Grammar-Guided Learning and Search"). To address this, we adopt a production rule selection function analogous to PUCT (Silver et al., [2017](https://arxiv.org/html/2601.22119v1#bib.bib5 "Mastering the game of go without human knowledge")).

|  |  |  |  |
| --- | --- | --- | --- |
|  | aâˆ—=argâ¡maxaâ¡(Qâ€‹(s,a)+cpuctâ‹…bbrefâ‹…Pâ€‹(s,a)â‹…âˆ‘bNâ€‹(s,b)1+Nâ€‹(s,a)),a^{\*}=\arg\max\_{a}\left(Q(s,a)+c\_{\text{puct}}\cdot\sqrt{\tfrac{b}{b\_{\text{ref}}}}\cdot P(s,a)\cdot\frac{\sqrt{\sum\_{b}N(s,b)}}{1+N(s,a)}\right), |  | (8) |

Here, Qâ€‹(s,a)Q(s,a) is the value of selecting production rule aa for formula ss, and Pâ€‹(s,a)P(s,a) is the probability of selecting aa under ss.
bb is the number of branches at the current depth, and brefb\_{\text{ref}} is the branch balance constant (defined by the maximum number of branches)
Eq. ([8](https://arxiv.org/html/2601.22119v1#A2.E8 "Equation 8 â€£ B.3 Algorithm of Four Stages of MCTS â€£ Appendix B Algorithms â€£ Alpha Discovery via Grammar-Guided Learning and Search")) balances irregular branching through the adaptive term b/bref\sqrt{b/b\_{\text{ref}}}: smaller branching factors emphasize exploitation, while larger ones promote broader exploration.

Expansion. After finding such a frontier alpha expression node, the MCTS agent will execute a certain production rule on it, generating a new alpha expression which has not yet been covered by MiM\_{i} (e.g., round-box node (2) in [FigureÂ 3](https://arxiv.org/html/2601.22119v1#S4.F3 "In 4.1 Decision-Making on Large Tree â€£ 4 Reinforced Alpha Language Tree Search â€£ Alpha Discovery via Grammar-Guided Learning and Search")), and also attaching all the corresponding possible production rules to this new alpha expression (e.g., the two arrows attached to node (2)).
The probabilities for executing available production rules for expression ss follow the distribution Pâ€‹(s)P(s).

Evaluation. Since the newly expanded alpha expression is at the head of the current agent MtM\_{t} and remains incomplete, the existing policy cannot assess its quality. Thus, MCTS requires a method to evaluate it. Given the vastness of the TSL-MDP, traditional simulation-based evaluation is infeasible. Moreover, as shown in [DefinitionÂ 1](https://arxiv.org/html/2601.22119v1#Thmdefinition1 "Definition 1. â€£ Structural Well-Formedness. â€£ 3.1 Syntactically-Valid Alpha Language â€£ 3 Design Language of Interpretable Alphas â€£ Alpha Discovery via Grammar-Guided Learning and Search"), the expressions at any state in TSL-MDP are small tree structures (i.e., the small trees inside each round-box in [FigureÂ 3](https://arxiv.org/html/2601.22119v1#S4.F3 "In 4.1 Decision-Making on Large Tree â€£ 4 Reinforced Alpha Language Tree Search â€£ Alpha Discovery via Grammar-Guided Learning and Search")). Therefore, in the next section, we design a Tree-LSTMâ€“based representation learning method to construct a value network for Vâ€‹(s)V(s), as well as a policy network Pâ€‹(s,a)P(s,a) over any expression.

Backpropagation. The result Vâ€‹(s)V(s) of evaluation is backpropagated from the path of selection (the path directed by black arrow in the third tree of [FigureÂ 3](https://arxiv.org/html/2601.22119v1#S4.F3 "In 4.1 Decision-Making on Large Tree â€£ 4 Reinforced Alpha Language Tree Search â€£ Alpha Discovery via Grammar-Guided Learning and Search")). Mean value of each edge in the path is updated by Vâ€‹(s)V(s) and visit count Nâ€‹(s,a)N(s,a) of each edge in the path increases by one.

The MCTS agent MiM\_{i} executes the above procedures at each iteration ii ([AlgorithmÂ 3](https://arxiv.org/html/2601.22119v1#alg3 "In B.3 Algorithm of Four Stages of MCTS â€£ Appendix B Algorithms â€£ Alpha Discovery via Grammar-Guided Learning and Search") shows the procedure of MCTS search.). Since one node is expanded at each step, the MCTS agent MiM\_{i} will eventually cover enough nodes and edges of the TSL-MDP. The resulting search assigns a basic value to every node and obtain a basic policy for the TSL-MDP, which two can be used to further optimize the policy.

## Appendix C Supplement to Problem Formulation

[FigureÂ 6(a)](https://arxiv.org/html/2601.22119v1#A3.F6.sf1 "In Figure 6 â€£ Appendix C Supplement to Problem Formulation â€£ Alpha Discovery via Grammar-Guided Learning and Search") illustrates the calculation process of an alpha factor. For a period of TT trading days, we compute the alpha factor for each stock using an alpha factor function

|  |  |  |
| --- | --- | --- |
|  | ğ²t=(yt,1,â€¦,yt,n)âˆˆâ„n,\mathbf{y}\_{t}=(y\_{t,1},\dots,y\_{t,n})\in\mathbb{R}^{n}, |  |

which takes as input the feature data of nn stocks over the current day tt and the previous Ï„â€²âˆ’1\tau^{\prime}-1 days. The resulting values represent the score of each stock for the current day, i.e., the alpha factor. These alpha values are subsequently used for stock selection and the formulation of trading strategies.

[FigureÂ 6(b)](https://arxiv.org/html/2601.22119v1#A3.F6.sf2 "In Figure 6 â€£ Appendix C Supplement to Problem Formulation â€£ Alpha Discovery via Grammar-Guided Learning and Search") shows an example of formulaic factor: The factor Sumâ€‹(Subâ€‹(vâ€‹wâ€‹aâ€‹p,1),2â€‹d)\mathrm{Sum}(\mathrm{Sub}(vwap,1),2d) computes the sum of the most recent two days of VWAP values after subtracting 11 from each.
To obtain the factor value on Wednesday, the operator first evaluates Subâ€‹(vâ€‹wâ€‹aâ€‹p,1)\mathrm{Sub}(vwap,1) for Tuesday and Wednesday and then aggregates them:
(2âˆ’1)+(3âˆ’1)=3(2-1)+(3-1)=3.
This output serves as the alpha signal, the predicted return for Wednesday which is subsequently used in downstream stock-selection or portfolio-construction procedures.

![Refer to caption](factor-new.png)


(a) Illustration of an alpha factor.

![Refer to caption](cal.png)


(b) An example of a formulaic factor.

Figure 6: Alpha example.

## Appendix D Reinforcement Learning Framework

We present pseudo-code of MCTS combined with reinforcement learning method ([AlgorithmÂ 4](https://arxiv.org/html/2601.22119v1#alg4 "In Appendix D Reinforcement Learning Framework â€£ Alpha Discovery via Grammar-Guided Learning and Search")). This is a reinforcement learning-based factor mining method designed to automatically discover a combination of factors from stock market data that can effectively predict stock returns. Specifically, the algorithm initializes a set of factors, their corresponding weights, and a policy-value network. In the process of obtaining data through reinforcement learning, it employs a MCTS policy to generate actions for each state, thereby constructing a multi-step factor generation path. The final state of the path is parsed into a computable alpha expression, evaluated using the Iâ€‹Câ„±IC\_{\mathcal{F}} as the reward signal. The reward is given along with the optimization of the factor combination â„±\mathcal{F}. The actual value for each step along the path, denoted as ztz\_{t} is computed based on Iâ€‹Câ„±IC\_{\mathcal{F}} and the similarity between the newly generated factor and existing ones, following the formulation in [EquationÂ 5](https://arxiv.org/html/2601.22119v1#S4.E5 "In 4.3 Syntax Representation Learning â€£ 4 Reinforced Alpha Language Tree Search â€£ Alpha Discovery via Grammar-Guided Learning and Search") in [SectionÂ 4.3](https://arxiv.org/html/2601.22119v1#S4.SS3 "4.3 Syntax Representation Learning â€£ 4 Reinforced Alpha Language Tree Search â€£ Alpha Discovery via Grammar-Guided Learning and Search").

After generating multi-step factor paths in each iteration, the policy and value networks are trained using the collected path data (sj,Ï€â€‹(a|sj),zj)(s\_{j},\pi(a|s\_{j}),z\_{j}) stored in a replay buffer, where sjs\_{j} is the state vector encoded by TreeLSTM, Ï€â€‹(a|sj)\pi(a|s\_{j}) is the policy from MCTS, and ztz\_{t} is shown above. After training, the networks are redeployed to guide a new round of search. Through iterative training and exploration, the IC of the learned factor combination is progressively improved. The algorithm outputs the final optimized factor combination set along with its corresponding weights when the IC shows no more significant improvement.

The overall workflow of this algorithm is illustrated in [FigureÂ 7](https://arxiv.org/html/2601.22119v1#A4.F7 "In Appendix D Reinforcement Learning Framework â€£ Alpha Discovery via Grammar-Guided Learning and Search") in the following page, while a specific illustration of its MCTS component [AlgorithmÂ 3](https://arxiv.org/html/2601.22119v1#alg3 "In B.3 Algorithm of Four Stages of MCTS â€£ Appendix B Algorithms â€£ Alpha Discovery via Grammar-Guided Learning and Search") is in [FigureÂ 3](https://arxiv.org/html/2601.22119v1#S4.F3 "In 4.1 Decision-Making on Large Tree â€£ 4 Reinforced Alpha Language Tree Search â€£ Alpha Discovery via Grammar-Guided Learning and Search"), and the illustration of its neural network part is in [FigureÂ 3](https://arxiv.org/html/2601.22119v1#S4.F3 "In 4.1 Decision-Making on Large Tree â€£ 4 Reinforced Alpha Language Tree Search â€£ Alpha Discovery via Grammar-Guided Learning and Search") (b).

![Refer to caption](AlphaCFG-4.png)


Figure 7: The overall framework of AlphaCFG.




Algorithm 4  Alpha Mining via Reinforcement Learning

Input: stock trend dataset Y={yt}Y=\{y\_{t}\}

Output: optimal alpha subset Fâˆ—F^{\*}, optimal weights wâˆ—w^{\*}

Initialize alpha set FF and weights ww

Initialize policy-value network fÎ¸f\_{\theta} and replay buffer DD

for each epoch do

for each factor path search do

Initialize empty trajectory Eâ†[]E\leftarrow[\;]

for j=0j=0 to JJ do

Append state sjs\_{j} to EE

srootâ†sjs\_{\mathrm{root}}\leftarrow s\_{j}

Ï€â€‹(aâˆ£sj)â†Ï€â€‹(aâˆ£sroot)\pi(a\mid s\_{j})\leftarrow\pi(a\mid s\_{\mathrm{root}})

Sample action ajâˆ¼Ï€â€‹(aâˆ£sj)a\_{j}\sim\pi(a\mid s\_{j})

sj+1â†applyâ€‹(sj,aj)s\_{j+1}\leftarrow\text{apply}(s\_{j},a\_{j})

end for

fjâ†parseâ€‹(sJ)f\_{j}\leftarrow\text{parse}(s\_{J})

Obtain reward ICâ„±\mathrm{IC}\_{\mathcal{F}} using [AlgorithmÂ 1](https://arxiv.org/html/2601.22119v1#alg1 "In B.1 Linear combination alpha factor algorithm â€£ Appendix B Algorithms â€£ Alpha Discovery via Grammar-Guided Learning and Search")

Reward Assignment

for j=0j=0 to JJ do

zâ€‹(sj)â†(1âˆ’maxâ¡(0,maxftâˆˆFâ¡simâ€‹(ft,fj)))â‹…ICâ„±z(s\_{j})\leftarrow\Bigl(1-\max\bigl(0,\max\_{f\_{t}\in F}\mathrm{sim}(f\_{t},f\_{j})\bigr)\Bigr)\cdot\mathrm{IC}\_{\mathcal{F}}

Dâ†Dâˆª{(sj,Ï€â€‹(aâˆ£sj),zâ€‹(sj))}D\leftarrow D\cup\{(s\_{j},\pi(a\mid s\_{j}),z(s\_{j}))\}

end for

end for

Network Update

for each gradient step do

Sample minibatch BâŠ‚DB\subset D

LÎ¸=(zâ€‹(st)âˆ’VÎ¸â€‹(st))2âˆ’âˆ‘aÏ€â€‹(aâˆ£st)â€‹logâ¡PÎ¸â€‹(aâˆ£st)+câ€‹â€–Î¸â€–2L\_{\theta}=\left(z(s\_{t})-V\_{\theta}(s\_{t})\right)^{2}-\sum\_{a}\pi(a\mid s\_{t})\log P\_{\theta}(a\mid s\_{t})+c\|\theta\|^{2}

Î¸â†Î¸âˆ’Î·â€‹âˆ‡Î¸LÎ¸\theta\leftarrow\theta-\eta\nabla\_{\theta}L\_{\theta}

end for

end for

Return Fâˆ—,wâˆ—F^{\*},w^{\*}

## Appendix E Search Space Complexity

To compare the sizes of expression search spaces under different generation methods, we study three methods from a combinatorial perspective:
(i) a purely exponential baseline (arbitrary combination of all symbols corresponding to Î£âˆ—\Sigma^{\*});
(ii) Î±\alpha-Syn (corresponding to â„’syn\mathcal{L}\_{\mathrm{syn}});
(iii) Î±\alpha-Sem (corresponding to â„’syn\mathcal{L}\_{\mathrm{syn}}).
All three methods share the same parameter sets (operator types, number of features, constants, etc.), but progressively impose stricter constraints, resulting in smaller search spaces.

We set the following notation: the size of the unary operator set is |U||U|, the size of the binary operator set is |B||B|, the size of the asymmetric binary operator set is |Basym||B\_{\text{asym}}|, the size of the rolling operator set is |R||R|, the size of the paired rolling operator set is |Rpair||R\_{\text{pair}}|, the number of features is |â„±||\mathcal{F}|, the number of constant parameters is |ğ’||\mathcal{C}|, and the number of rolling-window parameters is |ğ’©||\mathcal{N}|.

### E.1 Unstructured Space Î£âˆ—\Sigma^{\*}

The method of arbitrary symbol combination (referred to ) takes one symbol equally at each step from all available symbols.
Let the total number of symbols be:

|  |  |  |
| --- | --- | --- |
|  | r=|â„±|+|ğ’|+|ğ’©|+|U|+|B|+|Basym|+|R|+|Rpair|.r=|\mathcal{F}|+|\mathcal{C}|+|\mathcal{N}|+|U|+|B|+|B\_{\text{asym}}|+|R|+|R\_{\text{pair}}|. |  |

Then the number of sequences of length nn is
rn=rn,r\_{n}=r^{n},
and the cumulative size is
âˆ‘iâ‰¤nri=Î˜â€‹(rn).\sum\_{i\leq n}r\_{i}=\Theta(r^{n}).

### E.2 Syntactically Legal Space â„’syn\mathcal{L}\_{\mathrm{syn}}

We introduce syntax constraints to ensure that generated expressions are all syntactically valid.
We consider the grammar Î±\alpha-Syn:

|  |  |  |
| --- | --- | --- |
|  | ğ–¤ğ—‘ğ—‰ğ—‹â†’ğ–´ğ—‡ğ–ºğ—‹ğ—’ğ–®ğ—‰â€‹(E)â€‹âˆ£ğ–¡ğ—‚ğ—‡ğ–ºğ—‹ğ—’ğ–®ğ—‰â€‹(E,E)âˆ£â€‹ğ–±ğ—ˆğ—…ğ—…ğ—‚ğ—‡ğ—€ğ–®ğ—‰â€‹(E,E)â€‹âˆ£ğ–¯ğ–ºğ—‚ğ—‹ğ–¾ğ–½ğ–±ğ—ˆğ—…ğ—…ğ—‚ğ—‡ğ—€ğ–®ğ—‰â€‹(E,E,E)âˆ£â€‹ğ–³ğ–¾ğ—‹ğ—†ğ–²ğ—’ğ–».\mathsf{Expr}\rightarrow\mathsf{UnaryOp}(E)\mid\mathsf{BinaryOp}(E,E)\mid\mathsf{RollingOp}(E,E)\mid\mathsf{PairedRollingOp}(E,E,E)\mid\mathsf{TermSyb}. |  |

Let hnh\_{n} be the number of valid expressions of length nn.
The terminal set size is:
T=|â„±|+|ğ’|+|ğ’©|.T=|\mathcal{F}|+|\mathcal{C}|+|\mathcal{N}|.

Define operator cardinalities:
U=|U|,Q=|B|+|Basym|,R=|R|,P=|Rpair|U=|U|,Q=|B|+|B\_{\text{asym}}|,R=|R|,P=|R\_{\text{pair}}|, respectively(The meanings of the notations are as shown in D).

The recurrence formula is:
h1=T,h\_{1}=T,
and for nâ‰¥2n\geq 2:

|  |  |  |
| --- | --- | --- |
|  | hn=Uâ€‹hnâˆ’1+(Q+R)â€‹âˆ‘i=1nâˆ’2hiâ€‹hnâˆ’1âˆ’i+Pâ€‹âˆ‘i+j+k=nâˆ’1i,j,kâ‰¥1hiâ€‹hjâ€‹hk.h\_{n}=Uh\_{n-1}+(Q+R)\sum\_{i=1}^{n-2}h\_{i}\,h\_{n-1-i}+P\!\!\!\sum\_{\begin{subarray}{c}i+j+k=n-1\\ i,j,k\geq 1\end{subarray}}h\_{i}h\_{j}h\_{k}. |  |

The subsequent derivation of an explicit form from this recurrence becomes rather cumbersome. Since the technical steps mirror the usual treatment of general cubic functional equations, we omit the full derivation here.

### E.3 Semantically Legal Space â„’sem\mathcal{L}\_{\mathrm{sem}}

Î±\alpha-Sem introduces more constraints on constants, argument types, and rolling windows:

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ–¤ğ—‘ğ—‰ğ—‹â†’\displaystyle\mathsf{Expr}\to | ğ–¥ğ–¾ğ–ºğ—ğ—ğ—‹ğ–¾âˆ£ğ–´ğ—‡ğ–ºğ—‹ğ—’ğ–®ğ—‰â€‹(ğ–¤ğ—‘ğ—‰ğ—‹)\displaystyle\ \mathsf{Feature}\mid\mathsf{UnaryOp}(\mathsf{Expr}) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | âˆ£ğ–¡ğ—‚ğ—‡ğ–ºğ—‹ğ—’ğ–®ğ—‰â€‹(ğ–¤ğ—‘ğ—‰ğ—‹,ğ–¤ğ—‘ğ—‰ğ—‹)âˆ£â€‹ğ–¡ğ—‚ğ—‡ğ–ºğ—‹ğ—’ğ–®ğ—‰â€‹(ğ–¤ğ—‘ğ—‰ğ—‹,ğ–¢ğ—ˆğ—‡ğ—Œğ—ğ–ºğ—‡ğ—)\displaystyle\mid\mathsf{BinaryOp}(\mathsf{Expr},\mathsf{Expr})\mid\mathsf{BinaryOp}(\mathsf{Expr},\mathsf{Constant}) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | âˆ£ğ–¡ğ—‚ğ—‡ğ–ºğ—‹ğ—’ğ–®ğ—‰â€‹\_â€‹ğ– ğ—Œğ—’ğ—†â€‹(ğ–¢ğ—ˆğ—‡ğ—Œğ—ğ–ºğ—‡ğ—,ğ–¤ğ—‘ğ—‰ğ—‹)âˆ£â€‹ğ–±ğ—ˆğ—…ğ—…ğ—‚ğ—‡ğ—€ğ–®ğ—‰â€‹(ğ–¤ğ—‘ğ—‰ğ—‹,ğ–­ğ—ğ—†)\displaystyle\mid\mathsf{BinaryOp\\_Asym}(\mathsf{Constant},\mathsf{Expr})\mid\mathsf{RollingOp}(\mathsf{Expr},\mathsf{Num}) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | âˆ£ğ–¯ğ–ºğ—‚ğ—‹ğ–¾ğ–½ğ–±ğ—ˆğ—…ğ—…ğ—‚ğ—‡ğ—€ğ–®ğ—‰(ğ–¤ğ—‘ğ—‰ğ—‹,ğ–¤ğ—‘ğ—‰ğ—‹,ğ–­ğ—ğ—†),\displaystyle\mid\mathsf{PairedRollingOp}(\mathsf{Expr},\mathsf{Expr},\mathsf{Num}), |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ–­ğ—ğ—†â†’\displaystyle\mathsf{Num}\to | 20âˆ£â‹¯,ğ–¢ğ—ˆğ—‡ğ—Œğ—ğ–ºğ—‡ğ—â†’âˆ’0.01âˆ£â‹¯\displaystyle 20\mid\cdots,\qquad\mathsf{Constant}\to-01\mid\cdots |  |

Let fnf\_{n} denotes the number of valid expressions of length nn.

The recurrence formula becomes

|  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- |
|  | fn=\displaystyle f\_{n}= | |U|â€‹fnâˆ’1\displaystyle|U|\,f\_{n-1} |  | (unary) |  |
|  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- |
|  |  | +|B|â€‹âˆ‘i=1nâˆ’2fiâ€‹fnâˆ’1âˆ’i\displaystyle+|B|\sum\_{i=1}^{n-2}f\_{i}f\_{n-1-i} |  | (binary) |  |
|  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- |
|  |  | +|B|â€‹|ğ’|â€‹fnâˆ’2\displaystyle+|B|\,|\mathcal{C}|\,f\_{n-2} |  | (binary + right constant) |  |
|  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- |
|  |  | +|Basym|â€‹|ğ’|â€‹fnâˆ’2\displaystyle+|B\_{\text{asym}}|\,|\mathcal{C}|\,f\_{n-2} |  | (asymmetric binary + left constant) |  |
|  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- |
|  |  | +|R|â€‹|ğ’©|â€‹fnâˆ’2\displaystyle+|R|\,|\mathcal{N}|\,f\_{n-2} |  | (rolling) |  |
|  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- |
|  |  | +|Rpair|â€‹|ğ’©|â€‹âˆ‘i=1nâˆ’3fiâ€‹fnâˆ’2âˆ’i\displaystyle+|R\_{\text{pair}}|\,|\mathcal{N}|\sum\_{i=1}^{n-3}f\_{i}f\_{n-2-i} |  | (paired rolling).\displaystyle\text{(paired rolling)}. |  |

The recurrence formula is similar, and compared with Î±\alpha-Syn, recurrence of Î±\alpha-Sem includes more convolution terms and more realistic constraints, providing a more accurate operator usage. In the following, we present the overall analysis.

Because the expression length is unbounded, the search spaces of all three generation methods are infinite.
Therefore, the comparison does not concern the total size of each space, but rather the size of the finite subspace consisting of expressions whose length is at most nn.

For each grammar, the production rules yield a recurrence for the number of expressions of exact length nn ) ( rn,hn,fnr\_{n},h\_{n},f\_{n}), and accumulating these values from 11 to nn gives the size of the corresponding truncated subspace. By computing these cumulative counts and plotting their growth as functions of nn, we can directly compare how quickly the reachable portions of the three search spaces expand.

### E.4 Empirical Verification

Based on the recurrence formulas, We compute the cumulative counts of {rn}\{r\_{n}\}, {hn}\{h\_{n}\}, and {fn}\{f\_{n}\} for n=1âˆ¼n=1\sim NN, and plot their growth curves to visualize differences between the three methods (shown in [FigureÂ 8](https://arxiv.org/html/2601.22119v1#A5.F8 "In E.4 Empirical Verification â€£ Appendix E Search Space Complexity â€£ Alpha Discovery via Grammar-Guided Learning and Search")). Since all three methods yield inherently infinite search spaces, we further design Î±\alpha-Sem-k based on [AlgorithmÂ 2](https://arxiv.org/html/2601.22119v1#alg2 "In B.2 Length control of semantic interpretable alpha factor generator â€£ Appendix B Algorithms â€£ Alpha Discovery via Grammar-Guided Learning and Search"), which can be seen as the red dotted line in [FigureÂ 8](https://arxiv.org/html/2601.22119v1#A5.F8 "In E.4 Empirical Verification â€£ Appendix E Search Space Complexity â€£ Alpha Discovery via Grammar-Guided Learning and Search"). The results are consistent with the analysis in [FigureÂ 1](https://arxiv.org/html/2601.22119v1#S3.F1 "In 3 Design Language of Interpretable Alphas â€£ Alpha Discovery via Grammar-Guided Learning and Search"), which further strengthens the superiority of our approach in theory.

[FigureÂ 8](https://arxiv.org/html/2601.22119v1#A5.F8 "In E.4 Empirical Verification â€£ Appendix E Search Space Complexity â€£ Alpha Discovery via Grammar-Guided Learning and Search") explains the core of the superiority of our method: By introducing constraints of syntax and semantics, We get an infinite set containing only valid factors. In actual factor search tasks, we cannot exhaust this space that exploring a finite subset is realistic. Therefore, We utilize the recursive feature of CFG and further designed Î±\alpha-Sem-k capable of generating factors of only a finite length. Ultimately, we reduced the complexity of the search space from an exponential level to a constant level, making this task solvable.

![Refer to caption](x4.png)


Figure 8: Comparison of cumulative search space sizes of different grammar levels.

## Appendix F Details of Tree-LSTM

Starting from ASR leaf nodes, the Tree-LSTM recursively aggregates child hidden and cell states through gating (input, forget, output), combining them with the nodeâ€™s input embedding. This bottom-up process continues until the root, yielding a fixed-dimensional state vector that encodes both the syntax and operator-specific dependencies of the entire expression. Thus, the Tree-LSTM transforms variable-sized trees into single vectors while preserving structural and semantic information.

In our Î±\alpha-CFG, operators are different: (i) symmetric operators, where order is irrelevant, and (ii)asymmetrical (order-sensitive) operators, where order must be preserved.
Tree-LSTM naturally supports both cases through two variants: the N-ary Tree-LSTM, which uses position-sensitive parameters to encode child order, and the Child-Sum Tree-LSTM, which aggregates child states by their mean to provide order-invariant representations.
Based on these, we tailor aggregation strategies: for symmetric binary operators (ğ–¤ğ—‘ğ—‰ğ—‹â†’ğ–¡ğ—‚ğ—‡ğ–ºğ—‹ğ—’ğ–®ğ—‰â€‹(ğ–¤ğ—‘ğ—‰ğ—‹,ğ–¤ğ—‘ğ—‰ğ—‹)\mathsf{Expr}\to\mathsf{BinaryOp}(\mathsf{Expr},\mathsf{Expr})) we adopt Child-Sum to avoid redundant encodings; for paired rolling operators (ğ–¤ğ—‘ğ—‰ğ—‹â†’ğ–¯ğ–ºğ—‚ğ—‹ğ–¾ğ–½ğ–±ğ—ˆğ—…ğ—…ğ—‚ğ—‡ğ—€ğ–®ğ—‰â€‹(ğ–¤ğ—‘ğ—‰ğ—‹,ğ–¤ğ—‘ğ—‰ğ—‹,ğ–­ğ—ğ—†)\mathsf{Expr}\to\mathsf{PairedRollingOp}(\mathsf{Expr},\mathsf{Expr},\mathsf{Num})) we first apply unordered aggregation to operands and then use N-ary encoding to incorporate the time-window parameter; and for all other operators we employ standard N-ary encoding. Such operation can address the problem of isomorphic redundancy of alpha factors defined in [DefinitionÂ 6](https://arxiv.org/html/2601.22119v1#Thmdefinition6 "Definition 6 (Isomorphism of ASR(Tree)). â€£ Appendix G Calculation of Tree Similarity â€£ Alpha Discovery via Grammar-Guided Learning and Search").The resulting tree embeddings are treated as input to be given into the policy and value heads to predict next-rule probabilities and estimated state value.

### F.1 N-ary Tree-LSTM (Position-Sensitive)

Let node jj have NN children with hidden states ğ¡1,â€¦,ğ¡N\mathbf{h}\_{1},\dots,\mathbf{h}\_{N}, input ğ±j\mathbf{x}\_{j}, output hidden state ğ¡j\mathbf{h}\_{j} and cell state ğœj\mathbf{c}\_{j}:

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ¢j\displaystyle\mathbf{i}\_{j} | =Ïƒâ€‹(W(i)â€‹ğ±j+âˆ‘k=1NUk(i)â€‹ğ¡k+ğ›(i))\displaystyle=\sigma\left(W^{(i)}\mathbf{x}\_{j}+\sum\_{k=1}^{N}U\_{k}^{(i)}\mathbf{h}\_{k}+\mathbf{b}^{(i)}\right) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | ğŸjâ€‹k\displaystyle\mathbf{f}\_{jk} | =Ïƒâ€‹(W(f)â€‹ğ±j+Uk(f)â€‹ğ¡k+ğ›(f)),k=1,â€¦,N\displaystyle=\sigma\left(W^{(f)}\mathbf{x}\_{j}+U\_{k}^{(f)}\mathbf{h}\_{k}+\mathbf{b}^{(f)}\right),\qquad k=1,\dots,N |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ¨j\displaystyle\mathbf{o}\_{j} | =Ïƒâ€‹(W(o)â€‹ğ±j+âˆ‘k=1NUk(o)â€‹ğ¡k+ğ›(o))\displaystyle=\sigma\left(W^{(o)}\mathbf{x}\_{j}+\sum\_{k=1}^{N}U\_{k}^{(o)}\mathbf{h}\_{k}+\mathbf{b}^{(o)}\right) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ®j\displaystyle\mathbf{u}\_{j} | =tanhâ¡(W(u)â€‹ğ±j+âˆ‘k=1NUk(u)â€‹ğ¡k+ğ›(u))\displaystyle=\tanh\left(W^{(u)}\mathbf{x}\_{j}+\sum\_{k=1}^{N}U\_{k}^{(u)}\mathbf{h}\_{k}+\mathbf{b}^{(u)}\right) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | ğœj\displaystyle\mathbf{c}\_{j} | =ğ¢jâŠ™ğ®j+âˆ‘k=1NğŸjâ€‹kâŠ™ğœk\displaystyle=\mathbf{i}\_{j}\odot\mathbf{u}\_{j}+\sum\_{k=1}^{N}\mathbf{f}\_{jk}\odot\mathbf{c}\_{k} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ¡j\displaystyle\mathbf{h}\_{j} | =ğ¨jâŠ™tanhâ¡(ğœj)\displaystyle=\mathbf{o}\_{j}\odot\tanh(\mathbf{c}\_{j}) |  |

### F.2 Child-Sum Tree-LSTM

Let node jj have a set of children Câ€‹(j)C(j) with hidden states ğ¡k\mathbf{h}\_{k}, kâˆˆCâ€‹(j)k\in C(j):

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ¡~j\displaystyle\tilde{\mathbf{h}}\_{j} | =1|Câ€‹(j)|â€‹âˆ‘kâˆˆCâ€‹(j)ğ¡k\displaystyle=\frac{1}{|C(j)|}\sum\_{k\in C(j)}\mathbf{h}\_{k} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ¢j\displaystyle\mathbf{i}\_{j} | =Ïƒâ€‹(W(i)â€‹ğ±j+U(i)â€‹ğ¡~j+ğ›(i))\displaystyle=\sigma\left(W^{(i)}\mathbf{x}\_{j}+U^{(i)}\tilde{\mathbf{h}}\_{j}+\mathbf{b}^{(i)}\right) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | ğŸjâ€‹k\displaystyle\mathbf{f}\_{jk} | =Ïƒâ€‹(W(f)â€‹ğ±j+U(f)â€‹ğ¡k+ğ›(f)),kâˆˆCâ€‹(j)\displaystyle=\sigma\left(W^{(f)}\mathbf{x}\_{j}+U^{(f)}\mathbf{h}\_{k}+\mathbf{b}^{(f)}\right),\quad k\in C(j) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ¨j\displaystyle\mathbf{o}\_{j} | =Ïƒâ€‹(W(o)â€‹ğ±j+U(o)â€‹ğ¡~j+ğ›(o))\displaystyle=\sigma\left(W^{(o)}\mathbf{x}\_{j}+U^{(o)}\tilde{\mathbf{h}}\_{j}+\mathbf{b}^{(o)}\right) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ®j\displaystyle\mathbf{u}\_{j} | =tanhâ¡(W(u)â€‹ğ±j+U(u)â€‹ğ¡~j+ğ›(u))\displaystyle=\tanh\left(W^{(u)}\mathbf{x}\_{j}+U^{(u)}\tilde{\mathbf{h}}\_{j}+\mathbf{b}^{(u)}\right) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | ğœj\displaystyle\mathbf{c}\_{j} | =ğ¢jâŠ™ğ®j+âˆ‘kâˆˆCâ€‹(j)ğŸjâ€‹kâŠ™ğœk\displaystyle=\mathbf{i}\_{j}\odot\mathbf{u}\_{j}+\sum\_{k\in C(j)}\mathbf{f}\_{jk}\odot\mathbf{c}\_{k} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ¡j\displaystyle\mathbf{h}\_{j} | =ğ¨jâŠ™tanhâ¡(ğœj)\displaystyle=\mathbf{o}\_{j}\odot\tanh(\mathbf{c}\_{j}) |  |

## Appendix G Calculation of Tree Similarity

###### Definition 6 (Isomorphism of ASR(Tree)).

ASR T1T\_{1} and T2T\_{2} are isomorphic only if:

1. 1.

   The label of root nodes must be the same;
2. 2.

   Recursively check each child node, the labels of the child nodes are equivalent: for asymmetrical operations, the order of the subtrees must be preserved; for symmetrical operations (Binary type operators in [TableÂ 6](https://arxiv.org/html/2601.22119v1#A1.T6 "In Appendix A Tables â€£ Alpha Discovery via Grammar-Guided Learning and Search")) or partially symmetrical operations (Corr, Cov, where the order of the first two operandsâ€™ child nodes doesnâ€™t matter), the order of the subtrees doesnâ€™t matter as long as the operands match;
3. 3.

   Recursively check that all child nodes and their structures are isomorphic.

Given two alpha factor expresions(partial or completed), they correspond to two ASRs T1T\_{1} and T2T\_{2} which are also two trees. Let Subâ€‹(T)\text{Sub}(T) denote the set of all subtrees of TT, where each subtree is induced by a child of node in TT along with all its descendant nodes (including the child node itself). Let Nâ€‹(T)N(T) denote the total number of subtrees in TT, recursively defined as:

|  |  |  |
| --- | --- | --- |
|  | Nâ€‹(T)=1+âˆ‘câˆˆChildrenâ€‹(T)Nâ€‹(c).N(T)=1+\sum\_{c\in\text{Children}(T)}N(c). |  |

The normalized similarity between the two ASR is defined as:

|  |  |  |
| --- | --- | --- |
|  | simâ€‹(T1,T2)=maxt1âˆˆSubâ€‹(T1)t2âˆˆSubâ€‹(T2)â¡cssâ€‹(t1,t2)maxâ¡(Nâ€‹(T1),Nâ€‹(T2)),\mathrm{sim}(T\_{1},T\_{2})=\frac{\max\_{\begin{subarray}{c}t\_{1}\in\text{Sub}(T\_{1})\\ t\_{2}\in\text{Sub}(T\_{2})\end{subarray}}\mathrm{css}(t\_{1},t\_{2})}{\max\left(N(T\_{1}),\;N(T\_{2})\right)}, |  |

where the numerator represents the size of the largest isomorphic subtree shared by T1T\_{1} and T2T\_{2}, i.e., the number of matching nodes in the largest common subtree. Tree isomorphism is defined formally in Â [DefinitionÂ 6](https://arxiv.org/html/2601.22119v1#Thmdefinition6 "Definition 6 (Isomorphism of ASR(Tree)). â€£ Appendix G Calculation of Tree Similarity â€£ Alpha Discovery via Grammar-Guided Learning and Search"). If no such isomorphic subtree exists, then cssâ€‹(t1,t2)=0\mathrm{css}(t\_{1},t\_{2})=0.

The denominator maxâ¡(Nâ€‹(T1),Nâ€‹(T2))\max(N(T\_{1}),N(T\_{2})) corresponds to the number of nodes in the larger of the two trees, serving as an upper bound for the size of any common subtree. Intuitively, it reflects the maximum number of matching nodes that could be achieved if one tree were a subtree of the other, or if the two trees were structurally identical. As such, the denominator defines the *maximum potential scale* of a common subtree, and serves to normalize the matching node count in the numerator. This ensures that the resulting similarity score lies within the standardized range [0,1][0,1], thereby facilitating both quantitative analysis and intuitive comparison of structural similarity between expression trees.

## Appendix H AlphaCFG Framework Parameter Setting for Experiment

### H.1 MCTS Parameters

* â€¢

  Exploration Parameter : The exploration-exploitation trade-off parameter in the UCT formula is set to c=1c=1.
* â€¢

  MCTS Simulations : 64 simulations are performed per state.
* â€¢

  MCTS Parallelism: 8 parallel simulations are used to speed up the exploration.
* â€¢

  Eval Batch Size: 2 evaluations using network are carried out simultaneously each time.
* â€¢

  Branch balance coefficient: 40

### H.2 Network Architecture

Feature Extractor (Tree-LSTM):

* â€¢

  Embedding Dimension: 128.
* â€¢

  Hidden Size: 128.
* â€¢

  Dropout Rate: 0.1.

Policy Network:

* â€¢

  Input: Features extracted by the feature extractor (Tree-LSTM).
* â€¢

  Hidden Layers:

  + â€“

    Layer 1: Fully connected layer with 128 input features and 64 output features.
  + â€“

    Layer 2: Fully connected layer with 64 input features and 128 output features (embedding dimension).
* â€¢

  Activation Function: Softmax

Value Network:

* â€¢

  Input: Features extracted by the feature extractor (Tree-LSTM).
* â€¢

  Hidden Layers:

  + â€“

    Layer 1: Fully connected layer with the embedding dimension (128) as input and 64 output features.
  + â€“

    Layer 2: Fully connected layer with 64 input features and 64 output features.
* â€¢

  Activation Functions: ReLU activation functions applied to the hidden layers.
* â€¢

  Output: A fully connected layer with a single output value without activation function.

### H.3 Optimizer and Training Parameters

* â€¢

  Optimizer: Adam optimizer with default settings
* â€¢

  Learning Rate: A learning rate of 10âˆ’410^{-4}.
* â€¢

  Batch Size: 64.
* â€¢

  Number of factor trajectories in an iteration: 100(2\*50).
* â€¢

  Training Iterations: 100 iterations.
* â€¢

  Batch Size for Training: 64.
* â€¢

  Replay Buffer Size: 20,000.
* â€¢

  Early Stopping Criteria: Early stopping based on validation performance, with a threshold of 20% iterations without improvement.

## Appendix I More Results of Experiment

We evaluate the proposed framework on both the China A-share and U.S. equity markets. Our experiments are designed to: (1) demonstrate that the proposed context-free grammar provides practical advantages over linear generation methods (e.g., Reverse Polish Notation) for representing and generating alpha factors; (2) validate that the syntax representation learning method using Tree-LSTM to encode state outperforms linear network architectures; (3) evaluate the performance of the grammar-aware discovery framework across multiple metrics in comparison with existing factor-mining methods; (4) assess whether the alpha factors discovered by our model deliver superior trading performance in realistic backtesting scenarios; and (5) examine how our model enhances the performance of existing classical factors.

### I.1 Data

For the A-share market, we adopt the constituent stocks of the CSIÂ 300 index, and for the U.S. market, we use the constituent stocks of the S&PÂ 500 index. The dataset is temporally partitioned into three subsets: the training set (2010-01-01 to 2017-12-31), the validation set (2018-01-01 to 2019-12-31), and the testing set (2021-01-01 to 2024-12-31). To avoid distortions caused by abnormal market volatility and structural irregularities during the COVID-19 pandemic, data from calendar year 2020 are excluded by design. Six raw stock-level features are used as model inputs: {open,close,high,low,volume,vwap}\{\mathrm{open},\;\mathrm{close},\;\mathrm{high},\;\mathrm{low},\;\mathrm{volume},\;\mathrm{vwap}\}. Formulaic alpha factors are constructed by applying arithmetic operators to these base features under the grammar constraints described earlier. The prediction target for factors is the 20-day forward return, computed using closing prices for both buying and selling, i.e., Rt(20)=Refâ€‹(close,âˆ’20)closeâˆ’1R\_{t}^{(20)}=\frac{\mathrm{Ref}(\mathrm{close},\,-20)}{\mathrm{close}}-1.

### I.2 Comparison Methods

We evaluate three variants of grammar-constrained factor discovery method: (i) Î±\alpha-Syn (generation constrained solely by syntactic rules) (ii) Î±\alpha-Sem (generation constrained by both syntactic and semantic rules) (iii) Î±\alpha-Sem-kk (generation further restricted by a length-bounding mechanism in [AlgorithmÂ 2](https://arxiv.org/html/2601.22119v1#alg2 "In B.2 Length control of semantic interpretable alpha factor generator â€£ Appendix B Algorithms â€£ Alpha Discovery via Grammar-Guided Learning and Search")). To further validate the grammar effectiveness, we also incorporate Reverse Polish Notation (RPN).
(Specifically for Î±\alpha-Syn, we constrain the rolling window size to be an integer constant in Î±\alpha-Syn to facilitate smooth training.)

For a broader performance assessment of the entire framework, we compare our method against two state-of-the-art factor mining baselines: AlphaGenÂ (Yu et al., [2023](https://arxiv.org/html/2601.22119v1#bib.bib30 "Generating synergistic formulaic alpha collections via reinforcement learning")) and AlphaQCMÂ (Zhu and Zhu, [2025](https://arxiv.org/html/2601.22119v1#bib.bib29 "AlphaQCM: alpha discovery in finance with distributional reinforcement learning")). Both employ RPN, with AlphaGen using Proximal Policy Optimization (PPO) and AlphaQCM using distributed reinforcement learning. Additionally, GPlearn (Zhang et al., [2020](https://arxiv.org/html/2601.22119v1#bib.bib3 "Autoalpha: an efficient hierarchical evolutionary algorithm for mining alpha factors in quantitative investment")) is included as a symbolic-regression baseline, which generates formula trees through genetic programming. All of the above factor generation methods optimize the Information Coefficient (IC) of the linear combination of factors.

To further validate our approach, we include several widely used machine learning models as additional baselines: XGBoostÂ (Wang et al., [2023](https://arxiv.org/html/2601.22119v1#bib.bib42 "An xgboost-based multivariate deep learning framework for stock index futures price forecasting")), LightGBMÂ (Bisdoulis, [2024](https://arxiv.org/html/2601.22119v1#bib.bib41 "Assets forecasting with feature engineering and transformation methods for lightgbm")), LSTMÂ (Bhandari et al., [2022](https://arxiv.org/html/2601.22119v1#bib.bib43 "Predicting stock market index using lstm")), ALSTMÂ (Qin et al., [2017](https://arxiv.org/html/2601.22119v1#bib.bib44 "A dual-stage attention-based recurrent neural network for time series prediction")), TCNÂ (Dai et al., [2022](https://arxiv.org/html/2601.22119v1#bib.bib45 "Price change prediction of ultra high frequency financial data based on temporal convolutional network")), and TransformerÂ (Mozaffari and Zhang, [2024](https://arxiv.org/html/2601.22119v1#bib.bib46 "Predictive modeling of stock prices using transformer model")).The hyperparameters of these models are set according to the benchmark configurations provided by QlibÂ (Yang et al., [2020](https://arxiv.org/html/2601.22119v1#bib.bib23 "Qlib: an ai-oriented quantitative investment platform")). To mitigate the impact of randomness, all models are trained and evaluated 5 times with different fixed random seeds.

### I.3 Evaluation Metrics

We evaluate factor effectiveness from two complementary perspectives: correlation metrics, including IC, RankIC, ICIR, and RankICIR, capture the statistical relationship between factors and future returns.
Backtesting metrics, which are obtained by investment simulation using a top-k/drop-n strategy (see the next paragraph for details ), including MaxDD and Sharpe, assess the profitability and risk characteristics of factors in simulated trading (see [TableÂ 8](https://arxiv.org/html/2601.22119v1#A9.T8 "In I.3 Evaluation Metrics â€£ Appendix I More Results of Experiment â€£ Alpha Discovery via Grammar-Guided Learning and Search") for details).

Top-kk/drop-nn strategy is applied to simulate actual trading operations: for each trading day, we first ranked stocks based on their factor prediction scores, then selected the top kk stocks from the sorted list.
To balance return potential and trading costs, we adopted an equal-weight allocation approach while limiting daily portfolio adjustments to a maximum of n stocks.
In our experiment, we set k=60k=60 and n=5n=5, ensuring sufficient portfolio diversification while controlling transaction costs.

[TableÂ 8](https://arxiv.org/html/2601.22119v1#A9.T8 "In I.3 Evaluation Metrics â€£ Appendix I More Results of Experiment â€£ Alpha Discovery via Grammar-Guided Learning and Search") provides the specific calculation methods for all evaluation metrics.

Table 8: Summary of Evaluation Metrics

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
| Category | Metric Name | Abbrev. | Formula | Description |
| Correlation Metrics | Information Coefficient | IC | IC=Ïâ€‹(Î±i,Ri)\mathrm{IC}=\rho(\alpha\_{i},R\_{i}) | Pearson correlation between factor values Î±i\alpha\_{i} and future returns RiR\_{i}. |
|  | Rank Information Coefficient | RankIC | RankIC=Ïâ€‹(râ€‹(Î±i),râ€‹(Ri))\mathrm{RankIC}=\rho(r(\alpha\_{i}),r(R\_{i})) | Spearman correlation after ranking; râ€‹(â‹…)r(\cdot) is the rank function. |
|  | Information Ratio | ICIR | ICIR=ICÂ¯ÏƒIC\mathrm{ICIR}=\dfrac{\overline{\mathrm{IC}}}{\sigma\_{\mathrm{IC}}} | Ratio of mean IC to its volatility, measuring prediction stability. |
|  | Rank Information Ratio | RankICIR | RankICIR=RankICÂ¯ÏƒRankIC\mathrm{RankICIR}=\dfrac{\overline{\mathrm{RankIC}}}{\sigma\_{\mathrm{RankIC}}} | Ratio of mean RankIC to its volatility, evaluating rank correlation stability. |
| Backtesting Metrics | Maximum Drawdown | MaxDD | MaxDD=maxtâ¡Pmaxâ€‹(0,t)âˆ’PtPmaxâ€‹(0,t)\mathrm{MaxDD}=\max\_{t}\dfrac{P\_{\max}(0,t)-P\_{t}}{P\_{\max}(0,t)} | Largest peak-to-trough decline in backtest; PtP\_{t} is NAV, Pmaxâ€‹(0,t)=maxsâ‰¤tâ¡PsP\_{\max}(0,t)=\max\_{s\leq t}P\_{s}. |
|  | Sharpe Ratio | Sharpe | Sharpe=ğ”¼â€‹[rpâˆ’rf]ÏƒrpÃ—N\mathrm{Sharpe}=\dfrac{\mathbb{E}[r\_{p}-r\_{f}]}{\sigma\_{r\_{p}}}\times\sqrt{N} | Annualized excess return per unit risk; rpr\_{p}: daily return, rfr\_{f}: risk-free rate, NN: 252 (trading days). |

### I.4 Comparison of Different Network Architectures

We conducted comparative experiments under different network architectures (Transformer, LSTM, CNN) while keeping other conditions constant. With a pool size of 10 and max length 5, [FigureÂ 9](https://arxiv.org/html/2601.22119v1#A9.F9 "In I.4 Comparison of Different Network Architectures â€£ Appendix I More Results of Experiment â€£ Alpha Discovery via Grammar-Guided Learning and Search") shows training IC across epochs. Results demonstrate the effectiveness and superiority of syntax representation learning. Tree-LSTM not only extracts the structural and semantic information of expressions but also reduces redundancy caused by isomorphic forms ([DefinitionÂ 6](https://arxiv.org/html/2601.22119v1#Thmdefinition6 "Definition 6 (Isomorphism of ASR(Tree)). â€£ Appendix G Calculation of Tree Similarity â€£ Alpha Discovery via Grammar-Guided Learning and Search")).

![Refer to caption](x5.png)


Figure 9: Comparison of training curves of different network architectures.

### I.5 Optimization of Combined Factor Parameters on the Validation Set

To obtain the optimized combined factor parameters, we conducted experiments on the validation set for two dimensions: Maximum Length of Individual Factors (Max Length) and Factor Pool Size (Pool Size) (results shown in [FigureÂ 10](https://arxiv.org/html/2601.22119v1#A9.F10 "In I.5 Optimization of Combined Factor Parameters on the Validation Set â€£ Appendix I More Results of Experiment â€£ Alpha Discovery via Grammar-Guided Learning and Search")). Specifically, we first fix the maximum length of individual factors and then evaluate the valid IC for different pool sizes {1, 5, 10, 20, 30} to select the optimal pool size. After selecting the optimal pool size under Î±\alpha-Sem-kk, we fix it and then explore different values of the maximum length of individual factors {5, 10, 15, 20, 25} to identify the best configuration.

![Refer to caption](x6.png)


Figure 10:  Valid IC of various generation approaches.

Finally, we obtain the best combined factor parameters:

CSIÂ 300:

* â€¢

  RPN+MCTS: Max Length: 10; Pool Size: 20
* â€¢

  Î±\alpha-Syn: Max Length: 10; Pool Size: 20
* â€¢

  Î±\alpha-Sem: Max Length: 10; Pool Size: 10
* â€¢

  Î±\alpha-Sem-kk: Max Length: 10; Pool Size: 10
* â€¢

  RPN+PPO: Max Length: 20; Pool Size: 20

S&PÂ 500:

* â€¢

  RPN+MCTS: Max Length: 20; Pool Size: 20
* â€¢

  Î±\alpha-Syn: Max Length: 10; Pool Size: 20
* â€¢

  Î±\alpha-Sem: Max Length: 10; Pool Size: 20
* â€¢

  Î±\alpha-Sem-kk: Max Length: 10; Pool Size: 20
* â€¢

  RPN+PPO: Max Length: 20; Pool Size: 20

The optimization objective of the GP method using a combined model has little effect (the generated combined factors are highly similar), so only the single-factor IC is used as its optimization objective.

### I.6 Case Study of the interpretability of formulaic factors

[TableÂ 9](https://arxiv.org/html/2601.22119v1#A9.T9 "In I.6 Case Study of the interpretability of formulaic factors â€£ Appendix I More Results of Experiment â€£ Alpha Discovery via Grammar-Guided Learning and Search") shows an example of alpha factors generated by our framework, tested on the CSI 300 index constituents.
The mined factors exhibit strong interpretability grounded in market microstructure theory. For example, the factor Log(|Std((0.05-volume),40)|) measures the volatility of inverse trading volume over a 40-day window. This factor gauges the temporal variability of illiquidity, which may signal market stress or substantial price impact. Another example, Cov(volume,vwap,40), captures the co-movement between trading volume and the volume-weighted average price in past 40 days. A high covariance indicates strong directional consensus, potentially reflecting persistent momentum or, conversely, price reversals.

Table 9: Top 10 Ranked Alphas and Their Weights

#
Alpha Expression
Weight


1
Mean(Corr(Sum(open,40),(high-volume),20),20)
-0.00889

2
volume
-0.01278

3
Std(close,40)
0.01778

4
Pow(Med(Cov(high,low,30),30),0.1)
0.01411

5
Delta(Log(|Min(high,30)/0.01|),30)
-0.01649

6
Cov((-0.1-Sum(close,40)),volume,20)+low
-0.01649

7
0.01Greater(-0.1/Corr(high,close,30),volume)
-0.00823

8
Log(|Std((0.05-volume),40)|)
0.01224

9
Greater(-0.01,Log(|Log(|low|)|))
-0.04616

10
Cov(volume,vwap,40)
-0.01412