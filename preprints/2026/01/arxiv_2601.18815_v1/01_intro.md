---
authors:
- Juan Pablo Madrigal-Cianci
- Camilo Monsalve Maya
- Lachlan Breakey
doc_id: arxiv:2601.18815v1
family_id: arxiv:2601.18815
is_current: true
taxonomy:
  alpha_families: []
  asset_classes: []
  horizons: []
  themes: []
title: 'Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification,
  Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types'
url_abs: http://arxiv.org/abs/2601.18815v1
url_html: https://arxiv.org/html/2601.18815v1
venue: arXiv q-fin
version: 1
year: 2026
---


Juan P. Madrigal-Cianci 111Corresponding author ğŸ–‚Juan.madrigalcianci@alumni.epfl.ch
Kosmos Ventures, Australia
MC SAS, Colombia

Camilo Monsalve Maya
MC SAS, Colombia

Lachlan Breakey
Kosmos Ventures, Australia

###### Abstract

Prediction markets are often described as mechanisms that â€œaggregate informationâ€ into prices, yet the mapping from dispersed private information to observed market histories is typically noisy, endogenous, and shaped by heterogeneous and strategic participation. This paper formulates prediction markets as Bayesian inverse problems in which the unknown event outcome Yâˆˆ{0,1}Y\in\{0,1\} is inferred from an observed history of market-implied probabilities and traded volumes. We introduce a mechanism-agnostic observation model in log-odds space in which price increments conditional on volume arise from a latent mixture of trader types. The resulting likelihood class encompasses informed and uninformed trading, heavy-tailed microstructure noise, and adversarial or manipulative flow, while requiring only price and volume as observables.

Within this framework we define posterior uncertainty quantification for YY, provide identifiability and well-posedness criteria in terms of Kullbackâ€“Leibler separation between outcome-conditional increment laws, and derive posterior concentration statements and finite-sample error bounds under general regularity assumptions. We further study stability of posterior odds to perturbations of the observed priceâ€“volume path and define realized and expected information gain via the posterior-vs-prior KL divergence and mutual information. The inverse-problem formulation yields explicit diagnostics for regimes in which market histories are informative and stable versus regimes in which inference is ill-posed due to type-composition confounding or outcomeâ€“nuisance symmetries.

Extensive experiments on synthetic data validate our theoretical predictions regarding posterior concentration rates and identifiability thresholds.

### 1 Introduction

Prediction markets are widely deployed as forecasting tools, as information elicitation mechanisms, and as components of decentralized oracle architectures (Arrow et al., [2008](https://arxiv.org/html/2601.18815v1#bib.bib1 "The promise of prediction markets"); Wolfers and Zitzewitz, [2004](https://arxiv.org/html/2601.18815v1#bib.bib2 "Prediction markets")). In many theoretical treatments, the market-implied probability is interpreted as a posterior belief and the market itself is treated as an efficient aggregator of dispersed information (Manski, [2006](https://arxiv.org/html/2601.18815v1#bib.bib3 "Interpreting the predictions of prediction markets"); Gjerstad, [2005](https://arxiv.org/html/2601.18815v1#bib.bib9 "Risk aversion, beliefs, and prediction market equilibrium")). In practice, however, market prices and volumes are generated by heterogeneous participants with varying information quality, risk preferences, budgets, latency, and strategic incentives. The resulting market history is a noisy and endogenous observation of the underlying event outcome. From the standpoint of inference, prediction markets can therefore be viewed as *stochastic sensing systems*, and the central question becomes: what can be reliably inferred about the outcome from the observed market history, and with what quantified uncertainty?

This paper develops a Bayesian inverse-problem formulation of that question in a setting where the analyst observes only a price path (expressed as a market-implied probability) and a volume process. The unknown event outcome is binary, Yâˆˆ{0,1}Y\in\{0,1\}, and the observable history over a finite horizon consists of pairs (pt,vt)(p\_{t},v\_{t}) where ptâˆˆ(0,1)p\_{t}\in(0,1) is a market-implied probability and vtâ‰¥0v\_{t}\geq 0 is the traded volume over an interval or at an event time. The key modeling challenge is that the observation operator mapping YY to (pt,vt)(p\_{t},v\_{t}) is mediated by unobserved agent-level behavior. We treat this mediation explicitly by introducing latent trader *types* whose aggregate effect on price increments is captured by a mixture-of-experts model in log-odds space (Jacobs et al., [1991](https://arxiv.org/html/2601.18815v1#bib.bib34 "Adaptive mixtures of local experts"); Jordan and Jacobs, [1994](https://arxiv.org/html/2601.18815v1#bib.bib35 "Hierarchical mixtures of experts and the EM algorithm")). The resulting likelihood class is deliberately mechanism-agnostic: it is not tied to a particular market microstructure (automated market maker, limit-order book, call auction) and instead describes the conditional distribution of log-odds increments given volume and outcome. This choice permits inference from the minimal information set that is often available in practice, while still allowing principled uncertainty quantification, identifiability analysis, and stability diagnostics.

#### 1.1 Contributions

The inverse-problem perspective yields three primary benefits:

###### Principled uncertainty quantification.

It separates the inferential target YY from the nuisance structure that shapes the observations and provides a coherent Bayesian posterior â„™â€‹(Y=1âˆ£HT)\mathbb{P}(Y=1\mid H\_{T}) with quantified sensitivity to modeling choices. We derive posterior concentration and finite-sample bounds on posterior error probability that decay exponentially in a KL-projection gap characterizing the distinguishability of outcomes after optimizing over nuisance structure (TheoremÂ [4.3](https://arxiv.org/html/2601.18815v1#S4.Thmtheorem3 "Theorem 4.3 (Posterior consistency for the outcome). â€£ 4.4.3 Posterior Consistency â€£ 4.4 Posterior Concentration and Finite-Sample Error Bounds â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types"), PropositionÂ [4.2](https://arxiv.org/html/2601.18815v1#S4.Thmproposition2 "Proposition 4.2 (Finite-sample posterior error bound (robust form)). â€£ 4.4.4 Finite-Sample Error Bounds via Truncation â€£ 4.4 Posterior Concentration and Finite-Sample Error Bounds â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types")).

###### Explicit identifiability and well-posedness criteria.

Inverse problems are ill-posed when distinct latent states induce nearly indistinguishable observation laws or when small perturbations of the observations lead to large changes in the inferred state. By translating these notions into the language of priceâ€“volume histories, we obtain conditions under which the outcome is identifiable and posterior odds are stable (SectionÂ [4.1](https://arxiv.org/html/2601.18815v1#S4.SS1 "4.1 Induced Data Laws and KL Projections â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types")), as well as conditions under which inference is fundamentally ambiguous due to type-composition confounding or outcomeâ€“nuisance symmetries.

###### Information-theoretic metrics.

We define realized information gain as the KL divergence between the posterior and prior on YY, and expected information gain as a mutual-information functional, thereby quantifying the extent to which the market history functions as an informative measurement of the outcome (SectionÂ [4.6](https://arxiv.org/html/2601.18815v1#S4.SS6 "4.6 Information Gain and Market-as-Sensor Metrics â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types")). These metrics enable principled comparison of market designs and participation regimes.

#### 1.2 Paper Organization

To reduce fragmentation without changing scope, we organize the technical core into two main blocks. SectionÂ [2](https://arxiv.org/html/2601.18815v1#S2 "2 Background and Related Work â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types") reviews related work. SectionÂ [3](https://arxiv.org/html/2601.18815v1#S3 "3 Model and Bayesian Formulation â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types") introduces the observation space, the log-odds representation, the latent-type likelihood class, and the Bayesian posterior. SectionÂ [4](https://arxiv.org/html/2601.18815v1#S4 "4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types") develops the inverse-problem analysis: identifiability via KL projections, posterior concentration and finite-sample bounds, stability under perturbations, and information-gain functionals. SectionÂ [5](https://arxiv.org/html/2601.18815v1#S5 "5 Computation and Empirical Validation â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types") presents computational methods and empirical validation on synthetic and real market data. SectionÂ [6](https://arxiv.org/html/2601.18815v1#S6 "6 Discussion â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types") concludes with limitations and extensions.

### 2 Background and Related Work

Our work connects several distinct literatures: prediction market theory and information aggregation, market microstructure models of informed trading, Bayesian inverse problems and posterior consistency, and mixture-of-experts models in machine learning.

#### 2.1 Prediction Markets and Information Aggregation

The theoretical foundations of prediction markets rest on the idea that market prices aggregate dispersed private information (Hayek, [1945](https://arxiv.org/html/2601.18815v1#bib.bib20 "The use of knowledge in society")). Hanson ([2003](https://arxiv.org/html/2601.18815v1#bib.bib4 "Combinatorial information market design"), [2007](https://arxiv.org/html/2601.18815v1#bib.bib5 "Logarithmic market scoring rules for modular combinatorial information aggregation")) introduced the logarithmic market scoring rule (LMSR), which provides bounded loss market making and connects prediction markets to proper scoring rules (Gneiting and Raftery, [2007](https://arxiv.org/html/2601.18815v1#bib.bib41 "Strictly proper scoring rules, prediction, and estimation"); Savage, [1971](https://arxiv.org/html/2601.18815v1#bib.bib42 "Elicitation of personal probabilities and expectations")). Chen and Pennock ([2007](https://arxiv.org/html/2601.18815v1#bib.bib6 "A utility framework for bounded-loss market makers")) analyzed the relationship between market scoring rules and cost functions, establishing equivalences that underpin modern automated market makers.

The question of whether prediction markets actually achieve efficient information aggregation has received substantial attention. Wolfers and Zitzewitz ([2004](https://arxiv.org/html/2601.18815v1#bib.bib2 "Prediction markets")) provided early evidence that prediction market prices track event probabilities well on average, while Manski ([2006](https://arxiv.org/html/2601.18815v1#bib.bib3 "Interpreting the predictions of prediction markets")) cautioned that market prices confound beliefs with risk preferences, questioning the interpretation of prices as probabilities. Ottaviani and SÃ¸rensen ([2015](https://arxiv.org/html/2601.18815v1#bib.bib10 "Price reaction to information with heterogeneous beliefs and wealth effects: underreaction, momentum, and reversal")) showed that even with risk-neutral traders, strategic behavior can prevent full information revelation. Ostrovsky ([2012](https://arxiv.org/html/2601.18815v1#bib.bib7 "Information aggregation in dynamic markets with strategic traders")) identified conditions under which repeated trading leads to information aggregation, while Iyer et al. ([2014](https://arxiv.org/html/2601.18815v1#bib.bib8 "Information aggregation and allocative efficiency in smooth markets")) studied aggregation in combinatorial markets.

Our framework complements this literature by taking an *inferential* rather than *equilibrium* perspective: we ask what can be learned about the outcome from the market history, treating participant behavior as latent structure to be marginalized rather than strategically modeled.

#### 2.2 Market Microstructure and Informed Trading

The market microstructure literature provides structural models of how private information affects prices. The seminal Kyle ([1985](https://arxiv.org/html/2601.18815v1#bib.bib12 "Continuous auctions and insider trading")) model describes a single informed trader, market makers, and noise traders, showing that price impact is linear in order flow and that the informed traderâ€™s private information is gradually incorporated into prices. Glosten and Milgrom ([1985](https://arxiv.org/html/2601.18815v1#bib.bib13 "Bid, ask and transaction prices in a specialist market with heterogeneously informed traders")) introduced a sequential trade model where bid-ask spreads arise from adverse selection against informed traders.

The probability of informed trading (PIN) model of Easley et al. ([1996](https://arxiv.org/html/2601.18815v1#bib.bib14 "Liquidity, information, and infrequently traded stocks"), [2002](https://arxiv.org/html/2601.18815v1#bib.bib15 "Is information risk a determinant of asset returns?")) decomposes order flow into informed and uninformed components, enabling estimation of information asymmetry from trade data. Extensions include time-varying arrival rates (Easley et al., [2008](https://arxiv.org/html/2601.18815v1#bib.bib16 "Time-varying arrival rates of informed and uninformed trades")) and high-frequency adaptations (Easley et al., [2012](https://arxiv.org/html/2601.18815v1#bib.bib17 "Flow toxicity and liquidity in a high-frequency world")). Back et al. ([2000](https://arxiv.org/html/2601.18815v1#bib.bib18 "Imperfect competition among informed traders")) and Back and Baruch ([2004](https://arxiv.org/html/2601.18815v1#bib.bib19 "Information in securities markets: Kyle meets Glosten and Milgrom")) extended Kyleâ€™s framework to multiple informed traders and different information structures.

Our latent-type mixture model can be viewed as a reduced-form representation of these structural models. The key distinction is that we do not require order-level data or specific assumptions about strategic behavior; instead, we model the aggregate effect of different trader populations on price increments conditional on volume.

#### 2.3 Bayesian Inverse Problems

The Bayesian approach to inverse problems treats unknown quantities as random variables and uses observed data to update prior beliefs (Stuart, [2010](https://arxiv.org/html/2601.18815v1#bib.bib21 "Inverse problems: a Bayesian perspective"); Kaipio and Somersalo, [2006](https://arxiv.org/html/2601.18815v1#bib.bib22 "Statistical and computational inverse problems")). Well-posedness in this framework requires that posteriors exist, are unique, and depend continuously on data (Dashti and Stuart, [2017](https://arxiv.org/html/2601.18815v1#bib.bib23 "The Bayesian approach to inverse problems"); Latz, [2020](https://arxiv.org/html/2601.18815v1#bib.bib24 "On the well-posedness of Bayesian inverse problems"); Madrigal-Cianci, [2022](https://arxiv.org/html/2601.18815v1#bib.bib25 "Hierarchical Markov Chain Monte Carlo for Bayesian inverse problems")). Posterior consistency (i.e., convergence of the posterior to the true parameter as data accumulate-) has been extensively studied (Ghosal et al., [2000](https://arxiv.org/html/2601.18815v1#bib.bib26 "Convergence rates of posterior distributions"); Ghosal and Van der Vaart, [2017](https://arxiv.org/html/2601.18815v1#bib.bib27 "Fundamentals of nonparametric Bayesian inference"); Van der Vaart and Van Zanten, [2008](https://arxiv.org/html/2601.18815v1#bib.bib28 "Rates of contraction of posterior distributions based on Gaussian process priors")).

For parametric models, the classical Bernsteinâ€“von Mises theorem establishes that posteriors concentrate around the maximum likelihood estimator at rate nâˆ’1/2n^{-1/2} and become asymptotically normal (Van der Vaart, [2000](https://arxiv.org/html/2601.18815v1#bib.bib29 "Asymptotic statistics")). For model selection and hypothesis testing, Bayes factors provide a coherent framework for comparing models with different nuisance structures (Kass and Raftery, [1995](https://arxiv.org/html/2601.18815v1#bib.bib30 "Bayes factors")). Walker ([2004](https://arxiv.org/html/2601.18815v1#bib.bib31 "New approaches to Bayesian consistency")) developed general posterior convergence rate theory based on testing conditions, which we adapt to our setting.

Our contribution is to instantiate this general framework for the specific structure of prediction market inference, where the â€œinverse problemâ€ is recovering a binary outcome from priceâ€“volume histories mediated by latent trader types.

#### 2.4 Mixture Models and Mixture of Experts

Mixture models provide a flexible framework for modeling heterogeneous populations (McLachlan and Peel, [2000](https://arxiv.org/html/2601.18815v1#bib.bib32 "Finite mixture models"); FrÃ¼hwirth-Schnatter, [2006](https://arxiv.org/html/2601.18815v1#bib.bib33 "Finite mixture and Markov switching models")). The mixture-of-experts architecture (Jacobs et al., [1991](https://arxiv.org/html/2601.18815v1#bib.bib34 "Adaptive mixtures of local experts"); Jordan and Jacobs, [1994](https://arxiv.org/html/2601.18815v1#bib.bib35 "Hierarchical mixtures of experts and the EM algorithm")) extends this by allowing mixture weights to depend on input features through a gating network. This architecture has seen renewed interest in large-scale machine learning (Shazeer et al., [2017](https://arxiv.org/html/2601.18815v1#bib.bib36 "Outrageously large neural networks: the sparsely-gated mixture-of-experts layer"); Fedus et al., [2022](https://arxiv.org/html/2601.18815v1#bib.bib37 "Switch transformers: scaling to trillion parameter models with simple and efficient sparsity")).

Identifiability of finite mixtures is a classical concern (Teicher, [1963](https://arxiv.org/html/2601.18815v1#bib.bib38 "Identifiability of finite mixtures"); Yakowitz and Spragins, [1968](https://arxiv.org/html/2601.18815v1#bib.bib39 "On the identifiability of finite mixtures")). Location-scale mixtures are generally identifiable under mild conditions (Holzmann et al., [2006](https://arxiv.org/html/2601.18815v1#bib.bib40 "Identifiability of finite mixtures of elliptical distributions")), but identifiability can fail when components overlap substantially or when the number of components is misspecified. In our setting, identifiability of the *outcome* YY is primary, with mixture components serving as nuisance structure; this shifts the identifiability analysis from component recovery to outcome distinguishability.

#### 2.5 Learning from Strategic Data

A growing literature in machine learning addresses learning from data generated by strategic agents (Hardt et al., [2016](https://arxiv.org/html/2601.18815v1#bib.bib45 "Strategic classification"); Perdomo et al., [2020](https://arxiv.org/html/2601.18815v1#bib.bib46 "Performative prediction")). In online learning, adversarial bandits and experts settings study learning when data may be adversarially generated (Cesa-Bianchi and Lugosi, [2006](https://arxiv.org/html/2601.18815v1#bib.bib47 "Prediction, learning, and games"); Bubeck and Cesa-Bianchi, [2012](https://arxiv.org/html/2601.18815v1#bib.bib48 "Regret analysis of stochastic and nonstochastic multi-armed bandit problems")). Mechanism design approaches seek to incentivize truthful reporting (Chen et al., [2005](https://arxiv.org/html/2601.18815v1#bib.bib43 "Information markets vs. opinion pools: an empirical comparison"); Lambert et al., [2008](https://arxiv.org/html/2601.18815v1#bib.bib44 "Self-financed wagering mechanisms for forecasting")).

Our framework relates to this literature through the latent-type structure: adversarial or manipulative traders correspond to types whose increment distributions do not depend on the true outcome (or actively oppose it). The KL-projection gap condition for identifiability can be interpreted as requiring that informative types have sufficient weight to overcome noise and manipulation.

### 3 Model and Bayesian Formulation

#### 3.1 Outcome and Observed History

Fix a time horizon Tâˆˆâ„•T\in\mathbb{N}. The unknown event outcome is a Bernoulli random variable

|  |  |  |
| --- | --- | --- |
|  | Yâˆˆğ’´:={0,1},â„™â€‹(Y=1)=Ï€0âˆˆ(0,1).Y\in\mathcal{Y}:=\{0,1\},\qquad\mathbb{P}(Y=1)=\pi\_{0}\in(0,1). |  |

The analyst observes a priceâ€“volume history

|  |  |  |
| --- | --- | --- |
|  | HT:=((Pt,Vt))t=0T,H\_{T}:=\bigl((P\_{t},V\_{t})\bigr)\_{t=0}^{T}, |  |

where Ptâˆˆ(0,1)P\_{t}\in(0,1) is a market-implied probability at time tt and Vtâˆˆ[0,âˆ)V\_{t}\in[0,\infty) is the traded volume in the interval (tâˆ’1,t](t-1,t] for tâ‰¥1t\geq 1. We include P0âˆˆ(0,1)P\_{0}\in(0,1) as an initial market state and do not define V0V\_{0}. The associated measurable spaces are

|  |  |  |
| --- | --- | --- |
|  | ğ’«:=(0,1)T+1,ğ’±:=[0,âˆ)T,â„‹:=ğ’«Ã—ğ’±,\mathcal{P}:=(0,1)^{T+1},\qquad\mathcal{V}:=[0,\infty)^{T},\qquad\mathcal{H}:=\mathcal{P}\times\mathcal{V}, |  |

each endowed with the product Borel Ïƒ\sigma-algebra. We write an element of â„‹\mathcal{H} as h=(p0:T,v1:T)h=(p\_{0:T},v\_{1:T}).

###### Remark 3.1 (Volume as design).

The present paper takes the volume sequence v1:Tv\_{1:T} as part of the observation, but does not require an explicit generative model for it. In many empirical settings, modeling volume is valuable, but treating v1:Tv\_{1:T} as a realized design sequence yields an analytically clean starting point and allows one to interpret volume as an endogenous â€œmeasurement intensityâ€ chosen by participants. When needed, a generative model for V1:TV\_{1:T} can be added without changing the inferential target; see SectionÂ [6](https://arxiv.org/html/2601.18815v1#S6 "6 Discussion â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types") for extensions.

#### 3.2 Log-Odds Coordinates

Define the logit map logit:(0,1)â†’â„\mathrm{logit}:(0,1)\to\mathbb{R} by

|  |  |  |
| --- | --- | --- |
|  | logitâ€‹(p):=logâ¡p1âˆ’p,\mathrm{logit}(p):=\log\frac{p}{1-p}, |  |

and its inverse, the logistic map Ïƒ:â„â†’(0,1)\sigma:\mathbb{R}\to(0,1), by

|  |  |  |
| --- | --- | --- |
|  | Ïƒâ€‹(x):=11+eâˆ’x.\sigma(x):=\frac{1}{1+e^{-x}}. |  |

We define the log-odds process Xt:=logitâ€‹(Pt)X\_{t}:=\mathrm{logit}(P\_{t}) and its increments Î”â€‹Xt:=Xtâˆ’Xtâˆ’1\Delta X\_{t}:=X\_{t}-X\_{t-1} for tâ‰¥1t\geq 1. Given an observed history h=(p0:T,v1:T)h=(p\_{0:T},v\_{1:T}), the corresponding log-odds history is x0:T=logitâ€‹(p0:T)x\_{0:T}=\mathrm{logit}(p\_{0:T}) and increments Î”â€‹x1:T\Delta x\_{1:T}.

Working in log-odds space is natural for several reasons:

1. 1.

   Additivity of evidence: Under conditional independence, log-likelihood ratios (and hence log-odds updates) are additive, making the log-odds representation natural for sequential Bayesian updating.
2. 2.

   Unbounded support: The log-odds transformation maps (0,1)(0,1) to â„\mathbb{R}, avoiding boundary behavior near 0 and 11 and permitting Gaussian and other unbounded noise models.
3. 3.

   Microstructure compatibility: Many market microstructure models describe price changes as approximately additive in log-odds (or log-prices) under small moves (Kyle, [1985](https://arxiv.org/html/2601.18815v1#bib.bib12 "Continuous auctions and insider trading"); Back et al., [2000](https://arxiv.org/html/2601.18815v1#bib.bib18 "Imperfect competition among informed traders")).

#### 3.3 Latent Types as Nuisance Structure

We introduce a finite set of latent types {1,â€¦,K}\{1,\dots,K\}, where Kâˆˆâ„•K\in\mathbb{N} is fixed. Types are not required to correspond to individual agents; rather, they represent latent behavioral regimes whose aggregate effect on price increments can be described probabilistically. This is a standard abstraction in mixture modeling (McLachlan and Peel, [2000](https://arxiv.org/html/2601.18815v1#bib.bib32 "Finite mixture models")) and is particularly appropriate when the analyst observes only aggregated market outcomes rather than identity-level order flow.

The nuisance parameter ğš¯\bm{\Theta} collects the quantities that govern type prevalence, type activation as a function of volume, and type-specific increment laws. We define:

1. 1.

   A vector of base mixture weights ğ=(Ï‰1,â€¦,Ï‰K)\bm{\omega}=(\omega\_{1},\dots,\omega\_{K}) in the simplex

   |  |  |  |
   | --- | --- | --- |
   |  | Î”Kâˆ’1:={ğâˆˆ[0,1]K:âˆ‘k=1KÏ‰k=1}.\Delta^{K-1}:=\Bigl\{\bm{\omega}\in[0,1]^{K}:\sum\_{k=1}^{K}\omega\_{k}=1\Bigr\}. |  |
2. 2.

   Gating parameters ğœ¸âˆˆÎ“\bm{\gamma}\in\Gamma, where Î“âŠ†â„dÎ³\Gamma\subseteq\mathbb{R}^{d\_{\gamma}} is a compact parameter space, used to define volume-dependent participation weights.
3. 3.

   Type-specific parameters ğœ½kâˆˆÎ˜k\bm{\theta}\_{k}\in\Theta\_{k}, where each Î˜kâŠ†â„dk\Theta\_{k}\subseteq\mathbb{R}^{d\_{k}} is a compact parameter space.

We write ğš¯:=(ğ,ğœ¸,ğœ½1:K)\bm{\Theta}:=(\bm{\omega},\bm{\gamma},\bm{\theta}\_{1:K}) and Î˜:=Î”Kâˆ’1Ã—Î“Ã—Î˜1Ã—â‹¯Ã—Î˜K\Theta:=\Delta^{K-1}\times\Gamma\times\Theta\_{1}\times\cdots\times\Theta\_{K} for the global parameter space.

#### 3.4 Volume-Dependent Gating

Let Ïk:[0,âˆ)Ã—Î”Kâˆ’1Ã—Î“â†’[0,1]\rho\_{k}:[0,\infty)\times\Delta^{K-1}\times\Gamma\to[0,1] be measurable functions satisfying, for all vâ‰¥0v\geq 0,

|  |  |  |
| --- | --- | --- |
|  | âˆ‘k=1KÏkâ€‹(v;ğ,ğœ¸)=1.\sum\_{k=1}^{K}\rho\_{k}(v;\bm{\omega},\bm{\gamma})=1. |  |

We interpret Ïkâ€‹(v;ğ,ğœ¸)\rho\_{k}(v;\bm{\omega},\bm{\gamma}) as the probability that, at volume level vv, the effective price increment is generated by type kk. This formulation allows the active composition of market participants to change with market activity while maintaining a parsimonious parameterization through (ğ,ğœ¸)(\bm{\omega},\bm{\gamma}).

A canonical choice is a softmax gate:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ïkâ€‹(v;ğ,ğœ¸)=Ï‰kâ€‹expâ¡(akâ€‹(v;ğœ¸))âˆ‘j=1KÏ‰jâ€‹expâ¡(ajâ€‹(v;ğœ¸)),\rho\_{k}(v;\bm{\omega},\bm{\gamma})=\frac{\omega\_{k}\exp(a\_{k}(v;\bm{\gamma}))}{\sum\_{j=1}^{K}\omega\_{j}\exp(a\_{j}(v;\bm{\gamma}))}, |  | (1) |

where ak:[0,âˆ)Ã—Î“â†’â„a\_{k}:[0,\infty)\times\Gamma\to\mathbb{R} is a gating *logit* function (renamed to avoid collision with the noise density gkg\_{k} below). Natural choices include:

* â€¢

  Volume-linear: akâ€‹(v;ğœ¸)=Î³k,0+Î³k,1â€‹va\_{k}(v;\bm{\gamma})=\gamma\_{k,0}+\gamma\_{k,1}v
* â€¢

  Log-volume: akâ€‹(v;ğœ¸)=Î³k,0+Î³k,1â€‹logâ¡(1+v)a\_{k}(v;\bm{\gamma})=\gamma\_{k,0}+\gamma\_{k,1}\log(1+v)
* â€¢

  Threshold: akâ€‹(v;ğœ¸)=Î³k,0+Î³k,1â€‹ğŸ™â€‹{v>Ï„k}a\_{k}(v;\bm{\gamma})=\gamma\_{k,0}+\gamma\_{k,1}\mathbbm{1}\{v>\tau\_{k}\}

These parameterizations capture the empirical regularity that informed traders may be more active during high-volume periods (Easley et al., [1996](https://arxiv.org/html/2601.18815v1#bib.bib14 "Liquidity, information, and infrequently traded stocks")), while noise traders may dominate low-volume periods.

#### 3.5 Type-Specific Increment Laws

For each type kâˆˆ{1,â€¦,K}k\in\{1,\dots,K\} and outcome yâˆˆ{0,1}y\in\{0,1\}, we define a conditional density fk,y(â‹…âˆ£v,ğœ½k)f\_{k,y}(\cdot\mid v,\bm{\theta}\_{k}) on â„\mathbb{R} with respect to Lebesgue measure. We assume a locationâ€“scale representation

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î”â€‹X=mk,yâ€‹(v;ğœ½k)+skâ€‹(v;ğœ½k)â€‹Îµ,Îµâˆ¼Gk,\Delta X=m\_{k,y}(v;\bm{\theta}\_{k})+s\_{k}(v;\bm{\theta}\_{k})\,\varepsilon,\qquad\varepsilon\sim G\_{k}, |  | (2) |

where mk,y:[0,âˆ)Ã—Î˜kâ†’â„m\_{k,y}:[0,\infty)\times\Theta\_{k}\to\mathbb{R} is a measurable location function, sk:[0,âˆ)Ã—Î˜kâ†’(0,âˆ)s\_{k}:[0,\infty)\times\Theta\_{k}\to(0,\infty) is a measurable scale function, and GkG\_{k} is a base noise distribution on â„\mathbb{R} with density gkg\_{k}. Under ([2](https://arxiv.org/html/2601.18815v1#S3.E2 "In 3.5 Type-Specific Increment Laws â€£ 3 Model and Bayesian Formulation â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types")),

|  |  |  |
| --- | --- | --- |
|  | fk,yâ€‹(Î´â€‹xâˆ£v,ğœ½k)=1skâ€‹(v;ğœ½k)â€‹gkâ€‹(Î´â€‹xâˆ’mk,yâ€‹(v;ğœ½k)skâ€‹(v;ğœ½k)).f\_{k,y}(\delta x\mid v,\bm{\theta}\_{k})=\frac{1}{s\_{k}(v;\bm{\theta}\_{k})}\,g\_{k}\!\left(\frac{\delta x-m\_{k,y}(v;\bm{\theta}\_{k})}{s\_{k}(v;\bm{\theta}\_{k})}\right). |  |

The dependence on yy is absorbed into mk,ym\_{k,y}, which is the reduced-form object encoding how truth influences directional drift in log-odds increments conditional on volume.

###### Definition 3.1 (Trader type taxonomy).

Given type parameters ğ›‰k\bm{\theta}\_{k}, we classify type kk as:

1. 1.

   Informed if mk,1â€‹(v;ğœ½k)>mk,0â€‹(v;ğœ½k)m\_{k,1}(v;\bm{\theta}\_{k})>m\_{k,0}(v;\bm{\theta}\_{k}) on a relevant volume range (prices drift toward truth).
2. 2.

   Uninformed/Noise if mk,1â€‹(v;ğœ½k)=mk,0â€‹(v;ğœ½k)m\_{k,1}(v;\bm{\theta}\_{k})=m\_{k,0}(v;\bm{\theta}\_{k}) (increments are outcome-independent).
3. 3.

   Adversarial/Manipulative if mk,1â€‹(v;ğœ½k)<mk,0â€‹(v;ğœ½k)m\_{k,1}(v;\bm{\theta}\_{k})<m\_{k,0}(v;\bm{\theta}\_{k}) (prices drift away from truth).

#### 3.6 Mixture Likelihood and Conditional Independence

Given the nuisance parameter ğš¯\bm{\Theta}, the outcome Y=yY=y, and a realized volume Vt=vV\_{t}=v, we model the increment Î”â€‹Xt\Delta X\_{t} as having density

|  |  |  |  |
| --- | --- | --- | --- |
|  | fyâ€‹(Î´â€‹xâˆ£v,ğš¯):=âˆ‘k=1KÏkâ€‹(v;ğ,ğœ¸)â€‹fk,yâ€‹(Î´â€‹xâˆ£v,ğœ½k).f\_{y}(\delta x\mid v,\bm{\Theta}):=\sum\_{k=1}^{K}\rho\_{k}(v;\bm{\omega},\bm{\gamma})\,f\_{k,y}(\delta x\mid v,\bm{\theta}\_{k}). |  | (3) |

Let Î”â€‹X1:T=(Î”â€‹X1,â€¦,Î”â€‹XT)\Delta X\_{1:T}=(\Delta X\_{1},\dots,\Delta X\_{T}) and V1:T=(V1,â€¦,VT)V\_{1:T}=(V\_{1},\dots,V\_{T}). The central simplifying assumption is conditional independence.

###### Assumption 3.1 (Conditional independence of increments).

Conditional on YY, ğš¯\bm{\Theta}, and V1:TV\_{1:T}, the increments Î”â€‹Xt\Delta X\_{t} are independent with the time-inhomogeneous product density:

|  |  |  |  |
| --- | --- | --- | --- |
|  | pâ€‹(Î”â€‹x1:Tâˆ£v1:T,Y=y,ğš¯)=âˆt=1Tfyâ€‹(Î”â€‹xtâˆ£vt,ğš¯).p(\Delta x\_{1:T}\mid v\_{1:T},Y=y,\bm{\Theta})=\prod\_{t=1}^{T}f\_{y}(\Delta x\_{t}\mid v\_{t},\bm{\Theta}). |  | (4) |

This assumption can be relaxed to allow Markovian dependence or self-exciting volatility; see SectionÂ [6](https://arxiv.org/html/2601.18815v1#S6 "6 Discussion â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types") for extensions.

#### 3.7 A Concrete Instantiation: Gaussian Latent-Type Model

To ground the abstract framework, we present a fully specified model that satisfies our assumptions and admits efficient computation. We explicitly impose an *orientation constraint* on drift magnitudes to eliminate the otherwise unavoidable outcomeâ€“nuisance sign symmetry.

###### Example 3.1 (Gaussian latent-type model).

Consider K=3K=3 types representing informed traders, noise traders, and manipulators. Let Î¼1,Î¼3â‰¥0\mu\_{1},\mu\_{3}\geq 0 be drift magnitudes (orientation constraint) and let Î»1,Îº1,Ïƒ1,Ïƒ2,Ïƒ3>0\lambda\_{1},\kappa\_{1},\sigma\_{1},\sigma\_{2},\sigma\_{3}>0, Ï„3â‰¥0\tau\_{3}\geq 0.

Type 1 (Informed):

|  |  |  |  |
| --- | --- | --- | --- |
|  | m1,yâ€‹(v;ğœ½1)\displaystyle m\_{1,y}(v;\bm{\theta}\_{1}) | =Î¼1â€‹(2â€‹yâˆ’1)â‹…(1âˆ’eâˆ’Î»1â€‹v),\displaystyle=\mu\_{1}(2y-1)\cdot(1-e^{-\lambda\_{1}v}), |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | s1â€‹(v;ğœ½1)\displaystyle s\_{1}(v;\bm{\theta}\_{1}) | =Ïƒ1/1+Îº1â€‹v,\displaystyle=\sigma\_{1}/\sqrt{1+\kappa\_{1}v}, |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | G1\displaystyle G\_{1} | =ğ’©â€‹(0,1).\displaystyle=\mathcal{N}(0,1). |  |

The drift Î¼1â€‹(2â€‹yâˆ’1)\mu\_{1}(2y-1) pushes prices toward the truth (y=1y=1 implies positive drift), with magnitude increasing in volume at rate Î»1\lambda\_{1}. Scale decreases with volume, reflecting improved price efficiency.

Type 2 (Noise):

|  |  |  |  |
| --- | --- | --- | --- |
|  | m2,yâ€‹(v;ğœ½2)\displaystyle m\_{2,y}(v;\bm{\theta}\_{2}) | =0(outcome-independent),\displaystyle=0\quad\text{(outcome-independent)}, |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | s2â€‹(v;ğœ½2)\displaystyle s\_{2}(v;\bm{\theta}\_{2}) | =Ïƒ2,\displaystyle=\sigma\_{2}, |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | G2\displaystyle G\_{2} | =ğ’©â€‹(0,1).\displaystyle=\mathcal{N}(0,1). |  |

Type 3 (Manipulator):

|  |  |  |  |
| --- | --- | --- | --- |
|  | m3,yâ€‹(v;ğœ½3)\displaystyle m\_{3,y}(v;\bm{\theta}\_{3}) | =âˆ’Î¼3â€‹(2â€‹yâˆ’1)â‹…ğŸ™â€‹{v>Ï„3},\displaystyle=-\mu\_{3}(2y-1)\cdot\mathbbm{1}\{v>\tau\_{3}\}, |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | s3â€‹(v;ğœ½3)\displaystyle s\_{3}(v;\bm{\theta}\_{3}) | =Ïƒ3,\displaystyle=\sigma\_{3}, |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | G3\displaystyle G\_{3} | =tÎ½(Student-â€‹tâ€‹Â withÂ â€‹Î½â€‹Â degrees of freedom).\displaystyle=t\_{\nu}\quad\text{(Student-}t\text{ with }\nu\text{ degrees of freedom)}. |  |

Gating: Use the general gating family Ïk\rho\_{k}. A convenient instantiation is the softmax gate ([1](https://arxiv.org/html/2601.18815v1#S3.E1 "In 3.4 Volume-Dependent Gating â€£ 3 Model and Bayesian Formulation â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types")) with log-volume logits

|  |  |  |
| --- | --- | --- |
|  | akâ€‹(v;ğœ¸)=Î³k,0+Î³k,1â€‹logâ¡(1+v),a\_{k}(v;\bm{\gamma})=\gamma\_{k,0}+\gamma\_{k,1}\log(1+v), |  |

optionally combined with hard participation constraints such as Ï3â€‹(v;ğ›š,ğ›„)=0\rho\_{3}(v;\bm{\omega},\bm{\gamma})=0 for vâ‰¤Ï„3v\leq\tau\_{3} when one wants a strict â€œactivationâ€ regime for manipulation (useful for theoretical separation arguments; empirically, this can be approximated by very negative logits below Ï„3\tau\_{3}).

The full parameter vector is:

|  |  |  |
| --- | --- | --- |
|  | ğœ½=(ğ,ğœ¸,Î¼1,Î»1,Ïƒ1,Îº1,Ïƒ2,Î¼3,Ï„3,Ïƒ3,Î½).\bm{\theta}=(\bm{\omega},\bm{\gamma},\mu\_{1},\lambda\_{1},\sigma\_{1},\kappa\_{1},\sigma\_{2},\mu\_{3},\tau\_{3},\sigma\_{3},\nu). |  |

###### Remark 3.2 (Orientation and outcomeâ€“nuisance symmetries).

Without sign restrictions (e.g. allowing Î¼1<0\mu\_{1}<0), the mapping (y,Î¼1)â†¦(1âˆ’y,âˆ’Î¼1)(y,\mu\_{1})\mapsto(1-y,-\mu\_{1}) can render outcome inference non-identifiable, since the conditional increment law may be reproduced under the wrong outcome by a sign flip in nuisance parameters. We therefore treat the orientation constraint Î¼1,Î¼3â‰¥0\mu\_{1},\mu\_{3}\geq 0 as part of the model specification.

#### 3.8 Prior and Posterior

We place a prior Î \Pi on ğš¯\bm{\Theta}. A convenient factorization is

|  |  |  |
| --- | --- | --- |
|  | ğâˆ¼Dirichletâ€‹(Î±1,â€¦,Î±K),ğœ¸âˆ¼Î Î“,ğœ½kâˆ¼Î kâ€‹Â independently forÂ â€‹k=1,â€¦,K.\bm{\omega}\sim\mathrm{Dirichlet}(\alpha\_{1},\dots,\alpha\_{K}),\qquad\bm{\gamma}\sim\Pi\_{\Gamma},\qquad\bm{\theta}\_{k}\sim\Pi\_{k}\ \text{ independently for }k=1,\dots,K. |  |

Given an observed history h=(p0:T,v1:T)h=(p\_{0:T},v\_{1:T}), let Î”â€‹x1:T\Delta x\_{1:T} be the implied log-odds increments. The joint posterior on (Y,ğš¯)(Y,\bm{\Theta}) is

|  |  |  |
| --- | --- | --- |
|  | â„™â€‹(Y=y,ğš¯âˆˆdâ€‹Î¸âˆ£h)âˆÏ€0yâ€‹(1âˆ’Ï€0)1âˆ’yâ€‹pâ€‹(Î”â€‹x1:Tâˆ£v1:T,Y=y,Î¸)â€‹Î â€‹(dâ€‹Î¸).\mathbb{P}(Y=y,\bm{\Theta}\in d\theta\mid h)\propto\pi\_{0}^{y}(1-\pi\_{0})^{1-y}\,p(\Delta x\_{1:T}\mid v\_{1:T},Y=y,\theta)\,\Pi(d\theta). |  |

The marginal posterior on the outcome is

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„™â€‹(Y=1âˆ£h)=Ï€0â€‹m1â€‹(h)Ï€0â€‹m1â€‹(h)+(1âˆ’Ï€0)â€‹m0â€‹(h),\mathbb{P}(Y=1\mid h)=\frac{\pi\_{0}\,m\_{1}(h)}{\pi\_{0}\,m\_{1}(h)+(1-\pi\_{0})\,m\_{0}(h)}, |  | (5) |

where the marginal likelihood under outcome yy is

|  |  |  |  |
| --- | --- | --- | --- |
|  | myâ€‹(h):=âˆ«Î˜pâ€‹(Î”â€‹x1:Tâˆ£v1:T,Y=y,Î¸)â€‹Î â€‹(dâ€‹Î¸).m\_{y}(h):=\int\_{\Theta}p(\Delta x\_{1:T}\mid v\_{1:T},Y=y,\theta)\,\Pi(d\theta). |  | (6) |

Define the Bayes factor

|  |  |  |
| --- | --- | --- |
|  | BFTâ€‹(h):=m1â€‹(h)m0â€‹(h).\mathrm{BF}\_{T}(h):=\frac{m\_{1}(h)}{m\_{0}(h)}. |  |

Then the posterior odds admit the closed form

|  |  |  |  |
| --- | --- | --- | --- |
|  | logâ¡â„™â€‹(Y=1âˆ£h)â„™â€‹(Y=0âˆ£h)=logâ¡Ï€01âˆ’Ï€0+logâ¡BFTâ€‹(h).\log\frac{\mathbb{P}(Y=1\mid h)}{\mathbb{P}(Y=0\mid h)}=\log\frac{\pi\_{0}}{1-\pi\_{0}}+\log\mathrm{BF}\_{T}(h). |  | (7) |

EquationÂ ([7](https://arxiv.org/html/2601.18815v1#S3.E7 "In 3.8 Prior and Posterior â€£ 3 Model and Bayesian Formulation â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types")) makes explicit that inference on YY reduces to understanding the marginal Bayes factor induced by the priceâ€“volume history after integrating out latent type structure.

### 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain

#### 4.1 Induced Data Laws and KL Projections

Fix a deterministic volume design v1:Tâˆˆğ’±v\_{1:T}\in\mathcal{V}. For each outcome yâˆˆ{0,1}y\in\{0,1\} and nuisance parameter Î¸âˆˆÎ˜\theta\in\Theta, let Py,Î¸(T)P\_{y,\theta}^{(T)} denote the probability measure on (â„T,â„¬â€‹(â„T))(\mathbb{R}^{T},\mathcal{B}(\mathbb{R}^{T})) with density

|  |  |  |
| --- | --- | --- |
|  | Î´â€‹x1:Tâ†¦âˆt=1Tfyâ€‹(Î´â€‹xtâˆ£vt,Î¸).\delta x\_{1:T}\mapsto\prod\_{t=1}^{T}f\_{y}(\delta x\_{t}\mid v\_{t},\theta). |  |

We interpret Py,Î¸(T)P\_{y,\theta}^{(T)} as the induced law of log-odds increments conditional on the realized volume sequence.

For Î¸,Î¸â€²âˆˆÎ˜\theta,\theta^{\prime}\in\Theta and y,yâ€²âˆˆ{0,1}y,y^{\prime}\in\{0,1\}, define the finite-horizon KL divergence

|  |  |  |
| --- | --- | --- |
|  | DKL(Py,Î¸(T)âˆ¥Pyâ€²,Î¸â€²(T)):=ğ”¼Py,Î¸(T)[logpy,Î¸(T)â€‹(Î”â€‹X1:T)pyâ€²,Î¸â€²(T)â€‹(Î”â€‹X1:T)],\mathrm{D\_{KL}}\!\left(P\_{y,\theta}^{(T)}\,\middle\|\,P\_{y^{\prime},\theta^{\prime}}^{(T)}\right):=\mathbb{E}\_{P\_{y,\theta}^{(T)}}\!\left[\log\frac{p\_{y,\theta}^{(T)}(\Delta X\_{1:T})}{p\_{y^{\prime},\theta^{\prime}}^{(T)}(\Delta X\_{1:T})}\right], |  |

where py,Î¸(T)p\_{y,\theta}^{(T)} denotes the corresponding density. Under AssumptionÂ [3.1](https://arxiv.org/html/2601.18815v1#S3.Thmassumption1 "Assumption 3.1 (Conditional independence of increments). â€£ 3.6 Mixture Likelihood and Conditional Independence â€£ 3 Model and Bayesian Formulation â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types"), this divergence decomposes as

|  |  |  |  |
| --- | --- | --- | --- |
|  | DKL(Py,Î¸(T)âˆ¥Pyâ€²,Î¸â€²(T))=âˆ‘t=1TDKL(fy(â‹…âˆ£vt,Î¸)âˆ¥fyâ€²(â‹…âˆ£vt,Î¸â€²)).\mathrm{D\_{KL}}\!\left(P\_{y,\theta}^{(T)}\,\middle\|\,P\_{y^{\prime},\theta^{\prime}}^{(T)}\right)=\sum\_{t=1}^{T}\mathrm{D\_{KL}}\!\left(f\_{y}(\cdot\mid v\_{t},\theta)\,\middle\|\,f\_{y^{\prime}}(\cdot\mid v\_{t},\theta^{\prime})\right). |  | (8) |

Because the nuisance parameter is unknown, the relevant separation notion is a *projection gap* between the outcome-indexed families after optimizing over nuisance structure.

###### Definition 4.1 (KL projection gap).

Fix (yâ‹†,Î¸â‹†)(y^{\star},\theta^{\star}) and v1:Tv\_{1:T}. For yâˆˆ{0,1}y\in\{0,1\}, define the (normalized) KL projection

|  |  |  |  |
| --- | --- | --- | --- |
|  | KT(yâ‹†,Î¸â‹†â†’y):=infÎ¸âˆˆÎ˜1TDKL(Pyâ‹†,Î¸â‹†(T)âˆ¥Py,Î¸(T)).K\_{T}(y^{\star},\theta^{\star}\to y):=\inf\_{\theta\in\Theta}\frac{1}{T}\mathrm{D\_{KL}}\!\left(P\_{y^{\star},\theta^{\star}}^{(T)}\,\middle\|\,P\_{y,\theta}^{(T)}\right). |  | (9) |

The *outcome-separation gap* is

|  |  |  |
| --- | --- | --- |
|  | Î´Tâ€‹(yâ‹†,Î¸â‹†):=KTâ€‹(yâ‹†,Î¸â‹†â†’1âˆ’yâ‹†).\delta\_{T}(y^{\star},\theta^{\star}):=K\_{T}(y^{\star},\theta^{\star}\to 1-y^{\star}). |  |

###### Definition 4.2 (Outcome identifiability (at the truth)).

Fix (yâ‹†,Î¸â‹†)(y^{\star},\theta^{\star}) and v1:Tv\_{1:T}. The outcome is *identifiable at horizon TT at (yâ‹†,Î¸â‹†)(y^{\star},\theta^{\star})* if Î´Tâ€‹(yâ‹†,Î¸â‹†)>0\delta\_{T}(y^{\star},\theta^{\star})>0. If additionally the reverse-direction projection KTâ€‹(1âˆ’yâ‹†,Î¸â†’yâ‹†)K\_{T}(1-y^{\star},\theta\to y^{\star}) is bounded away from zero uniformly over Î¸âˆˆÎ˜\theta\in\Theta, we say the model class exhibits *two-sided* outcome separation.

###### Remark 4.1 (Uniform vs. truth-relative identifiability).

Demanding Î´Tâ€‹(yâ‹†,Î¸â‹†)>0\delta\_{T}(y^{\star},\theta^{\star})>0 for *every* Î¸â‹†âˆˆÎ˜\theta^{\star}\in\Theta is typically too strong in latent-type models, since Î˜\Theta often includes regimes in which all active types are outcome-independent (e.g. Ïk\rho\_{k} concentrates on noise types). Our definition isolates the statistically meaningful question: whether the realized data-generating pair (yâ‹†,Î¸â‹†)(y^{\star},\theta^{\star}) is distinguishable from the alternative outcome family.

#### 4.2 Mechanisms of Non-Identifiability

The latent-type structure ([3](https://arxiv.org/html/2601.18815v1#S3.E3 "In 3.6 Mixture Likelihood and Conditional Independence â€£ 3 Model and Bayesian Formulation â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types")) highlights key sources of outcome non-identifiability.

###### Proposition 4.1 (Sufficient conditions for outcome non-identifiability).

Fix (yâ‹†,Î¸â‹†)(y^{\star},\theta^{\star}) and v1:Tv\_{1:T}. The separation gap satisfies Î´Tâ€‹(yâ‹†,Î¸â‹†)=0\delta\_{T}(y^{\star},\theta^{\star})=0 whenever any of the following hold:

1. 1.

   Type-composition confounding (uninformative regime): for each tt and each type kk with Ïkâ€‹(vt;Î¸â‹†)>0\rho\_{k}(v\_{t};\theta^{\star})>0, one has fk,1(â‹…âˆ£vt,ğœ½kâ‹†)=fk,0(â‹…âˆ£vt,ğœ½kâ‹†)f\_{k,1}(\cdot\mid v\_{t},\bm{\theta}\_{k}^{\star})=f\_{k,0}(\cdot\mid v\_{t},\bm{\theta}\_{k}^{\star}). In particular, if f1(â‹…âˆ£vt,Î¸â‹†)=f0(â‹…âˆ£vt,Î¸â‹†)f\_{1}(\cdot\mid v\_{t},\theta^{\star})=f\_{0}(\cdot\mid v\_{t},\theta^{\star}) for all tt, then the two outcomes induce the same law and are indistinguishable.
2. 2.

   Outcomeâ€“nuisance symmetry: there exists a measurable map Î¨:Î˜â†’Î˜\Psi:\Theta\to\Theta such that for all tt,

   |  |  |  |
   | --- | --- | --- |
   |  | f1(â‹…âˆ£vt,Î¸)=f0(â‹…âˆ£vt,Î¨(Î¸))as densities onÂ â„.f\_{1}(\cdot\mid v\_{t},\theta)=f\_{0}(\cdot\mid v\_{t},\Psi(\theta))\quad\text{as densities on }\mathbb{R}. |  |

   Then infÎ¸DKLâ€‹(P1,Î¸(T)âˆ¥P0,Î¸â€²(T))=0\inf\_{\theta}\mathrm{D\_{KL}}(P\_{1,\theta}^{(T)}\|P\_{0,\theta^{\prime}}^{(T)})=0 and vice versa. This is the symmetry eliminated by the orientation constraint in ExampleÂ [3.1](https://arxiv.org/html/2601.18815v1#S3.Thmexample1 "Example 3.1 (Gaussian latent-type model). â€£ 3.7 A Concrete Instantiation: Gaussian Latent-Type Model â€£ 3 Model and Bayesian Formulation â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
3. 3.

   Adversarial mimicry: there exists Î¸âˆˆÎ˜\theta\in\Theta such that Pyâ‹†,Î¸â‹†(T)=P1âˆ’yâ‹†,Î¸(T)P\_{y^{\star},\theta^{\star}}^{(T)}=P\_{1-y^{\star},\theta}^{(T)}. This includes degenerate cases where adversarial components perfectly cancel informed drift on the observed volume range.

###### Proof.

If Pyâ‹†,Î¸â‹†(T)=P1âˆ’yâ‹†,Î¸(T)P\_{y^{\star},\theta^{\star}}^{(T)}=P\_{1-y^{\star},\theta}^{(T)} for some Î¸\theta, then Î´Tâ€‹(yâ‹†,Î¸â‹†)=0\delta\_{T}(y^{\star},\theta^{\star})=0 by DefinitionÂ [4.1](https://arxiv.org/html/2601.18815v1#S4.Thmdefinition1 "Definition 4.1 (KL projection gap). â€£ 4.1 Induced Data Laws and KL Projections â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types"). Items (1)â€“(2) provide explicit sufficient conditions for such equality.
âˆ

#### 4.3 Sufficient Conditions for Separation in the Gaussian Model

The following result gives a robust (and checkable) route to a positive separation gap in ExampleÂ [3.1](https://arxiv.org/html/2601.18815v1#S3.Thmexample1 "Example 3.1 (Gaussian latent-type model). â€£ 3.7 A Concrete Instantiation: Gaussian Latent-Type Model â€£ 3 Model and Bayesian Formulation â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types"). The main idea is to identify a subset of times where (i) the outcome-dependent drift is activated, (ii) manipulation is inactive, and (iii) the remaining finite mixture is identifiable as a parametric family, so equality of laws under opposite outcomes is impossible.

###### Theorem 4.1 (A design-based sufficient condition for outcome separation).

Assume the Gaussian latent-type model of ExampleÂ [3.1](https://arxiv.org/html/2601.18815v1#S3.Thmexample1 "Example 3.1 (Gaussian latent-type model). â€£ 3.7 A Concrete Instantiation: Gaussian Latent-Type Model â€£ 3 Model and Bayesian Formulation â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types") with the orientation constraint Î¼1,Î¼3â‰¥0\mu\_{1},\mu\_{3}\geq 0. Fix (yâ‹†,Î¸â‹†)(y^{\star},\theta^{\star}) and suppose there exists an index set ITâŠ‚{1,â€¦,T}I\_{T}\subset\{1,\dots,T\} and constants ÏÂ¯,mÂ¯,ÏƒÂ¯,ÏƒÂ¯>0\underline{\rho},\underline{m},\underline{\sigma},\overline{\sigma}>0 such that for all tâˆˆITt\in I\_{T}:

1. 1.

   Informed activation: Ï1â€‹(vt;Î¸â‹†)â‰¥ÏÂ¯\rho\_{1}(v\_{t};\theta^{\star})\geq\underline{\rho} and |m1,1â€‹(vt;ğœ½1â‹†)âˆ’m1,0â€‹(vt;ğœ½1â‹†)|â‰¥mÂ¯|m\_{1,1}(v\_{t};\bm{\theta}\_{1}^{\star})-m\_{1,0}(v\_{t};\bm{\theta}\_{1}^{\star})|\geq\underline{m}.
2. 2.

   Manipulator inactivity: m3,1â€‹(vt;ğœ½3)=m3,0â€‹(vt;ğœ½3)=0m\_{3,1}(v\_{t};\bm{\theta}\_{3})=m\_{3,0}(v\_{t};\bm{\theta}\_{3})=0 for all ğœ½3âˆˆÎ˜3\bm{\theta}\_{3}\in\Theta\_{3} (e.g. vtâ‰¤Ï„3v\_{t}\leq\tau\_{3} and manipulator drift is thresholded as in ExampleÂ [3.1](https://arxiv.org/html/2601.18815v1#S3.Thmexample1 "Example 3.1 (Gaussian latent-type model). â€£ 3.7 A Concrete Instantiation: Gaussian Latent-Type Model â€£ 3 Model and Bayesian Formulation â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types")).
3. 3.

   Non-degeneracy of scales: for all Î¸âˆˆÎ˜\theta\in\Theta, the type-specific scales satisfy ÏƒÂ¯â‰¤skâ€‹(vt;ğœ½k)â‰¤ÏƒÂ¯\underline{\sigma}\leq s\_{k}(v\_{t};\bm{\theta}\_{k})\leq\overline{\sigma} for kâˆˆ{1,2}k\in\{1,2\}.

Then there exists Îº>0\kappa>0 (depending on the constants above and on Î˜\Theta) such that

|  |  |  |
| --- | --- | --- |
|  | Î´Tâ€‹(yâ‹†,Î¸â‹†)â‰¥|IT|Tâ€‹Îº.\delta\_{T}(y^{\star},\theta^{\star})\ \geq\ \frac{|I\_{T}|}{T}\,\kappa. |  |

In particular, if lim infTâ†’âˆ|IT|/T>0\liminf\_{T\to\infty}|I\_{T}|/T>0, then lim infTâ†’âˆÎ´Tâ€‹(yâ‹†,Î¸â‹†)>0\liminf\_{T\to\infty}\delta\_{T}(y^{\star},\theta^{\star})>0.

###### Proof sketch.

Fix tâˆˆITt\in I\_{T}. By item (2), the conditional increment law at volume vtv\_{t} reduces (uniformly over Î¸âˆˆÎ˜\theta\in\Theta) to a two-component Gaussian mixture in which the only outcome-dependent location shift occurs in the informed component. Under mild non-degeneracy (item (3)) and compactness of Î˜\Theta, the family of such mixtures is identifiable up to label-switching (Teicher, [1963](https://arxiv.org/html/2601.18815v1#bib.bib38 "Identifiability of finite mixtures"); Holzmann et al., [2006](https://arxiv.org/html/2601.18815v1#bib.bib40 "Identifiability of finite mixtures of elliptical distributions")). The orientation constraint pins the sign of the informed drift under each outcome. Hence no choice of nuisance parameters under the wrong outcome can exactly reproduce the outcome-conditional density at vtv\_{t} generated by (yâ‹†,Î¸â‹†)(y^{\star},\theta^{\star}) when item (1) holds. Continuity of Î¸â†¦DKL(fyâ‹†(â‹…âˆ£vt,Î¸â‹†)âˆ¥f1âˆ’yâ‹†(â‹…âˆ£vt,Î¸))\theta\mapsto\mathrm{D\_{KL}}(f\_{y^{\star}}(\cdot\mid v\_{t},\theta^{\star})\|f\_{1-y^{\star}}(\cdot\mid v\_{t},\theta)) and compactness of Î˜\Theta imply the infimum over Î¸\theta is achieved and strictly positive at each such tt; denote the minimum by Îºt>0\kappa\_{t}>0. Let Îº:=mintâˆˆITâ¡Îºt\kappa:=\min\_{t\in I\_{T}}\kappa\_{t}. Summing over tâˆˆITt\in I\_{T} using ([8](https://arxiv.org/html/2601.18815v1#S4.E8 "In 4.1 Induced Data Laws and KL Projections â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types")) yields the claim.
âˆ

###### Remark 4.2 (Interpretation).

TheoremÂ [4.1](https://arxiv.org/html/2601.18815v1#S4.Thmtheorem1 "Theorem 4.1 (A design-based sufficient condition for outcome separation). â€£ 4.3 Sufficient Conditions for Separation in the Gaussian Model â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types") isolates a practically meaningful regime: a positive fraction of â€œcleanâ€ volume periods in which informative flow is present and adversarial drift is inactive suffices to render outcome inference well-posed, even when other periods are dominated by noise or heavy tails.

#### 4.4 Posterior Concentration and Finite-Sample Error Bounds

This section links posterior behavior to the KL projection gap. Because nuisance structure is integrated out, the key object is the marginal likelihood ratio.

##### 4.4.1 Regularity Conditions

We impose standard conditions ensuring measurability and allowing uniform laws of large numbers over Î˜\Theta.

###### Assumption 4.1 (Compactness and continuity).

The parameter space Î˜\Theta is a compact metric space, and for each tt, the map Î¸â†¦logâ¡fyâ€‹(Î´â€‹xâˆ£vt,Î¸)\theta\mapsto\log f\_{y}(\delta x\mid v\_{t},\theta) is continuous for Lebesgue-a.e. Î´â€‹xâˆˆâ„\delta x\in\mathbb{R}.

###### Assumption 4.2 (Envelope and integrability).

There exists a measurable envelope function M:â„â†’[0,âˆ)M:\mathbb{R}\to[0,\infty) such that for all tt, all Î¸âˆˆÎ˜\theta\in\Theta, and all Î´â€‹xâˆˆâ„\delta x\in\mathbb{R},

|  |  |  |
| --- | --- | --- |
|  | |logfy(Î´xâˆ£vt,Î¸)|â‰¤M(Î´x),\bigl|\log f\_{y}(\delta x\mid v\_{t},\theta)\bigr|\leq M(\delta x), |  |

and suptâ‰¤Tğ”¼Pyâ‹†,Î¸â‹†(T)â€‹[Mâ€‹(Î”â€‹Xt)]<âˆ\sup\_{t\leq T}\mathbb{E}\_{P\_{y^{\star},\theta^{\star}}^{(T)}}[M(\Delta X\_{t})]<\infty for each TT.

###### Assumption 4.3 (KL support at (near) projections).

Fix (yâ‹†,Î¸â‹†)(y^{\star},\theta^{\star}) and v1:Tv\_{1:T}. For each yâˆˆ{0,1}y\in\{0,1\} and Ïµ>0\epsilon>0, define the Ïµ\epsilon-projection set

|  |  |  |
| --- | --- | --- |
|  | Î˜y,T(Ïµ):={Î¸âˆˆÎ˜:1TDKL(Pyâ‹†,Î¸â‹†(T)âˆ¥Py,Î¸(T))â‰¤KT(yâ‹†,Î¸â‹†â†’y)+Ïµ}.\Theta\_{y,T}(\epsilon):=\left\{\theta\in\Theta:\frac{1}{T}\mathrm{D\_{KL}}\!\left(P\_{y^{\star},\theta^{\star}}^{(T)}\,\middle\|\,P\_{y,\theta}^{(T)}\right)\leq K\_{T}(y^{\star},\theta^{\star}\to y)+\epsilon\right\}. |  |

Assume Î â€‹(Î˜y,Tâ€‹(Ïµ))>0\Pi(\Theta\_{y,T}(\epsilon))>0 for all yy and Ïµ>0\epsilon>0 (for each fixed TT).

AssumptionÂ [4.3](https://arxiv.org/html/2601.18815v1#S4.Thmassumption3 "Assumption 4.3 (KL support at (near) projections). â€£ 4.4.1 Regularity Conditions â€£ 4.4 Posterior Concentration and Finite-Sample Error Bounds â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types") is the natural â€œKL supportâ€ condition: the prior must not exclude parameters that achieve (or nearly achieve) the best KL fit for each outcome hypothesis. Unlike uniform-in-TT lower bounds, this condition is compatible with shrinking neighborhoods of minimizers.

##### 4.4.2 Bayes Factor Separation

Write the log-likelihood under outcome yy as

|  |  |  |
| --- | --- | --- |
|  | â„“y,Tâ€‹(Î¸):=âˆ‘t=1Tlogâ¡fyâ€‹(Î”â€‹Xtâˆ£vt,Î¸),myâ€‹(HT)=âˆ«Î˜eâ„“y,Tâ€‹(Î¸)â€‹Î â€‹(dâ€‹Î¸).\ell\_{y,T}(\theta):=\sum\_{t=1}^{T}\log f\_{y}(\Delta X\_{t}\mid v\_{t},\theta),\qquad m\_{y}(H\_{T})=\int\_{\Theta}e^{\ell\_{y,T}(\theta)}\,\Pi(d\theta). |  |

Under the true law Pyâ‹†,Î¸â‹†(T)P\_{y^{\star},\theta^{\star}}^{(T)}, define the (normalized) expected log-likelihood

|  |  |  |
| --- | --- | --- |
|  | Î›y,Tâ€‹(Î¸):=1Tâ€‹ğ”¼Pyâ‹†,Î¸â‹†(T)â€‹[â„“y,Tâ€‹(Î¸)].\Lambda\_{y,T}(\theta):=\frac{1}{T}\mathbb{E}\_{P\_{y^{\star},\theta^{\star}}^{(T)}}[\ell\_{y,T}(\theta)]. |  |

Then for each yy and Î¸\theta,

|  |  |  |
| --- | --- | --- |
|  | Î›yâ‹†,T(Î¸â‹†)âˆ’Î›y,T(Î¸)=1TDKL(Pyâ‹†,Î¸â‹†(T)âˆ¥Py,Î¸(T)).\Lambda\_{y^{\star},T}(\theta^{\star})-\Lambda\_{y,T}(\theta)=\frac{1}{T}\mathrm{D\_{KL}}\!\left(P\_{y^{\star},\theta^{\star}}^{(T)}\,\middle\|\,P\_{y,\theta}^{(T)}\right). |  |

###### Assumption 4.4 (Uniform law of large numbers).

For each yâˆˆ{0,1}y\in\{0,1\},

|  |  |  |
| --- | --- | --- |
|  | supÎ¸âˆˆÎ˜|1Tâ€‹â„“y,Tâ€‹(Î¸)âˆ’Î›y,Tâ€‹(Î¸)|â†’0almost surely underÂ â€‹Pyâ‹†,Î¸â‹†(T).\sup\_{\theta\in\Theta}\left|\frac{1}{T}\ell\_{y,T}(\theta)-\Lambda\_{y,T}(\theta)\right|\to 0\qquad\text{almost surely under }P\_{y^{\star},\theta^{\star}}^{(T)}. |  |

###### Theorem 4.2 (Asymptotic separation of Bayes factors (robust form)).

Assume AssumptionsÂ [3.1](https://arxiv.org/html/2601.18815v1#S3.Thmassumption1 "Assumption 3.1 (Conditional independence of increments). â€£ 3.6 Mixture Likelihood and Conditional Independence â€£ 3 Model and Bayesian Formulation â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types") and [4.1](https://arxiv.org/html/2601.18815v1#S4.Thmassumption1 "Assumption 4.1 (Compactness and continuity). â€£ 4.4.1 Regularity Conditions â€£ 4.4 Posterior Concentration and Finite-Sample Error Bounds â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types")â€“[4.4](https://arxiv.org/html/2601.18815v1#S4.Thmassumption4 "Assumption 4.4 (Uniform law of large numbers). â€£ 4.4.2 Bayes Factor Separation â€£ 4.4 Posterior Concentration and Finite-Sample Error Bounds â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types"). Fix (yâ‹†,Î¸â‹†)(y^{\star},\theta^{\star}) and v1:Tv\_{1:T}. Then for any Ïµ>0\epsilon>0, almost surely for all sufficiently large TT,

|  |  |  |  |
| --- | --- | --- | --- |
|  | 1Tâ€‹logâ¡BFTâ€‹(HT)â‰¥Î´Tâ€‹(yâ‹†,Î¸â‹†)âˆ’Ïµ,\frac{1}{T}\log\mathrm{BF}\_{T}(H\_{T})\geq\delta\_{T}(y^{\star},\theta^{\star})-\epsilon, |  | (10) |

and similarly

|  |  |  |  |
| --- | --- | --- | --- |
|  | 1Tâ€‹logâ¡BFTâ€‹(HT)â‰¤Î´Tâ€‹(yâ‹†,Î¸â‹†)+ÏµwhenÂ â€‹yâ‹†=1,\frac{1}{T}\log\mathrm{BF}\_{T}(H\_{T})\leq\delta\_{T}(y^{\star},\theta^{\star})+\epsilon\qquad\text{when }y^{\star}=1, |  | (11) |

with the inequalities reversed when yâ‹†=0y^{\star}=0.
Consequently, if lim infTâ†’âˆÎ´Tâ€‹(yâ‹†,Î¸â‹†)>0\liminf\_{T\to\infty}\delta\_{T}(y^{\star},\theta^{\star})>0, then |logâ¡BFTâ€‹(HT)||\log\mathrm{BF}\_{T}(H\_{T})| grows at least linearly with TT.

###### Proof sketch.

Under AssumptionÂ [4.4](https://arxiv.org/html/2601.18815v1#S4.Thmassumption4 "Assumption 4.4 (Uniform law of large numbers). â€£ 4.4.2 Bayes Factor Separation â€£ 4.4 Posterior Concentration and Finite-Sample Error Bounds â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types"), 1Tâ€‹logâ¡myâ€‹(HT)\frac{1}{T}\log m\_{y}(H\_{T}) concentrates around supÎ¸Î›y,Tâ€‹(Î¸)\sup\_{\theta}\Lambda\_{y,T}(\theta) up to oâ€‹(1)o(1), by standard Laplace-principle bounds (upper bound by the supremum; lower bound by restricting the integral to any neighborhood with positive prior mass). The difference supÎ¸Î›yâ‹†,Tâ€‹(Î¸)âˆ’supÎ¸Î›1âˆ’yâ‹†,Tâ€‹(Î¸)\sup\_{\theta}\Lambda\_{y^{\star},T}(\theta)-\sup\_{\theta}\Lambda\_{1-y^{\star},T}(\theta) equals the KL projection gap Î´Tâ€‹(yâ‹†,Î¸â‹†)\delta\_{T}(y^{\star},\theta^{\star}) because Î›yâ‹†,T\Lambda\_{y^{\star},T} is maximized at Î¸â‹†\theta^{\star} and supÎ¸Î›1âˆ’yâ‹†,Tâ€‹(Î¸)=Î›yâ‹†,Tâ€‹(Î¸â‹†)âˆ’infÎ¸1Tâ€‹DKLâ€‹(Pyâ‹†,Î¸â‹†(T)âˆ¥P1âˆ’yâ‹†,Î¸(T))\sup\_{\theta}\Lambda\_{1-y^{\star},T}(\theta)=\Lambda\_{y^{\star},T}(\theta^{\star})-\inf\_{\theta}\frac{1}{T}\mathrm{D\_{KL}}(P\_{y^{\star},\theta^{\star}}^{(T)}\|P\_{1-y^{\star},\theta}^{(T)}).
âˆ

##### 4.4.3 Posterior Consistency

###### Theorem 4.3 (Posterior consistency for the outcome).

Assume the conditions of TheoremÂ [4.2](https://arxiv.org/html/2601.18815v1#S4.Thmtheorem2 "Theorem 4.2 (Asymptotic separation of Bayes factors (robust form)). â€£ 4.4.2 Bayes Factor Separation â€£ 4.4 Posterior Concentration and Finite-Sample Error Bounds â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types"). If

|  |  |  |  |
| --- | --- | --- | --- |
|  | lim infTâ†’âˆÎ´Tâ€‹(yâ‹†,Î¸â‹†)>0,\liminf\_{T\to\infty}\delta\_{T}(y^{\star},\theta^{\star})>0, |  | (12) |

then

|  |  |  |
| --- | --- | --- |
|  | â„™â€‹(Y=yâ‹†âˆ£HT)â†’1almost surely underÂ â€‹Pyâ‹†,Î¸â‹†(T).\mathbb{P}(Y=y^{\star}\mid H\_{T})\to 1\quad\text{almost surely under }P\_{y^{\star},\theta^{\star}}^{(T)}. |  |

###### Proof.

By ([7](https://arxiv.org/html/2601.18815v1#S3.E7 "In 3.8 Prior and Posterior â€£ 3 Model and Bayesian Formulation â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types")), the posterior odds equal the prior odds times BFTâ€‹(HT)\mathrm{BF}\_{T}(H\_{T}). Under ([12](https://arxiv.org/html/2601.18815v1#S4.E12 "In Theorem 4.3 (Posterior consistency for the outcome). â€£ 4.4.3 Posterior Consistency â€£ 4.4 Posterior Concentration and Finite-Sample Error Bounds â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types")), TheoremÂ [4.2](https://arxiv.org/html/2601.18815v1#S4.Thmtheorem2 "Theorem 4.2 (Asymptotic separation of Bayes factors (robust form)). â€£ 4.4.2 Bayes Factor Separation â€£ 4.4 Posterior Concentration and Finite-Sample Error Bounds â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types") implies the log Bayes factor diverges linearly with TT with the correct sign, forcing posterior mass onto the true outcome.
âˆ

##### 4.4.4 Finite-Sample Error Bounds via Truncation

Uniform boundedness of log-likelihood ratios is incompatible with unbounded-support increment models (Gaussian, Student-tt, etc.). We therefore state finite-sample guarantees in a robust, truncation-based form.

###### Assumption 4.5 (Moment control for increments).

There exists q>2q>2 and Cq<âˆC\_{q}<\infty such that suptâ‰¤Tğ”¼â€‹[|Î”â€‹Xt|q]â‰¤Cq\sup\_{t\leq T}\mathbb{E}[|\Delta X\_{t}|^{q}]\leq C\_{q} under the true law Pyâ‹†,Î¸â‹†(T)P\_{y^{\star},\theta^{\star}}^{(T)} (for each fixed TT).

For R>0R>0, define the â€œtypical incrementâ€ event ER:={max1â‰¤tâ‰¤Tâ¡|Î”â€‹Xt|â‰¤R}E\_{R}:=\{\max\_{1\leq t\leq T}|\Delta X\_{t}|\leq R\}. Under AssumptionsÂ [4.1](https://arxiv.org/html/2601.18815v1#S4.Thmassumption1 "Assumption 4.1 (Compactness and continuity). â€£ 4.4.1 Regularity Conditions â€£ 4.4 Posterior Concentration and Finite-Sample Error Bounds â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types") and [4.2](https://arxiv.org/html/2601.18815v1#S4.Thmassumption2 "Assumption 4.2 (Envelope and integrability). â€£ 4.4.1 Regularity Conditions â€£ 4.4 Posterior Concentration and Finite-Sample Error Bounds â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types"), continuity on compact sets implies that, for each fixed RR, there exists BR<âˆB\_{R}<\infty such that on ERE\_{R},

|  |  |  |
| --- | --- | --- |
|  | supÎ¸âˆˆÎ˜|logâ¡fyâ‹†â€‹(Î”â€‹Xtâˆ£vt,Î¸â‹†)f1âˆ’yâ‹†â€‹(Î”â€‹Xtâˆ£vt,Î¸)|â‰¤BRfor allÂ â€‹t.\sup\_{\theta\in\Theta}\left|\log\frac{f\_{y^{\star}}(\Delta X\_{t}\mid v\_{t},\theta^{\star})}{f\_{1-y^{\star}}(\Delta X\_{t}\mid v\_{t},\theta)}\right|\leq B\_{R}\qquad\text{for all }t. |  |

###### Proposition 4.2 (Finite-sample posterior error bound (robust form)).

Suppose Î”â€‹X1:Tâˆ¼Pyâ‹†,Î¸â‹†(T)\Delta X\_{1:T}\sim P\_{y^{\star},\theta^{\star}}^{(T)} and AssumptionsÂ [4.3](https://arxiv.org/html/2601.18815v1#S4.Thmassumption3 "Assumption 4.3 (KL support at (near) projections). â€£ 4.4.1 Regularity Conditions â€£ 4.4 Posterior Concentration and Finite-Sample Error Bounds â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types") andÂ [4.5](https://arxiv.org/html/2601.18815v1#S4.Thmassumption5 "Assumption 4.5 (Moment control for increments). â€£ 4.4.4 Finite-Sample Error Bounds via Truncation â€£ 4.4 Posterior Concentration and Finite-Sample Error Bounds â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types") hold. Let Î´T:=Î´Tâ€‹(yâ‹†,Î¸â‹†)\delta\_{T}:=\delta\_{T}(y^{\star},\theta^{\star}). Then for any Ïµâˆˆ(0,Î´T)\epsilon\in(0,\delta\_{T}) and any R>0R>0,

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„™â€‹(â„™â€‹(Yâ‰ yâ‹†âˆ£HT)â‰¥eâˆ’Tâ€‹(Î´Tâˆ’Ïµ))â‰¤expâ¡(âˆ’Tâ€‹Ïµ22â€‹BR2)+â„™â€‹(ERc),\mathbb{P}\!\left(\mathbb{P}(Y\neq y^{\star}\mid H\_{T})\geq e^{-T(\delta\_{T}-\epsilon)}\right)\leq\exp\!\left(-\frac{T\epsilon^{2}}{2B\_{R}^{2}}\right)+\mathbb{P}(E\_{R}^{c}), |  | (13) |

where â„™â€‹(ERc)â‰¤Tâ€‹Cqâ€‹Râˆ’q\mathbb{P}(E\_{R}^{c})\leq T\,C\_{q}\,R^{-q} by Markovâ€™s inequality and a union bound.

###### Proof sketch.

On ERE\_{R}, the log-likelihood ratio increments are uniformly bounded by BRB\_{R}, so concentration for the (centered) log Bayes factor follows from Hoeffding/Azuma-type inequalities applied to independent increments. Outside ERE\_{R}, the bound is trivial and we pay â„™â€‹(ERc)\mathbb{P}(E\_{R}^{c}).
âˆ

###### Corollary 4.1 (Expected posterior error).

Under the conditions of PropositionÂ [4.2](https://arxiv.org/html/2601.18815v1#S4.Thmproposition2 "Proposition 4.2 (Finite-sample posterior error bound (robust form)). â€£ 4.4.4 Finite-Sample Error Bounds via Truncation â€£ 4.4 Posterior Concentration and Finite-Sample Error Bounds â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types"), for any Ïµâˆˆ(0,Î´T)\epsilon\in(0,\delta\_{T}) and R>0R>0,

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[â„™â€‹(Yâ‰ yâ‹†âˆ£HT)]â‰¤eâˆ’Tâ€‹(Î´Tâˆ’Ïµ)+expâ¡(âˆ’Tâ€‹Ïµ22â€‹BR2)+â„™â€‹(ERc).\mathbb{E}[\mathbb{P}(Y\neq y^{\star}\mid H\_{T})]\leq e^{-T(\delta\_{T}-\epsilon)}+\exp\!\left(-\frac{T\epsilon^{2}}{2B\_{R}^{2}}\right)+\mathbb{P}(E\_{R}^{c}). |  |

#### 4.5 Stability of Posterior Odds under Perturbations

This section studies well-posedness of outcome inference as a map from observed histories to posterior beliefs.

##### 4.5.1 Metric on Histories

Let h=(p0:T,v1:T)h=(p\_{0:T},v\_{1:T}) and hâ€²=(p0:Tâ€²,v1:Tâ€²)h^{\prime}=(p^{\prime}\_{0:T},v^{\prime}\_{1:T}) be two histories with log-odds increments Î”â€‹x1:T\Delta x\_{1:T} and Î”â€‹x1:Tâ€²\Delta x^{\prime}\_{1:T}. Define the weighted history distance

|  |  |  |  |
| --- | --- | --- | --- |
|  | dâ€‹(h,hâ€²):=Î»xâ€‹âˆ‘t=1T|Î”â€‹xtâˆ’Î”â€‹xtâ€²|+Î»vâ€‹âˆ‘t=1T|vtâˆ’vtâ€²|,d(h,h^{\prime}):=\lambda\_{x}\sum\_{t=1}^{T}|\Delta x\_{t}-\Delta x^{\prime}\_{t}|+\lambda\_{v}\sum\_{t=1}^{T}|v\_{t}-v^{\prime}\_{t}|, |  | (14) |

for weights Î»x,Î»v>0\lambda\_{x},\lambda\_{v}>0.

##### 4.5.2 Local Lipschitz Regularity

Global Lipschitz bounds on logâ¡fyâ€‹(â‹…)\log f\_{y}(\cdot) are incompatible with Gaussian (and most other) increment models on â„\mathbb{R}. We therefore adopt a local form that holds on â€œtypicalâ€ events.

###### Assumption 4.6 (Local Lipschitz regularity on bounded increments).

For each R>0R>0 there exist constants Lxâ€‹(R),Lvâ€‹(R)<âˆL\_{x}(R),L\_{v}(R)<\infty such that for all yâˆˆ{0,1}y\in\{0,1\}, all Î¸âˆˆÎ˜\theta\in\Theta, all v,vâ€²â‰¥0v,v^{\prime}\geq 0, and all Î´â€‹x,Î´â€‹xâ€²âˆˆ[âˆ’R,R]\delta x,\delta x^{\prime}\in[-R,R],

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | |logfy(Î´xâˆ£v,Î¸)âˆ’logfy(Î´xâ€²âˆ£v,Î¸)|\displaystyle\bigl|\log f\_{y}(\delta x\mid v,\theta)-\log f\_{y}(\delta x^{\prime}\mid v,\theta)\bigr| | â‰¤Lxâ€‹(R)â€‹|Î´â€‹xâˆ’Î´â€‹xâ€²|,\displaystyle\leq L\_{x}(R)\,|\delta x-\delta x^{\prime}|, |  | (15) |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | |logfy(Î´xâˆ£v,Î¸)âˆ’logfy(Î´xâˆ£vâ€²,Î¸)|\displaystyle\bigl|\log f\_{y}(\delta x\mid v,\theta)-\log f\_{y}(\delta x\mid v^{\prime},\theta)\bigr| | â‰¤Lvâ€‹(R)â€‹|vâˆ’vâ€²|.\displaystyle\leq L\_{v}(R)\,|v-v^{\prime}|. |  | (16) |

###### Remark 4.3 (Gaussian example).

For a Gaussian location-scale family, |âˆ‚xlogâ¡f||\partial\_{x}\log f| grows at most linearly in |x||x|; hence Lxâ€‹(R)L\_{x}(R) can be taken of the form c0+c1â€‹Rc\_{0}+c\_{1}R, uniformly over compact parameter sets. Analogous bounds hold for mixtures under mild non-degeneracy.

##### 4.5.3 Stability Theorem

For R>0R>0, define the bounded-increment event for two histories

|  |  |  |
| --- | --- | --- |
|  | ERâ€‹(h,hâ€²):={maxtâ‰¤Tâ¡|Î”â€‹xt|â‰¤Râ€‹andâ€‹maxtâ‰¤Tâ¡|Î”â€‹xtâ€²|â‰¤R}.E\_{R}(h,h^{\prime}):=\left\{\max\_{t\leq T}|\Delta x\_{t}|\leq R\ \text{and}\ \max\_{t\leq T}|\Delta x^{\prime}\_{t}|\leq R\right\}. |  |

###### Theorem 4.4 (Stability of posterior odds on typical histories).

Under AssumptionÂ [4.6](https://arxiv.org/html/2601.18815v1#S4.Thmassumption6 "Assumption 4.6 (Local Lipschitz regularity on bounded increments). â€£ 4.5.2 Local Lipschitz Regularity â€£ 4.5 Stability of Posterior Odds under Perturbations â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types"), for any two histories h,hâ€²h,h^{\prime} and any R>0R>0, on the event ERâ€‹(h,hâ€²)E\_{R}(h,h^{\prime}),

|  |  |  |  |
| --- | --- | --- | --- |
|  | |logâ¡BFTâ€‹(h)âˆ’logâ¡BFTâ€‹(hâ€²)|â‰¤2â€‹(Lxâ€‹(R)â€‹âˆ‘t=1T|Î”â€‹xtâˆ’Î”â€‹xtâ€²|+Lvâ€‹(R)â€‹âˆ‘t=1T|vtâˆ’vtâ€²|).\left|\log\mathrm{BF}\_{T}(h)-\log\mathrm{BF}\_{T}(h^{\prime})\right|\leq 2\left(L\_{x}(R)\sum\_{t=1}^{T}|\Delta x\_{t}-\Delta x^{\prime}\_{t}|+L\_{v}(R)\sum\_{t=1}^{T}|v\_{t}-v\_{t}^{\prime}|\right). |  | (17) |

Furthermore, the posterior probability satisfies

|  |  |  |  |
| --- | --- | --- | --- |
|  | |â„™(Y=1âˆ£h)âˆ’â„™(Y=1âˆ£hâ€²)|â‰¤14|logBFT(h)âˆ’logBFT(hâ€²)|.\left|\mathbb{P}(Y=1\mid h)-\mathbb{P}(Y=1\mid h^{\prime})\right|\leq\frac{1}{4}\left|\log\mathrm{BF}\_{T}(h)-\log\mathrm{BF}\_{T}(h^{\prime})\right|. |  | (18) |

###### Proof sketch.

For fixed yy, the map hâ†¦logâ¡myâ€‹(h)h\mapsto\log m\_{y}(h) is 11-Lipschitz with respect to the supÎ¸\sup\_{\theta} deviation of the log-likelihood â„“y,Tâ€‹(Î¸)\ell\_{y,T}(\theta), since logâ€‹âˆ«euÎ¸â€‹Î â€‹(dâ€‹Î¸)\log\int e^{u\_{\theta}}\Pi(d\theta) is a log-sum-exp functional. On ERâ€‹(h,hâ€²)E\_{R}(h,h^{\prime}), AssumptionÂ [4.6](https://arxiv.org/html/2601.18815v1#S4.Thmassumption6 "Assumption 4.6 (Local Lipschitz regularity on bounded increments). â€£ 4.5.2 Local Lipschitz Regularity â€£ 4.5 Stability of Posterior Odds under Perturbations â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types") bounds |â„“y,Tâ€‹(Î¸;h)âˆ’â„“y,Tâ€‹(Î¸;hâ€²)||\ell\_{y,T}(\theta;h)-\ell\_{y,T}(\theta;h^{\prime})| uniformly over Î¸\theta. Taking the difference of y=1y=1 and y=0y=0 yields ([17](https://arxiv.org/html/2601.18815v1#S4.E17 "In Theorem 4.4 (Stability of posterior odds on typical histories). â€£ 4.5.3 Stability Theorem â€£ 4.5 Stability of Posterior Odds under Perturbations â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types")). Inequality ([18](https://arxiv.org/html/2601.18815v1#S4.E18 "In Theorem 4.4 (Stability of posterior odds on typical histories). â€£ 4.5.3 Stability Theorem â€£ 4.5 Stability of Posterior Odds under Perturbations â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types")) follows from the fact that aâ†¦Ïƒâ€‹(logitâ€‹(Ï€0)+a)a\mapsto\sigma(\mathrm{logit}(\pi\_{0})+a) has derivative bounded by 1/41/4.
âˆ

###### Remark 4.4 (Condition-number interpretation).

TheoremÂ [4.4](https://arxiv.org/html/2601.18815v1#S4.Thmtheorem4 "Theorem 4.4 (Stability of posterior odds on typical histories). â€£ 4.5.3 Stability Theorem â€£ 4.5 Stability of Posterior Odds under Perturbations â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types") formalizes a â€œcondition numberâ€ for market inference: posterior odds are stable on typical histories when log-densities are not overly sensitive to perturbations on the increment scale that is realized under the data-generating process. Instability can arise either from extreme price impact (large Lxâ€‹(R)L\_{x}(R)) or from strong volume sensitivity (large Lvâ€‹(R)L\_{v}(R)).

#### 4.6 Information Gain and Market-as-Sensor Metrics

##### 4.6.1 Realized Information Gain

Let Ï€Tâ€‹(h):=â„™â€‹(Y=1âˆ£h)\pi\_{T}(h):=\mathbb{P}(Y=1\mid h). The realized information gain is the KL divergence from the posterior to the prior:

|  |  |  |  |
| --- | --- | --- | --- |
|  | IG(h):=DKL(Bern(Ï€T(h))âˆ¥Bern(Ï€0))=Ï€T(h)logÏ€Tâ€‹(h)Ï€0+(1âˆ’Ï€T(h))log1âˆ’Ï€Tâ€‹(h)1âˆ’Ï€0.\mathrm{IG}(h):=\mathrm{D\_{KL}}\!\left(\mathrm{Bern}(\pi\_{T}(h))\,\middle\|\,\mathrm{Bern}(\pi\_{0})\right)=\pi\_{T}(h)\log\frac{\pi\_{T}(h)}{\pi\_{0}}+(1-\pi\_{T}(h))\log\frac{1-\pi\_{T}(h)}{1-\pi\_{0}}. |  | (19) |

###### Proposition 4.3 (Properties of realized information gain).

The realized information gain satisfies:

1. 1.

   IGâ€‹(h)â‰¥0\mathrm{IG}(h)\geq 0 with equality iff Ï€Tâ€‹(h)=Ï€0\pi\_{T}(h)=\pi\_{0}.
2. 2.

   IGâ€‹(h)â‰¤maxâ¡{logâ¡(1/Ï€0),logâ¡(1/(1âˆ’Ï€0))}\mathrm{IG}(h)\leq\max\{\log(1/\pi\_{0}),\log(1/(1-\pi\_{0}))\}, with equality approached as Ï€Tâ€‹(h)â†’1\pi\_{T}(h)\to 1 or Ï€Tâ€‹(h)â†’0\pi\_{T}(h)\to 0. In particular, when Ï€0=1/2\pi\_{0}=1/2, IGâ€‹(h)â‰¤logâ¡2\mathrm{IG}(h)\leq\log 2.
3. 3.

   IGâ€‹(h)\mathrm{IG}(h) is a monotone function of |logâ¡BFTâ€‹(h)||\log\mathrm{BF}\_{T}(h)|.

##### 4.6.2 Expected Information Gain and Mutual Information

Because we treat v1:Tv\_{1:T} as a realized design, the natural expected information gain is conditional on that design.

###### Lemma 4.1 (Information gain identity (conditional on design)).

Fix v1:Tv\_{1:T} and let HT=(P0:T,V1:T)H\_{T}=(P\_{0:T},V\_{1:T}) with V1:Tâ‰¡v1:TV\_{1:T}\equiv v\_{1:T}. Then

|  |  |  |
| --- | --- | --- |
|  | ğ”¼[IG(HT)âˆ£V1:T=v1:T]=I(Y;Î”X1:T|V1:T=v1:T),\mathbb{E}\big[\mathrm{IG}(H\_{T})\mid V\_{1:T}=v\_{1:T}\big]=I\!\left(Y;\Delta X\_{1:T}\,\middle|\,V\_{1:T}=v\_{1:T}\right), |  |

where I(â‹…;â‹…âˆ£â‹…)I(\cdot;\cdot\mid\cdot) denotes conditional mutual information under the joint model induced by the prior on YY and the likelihood for Î”â€‹X1:T\Delta X\_{1:T}.

###### Proof.

Mutual information admits the representation I(Y;Zâˆ£W)=ğ”¼[DKL(p(Yâˆ£Z,W)âˆ¥p(Yâˆ£W))]I(Y;Z\mid W)=\mathbb{E}[\mathrm{D\_{KL}}(p(Y\mid Z,W)\|p(Y\mid W))]. Here WW is the fixed design, and Z=Î”â€‹X1:TZ=\Delta X\_{1:T}. For binary YY, the conditional posterior pâ€‹(Yâˆ£Z,W)p(Y\mid Z,W) is Bernoulli with parameter Ï€Tâ€‹(HT)\pi\_{T}(H\_{T}), yielding the expected KL divergence ([19](https://arxiv.org/html/2601.18815v1#S4.E19 "In 4.6.1 Realized Information Gain â€£ 4.6 Information Gain and Market-as-Sensor Metrics â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types")).
âˆ

Since YY is a single bit, Iâ€‹(Y;Î”â€‹X1:Tâˆ£V1:T)â‰¤Hâ€‹(Y)I(Y;\Delta X\_{1:T}\mid V\_{1:T})\leq H(Y), where Hâ€‹(Y)=âˆ’Ï€0â€‹logâ¡Ï€0âˆ’(1âˆ’Ï€0)â€‹logâ¡(1âˆ’Ï€0)H(Y)=-\pi\_{0}\log\pi\_{0}-(1-\pi\_{0})\log(1-\pi\_{0}). This bounds the *expected* information gain regardless of horizonâ€”once the outcome is resolved, additional observations provide diminishing returns.

##### 4.6.3 Information Destruction by Type Composition

The latent-type structure affects information gain through the induced conditional densities. When uninformed or adversarial types dominate, the separation gap shrinks and both posterior concentration and information gain weaken.

###### Definition 4.3 (Effective informativeness).

The effective informativeness of a market at volume level vv is

|  |  |  |
| --- | --- | --- |
|  | Î·â€‹(v;Î¸):=âˆ‘k=1KÏkâ€‹(v;ğ,ğœ¸)â€‹signâ€‹(mk,1â€‹(v)âˆ’mk,0â€‹(v))â€‹|mk,1â€‹(v)âˆ’mk,0â€‹(v)|.\eta(v;\theta):=\sum\_{k=1}^{K}\rho\_{k}(v;\bm{\omega},\bm{\gamma})\,\mathrm{sign}\!\bigl(m\_{k,1}(v)-m\_{k,0}(v)\bigr)\,\bigl|m\_{k,1}(v)-m\_{k,0}(v)\bigr|. |  |

When Î·â€‹(v;Î¸)>0\eta(v;\theta)>0, informed trading dominates and prices drift toward truth. When Î·â€‹(v;Î¸)â‰ˆ0\eta(v;\theta)\approx 0, the market is uninformative at volume vv. When Î·â€‹(v;Î¸)<0\eta(v;\theta)<0, manipulation dominates.

### 5 Computation and Empirical Validation

Computing â„™â€‹(Y=1âˆ£h)\mathbb{P}(Y=1\mid h) requires evaluating marginal likelihoods myâ€‹(h)m\_{y}(h) that integrate out nuisance parameters. We present two complementary approaches and validate the theory empirically.

#### 5.1 Computational Methods

##### 5.1.1 Sequential Monte Carlo

Sequential Monte Carlo (SMC) (Doucet et al., [2001](https://arxiv.org/html/2601.18815v1#bib.bib49 "Sequential Monte Carlo methods in practice"); Chopin and Papaspiliopoulos, [2020](https://arxiv.org/html/2601.18815v1#bib.bib50 "An introduction to sequential Monte Carlo")) provides a flexible framework for estimating marginal likelihoods in sequential models.

Algorithm 1  SMC for Marginal Likelihood Estimation

1:Observations Î”â€‹x1:T,v1:T\Delta x\_{1:T},v\_{1:T}, outcome yy, prior Î \Pi, particles NN

2:Initialize: Sample Î¸(i)âˆ¼Î \theta^{(i)}\sim\Pi for i=1,â€¦,Ni=1,\ldots,N, set w0(i)=1/Nw\_{0}^{(i)}=1/N

3:Set m^y=1\hat{m}\_{y}=1

4:for t=1,â€¦,Tt=1,\ldots,T do

5:â€ƒâ€‚Compute incremental weights: w~t(i)=wtâˆ’1(i)â‹…fyâ€‹(Î”â€‹xtâˆ£vt,Î¸(i))\tilde{w}\_{t}^{(i)}=w\_{t-1}^{(i)}\cdot f\_{y}(\Delta x\_{t}\mid v\_{t},\theta^{(i)})

6:â€ƒâ€‚Update normalizing constant: m^yâ†m^yâ‹…âˆ‘i=1Nw~t(i)\hat{m}\_{y}\leftarrow\hat{m}\_{y}\cdot\sum\_{i=1}^{N}\tilde{w}\_{t}^{(i)}

7:â€ƒâ€‚Normalize: wt(i)=w~t(i)/âˆ‘jw~t(j)w\_{t}^{(i)}=\tilde{w}\_{t}^{(i)}/\sum\_{j}\tilde{w}\_{t}^{(j)}

8:â€ƒâ€‚Compute ESS: ESS=1/âˆ‘i(wt(i))2\mathrm{ESS}=1/\sum\_{i}(w\_{t}^{(i)})^{2}

9:â€ƒâ€‚if ESS<N/2\mathrm{ESS}<N/2 then

10:â€ƒâ€ƒâ€ƒResample: Î¸(i)âˆ¼âˆ‘jwt(j)â€‹Î´Î¸(j)\theta^{(i)}\sim\sum\_{j}w\_{t}^{(j)}\delta\_{\theta^{(j)}}

11:â€ƒâ€ƒâ€ƒReset weights: wt(i)=1/Nw\_{t}^{(i)}=1/N

12:â€ƒâ€ƒâ€ƒRejuvenate: MCMC move on Î¸(i)\theta^{(i)} targeting Î â€‹(Î¸âˆ£Î”â€‹x1:t,v1:t,y)\Pi(\theta\mid\Delta x\_{1:t},v\_{1:t},y)

13:â€ƒâ€‚end if

14:end for

15:return m^y\hat{m}\_{y}

The Bayes factor is estimated as BF^T=m^1/m^0\widehat{\mathrm{BF}}\_{T}=\hat{m}\_{1}/\hat{m}\_{0}, running AlgorithmÂ [1](https://arxiv.org/html/2601.18815v1#alg1 "Algorithm 1 â€£ 5.1.1 Sequential Monte Carlo â€£ 5.1 Computational Methods â€£ 5 Computation and Empirical Validation â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types") separately for each outcome.

###### Proposition 5.1 (SMC consistency).

Under regularity conditions (Chopin, [2004](https://arxiv.org/html/2601.18815v1#bib.bib51 "Central limit theorem for sequential Monte Carlo methods and its application to Bayesian inference")), as Nâ†’âˆN\to\infty:

|  |  |  |
| --- | --- | --- |
|  | m^yâ†’myalmost surely,Nâ€‹(m^yâˆ’my)â‡’ğ’©â€‹(0,Ïƒy2)\hat{m}\_{y}\to m\_{y}\quad\text{almost surely},\qquad\sqrt{N}(\hat{m}\_{y}-m\_{y})\Rightarrow\mathcal{N}(0,\sigma\_{y}^{2}) |  |

for some asymptotic variance Ïƒy2\sigma\_{y}^{2}.

##### 5.1.2 Variational Inference

For large-scale applications, variational inference (Blei et al., [2017](https://arxiv.org/html/2601.18815v1#bib.bib52 "Variational inference: a review for statisticians")) provides a scalable alternative. We approximate the posterior Î â€‹(Î¸âˆ£h,y)\Pi(\theta\mid h,y) with a tractable family qâ€‹(Î¸;Ï•)q(\theta;\phi) by maximizing the evidence lower bound (ELBO):

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„’â€‹(Ï•;y)=ğ”¼qâ€‹(Î¸;Ï•)â€‹[logâ¡pâ€‹(Î”â€‹x1:Tâˆ£v1:T,y,Î¸)]âˆ’DKLâ€‹(qâ€‹(Î¸;Ï•)âˆ¥Î â€‹(Î¸)).\mathcal{L}(\phi;y)=\mathbb{E}\_{q(\theta;\phi)}[\log p(\Delta x\_{1:T}\mid v\_{1:T},y,\theta)]-\mathrm{D\_{KL}}(q(\theta;\phi)\|\Pi(\theta)). |  | (20) |

The ELBO provides a lower bound on the log marginal likelihood: â„’â€‹(Ï•;y)â‰¤logâ¡myâ€‹(h)\mathcal{L}(\phi;y)\leq\log m\_{y}(h).

For the Gaussian model (ExampleÂ [3.1](https://arxiv.org/html/2601.18815v1#S3.Thmexample1 "Example 3.1 (Gaussian latent-type model). â€£ 3.7 A Concrete Instantiation: Gaussian Latent-Type Model â€£ 3 Model and Bayesian Formulation â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types")), we use a mean-field approximation:

|  |  |  |
| --- | --- | --- |
|  | qâ€‹(Î¸;Ï•)=qâ€‹(ğ;Ï•Ï‰)â€‹âˆkqâ€‹(ğœ½k;Ï•k),q(\theta;\phi)=q(\bm{\omega};\phi\_{\omega})\prod\_{k}q(\bm{\theta}\_{k};\phi\_{k}), |  |

with Dirichlet variational distribution for ğ\bm{\omega} and Gaussian distributions for continuous parameters.

Algorithm 2  Variational Inference for Bayes Factor (approximation)

1:Observations Î”â€‹x1:T,v1:T\Delta x\_{1:T},v\_{1:T}, prior Î \Pi, learning rate Î·\eta

2:for yâˆˆ{0,1}y\in\{0,1\} do

3:â€ƒâ€‚Initialize variational parameters Ï•y\phi\_{y}

4:â€ƒâ€‚repeat

5:â€ƒâ€ƒâ€ƒSample Î¸âˆ¼qâ€‹(Î¸;Ï•y)\theta\sim q(\theta;\phi\_{y}) (reparameterization trick)

6:â€ƒâ€ƒâ€ƒCompute gradient âˆ‡Ï•yâ„’â€‹(Ï•y;y)\nabla\_{\phi\_{y}}\mathcal{L}(\phi\_{y};y)

7:â€ƒâ€ƒâ€ƒUpdate Ï•yâ†Ï•y+Î·â€‹âˆ‡Ï•yâ„’\phi\_{y}\leftarrow\phi\_{y}+\eta\nabla\_{\phi\_{y}}\mathcal{L}

8:â€ƒâ€‚until convergence

9:â€ƒâ€‚Store â„’yâˆ—=â„’â€‹(Ï•yâˆ—;y)\mathcal{L}^{\*}\_{y}=\mathcal{L}(\phi\_{y}^{\*};y)

10:end for

11:return Approximate log Bayes factor: â„’1âˆ—âˆ’â„’0âˆ—\mathcal{L}^{\*}\_{1}-\mathcal{L}^{\*}\_{0}

###### Remark 5.1 (ELBO difference is not a one-sided bound on logâ¡BFT\log\mathrm{BF}\_{T}).

While â„’â€‹(Ï•;y)â‰¤logâ¡myâ€‹(h)\mathcal{L}(\phi;y)\leq\log m\_{y}(h) for each yy, the *difference* â„’1âˆ—âˆ’â„’0âˆ—\mathcal{L}^{\*}\_{1}-\mathcal{L}^{\*}\_{0} is generally neither a lower nor an upper bound on logâ¡BFT\log\mathrm{BF}\_{T}. Indeed,

|  |  |  |
| --- | --- | --- |
|  | logmy(h)=â„’(Ï•;y)+DKL(q(Î¸;Ï•)âˆ¥Î (Î¸âˆ£h,y)),\log m\_{y}(h)=\mathcal{L}(\phi;y)+\mathrm{D\_{KL}}\!\left(q(\theta;\phi)\,\middle\|\,\Pi(\theta\mid h,y)\right), |  |

so the Bayes-factor error equals a *difference* of two nonnegative KL terms. In practice, accuracy can be assessed by (i) tighter bounds such as importance-weighted objectives, and/or (ii) post-hoc corrections via importance sampling or bridge sampling when feasible.

##### 5.1.3 Computational Complexity

Table 1: Computational complexity comparison

| Method | Time per iteration | Space |
| --- | --- | --- |
| SMC | Oâ€‹(Nâ‹…Tâ‹…K)O(N\cdot T\cdot K) | Oâ€‹(Nâ‹…d)O(N\cdot d) |
| VI (mean-field) | Oâ€‹(Sâ‹…Tâ‹…K)O(S\cdot T\cdot K) | Oâ€‹(d)O(d) |
| Laplace approximation | Oâ€‹(Tâ‹…Kâ‹…d2)O(T\cdot K\cdot d^{2}) | Oâ€‹(d2)O(d^{2}) |

Here NN is the number of particles, SS is the number of samples for gradient estimation, KK is the number of types, TT is the horizon, and d=dim(Î¸)d=\dim(\theta).

#### 5.2 Experiments

We evaluate the theory in controlled synthetic settings where the data-generating process is known, and we report summaries over repeated simulations to assess variability. Inference is performed under the same model class used for generation unless stated otherwise.

##### 5.2.1 Synthetic Experiments

###### Setup.

Data are generated from the Gaussian latent-type model of ExampleÂ [3.1](https://arxiv.org/html/2601.18815v1#S3.Thmexample1 "Example 3.1 (Gaussian latent-type model). â€£ 3.7 A Concrete Instantiation: Gaussian Latent-Type Model â€£ 3 Model and Bayesian Formulation â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types") with K=3K=3 types. Unless stated otherwise, parameters are fixed at:

* â€¢

  Informed: Î¼1=0.5\mu\_{1}=0.5, Î»1=0.1\lambda\_{1}=0.1, Ïƒ1=0.3\sigma\_{1}=0.3, Îº1=0.05\kappa\_{1}=0.05
* â€¢

  Noise: Ïƒ2=0.5\sigma\_{2}=0.5
* â€¢

  Manipulator: Î¼3=0.3\mu\_{3}=0.3, Ï„3=5\tau\_{3}=5, Ïƒ3=0.4\sigma\_{3}=0.4, Î½=5\nu=5
* â€¢

  Base weights: ğ=(0.4,0.4,0.2)\bm{\omega}=(0.4,0.4,0.2)
* â€¢

  Gating logits: Î³k,0=0\gamma\_{k,0}=0, Î³1,1=0.5\gamma\_{1,1}=0.5, Î³2,1=0\gamma\_{2,1}=0, Î³3,1=0.3\gamma\_{3,1}=0.3

Volumes are sampled i.i.d. as Vtâˆ¼Gammaâ€‹(2,0.5)V\_{t}\sim\mathrm{Gamma}(2,0.5) (shape 22, scale 0.50.5). Each configuration is replicated 1000 times. Posterior quantities are computed by SMC with N=1000N=1000 particles, targeting the joint posterior on (Y,Î¸)(Y,\theta) under the priors specified in SectionÂ [3](https://arxiv.org/html/2601.18815v1#S3 "3 Model and Bayesian Formulation â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").222In synthetic experiments we use weakly informative priors with full support on the constrained parameter space and enforce the orientation constraints described in SectionÂ [4.1](https://arxiv.org/html/2601.18815v1#S4.SS1 "4.1 Induced Data Laws and KL Projections â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types") to avoid outcomeâ€“parameter sign symmetries.

###### Experiment 1 (Posterior concentration).

We study the decay of posterior error with horizon TT. For Tâˆˆ{10,25,50,100,200,500}T\in\{10,25,50,100,200,500\}, we simulate histories under Y=1Y=1, compute Ï€T=â„™â€‹(Y=1âˆ£HT)\pi\_{T}=\mathbb{P}(Y=1\mid H\_{T}), and record the error 1âˆ’Ï€T1-\pi\_{T}. FigureÂ [1](https://arxiv.org/html/2601.18815v1#S5.F1 "Figure 1 â€£ Experiment 1 (Posterior concentration). â€£ 5.2.1 Synthetic Experiments â€£ 5.2 Experiments â€£ 5 Computation and Empirical Validation â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types") reports the median of logâ¡(1âˆ’Ï€T)\log(1-\pi\_{T}) across replications with 10â€“90% quantile bands.

To relate the empirical decay to the theory, we estimate the separation gap Î´Tâ€‹(1,Î¸â‹†)\delta\_{T}(1,\theta^{\star}) by approximately solving the KL-projection problem

|  |  |  |
| --- | --- | --- |
|  | Î´T(1,Î¸â‹†)=infÎ¸âˆˆÎ˜1TDKL(P1,Î¸â‹†(T)âˆ¥P0,Î¸(T)),\delta\_{T}(1,\theta^{\star})=\inf\_{\theta\in\Theta}\frac{1}{T}\mathrm{D\_{KL}}\!\left(P\_{1,\theta^{\star}}^{(T)}\,\middle\|\,P\_{0,\theta}^{(T)}\right), |  |

using Monte Carlo estimates of the objective and stochastic optimization (Appendix). The dashed line in FigureÂ [1](https://arxiv.org/html/2601.18815v1#S5.F1 "Figure 1 â€£ Experiment 1 (Posterior concentration). â€£ 5.2.1 Synthetic Experiments â€£ 5.2 Experiments â€£ 5 Computation and Empirical Validation â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types") overlays slope âˆ’Î´^T-\widehat{\delta}\_{T}. For the default parameters, Î´^Tâ‰ˆ1.5Ã—10âˆ’2\widehat{\delta}\_{T}\approx 1.5\times 10^{-2} nats/period, consistent with a weak per-step signal under low-to-moderate realized volumes.

![Refer to caption](fig1_concentration.png)


Figure 1: Posterior concentration under Y=1Y=1. Points show the median of logâ¡(1âˆ’Ï€T)\log(1-\pi\_{T}) across 1000 replications; the shaded region is the 10â€“90% quantile range. The dashed line has slope âˆ’Î´^T-\widehat{\delta}\_{T}, where Î´^T\widehat{\delta}\_{T} is an estimated KL-projection gap.

###### Experiment 2 (Identifiability threshold under type-composition shifts).

We quantify how inference degrades as the informed component becomes rare. We vary Ï‰1âˆˆ{0.05,0.1,0.2,0.3,0.4,0.5}\omega\_{1}\in\{0.05,0.1,0.2,0.3,0.4,0.5\}, setting Ï‰2=Ï‰3=(1âˆ’Ï‰1)/2\omega\_{2}=\omega\_{3}=(1-\omega\_{1})/2, generate histories with T=100T=100, and report the fraction of replications in which argâ€‹maxyâ¡â„™â€‹(Y=yâˆ£HT)\operatorname\*{arg\,max}\_{y}\mathbb{P}(Y=y\mid H\_{T}) matches the truth.

FigureÂ [2](https://arxiv.org/html/2601.18815v1#S5.F2 "Figure 2 â€£ Experiment 2 (Identifiability threshold under type-composition shifts). â€£ 5.2.1 Synthetic Experiments â€£ 5.2 Experiments â€£ 5 Computation and Empirical Validation â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types") shows a marked deterioration as Ï‰1\omega\_{1} falls below â‰ˆ0.15\approx 0.15. In this regime, the fitted KL-projection gap Î´^T\widehat{\delta}\_{T} becomes small, consistent with the type-composition confounding mechanism in PropositionÂ [4.1](https://arxiv.org/html/2601.18815v1#S4.Thmproposition1 "Proposition 4.1 (Sufficient conditions for outcome non-identifiability). â€£ 4.2 Mechanisms of Non-Identifiability â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").

![Refer to caption](fig2_identifiability.png)


Figure 2: Identifiability under decreasing informed weight. Posterior accuracy is the proportion of replications assigning higher posterior probability to the true outcome (here T=100T=100). Accuracy degrades sharply when Ï‰1\omega\_{1} is sufficiently small, consistent with a vanishing separation gap.

###### Experiment 3 (Stability to perturbations).

We assess the stability of posterior odds under perturbations of a typical history. For a baseline draw hh with T=100T=100, we form perturbed histories hâ€²h^{\prime} by adding i.i.d. Gaussian noise Ïµtâˆ¼ğ’©â€‹(0,Ïƒ2)\epsilon\_{t}\sim\mathcal{N}(0,\sigma^{2}) to the log-odds increments, i.e. Î”â€‹xtâ€²=Î”â€‹xt+Ïµt\Delta x^{\prime}\_{t}=\Delta x\_{t}+\epsilon\_{t}, with Ïƒâˆˆ{0.01,0.02,0.05,0.1,0.2}\sigma\in\{0.01,0.02,0.05,0.1,0.2\}. We compare the observed change in log Bayes factor, |logâ¡BFTâ€‹(h)âˆ’logâ¡BFTâ€‹(hâ€²)||\log\mathrm{BF}\_{T}(h)-\log\mathrm{BF}\_{T}(h^{\prime})|, to the stability bound of TheoremÂ [4.4](https://arxiv.org/html/2601.18815v1#S4.Thmtheorem4 "Theorem 4.4 (Stability of posterior odds on typical histories). â€£ 4.5.3 Stability Theorem â€£ 4.5 Stability of Posterior Odds under Perturbations â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types") instantiated on the truncation event ER={maxtâ¡|Î”â€‹xt|â‰¤R}E\_{R}=\{\max\_{t}|\Delta x\_{t}|\leq R\}, with RR set to the empirical 0.990.99-quantile of maxtâ¡|Î”â€‹xt|\max\_{t}|\Delta x\_{t}| across baseline draws.

FigureÂ [3](https://arxiv.org/html/2601.18815v1#S5.F3 "Figure 3 â€£ Experiment 3 (Stability to perturbations). â€£ 5.2.1 Synthetic Experiments â€£ 5.2 Experiments â€£ 5 Computation and Empirical Validation â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types") exhibits the predicted linear scaling in perturbation magnitude; the bound is conservative, as expected from worst-case control on ERE\_{R}.

![Refer to caption](fig3_stability.png)


Figure 3: Stability under perturbations of the increment path. Points show |logâ¡BFTâ€‹(h)âˆ’logâ¡BFTâ€‹(hâ€²)||\log\mathrm{BF}\_{T}(h)-\log\mathrm{BF}\_{T}(h^{\prime})| as a function of noise level Ïƒ\sigma; the dashed line is the stability bound from TheoremÂ [4.4](https://arxiv.org/html/2601.18815v1#S4.Thmtheorem4 "Theorem 4.4 (Stability of posterior odds on typical histories). â€£ 4.5.3 Stability Theorem â€£ 4.5 Stability of Posterior Odds under Perturbations â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types") evaluated with a high-probability truncation radius RR.

###### Experiment 4 (Information gain dynamics).

We track realized information gain IGâ€‹(Ht)\mathrm{IG}(H\_{t}) over long horizons. We simulate histories with T=600T=600 and compute IGâ€‹(Ht)\mathrm{IG}(H\_{t}) sequentially. FigureÂ [4](https://arxiv.org/html/2601.18815v1#S5.F4 "Figure 4 â€£ Experiment 4 (Information gain dynamics). â€£ 5.2.1 Synthetic Experiments â€£ 5.2 Experiments â€£ 5 Computation and Empirical Validation â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types") shows that IGâ€‹(Ht)\mathrm{IG}(H\_{t}) rises rapidly early in the history and then plateaus as the posterior concentrates. For the symmetric prior Ï€0=1/2\pi\_{0}=1/2, the realized information gain is bounded above by logâ¡2\log 2 nats, and the observed saturation is consistent with this ceiling.

![Refer to caption](fig4_infogain.png)


Figure 4: Information gain dynamics. Solid line: mean realized IGâ€‹(Ht)\mathrm{IG}(H\_{t}) across replications; shaded region: 10â€“90% quantile range. Dashed line: upper bound logâ¡2\log 2 for Ï€0=1/2\pi\_{0}=1/2.

### 6 Discussion

This paper develops a Bayesian inverse-problem perspective on inference from prediction market histories. The central object is the map from an unobserved binary outcome Yâˆˆ{0,1}Y\in\{0,1\} to an observed priceâ€“volume record HT=((Pt,Vt))t=0TH\_{T}=((P\_{t},V\_{t}))\_{t=0}^{T}, viewed as a stochastic sensing mechanism mediated by heterogeneous and partially strategic participation. Our modeling choice is deliberately mechanism-agnostic: we work in log-odds space and posit that increments Î”â€‹Xt\Delta X\_{t} arise from a volume-gated mixture of latent trader types. This likelihood class is flexible enough to represent informative flow, uninformed trading, heavy-tailed microstructure noise, and adversarial or manipulative pressure while requiring only the observables that are typically available in practice.

Within this framework, inference about YY reduces to the marginal Bayes factor after integrating over nuisance structure. This makes it possible to state identifiability and well-posedness in operational terms. In particular, we formulate outcome distinguishability through KL projection gaps between the outcome-indexed families after optimizing over nuisance parameters, and we show how these gaps control posterior concentration and finite-sample error behavior under regularity conditions. Complementing concentration, we analyze stability of posterior odds to perturbations of the observed history on typical paths (formally, on high-probability truncation events). Finally, by expressing information gain in terms of posterior-vs-prior KL divergence and its expectation as mutual information, we obtain interpretable metrics for treating markets as sensors and for comparing regimes in which the priceâ€“volume record is genuinely informative versus regimes in which the inverse problem is ill-posed.

At the same time, several modeling choices delimit what the current results do and do not cover. AssumptionÂ [3.1](https://arxiv.org/html/2601.18815v1#S3.Thmassumption1 "Assumption 3.1 (Conditional independence of increments). â€£ 3.6 Mixture Likelihood and Conditional Independence â€£ 3 Model and Bayesian Formulation â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types") abstracts away serial dependence such as volatility clustering, persistence in order flow, and regime switching. These effects are prominent in real markets and can matter precisely because they may imitate outcome-dependent drift or amplify the influence of rare large moves. Our observation model is also intentionally sparse: we condition only on price and volume, leaving out order book state, signed flow, open interest, and trader identities. Incorporating richer observables would likely sharpen identifiability diagnostics and reduce the extent to which nuisance structure can mimic outcome dependence. In addition, the latent-type representation fixes the number of types KK, which is useful for interpretability but potentially restrictive in heterogeneous real-world settings; model selection over KK or nonparametric mixtures could add flexibility at a computational cost. Finally, treating volume as a realized design sequence simplifies analysis but does not remove endogeneity: volume is itself shaped by information arrivals and strategic participation, and a fully coupled model for (Î”â€‹Xt,Vt)(\Delta X\_{t},V\_{t}) would be needed to explicitly quantify how endogeneity affects separation, concentration, and stability.

These limitations also point to natural extensions that preserve the inverse-problem viewpoint. One direction is to replace conditional independence with dependent increment modelsâ€”for example, Markov structure or conditionally heteroskedastic dynamicsâ€”and to adapt the concentration and robustness arguments using martingale techniques. Another is to generalize the inferential target beyond a binary outcome to categorical or continuous YY, where identifiability becomes a multi-way separation problem among outcome-conditioned families. On the computational side, sequential Monte Carlo naturally supports online inference, enabling streaming posterior updates and uncertainty quantification in live markets. Finally, the separation gap and information-gain functionals suggest a route toward market design: mechanism parameters such as fees, batching intervals, disclosure rules, or market-maker settings can be evaluated in terms of how they affect distinguishability of outcomes from observable histories, rather than only in terms of equilibrium benchmarks.

Prediction markets continue to grow in importance for forecasting and decision support, yet the relationship between dispersed information and observed prices is mediated by noise, heterogeneity, and incentives. By casting the problem as Bayesian inversion from priceâ€“volume histories, the present framework provides tools for quantifying what can be learned, for diagnosing when inference is reliable and stable, and for identifying regimes in which ambiguity is structural rather than merely a reflection of limited data.

### References

* K. J. Arrow, R. Forsythe, M. Gorham, R. Hahn, R. Hanson, J. O. Ledyard, S. Levmore, R. Litan, P. Milgrom, F. D. Nelson, et al. (2008)
  The promise of prediction markets.
  Science 320 (5878),  pp.Â 877â€“878.
  External Links: [Document](https://dx.doi.org/https%3A//doi.org/10.1126/science.1157679)
  Cited by: [Â§1](https://arxiv.org/html/2601.18815v1#S1.p1.1 "1 Introduction â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* K. Back and S. Baruch (2004)
  Information in securities markets: Kyle meets Glosten and Milgrom.
  Econometrica 72 (2),  pp.Â 433â€“465.
  External Links: [Document](https://dx.doi.org/https%3A//doi.org/10.1111/j.1468-0262.2004.00497.x)
  Cited by: [Â§2.2](https://arxiv.org/html/2601.18815v1#S2.SS2.p2.1 "2.2 Market Microstructure and Informed Trading â€£ 2 Background and Related Work â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* K. Back, C. H. Cao, and G. A. Willard (2000)
  Imperfect competition among informed traders.
  The Journal of Finance 55 (5),  pp.Â 2117â€“2155.
  External Links: [Document](https://dx.doi.org/https%3A//doi.org/10.1111/0022-1082.00282)
  Cited by: [Â§2.2](https://arxiv.org/html/2601.18815v1#S2.SS2.p2.1 "2.2 Market Microstructure and Informed Trading â€£ 2 Background and Related Work â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types"),
  [itemÂ 3](https://arxiv.org/html/2601.18815v1#S3.I1.i3.p1.1 "In 3.2 Log-Odds Coordinates â€£ 3 Model and Bayesian Formulation â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* D. M. Blei, A. Kucukelbir, and J. D. McAuliffe (2017)
  Variational inference: a review for statisticians.
  Journal of the American Statistical Association 112 (518),  pp.Â 859â€“877.
  External Links: [Document](https://dx.doi.org/https%3A//doi.org/10.1080/01621459.2017.1285773)
  Cited by: [Â§5.1.2](https://arxiv.org/html/2601.18815v1#S5.SS1.SSS2.p1.2 "5.1.2 Variational Inference â€£ 5.1 Computational Methods â€£ 5 Computation and Empirical Validation â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* S. Bubeck and N. Cesa-Bianchi (2012)
  Regret analysis of stochastic and nonstochastic multi-armed bandit problems.
  Foundations and TrendsÂ® in Machine Learning 5 (1),  pp.Â 1â€“122.
  External Links: [Document](https://dx.doi.org/https%3A//doi.org/10.1561/2200000024)
  Cited by: [Â§2.5](https://arxiv.org/html/2601.18815v1#S2.SS5.p1.1 "2.5 Learning from Strategic Data â€£ 2 Background and Related Work â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* N. Cesa-Bianchi and G. Lugosi (2006)
  Prediction, learning, and games.
   Cambridge University Press.
  External Links: [Document](https://dx.doi.org/https%3A//doi.org/10.1017/CBO9780511546921)
  Cited by: [Â§2.5](https://arxiv.org/html/2601.18815v1#S2.SS5.p1.1 "2.5 Learning from Strategic Data â€£ 2 Background and Related Work â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* Y. Chen, C. Chu, T. Mullen, and D. M. Pennock (2005)
  Information markets vs. opinion pools: an empirical comparison.
  In Proceedings of the 6th ACM Conference on Electronic Commerce,
   pp.Â 58â€“67.
  External Links: [Document](https://dx.doi.org/https%3A//doi.org/10.1145/1064009.1064016)
  Cited by: [Â§2.5](https://arxiv.org/html/2601.18815v1#S2.SS5.p1.1 "2.5 Learning from Strategic Data â€£ 2 Background and Related Work â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* Y. Chen and D. M. Pennock (2007)
  A utility framework for bounded-loss market makers.
  In Proceedings of the Twenty-Third Conference on Uncertainty in Artificial Intelligence (UAI),
   pp.Â 49â€“56.
  Cited by: [Â§2.1](https://arxiv.org/html/2601.18815v1#S2.SS1.p1.1 "2.1 Prediction Markets and Information Aggregation â€£ 2 Background and Related Work â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* N. Chopin and O. Papaspiliopoulos (2020)
  An introduction to sequential Monte Carlo.
   Springer.
  External Links: [Document](https://dx.doi.org/https%3A//doi.org/10.1007/978-3-030-47845-2)
  Cited by: [Â§5.1.1](https://arxiv.org/html/2601.18815v1#S5.SS1.SSS1.p1.1 "5.1.1 Sequential Monte Carlo â€£ 5.1 Computational Methods â€£ 5 Computation and Empirical Validation â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* N. Chopin (2004)
  Central limit theorem for sequential Monte Carlo methods and its application to Bayesian inference.
  The Annals of Statistics 32 (6),  pp.Â 2385â€“2411.
  External Links: [Document](https://dx.doi.org/https%3A//doi.org/10.1214/009053604000000698)
  Cited by: [Proposition 5.1](https://arxiv.org/html/2601.18815v1#S5.Thmproposition1.p1.1.1 "Proposition 5.1 (SMC consistency). â€£ 5.1.1 Sequential Monte Carlo â€£ 5.1 Computational Methods â€£ 5 Computation and Empirical Validation â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* M. Dashti and A. M. Stuart (2017)
  The Bayesian approach to inverse problems.
  Handbook of Uncertainty Quantification,  pp.Â 311â€“428.
  External Links: [Document](https://dx.doi.org/https%3A//doi.org/10.1007/978-3-319-12385-1%5F10)
  Cited by: [Â§2.3](https://arxiv.org/html/2601.18815v1#S2.SS3.p1.1 "2.3 Bayesian Inverse Problems â€£ 2 Background and Related Work â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* A. Doucet, N. De Freitas, and N. Gordon (2001)
  Sequential Monte Carlo methods in practice.
   Springer.
  External Links: [Document](https://dx.doi.org/https%3A//doi.org/10.1007/978-1-4757-3437-9)
  Cited by: [Â§5.1.1](https://arxiv.org/html/2601.18815v1#S5.SS1.SSS1.p1.1 "5.1.1 Sequential Monte Carlo â€£ 5.1 Computational Methods â€£ 5 Computation and Empirical Validation â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* D. Easley, R. F. Engle, M. Oâ€™Hara, and L. Wu (2008)
  Time-varying arrival rates of informed and uninformed trades.
  Journal of Financial Econometrics 6 (2),  pp.Â 171â€“207.
  External Links: [Document](https://dx.doi.org/https%3A//doi.org/10.1093/jjfinec/nbn003)
  Cited by: [Â§2.2](https://arxiv.org/html/2601.18815v1#S2.SS2.p2.1 "2.2 Market Microstructure and Informed Trading â€£ 2 Background and Related Work â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* D. Easley, S. Hvidkjaer, and M. Oâ€™Hara (2002)
  Is information risk a determinant of asset returns?.
  The Journal of Finance 57 (5),  pp.Â 2185â€“2221.
  External Links: [Document](https://dx.doi.org/https%3A//doi.org/10.1111/1540-6261.00493)
  Cited by: [Â§2.2](https://arxiv.org/html/2601.18815v1#S2.SS2.p2.1 "2.2 Market Microstructure and Informed Trading â€£ 2 Background and Related Work â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* D. Easley, N. M. Kiefer, M. Oâ€™Hara, and J. B. Paperman (1996)
  Liquidity, information, and infrequently traded stocks.
  The Journal of Finance 51 (4),  pp.Â 1405â€“1436.
  External Links: [Document](https://dx.doi.org/https%3A//doi.org/10.1111/j.1540-6261.1996.tb04074.x)
  Cited by: [Â§2.2](https://arxiv.org/html/2601.18815v1#S2.SS2.p2.1 "2.2 Market Microstructure and Informed Trading â€£ 2 Background and Related Work â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types"),
  [Â§3.4](https://arxiv.org/html/2601.18815v1#S3.SS4.p3.1 "3.4 Volume-Dependent Gating â€£ 3 Model and Bayesian Formulation â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* D. Easley, M. M. LÃ³pez de Prado, and M. Oâ€™Hara (2012)
  Flow toxicity and liquidity in a high-frequency world.
  The Review of Financial Studies 25 (5),  pp.Â 1457â€“1493.
  External Links: [Document](https://dx.doi.org/https%3A//doi.org/10.1093/rfs/hhs053)
  Cited by: [Â§2.2](https://arxiv.org/html/2601.18815v1#S2.SS2.p2.1 "2.2 Market Microstructure and Informed Trading â€£ 2 Background and Related Work â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* W. Fedus, B. Zoph, and N. Shazeer (2022)
  Switch transformers: scaling to trillion parameter models with simple and efficient sparsity.
  The Journal of Machine Learning Research 23 (1),  pp.Â 5232â€“5270.
  External Links: [Link](http://jmlr.org/papers/v23/21-0998.html)
  Cited by: [Â§2.4](https://arxiv.org/html/2601.18815v1#S2.SS4.p1.1 "2.4 Mixture Models and Mixture of Experts â€£ 2 Background and Related Work â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* S. FrÃ¼hwirth-Schnatter (2006)
  Finite mixture and Markov switching models.
   Springer Science & Business Media.
  External Links: [Document](https://dx.doi.org/https%3A//doi.org/10.1007/978-0-387-35768-3)
  Cited by: [Â§2.4](https://arxiv.org/html/2601.18815v1#S2.SS4.p1.1 "2.4 Mixture Models and Mixture of Experts â€£ 2 Background and Related Work â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* S. Ghosal, J. K. Ghosh, and A. W. Van Der Vaart (2000)
  Convergence rates of posterior distributions.
  The Annals of Statistics 28 (2),  pp.Â 500â€“531.
  External Links: [Document](https://dx.doi.org/https%3A//doi.org/10.1214/aos/1016218228)
  Cited by: [Â§2.3](https://arxiv.org/html/2601.18815v1#S2.SS3.p1.1 "2.3 Bayesian Inverse Problems â€£ 2 Background and Related Work â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* S. Ghosal and A. Van der Vaart (2017)
  Fundamentals of nonparametric Bayesian inference.
  Vol. 44, Cambridge University Press.
  External Links: [Document](https://dx.doi.org/https%3A//doi.org/10.1017/9781139029834)
  Cited by: [Â§2.3](https://arxiv.org/html/2601.18815v1#S2.SS3.p1.1 "2.3 Bayesian Inverse Problems â€£ 2 Background and Related Work â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* S. Gjerstad (2005)
  Risk aversion, beliefs, and prediction market equilibrium.
  Working Paper
   Economic Science Laboratory, University of Arizona.
  Cited by: [Â§1](https://arxiv.org/html/2601.18815v1#S1.p1.1 "1 Introduction â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* L. R. Glosten and P. R. Milgrom (1985)
  Bid, ask and transaction prices in a specialist market with heterogeneously informed traders.
  Journal of Financial Economics 14 (1),  pp.Â 71â€“100.
  External Links: [Document](https://dx.doi.org/https%3A//doi.org/10.1016/0304-405X%2885%2990044-3)
  Cited by: [Â§2.2](https://arxiv.org/html/2601.18815v1#S2.SS2.p1.1 "2.2 Market Microstructure and Informed Trading â€£ 2 Background and Related Work â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* T. Gneiting and A. E. Raftery (2007)
  Strictly proper scoring rules, prediction, and estimation.
  Journal of the American Statistical Association 102 (477),  pp.Â 359â€“378.
  External Links: [Document](https://dx.doi.org/https%3A//doi.org/10.1198/016214506000001437)
  Cited by: [Â§2.1](https://arxiv.org/html/2601.18815v1#S2.SS1.p1.1 "2.1 Prediction Markets and Information Aggregation â€£ 2 Background and Related Work â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* R. Hanson (2003)
  Combinatorial information market design.
  Information Systems Frontiers 5 (1),  pp.Â 107â€“119.
  External Links: [Document](https://dx.doi.org/https%3A//doi.org/10.1023/A%3A1022058209073)
  Cited by: [Â§2.1](https://arxiv.org/html/2601.18815v1#S2.SS1.p1.1 "2.1 Prediction Markets and Information Aggregation â€£ 2 Background and Related Work â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* R. Hanson (2007)
  Logarithmic market scoring rules for modular combinatorial information aggregation.
  The Journal of Prediction Markets 1 (1),  pp.Â 3â€“15.
  External Links: [Document](https://dx.doi.org/https%3A//doi.org/10.5750/jpm.v1i1.417)
  Cited by: [Â§2.1](https://arxiv.org/html/2601.18815v1#S2.SS1.p1.1 "2.1 Prediction Markets and Information Aggregation â€£ 2 Background and Related Work â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* M. Hardt, N. Megiddo, C. Papadimitriou, and M. Wootters (2016)
  Strategic classification.
  In Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science (ITCS),
   pp.Â 111â€“122.
  External Links: [Document](https://dx.doi.org/https%3A//doi.org/10.1145/2840728.2840730)
  Cited by: [Â§2.5](https://arxiv.org/html/2601.18815v1#S2.SS5.p1.1 "2.5 Learning from Strategic Data â€£ 2 Background and Related Work â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* F. A. Hayek (1945)
  The use of knowledge in society.
  The American Economic Review 35 (4),  pp.Â 519â€“530.
  Cited by: [Â§2.1](https://arxiv.org/html/2601.18815v1#S2.SS1.p1.1 "2.1 Prediction Markets and Information Aggregation â€£ 2 Background and Related Work â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* H. Holzmann, A. Munk, and T. Gneiting (2006)
  Identifiability of finite mixtures of elliptical distributions.
  Scandinavian Journal of Statistics 33 (4),  pp.Â 753â€“763.
  External Links: [Document](https://dx.doi.org/https%3A//doi.org/10.1111/j.1467-9469.2006.00505.x)
  Cited by: [Â§2.4](https://arxiv.org/html/2601.18815v1#S2.SS4.p2.1 "2.4 Mixture Models and Mixture of Experts â€£ 2 Background and Related Work â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types"),
  [Â§4.3](https://arxiv.org/html/2601.18815v1#S4.SS3.1.p1.13 "Proof sketch. â€£ 4.3 Sufficient Conditions for Separation in the Gaussian Model â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* K. Iyer, R. Johari, and C. C. Moallemi (2014)
  Information aggregation and allocative efficiency in smooth markets.
  Management Science 60 (10),  pp.Â 2509â€“2524.
  External Links: [Document](https://dx.doi.org/https%3A//doi.org/10.1287/mnsc.2014.1925)
  Cited by: [Â§2.1](https://arxiv.org/html/2601.18815v1#S2.SS1.p2.1 "2.1 Prediction Markets and Information Aggregation â€£ 2 Background and Related Work â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton (1991)
  Adaptive mixtures of local experts.
  Neural Computation 3 (1),  pp.Â 79â€“87.
  External Links: [Document](https://dx.doi.org/https%3A//doi.org/10.1162/neco.1991.3.1.79)
  Cited by: [Â§1](https://arxiv.org/html/2601.18815v1#S1.p2.6 "1 Introduction â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types"),
  [Â§2.4](https://arxiv.org/html/2601.18815v1#S2.SS4.p1.1 "2.4 Mixture Models and Mixture of Experts â€£ 2 Background and Related Work â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* M. I. Jordan and R. A. Jacobs (1994)
  Hierarchical mixtures of experts and the EM algorithm.
  Neural Computation 6 (2),  pp.Â 181â€“214.
  External Links: [Document](https://dx.doi.org/https%3A//doi.org/10.1162/neco.1994.6.2.181)
  Cited by: [Â§1](https://arxiv.org/html/2601.18815v1#S1.p2.6 "1 Introduction â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types"),
  [Â§2.4](https://arxiv.org/html/2601.18815v1#S2.SS4.p1.1 "2.4 Mixture Models and Mixture of Experts â€£ 2 Background and Related Work â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* J. Kaipio and E. Somersalo (2006)
  Statistical and computational inverse problems.
  Vol. 160, Springer Science & Business Media.
  External Links: [Document](https://dx.doi.org/https%3A//doi.org/10.1007/b138659)
  Cited by: [Â§2.3](https://arxiv.org/html/2601.18815v1#S2.SS3.p1.1 "2.3 Bayesian Inverse Problems â€£ 2 Background and Related Work â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* R. E. Kass and A. E. Raftery (1995)
  Bayes factors.
  Journal of the American Statistical Association 90 (430),  pp.Â 773â€“795.
  External Links: [Document](https://dx.doi.org/https%3A//doi.org/10.1080/01621459.1995.10476572)
  Cited by: [Â§2.3](https://arxiv.org/html/2601.18815v1#S2.SS3.p2.1 "2.3 Bayesian Inverse Problems â€£ 2 Background and Related Work â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* A. S. Kyle (1985)
  Continuous auctions and insider trading.
  Econometrica 53 (6),  pp.Â 1315â€“1335.
  External Links: [Document](https://dx.doi.org/https%3A//doi.org/10.2307/1913210)
  Cited by: [Â§2.2](https://arxiv.org/html/2601.18815v1#S2.SS2.p1.1 "2.2 Market Microstructure and Informed Trading â€£ 2 Background and Related Work â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types"),
  [itemÂ 3](https://arxiv.org/html/2601.18815v1#S3.I1.i3.p1.1 "In 3.2 Log-Odds Coordinates â€£ 3 Model and Bayesian Formulation â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* N. S. Lambert, J. Langford, J. Wortman, Y. Chen, D. Reeves, Y. Shoham, and D. M. Pennock (2008)
  Self-financed wagering mechanisms for forecasting.
  In Proceedings of the 9th ACM Conference on Electronic Commerce,
   pp.Â 170â€“179.
  External Links: [Document](https://dx.doi.org/https%3A//doi.org/10.1145/1386790.13868)
  Cited by: [Â§2.5](https://arxiv.org/html/2601.18815v1#S2.SS5.p1.1 "2.5 Learning from Strategic Data â€£ 2 Background and Related Work â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* J. Latz (2020)
  On the well-posedness of Bayesian inverse problems.
  SIAM/ASA Journal on Uncertainty Quantification 8 (1),  pp.Â 451â€“482.
  External Links: [Document](https://dx.doi.org/https%3A//doi.org/10.1137/19M1247176)
  Cited by: [Â§2.3](https://arxiv.org/html/2601.18815v1#S2.SS3.p1.1 "2.3 Bayesian Inverse Problems â€£ 2 Background and Related Work â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* J. P. Madrigal-Cianci (2022)
  Hierarchical Markov Chain Monte Carlo for Bayesian inverse problems.
  Ph.D. Thesis, EPFL.
  External Links: [Document](https://dx.doi.org/https%3A//doi.org/10.5075/epfl-thesis-8951)
  Cited by: [Â§2.3](https://arxiv.org/html/2601.18815v1#S2.SS3.p1.1 "2.3 Bayesian Inverse Problems â€£ 2 Background and Related Work â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* C. F. Manski (2006)
  Interpreting the predictions of prediction markets.
  Economics Letters 91 (3),  pp.Â 425â€“429.
  External Links: [Document](https://dx.doi.org/https%3A//doi.org/10.1016/j.econlet.2006.01.004)
  Cited by: [Â§1](https://arxiv.org/html/2601.18815v1#S1.p1.1 "1 Introduction â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types"),
  [Â§2.1](https://arxiv.org/html/2601.18815v1#S2.SS1.p2.1 "2.1 Prediction Markets and Information Aggregation â€£ 2 Background and Related Work â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* G. J. McLachlan and D. Peel (2000)
  Finite mixture models.
   John Wiley & Sons.
  External Links: [Document](https://dx.doi.org/https%3A//doi.org/10.1002/0471721182)
  Cited by: [Â§2.4](https://arxiv.org/html/2601.18815v1#S2.SS4.p1.1 "2.4 Mixture Models and Mixture of Experts â€£ 2 Background and Related Work â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types"),
  [Â§3.3](https://arxiv.org/html/2601.18815v1#S3.SS3.p1.2 "3.3 Latent Types as Nuisance Structure â€£ 3 Model and Bayesian Formulation â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* M. Ostrovsky (2012)
  Information aggregation in dynamic markets with strategic traders.
  Econometrica 80 (6),  pp.Â 2595â€“2647.
  External Links: [Document](https://dx.doi.org/https%3A//doi.org/10.3982/ECTA8479)
  Cited by: [Â§2.1](https://arxiv.org/html/2601.18815v1#S2.SS1.p2.1 "2.1 Prediction Markets and Information Aggregation â€£ 2 Background and Related Work â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* M. Ottaviani and P. N. SÃ¸rensen (2015)
  Price reaction to information with heterogeneous beliefs and wealth effects: underreaction, momentum, and reversal.
  American Economic Review 105 (1),  pp.Â 1â€“34.
  External Links: [Document](https://dx.doi.org/https%3A//doi.org/10.3982/ECTA8479)
  Cited by: [Â§2.1](https://arxiv.org/html/2601.18815v1#S2.SS1.p2.1 "2.1 Prediction Markets and Information Aggregation â€£ 2 Background and Related Work â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* J. Perdomo, T. Zrnic, C. Mendler-DÃ¼nner, and M. Hardt (2020)
  Performative prediction.
  In Proceedings of the 37th International Conference on Machine Learning (ICML),
  Vol. 119,  pp.Â 7599â€“7609.
  External Links: [Link](http://proceedings.mlr.press/v119/perdomo20a.html)
  Cited by: [Â§2.5](https://arxiv.org/html/2601.18815v1#S2.SS5.p1.1 "2.5 Learning from Strategic Data â€£ 2 Background and Related Work â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* L. J. Savage (1971)
  Elicitation of personal probabilities and expectations.
  Journal of the American Statistical Association 66 (336),  pp.Â 783â€“801.
  External Links: [Document](https://dx.doi.org/https%3A//doi.org/10.2307/2284229)
  Cited by: [Â§2.1](https://arxiv.org/html/2601.18815v1#S2.SS1.p1.1 "2.1 Prediction Markets and Information Aggregation â€£ 2 Background and Related Work â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean (2017)
  Outrageously large neural networks: the sparsely-gated mixture-of-experts layer.
  arXiv preprint arXiv:1701.06538.
  Cited by: [Â§2.4](https://arxiv.org/html/2601.18815v1#S2.SS4.p1.1 "2.4 Mixture Models and Mixture of Experts â€£ 2 Background and Related Work â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* A. M. Stuart (2010)
  Inverse problems: a Bayesian perspective.
  Acta Numerica 19,  pp.Â 451â€“559.
  External Links: [Document](https://dx.doi.org/https%3A//doi.org/10.1017/S0962492910000061)
  Cited by: [Â§2.3](https://arxiv.org/html/2601.18815v1#S2.SS3.p1.1 "2.3 Bayesian Inverse Problems â€£ 2 Background and Related Work â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* H. Teicher (1963)
  Identifiability of finite mixtures.
  The Annals of Mathematical Statistics 34 (4),  pp.Â 1265â€“1269.
  External Links: [Document](https://dx.doi.org/https%3A//doi.org/10.1214/aoms/1177703862)
  Cited by: [Â§2.4](https://arxiv.org/html/2601.18815v1#S2.SS4.p2.1 "2.4 Mixture Models and Mixture of Experts â€£ 2 Background and Related Work â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types"),
  [Â§4.3](https://arxiv.org/html/2601.18815v1#S4.SS3.1.p1.13 "Proof sketch. â€£ 4.3 Sufficient Conditions for Separation in the Gaussian Model â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* A. W. Van der Vaart and J. H. Van Zanten (2008)
  Rates of contraction of posterior distributions based on Gaussian process priors.
  The Annals of Statistics 36 (3),  pp.Â 1435â€“1463.
  External Links: [Document](https://dx.doi.org/https%3A//doi.org/10.1214/009053607000000613)
  Cited by: [Â§2.3](https://arxiv.org/html/2601.18815v1#S2.SS3.p1.1 "2.3 Bayesian Inverse Problems â€£ 2 Background and Related Work â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* A. W. Van der Vaart (2000)
  Asymptotic statistics.
  Vol. 3, Cambridge University Press.
  External Links: [Document](https://dx.doi.org/https%3A//doi.org/10.1017/CBO9780511802256)
  Cited by: [Â§2.3](https://arxiv.org/html/2601.18815v1#S2.SS3.p2.1 "2.3 Bayesian Inverse Problems â€£ 2 Background and Related Work â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* S. G. Walker (2004)
  New approaches to Bayesian consistency.
  The Annals of Statistics 32 (5),  pp.Â 2028â€“2043.
  External Links: [Document](https://dx.doi.org/https%3A//doi.org/10.1214/009053604000000409)
  Cited by: [Â§2.3](https://arxiv.org/html/2601.18815v1#S2.SS3.p2.1 "2.3 Bayesian Inverse Problems â€£ 2 Background and Related Work â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* J. Wolfers and E. Zitzewitz (2004)
  Prediction markets.
  Journal of Economic Perspectives 18 (2),  pp.Â 107â€“126.
  External Links: [Document](https://dx.doi.org/https%3A//doi.org/10.1257/0895330041371321)
  Cited by: [Â§1](https://arxiv.org/html/2601.18815v1#S1.p1.1 "1 Introduction â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types"),
  [Â§2.1](https://arxiv.org/html/2601.18815v1#S2.SS1.p2.1 "2.1 Prediction Markets and Information Aggregation â€£ 2 Background and Related Work â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
* S. J. Yakowitz and J. D. Spragins (1968)
  On the identifiability of finite mixtures.
  The Annals of Mathematical Statistics 39 (1),  pp.Â 209â€“214.
  External Links: [Document](https://dx.doi.org/https%3A//doi.org/10.1214/aoms/1177698520)
  Cited by: [Â§2.4](https://arxiv.org/html/2601.18815v1#S2.SS4.p2.1 "2.4 Mixture Models and Mixture of Experts â€£ 2 Background and Related Work â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").

## Appendix

### Appendix A Proofs and Additional Technical Results

This appendix collects full proofs and auxiliary technical statements supporting the results in SectionÂ [4](https://arxiv.org/html/2601.18815v1#S4 "4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
Throughout, we fix a deterministic volume design v1:Tv\_{1:T} and write Py,Î¸(T)P\_{y,\theta}^{(T)} for the induced law of Î”â€‹X1:T\Delta X\_{1:T} under outcome yy and nuisance parameter Î¸\theta.

#### A.1 Identifiability and non-identifiability

##### A.1.1 Proof of PropositionÂ [4.1](https://arxiv.org/html/2601.18815v1#S4.Thmproposition1 "Proposition 4.1 (Sufficient conditions for outcome non-identifiability). â€£ 4.2 Mechanisms of Non-Identifiability â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types")

###### Proof.

Fix a horizon TT and a realized volume design v1:Tv\_{1:T}. Recall the (truth-relative) KL gap

|  |  |  |
| --- | --- | --- |
|  | Î´T=KT(yâ‹†,Î¸â‹†â†’1âˆ’yâ‹†):=infÎ¸âˆˆÎ˜1TDKL(Pyâ‹†,Î¸â‹†(T)âˆ¥P1âˆ’yâ‹†,Î¸(T)).\delta\_{T}\;=\;K\_{T}\!\left(y^{\star},\theta^{\star}\to 1-y^{\star}\right)\;:=\;\inf\_{\theta\in\Theta}\frac{1}{T}\mathrm{D\_{KL}}\!\left(P\_{y^{\star},\theta^{\star}}^{(T)}\,\middle\|\,P\_{1-y^{\star},\theta}^{(T)}\right). |  |

By definition, non-identifiability at horizon TT for the truth (yâ‹†,Î¸â‹†)(y^{\star},\theta^{\star}) occurs whenever Î´T=0\delta\_{T}=0.

###### (1) Type-composition confounding.

Assume that along the realized volume design v1:Tv\_{1:T}, all *active* types under Î¸â‹†\theta^{\star} are outcome-independent, i.e.

|  |  |  |
| --- | --- | --- |
|  | Ïk(vt;Î¸â‹†)>0âŸ¹fk,1(â‹…âˆ£vt,Î¸kâ‹†)=fk,0(â‹…âˆ£vt,Î¸kâ‹†)for everyÂ tâˆˆ{1,â€¦,T}.\rho\_{k}(v\_{t};\theta^{\star})>0\implies f\_{k,1}(\cdot\mid v\_{t},\theta\_{k}^{\star})=f\_{k,0}(\cdot\mid v\_{t},\theta\_{k}^{\star})\qquad\text{for every }t\in\{1,\dots,T\}. |  |

Then for each tt,

|  |  |  |  |
| --- | --- | --- | --- |
|  | f1(â‹…âˆ£vt,Î¸â‹†)\displaystyle f\_{1}(\cdot\mid v\_{t},\theta^{\star}) | =âˆ‘k=1KÏk(vt;Î¸â‹†)fk,1(â‹…âˆ£vt,Î¸kâ‹†)=âˆ‘k=1KÏk(vt;Î¸â‹†)fk,0(â‹…âˆ£vt,Î¸kâ‹†)=f0(â‹…âˆ£vt,Î¸â‹†).\displaystyle=\sum\_{k=1}^{K}\rho\_{k}(v\_{t};\theta^{\star})\,f\_{k,1}(\cdot\mid v\_{t},\theta\_{k}^{\star})=\sum\_{k=1}^{K}\rho\_{k}(v\_{t};\theta^{\star})\,f\_{k,0}(\cdot\mid v\_{t},\theta\_{k}^{\star})=f\_{0}(\cdot\mid v\_{t},\theta^{\star}). |  |

By conditional independence (AssumptionÂ [3.1](https://arxiv.org/html/2601.18815v1#S3.Thmassumption1 "Assumption 3.1 (Conditional independence of increments). â€£ 3.6 Mixture Likelihood and Conditional Independence â€£ 3 Model and Bayesian Formulation â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types")), this implies equality of the induced product measures:

|  |  |  |
| --- | --- | --- |
|  | P1,Î¸â‹†(T)=P0,Î¸â‹†(T).P\_{1,\theta^{\star}}^{(T)}=P\_{0,\theta^{\star}}^{(T)}. |  |

Consequently,

|  |  |  |
| --- | --- | --- |
|  | DKL(Pyâ‹†,Î¸â‹†(T)âˆ¥P1âˆ’yâ‹†,Î¸â‹†(T))=0,\mathrm{D\_{KL}}\!\left(P\_{y^{\star},\theta^{\star}}^{(T)}\,\middle\|\,P\_{1-y^{\star},\theta^{\star}}^{(T)}\right)=0, |  |

so Î´T=0\delta\_{T}=0.

###### (2) Outcome-symmetry of the nuisance class.

Assume there exists a measurable map Î¨:Î˜â†’Î˜\Psi:\Theta\to\Theta such that, for each tt,

|  |  |  |
| --- | --- | --- |
|  | f1(â‹…âˆ£vt,Î¸)=f0(â‹…âˆ£vt,Î¨(Î¸))for allÂ Î¸âˆˆÎ˜.f\_{1}(\cdot\mid v\_{t},\theta)=f\_{0}(\cdot\mid v\_{t},\Psi(\theta))\qquad\text{for all }\theta\in\Theta. |  |

Then again by product structure,

|  |  |  |
| --- | --- | --- |
|  | P1,Î¸(T)=P0,Î¨â€‹(Î¸)(T)for allÂ â€‹Î¸âˆˆÎ˜.P\_{1,\theta}^{(T)}=P\_{0,\Psi(\theta)}^{(T)}\qquad\text{for all }\theta\in\Theta. |  |

If the truth has yâ‹†=1y^{\star}=1, choose the alternative parameter Î¸:=Î¨â€‹(Î¸â‹†)\theta:=\Psi(\theta^{\star}); then
P1,Î¸â‹†(T)=P0,Î¸(T)P\_{1,\theta^{\star}}^{(T)}=P\_{0,\theta}^{(T)} and hence Î´T=0\delta\_{T}=0.
If the truth has yâ‹†=0y^{\star}=0 and Î¨\Psi is surjective (in particular, if Î¨\Psi is an involution, Î¨âˆ˜Î¨=id\Psi\circ\Psi=\mathrm{id}), then there exists Î¸~\tilde{\theta} with Î¨â€‹(Î¸~)=Î¸â‹†\Psi(\tilde{\theta})=\theta^{\star}, so P0,Î¸â‹†(T)=P1,Î¸~(T)P\_{0,\theta^{\star}}^{(T)}=P\_{1,\tilde{\theta}}^{(T)} and again Î´T=0\delta\_{T}=0.

###### (3) Adversarial mimicry.

Assume there exists Î¸~âˆˆÎ˜\tilde{\theta}\in\Theta such that

|  |  |  |
| --- | --- | --- |
|  | Pyâ‹†,Î¸â‹†(T)=P1âˆ’yâ‹†,Î¸~(T).P\_{y^{\star},\theta^{\star}}^{(T)}=P\_{1-y^{\star},\tilde{\theta}}^{(T)}. |  |

Then

|  |  |  |
| --- | --- | --- |
|  | DKL(Pyâ‹†,Î¸â‹†(T)âˆ¥P1âˆ’yâ‹†,Î¸~(T))=0,\mathrm{D\_{KL}}\!\left(P\_{y^{\star},\theta^{\star}}^{(T)}\,\middle\|\,P\_{1-y^{\star},\tilde{\theta}}^{(T)}\right)=0, |  |

and taking the infimum over Î¸\theta yields Î´T=0\delta\_{T}=0.

In each case Î´T=0\delta\_{T}=0, so the outcome is not identifiable at horizon TT for the truth (yâ‹†,Î¸â‹†)(y^{\star},\theta^{\star}).
âˆ

##### A.1.2 Proof of TheoremÂ [4.1](https://arxiv.org/html/2601.18815v1#S4.Thmtheorem1 "Theorem 4.1 (A design-based sufficient condition for outcome separation). â€£ 4.3 Sufficient Conditions for Separation in the Gaussian Model â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types")

###### Proof.

Fix TT and a truth (yâ‹†,Î¸â‹†)(y^{\star},\theta^{\star}) satisfying the theorem hypotheses.
For each time index tâˆˆ{1,â€¦,T}t\in\{1,\dots,T\} define the *one-step* KL divergence functional

|  |  |  |
| --- | --- | --- |
|  | Îºt(Î¸):=DKL(fyâ‹†(â‹…âˆ£vt,Î¸â‹†)âˆ¥f1âˆ’yâ‹†(â‹…âˆ£vt,Î¸)),Î¸âˆˆÎ˜.\kappa\_{t}(\theta)\;:=\;\mathrm{D\_{KL}}\!\left(f\_{y^{\star}}(\cdot\mid v\_{t},\theta^{\star})\,\middle\|\,f\_{1-y^{\star}}(\cdot\mid v\_{t},\theta)\right),\qquad\theta\in\Theta. |  |

By AssumptionÂ [3.1](https://arxiv.org/html/2601.18815v1#S3.Thmassumption1 "Assumption 3.1 (Conditional independence of increments). â€£ 3.6 Mixture Likelihood and Conditional Independence â€£ 3 Model and Bayesian Formulation â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types") and additivity of KL for product measures,

|  |  |  |  |
| --- | --- | --- | --- |
|  | DKL(Pyâ‹†,Î¸â‹†(T)âˆ¥P1âˆ’yâ‹†,Î¸(T))=âˆ‘t=1TÎºt(Î¸).\mathrm{D\_{KL}}\!\left(P\_{y^{\star},\theta^{\star}}^{(T)}\,\middle\|\,P\_{1-y^{\star},\theta}^{(T)}\right)=\sum\_{t=1}^{T}\kappa\_{t}(\theta). |  | (21) |

###### Step 1: Pointwise positivity on informative times.

Let tâˆˆITt\in I\_{T} be an informative time index. We show that Îºtâ€‹(Î¸)>0\kappa\_{t}(\theta)>0 for every Î¸âˆˆÎ˜\theta\in\Theta.

On ITI\_{T}, the hypotheses impose:
(i) *informed activation* under Î¸â‹†\theta^{\star}, i.e. Ï1â€‹(vt;Î¸â‹†)â‰¥ÏÂ¯\rho\_{1}(v\_{t};\theta^{\star})\geq\underline{\rho} and
|m1,1â€‹(vt;Î¸1â‹†)âˆ’m1,0â€‹(vt;Î¸1â‹†)|â‰¥mÂ¯|m\_{1,1}(v\_{t};\theta\_{1}^{\star})-m\_{1,0}(v\_{t};\theta\_{1}^{\star})|\geq\underline{m};
(ii) *manipulator inactivity* (the type-3 drift is outcome-independent, hence contributes zero mean shift);
and (iii) *orientation constraint* Î¼1â‰¥0\mu\_{1}\geq 0 for typeÂ 1, so that m1,1â€‹(v;Î¸1)â‰¥0m\_{1,1}(v;\theta\_{1})\geq 0 and m1,0â€‹(v;Î¸1)â‰¤0m\_{1,0}(v;\theta\_{1})\leq 0 for all Î¸1\theta\_{1} and all vv.

Consider the conditional mean of the increment at time tt.
Because typeÂ 2 has zero drift by construction and typeÂ 3 has m3,1â‰¡m3,0â‰¡0m\_{3,1}\equiv m\_{3,0}\equiv 0 on ITI\_{T}, we have

|  |  |  |
| --- | --- | --- |
|  | ğ”¼fy(â‹…âˆ£vt,Î¸)â€‹[Î”â€‹Xt]=Ï1â€‹(vt;Î¸)â€‹m1,yâ€‹(vt;Î¸1)forÂ â€‹yâˆˆ{0,1}.\mathbb{E}\_{f\_{y}(\cdot\mid v\_{t},\theta)}[\Delta X\_{t}]=\rho\_{1}(v\_{t};\theta)\,m\_{1,y}(v\_{t};\theta\_{1})\qquad\text{for }y\in\{0,1\}. |  |

For the truth (yâ‹†,Î¸â‹†)(y^{\star},\theta^{\star}), the informed-drift separation implies

|  |  |  |
| --- | --- | --- |
|  | |m1,yâ‹†â€‹(vt;Î¸1â‹†)|=12â€‹|m1,1â€‹(vt;Î¸1â‹†)âˆ’m1,0â€‹(vt;Î¸1â‹†)|â‰¥mÂ¯/2,|m\_{1,y^{\star}}(v\_{t};\theta\_{1}^{\star})|=\tfrac{1}{2}|m\_{1,1}(v\_{t};\theta\_{1}^{\star})-m\_{1,0}(v\_{t};\theta\_{1}^{\star})|\geq\underline{m}/2, |  |

and hence

|  |  |  |  |
| --- | --- | --- | --- |
|  | |ğ”¼fyâ‹†(â‹…âˆ£vt,Î¸â‹†)â€‹[Î”â€‹Xt]|=Ï1â€‹(vt;Î¸â‹†)â€‹|m1,yâ‹†â€‹(vt;Î¸1â‹†)|â‰¥ÏÂ¯â€‹mÂ¯/2.\Big|\mathbb{E}\_{f\_{y^{\star}}(\cdot\mid v\_{t},\theta^{\star})}[\Delta X\_{t}]\Big|=\rho\_{1}(v\_{t};\theta^{\star})\,|m\_{1,y^{\star}}(v\_{t};\theta\_{1}^{\star})|\geq\underline{\rho}\,\underline{m}/2. |  | (22) |

Moreover, the sign of this mean is signâ¡(2â€‹yâ‹†âˆ’1)\operatorname{sign}(2y^{\star}-1).

Now fix any Î¸âˆˆÎ˜\theta\in\Theta and consider the wrong-outcome density f1âˆ’yâ‹†(â‹…âˆ£vt,Î¸)f\_{1-y^{\star}}(\cdot\mid v\_{t},\theta).
By the orientation constraint, m1,1âˆ’yâ‹†â€‹(vt;Î¸1)m\_{1,1-y^{\star}}(v\_{t};\theta\_{1}) has sign *opposite* to m1,yâ‹†â€‹(vt;Î¸1â‹†)m\_{1,y^{\star}}(v\_{t};\theta\_{1}^{\star}) (or is zero), and Ï1â€‹(vt;Î¸)â‰¥0\rho\_{1}(v\_{t};\theta)\geq 0.
Therefore,

|  |  |  |
| --- | --- | --- |
|  | signâ¡(ğ”¼f1âˆ’yâ‹†(â‹…âˆ£vt,Î¸)â€‹[Î”â€‹Xt])âˆˆ{0,âˆ’signâ¡(2â€‹yâ‹†âˆ’1)}.\operatorname{sign}\Big(\mathbb{E}\_{f\_{1-y^{\star}}(\cdot\mid v\_{t},\theta)}[\Delta X\_{t}]\Big)\in\{0,-\operatorname{sign}(2y^{\star}-1)\}. |  |

In particular, the conditional means under fyâ‹†(â‹…âˆ£vt,Î¸â‹†)f\_{y^{\star}}(\cdot\mid v\_{t},\theta^{\star}) and f1âˆ’yâ‹†(â‹…âˆ£vt,Î¸)f\_{1-y^{\star}}(\cdot\mid v\_{t},\theta) cannot coincide:

|  |  |  |
| --- | --- | --- |
|  | ğ”¼fyâ‹†(â‹…âˆ£vt,Î¸â‹†)â€‹[Î”â€‹Xt]â‰ ğ”¼f1âˆ’yâ‹†(â‹…âˆ£vt,Î¸)â€‹[Î”â€‹Xt].\mathbb{E}\_{f\_{y^{\star}}(\cdot\mid v\_{t},\theta^{\star})}[\Delta X\_{t}]\neq\mathbb{E}\_{f\_{1-y^{\star}}(\cdot\mid v\_{t},\theta)}[\Delta X\_{t}]. |  |

Since equality of densities a.e. would imply equality of their first moments, we conclude that
fyâ‹†(â‹…âˆ£vt,Î¸â‹†)â‰ f1âˆ’yâ‹†(â‹…âˆ£vt,Î¸)f\_{y^{\star}}(\cdot\mid v\_{t},\theta^{\star})\neq f\_{1-y^{\star}}(\cdot\mid v\_{t},\theta) a.e., hence

|  |  |  |
| --- | --- | --- |
|  | Îºt(Î¸)=DKL(fyâ‹†(â‹…âˆ£vt,Î¸â‹†)âˆ¥f1âˆ’yâ‹†(â‹…âˆ£vt,Î¸))>0for allÂ Î¸âˆˆÎ˜.\kappa\_{t}(\theta)=\mathrm{D\_{KL}}\!\left(f\_{y^{\star}}(\cdot\mid v\_{t},\theta^{\star})\,\middle\|\,f\_{1-y^{\star}}(\cdot\mid v\_{t},\theta)\right)>0\qquad\text{for all }\theta\in\Theta. |  |

###### Step 2: Uniform positivity via compactness.

For each fixed tâˆˆITt\in I\_{T}, the map Î¸â†¦Îºtâ€‹(Î¸)\theta\mapsto\kappa\_{t}(\theta) is continuous on Î˜\Theta under the regularity conditions in itemÂ (3) of TheoremÂ [4.1](https://arxiv.org/html/2601.18815v1#S4.Thmtheorem1 "Theorem 4.1 (A design-based sufficient condition for outcome separation). â€£ 4.3 Sufficient Conditions for Separation in the Gaussian Model â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types") (bounded nondegenerate scales and smoothness of the component families).
Since Î˜\Theta is compact, Îºt\kappa\_{t} attains its minimum:

|  |  |  |
| --- | --- | --- |
|  | ÎºÂ¯t:=minÎ¸âˆˆÎ˜â¡Îºtâ€‹(Î¸).\underline{\kappa}\_{t}\;:=\;\min\_{\theta\in\Theta}\kappa\_{t}(\theta). |  |

By StepÂ 1, ÎºÂ¯t>0\underline{\kappa}\_{t}>0 for every tâˆˆITt\in I\_{T}.
Define

|  |  |  |
| --- | --- | --- |
|  | Îº:=mintâˆˆITâ¡ÎºÂ¯t,\kappa\;:=\;\min\_{t\in I\_{T}}\underline{\kappa}\_{t}, |  |

which is strictly positive because ITI\_{T} is finite for fixed TT.

###### Step 3: Lower bound on the KL gap.

Combining ([21](https://arxiv.org/html/2601.18815v1#A1.E21 "In Proof. â€£ A.1.2 Proof of Theorem 4.1 â€£ A.1 Identifiability and non-identifiability â€£ Appendix A Proofs and Additional Technical Results â€£ Appendix â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types")) with the definition of Îº\kappa yields, for any Î¸âˆˆÎ˜\theta\in\Theta,

|  |  |  |
| --- | --- | --- |
|  | 1TDKL(Pyâ‹†,Î¸â‹†(T)âˆ¥P1âˆ’yâ‹†,Î¸(T))=1Tâˆ‘t=1TÎºt(Î¸)â‰¥1Tâˆ‘tâˆˆITÎºt(Î¸)â‰¥|IT|TÎº.\frac{1}{T}\mathrm{D\_{KL}}\!\left(P\_{y^{\star},\theta^{\star}}^{(T)}\,\middle\|\,P\_{1-y^{\star},\theta}^{(T)}\right)=\frac{1}{T}\sum\_{t=1}^{T}\kappa\_{t}(\theta)\geq\frac{1}{T}\sum\_{t\in I\_{T}}\kappa\_{t}(\theta)\geq\frac{|I\_{T}|}{T}\,\kappa. |  |

Taking the infimum over Î¸\theta gives

|  |  |  |
| --- | --- | --- |
|  | Î´T=infÎ¸âˆˆÎ˜1TDKL(Pyâ‹†,Î¸â‹†(T)âˆ¥P1âˆ’yâ‹†,Î¸(T))â‰¥|IT|TÎº,\delta\_{T}=\inf\_{\theta\in\Theta}\frac{1}{T}\mathrm{D\_{KL}}\!\left(P\_{y^{\star},\theta^{\star}}^{(T)}\,\middle\|\,P\_{1-y^{\star},\theta}^{(T)}\right)\geq\frac{|I\_{T}|}{T}\,\kappa, |  |

which is the desired bound.
âˆ

#### A.2 Bayes factors and posterior concentration

##### A.2.1 Proof of TheoremÂ [4.2](https://arxiv.org/html/2601.18815v1#S4.Thmtheorem2 "Theorem 4.2 (Asymptotic separation of Bayes factors (robust form)). â€£ 4.4.2 Bayes Factor Separation â€£ 4.4 Posterior Concentration and Finite-Sample Error Bounds â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types")

###### Proof.

Fix Ïµ>0\epsilon>0 and work on an event of probability one on which the uniform law of large numbers in AssumptionÂ [4.4](https://arxiv.org/html/2601.18815v1#S4.Thmassumption4 "Assumption 4.4 (Uniform law of large numbers). â€£ 4.4.2 Bayes Factor Separation â€£ 4.4 Posterior Concentration and Finite-Sample Error Bounds â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types") holds for both y=0y=0 and y=1y=1.

For yâˆˆ{0,1}y\in\{0,1\} define the empirical and population per-time log-likelihoods

|  |  |  |
| --- | --- | --- |
|  | Ly,Tâ€‹(Î¸):=1Tâ€‹â„“y,Tâ€‹(Î¸)=1Tâ€‹âˆ‘t=1Tlogâ¡fyâ€‹(Î”â€‹Xtâˆ£vt,Î¸),Î›y,Tâ€‹(Î¸):=1Tâ€‹ğ”¼Pyâ‹†,Î¸â‹†(T)â€‹[â„“y,Tâ€‹(Î¸)].L\_{y,T}(\theta):=\frac{1}{T}\,\ell\_{y,T}(\theta)=\frac{1}{T}\sum\_{t=1}^{T}\log f\_{y}(\Delta X\_{t}\mid v\_{t},\theta),\qquad\Lambda\_{y,T}(\theta):=\frac{1}{T}\,\mathbb{E}\_{P\_{y^{\star},\theta^{\star}}^{(T)}}[\ell\_{y,T}(\theta)]. |  |

Write

|  |  |  |
| --- | --- | --- |
|  | aT:=maxyâˆˆ{0,1}â€‹supÎ¸âˆˆÎ˜|Ly,Tâ€‹(Î¸)âˆ’Î›y,Tâ€‹(Î¸)|.a\_{T}:=\max\_{y\in\{0,1\}}\sup\_{\theta\in\Theta}\big|L\_{y,T}(\theta)-\Lambda\_{y,T}(\theta)\big|. |  |

By AssumptionÂ [4.4](https://arxiv.org/html/2601.18815v1#S4.Thmassumption4 "Assumption 4.4 (Uniform law of large numbers). â€£ 4.4.2 Bayes Factor Separation â€£ 4.4 Posterior Concentration and Finite-Sample Error Bounds â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types"), aTâ†’0a\_{T}\to 0 almost surely.

###### Step 1: Laplace-type bounds for 1Tâ€‹logâ¡myâ€‹(HT)\frac{1}{T}\log m\_{y}(H\_{T}).

Recall

|  |  |  |
| --- | --- | --- |
|  | myâ€‹(HT)=âˆ«Î˜expâ¡(Tâ€‹Ly,Tâ€‹(Î¸))â€‹Î â€‹(dâ€‹Î¸).m\_{y}(H\_{T})=\int\_{\Theta}\exp\!\big(T\,L\_{y,T}(\theta)\big)\,\Pi(d\theta). |  |

*Upper bound.*
Since Î \Pi is a probability measure,

|  |  |  |
| --- | --- | --- |
|  | myâ€‹(HT)â‰¤supÎ¸âˆˆÎ˜expâ¡(Tâ€‹Ly,Tâ€‹(Î¸))=expâ¡(Tâ€‹supÎ¸âˆˆÎ˜Ly,Tâ€‹(Î¸)),m\_{y}(H\_{T})\leq\sup\_{\theta\in\Theta}\exp\!\big(T\,L\_{y,T}(\theta)\big)=\exp\!\Big(T\,\sup\_{\theta\in\Theta}L\_{y,T}(\theta)\Big), |  |

so

|  |  |  |
| --- | --- | --- |
|  | 1Tâ€‹logâ¡myâ€‹(HT)â‰¤supÎ¸âˆˆÎ˜Ly,Tâ€‹(Î¸)â‰¤supÎ¸âˆˆÎ˜Î›y,Tâ€‹(Î¸)+aT.\frac{1}{T}\log m\_{y}(H\_{T})\leq\sup\_{\theta\in\Theta}L\_{y,T}(\theta)\leq\sup\_{\theta\in\Theta}\Lambda\_{y,T}(\theta)+a\_{T}. |  |

*Lower bound.*
Fix Î·>0\eta>0 and define the near-maximizer set

|  |  |  |
| --- | --- | --- |
|  | Ay,Tâ€‹(Î·):={Î¸âˆˆÎ˜:Î›y,Tâ€‹(Î¸)â‰¥supÏ‘âˆˆÎ˜Î›y,Tâ€‹(Ï‘)âˆ’Î·}.A\_{y,T}(\eta):=\Big\{\theta\in\Theta:\ \Lambda\_{y,T}(\theta)\geq\sup\_{\vartheta\in\Theta}\Lambda\_{y,T}(\vartheta)-\eta\Big\}. |  |

Because Î›y,Tâ€‹(Î¸)=Î›yâ‹†,Tâ€‹(Î¸â‹†)âˆ’1Tâ€‹DKLâ€‹(Pyâ‹†,Î¸â‹†(T)âˆ¥Py,Î¸(T))\Lambda\_{y,T}(\theta)=\Lambda\_{y^{\star},T}(\theta^{\star})-\frac{1}{T}\mathrm{D\_{KL}}(P\_{y^{\star},\theta^{\star}}^{(T)}\|P\_{y,\theta}^{(T)}), the set Ay,Tâ€‹(Î·)A\_{y,T}(\eta) coincides with the â€œnear-projectionâ€ set in AssumptionÂ [4.3](https://arxiv.org/html/2601.18815v1#S4.Thmassumption3 "Assumption 4.3 (KL support at (near) projections). â€£ 4.4.1 Regularity Conditions â€£ 4.4 Posterior Concentration and Finite-Sample Error Bounds â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types"). Hence Î â€‹(Ay,Tâ€‹(Î·))â‰¥câ€‹(Î·)>0\Pi(A\_{y,T}(\eta))\geq c(\eta)>0 for TT large enough.

Then

|  |  |  |  |
| --- | --- | --- | --- |
|  | myâ€‹(HT)\displaystyle m\_{y}(H\_{T}) | â‰¥âˆ«Ay,Tâ€‹(Î·)expâ¡(Tâ€‹Ly,Tâ€‹(Î¸))â€‹Î â€‹(dâ€‹Î¸)\displaystyle\geq\int\_{A\_{y,T}(\eta)}\exp\!\big(T\,L\_{y,T}(\theta)\big)\,\Pi(d\theta) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¥Î â€‹(Ay,Tâ€‹(Î·))â€‹expâ¡(Tâ€‹infÎ¸âˆˆAy,Tâ€‹(Î·)Ly,Tâ€‹(Î¸)).\displaystyle\geq\Pi(A\_{y,T}(\eta))\,\exp\!\Big(T\,\inf\_{\theta\in A\_{y,T}(\eta)}L\_{y,T}(\theta)\Big). |  |

Using |Ly,Tâˆ’Î›y,T|â‰¤aT|L\_{y,T}-\Lambda\_{y,T}|\leq a\_{T},

|  |  |  |
| --- | --- | --- |
|  | infÎ¸âˆˆAy,Tâ€‹(Î·)Ly,Tâ€‹(Î¸)â‰¥infÎ¸âˆˆAy,Tâ€‹(Î·)Î›y,Tâ€‹(Î¸)âˆ’aTâ‰¥supÎ¸âˆˆÎ˜Î›y,Tâ€‹(Î¸)âˆ’Î·âˆ’aT.\inf\_{\theta\in A\_{y,T}(\eta)}L\_{y,T}(\theta)\geq\inf\_{\theta\in A\_{y,T}(\eta)}\Lambda\_{y,T}(\theta)-a\_{T}\geq\sup\_{\theta\in\Theta}\Lambda\_{y,T}(\theta)-\eta-a\_{T}. |  |

Therefore

|  |  |  |
| --- | --- | --- |
|  | 1Tâ€‹logâ¡myâ€‹(HT)â‰¥supÎ¸âˆˆÎ˜Î›y,Tâ€‹(Î¸)âˆ’Î·âˆ’aT+1Tâ€‹logâ¡Î â€‹(Ay,Tâ€‹(Î·)).\frac{1}{T}\log m\_{y}(H\_{T})\geq\sup\_{\theta\in\Theta}\Lambda\_{y,T}(\theta)-\eta-a\_{T}+\frac{1}{T}\log\Pi(A\_{y,T}(\eta)). |  |

###### Step 2: Relating suprema to Î´T\delta\_{T}.

First note that supÎ¸Î›yâ‹†,Tâ€‹(Î¸)=Î›yâ‹†,Tâ€‹(Î¸â‹†)\sup\_{\theta}\Lambda\_{y^{\star},T}(\theta)=\Lambda\_{y^{\star},T}(\theta^{\star}) because
DKLâ€‹(Pyâ‹†,Î¸â‹†(T)âˆ¥Pyâ‹†,Î¸(T))â‰¥0\mathrm{D\_{KL}}(P\_{y^{\star},\theta^{\star}}^{(T)}\|P\_{y^{\star},\theta}^{(T)})\geq 0 with equality at Î¸=Î¸â‹†\theta=\theta^{\star}.

Moreover,

|  |  |  |
| --- | --- | --- |
|  | supÎ¸âˆˆÎ˜Î›1âˆ’yâ‹†,Tâ€‹(Î¸)=Î›yâ‹†,Tâ€‹(Î¸â‹†)âˆ’Î´T,\sup\_{\theta\in\Theta}\Lambda\_{1-y^{\star},T}(\theta)=\Lambda\_{y^{\star},T}(\theta^{\star})-\delta\_{T}, |  |

because

|  |  |  |
| --- | --- | --- |
|  | Î›yâ‹†,T(Î¸â‹†)âˆ’Î›1âˆ’yâ‹†,T(Î¸)=1TDKL(Pyâ‹†,Î¸â‹†(T)âˆ¥P1âˆ’yâ‹†,Î¸(T))\Lambda\_{y^{\star},T}(\theta^{\star})-\Lambda\_{1-y^{\star},T}(\theta)=\frac{1}{T}\mathrm{D\_{KL}}\!\left(P\_{y^{\star},\theta^{\star}}^{(T)}\,\middle\|\,P\_{1-y^{\star},\theta}^{(T)}\right) |  |

and Î´T\delta\_{T} is the infimum of the right-hand side over Î¸\theta.

###### Step 3: Conclude the Bayes-factor bounds.

Apply the bounds from StepÂ 1 with Î·=Ïµ/4\eta=\epsilon/4 and take TT large enough so that aTâ‰¤Ïµ/4a\_{T}\leq\epsilon/4 and 1Tâ€‹|logâ¡Î â€‹(Ay,Tâ€‹(Î·))|â‰¤Ïµ/4\frac{1}{T}|\log\Pi(A\_{y,T}(\eta))|\leq\epsilon/4 (the latter holds because Î â€‹(Ay,Tâ€‹(Î·))â‰¥câ€‹(Î·)>0\Pi(A\_{y,T}(\eta))\geq c(\eta)>0).

For y=yâ‹†y=y^{\star} we obtain

|  |  |  |
| --- | --- | --- |
|  | 1Tâ€‹logâ¡myâ‹†â€‹(HT)â‰¥Î›yâ‹†,Tâ€‹(Î¸â‹†)âˆ’Ïµ/2,1Tâ€‹logâ¡myâ‹†â€‹(HT)â‰¤Î›yâ‹†,Tâ€‹(Î¸â‹†)+Ïµ/4.\frac{1}{T}\log m\_{y^{\star}}(H\_{T})\geq\Lambda\_{y^{\star},T}(\theta^{\star})-\epsilon/2,\qquad\frac{1}{T}\log m\_{y^{\star}}(H\_{T})\leq\Lambda\_{y^{\star},T}(\theta^{\star})+\epsilon/4. |  |

For y=1âˆ’yâ‹†y=1-y^{\star} we obtain

|  |  |  |
| --- | --- | --- |
|  | 1Tâ€‹logâ¡m1âˆ’yâ‹†â€‹(HT)â‰¤supÎ¸Î›1âˆ’yâ‹†,Tâ€‹(Î¸)+Ïµ/4=Î›yâ‹†,Tâ€‹(Î¸â‹†)âˆ’Î´T+Ïµ/4,\frac{1}{T}\log m\_{1-y^{\star}}(H\_{T})\leq\sup\_{\theta}\Lambda\_{1-y^{\star},T}(\theta)+\epsilon/4=\Lambda\_{y^{\star},T}(\theta^{\star})-\delta\_{T}+\epsilon/4, |  |

and

|  |  |  |
| --- | --- | --- |
|  | 1Tâ€‹logâ¡m1âˆ’yâ‹†â€‹(HT)â‰¥supÎ¸Î›1âˆ’yâ‹†,Tâ€‹(Î¸)âˆ’Ïµ/2=Î›yâ‹†,Tâ€‹(Î¸â‹†)âˆ’Î´Tâˆ’Ïµ/2.\frac{1}{T}\log m\_{1-y^{\star}}(H\_{T})\geq\sup\_{\theta}\Lambda\_{1-y^{\star},T}(\theta)-\epsilon/2=\Lambda\_{y^{\star},T}(\theta^{\star})-\delta\_{T}-\epsilon/2. |  |

Subtracting yields, for all sufficiently large TT,

|  |  |  |
| --- | --- | --- |
|  | 1Tâ€‹logâ¡BFTâ€‹(HT)=1Tâ€‹logâ¡m1â€‹(HT)âˆ’1Tâ€‹logâ¡m0â€‹(HT)â‰¥Î´Tâˆ’Ïµ\frac{1}{T}\log\mathrm{BF}\_{T}(H\_{T})=\frac{1}{T}\log m\_{1}(H\_{T})-\frac{1}{T}\log m\_{0}(H\_{T})\geq\delta\_{T}-\epsilon |  |

if yâ‹†=1y^{\star}=1, and similarly the corresponding lower bound on âˆ’1Tâ€‹logâ¡BFTâ€‹(HT)-\frac{1}{T}\log\mathrm{BF}\_{T}(H\_{T}) if yâ‹†=0y^{\star}=0.
The stated upper bounds follow by subtracting the corresponding upper and lower bounds in the opposite direction (which is why the theorem states the upper bound only for the â€œcorrectly orientedâ€ Bayes factor).
âˆ

##### A.2.2 Proof of PropositionÂ [4.2](https://arxiv.org/html/2601.18815v1#S4.Thmproposition2 "Proposition 4.2 (Finite-sample posterior error bound (robust form)). â€£ 4.4.4 Finite-Sample Error Bounds via Truncation â€£ 4.4 Posterior Concentration and Finite-Sample Error Bounds â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types")

###### Proof.

We provide a finite-sample bound by combining (i) a truncation argument ensuring bounded log-likelihood ratios on a high-probability event and (ii) an exponential deviation bound for a uniform LLN event.

###### Step 1: Truncation and a bounded-ratio event.

Fix R>0R>0 and recall

|  |  |  |
| --- | --- | --- |
|  | ER:={max1â‰¤tâ‰¤Tâ¡|Î”â€‹Xt|â‰¤R}.E\_{R}:=\Big\{\max\_{1\leq t\leq T}|\Delta X\_{t}|\leq R\Big\}. |  |

By AssumptionÂ [4.5](https://arxiv.org/html/2601.18815v1#S4.Thmassumption5 "Assumption 4.5 (Moment control for increments). â€£ 4.4.4 Finite-Sample Error Bounds via Truncation â€£ 4.4 Posterior Concentration and Finite-Sample Error Bounds â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types"), Markovâ€™s inequality and a union bound give

|  |  |  |
| --- | --- | --- |
|  | â„™â€‹(ERc)â‰¤âˆ‘t=1Tâ„™â€‹(|Î”â€‹Xt|>R)â‰¤âˆ‘t=1Tğ”¼â€‹|Î”â€‹Xt|qRqâ‰¤Tâ€‹Cqâ€‹Râˆ’q.\mathbb{P}(E\_{R}^{c})\leq\sum\_{t=1}^{T}\mathbb{P}(|\Delta X\_{t}|>R)\leq\sum\_{t=1}^{T}\frac{\mathbb{E}|\Delta X\_{t}|^{q}}{R^{q}}\leq T\,C\_{q}\,R^{-q}. |  |

On ERE\_{R}, the constant

|  |  |  |
| --- | --- | --- |
|  | BR:=supÎ¸âˆˆÎ˜suptâ‰¤Tsup|x|â‰¤R|logâ¡fyâ‹†â€‹(xâˆ£vt,Î¸â‹†)f1âˆ’yâ‹†â€‹(xâˆ£vt,Î¸)|B\_{R}:=\sup\_{\theta\in\Theta}\ \sup\_{t\leq T}\ \sup\_{|x|\leq R}\left|\log\frac{f\_{y^{\star}}(x\mid v\_{t},\theta^{\star})}{f\_{1-y^{\star}}(x\mid v\_{t},\theta)}\right| |  |

is finite by compactness of Î˜\Theta, continuity of (x,Î¸)â†¦fyâ€‹(xâˆ£vt,Î¸)(x,\theta)\mapsto f\_{y}(x\mid v\_{t},\theta), and strict positivity of the component families for locationâ€“scale models with nondegenerate scales.

###### Step 2: A deviation event controlling the Bayes factor.

Fix Ïµâˆˆ(0,Î´T)\epsilon\in(0,\delta\_{T}) and define Î·:=Ïµ/4\eta:=\epsilon/4.
Let ATâ€‹(Î·)A\_{T}(\eta) denote the event that the Laplace bounds in the proof of TheoremÂ [4.2](https://arxiv.org/html/2601.18815v1#S4.Thmtheorem2 "Theorem 4.2 (Asymptotic separation of Bayes factors (robust form)). â€£ 4.4.2 Bayes Factor Separation â€£ 4.4 Posterior Concentration and Finite-Sample Error Bounds â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types") hold with error at most Î·\eta for both outcomes y=0,1y=0,1.
(Concretely, ATâ€‹(Î·)A\_{T}(\eta) is implied by the uniform LLN event maxyâ€‹supÎ¸|Ly,Tâ€‹(Î¸)âˆ’Î›y,Tâ€‹(Î¸)|â‰¤Î·\max\_{y}\sup\_{\theta}|L\_{y,T}(\theta)-\Lambda\_{y,T}(\theta)|\leq\eta together with 1Tâ€‹|logâ¡Î â€‹(Ay,Tâ€‹(Î·))|â‰¤Î·\frac{1}{T}|\log\Pi(A\_{y,T}(\eta))|\leq\eta, which holds for TT large by AssumptionÂ [4.3](https://arxiv.org/html/2601.18815v1#S4.Thmassumption3 "Assumption 4.3 (KL support at (near) projections). â€£ 4.4.1 Regularity Conditions â€£ 4.4 Posterior Concentration and Finite-Sample Error Bounds â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").)

On ATâ€‹(Î·)A\_{T}(\eta) we have, by the same subtraction as in TheoremÂ [4.2](https://arxiv.org/html/2601.18815v1#S4.Thmtheorem2 "Theorem 4.2 (Asymptotic separation of Bayes factors (robust form)). â€£ 4.4.2 Bayes Factor Separation â€£ 4.4 Posterior Concentration and Finite-Sample Error Bounds â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types"),

|  |  |  |
| --- | --- | --- |
|  | 1Tâ€‹logâ¡BFTâ€‹(HT)â‰¥Î´Tâˆ’4â€‹Î·=Î´Tâˆ’ÏµifÂ â€‹yâ‹†=1,\frac{1}{T}\log\mathrm{BF}\_{T}(H\_{T})\geq\delta\_{T}-4\eta=\delta\_{T}-\epsilon\qquad\text{if }y^{\star}=1, |  |

and analogously âˆ’1Tâ€‹logâ¡BFTâ€‹(HT)â‰¥Î´Tâˆ’Ïµ-\frac{1}{T}\log\mathrm{BF}\_{T}(H\_{T})\geq\delta\_{T}-\epsilon if yâ‹†=0y^{\star}=0.
In either case, |logâ¡BFTâ€‹(HT)|â‰¥Tâ€‹(Î´Tâˆ’Ïµ)|\log\mathrm{BF}\_{T}(H\_{T})|\geq T(\delta\_{T}-\epsilon) with the correct sign.

Since posterior odds satisfy ([7](https://arxiv.org/html/2601.18815v1#S3.E7 "In 3.8 Prior and Posterior â€£ 3 Model and Bayesian Formulation â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types")), this implies an exponential bound on posterior error on ATâ€‹(Î·)A\_{T}(\eta):

|  |  |  |
| --- | --- | --- |
|  | â„™â€‹(Yâ‰ yâ‹†âˆ£HT)â‰¤1âˆ’Ï€0Ï€0â€‹expâ¡(âˆ’Tâ€‹(Î´Tâˆ’Ïµ))ifÂ â€‹yâ‹†=1,\mathbb{P}(Y\neq y^{\star}\mid H\_{T})\leq\frac{1-\pi\_{0}}{\pi\_{0}}\exp\!\big(-T(\delta\_{T}-\epsilon)\big)\qquad\text{if }y^{\star}=1, |  |

and the analogous bound with Ï€0/(1âˆ’Ï€0)\pi\_{0}/(1-\pi\_{0}) if yâ‹†=0y^{\star}=0.
Absorbing the constant prefactor into the exponential for large TT yields the stated form (since (1/T)â€‹logâ¡1âˆ’Ï€0Ï€0â†’0(1/T)\log\frac{1-\pi\_{0}}{\pi\_{0}}\to 0).

Therefore, for TT large enough,

|  |  |  |
| --- | --- | --- |
|  | {â„™â€‹(Yâ‰ yâ‹†âˆ£HT)â‰¥eâˆ’Tâ€‹(Î´Tâˆ’Ïµ)}âŠ†ATâ€‹(Î·)c.\Big\{\mathbb{P}(Y\neq y^{\star}\mid H\_{T})\geq e^{-T(\delta\_{T}-\epsilon)}\Big\}\subseteq A\_{T}(\eta)^{c}. |  |

###### Step 3: Exponential control of ATâ€‹(Î·)cA\_{T}(\eta)^{c} on ERE\_{R}.

To obtain an explicit tail, it suffices to control the uniform LLN deviation probability on ERE\_{R}.
Under bounded-ratio control on ERE\_{R}, standard empirical-process arguments (e.g. Îµ\varepsilon-net discretization of Î˜\Theta combined with Hoeffdingâ€™s inequality and Lipschitz dependence of logfy(â‹…âˆ£vt,Î¸)\log f\_{y}(\cdot\mid v\_{t},\theta) in Î¸\theta over compact Î˜\Theta) yield an exponential bound of the form

|  |  |  |
| --- | --- | --- |
|  | â„™â€‹(ATâ€‹(Î·)câˆ©ER)â‰¤expâ¡(âˆ’Tâ€‹Ïµ22â€‹BR2),\mathbb{P}\big(A\_{T}(\eta)^{c}\cap E\_{R}\big)\leq\exp\!\Big(-\frac{T\epsilon^{2}}{2B\_{R}^{2}}\Big), |  |

up to a subexponential prefactor depending on the metric entropy of Î˜\Theta.
For notational simplicity (and consistent with the main text statement), we absorb such prefactors into the leading exponential rate.

###### Step 4: Combine.

Finally,

|  |  |  |
| --- | --- | --- |
|  | â„™â€‹(â„™â€‹(Yâ‰ yâ‹†âˆ£HT)â‰¥eâˆ’Tâ€‹(Î´Tâˆ’Ïµ))â‰¤â„™â€‹(ATâ€‹(Î·)câˆ©ER)+â„™â€‹(ERc)â‰¤expâ¡(âˆ’Tâ€‹Ïµ22â€‹BR2)+â„™â€‹(ERc),\mathbb{P}\!\left(\mathbb{P}(Y\neq y^{\star}\mid H\_{T})\geq e^{-T(\delta\_{T}-\epsilon)}\right)\leq\mathbb{P}\big(A\_{T}(\eta)^{c}\cap E\_{R}\big)+\mathbb{P}(E\_{R}^{c})\leq\exp\!\Big(-\frac{T\epsilon^{2}}{2B\_{R}^{2}}\Big)+\mathbb{P}(E\_{R}^{c}), |  |

as claimed.
âˆ

#### A.3 Stability of posterior odds

##### A.3.1 Proof of TheoremÂ [4.4](https://arxiv.org/html/2601.18815v1#S4.Thmtheorem4 "Theorem 4.4 (Stability of posterior odds on typical histories). â€£ 4.5.3 Stability Theorem â€£ 4.5 Stability of Posterior Odds under Perturbations â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types")

###### Proof.

Fix histories h=(p0:T,v1:T)h=(p\_{0:T},v\_{1:T}) and hâ€²=(p0:Tâ€²,v1:Tâ€²)h^{\prime}=(p^{\prime}\_{0:T},v^{\prime}\_{1:T}) with log-odds increments Î”â€‹x1:T\Delta x\_{1:T} and Î”â€‹x1:Tâ€²\Delta x^{\prime}\_{1:T}.
Fix R>0R>0 and work on the event ERâ€‹(h,hâ€²)E\_{R}(h,h^{\prime}) defined in (LABEL:eq:trunc-event) so that |Î”â€‹xt|âˆ¨|Î”â€‹xtâ€²|â‰¤R|\Delta x\_{t}|\vee|\Delta x^{\prime}\_{t}|\leq R for all tt.

###### Step 1: Pointwise log-likelihood stability.

AssumptionÂ [4.6](https://arxiv.org/html/2601.18815v1#S4.Thmassumption6 "Assumption 4.6 (Local Lipschitz regularity on bounded increments). â€£ 4.5.2 Local Lipschitz Regularity â€£ 4.5 Stability of Posterior Odds under Perturbations â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types") implies that for any yâˆˆ{0,1}y\in\{0,1\}, any Î¸âˆˆÎ˜\theta\in\Theta, and any tâ‰¤Tt\leq T,

|  |  |  |
| --- | --- | --- |
|  | |logfy(Î”xtâˆ£vt,Î¸)âˆ’logfy(Î”xtâ€²âˆ£vtâ€²,Î¸)|â‰¤Lx(R)|Î”xtâˆ’Î”xtâ€²|+Lv|vtâˆ’vtâ€²|.\big|\log f\_{y}(\Delta x\_{t}\mid v\_{t},\theta)-\log f\_{y}(\Delta x^{\prime}\_{t}\mid v^{\prime}\_{t},\theta)\big|\leq L\_{x}(R)\,|\Delta x\_{t}-\Delta x^{\prime}\_{t}|+L\_{v}\,|v\_{t}-v^{\prime}\_{t}|. |  |

Summing over tt gives

|  |  |  |  |
| --- | --- | --- | --- |
|  | |â„“y,Tâ€‹(Î¸;h)âˆ’â„“y,Tâ€‹(Î¸;hâ€²)|â‰¤Lxâ€‹(R)â€‹âˆ‘t=1T|Î”â€‹xtâˆ’Î”â€‹xtâ€²|+Lvâ€‹âˆ‘t=1T|vtâˆ’vtâ€²|.\Big|\ell\_{y,T}(\theta;h)-\ell\_{y,T}(\theta;h^{\prime})\Big|\leq L\_{x}(R)\sum\_{t=1}^{T}|\Delta x\_{t}-\Delta x^{\prime}\_{t}|+L\_{v}\sum\_{t=1}^{T}|v\_{t}-v^{\prime}\_{t}|. |  | (23) |

###### Step 2: Log-sum-exp Lipschitz lemma.

We use the elementary inequality: for any measurable aÎ¸,bÎ¸a\_{\theta},b\_{\theta} and probability measure Î \Pi,

|  |  |  |
| --- | --- | --- |
|  | |logâ€‹âˆ«eaÎ¸â€‹Î â€‹(dâ€‹Î¸)âˆ’logâ€‹âˆ«ebÎ¸â€‹Î â€‹(dâ€‹Î¸)|â‰¤supÎ¸âˆˆÎ˜|aÎ¸âˆ’bÎ¸|.\left|\log\int e^{a\_{\theta}}\,\Pi(d\theta)-\log\int e^{b\_{\theta}}\,\Pi(d\theta)\right|\leq\sup\_{\theta\in\Theta}|a\_{\theta}-b\_{\theta}|. |  |

It follows by writing bÎ¸â‰¤aÎ¸+sup|aâˆ’b|b\_{\theta}\leq a\_{\theta}+\sup|a-b|, integrating, and taking logs; symmetry gives the absolute value bound.

###### Step 3: Stability of marginal likelihoods and Bayes factors.

Apply the lemma with
aÎ¸=â„“y,Tâ€‹(Î¸;h)a\_{\theta}=\ell\_{y,T}(\theta;h) and bÎ¸=â„“y,Tâ€‹(Î¸;hâ€²)b\_{\theta}=\ell\_{y,T}(\theta;h^{\prime}):

|  |  |  |
| --- | --- | --- |
|  | |logâ¡myâ€‹(h)âˆ’logâ¡myâ€‹(hâ€²)|â‰¤supÎ¸âˆˆÎ˜|â„“y,Tâ€‹(Î¸;h)âˆ’â„“y,Tâ€‹(Î¸;hâ€²)|.|\log m\_{y}(h)-\log m\_{y}(h^{\prime})|\leq\sup\_{\theta\in\Theta}\big|\ell\_{y,T}(\theta;h)-\ell\_{y,T}(\theta;h^{\prime})\big|. |  |

Combining with ([23](https://arxiv.org/html/2601.18815v1#A1.E23 "In Step 1: Pointwise log-likelihood stability. â€£ A.3.1 Proof of Theorem 4.4 â€£ A.3 Stability of posterior odds â€£ Appendix A Proofs and Additional Technical Results â€£ Appendix â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types")) yields, on ERâ€‹(h,hâ€²)E\_{R}(h,h^{\prime}),

|  |  |  |
| --- | --- | --- |
|  | |logâ¡myâ€‹(h)âˆ’logâ¡myâ€‹(hâ€²)|â‰¤Lxâ€‹(R)â€‹âˆ‘t=1T|Î”â€‹xtâˆ’Î”â€‹xtâ€²|+Lvâ€‹âˆ‘t=1T|vtâˆ’vtâ€²|.|\log m\_{y}(h)-\log m\_{y}(h^{\prime})|\leq L\_{x}(R)\sum\_{t=1}^{T}|\Delta x\_{t}-\Delta x^{\prime}\_{t}|+L\_{v}\sum\_{t=1}^{T}|v\_{t}-v^{\prime}\_{t}|. |  |

Hence

|  |  |  |
| --- | --- | --- |
|  | |logâ¡BFTâ€‹(h)âˆ’logâ¡BFTâ€‹(hâ€²)|â‰¤2â€‹Lxâ€‹(R)â€‹âˆ‘t=1T|Î”â€‹xtâˆ’Î”â€‹xtâ€²|+2â€‹Lvâ€‹âˆ‘t=1T|vtâˆ’vtâ€²|,\big|\log\mathrm{BF}\_{T}(h)-\log\mathrm{BF}\_{T}(h^{\prime})\big|\leq 2L\_{x}(R)\sum\_{t=1}^{T}|\Delta x\_{t}-\Delta x^{\prime}\_{t}|+2L\_{v}\sum\_{t=1}^{T}|v\_{t}-v^{\prime}\_{t}|, |  |

which is ([17](https://arxiv.org/html/2601.18815v1#S4.E17 "In Theorem 4.4 (Stability of posterior odds on typical histories). â€£ 4.5.3 Stability Theorem â€£ 4.5 Stability of Posterior Odds under Perturbations â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types")).

###### Step 4: Stability of posterior probabilities.

Write posterior odds as in ([7](https://arxiv.org/html/2601.18815v1#S3.E7 "In 3.8 Prior and Posterior â€£ 3 Model and Bayesian Formulation â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types")) and recall that
Ï€â†¦logitâ€‹(Ï€)\pi\mapsto\mathrm{logit}(\pi) is the inverse of the logistic sigmoid Ïƒâ€‹(x)=1/(1+eâˆ’x)\sigma(x)=1/(1+e^{-x}).
Since |Ïƒâ€²â€‹(x)|â‰¤1/4|\sigma^{\prime}(x)|\leq 1/4 for all xx, Ïƒ\sigma is 1/41/4-Lipschitz. Therefore,

|  |  |  |
| --- | --- | --- |
|  | |â„™(Y=1âˆ£h)âˆ’â„™(Y=1âˆ£hâ€²)|â‰¤14|logBFT(h)âˆ’logBFT(hâ€²)|,\big|\mathbb{P}(Y=1\mid h)-\mathbb{P}(Y=1\mid h^{\prime})\big|\leq\frac{1}{4}\,\big|\log\mathrm{BF}\_{T}(h)-\log\mathrm{BF}\_{T}(h^{\prime})\big|, |  |

which is ([18](https://arxiv.org/html/2601.18815v1#S4.E18 "In Theorem 4.4 (Stability of posterior odds on typical histories). â€£ 4.5.3 Stability Theorem â€£ 4.5 Stability of Posterior Odds under Perturbations â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types")).
âˆ

#### A.4 Information gain

##### A.4.1 Proof of PropositionÂ [4.3](https://arxiv.org/html/2601.18815v1#S4.Thmproposition3 "Proposition 4.3 (Properties of realized information gain). â€£ 4.6.1 Realized Information Gain â€£ 4.6 Information Gain and Market-as-Sensor Metrics â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types")

###### Proof.

Write Ï€T=Ï€Tâ€‹(h):=â„™â€‹(Y=1âˆ£h)\pi\_{T}=\pi\_{T}(h):=\mathbb{P}(Y=1\mid h) and recall

|  |  |  |
| --- | --- | --- |
|  | IG(h)=DKL(Bern(Ï€T)âˆ¥Bern(Ï€0))=Ï€TlogÏ€TÏ€0+(1âˆ’Ï€T)log1âˆ’Ï€T1âˆ’Ï€0.\mathrm{IG}(h)=\mathrm{D\_{KL}}\!\left(\mathrm{Bern}(\pi\_{T})\,\middle\|\,\mathrm{Bern}(\pi\_{0})\right)=\pi\_{T}\log\frac{\pi\_{T}}{\pi\_{0}}+(1-\pi\_{T})\log\frac{1-\pi\_{T}}{1-\pi\_{0}}. |  |

###### (1) Nonnegativity.

This is the standard nonnegativity of KL divergence. Equality holds iff the two Bernoulli laws coincide, i.e. Ï€T=Ï€0\pi\_{T}=\pi\_{0}.

###### (2) Upper bound.

For fixed Ï€0âˆˆ(0,1)\pi\_{0}\in(0,1), the function

|  |  |  |
| --- | --- | --- |
|  | Ï€â†¦DKLâ€‹(Bernâ€‹(Ï€)âˆ¥Bernâ€‹(Ï€0))\pi\mapsto\mathrm{D\_{KL}}(\mathrm{Bern}(\pi)\,\|\,\mathrm{Bern}(\pi\_{0})) |  |

is convex in Ï€\pi and hence attains its maximum over Ï€âˆˆ[0,1]\pi\in[0,1] at the endpoints.
Evaluating at Ï€=1\pi=1 and Ï€=0\pi=0 gives

|  |  |  |
| --- | --- | --- |
|  | IGâ€‹(h)â‰¤maxâ¡{logâ¡1Ï€0,logâ¡11âˆ’Ï€0}.\mathrm{IG}(h)\leq\max\Big\{\log\frac{1}{\pi\_{0}},\,\log\frac{1}{1-\pi\_{0}}\Big\}. |  |

###### (3) Monotonicity in |logâ¡BFTâ€‹(h)||\log\mathrm{BF}\_{T}(h)|.

From ([7](https://arxiv.org/html/2601.18815v1#S3.E7 "In 3.8 Prior and Posterior â€£ 3 Model and Bayesian Formulation â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types")), logitâ€‹(Ï€T)=logitâ€‹(Ï€0)+logâ¡BFTâ€‹(h)\mathrm{logit}(\pi\_{T})=\mathrm{logit}(\pi\_{0})+\log\mathrm{BF}\_{T}(h).
Define u:=logitâ€‹(Ï€T)âˆ’logitâ€‹(Ï€0)=logâ¡BFTâ€‹(h)u:=\mathrm{logit}(\pi\_{T})-\mathrm{logit}(\pi\_{0})=\log\mathrm{BF}\_{T}(h) and write Ï€â€‹(u):=Ïƒâ€‹(logitâ€‹(Ï€0)+u)\pi(u):=\sigma(\mathrm{logit}(\pi\_{0})+u).
Then IGâ€‹(h)=Fâ€‹(u)\mathrm{IG}(h)=F(u) where

|  |  |  |
| --- | --- | --- |
|  | Fâ€‹(u)=DKLâ€‹(Bernâ€‹(Ï€â€‹(u))âˆ¥Bernâ€‹(Ï€0)).F(u)=\mathrm{D\_{KL}}(\mathrm{Bern}(\pi(u))\,\|\,\mathrm{Bern}(\pi\_{0})). |  |

A direct differentiation shows Fâ€²â€‹(u)=uâ€‹Ï€â€‹(u)â€‹(1âˆ’Ï€â€‹(u))F^{\prime}(u)=u\,\pi(u)(1-\pi(u)), hence Fâ€²â€‹(u)F^{\prime}(u) has the same sign as uu.
Therefore Fâ€‹(u)F(u) is increasing in uu for u>0u>0 and decreasing for u<0u<0, which implies Fâ€‹(u)F(u) is a monotone function of |u|=|logâ¡BFTâ€‹(h)||u|=|\log\mathrm{BF}\_{T}(h)|.
âˆ

#### A.5 Verifying regularity assumptions for the Gaussian latent-type model

We record a convenient sufficient condition ensuring that the Gaussian latent-type model in ExampleÂ [3.1](https://arxiv.org/html/2601.18815v1#S3.Thmexample1 "Example 3.1 (Gaussian latent-type model). â€£ 3.7 A Concrete Instantiation: Gaussian Latent-Type Model â€£ 3 Model and Bayesian Formulation â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types") satisfies the regularity assumptions used in SectionÂ [4](https://arxiv.org/html/2601.18815v1#S4 "4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").

###### Proposition A.1 (Sufficient parameter constraints for regularity).

Consider ExampleÂ [3.1](https://arxiv.org/html/2601.18815v1#S3.Thmexample1 "Example 3.1 (Gaussian latent-type model). â€£ 3.7 A Concrete Instantiation: Gaussian Latent-Type Model â€£ 3 Model and Bayesian Formulation â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types") with the softmax gating ([1](https://arxiv.org/html/2601.18815v1#S3.E1 "In 3.4 Volume-Dependent Gating â€£ 3 Model and Bayesian Formulation â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types")).
Assume the parameter space is restricted by the following bounds:

1. 1.

   *Compactness:* all scalar parameters lie in closed bounded intervals and ğ\bm{\omega} lies in the simplex; in particular Î¼1,Î¼3âˆˆ[0,MÎ¼]\mu\_{1},\mu\_{3}\in[0,M\_{\mu}], Î»1,Îº1âˆˆ[0,MÎ»]\lambda\_{1},\kappa\_{1}\in[0,M\_{\lambda}], Ï„3âˆˆ[0,MÏ„]\tau\_{3}\in[0,M\_{\tau}], and Î½âˆˆ[Î½min,Î½max]\nu\in[\nu\_{\min},\nu\_{\max}] for some Î½min>2\nu\_{\min}>2.
2. 2.

   *Nondegenerate scales:* there exist 0<Ïƒmin<Ïƒmax<âˆ0<\sigma\_{\min}<\sigma\_{\max}<\infty such that Ïƒkâˆˆ[Ïƒmin,Ïƒmax]\sigma\_{k}\in[\sigma\_{\min},\sigma\_{\max}] for k=1,2,3k=1,2,3.
3. 3.

   *Volume-range control (design-dependent):* for the realized design v1:Tv\_{1:T}, the implied scales satisfy

   |  |  |  |
   | --- | --- | --- |
   |  | 0<smin,Tâ‰¤skâ€‹(vt;Î¸k)â‰¤smax,T<âˆfor allÂ â€‹tâ‰¤T,kâˆˆ{1,2,3},Î¸âˆˆÎ˜,0<s\_{\min,T}\leq s\_{k}(v\_{t};\theta\_{k})\leq s\_{\max,T}<\infty\qquad\text{for all }t\leq T,\ k\in\{1,2,3\},\ \theta\in\Theta, |  |

   where smin,T,smax,Ts\_{\min,T},s\_{\max,T} may depend on v1:Tv\_{1:T} through maxtâ‰¤Tâ¡vt\max\_{t\leq T}v\_{t}.

Then AssumptionsÂ [4.1](https://arxiv.org/html/2601.18815v1#S4.Thmassumption1 "Assumption 4.1 (Compactness and continuity). â€£ 4.4.1 Regularity Conditions â€£ 4.4 Posterior Concentration and Finite-Sample Error Bounds â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types")â€“[4.6](https://arxiv.org/html/2601.18815v1#S4.Thmassumption6 "Assumption 4.6 (Local Lipschitz regularity on bounded increments). â€£ 4.5.2 Local Lipschitz Regularity â€£ 4.5 Stability of Posterior Odds under Perturbations â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types") and AssumptionÂ [4.5](https://arxiv.org/html/2601.18815v1#S4.Thmassumption5 "Assumption 4.5 (Moment control for increments). â€£ 4.4.4 Finite-Sample Error Bounds via Truncation â€£ 4.4 Posterior Concentration and Finite-Sample Error Bounds â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types") hold (with constants allowed to depend on the realized design through smin,T,smax,Ts\_{\min,T},s\_{\max,T}).

###### Proof sketch.

Compactness and continuity follow because the Gaussian and Student-tt densities are continuous in their parameters and strictly positive on â„\mathbb{R}, and the softmax gating is continuous in (v,Î¸)(v,\theta) on bounded parameter sets.

The envelope condition follows from standard bounds on Gaussian and Student-tt log-densities: on any fixed design and under nondegenerate scales, there exist constants c0,c1<âˆc\_{0},c\_{1}<\infty such that

|  |  |  |
| --- | --- | --- |
|  | supy,Î¸,t|logfy(xâˆ£vt,Î¸)|â‰¤c0+c1x2,\sup\_{y,\theta,t}\big|\log f\_{y}(x\mid v\_{t},\theta)\big|\leq c\_{0}+c\_{1}x^{2}, |  |

and xâ†¦c0+c1â€‹x2x\mapsto c\_{0}+c\_{1}x^{2} is integrable under the true law because Gaussian and Student-tt components have finite second moments when Î½min>2\nu\_{\min}>2.

AssumptionÂ [4.5](https://arxiv.org/html/2601.18815v1#S4.Thmassumption5 "Assumption 4.5 (Moment control for increments). â€£ 4.4.4 Finite-Sample Error Bounds via Truncation â€£ 4.4 Posterior Concentration and Finite-Sample Error Bounds â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types") holds for any q<Î½minq<\nu\_{\min}, since the qq-th moment of a Student-tÎ½t\_{\nu} distribution is finite iff Î½>q\nu>q, and Gaussian moments are finite of all orders.

Local Lipschitz in xx on [âˆ’R,R][-R,R] follows by bounding derivatives of logâ¡fyâ€‹(xâˆ£v,Î¸)\log f\_{y}(x\mid v,\theta): for each locationâ€“scale component, âˆ‚xlogâ¡f\partial\_{x}\log f is affine in xx divided by s2s^{2}, hence bounded on [âˆ’R,R][-R,R] when ss is bounded away from zero; mixture derivatives can be bounded by convexity using the component-wise bounds.
âˆ

#### A.6 Extension to dependent increments

We record a standard extension showing how concentration statements adapt when increments are not conditionally independent but remain â€œwell-behavedâ€ in a martingale sense.

###### Proposition A.2 (Martingale-difference extension (sketch)).

Suppose (Î”â€‹Xt)tâ‰¤T(\Delta X\_{t})\_{t\leq T} is adapted to a filtration (â„±t)(\mathcal{F}\_{t}), and conditional on (Y,Î¸)(Y,\theta) satisfies a martingale-difference structure under the true law:

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[Î”â€‹Xtâˆ£â„±tâˆ’1,Y=yâ‹†,Î¸=Î¸â‹†]=myâ‹†â€‹(vt;Î¸â‹†),\mathbb{E}[\Delta X\_{t}\mid\mathcal{F}\_{t-1},Y=y^{\star},\theta=\theta^{\star}]=m\_{y^{\star}}(v\_{t};\theta^{\star}), |  |

with increments uniformly bounded on a truncation event ERE\_{R} as in AssumptionÂ [4.6](https://arxiv.org/html/2601.18815v1#S4.Thmassumption6 "Assumption 4.6 (Local Lipschitz regularity on bounded increments). â€£ 4.5.2 Local Lipschitz Regularity â€£ 4.5 Stability of Posterior Odds under Perturbations â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types") and PropositionÂ [4.2](https://arxiv.org/html/2601.18815v1#S4.Thmproposition2 "Proposition 4.2 (Finite-sample posterior error bound (robust form)). â€£ 4.4.4 Finite-Sample Error Bounds via Truncation â€£ 4.4 Posterior Concentration and Finite-Sample Error Bounds â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types").
Then exponential error bounds analogous to PropositionÂ [4.2](https://arxiv.org/html/2601.18815v1#S4.Thmproposition2 "Proposition 4.2 (Finite-sample posterior error bound (robust form)). â€£ 4.4.4 Finite-Sample Error Bounds via Truncation â€£ 4.4 Posterior Concentration and Finite-Sample Error Bounds â€£ 4 Inverse-Problem Analysis: Identifiability, Concentration, Stability, and Information Gain â€£ Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Priceâ€“Volume Histories under Latent Types") follow by replacing Hoeffding inequalities with Azumaâ€“Hoeffding inequalities for martingales.

###### Proof sketch.

The key object is again the log Bayes factor written as a sum of one-step predictive log-likelihood ratios; these form a martingale with bounded increments on ERE\_{R}. Azumaâ€“Hoeffding yields exponential deviation bounds around the (conditional) expectation, and the deterministic KL-gap lower bounds enter as in the independent case once one controls the conditional expectations via a tower property.
âˆ