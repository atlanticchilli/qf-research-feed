---
authors:
- Haochong Xia
- Yao Long Teng
- Regan Tan
- Molei Qin
- Xinrun Wang
- Bo An
doc_id: arxiv:2601.10143v1
family_id: arxiv:2601.10143
is_current: true
taxonomy:
  alpha_families: []
  asset_classes: []
  horizons: []
  themes: []
title: 'History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series
  Synthesis'
url_abs: http://arxiv.org/abs/2601.10143v1
url_html: https://arxiv.org/html/2601.10143v1
venue: arXiv q-fin
version: 1
year: 2026
---


Haochong Xia1,\*{}^{\textit{1,\*}}, Yao Long Teng1,\*{}^{\textit{1,\*}}, Regan Tan1{}^{\textit{1}} , Molei Qin1{}^{\textit{1}} , Xinrun Wang2{}^{\textit{2}}, Bo An1,â£â€ {}^{\textit{1,}\dagger}
  
1College of Computing and Data Science, Nanyang Technological University, Singapore
  
2School of Computing and Information Systems, Singapore Management University, Singapore
  
{HAOCHONG001, yaolong001, rtan072, molei001}@e.ntu.edu.sg, xrwang@smu.edu.sg, boan@ntu.edu.sg

###### Abstract

In quantitative finance, the gap between training and real-world performanceâ€”driven by concept drift and distributional non-stationarityâ€”remains a critical obstacle for building reliable data-driven systems. Models trained on static historical data often overfit, resulting in poor generalization in dynamic markets. The mantra â€œHistory Is Not Enoughâ€ underscores the need for adaptive data generation that learns to evolve with the market rather than relying solely on past observations. We present a drift-aware dataflow system that integrates machine learningâ€“based adaptive control into the data curation process. The system couples a parameterized data manipulation module comprising single-stock transformations, multi-stock mix-ups, and curation operations, with an adaptive plannerâ€“scheduler that employs gradient-based bi-level optimization to control the system. This design unifies data augmentation, curriculum learning, and data workflow management under a single differentiable framework, enabling provenance-aware replay and continuous data quality monitoring. Extensive experiments on forecasting and reinforcement learning trading tasks demonstrate that our framework enhances model robustness and improves risk-adjusted returns. The system provides a generalizable approach to adaptive data management and learning-guided workflow automation for financial data.

â€ â€ footnotetext: âˆ—Equal contribution.â€ â€ footnotetext: â€ Corresponding author.

## I Introduction

Machine learning techniques have been widely applied to quantitative finance research, encompassing tasks such as trading [[27](https://arxiv.org/html/2601.10143v1#bib.bib26 "TradeMaster: a holistic quantitative trading platform empowered by reinforcement learning")] and forecasting [[24](https://arxiv.org/html/2601.10143v1#bib.bib27 "Stock price forecasting with deep learning: a comparative study")]. The success of machine learning in quantitative finance largely stems from its ability to leverage the vast amount of financial data generated by markets [[20](https://arxiv.org/html/2601.10143v1#bib.bib29 "An overview of machine learning, deep learning, and reinforcement learning-based techniques in quantitative finance: recent progress and challenges")]. However, the constantly changing dynamics of the market present significant challenges. A fundamental assumption in applying machine learning techniques is that the data is independent and identically distributed (i.i.d.). When this i.i.d. assumption does not hold true, machine learning models tend to overfit the training data, which in turn reduces their robustness when applied to unseen data. The financial market, shaped by complex trader interactions, naturally evolves, leading to concept driftâ€” inconsistency in the joint probability distribution between time
tt and t+kt+k as Ptâ€‹(X,Y)â‰ Pt+kâ€‹(X,Y),k>0P\_{t}(X,Y)\neq P\_{t+k}(X,Y),k>0, where XX is the feature and YY is the target variable. For data-driven financial systems, such drift is not only a modeling challenge but also a data management problem: pipelines trained on static data lack mechanisms to adapt to distributional shifts over time. Addressing this gap requires an adaptive dataflow capable of managing evolving financial data streams.

Data manipulation techniques, such as data augmentation, play a crucial role in enhancing the robustness and generalization of machine learning models by expanding the training dataset to cover a broader range of potential market conditions [[25](https://arxiv.org/html/2601.10143v1#bib.bib10 "Neural stochastic agent-based limit order book simulation: a hybrid methodology")]. Agent-based models [[21](https://arxiv.org/html/2601.10143v1#bib.bib11 "Agent-based models of financial markets")] and deep generative models [[31](https://arxiv.org/html/2601.10143v1#bib.bib22 "Market-gan: adding control to financial market data generation with semantic context")] require significant computational resources and complex modeling. In contrast, data augmentation is simple and efficient. However, unlike in computer vision (CV), data augmentation is not yet extensively integrated into the quantitative finance pipeline, even though it can address both aleatoric and epistemic uncertainties [[10](https://arxiv.org/html/2601.10143v1#bib.bib33 "Aleatory and epistemic uncertainty in probability elicitation with an example from hazardous waste management"), [12](https://arxiv.org/html/2601.10143v1#bib.bib31 "On uncertainty, tempering, and data augmentation in bayesian classification")]. One of the key challenges is the absence of a commonly agreed-upon benchmark for augmentation operations in time-series data, particularly in the context of financial data. This gap exists because augmentation can easily compromise the fidelity and correlations inherent in financial data. To address this, we design a parameterized data manipulation module that incorporates domain-specific constraints (e.g., K-line consistency, cointegration, and temporal non-stationarity) through single-stock transformations, curation steps, multi-stock mix-ups, and interpolation. This transforms augmentation from a heuristic preprocessing step into a controllable and auditable dataflow component that generates diverse yet faithful data.

Given the diversity in data, models, and tasks in quantitative finance, developing augmentations for varying use cases requires domain expertise and meticulous fine-tuning, making adaptation challenging. Existing adaptive approaches to data handling and augmentation [[13](https://arxiv.org/html/2601.10143v1#bib.bib6 "DDG-da: data distribution generation for predictable concept drift adaptation"), [2](https://arxiv.org/html/2601.10143v1#bib.bib18 "Adaaug: learning class-and instance-adaptive data augmentation policies"), [11](https://arxiv.org/html/2601.10143v1#bib.bib1 "When to learn what: model-adaptive data augmentation curriculum")] address distributional bias in different ways: some rely solely on resampling existing data without generating new samples, while others employ augmentation strategies designed for fixed and stationary datasets. However, none of these methods continuously adapt their transformation policies as data and model states evolve. As a result, they are insufficient for non-stationary financial environments where concept drift is persistent and dynamic. From a data-system perspective, current pipelines lack a controller that can automatically regulate transformation operations based on feedback from downstream learning tasks. To fill this gap, we introduce a learning-guided planner that operates in the validation loop of the task model. This controller comprises an adaptive planner and a pacing scheduler which monitors both model state and data quality to determine optimal transformation probabilities and intensities. In effect, it serves as an autonomous workflow manager that dynamically adjusts data synthesis operations.

In the spirit of â€œHistory Is Not Enough,â€ we present a drift-aware adaptive dataflow system that unifies financial data synthesis, augmentation scheduling, and learning-based feedback control within a single workflow architecture. The system integrates the parameterized data manipulation module with a machine-learning-driven plannerâ€“scheduler that continuously optimizes transformation policies based on validation performance and overfitting signals. Together, these components constitute an adaptive data pipeline capable of improving data quality, diversity, and downstream model robustness under concept drift. To the best of our knowledge, this work provides the first learning-guided dataflow architecture tailored for financial time-series management. Our key contributions are summarized as follows:

1. 1.

   Adaptive dataflow framework.
   Motivated by empirical evidence of strong concept drift in financial data,
   we design a unified *adaptive dataflow system* that jointly *generates*, *curates*, and *schedules* augmented data through a machine learningâ€“based plannerâ€“scheduler architecture.
2. 2.

   Financially grounded synthesis module.
   We develop a parameterized data manipulation module â„³\mathcal{M} that embeds financial priors into the transformation operations to ensure both realism and statistical diversity of the synthesized data.
3. 3.

   Learning-guided augmentation control.
   Our system employs a bi-level optimization scheme where an adaptive planner and an overfitting-aware scheduler dynamically coordinate manipulation strength and proportion of data to be manipulated according to model feedback. This enables self-adjusting data synthesis and curriculum pacing under concept drift, aligning data management decisions with learning dynamics.
4. 4.

   Empirical and systemic effectiveness.
   Experiments on forecasting and reinforcement learning trading tasks show consistent gains in risk-adjusted performance while preserving data fidelity across various models and two mainstream financial markets.

![Refer to caption](x1.png)


(a) MCD

![Refer to caption](x2.png)


(b) Weather

![Refer to caption](x3.png)


(c) Electricity

![Refer to caption](x4.png)


(d) ETT

![Refer to caption](x5.png)


(e) MCD

![Refer to caption](x6.png)


(f) Weather

![Refer to caption](x7.png)


(g) Electricity

![Refer to caption](x8.png)


(h) ETT

Figure 1: t-SNE visualizations to assess concept drift. Top row: t-SNE plot for
Pâ€‹(Y|X)P(Y|X), where the x-axis represents the feature XX and the y-axis represents the daily return as the YY. Bottom row: t-SNE plot for Pâ€‹(X)P(X). Orange dots mark the data in the training set, while blue dots mark the data in the test set.

## II Background and Related Works

Data augmentation techniques are important; however, they are less explored in time-series analysis. The method proposed by [[6](https://arxiv.org/html/2601.10143v1#bib.bib2 "Finding order in chaos: a novel data augmentation method for time series in contrastive learning")] is specifically designed for non-stationary time-series. However, it requires the data to be periodic and is specifically designed for contrastive learning. There is still a lack of a benchmark for augmentation techniques for time-series, especially financial time-series data. Data augmentation techniques can be use-case specific, and finding the optimal policies requires domain knowledge and fine-tuning. Fixed policies often suffer from insufficient randomness. Conversely, automated augmentation like AutoAugment [[4](https://arxiv.org/html/2601.10143v1#bib.bib17 "Autoaugment: learning augmentation policies from data")] utilizes reinforcement learning to identify suitable augmentation policies, but requires extensive computational resources and restricts the randomness in its policies.

Curriculum learning initiates training with simple patterns and progressively moves to more complex ones. Empirical evidence has shown that curriculum learning [[1](https://arxiv.org/html/2601.10143v1#bib.bib20 "Curriculum learning")] can enhance generalization. Recent works, such as AdaAug [[2](https://arxiv.org/html/2601.10143v1#bib.bib18 "Adaaug: learning class-and instance-adaptive data augmentation policies")] and MADAug [[11](https://arxiv.org/html/2601.10143v1#bib.bib1 "When to learn what: model-adaptive data augmentation curriculum")], have introduced the idea of curriculum learning accompanied by data augmentation techniques. AdaAug learns adaptive augmentation policies in a class and instance dependent manner. On top of this, MADAug applies a monotonic curriculum to introduce the augmented data to the training process. These methods are designed to learn robust image features. Consequently, there remains a lack of workflows that can generate effective augmentation policies tailored to the challenges of time-series data, particularly in the financial domain.

Generating augmented data has been an important way to expand historical data. Agent-based modeling [[21](https://arxiv.org/html/2601.10143v1#bib.bib11 "Agent-based models of financial markets"), [25](https://arxiv.org/html/2601.10143v1#bib.bib10 "Neural stochastic agent-based limit order book simulation: a hybrid methodology"), [35](https://arxiv.org/html/2601.10143v1#bib.bib24 "FinAgent: a multimodal foundation agent for financial trading: tool-augmented, diversified, and generalist")] generates the data stream with the actions of autonomous agents. However, these models rely on the empirical behavior models of agents in the market which are subjective and computationally expensive, suffering from complex parameter tuning and thus lack generalizability [[8](https://arxiv.org/html/2601.10143v1#bib.bib12 "Limit order books"), [18](https://arxiv.org/html/2601.10143v1#bib.bib13 "Statistical analysis of financial returns for a multiagent order book model of asset trading"), [30](https://arxiv.org/html/2601.10143v1#bib.bib14 "Get real: realism metrics for robust limit order book market simulations")]. Deep generative models generate data from an underlying distribution learned from historical data. TimeGAN [[32](https://arxiv.org/html/2601.10143v1#bib.bib49 "Time-series generative adversarial networks")] combines an autoencoder and GAN in a two-stage training to capture global and local temporal patterns. Diffusion-TS[[34](https://arxiv.org/html/2601.10143v1#bib.bib54 "Diffusion-ts: interpretable diffusion for general time series generation")] integrates seasonal-trend decomposition into a transformer-based diffusion model with a Fourier reconstruction loss, enabling interpretable multivariate time-series generation.
However, deep generative models need complex training and have less explainable evaluation metrics, complicating their integration into data management pipelines.

## III Preliminaries

### III-A Definitions

Financial Data and K-line Representation.
Financial data are organized as sequences of *K-line* (candlestick) tuples

|  |  |  |  |
| --- | --- | --- | --- |
|  | xt=[Ot,Ht,Lt,Ct,Vt],x\_{t}=[O\_{t},\,H\_{t},\,L\_{t},\,C\_{t},\,V\_{t}], |  | (1) |

where OtO\_{t}, HtH\_{t}, LtL\_{t}, and CtC\_{t} denote the open, high, low, and close prices within an interval, and VtV\_{t} is the traded volume.
A valid K-line must satisfy the consistency constraint

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ltâ‰¤minâ¡(Ot,Ct)â‰¤maxâ¡(Ot,Ct)â‰¤Ht,L\_{t}\leq\min(O\_{t},C\_{t})\leq\max(O\_{t},C\_{t})\leq H\_{t}, |  | (2) |

preserving market realism. These *K-line features* encapsulate short-term momentum, volatility, and asymmetry of price movements.
They serve as the core components of both the forecasting input ğ’³\mathcal{X} and the RL state sts\_{t}.
Our data-manipulation module (Section [IV](https://arxiv.org/html/2601.10143v1#S4 "IV Method â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis")) explicitly enforces these constraints during augmentation to ensure economic plausibility.

Time Series Forecasting.
Let ğ’³={xtâˆˆâ„d}t=1T\mathcal{X}=\{x\_{t}\in\mathbb{R}^{d}\}\_{t=1}^{T} denote a multivariate financial time series, where xtx\_{t} collects dd market features at timestamp tt.
Given a look-back window of length LL, a forecasting model

|  |  |  |  |
| --- | --- | --- | --- |
|  | fÎ¸:â„dÃ—Lâ†’â„dâ€²f\_{\theta}:\mathbb{R}^{d\times L}\rightarrow\mathbb{R}^{d^{\prime}} |  | (3) |

predicts the next-period target y^t=fÎ¸â€‹(xtâˆ’L+1:t)\hat{y}\_{t}=f\_{\theta}(x\_{t-L+1:t}).
The training objective minimizes the expected loss

|  |  |  |  |
| --- | --- | --- | --- |
|  | minÎ¸â¡ğ”¼(xt,yt)âˆ¼ğ’Ÿâ€‹[â„“â€‹(fÎ¸â€‹(xtâˆ’L+1:t),yt)],\min\_{\theta}\ \mathbb{E}\_{(x\_{t},y\_{t})\sim\mathcal{D}}[\,\ell(f\_{\theta}(x\_{t-L+1:t}),y\_{t})\,], |  | (4) |

where â„“â€‹(â‹…)\ell(\cdot) is the mean squared error (MSE).

The prediction target yty\_{t} is defined as the *one-step close-to-close return*:

|  |  |  |  |
| --- | --- | --- | --- |
|  | yt=Ct+1âˆ’CtCt,y\_{t}=\frac{C\_{t+1}-C\_{t}}{C\_{t}}, |  | (5) |

where CtC\_{t} and Ctâˆ’1C\_{t-1} denote the closing prices at time tt and tâˆ’1t\!-\!1, respectively.
This formulation captures short-term price dynamics and aligns with standard forecasting objectives in quantitative finance tasks.

Reinforcement Learning (RL) for Trading.
We formalize the trading environment as a Markov decision process (MDP)

|  |  |  |
| --- | --- | --- |
|  | â„³=(ğ’®,ğ’œ,P,r,Î³),\mathcal{M}=(\mathcal{S},\mathcal{A},P,r,\gamma), |  |

where:

* â€¢

  *State space* ğ’®\mathcal{S}:
  st=[xtâˆ’L+1:t,pt]s\_{t}=[x\_{t-L+1:t},\,p\_{t}] concatenates recent features and the current position ptp\_{t}.
* â€¢

  *Action space* ğ’œ\mathcal{A}:
  discrete {âˆ’1,0,1}\{-1,0,1\} for *sell, hold, buy*.
* â€¢

  *Transition probability function* Pâ€‹(st+1|st,at)P(s\_{t+1}|s\_{t},a\_{t}):
  follows market evolution xt+1âˆ¼PX(â‹…|xt)x\_{t+1}\!\sim\!P\_{X}(\cdot|x\_{t}).
* â€¢

  *Reward function:*

  |  |  |  |  |
  | --- | --- | --- | --- |
  |  | rt=ptâˆ’1â€‹rtmktâˆ’câ€‹|Î”â€‹pt|.r\_{t}\;=\;p\_{t-1}\,r^{\text{mkt}}\_{t}\;-\;c\,|\Delta p\_{t}|\,. |  | (6) |

  where rtmkt=Ct+1âˆ’CtCtr\_{t}^{\text{mkt}}=\frac{C\_{t+1}-C\_{t}}{C\_{t}} is the market return and cc the transaction-cost ratio.

The agent Ï€Î¸â€‹(at|st)\pi\_{\theta}(a\_{t}|s\_{t}) maximizes

|  |  |  |  |
| --- | --- | --- | --- |
|  | Jâ€‹(Ï€Î¸)=ğ”¼Ï€Î¸â€‹[âˆ‘t=0Tâˆ’1Î³tâ€‹rt],J(\pi\_{\theta})=\mathbb{E}\_{\pi\_{\theta}}\big[\sum\_{t=0}^{T-1}\gamma^{t}r\_{t}\big], |  | (7) |

with value function

|  |  |  |  |
| --- | --- | --- | --- |
|  | Qâˆ—â€‹(s,a)=râ€‹(s,a)+Î³â€‹ğ”¼sâ€²âˆ¼Pâ€‹[maxaâ€²â¡Qâˆ—â€‹(sâ€²,aâ€²)],Q^{\*}(s,a)=r(s,a)+\gamma\,\mathbb{E}\_{s^{\prime}\!\sim\!P}[\,\max\_{a^{\prime}}Q^{\*}(s^{\prime},a^{\prime})\,], |  | (8) |

and optimal policy Ï€âˆ—â€‹(s)=argâ¡maxaâ¡Qâˆ—â€‹(s,a)\pi^{\*}(s)=\arg\max\_{a}Q^{\*}(s,a).
This setting aligns with the RL trading experiments in Table [III](https://arxiv.org/html/2601.10143v1#S5.T3 "TABLE III â€£ V-B1 Forecasting â€£ V-B Main Results â€£ V Experiments â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis").

![Refer to caption](x9.png)


Figure 2: The workflow of training the planner and task model to learn a policy of controlling the data manipulation module with the validation loss of the task model. The training step of the planner is marked with (1), (2), (3), and fÎ¸â€²f\_{\theta^{{}^{\prime}}} is a copy of fÎ¸f\_{\theta}. The fire icon marks the flow where parameters are updated.

### III-B Observation of Concept Drift in Financial Data

We investigate concept drift in financial data and compare it with time-series datasets: Weather111https://www.bgc-jena.mpg.de/wetter/, Electricity 222https://archive.ics.uci.edu/dataset/321/electricityloaddiagrams20112014, and ETT [[36](https://arxiv.org/html/2601.10143v1#bib.bib42 "Informer: beyond efficient transformer for long sequence time-series forecasting")]. To examine the concept drift in Ptâ€‹(X,Y)=Ptâ€‹(Y|X)Ã—Ptâ€‹(X)P\_{t}(X,Y)=P\_{t}(Y|X)\times P\_{t}(X), we visualize Ptâ€‹(Y|X)P\_{t}(Y|X) and Pâ€‹(X)P(X) respectively. For financial data, the XX comprises market prices and features, while the YY is the next dayâ€™s return of the close price. We use a t-SNE [[29](https://arxiv.org/html/2601.10143v1#bib.bib39 "Visualizing data using t-sne.")] plot to mark training and testing data with different colors. Similarly, we use another t-SNE plot to visualize Ptâ€‹(X)P\_{t}(X) over time. As shown in Fig.Â [1](https://arxiv.org/html/2601.10143v1#S1.F1 "Figure 1 â€£ I Introduction â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis"), when compared to the stock price of McDonaldâ€™s (MCD), the benchmark datasets exhibit a more overlapping distribution for both Pâ€‹(Y|X)P(Y|X) and Pâ€‹(X)P(X). This indicates that the stock data demonstrate a more evident concept drift compared to other benchmarks.

### III-C Validationâ€“Test Proximity

While the t-SNE visualization qualitatively illustrates concept drift between training and test sets, our method assumes that the validation data provide a closer approximation to the near-future test distribution than the historical training data.
To verify this assumption, we perform a quantitative proximity analysis on both datasets using three statistical distance metricsâ€”Population Stability Index (PSI), Kolmogorovâ€“Smirnov (Kâ€“S) statistic, and Maximum Mean Discrepancy (MMD)â€”computed for Trainâ€“Test and Validationâ€“Test pairs. The metrics are given by:

|  |  |  |  |
| --- | --- | --- | --- |
|  | PSI=âˆ‘i=1k(piâˆ’qi)â€‹lnâ¡(piqi),\mathrm{PSI}=\sum\_{i=1}^{k}(p\_{i}-q\_{i})\ln\left(\frac{p\_{i}}{q\_{i}}\right), |  | (9) |

where pip\_{i} and qiq\_{i} denote the proportions of observations falling into bin ii for the baseline and target distributions, respectively, and kk is the total number of bins.

|  |  |  |  |
| --- | --- | --- | --- |
|  | DKS=supx|F1â€‹(x)âˆ’F2â€‹(x)|,D\_{\mathrm{KS}}=\sup\_{x}\left|F\_{1}(x)-F\_{2}(x)\right|, |  | (10) |

where F1â€‹(x)F\_{1}(x) and F2â€‹(x)F\_{2}(x) are the empirical cumulative distribution functions (CDFs) of the two samples, and supx\sup\_{x} denotes the maximum difference over all xx.

|  |  |  |  |
| --- | --- | --- | --- |
|  | MMD2â€‹(â„±,U,V)\displaystyle\mathrm{MMD}^{2}(\mathcal{F},U,V) | =ğ”¼u,uâ€²âˆ¼PUâ€‹[kâ€‹(u,uâ€²)]+ğ”¼v,vâ€²âˆ¼PVâ€‹[kâ€‹(v,vâ€²)]\displaystyle=\mathbb{E}\_{u,u^{\prime}\sim P\_{U}}[k(u,u^{\prime})]+\mathbb{E}\_{v,v^{\prime}\sim P\_{V}}[k(v,v^{\prime})] |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | âˆ’2â€‹ğ”¼uâˆ¼PU,vâˆ¼PVâ€‹[kâ€‹(u,v)],\displaystyle\quad-2\,\mathbb{E}\_{u\sim P\_{U},v\sim P\_{V}}[k(u,v)], |  | (11) |

where kâ€‹(â‹…,â‹…)k(\cdot,\cdot) is a positive-definite kernel function, and PUP\_{U} and PVP\_{V} denote the two probability distributions being compared. In our experiments, we use the Radial Basis Function kernel.

A higher PSI or MMD, or a larger Kâ€“S statistic, indicates a greater degree of distributional shift between the baseline and target samples.

We combine all market features with the target as a multi-variant time-series data. For the stocks dataset (daily, 2000â€“2024), we adopt a rolling-year protocol where each fold incrementally extends the training horizon by one year, splitting all samples from 2000 to (2010+k)(2010+k) into Train/Validation/Test subsets with ratios of 0.6/0.2/0.2.
For the crypto dataset (hourly, 2023-09-27 to 2025-09-26), we apply a higher-frequency rolling protocol, expanding the training horizon by one month per fold while maintaining the same 0.6/0.2/0.2 chronological split.
In each fold, PSI, Kâ€“S, and MMD are evaluated between the Trainâ€“Test and Validationâ€“Test segments on feature distributions standardized using statistics computed from the training set.
Across both markets and temporal granularities, the results in Table [I](https://arxiv.org/html/2601.10143v1#S3.T1 "TABLE I â€£ III-C Validationâ€“Test Proximity â€£ III Preliminaries â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis") consistently show that Distâ€‹(Val,Test)<Distâ€‹(Train,Test)\text{Dist}(\text{Val},\text{Test})<\text{Dist}(\text{Train},\text{Test}), confirming that the validation window statistically resembles the near-future test distribution more closely than the historical training data.
This quantitative evidence supports the use of validation feedback to guide our adaptive augmentation and planner updates in SectionÂ [IV-C](https://arxiv.org/html/2601.10143v1#S4.SS3 "IV-C Adaptive Curriculum â€£ IV Method â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis").

TABLE I: Average distributional distances between Trainâ€“Test and Validationâ€“Test sets
across two datasets. Lower values indicate that the validation distribution is
closer to the test distribution than the distribution.

| Dataset | Trainâ€“Test | | | Validationâ€“Test | | |
| --- | --- | --- | --- | --- | --- | --- |
| PSI | Kâ€“S | MMD | PSI | Kâ€“S | MMD |
| Stocks (2000â€“2024, daily) | 12.62 | 0.7415 | 0.7528 | 9.075 | 0.6367 | 0.6177 |
| Crypto (2023â€“2025, hourly) | 4.396 | 0.2988 | 0.1841 | 2.867 | 0.2680 | 0.1781 |

### III-D Formulation of the Adaptive Control Objective

To mitigate the uncertainty caused by strong concept drift, we manipulate the training data to enhance generalization. Specifically, let fÎ¸â€‹(x)f\_{\theta}(x) denote the task model. To best estimate the generalization ability of the model [[2](https://arxiv.org/html/2601.10143v1#bib.bib18 "Adaaug: learning class-and instance-adaptive data augmentation policies")], the objective for learning the task model is:

|  |  |  |  |
| --- | --- | --- | --- |
|  | minâ„’valâ€‹(fÎ¸,Dvalid),\text{min}\quad\mathcal{L}\_{\text{val}}(f\_{\theta},D\_{\text{valid}}), |  | (12) |

where â„’val\mathcal{L}\_{\text{val}} is the validation loss, and DvalidD\_{\text{valid}} is the validation dataset. Let DtrainD\_{\text{train}} denote the training dataset of the task model. Let â„³\mathcal{M} denote the manipulation module, we have x~trainâ†Mâ€‹(xtrain)\tilde{x}\_{\text{train}}\leftarrow M(x\_{\text{train}}) for xtrainâˆˆDtrainx\_{\text{train}}\in D\_{\text{train}}.
Our objective is to develop an adaptive â„³\mathcal{M} that is simple yet effective in addressing the poor generalization caused by concept drift in financial data.

### III-E Augmentation Operations

We introduce the augmentation operations we will be using in our data manipulation module â„³\mathcal{M}.

Single stock transformation operations.

* â€¢

  Robustness Enhancement: Introduces controlled variations suited for noisy financial data. Jittering adds noise to improve signal discrimination. Scaling mitigates volatility-induced magnitude bias, and Magnitude Warping [[28](https://arxiv.org/html/2601.10143v1#bib.bib55 "Data augmentation of wearable sensor data for parkinsonâ€™s disease monitoring using convolutional neural networks")] applies non-linear distortions common in price dynamics with cubic spline interpolation.
* â€¢

  Structural Variation: Permutation preserves local continuity but breaks strict sequencing, reflecting the partially stochastic nature of market evolution.
* â€¢

  Decomposition and Recombination: STL Augmentation [[3](https://arxiv.org/html/2601.10143v1#bib.bib56 "STL: a seasonal-trend decomposition")] decomposes series into trend, seasonality, and residualsâ€”components naturally present in financial time seriesâ€”and bootstraps residuals to better model regime shifts and non-stationarity.

Multi-stock mix-up operations.

* â€¢

  Segment Replacement Operations: Replace segments of one stock with another, introducing local disruptions. Cut Mix replaces a portion of one stock with another.
* â€¢

  Weighted Average Operations: Combine data using weighted averages, creating smoother transitions between points. Linear Mix linearly combines two stocks.
* â€¢

  Frequency Domain Operations: Combine underlying frequency components to capture cyclical patterns. Amplitude Mix mixes the amplitude of the Fourier transform of two stocks which is essential for modeling cyclical patterns, such as seasonal effects and market cycles. Demirel and Holz (2024) combine significant frequencies of two stocks by mixing their phases and magnitudes.

![Refer to caption](x10.png)


Figure 3: The proposed data manipulation module. The manipulated dimension of data is marked with the respective color.

## IV Method

In this section, we first describe the overall workflow, outlining how data is manipulated through the workflow. Next, we introduce the parameterized data manipulation module â„³\mathcal{M}, which implements financially grounded operations to enhance data diversity while preserving realism and traceability. Finally, we detail the learning-guided controller, composed of a curriculum-based planner and an overfitting-aware scheduler, which continuously regulates manipulation strength and proportion of data to be manipulated based on validation feedback.
This controller closes the loop between data curation and model learning, enabling automated quality assurance and provenance tracking within a reproducible workflow.

### IV-A Overall Workflow

To optimize the objective in equation ([12](https://arxiv.org/html/2601.10143v1#S3.E12 "In III-D Formulation of the Adaptive Control Objective â€£ III Preliminaries â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis")) effectively with the guidance of gradients, we propose the workflow shown in Fig.Â [2](https://arxiv.org/html/2601.10143v1#S3.F2 "Figure 2 â€£ III-A Definitions â€£ III Preliminaries â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis"). The data manipulation module â„³\mathcal{M} generates augmented training samples x~train\tilde{x}\_{\text{train}} with operation choosing probability matrix pp and manipulation strength parameter Î»\lambda. To adaptively control â„³\mathcal{M}, we introduce a trainable planner gâ€‹((fÎ¸,xi);Ï•)g((f\_{\theta},x\_{i});\phi), which learns the policy Ï€Ï•â€‹(p,Î»|f,xi)\pi\_{\phi}(p,\lambda|f,x\_{i}) while the scheduler determines the proportion of data to be manipulated Î±\alpha using a heuristic algorithm. Additionally, we interleave task-model updates with planner updates on validation feedback, while provenance hooks (policy, probabilities, manipulation strengths, proportion of data to be manipulated) are persisted to enable exact replay.

In order to optimally control the data manipulation module â„³\mathcal{M} for each training sample xtrainx\_{\text{train}}, inspired by AdaAug and MADAug, we formulate the learning of the adaptive augmentation curriculum as a bi-level optimization problem:

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | minÏ•\displaystyle\min\_{\phi} | â„’vâ€‹aâ€‹lâ€‹(fÎ¸,xvalid),xvalidâˆˆDvalid\displaystyle\mathcal{L}\_{val}(f\_{\theta},x\_{\text{valid}}),\>\>x\_{\text{valid}}\in D\_{\text{valid}} |  | (13) |
|  | s.t. | Î¸=argâ¡minÎ¸â¡â„’tâ€‹râ€‹aâ€‹iâ€‹nâ€‹(fÎ¸,x~train)\displaystyle\theta=\arg\min\_{\theta}\mathcal{L}\_{train}(f\_{\theta},\tilde{x}\_{\text{train}}) |  |
|  |  | x~train=Mâ€‹(xtrain,Î±,p,Î»)\displaystyle\tilde{x}\_{\text{train}}=M(x\_{\text{train}},\alpha,p,\lambda) |  |

where the planner and the task model are trained alternately with their respective objectives.

### IV-B Parameterized Data Manipulation Module

Unlike simply aggregating existing augmentation operations and applying them directly to financial data, the proposed data manipulation module â„³\mathcal{M} is designed as a parameterized synthesis module specifically tailored to the statistical and structural properties of financial time series.
Each internal operation functions as a low-level augmentation primitive, while the *method* lies in how these operations are integrated, parameterized, and coordinated by the operation choosing probability matrix pp and manipulation strength factor Î»\lambda to preserve financial validity while introducing realistic diversity.
Rather than serving as a static augmentation toolkit, â„³\mathcal{M} provides a controllable mechanism for generating diverse yet high-fidelity financial data by accounting for temporal dependencies, cross-asset correlations, and market constraints such as K-line consistency and non-stationarity.
Given a training sample xtrainx\_{\text{train}} with the shape (Timestamps,Stocks,Features)(\text{Timestamps},\text{Stocks},\text{Features}), the manipulation is guided by pp and Î»\lambda as shown in Fig.Â [3](https://arxiv.org/html/2601.10143v1#S3.F3 "Figure 3 â€£ III-E Augmentation Operations â€£ III Preliminaries â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis").
For the ii-th operation among nn single-stock transformations and the jj-th operation among mm multi-stock mix-up operations, we have âˆ‘i=1nâˆ‘j=1mpiâ€‹j=1\sum\_{i=1}^{n}\sum\_{j=1}^{m}p\_{ij}=1 and Î»iâ€‹jâˆˆ[0,1]\lambda\_{ij}\in[0,1].
The transformation and mix-up layers introduce controlled diversity from different market perspectives, while the curation, normalization, and interpolation layers enforce economic plausibility and maintain consistency with real-world financial dynamics.
Financial time series exhibit strong temporal dependence, cross-asset co-movement, and price-based constraints (e.g., K-line consistency, non-stationarity, and heavy-tailed noise), and our module explicitly encodes these properties through four tightly coupled components, as illustrated in Fig.Â [3](https://arxiv.org/html/2601.10143v1#S3.F3 "Figure 3 â€£ III-E Augmentation Operations â€£ III Preliminaries â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis"). In our parameterized data manipulation module â„³\mathcal{M}, each primitive is gated by financial integrity constraints and normalization/denormalization checks, turning augmentation into curated synthesis with auditable parameters.

Transformation Layer. We apply single-stock transformation operations as introduced in SectionÂ [III-E](https://arxiv.org/html/2601.10143v1#S3.SS5 "III-E Augmentation Operations â€£ III Preliminaries â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis") to the raw input feature to manipulate each stock feature independently. The operations are done with the data in its original value, as we will apply the following curation step in the original space to ensure fidelity. These operations are target-invariant to preserve the fidelity of financial data. Operations are parameterized with:

* â€¢

  Jittering: Manipulation strength Î»\lambda controls the standard deviation of the perturbation.
* â€¢

  Scaling: Î»\lambda determines its amplitude.
* â€¢

  Magnitude WarpingÂ [[28](https://arxiv.org/html/2601.10143v1#bib.bib55 "Data augmentation of wearable sensor data for parkinsonâ€™s disease monitoring using convolutional neural networks")]: Î»\lambda controls its warp intensity.
* â€¢

  Permutation: Î»\lambda controls how many parts the sequence is divided into before reordering.
* â€¢

  STL AugmentationÂ [[3](https://arxiv.org/html/2601.10143v1#bib.bib56 "STL: a seasonal-trend decomposition")]: Î»\lambda controls its period.

Algorithm 1  Mix-up Target Stock Sampling

1:â€‚Input: Source stock aa; mix manipulation strength Î»âˆˆ[0,1]\lambda\!\in\![0,1]; cointegration pp-values ğ©={paâ€‹j}j\mathbf{p}\!=\!\{p\_{aj}\}\_{j}; candidate count kk

2:â€‚Output: Target stock index bb

3:â€‚Exclude self: paâ€‹aâ†âˆ…p\_{aa}\!\leftarrow\!\varnothing; define candidate set ğ’={jâ‰ aâˆ£paâ€‹jâ€‹valid}\mathcal{C}\!=\!\{j\!\neq\!a\mid p\_{aj}\ \text{valid}\}.

4:â€‚Set Î²â†(1âˆ’Î»)\beta\!\leftarrow\!(1-\lambda) if Î»â‰¤0.5\lambda\!\leq\!0.5 else Î»\lambda.

5:â€‚For each jâˆˆğ’j\!\in\!\mathcal{C}, compute score

|  |  |  |
| --- | --- | --- |
|  | Sj={âˆ’paâ€‹jÎ²,Î»â‰¤0.5â€‹(favor stronger cointegration)paâ€‹j1/Î²,Î»>0.5â€‹(favor weaker cointegration)S\_{j}\!=\begin{cases}-\,p\_{aj}^{\beta},&\lambda\!\leq\!0.5\ \text{(favor stronger cointegration)}\\[2.0pt] p\_{aj}^{1/\beta},&\lambda\!>\!0.5\ \text{(favor weaker cointegration)}\end{cases} |  |

6:â€‚Select top-kk indices TT by SjS\_{j} and apply a softmax over {Sj}jâˆˆT\{S\_{j}\}\_{j\in T} to form probabilities QjQ\_{j}.

7:â€‚Sample bâˆ¼Categoricalâ€‹(Q)b\!\sim\!\text{Categorical}(Q) and return bb.

Curation and Normalization Layer. After applying single-stock transformations, we curate the data to maintain financial consistency by setting the highest price feature to â€œHighâ€ and the lowest to â€œLowâ€. To conduct mix-up operations which involve the exchange of data between stocks, it is crucial to normalize the data. This is achieved using rolling-window standard normalization applied to each feature of each stock. For multiple layers of manipulation, the data is denormalized when reintroduced to the workflow.

Mix-up Layer. We apply multi-stock mix-up operations as introduced in SectionÂ [III-E](https://arxiv.org/html/2601.10143v1#S3.SS5 "III-E Augmentation Operations â€£ III Preliminaries â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis") to the normalized data to mix each source stock aa with target stock bb, as described in Algorithm [1](https://arxiv.org/html/2601.10143v1#alg1 "Algorithm 1 â€£ IV-B Parameterized Data Manipulation Module â€£ IV Method â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis"). We select a target stock bb by identifying the top kk most correlated stocks with the source stock aa based on cointegration test p-values. To control the skewness of the operation choosing probability, we apply a transformation to these p-values, adjusted by a manipulation strength parameter Î»\lambda. For Î»â‰¤0.5\lambda\leq 0.5, a power transformation compresses the p-values, favoring more cointegrated stocks. For Î»>0.5\lambda>0.5, an inverse power transformation expands the p-values, increasing the chance of selecting less cointegrated stocks. The transformed SS are normalized to QQ, and the target stock bb is sampled from this distribution. The mix-up methods are target-variant because they involve creating new samples by combining both the features and targets of two different stocks. Operations are parameterized with:

* â€¢

  Cut Mix: Î»\lambda determines the area of the patch replaced between two samples
* â€¢

  Linear Mix: Î»\lambda controls the interpolation ratio between the two inputs and their labels
* â€¢

  Amplitude Mix: Î»\lambda it adjusts the relative amplitude or energy contribution of each signal.
* â€¢

  Demirel and Holz (2024): Î»\lambda determines the blending ratio between the two signals.

Interpolation Compensation Layer. While the curation module sets hard constraints to maintain the fidelity of the financial data, we also apply an interpolation compensation to mitigate potential extreme samples. We propose a mutual-information-aware mixing strategy termed Binary Mix. Unlike standard interpolation methods that blend two samples uniformly or randomly, Binary Mix adaptively adjusts the interpolation ratio based on the similarity between the original and augmented data. Specifically, we compute the mutual information defined in Equation [14](https://arxiv.org/html/2601.10143v1#S4.E14 "In IV-B Parameterized Data Manipulation Module â€£ IV Method â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis"), between the two samples to estimate how semantically aligned they are, and reduce the mixing weight accordingly. This ensures that less similar augmentations contribute less to the final sample, preserving task-relevant structure. The procedure is detailed in AlgorithmÂ [2](https://arxiv.org/html/2601.10143v1#alg2 "Algorithm 2 â€£ IV-B Parameterized Data Manipulation Module â€£ IV Method â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis") where the factor bmixb\_{\text{mix}} is calculated with the mutual information. The less mutual information the augmented data has with the original data, the more it is compensated with the original data.

|  |  |  |  |
| --- | --- | --- | --- |
|  | MIâ€‹(X;Y)=âˆ¬ğ’³Ã—ğ’´fX,Yâ€‹(x,y)â€‹logâ¡(fX,Yâ€‹(x,y)fXâ€‹(x)â€‹fYâ€‹(y))â€‹dxâ€‹dy,\mathrm{MI}(X;Y)=\iint\_{\mathcal{X}\times\mathcal{Y}}f\_{X,Y}(x,y)\,\log\left(\frac{f\_{X,Y}(x,y)}{f\_{X}(x)\,f\_{Y}(y)}\right)\,\mathrm{d}x\,\mathrm{d}y, |  | (14) |

where XX and YY are random variables with joint distribution pâ€‹(x,y)p(x,y) and marginals pâ€‹(x)p(x) and pâ€‹(y)p(y).

Algorithm 2  Binary Mix: randomly selects segments from either of the two stocks based on a binomial distribution.

1:â€‚Input: Original Data ğ±\mathbf{x}, augmented data ğ²\mathbf{y}, Factor bmaxb\_{\text{max}}

2:â€‚Output: Compensated data ğ±â€²\mathbf{x^{\prime}}

3:â€‚Randomly select feature kk for fast estimation

4:â€‚Calculate mutual information MIxâ€‹y\text{MI}\_{xy} and baseline MIxâ€‹x\text{MI}\_{xx} to measure similarity and maximum similarity

5:â€‚Compute factor
bmix=bmaxâˆ’(MIxâ€‹yMIxâ€‹x)â€‹bmaxb\_{\text{mix}}=b\_{\text{max}}-\left(\frac{\text{MI}\_{xy}}{\text{MI}\_{xx}}\right)b\_{\text{max}}

6:â€‚Compute compensated data ğ±â€²=bmâ€‹iâ€‹xâ€‹ğ±+(1âˆ’bmâ€‹iâ€‹x)â€‹ğ²\mathbf{x^{{}^{\prime}}}=b\_{mix}\mathbf{x}+(1-b\_{mix})\mathbf{y}

7:â€‚return compensated data ğ±â€²=bmixâ€‹ğ±+(1âˆ’bmix)â€‹ğ²\mathbf{x^{\prime}}=b\_{\text{mix}}\mathbf{x}+(1-b\_{\text{mix}})\mathbf{y}




Algorithm 3  Proportion Î±\alpha\  Scheduler

1:â€‚Input: Current early stopping counter CesC\_{\text{es}}, Last early stopping counter ClesC\_{\text{les}}, Epoch EE, Threshold Ï„\tau

2:â€‚Output: Proportion of data to be manipulated Î±\alpha

3:â€‚Let rate penalty Rpenalty=1â€‹Â ifÂ â€‹Ces>Clesâ€‹Â elseÂ â€‹0.1R\_{\text{penalty}}=1\text{ if }C\_{\text{es}}>C\_{\text{les}}\text{ else }0.1.

4:â€‚Update Cles=CesC\_{\text{les}}=C\_{\text{es}}.

5:â€‚return Î±=minâ¡(tanhâ¡(EÏ„)+0.01,1.0)Ã—Rpenalty\alpha=\min(\tanh(\frac{E}{\tau})+0.01,1.0)\times R\_{\text{penalty}}

### IV-C Adaptive Curriculum

Given the heterogeneity of financial data, learning objectives, and model architectures,
designing a unified policy to control the parameterized data manipulation module â„³\mathcal{M} remains non-trivial.
We introduce an *adaptive planner* that jointly observes model and data states to determine optimal parameters,
while an *overfitting-aware scheduler* progressively integrates augmented samples through a dynamic curriculum.

Curriculum Planner. We train a planner gÏ•g\_{\phi} to learn the policy Ï€Ï•â€‹(p,Î»|f,xi)\pi\_{\phi}(p,\lambda|f,x\_{i}) to optimize the objective function in equation ([13](https://arxiv.org/html/2601.10143v1#S4.E13 "In IV-A Overall Workflow â€£ IV Method â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis")). The state includes both the task model fÎ¸f\_{\theta} and the input sample xiâˆˆDx\_{i}\in D, ensuring that the curriculum can be determined based on both the model and the data, as supported by findings in [[22](https://arxiv.org/html/2601.10143v1#bib.bib37 "Data parameters: a new family of parameters for learning a differentiable curriculum")]. To efficiently represent the state, we use high-level representations of fÎ¸f\_{\theta} and xix\_{i}. For the state of the model, we extract features from the second-to-last fully connected layer inserted into the task model. Comparably, for the state of the input sample xix\_{i}, we compute key metrics such as mean, volatility, momentum, skewness, kurtosis, and trend. More concretely, we calculate: Momentum=Xlastâˆ’Xfirst,\text{Momentum}=X\_{\text{last}}-X\_{\text{first}},
where XlastX\_{\text{last}} and XfirstX\_{\text{first}} are the values of the feature at the last and first observations in the window, respectively.
Skewness=1nâ€‹âˆ‘i=1n(Xiâˆ’XÂ¯)3(1nâ€‹âˆ‘i=1n(Xiâˆ’XÂ¯)2)3/2,\text{Skewness}=\frac{\frac{1}{n}\sum\_{i=1}^{n}(X\_{i}-\overline{X})^{3}}{\left(\frac{1}{n}\sum\_{i=1}^{n}(X\_{i}-\overline{X})^{2}\right)^{3/2}},
where XiX\_{i} is the value of the feature at observation ii, XÂ¯\overline{X} is the mean of the feature values, and nn is the number of observations in the window.
Kurtosis=1nâ€‹âˆ‘i=1n(Xiâˆ’XÂ¯)4(1nâ€‹âˆ‘i=1n(Xiâˆ’XÂ¯)2)2âˆ’3,\text{Kurtosis}=\frac{\frac{1}{n}\sum\_{i=1}^{n}(X\_{i}-\overline{X})^{4}}{\left(\frac{1}{n}\sum\_{i=1}^{n}(X\_{i}-\overline{X})^{2}\right)^{2}}-3,
where XiX\_{i} is the value of the feature at observation ii, XÂ¯\overline{X} is the mean of the feature values, and nn is the number of observations in the window.
Trend=âˆ‘i=1n(tiâˆ’tÂ¯)â€‹(Xiâˆ’XÂ¯)âˆ‘i=1n(tiâˆ’tÂ¯)2,\text{Trend}=\frac{\sum\_{i=1}^{n}(t\_{i}-\overline{t})(X\_{i}-\overline{X})}{\sum\_{i=1}^{n}(t\_{i}-\overline{t})^{2}},
where tit\_{i} is the time or index of observation ii, XiX\_{i} is the value of the feature at observation ii, and tÂ¯\overline{t} and XÂ¯\overline{X} are the means of the time and feature values within the window, respectively.

This ensures that the planner gÏ•g\_{\phi} can effectively learn a policy that adapts to the model and data, thereby optimizing the curriculum more efficiently.

To address risk from inference uncertainty in financial tasks, we propose a loss function inspired by the Sharpe ratio, incorporating standard deviation to penalize volatility:

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„’=Eâ€‹(lâ€‹oâ€‹sâ€‹s)+Î³Ã—Ïƒâ€‹(lâ€‹oâ€‹sâ€‹s)\mathcal{L}=E(loss)+\gamma\times\sigma(loss) |  | (15) |

This formulation guides the model away from risky inferences by penalizing. We set Î³=0.05\gamma=0.05 for experiments.

Over-fitting Aware Scheduler. While the planner controls the operation choosing probability pp and manipulation strength Î»\lambda, the proportion of data to be manipulated Î±\alpha is determined by the scheduler to provide a reasonable curriculum. In CV research, it is common to apply augmentation to all training samples [[4](https://arxiv.org/html/2601.10143v1#bib.bib17 "Autoaugment: learning augmentation policies from data"), [5](https://arxiv.org/html/2601.10143v1#bib.bib9 "Randaugment: practical automated data augmentation with a reduced search space")]. However, such a fixed application of augmentation may not be suitable for our objective of addressing uncertainty in financial markets. Therefore, we propose a dynamic data manipulation strategy, as outlined in Algorithm [3](https://arxiv.org/html/2601.10143v1#alg3 "Algorithm 3 â€£ IV-B Parameterized Data Manipulation Module â€£ IV Method â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis"). Curriculum learning encourages the model to progress from simpler to more challenging examples. As data augmentation increases sample diversity and complexity, the poportion of augmented data can be gradually raised during training. Hence, Î±\alpha is designed to increase over epochs, reflecting a soft curriculum rather than a fixed rule. However, excessive manipulation may adversely affect the learning process. Our objective is to mitigate poor generalization caused by concept drift, which is often indicated by overfittingâ€”where the model performs well on the training data but poorly on unseen validation data. To prevent over-manipulation of the data, we monitor overfitting during training by the validation loss: if the validation loss in the current epoch is not lower than in the previous epoch by a specified threshold, it suggests potential overfitting. When this occurs, the penalty to regulate frequent manipulation RpenaltyR\_{\text{penalty}} is removed to introduce more diverse data.

By scheduling the proportion of data to be manipulated, we balance generalization and robustness, enhancing the modelâ€™s ability to handle market uncertainty without added complexity.

Algorithm 4  Joint Training Scheme

1:â€‚Input: Training set DtrainD\_{\text{train}};
Validation set DvalidD\_{\text{valid}};
Update frequency fâ€‹râ€‹eâ€‹qfreq

2:â€‚Output: Planner gÏ•g\_{\phi}; task model fÎ¸f\_{\theta}

3:â€‚Initialize Î¸0\theta\_{0}, Ï•0\phi\_{0}

4:â€‚while max epoch not reached and no early stopping do

5:â€ƒâ€‚Get Î±\alpha with Algo [3](https://arxiv.org/html/2601.10143v1#alg3 "Algorithm 3 â€£ IV-B Parameterized Data Manipulation Module â€£ IV Method â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis") and p,Î»=gâ€‹(fÎ¸,xtrain)p,\lambda=g(f\_{\theta},x\_{\text{train}})

6:â€ƒâ€‚Update fÎ¸â€‹(x~train=â„³â€‹(Î±,p,Î»,xtrain);Ltrain)f\_{\theta}(\tilde{x}\_{\text{train}}=\mathcal{M}(\alpha,p,\lambda,x\_{\text{train}});L\_{\text{train}})

7:â€ƒâ€‚if current step is divisible by fâ€‹râ€‹eâ€‹qfreq then

8:â€ƒâ€ƒâ€‚Copy fÎ¸f\_{\theta} as fÎ¸â€²f\_{\theta^{{}^{\prime}}}, update it with x~~train\tilde{\tilde{x}}\_{\text{train}} with eq.([16](https://arxiv.org/html/2601.10143v1#S4.E16 "In IV-C Adaptive Curriculum â€£ IV Method â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis"))

9:â€ƒâ€ƒâ€‚Update gÏ•â€‹(xvalid;Lvalidâ€‹(fÎ¸â€²))g\_{\phi}(x\_{\text{valid}};L\_{\text{valid}}(f\_{\theta^{{}^{\prime}}}))

10:â€ƒâ€‚end if

11:â€‚end while

12:â€‚return Trained planner gÏ•g\_{\phi}; trained task model fÎ¸f\_{\theta}

Planner Training Scheme. Given the complexity of this bi-level optimization problem, we adopt methods from [[26](https://arxiv.org/html/2601.10143v1#bib.bib34 "Meta-weight-net: learning an explicit mapping for sample weighting"), [11](https://arxiv.org/html/2601.10143v1#bib.bib1 "When to learn what: model-adaptive data augmentation curriculum")] to jointly train the planner gÏ•g\_{\phi} and the task model fÎ¸f\_{\theta}. The task model is trained within the training loop, while the planner is trained within the validation loop. This relies on a local temporal smoothness assumption: when the data are partitioned chronologically, the validation segment is statistically closer to the upcoming test segment than the earlier training window as supported by Section [III](https://arxiv.org/html/2601.10143v1#S3 "III Preliminaries â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis"). This proximity supports using validation risk as a practical surrogate for test risk, thereby grounding the bi-level plannerâ€“scheduler optimization in a measurable temporal relationship. The alternating update scheme is outlined in Algorithm [4](https://arxiv.org/html/2601.10143v1#alg4 "Algorithm 4 â€£ IV-C Adaptive Curriculum â€£ IV Method â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis"). The task model fÎ¸f\_{\theta} is first updated. To stabilize the training process, the planner will then be trained for every frequency fâ€‹râ€‹eâ€‹qfreq steps to learn the policy Ï€Ï•â€‹(p,Î»|fÎ¸,xi)\pi\_{\phi}(p,\lambda|f\_{\theta},x\_{i}), with the validation loss of fÎ¸â€²f\_{\theta^{{}^{\prime}}}, which is a copy of fÎ¸f\_{\theta} at the current step.

To learn the operation choosing probability of operation pp, we
generate all the augmented data of the nÃ—mn\times m sets of the operations with their manipulation strength Î»iâ€‹j\lambda\_{ij} and sum all the augmented data up with their weight in the probabilistic piâ€‹jp\_{ij}, where

|  |  |  |  |
| --- | --- | --- | --- |
|  | x~~train=âˆ‘i=1nâˆ‘j=1mpiâ€‹jÃ—â„³â€‹(1,1,Î»iâ€‹j,xtrain).\tilde{\tilde{x}}\_{\text{train}}=\sum^{n}\_{i=1}\sum^{m}\_{j=1}p\_{ij}\times\mathcal{M}(1,1,\lambda\_{ij},x\_{\text{train}}). |  | (16) |

Using a weighted sum instead of sampling with piâ€‹jp\_{ij} accounts for the effect of every augmentation operation combination, thus leading to a better estimation when updating Ï•\phi.

To deal with non-differentiable operations, we use a straight-through gradient estimator [[14](https://arxiv.org/html/2601.10143v1#bib.bib45 "Differentiable automatic data augmentation")] to optimize the manipulation strength Î»\lambda, where âˆ‚Mâ€‹(xi)âˆ‚Î»=1\frac{\partial M(x\_{i})}{\partial\lambda}=1, with gradient:

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | âˆ‚â„’valâˆ‚Î»\displaystyle\frac{\partial\mathcal{L^{\text{val}}}}{\partial\lambda} | =âˆ‘ipiâ€‹jâ€‹âˆ‚â„’valâˆ‚fÎ¸â€‹âˆ‚fÎ¸âˆ‚Mâ€‹(xi)â€‹âˆ‚Mâ€‹(xi)âˆ‚Î»\displaystyle=\sum\_{i}p\_{ij}\frac{\partial\mathcal{L^{\text{val}}}}{\partial f\_{\theta}}\frac{\partial f\_{\theta}}{\partial M(x\_{i})}\frac{\partial M(x\_{i})}{\partial\lambda} |  | (17) |
|  |  | =âˆ‘ipiâ€‹jâ€‹âˆ‚â„’valâˆ‚fÎ¸â€‹âˆ‚fÎ¸âˆ‚Mâ€‹(xi).\displaystyle=\sum\_{i}p\_{ij}\frac{\partial\mathcal{L^{\text{val}}}}{\partial f\_{\theta}}\frac{\partial f\_{\theta}}{\partial M(x\_{i})}. |  |

The outer loop updates of planner Ï•t\phi\_{t} is given by:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ï•t+1=Ï•tâˆ’Î²â€‹1nvalâ€‹âˆ‘i=1nvalâˆ‡Ï•â„’ivalâ€‹(Î¸^tâ€‹(Ï•t))\phi\_{t+1}=\phi\_{t}-\beta\frac{1}{n^{\text{val}}}\sum\_{i=1}^{n^{\text{val}}}\nabla\_{\phi}\mathcal{L}^{\text{val}}\_{i}(\hat{\theta}\_{t}(\phi\_{t})) |  | (18) |

where Î²\beta is the learning rate and Î¸^\hat{\theta} is the task model.

TABLE II: Unified comparison over Stocks (upper block) and Cryptos (lower block) across five model families.

|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
|  | GRU | | | LSTM | | | DLinear | | | TCN | | | Transformer | | |
| Method | MSE | MAE | STD | MSE | MAE | STD | MSE | MAE | STD | MSE | MAE | STD | MSE | MAE | STD |
| Stocks â€„â€„ (exponents: MSE Ã—10âˆ’4\times 10^{-4}, MAE Ã—10âˆ’2\times 10^{-2}, STD Ã—10âˆ’3\times 10^{-3}) | | | | | | | | | | | | | | | |
| Original | 22.76 | 3.388 | 5.140 | 5.070 | 1.578 | 1.664 | 52.15 | 5.501 | 9.601 | 40.44 | 4.614 | 10.01 | 8.608 | 2.216 | 1.915 |
| RandAug | 16.74 | 2.864 | 3.994 | 4.646 | 1.495 | 1.613 | 662.7 | 19.86 | 116.7 | 58.74 | 4.925 | 22.12 | 8.264 | 2.160 | 1.892 |
| TrivialAug | 15.62 | 2.670 | 4.114 | 4.827 | 1.536 | 1.634 | 571.6 | 18.40 | 100.7 | 46.83 | 5.142 | 10.79 | 7.474 | 2.041 | 1.814 |
| AdaAug | 17.64 | 2.972 | 3.995 | 4.791 | 1.538 | 1.567 | 22.62 | 3.803 | 3.234 | 49.61 | 5.203 | 11.61 | 7.602 | 2.062 | 1.819 |
| Ours | 13.14 | 2.496 | 3.366 | 4.253 | 1.410 | 1.571 | 4.550 | 1.485 | 1.559 | 34.87 | 4.431 | 7.864 | 7.471 | 2.046 | 1.795 |
| Cryptos â€„â€„ (exponents: MSE Ã—10âˆ’5\times 10^{-5}, MAE Ã—10âˆ’3\times 10^{-3}, STD Ã—10âˆ’3\times 10^{-3}) | | | | | | | | | | | | | | | |
| Original | 8.144 | 6.551 | 8.975 | 4.426 | 4.568 | 6.453 | 4.291 | 4.428 | 6.339 | 913.3 | 29.57 | 90.95 | 8.356 | 6.726 | 7.555 |
| RandAug | 8.315 | 6.442 | 9.042 | 4.348 | 4.509 | 6.385 | 4.275 | 4.412 | 6.324 | 1883 | 40.94 | 125.5 | 7.295 | 6.400 | 7.232 |
| TrivialAug | 8.268 | 6.571 | 8.967 | 4.361 | 4.518 | 6.401 | 4.273 | 4.411 | 6.322 | 855.2 | 32.93 | 83.30 | 11.41 | 7.318 | 8.809 |
| AdaAug | 12.54 | 8.142 | 11.16 | 4.460 | 4.583 | 6.476 | 4.278 | 4.422 | 6.329 | 332.3 | 32.43 | 56.38 | 7.877 | 6.484 | 7.395 |
| Ours w/o mixup | 7.552 | 6.231 | 8.437 | 4.402 | 4.511 | 6.422 | 4.252 | 4.403 | 6.278 | 469.3 | 29.47 | 66.37 | 7.732 | 6.207 | 7.113 |
| Ours | 6.916 | 5.816 | 8.157 | 4.235 | 4.324 | 6.210 | 4.138 | 4.374 | 6.122 | 307.7 | 31.87 | 53.73 | 4.941 | 4.982 | 6.661 |

## V Experiments

We aim to evaluate whether our adaptive dataflow system effectively addresses three key challenges:

1. 1.

   Workflow effectiveness.
   Does the proposed dataflow system improve downstream task robustness and overall data-processing efficiency compared to existing augmentation and generation pipelines?
2. 2.

   System adaptability.
   Can the workflow generalize across heterogeneous models and tasks?
3. 3.

   Data fidelity and curation quality.
   Does the pipeline generate diverse yet realistic financial time-series data for downstream analysis?

### V-A Experiment Setup

##### Datasets

We evaluate on two real-world financial markets:
(i) Stocks (daily). Price and technical indicators for 27 stocks of the Dow Jones Industrial Average from 2000-01-01 to 2024-01-01;
(ii) Crypto (hourly). Price and technical indicators for BTC, ETH, DOT, and LTC from 2023-09-27 to 2025-09-26. The datasets were divided into training, validation, and test sets with ratios of 0.60.6, 0.20.2, and 0.20.2. All normalization statistics, cointegration measures, and other rolling or windowed computations were strictly estimated within the training set to prevent any temporal leakage. Experiments were run on a GeForce RTX 4090 GPU.

##### Benchmark for Augmented Data

We compared our augmented data with data generated by representative time-series generative models, including TimeGAN, SigCWGAN [[17](https://arxiv.org/html/2601.10143v1#bib.bib51 "Conditional sig-wasserstein gans for time series generation")], RCWGAN [[9](https://arxiv.org/html/2601.10143v1#bib.bib52 "A novel virtual sample generation method based on a modified conditional wasserstein gan to address the small sample size problem in soft sensing")], GMMN [[15](https://arxiv.org/html/2601.10143v1#bib.bib50 "Generative moment matching networks")], CWGAN [[33](https://arxiv.org/html/2601.10143v1#bib.bib53 "CWGAN: conditional wasserstein generative adversarial nets for fault data generation")], RCGAN [[7](https://arxiv.org/html/2601.10143v1#bib.bib48 "Real-valued (medical) time series generation with recurrent conditional gans")] and Diffusion-TS.

##### Benchmark for The Whole Workflow

In the experiments, the proposed method is compared with the following methods: (1) Original: Uses the original data without any manipulation to the training scheme. (2) RandAugment: Employs randomly chosen augmentations where Î»=1\lambda=1 for each augmentation and the proportion Î±=0.5\alpha=0.5 throughout the training process. (3) TrivialAugment: This baseline employs randomly chosen augmentations with a randomized Î»\lambda and fixed proportion Î±=0.5\alpha=0.5. (4) AdaAug: This setup utilizes the planner gÏ•g\_{\phi} for augmentations but maintains an Î±=0.5\alpha=0.5.

##### Task Model

We evaluate our method using five representative architectures across different model families: GRU, LSTM, DLinear, TCN, and Transformer on forecasting tasks, and DQN and PPO for reinforcement learningâ€“based trading tasks.
To integrate any model into our method, the downstream task model simply has to adopt a two-stage architectural pattern that explicitly separates feature extraction from prediction tasks. Unlike traditional models that employ a mapping from input sequences to predictions, we implement a modular design consisting of two distinct functions: the feature extraction function jâ€‹(â‹…)j(\cdot) that extracts learned representations from the second-to-last layer of the network, and the prediction function kâ€‹(â‹…)k(\cdot) that maps these extracted features to final predictions where kâ€‹(jâ€‹(â‹…))k(j(\cdot)) is the typical forward function. For models that do not have at least 2 fully-connected layers, we add them to facilitate this process. For the planner, we use a Transformer. For all models, we set the learning rate to be 0.001, and the planner input dimensions to be 128. The batch size for GRU and TCN at set at 128, LSTM and Transformer at 256, and DLinear at 1024. The patience for GRU, LSTM, TCN and Transformer is set at 5 while DLinear at 8. The hidden dimensions for the task model is set at 512 for GRU, LSTM, DLinear and TCN, and at 256 for Transformer. The planner layers are set at 2 for GRU, LSTM, TCN and Transformer, and at 5 for DLinear, while the planner hidden dimensions are set at 256 for GRU, LSTM, DLinear and TCN, and at 128 for Transformer.
The key hyperparameters are as follows:

* â€¢

  Threshold Ï„\tau: This has the same concept as the temperature parameter, and affects the proportion of data to be manipulated Î±\alpha. We set Ï„Gâ€‹Râ€‹U\tau\_{GRU} to 14, Ï„Lâ€‹Sâ€‹Tâ€‹M\tau\_{LSTM}, Ï„Tâ€‹Câ€‹N\tau\_{TCN} and Ï„Tâ€‹râ€‹aâ€‹nâ€‹sâ€‹fâ€‹oâ€‹râ€‹mâ€‹eâ€‹r\tau\_{Transformer} to 5, and Ï„Dâ€‹Lâ€‹iâ€‹nâ€‹eâ€‹aâ€‹r\tau\_{DLinear} to 30.
* â€¢

  Frequency fâ€‹râ€‹eâ€‹qfreq: This controls the frequency in which the planner updates. A smaller fâ€‹râ€‹eâ€‹qfreq value would increase the number of updates, which increases the time taken for training but helps the planner learn better (and vice versa). We set fâ€‹râ€‹eâ€‹qGâ€‹Râ€‹Ufreq\_{GRU} to 15, fâ€‹râ€‹eâ€‹qLâ€‹Sâ€‹Tâ€‹Mfreq\_{LSTM} and fâ€‹râ€‹eâ€‹qTâ€‹râ€‹aâ€‹nâ€‹sâ€‹fâ€‹oâ€‹râ€‹mâ€‹eâ€‹rfreq\_{Transformer} to 5, fâ€‹râ€‹eâ€‹qDâ€‹Lâ€‹iâ€‹nâ€‹eâ€‹aâ€‹rfreq\_{DLinear} to 8, and fâ€‹râ€‹eâ€‹qTâ€‹Câ€‹Nfreq\_{TCN} to 10.
* â€¢

  Planner training epoch start: This controls when the planner starts learning. A lower start would allow the planner to update earlier, which improves the quality of the synthetic time series but increase the time taken for training. We set the start epoch to 10 for GRU, 5 for LSTM, and 2 for DLinear, TCN and Transformer.

##### Reinforcement Learning Environment

To evaluate the transferability of the adaptive planner in downstream trading,
we implement a single-asset discrete-action trading environment.
The environment simulates realistic trading dynamics with transaction costs and evolving net value.

Action space, position, and valuation.
We use an all-in/all-out regime with discrete actions
atâˆˆ{âˆ’1,0,1}a\_{t}\in\{-1,0,1\} for *sell*, *hold*, *buy*.
Let stâ‰¥0s\_{t}\geq 0 denote the number of shares held at tt,
casht\text{cash}\_{t} the cash balance, PtP\_{t} the adjusted close,
and Vt=casht+stâ€‹PtV\_{t}=\text{cash}\_{t}+s\_{t}P\_{t} the portfolio value.
A proportional transaction cost c=10âˆ’3c=10^{-3} applies to traded notional.

*Execution:*
If at=1a\_{t}=1 (enter long) and the account was in cash, invest all money:

|  |  |  |
| --- | --- | --- |
|  | st=(1âˆ’c)â€‹Vtâˆ’1Pt,casht=0.s\_{t}=\frac{(1-c)\,V\_{t-1}}{P\_{t}},\qquad\text{cash}\_{t}=0. |  |

If at=âˆ’1a\_{t}=-1 (exit to cash) and the account was fully invested, liquidate all shares:

|  |  |  |
| --- | --- | --- |
|  | st=0,casht=(1âˆ’c)â€‹Vtâˆ’1.s\_{t}=0,\qquad\text{cash}\_{t}=(1-c)\,V\_{t-1}. |  |

If at=0a\_{t}=0 (hold), or the action repeats the current regime (already all-in or all-cash), no trade occurs:

|  |  |  |
| --- | --- | --- |
|  | st=stâˆ’1,casht=cashtâˆ’1.s\_{t}=s\_{t-1},\qquad\text{cash}\_{t}=\text{cash}\_{t-1}. |  |

The portfolio then satisfies

|  |  |  |
| --- | --- | --- |
|  | Vt=casht+stâ€‹Pt,V\_{t}=\text{cash}\_{t}+s\_{t}P\_{t}, |  |

with costs incurred only on regime switches (buy/sell), i.e., when a trade occurs.

RL agents.
We integrate this environment with two standard reinforcement learning algorithms:
(i)Â Deep Q-Network (DQN)Â [[16](https://arxiv.org/html/2601.10143v1#bib.bib46 "Human-level control through deep reinforcement learning")], which learns a stateâ€“action value function
QÎ¸â€‹(s,a)Q\_{\theta}(s,a) via temporal-difference learning, and
(ii)Â Proximal Policy Optimization (PPO)Â [[23](https://arxiv.org/html/2601.10143v1#bib.bib47 "Proximal policy optimization algorithms")],
which optimizes a clipped surrogate objective for the policy Ï€Î¸â€‹(a|s)\pi\_{\theta}(a|s).
Both agents share the same reward structure and hyper-parameters.

### V-B Main Results

#### V-B1 Forecasting

Five representative task models from different model families were used to test our method. GRU and LSTM capture temporal dependencies, Transformers handle long-range dependencies with self-attention, TCN employs convolution for sequence modeling, and DLinear uses linear decomposition. We train the models on a one-day close return forecasting task with a 60-step lookback window and evaluate their performance using MSE, MAE, and the standard deviation of the loss over each timestep in the whole test range to assess the prediction robustness as shown in Table [II](https://arxiv.org/html/2601.10143v1#S4.T2 "TABLE II â€£ IV-C Adaptive Curriculum â€£ IV Method â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis").
Results show that our method achieves the best performance, significantly reducing MSE, MAE, and STD across all models. Additionally, we observe that different models respond differently to augmented data. Stronger models such as GRU, LSTM, and Transformer, which have smaller initial losses, perform well with workflows like RandAug and TrivialAug. Conversely, applying these workflows to weaker models such as DLinear and TCN may have a detrimental effect, highlighting the need for an adaptive planner to ensure a model-agnostic workflow. Furthermore, the performance improvement over AdaAug demonstrates the efficacy of the scheduler.

TABLE III: Performance comparison of trading results when our method is integrated with RL methods.

| Method | MCD | | IBM | | INTC | |
| --- | --- | --- | --- | --- | --- | --- |
| TRâ†‘\uparrow | SRâ†‘\uparrow | TRâ†‘\uparrow | SRâ†‘\uparrow | TRâ†‘\uparrow | SRâ†‘\uparrow |
| DQN | 4.78 | 5.06 | 13.21 | 13.55 | 35.99 | 16.80 |
| DQN + Ours | 17.73 | 25.74 | 13.88 | 14.80 | 33.35 | 21.60 |
| PPO | 15.42 | 21.01 | -3.62 | -7.43 | 34.67 | 17.49 |
| PPO + Ours | 18.13 | 26.31 | -2.80 | -5.68 | 52.91 | 23.45 |

#### V-B2 Transfer to Reinforcement Learning Trading

A key quantitative trading task is utilizing reinforcement learning (RL) for trading decisions [[19](https://arxiv.org/html/2601.10143v1#bib.bib25 "Earnhft: efficient hierarchical reinforcement learning for high frequency trading"), [37](https://arxiv.org/html/2601.10143v1#bib.bib40 "MacroHFT: memory augmented context-aware reinforcement learning on high frequency trading")]. To evaluate the transferability of a learned policy, we conducted an experiment where the planner gÏ•g\_{\phi}, trained on a one-day return forecasting task with an LSTM task model, was applied to a single-stock trading task using DQN [[16](https://arxiv.org/html/2601.10143v1#bib.bib46 "Human-level control through deep reinforcement learning")] and PPO [[23](https://arxiv.org/html/2601.10143v1#bib.bib47 "Proximal policy optimization algorithms")]. To ensure fair comparisons without introducing additional data, all mix-up operations were removed. The trained planner gÏ•g\_{\phi} was then integrated into the RL single-stock training workflow. For the scheduler, we used a simplified approach where Î±=0\alpha=0 was set for the first 2Ã—1052\times 10^{5} training steps, Î±=0.05\alpha=0.05 was applied for the next 1Ã—1051\times 10^{5} steps, and Î±=0\alpha=0 was again set for the remaining steps to ensure convergence. For both DQN and PPO, the embedding dimensions were set to 128, depth at 1, initial amount at 1Ã—1041\times 10^{4}, transaction cost at 1Ã—10âˆ’31\times 10^{-3} and discount rate Î³\gamma at 0.99. For DQN, the policy learning rate is set at 2.5Ã—10âˆ’42.5\times 10^{-4}, exploration fraction at 0.5, train frequency at 10, batch size at 128 and target network update frequency at 500. For PPO, the policy learning rate is set at 5Ã—10âˆ’75\times 10^{-7}, value learning rate at 1Ã—10âˆ’61\times 10^{-6}, generalized advantage estimation Î»\lambda at 0.95, value function coefficient at 0.5, entropy coefficient at 0.01 and target KL at 0.02. We use the Total Return (TR) to measure profitability, and the Sharpe Ratio (SR) to assess risk control ability. Formally, they are defined as:

|  |  |  |  |
| --- | --- | --- | --- |
|  | TR=Ptâˆ’P0P0\text{TR}=\frac{P\_{t}-P\_{0}}{P\_{0}} |  | (19) |

where PtP\_{t} and P0P\_{0} denote the final and initial portfolio values, respectively.

|  |  |  |  |
| --- | --- | --- | --- |
|  | SR=ğ”¼â€‹[return]Ïƒâ€‹(return)\text{SR}=\frac{\mathbb{E}[\text{return}]}{\sigma(\text{return})} |  | (20) |

where ğ”¼â€‹[return]\mathbb{E}[\text{return}] denotes the expected return and Ïƒâ€‹(return)\sigma(\text{return}) represents the standard deviation of returns.
As shown in Table [III](https://arxiv.org/html/2601.10143v1#S5.T3 "TABLE III â€£ V-B1 Forecasting â€£ V-B Main Results â€£ V Experiments â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis"), our method increased profit and reduced risk, demonstrating the transferability. We conducted a case study of the DQN results on INTC, where we obtained a slightly lower total return (TR) but a much higher risk-adjusted return (SR) than the baseline method, indicating superior risk control, as shown in Fig.Â [4](https://arxiv.org/html/2601.10143v1#S5.F4 "Figure 4 â€£ V-B2 Transfer to Reinforcement Learning Trading â€£ V-B Main Results â€£ V Experiments â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis"). From the trading decisions, we can see that our method helps the DQN make more prudent actions, such as selling holdings before a downtrend, thus avoiding risk. This improved performance may result from encountering more diverse market scenarios during the training stage, alleviating the poor generalization caused by concept drift.

![Refer to caption](figs/dqn_intc_original.png)


(a) DQN on INTC

![Refer to caption](figs/dqn_intc_madaug.png)


(b) DQN + Ours on INTC

Figure 4: Trading results where buy and sell actions are marked with green and red labels.

### V-C Ablation Study

To assess the contribution of each component in our proposed data manipulation pipeline, we perform ablation experiments on the cryptocurrency dataset by systematically disabling individual modules. Removing the multi-stock mixup module leads to consistent increases in MSE, MAE, and STD across all forecasting models, indicating that cross-asset information is beneficial for learning more generalizable temporal dynamics. Likewise, replacing our adaptive scheduler with a fixed oneâ€”corresponding to the adapted AdaAug baselineâ€”results in degraded performance, highlighting the importance of a learnable curriculum that adjusts augmentation intensity over time. Furthermore, disabling both the scheduler and planner (i.e., using randomly selected augmentations as in TrivialAug and RandAug) yields further performance deterioration, particularly for TCN and Transformer architectures. These findings suggest that augmentations must be dynamically and model-specifically controlled throughout training. Overall, the results confirm that the combination of mixup, adaptive scheduling, and a learned planner substantially enhances both the robustness and predictive accuracy of our forecasting framework.

![Refer to caption](figs/weights/transformer.png)


(a) Transformer

![Refer to caption](figs/weights/LSTM_weights.png)


(b) LSTM

Figure 5: Operation weights from planner for (a) Transformer and (b) LSTM.



|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | Amplitude Mix | Cut Mix | Linear Mix | [[6](https://arxiv.org/html/2601.10143v1#bib.bib2 "Finding order in chaos: a novel data augmentation method for time series in contrastive learning")] Mix |
| Inner Permute |  |  |  |  |
| Jittering |  |  |  |  |
| Permutation |  |  |  |  |
| Scaling |  |  |  |  |

![Refer to caption](figs/plots_tsne_mag/inner_permutation_amplitude_mix.png)

![Refer to caption](figs/plots_tsne_mag/inner_permutation_cut_mix.png)

![Refer to caption](figs/plots_tsne_mag/inner_permutation_linear_mix.png)

![Refer to caption](figs/plots_tsne_mag/inner_permutation_proposed_mix.png)

![Refer to caption](figs/plots_tsne_mag/jittering_amplitude_mix.png)

![Refer to caption](figs/plots_tsne_mag/jittering_cut_mix.png)

![Refer to caption](figs/plots_tsne_mag/jittering_linear_mix.png)

![Refer to caption](figs/plots_tsne_mag/jittering_proposed_mix.png)

![Refer to caption](figs/plots_tsne_mag/permutation_amplitude_mix.png)

![Refer to caption](figs/plots_tsne_mag/permutation_cut_mix.png)

![Refer to caption](figs/plots_tsne_mag/permutation_linear_mix.png)

![Refer to caption](figs/plots_tsne_mag/permutation_proposed_mix.png)

![Refer to caption](figs/plots_tsne_mag/scaling_amplitude_mix.png)

![Refer to caption](figs/plots_tsne_mag/scaling_cut_mix.png)

![Refer to caption](figs/plots_tsne_mag/scaling_linear_mix.png)

![Refer to caption](figs/plots_tsne_mag/scaling_proposed_mix.png)

Figure 6: Comparison of data distribution as the parameters vary.
Y-axis shows single-stock augmentations while X-axis shows multi-stock mixups. Dots of different colors represent synthetic data from varying manipulation strength Î»\lambda.

### V-D Augmented Data Quality Evaluation

To further ensure the safety of using augmented data, we evaluate the quality of our augmented data from a financial perspective both qualitatively and quantitatively.

TABLE IV: Discriminative accuracy (Acc â€“ 50%)â†“\downarrow and stylizedâ€facts fidelity (ACF returns, ACF absolute returns, leverage correlation) for various generative methods. A lower discriminative score and smaller stylizedâ€facts differences both indicate better performance.

| Method | Dis. | ret | abs ret | Lev corr |
| --- | --- | --- | --- | --- |
| TimeGAN | 48.2 | 0.0231 | 0.00987 | 0.0263 |
| SIGCWGAN | 48.3 | 0.0625 | 0.0113 | 0.0678 |
| RCWGAN | 48.5 | 0.0278 | 0.0142 | 0.00776 |
| GMMN | 49.6 | 0.0280 | 0.0555 | 0.146 |
| CWGAN | 48.5 | 0.0118 | 0.03177 | 0.0309 |
| RCGAN | 48.0 | 0.0210 | 0.0154 | 0.0251 |
| Diffusion-TS | 42.4 | 0.0361 | 0.026 | 0.022 |
| Ours | 14.1 | 0.000133 | 0.000478 | 0.00224 |

##### Addressing Concept Drift

To assess how our augmented data mitigates concept drift, we compare the training and test distributions in Fig.Â [7](https://arxiv.org/html/2601.10143v1#S5.F7 "Figure 7 â€£ Discriminative Score â€£ V-D Augmented Data Quality Evaluation â€£ V Experiments â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis"). In (a), the original training set clusters further from the test set, indicating potential drift. In contrast, (b) demonstrates that the augmented training samples cluster more closely to the test set, highlighting the ability of our pipeline to address concept drift.

##### Variable Controllability

To visualize how our data manipulation parameters affect augmentation, we present t-SNE plots in Fig.Â [6](https://arxiv.org/html/2601.10143v1#S5.F6 "Figure 6 â€£ V-C Ablation Study â€£ V Experiments â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis"), showing that the module is tunable and parameter changes lead to gradual, predictable shifts in the synthetic data distribution, providing diverse and controlled augmented data.

##### Downstream Usability

We assess the suitability of the operations for financial time series with a general experiment. An LSTM was used to forecast the close returns of stocks in DJI index and we use the classification accuracy of the return direction as labels. The operations were applied to the entire training set respectively to determine if the augmented data was useful. The results in Table [V](https://arxiv.org/html/2601.10143v1#S5.T5 "TABLE V â€£ Market Stylized Facts â€£ V-D Augmented Data Quality Evaluation â€£ V Experiments â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis") indicate that all selected operations improved the forecasting accuracy over the baseline of using original historical data.

##### Discriminative Score

Following TimeGAN, we evaluate the fidelity of augmented data using post-hoc RNN classifiers to distinguish real from augmented data. An accuracy of 50%50\% indicates indistinguishability, and fidelity is measured as the classifierâ€™s accuracy minus 50%50\%. Although our pipeline is not designed to be a generative model, we include a comparison with deep generative models. This comparison provides additional insight into the closeness of our augmented data to the real distribution. As observed in Table [IV](https://arxiv.org/html/2601.10143v1#S5.T4 "TABLE IV â€£ V-D Augmented Data Quality Evaluation â€£ V Experiments â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis"), our method achieves the lowest discriminative score, showing the highest financial fidelity.

![Refer to caption](figs/tsne_ori_vs_aug/original.png)


(a) t-SNE of original data

![Refer to caption](x11.png)


(b) t-SNE of augmented data

Figure 7: 
t-SNE plots comparing original and augmented data.
The red box highlights that the augmented training data (orange)
becomes distributionally closer to test data (green).

##### Market Stylized Facts

Beyond classification-based metrics, it is crucial to confirm that the augmented data adheres to well-established stylized facts known to characterize real financial markets.Concretely, we compute three key statistical properties commonly observed in financial markets: the autocorrelation of returns, the autocorrelation of absolute returns, and the leverage effect. They are defined as follows:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ïrâ€‹(k)=Covâ€‹(rt,rtâˆ’k)Varâ€‹(rt)\rho\_{r}(k)=\frac{\text{Cov}(r\_{t},r\_{t-k})}{\text{Var}(r\_{t})} |  | (21) |

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ï|r|â€‹(k)=Covâ€‹(|rt|,|rtâˆ’k|)Varâ€‹(|rt|)\rho\_{|r|}(k)=\frac{\text{Cov}(|r\_{t}|,|r\_{t-k}|)}{\text{Var}(|r\_{t}|)} |  | (22) |

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ïr,Ïƒâ€‹(k)=Corrâ€‹(rt,Ïƒt+k)\rho\_{r,\sigma}(k)=\text{Corr}(r\_{t},\sigma\_{t+k}) |  | (23) |

where Ïrâ€‹(k)\rho\_{r}(k) and Ï|r|â€‹(k)\rho\_{|r|}(k) denote the lag-kk autocorrelation of returns and absolute returns, respectively, and Ïr,Ïƒâ€‹(k)\rho\_{r,\sigma}(k) represents the leverage effect. The autocorrelation of returns helps evaluate market efficiency, the autocorrelation of absolute returns is central to risk modeling, and the leverage effect is crucial for designing volatility models and assessing risk.

We evaluate how faithfully the augmented series captures core market dynamics. As shown in TableÂ [IV](https://arxiv.org/html/2601.10143v1#S5.T4 "TABLE IV â€£ V-D Augmented Data Quality Evaluation â€£ V Experiments â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis"), our augmented data exhibits stylized facts that most closely align with those found in true financial data, underscoring the practical relevance of our augmentation pipeline for financial tasks.

TABLE V: Accuracy after applying operations in training.

|  |  |
| --- | --- |
| Operation | Accuracy (%) |
| None | 50.72 |
| Jittering | 52.06 |
| Scaling | 51.54 |
| Magnitude Warping | 51.75 |
| Permutation | 51.95 |
| STL Augment | 51.75 |
| Cut Mix | 52.37 |
| Linear Mix | 51.85 |
| Amplitude Mix | 52.16 |
| [[6](https://arxiv.org/html/2601.10143v1#bib.bib2 "Finding order in chaos: a novel data augmentation method for time series in contrastive learning")] Mix | 51.65 |

### V-E Additional Analyses

##### Operation Weights

We visualize the weights of the probabilistic operations pp in Fig.Â [5(a)](https://arxiv.org/html/2601.10143v1#S5.F5.sf1 "In Figure 5 â€£ V-C Ablation Study â€£ V Experiments â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis") and Fig.Â [5(b)](https://arxiv.org/html/2601.10143v1#S5.F5.sf2 "In Figure 5 â€£ V-C Ablation Study â€£ V Experiments â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis") with our provenance aware replay. As observed, pp evolves as the task model undergoes training. The weights of these operations also vary significantly between different models, indicating that a planner is essential for a model-agnostic adaptive policy.

![Refer to caption](x12.png)


Figure 8: The training and validation loss curve w/wo our workflow applied.

##### Learning Curve

We conducted a qualitative analysis to understand why our method enhances performance, as shown in Fig.Â [8](https://arxiv.org/html/2601.10143v1#S5.F8 "Figure 8 â€£ Operation Weights â€£ V-E Additional Analyses â€£ V Experiments â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis"). Compared to the original workflow, our approach results in a lower validation loss, indicating successful generalization and effectively addressing potential overfitting caused by concept drift.

## VI Conclusion

In this paper, we introduced a novel adaptive dataflow system designed to bridge the gap between training and real-world performance in quantitative finance. To the best of our knowledge, this is the first workflow of its kind applied to quantitative finance tasks. The framework integrates a parameterized data manipulation module with a learning-guided plannerâ€“scheduler, forming a feedback loop that dynamically regulates manipulation strength and proportion of data to be manipulated as the model evolves.
This design allows the data pipeline to self-adjust to distributional drift, ensuring consistent data quality and realistic synthesis throughout the learning process.
Experiments on forecasting and trading tasks demonstrate that the system improves robustness and generalization across models and markets.

## References

* [1]
  Y. Bengio, J. Louradour, R. Collobert, and J. Weston (2009)
  Curriculum learning.
  In Proceedings of the 26th Annual International Conference on Machine Learning,
   pp.Â 41â€“48.
  Cited by: [Â§II](https://arxiv.org/html/2601.10143v1#S2.p2.1 "II Background and Related Works â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis").
* [2]
  T. Cheung and D. Yeung (2021)
  Adaaug: learning class-and instance-adaptive data augmentation policies.
  In International Conference on Learning Representations,
  Cited by: [Â§I](https://arxiv.org/html/2601.10143v1#S1.p3.1 "I Introduction â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis"),
  [Â§II](https://arxiv.org/html/2601.10143v1#S2.p2.1 "II Background and Related Works â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis"),
  [Â§III-D](https://arxiv.org/html/2601.10143v1#S3.SS4.p1.1 "III-D Formulation of the Adaptive Control Objective â€£ III Preliminaries â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis").
* [3]
  R. B. Cleveland, W. S. Cleveland, J. E. McRae, I. Terpenning, et al. (1990)
  STL: a seasonal-trend decomposition.
  J. off. Stat 6 (1),  pp.Â 3â€“73.
  Cited by: [3rd item](https://arxiv.org/html/2601.10143v1#S3.I2.i3.p1.1 "In III-E Augmentation Operations â€£ III Preliminaries â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis"),
  [5th item](https://arxiv.org/html/2601.10143v1#S4.I1.i5.p1.1 "In IV-B Parameterized Data Manipulation Module â€£ IV Method â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis").
* [4]
  E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, and Q. V. Le (2018)
  Autoaugment: learning augmentation policies from data.
  arXiv preprint arXiv:1805.09501.
  Cited by: [Â§II](https://arxiv.org/html/2601.10143v1#S2.p1.1 "II Background and Related Works â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis"),
  [Â§IV-C](https://arxiv.org/html/2601.10143v1#S4.SS3.p5.5 "IV-C Adaptive Curriculum â€£ IV Method â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis").
* [5]
  E. D. Cubuk, B. Zoph, J. Shlens, and Q. V. Le (2020)
  Randaugment: practical automated data augmentation with a reduced search space.
  In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops,
   pp.Â 702â€“703.
  Cited by: [Â§IV-C](https://arxiv.org/html/2601.10143v1#S4.SS3.p5.5 "IV-C Adaptive Curriculum â€£ IV Method â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis").
* [6]
  B. U. Demirel and C. Holz (2024)
  Finding order in chaos: a novel data augmentation method for time series in contrastive learning.
  Advances in Neural Information Processing Systems 36.
  Cited by: [Â§II](https://arxiv.org/html/2601.10143v1#S2.p1.1 "II Background and Related Works â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis"),
  [Figure 6](https://arxiv.org/html/2601.10143v1#S5.F6.20.1.1.5 "In V-C Ablation Study â€£ V Experiments â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis"),
  [TABLE V](https://arxiv.org/html/2601.10143v1#S5.T5.4.11.11.1 "In Market Stylized Facts â€£ V-D Augmented Data Quality Evaluation â€£ V Experiments â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis").
* [7]
  C. Esteban, S. L. Hyland, and G. RÃ¤tsch (2017)
  Real-valued (medical) time series generation with recurrent conditional gans.
  arXiv preprint arXiv:1706.02633.
  Cited by: [Â§V-A](https://arxiv.org/html/2601.10143v1#S5.SS1.SSS0.Px2.p1.1 "Benchmark for Augmented Data â€£ V-A Experiment Setup â€£ V Experiments â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis").
* [8]
  M. D. Gould, M. A. Porter, S. Williams, M. McDonald, D. J. Fenn, and S. D. Howison (2013)
  Limit order books.
  External Links: 1012.0349
  Cited by: [Â§II](https://arxiv.org/html/2601.10143v1#S2.p3.1 "II Background and Related Works â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis").
* [9]
  Y. He, X. Li, J. Ma, S. Lu, and Q. Zhu (2022)
  A novel virtual sample generation method based on a modified conditional wasserstein gan to address the small sample size problem in soft sensing.
  Journal of Process Control 113,  pp.Â 18â€“28.
  Cited by: [Â§V-A](https://arxiv.org/html/2601.10143v1#S5.SS1.SSS0.Px2.p1.1 "Benchmark for Augmented Data â€£ V-A Experiment Setup â€£ V Experiments â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis").
* [10]
  S. C. Hora (1996)
  Aleatory and epistemic uncertainty in probability elicitation with an example from hazardous waste management.
  Reliability Engineering & System Safety 54 (2-3),  pp.Â 217â€“223.
  Cited by: [Â§I](https://arxiv.org/html/2601.10143v1#S1.p2.1 "I Introduction â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis").
* [11]
  C. Hou, J. Zhang, and T. Zhou (2023)
  When to learn what: model-adaptive data augmentation curriculum.
  In Proceedings of the IEEE/CVF International Conference on Computer Vision,
   pp.Â 1717â€“1728.
  Cited by: [Â§I](https://arxiv.org/html/2601.10143v1#S1.p3.1 "I Introduction â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis"),
  [Â§II](https://arxiv.org/html/2601.10143v1#S2.p2.1 "II Background and Related Works â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis"),
  [Â§IV-C](https://arxiv.org/html/2601.10143v1#S4.SS3.p7.7 "IV-C Adaptive Curriculum â€£ IV Method â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis").
* [12]
  S. Kapoor, W. J. Maddox, P. Izmailov, and A. G. Wilson (2022)
  On uncertainty, tempering, and data augmentation in bayesian classification.
  Advances in Neural Information Processing Systems 35,  pp.Â 18211â€“18225.
  Cited by: [Â§I](https://arxiv.org/html/2601.10143v1#S1.p2.1 "I Introduction â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis").
* [13]
  W. Li, X. Yang, W. Liu, Y. Xia, and J. Bian (2022)
  DDG-da: data distribution generation for predictable concept drift adaptation.
  In Proceedings of the AAAI Conference on Artificial Intelligence,
  Vol. 36,  pp.Â 4092â€“4100.
  Cited by: [Â§I](https://arxiv.org/html/2601.10143v1#S1.p3.1 "I Introduction â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis").
* [14]
  Y. Li, G. Hu, Y. Wang, T. Hospedales, N. M. Robertson, and Y. Yang (2020)
  Differentiable automatic data augmentation.
  In Computer Visionâ€“ECCV 2020: 16th European Conference,
   pp.Â 580â€“595.
  Cited by: [Â§IV-C](https://arxiv.org/html/2601.10143v1#S4.SS3.p9.2 "IV-C Adaptive Curriculum â€£ IV Method â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis").
* [15]
  Y. Li, K. Swersky, and R. Zemel (2015)
  Generative moment matching networks.
  External Links: 1502.02761
  Cited by: [Â§V-A](https://arxiv.org/html/2601.10143v1#S5.SS1.SSS0.Px2.p1.1 "Benchmark for Augmented Data â€£ V-A Experiment Setup â€£ V Experiments â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis").
* [16]
  V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. (2015)
  Human-level control through deep reinforcement learning.
  Nature 518 (7540),  pp.Â 529â€“533.
  Cited by: [Â§V-A](https://arxiv.org/html/2601.10143v1#S5.SS1.SSS0.Px5.p4.2 "Reinforcement Learning Environment â€£ V-A Experiment Setup â€£ V Experiments â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis"),
  [Â§V-B2](https://arxiv.org/html/2601.10143v1#S5.SS2.SSS2.p1.14 "V-B2 Transfer to Reinforcement Learning Trading â€£ V-B Main Results â€£ V Experiments â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis").
* [17]
  H. Ni, L. Szpruch, M. Wiese, S. Liao, and B. Xiao (2020)
  Conditional sig-wasserstein gans for time series generation.
  arXiv preprint arXiv:2006.05421.
  Cited by: [Â§V-A](https://arxiv.org/html/2601.10143v1#S5.SS1.SSS0.Px2.p1.1 "Benchmark for Augmented Data â€£ V-A Experiment Setup â€£ V Experiments â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis").
* [18]
  T. Preis, S. Golke, W. Paul, and J. J. Schneider (2007-07)
  Statistical analysis of financial returns for a multiagent order book model of asset trading.
  Phys. Rev. E 76,  pp.Â 016108.
  External Links: [Document](https://dx.doi.org/10.1103/PhysRevE.76.016108),
  [Link](https://link.aps.org/doi/10.1103/PhysRevE.76.016108)
  Cited by: [Â§II](https://arxiv.org/html/2601.10143v1#S2.p3.1 "II Background and Related Works â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis").
* [19]
  M. Qin, S. Sun, W. Zhang, H. Xia, X. Wang, and B. An (2024)
  Earnhft: efficient hierarchical reinforcement learning for high frequency trading.
  In Proceedings of the AAAI Conference on Artificial Intelligence,
  Vol. 38,  pp.Â 14669â€“14676.
  Cited by: [Â§V-B2](https://arxiv.org/html/2601.10143v1#S5.SS2.SSS2.p1.14 "V-B2 Transfer to Reinforcement Learning Trading â€£ V-B Main Results â€£ V Experiments â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis").
* [20]
  S. K. Sahu, A. Mokhade, and N. D. Bokde (2023)
  An overview of machine learning, deep learning, and reinforcement learning-based techniques in quantitative finance: recent progress and challenges.
  Applied Sciences 13 (3),  pp.Â 1956.
  Cited by: [Â§I](https://arxiv.org/html/2601.10143v1#S1.p1.5 "I Introduction â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis").
* [21]
  E. Samanidou, E. Zschischang, D. Stauffer, and T. Lux (2007)
  Agent-based models of financial markets.
  Reports on Progress in Physics 70 (3),  pp.Â 409.
  Cited by: [Â§I](https://arxiv.org/html/2601.10143v1#S1.p2.1 "I Introduction â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis"),
  [Â§II](https://arxiv.org/html/2601.10143v1#S2.p3.1 "II Background and Related Works â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis").
* [22]
  S. Saxena, O. Tuzel, and D. DeCoste (2019)
  Data parameters: a new family of parameters for learning a differentiable curriculum.
  Advances in Neural Information Processing Systems 32.
  Cited by: [Â§IV-C](https://arxiv.org/html/2601.10143v1#S4.SS3.p2.27 "IV-C Adaptive Curriculum â€£ IV Method â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis").
* [23]
  J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov (2017)
  Proximal policy optimization algorithms.
  arXiv preprint arXiv:1707.06347.
  Cited by: [Â§V-A](https://arxiv.org/html/2601.10143v1#S5.SS1.SSS0.Px5.p4.2 "Reinforcement Learning Environment â€£ V-A Experiment Setup â€£ V Experiments â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis"),
  [Â§V-B2](https://arxiv.org/html/2601.10143v1#S5.SS2.SSS2.p1.14 "V-B2 Transfer to Reinforcement Learning Trading â€£ V-B Main Results â€£ V Experiments â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis").
* [24]
  T. B. Shahi, A. Shrestha, A. Neupane, and W. Guo (2020)
  Stock price forecasting with deep learning: a comparative study.
  Mathematics 8 (9),  pp.Â 1441.
  Cited by: [Â§I](https://arxiv.org/html/2601.10143v1#S1.p1.5 "I Introduction â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis").
* [25]
  Z. Shi and J. Cartlidge (2023)
  Neural stochastic agent-based limit order book simulation: a hybrid methodology.
  arXiv preprint arXiv:2303.00080.
  Cited by: [Â§I](https://arxiv.org/html/2601.10143v1#S1.p2.1 "I Introduction â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis"),
  [Â§II](https://arxiv.org/html/2601.10143v1#S2.p3.1 "II Background and Related Works â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis").
* [26]
  J. Shu, Q. Xie, L. Yi, Q. Zhao, S. Zhou, Z. Xu, and D. Meng (2019)
  Meta-weight-net: learning an explicit mapping for sample weighting.
  Advances in Neural Information Processing Systems 32.
  Cited by: [Â§IV-C](https://arxiv.org/html/2601.10143v1#S4.SS3.p7.7 "IV-C Adaptive Curriculum â€£ IV Method â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis").
* [27]
  S. Sun, M. Qin, W. Zhang, H. Xia, C. Zong, J. Ying, Y. Xie, L. Zhao, X. Wang, and B. An (2024)
  TradeMaster: a holistic quantitative trading platform empowered by reinforcement learning.
  Advances in Neural Information Processing Systems 36.
  Cited by: [Â§I](https://arxiv.org/html/2601.10143v1#S1.p1.5 "I Introduction â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis").
* [28]
  T. T. Um, F. M. Pfister, D. Pichler, S. Endo, M. Lang, S. Hirche, U. Fietzek, and D. KuliÄ‡ (2017)
  Data augmentation of wearable sensor data for parkinsonâ€™s disease monitoring using convolutional neural networks.
  In Proceedings of the 19th ACM international conference on multimodal interaction,
   pp.Â 216â€“220.
  Cited by: [1st item](https://arxiv.org/html/2601.10143v1#S3.I2.i1.p1.1 "In III-E Augmentation Operations â€£ III Preliminaries â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis"),
  [3rd item](https://arxiv.org/html/2601.10143v1#S4.I1.i3.p1.1 "In IV-B Parameterized Data Manipulation Module â€£ IV Method â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis").
* [29]
  L. Van der Maaten and G. Hinton (2008)
  Visualizing data using t-sne..
  Journal of Machine Learning Research 9 (11).
  Cited by: [Â§III-B](https://arxiv.org/html/2601.10143v1#S3.SS2.p1.8 "III-B Observation of Concept Drift in Financial Data â€£ III Preliminaries â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis").
* [30]
  S. Vyetrenko, D. Byrd, N. Petosa, M. Mahfouz, D. Dervovic, M. Veloso, and T. Balch (2020)
  Get real: realism metrics for robust limit order book market simulations.
  In Proceedings of the First ACM International Conference on AI in Finance,
   pp.Â 1â€“8.
  Cited by: [Â§II](https://arxiv.org/html/2601.10143v1#S2.p3.1 "II Background and Related Works â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis").
* [31]
  H. Xia, S. Sun, X. Wang, and B. An (2024)
  Market-gan: adding control to financial market data generation with semantic context.
  In Proceedings of the AAAI Conference on Artificial Intelligence,
  Vol. 38,  pp.Â 15996â€“16004.
  Cited by: [Â§I](https://arxiv.org/html/2601.10143v1#S1.p2.1 "I Introduction â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis").
* [32]
  J. Yoon, D. Jarrett, and M. Van der Schaar (2019)
  Time-series generative adversarial networks.
  Advances in Neural Information Processing Systems 32.
  Cited by: [Â§II](https://arxiv.org/html/2601.10143v1#S2.p3.1 "II Background and Related Works â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis").
* [33]
  Y. Yu, B. Tang, R. Lin, S. Han, T. Tang, and M. Chen (2019)
  CWGAN: conditional wasserstein generative adversarial nets for fault data generation.
  In 2019 IEEE International Conference on Robotics and Biomimetics (ROBIO),
   pp.Â 2713â€“2718.
  Cited by: [Â§V-A](https://arxiv.org/html/2601.10143v1#S5.SS1.SSS0.Px2.p1.1 "Benchmark for Augmented Data â€£ V-A Experiment Setup â€£ V Experiments â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis").
* [34]
  X. Yuan and Y. Qiao (2024)
  Diffusion-ts: interpretable diffusion for general time series generation.
  arXiv preprint arXiv:2403.01742.
  Cited by: [Â§II](https://arxiv.org/html/2601.10143v1#S2.p3.1 "II Background and Related Works â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis").
* [35]
  W. Zhang, L. Zhao, H. Xia, S. Sun, J. Sun, M. Qin, X. Li, Y. Zhao, Y. Zhao, X. Cai, et al. (2024)
  FinAgent: a multimodal foundation agent for financial trading: tool-augmented, diversified, and generalist.
  arXiv preprint arXiv:2402.18485.
  Cited by: [Â§II](https://arxiv.org/html/2601.10143v1#S2.p3.1 "II Background and Related Works â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis").
* [36]
  H. Zhou, S. Zhang, J. Peng, S. Zhang, J. Li, H. Xiong, and W. Zhang (2021)
  Informer: beyond efficient transformer for long sequence time-series forecasting.
  In Proceedings of the AAAI Conference on Artificial Intelligence,
  Vol. 35,  pp.Â 11106â€“11115.
  Cited by: [Â§III-B](https://arxiv.org/html/2601.10143v1#S3.SS2.p1.8 "III-B Observation of Concept Drift in Financial Data â€£ III Preliminaries â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis").
* [37]
  C. Zong, C. Wang, M. Qin, L. Feng, X. Wang, and B. An (2024)
  MacroHFT: memory augmented context-aware reinforcement learning on high frequency trading.
  arXiv preprint arXiv:2406.14537.
  Cited by: [Â§V-B2](https://arxiv.org/html/2601.10143v1#S5.SS2.SSS2.p1.14 "V-B2 Transfer to Reinforcement Learning Trading â€£ V-B Main Results â€£ V Experiments â€£ History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis").