---
authors:
- Koichiro Moriya
- Akihiko Noda
doc_id: arxiv:2601.21272v1
family_id: arxiv:2601.21272
is_current: true
taxonomy:
  alpha_families: []
  asset_classes: []
  horizons: []
  themes: []
title: Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic
  Regression Models
url_abs: http://arxiv.org/abs/2601.21272v1
url_html: https://arxiv.org/html/2601.21272v1
venue: arXiv q-fin
version: 1
year: 2026
---


Koichiro Moriyaa and Akihiko Nodab
a Graduate School of Media and Governance, Keio University, 5322 Endo, Fujisawa, Kanagawa 252-0882, Japan
b School of Commerce, Meiji University, 1-1 Kanda-Surugadai, Chiyoda-ku, Tokyo 101-8301, Japan
Corresponding Author. E-mail: moriya.koichiro@keio.jp, Tel/Fax: +81-466-49-3406.

(This Version: )

Abstract: This paper proposes a new multivariate model specification test that generalizes Durbin regression to a seemingly unrelated regression (SUR) framework and reframes the Durbin approach as a GLS-class estimator. The proposed estimator explicitly models cross-equation dependence and the joint second-order dynamics of regressors and disturbances. It remains consistent under a comparatively weak dependence condition in which conventional OLS- and GLS-based estimators can be inconsistent, and it is asymptotically efficient under stronger conditions. Monte Carlo experiments indicate that the associated Wald test achieves improved size control and competitive power in finite samples, especially when combined with a bootstrap-based bias correction. An empirical application further illustrates that the proposed procedure delivers stable inference and is practically useful for multi-equation specification testing.

Keywords: Multivariate Model; Model Specification; Durbin Regression; GLS

JEL Classification Numbers: C12; C32; C38; C58; G11.

## 1 Introduction

Economic phenomena typically involve multiple outcome variables that are simultaneously determined and interact with one another. Economists commonly employ specification tests within multivariate regression frameworks to assess the validity of economic theories. A prevalent feature of these frameworks is the correlation of error terms across equations, reflecting underlying economic mechanisms. This correlation arises in various contexts, including analyses involving firms, individuals, and countries. For instance, technological spillovers within industries frequently lead to correlated regression residuals in firm-level analyses (e.g., Bernstein and Nadiri ([1989](https://arxiv.org/html/2601.21272v1#bib.bib2130 "Research and development and intra-industry spillovers: an empirical application of dynamic duality")); Jaffe ([1989](https://arxiv.org/html/2601.21272v1#bib.bib2131 "Technological opportunity and spillovers of r&d: evidence from firmsâ€™ patents,profits, and market value")); Peremans and Aelst ([2018](https://arxiv.org/html/2601.21272v1#bib.bib2121 "Robust iference for semingly unrelated regression models"))). Similarly, panel data studies involving individuals or countries often encounter error dependence resulting from unobserved heterogeneity or coordinated policy responses (e.g., Pesaran ([2006](https://arxiv.org/html/2601.21272v1#bib.bib2132 "Estimation and inference in large heterogeneous panels with a multifactor error structure")); Martin et al. ([2007](https://arxiv.org/html/2601.21272v1#bib.bib2122 "The market for elective surgery: joint estimation of supply and demand")); Baltagi and Bresson ([2011](https://arxiv.org/html/2601.21272v1#bib.bib2133 "Maximum likelihood estimation and lagrange multiplier tests for panelseemingly unrelated regressions with spatial lag and spatial errors: anapplication to hedonic housing prices in paris"))). Ignoring these cross-equation dependencies by conducting specification tests on an equation-by-equation basis can result in size distortions and misleading inferences, as demonstrated by Breusch and Pagan ([1980](https://arxiv.org/html/2601.21272v1#bib.bib2112 "The lagrange multiplier test and its applications to model specification in econometrics")), Zhou ([1991](https://arxiv.org/html/2601.21272v1#bib.bib2129 "Small sample tests of portfolio efficiency")), and Pesaran and Yamagata ([2008](https://arxiv.org/html/2601.21272v1#bib.bib2128 "Testing slope homogeneity in large panels")). Consequently, multivariate specification tests explicitly accounting for error covariance structures are essential for rigorous model evaluation.

These error dependencies become especially pronounced during periods of significant uncertainty, such as financial crises, wars, or natural disasters (e.g., Pesaran et al. ([2004](https://arxiv.org/html/2601.21272v1#bib.bib2141 "Modeling regional interdependencies using a global error-correcting macroeconometric model")); Blomberg et al. ([2004](https://arxiv.org/html/2601.21272v1#bib.bib2143 "The macroeconomic consequences of terrorism")); Barrios et al. ([2010](https://arxiv.org/html/2601.21272v1#bib.bib2142 "Global inflation"))). For example, numerous studies documented substantial residual correlations among financial assets during the Global Financial Crisis and the COVID-19 pandemic (e.g., Diebold and Yilmaz ([2009](https://arxiv.org/html/2601.21272v1#bib.bib2052 "Measuring financial asset return and volatility spillovers, with application to global equity markets")); Billio et al. ([2012](https://arxiv.org/html/2601.21272v1#bib.bib2144 "Econometric measures of connectedness and systemic risk in the finance and insurance sectors")); Zhang et al. ([2020](https://arxiv.org/html/2601.21272v1#bib.bib2145 "Financial markets under the global pandemic of covid-19"))). These findings imply that ignoring covariance structures among asset returns may lead to incorrect interpretations of their interrelationships. Therefore, appropriate multivariate specification tests remain critical for accurately validating asset pricing models. Despite this, empirical studies frequently employ the test developed by Gibbons et al. ([1989](https://arxiv.org/html/2601.21272v1#bib.bib1878 "A test of the efficiency of a given portfolio")) (GRS test), which assumes independently and identically distributed multivariate normal residuals. However, empirical evidence shows that residuals often exhibit fat tails, skewness, and other departures from normality, even after controlling for risk factors (e.g., Cont ([2001](https://arxiv.org/html/2601.21272v1#bib.bib2110 "Empirical properties of asset returns: stylized facts and statistical issues")); Harvey and Siddique ([2002](https://arxiv.org/html/2601.21272v1#bib.bib2109 "Conditional skewness in asset pricing tests")); Huang et al. ([2012](https://arxiv.org/html/2601.21272v1#bib.bib2108 "Extreme downside risk and expected stock returns"))). Indeed, Affleck-Graves and McDonald ([1989](https://arxiv.org/html/2601.21272v1#bib.bib1501 "Nonnormalities and tests of asset pricing theories")) and Zhou ([1993](https://arxiv.org/html/2601.21272v1#bib.bib1837 "Asset-pricing tests under alternative distributions")) demonstrate that violations of normality assumptions significantly inflate the Type I error rate of the GRS test.

To tackle heteroskedasticity and autocorrelation without imposing stringent distributional assumptions, econometricians developed heteroskedasticity- and autocorrelation-consistent (HAC) robust tests, initially proposed by Newey and West ([1987](https://arxiv.org/html/2601.21272v1#bib.bib233 "A simple, positive semi-definite, heteroskedasticity and autocorrelation consistent covariance matrix"), [1994](https://arxiv.org/html/2601.21272v1#bib.bib1903 "Automatic lag selection in covariance matrix estimation")) and Andrews ([1991](https://arxiv.org/html/2601.21272v1#bib.bib13 "Heteroskedasticity and autocorrelation consistent covariance matrix estimation")). However, conventional HAC-based tests often display substantial size distortions in small samples, resulting in frequent over-rejection of the null hypothesis, as shown by Den Haan and Levin ([1997](https://arxiv.org/html/2601.21272v1#bib.bib2119 "A practitionerâ€™s guide to robust covariance matrix estimation,")) and MÃ¼ller ([2014](https://arxiv.org/html/2601.21272v1#bib.bib2120 "HAC corrections for strongly autocorrelatedtime series")). To improve size accuracy, Kiefer et al. ([2000](https://arxiv.org/html/2601.21272v1#bib.bib1937 "Simple robust testing of regression hypotheses")), Kiefer and Vogelsang ([2002a](https://arxiv.org/html/2601.21272v1#bib.bib1936 "Heteroskedasticity-autocorrelation robust standard errors using the bartlett kernel without truncation"), [b](https://arxiv.org/html/2601.21272v1#bib.bib1993 "Heteroskedasticity-autocorrelation robust testing using bandwidth equal to sample size"), [2005](https://arxiv.org/html/2601.21272v1#bib.bib2115 "A new asymptotic theory for heteroskedasticity-autocorrelation robust tests")) introduced the fixed bandwidth (bb) approach, setting bandwidth as a fixed fraction of the sample size and leveraging a nonstandard limiting distribution. Extending this approach, Phillips et al. ([2006](https://arxiv.org/html/2601.21272v1#bib.bib2116 "Spectral density estimation and robust hypothesis testing using steep origin kernels without truncation")) and Phillips et al. ([2007](https://arxiv.org/html/2601.21272v1#bib.bib1994 "Long run variance estimation and robust regression testing using sharp origin kernels with no truncation")) introduced the fixed lag (pp) method based on exponentiated kernels, offering flexible bias-variance tradeoff control. Further extending these methods, Sun et al. ([2008](https://arxiv.org/html/2601.21272v1#bib.bib2117 "Optimal bandwidth selection in heteroskedasticity-autocorrelation robust testing")) derived bandwidth selection rules via Edgeworth expansions, optimizing a loss function balancing Type I and Type II errors, and Lazarus et al. ([2018](https://arxiv.org/html/2601.21272v1#bib.bib2118 "HAR inference: recommendations for practice"), [2021](https://arxiv.org/html/2601.21272v1#bib.bib1946 "The size-power tradeoff in har inference")) advanced practical plug-in bandwidth selectors.

Despite these improvements, the HAR testing literature predominantly focuses on single-equation settings. Ray and Savin ([2008](https://arxiv.org/html/2601.21272v1#bib.bib1940 "The performance of heteroskedasticity and autocorrelation robust tests: a monte carlo study with an application to the three-factor fama: french asset-pricing model")) and Ray et al. ([2009](https://arxiv.org/html/2601.21272v1#bib.bib2113 "Testing the capm revisited")) extended the HAC-based framework to multivariate systems, assessing finite-sample performance within the multifactor model of Fama and French ([1993](https://arxiv.org/html/2601.21272v1#bib.bib1868 "Common risk factors in the returns on stocks and bonds")) and the Capital Asset Pricing Model (CAPM) of Sharpe ([1964](https://arxiv.org/html/2601.21272v1#bib.bib1915 "Capital asset prices: a theory of market equilibrium under conditions of risk")) and Lintner ([1965](https://arxiv.org/html/2601.21272v1#bib.bib1898 "The valuation of risky assets and the selection of risky investments in stock portfolios and budget constraints")). Their simulations indicate increasing size distortions as dimensionality grows, likely due to nonparametric kernel estimation suffering from the curse of dimensionality. Moreover, Baillie et al. ([2024](https://arxiv.org/html/2601.21272v1#bib.bib2090 "On robust inference in time-series regression")) highlighted a fundamental theoretical limitation of HAR tests, emphasizing that regressors correlated with errors render OLS-based HAR tests theoretically invalid due to inconsistency of the OLS estimator. To ensure valid inference, they proposed a parametric specification test based on Durbin regression (Durbin [1970](https://arxiv.org/html/2601.21272v1#bib.bib2135 "Testing for serial correlation in least-squares regression when some of the regressorsare lagged dependent variables")), demonstrating through simulations its superior performance under endogeneity. However, their approach remains limited to single-equation models, ignoring cross-equation error dependencies common in asset pricing models.

To overcome these limitations, this study introduces a novel multivariate specification test that generalize Durbin regression to the seemingly unrelated regression (SUR) setting by (Zellner [1962](https://arxiv.org/html/2601.21272v1#bib.bib2111 "An efficient method of estimating seemingly unrelated regressions and tests foraggregation bias")). Specifically, we derive dynamics dependence structure between regressors and error terms, and embed them within a SUR framework. We then construct a generalized least squares (GLS) estimator that accounts for cross-equation error covariance, and derive a Wald statistic for multivariate specification tests. Monte Carlo experiments demonstrate that the proposed Wald test exhibits superior size and power properties compared to conventional tests (the GRS and HAR tests), regardless of residual distributions or dependence structures. An empirical application to Fama and French ([1993](https://arxiv.org/html/2601.21272v1#bib.bib1868 "Common risk factors in the returns on stocks and bonds"), [2015](https://arxiv.org/html/2601.21272v1#bib.bib1869 "A five-factor asset pricing model")) multifactor models further demonstrates the robustness of our Wald test and practival reliability.

The remainder of the paper is organized as follows. Section [2](https://arxiv.org/html/2601.21272v1#S2 "2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") presents the model and assumptions and develops our generalized Durbin approach by reformulating Durbinâ€™s regression framework for a SUR system and positioning it as a GLS-class estimator. It then derives the resulting estimator and its asymptotic properties and introduces the associated Wald statistic. Section [3](https://arxiv.org/html/2601.21272v1#S3 "3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") evaluates finite-sample performance via Monte Carlo experiments, with particular emphasis on size control and the effectiveness of the bootstrap-based correction. Section [4](https://arxiv.org/html/2601.21272v1#S4 "4 Empirical Application â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") applies the proposed procedures to the Famaâ€“French multifactor models and compares the resulting inference with competing GLS-based Wald tests and commonly used alternatives. Section [5](https://arxiv.org/html/2601.21272v1#S5 "5 Conclusion â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") concludes.

## 2 Model and Test

A common feature of economic phenomena is that multiple outcome variables are determined simultaneously and influence one another. In such settings, disturbances are often correlated across equations, so equation-by-equation estimation and specification testing can induce size distortions and misleading inference; see Breusch and Pagan ([1980](https://arxiv.org/html/2601.21272v1#bib.bib2112 "The lagrange multiplier test and its applications to model specification in econometrics")), Zhou ([1991](https://arxiv.org/html/2601.21272v1#bib.bib2129 "Small sample tests of portfolio efficiency")), and Pesaran and Yamagata ([2008](https://arxiv.org/html/2601.21272v1#bib.bib2128 "Testing slope homogeneity in large panels")). To obtain efficient system estimates and valid joint tests, Zellner ([1962](https://arxiv.org/html/2601.21272v1#bib.bib2111 "An efficient method of estimating seemingly unrelated regressions and tests foraggregation bias")) introduces the SUR framework, which extends single-equation models to multiple-equation systems and employs GLS weighting to exploit cross-equation error covariance. In time-series applications, empirical work often estimates the multiple-equation model by OLS and relies on [Newey and West](https://arxiv.org/html/2601.21272v1#bib.bib233 "A simple, positive semi-definite, heteroskedasticity and autocorrelation consistent covariance matrix")â€™s ([1987](https://arxiv.org/html/2601.21272v1#bib.bib233 "A simple, positive semi-definite, heteroskedasticity and autocorrelation consistent covariance matrix"); [1994](https://arxiv.org/html/2601.21272v1#bib.bib1903 "Automatic lag selection in covariance matrix estimation")) HAC standard errors for specification tests to address heteroskedasticity and autocorrelation. However, it is widely known that these finite-sample performance deteriorates for multivariate systems (e.g., Ray and Savin ([2008](https://arxiv.org/html/2601.21272v1#bib.bib1940 "The performance of heteroskedasticity and autocorrelation robust tests: a monte carlo study with an application to the three-factor fama: french asset-pricing model")); Ray et al. ([2009](https://arxiv.org/html/2601.21272v1#bib.bib2113 "Testing the capm revisited"))).

More fundamentally, when regressors are dynamically correlated with the disturbances, OLS can be inconsistent, rendering HAC-based inference invalid. Recent work by Baillie et al. ([2024](https://arxiv.org/html/2601.21272v1#bib.bib2090 "On robust inference in time-series regression")) shows that, in time-series settings, conventional OLS and even GLS estimators can be inconsistent under dynamic regressorâ€“error dependence, whereas [Durbin](https://arxiv.org/html/2601.21272v1#bib.bib2135 "Testing for serial correlation in least-squares regression when some of the regressorsare lagged dependent variables")â€™s ([1970](https://arxiv.org/html/2601.21272v1#bib.bib2135 "Testing for serial correlation in least-squares regression when some of the regressorsare lagged dependent variables")) regression remains robust. However, their analysis is confined to a single-equation environment and therefore cannot accommodate cross-equation error covariance. In addition, their specification omits an intercept and implicitly treats regressors as mean-zero; when the true model features a nonzero intercept and regressors with nonzero means, ignoring these terms induces misspecification and can undermine consistency even in a single equation. To address these limitations, we extend Durbinâ€™s framework to a multiple equation setting. We develop a generalized Durbin specification that explicitly models the joint second-order dynamics of regressors and disturbances, derive a feasible GLS estimator that accounts for cross-equational covariance, and provide conditions for consistency and efficiency.

### 2.1 Setup

We consider the following linear regressions:

|  |  |  |  |
| --- | --- | --- | --- |
|  | yi,t=Î±i+ğ’™i,tâ€²â€‹ğœ·i+ui,t,i=1,â€¦,N,t=1,â€¦,T,y\_{i,t}=\alpha\_{i}+{\mbox{$x$}}\_{i,t}^{\prime}{\mbox{$\beta$}}\_{i}+u\_{i,t},\quad i=1,\ldots,N,\quad t=1,\ldots,T, |  | (1) |

where yi,ty\_{i,t} is the scalar dependent variable, ğ’™i,t=(xi,t,1,xi,t,2,â€¦,xi,t,ki)â€²{\mbox{$x$}}\_{i,t}=(x\_{i,t,1},x\_{i,t,2},\ldots,x\_{i,t,k\_{i}})^{\prime} is a kiÃ—1k\_{i}\times 1 vector of regressors, and ui,tu\_{i,t} is a scalar disturbance. The number of regressors kik\_{i} is allowed to vary across equations.

Let ğ’št=(y1,t,â€¦,yN,t)â€²{\mbox{$y$}}\_{t}=(y\_{1,t},\ldots,y\_{N,t})^{\prime}, ğ’–t=(u1,t,â€¦,uN,t)â€²{\mbox{$u$}}\_{t}=(u\_{1,t},\ldots,u\_{N,t})^{\prime} with ğ”¼â€‹[ğ’–t]=ğŸ{\mathbb{E}}[{\mbox{$u$}}\_{t}]={\mbox{$0$}}, and define ğœ¶=(Î±1,â€¦,Î±N)â€²{\mbox{$\alpha$}}=(\alpha\_{1},\ldots,\alpha\_{N})^{\prime} and ğœ·=(ğœ·1â€²,â€¦,ğœ·Nâ€²)â€²{\mbox{$\beta$}}=({\mbox{$\beta$}}\_{1}^{\prime},\ldots,{\mbox{$\beta$}}\_{N}^{\prime})^{\prime}, where k:=âˆ‘i=1Nkik:=\sum\_{i=1}^{N}k\_{i}. Further, define the block-diagonal regressor matrix

|  |  |  |
| --- | --- | --- |
|  | ğ‘¿t=[ğ’™1,tğŸâ‹¯ğŸğŸğ’™2,tâ‹¯ğŸâ‹®â‹®â‹±â‹®ğŸğŸâ‹¯ğ’™N,t]âˆˆâ„kÃ—N.{\mbox{$X$}}\_{t}=\begin{bmatrix}{\mbox{$x$}}\_{1,t}&{\mbox{$0$}}&\cdots&{\mbox{$0$}}\\ {\mbox{$0$}}&{\mbox{$x$}}\_{2,t}&\cdots&{\mbox{$0$}}\\ \vdots&\vdots&\ddots&\vdots\\ {\mbox{$0$}}&{\mbox{$0$}}&\cdots&{\mbox{$x$}}\_{N,t}\end{bmatrix}\in\mathbb{R}^{k\times N}. |  |

Stacking the NN equations, we can write the system compactly as

|  |  |  |
| --- | --- | --- |
|  | [y1,ty2,tâ‹®yN,t]=[Î±1Î±2â‹®Î±N]+[ğ’™1,tâ€²ğŸâ‹¯ğŸğŸğ’™2,tâ€²â‹¯ğŸâ‹®â‹®â‹±â‹®ğŸğŸâ‹¯ğ’™N,tâ€²]â€‹[ğœ·1ğœ·2â‹®ğœ·N]+[u1,tu2,tâ‹®uN,t],\begin{bmatrix}y\_{1,t}\\ y\_{2,t}\\ \vdots\\ y\_{N,t}\\ \end{bmatrix}=\begin{bmatrix}\alpha\_{1}\\ \alpha\_{2}\\ \vdots\\ \alpha\_{N}\\ \end{bmatrix}+\begin{bmatrix}{\mbox{$x$}}\_{1,t}^{\prime}&{\mbox{$0$}}&\cdots&{\mbox{$0$}}\\ {\mbox{$0$}}&{\mbox{$x$}}\_{2,t}^{\prime}&\cdots&{\mbox{$0$}}\\ \vdots&\vdots&\ddots&\vdots\\ {\mbox{$0$}}&{\mbox{$0$}}&\cdots&{\mbox{$x$}}\_{N,t}^{\prime}\end{bmatrix}\begin{bmatrix}{\mbox{$\beta$}}\_{1}\\ {\mbox{$\beta$}}\_{2}\\ \vdots\\ {\mbox{$\beta$}}\_{N}\\ \end{bmatrix}+\begin{bmatrix}u\_{1,t}\\ u\_{2,t}\\ \vdots\\ u\_{N,t}\\ \end{bmatrix}, |  |

equivalently

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ’št=ğœ¶+ğ‘¿tâ€²â€‹ğœ·+ğ’–t,t=1,â€¦,T.{\mbox{$y$}}\_{t}={\mbox{$\alpha$}}+{\mbox{$X$}}\_{t}^{\prime}{\mbox{$\beta$}}+{\mbox{$u$}}\_{t},\quad t=1,\ldots,T. |  | (2) |

To describe the joint second-order dynamics of regressors and disturbances, let ğ’™t=(ğ’™1,tâ€²,â€¦,ğ’™N,tâ€²)â€²{\mbox{$x$}}\_{t}=({\mbox{$x$}}\_{1,t}^{\prime},\ldots,{\mbox{$x$}}\_{N,t}^{\prime})^{\prime} with ğ”¼â€‹[ğ’™t]=ğx{\mathbb{E}}[{\mbox{$x$}}\_{t}]={\mbox{$\mu$}}\_{x}, and define

|  |  |  |
| --- | --- | --- |
|  | ğ’›t=[ğ’™tğ’–t]âˆˆâ„m,m:=k+N,{\mbox{$z$}}\_{t}=\begin{bmatrix}{\mbox{$x$}}\_{t}\\ {\mbox{$u$}}\_{t}\\ \end{bmatrix}\in\mathbb{R}^{m},\qquad m:=k+N, |  |

with mean ğz:=[ğx,ğŸ]=ğ”¼â€‹[ğ’›t]{\mbox{$\mu$}}\_{z}:=[{\mbox{$\mu$}}\_{x},{\mbox{$0$}}]={\mathbb{E}}[{\mbox{$z$}}\_{t}]. Let ğ’›Â¯t:=ğ’›tâˆ’ğz\bar{{\mbox{$z$}}}\_{t}:={\mbox{$z$}}\_{t}-{\mbox{$\mu$}}\_{z}. We work in the Hilbert space L2â€‹(Î©,â„±,â„™;â„m)L^{2}(\Omega,\mathscr{F},\mathbb{P};\mathbb{R}^{m}) equipped with inner product âŸ¨U,VâŸ©=ğ”¼â€‹[Uâ€²â€‹V]\langle U,V\rangle={\mathbb{E}}[U^{\prime}V], and define the closed linear subspace generated by the present and past centered variables,

|  |  |  |
| --- | --- | --- |
|  | â„‹t:=spanÂ¯â€‹{ğ’›Â¯t,ğ’›Â¯tâˆ’1,â€¦}.\mathscr{H}\_{t}:=\overline{\operatorname{span}}\{\bar{{\mbox{$z$}}}\_{t},\bar{{\mbox{$z$}}}\_{t-1},\ldots\}. |  |

Let ğ’«tâˆ’1\mathscr{P}\_{t-1} denote the L2L^{2}-orthogonal projection onto â„‹tâˆ’1\mathscr{H}\_{t-1}. The one-step-ahead innovation is then defined by

|  |  |  |
| --- | --- | --- |
|  | ğœºt:=ğ’›Â¯tâˆ’ğ’«tâˆ’1â€‹[ğ’›Â¯t]=[ğœºx,tğœºu,t].{\mbox{$\varepsilon$}}\_{t}:=\bar{{\mbox{$z$}}}\_{t}-\mathscr{P}\_{t-1}[\bar{{\mbox{$z$}}}\_{t}]=\begin{bmatrix}{\mbox{$\varepsilon$}}\_{x,t}\\ {\mbox{$\varepsilon$}}\_{u,t}\\ \end{bmatrix}. |  |

###### Assumption 1 (Finite-predictor exactness at lag p0p\_{0}):

(A1.1)
:   The process {ğ’›Â¯t}\{\bar{{\mbox{$z$}}}\_{t}\} is covariance-stationary with finite second moments and purely nondeterministic. For some finite p0â‰¥1p\_{0}\geq 1,

    |  |  |  |
    | --- | --- | --- |
    |  | ğ’«tâˆ’1â€‹[ğ’›Â¯t]=ğ’«tâˆ’1(p0)â€‹[ğ’›Â¯t],ğ’«tâˆ’1(p0):L2â†’spanâ€‹{ğ’›Â¯tâˆ’1,â€¦,ğ’›Â¯tâˆ’p0}.\mathscr{P}\_{t-1}[\bar{{\mbox{$z$}}}\_{t}]=\mathscr{P}^{(p\_{0})}\_{t-1}[\bar{{\mbox{$z$}}}\_{t}],\qquad\mathscr{P}^{(p\_{0})}\_{t-1}:L^{2}\to\mathrm{span}\{\bar{{\mbox{$z$}}}\_{t-1},\ldots,\bar{{\mbox{$z$}}}\_{t-p\_{0}}\}. |  |

(A1.2)
:   The innovation sequence {ğœºt}\{{\mbox{$\varepsilon$}}\_{t}\} is strongly mixing with coefficients Î±Îµâ€‹(â„“)\alpha\_{\varepsilon}(\ell) satisfying âˆ‘â„“=1âˆÎ±Îµâ€‹(â„“)Î´/(2+Î´)<âˆ\sum\_{\ell=1}^{\infty}\alpha\_{\varepsilon}(\ell)^{\delta/(2+\delta)}<\infty for some Î´>0\delta>0, has finite moments ğ”¼â€‹â€–ğœºtâ€–4+2â€‹Î´<âˆ{\mathbb{E}}\|{\mbox{$\varepsilon$}}\_{t}\|^{4+2\delta}<\infty, and ğšº:=ğ”¼â€‹[ğœºtâ€‹ğœºtâ€²]{\mbox{$\Sigma$}}:={\mathbb{E}}[{\mbox{$\varepsilon$}}\_{t}{\mbox{$\varepsilon$}}\_{t}^{\prime}] is positive definite.111The definition of the mixing coefficients aligns with Hansen ([2022](https://arxiv.org/html/2601.21272v1#bib.bib2177 "Econometrics")).

Under AssumptionÂ (A1.1), {ğ’›Â¯t}\{\bar{{\mbox{$z$}}}\_{t}\} is covariance-stationary and purely nondeterministic. Hence, by the Wold decomposition, ğ’›Â¯t\bar{{\mbox{$z$}}}\_{t} admits a unique orthogonal VMA(âˆ\infty) representation, with no deterministic (completely predictable) component. Furthermore, finite-predictor exactness at lag p0p\_{0} implies that {ğ’›Â¯t}\{\bar{{\mbox{$z$}}}\_{t}\} admits a VAR(p0p\_{0}) innovations representation. We adopt a reduced-form VAR(p0p\_{0}) as the benchmark because it summarizes multivariate dynamics with minimal structural assumptions and serves as the standard workhorse in empirical macroeconomics and finance (Sims ([1980](https://arxiv.org/html/2601.21272v1#bib.bib494 "Macroeconomics and reality")); Stock and Watson ([2001](https://arxiv.org/html/2601.21272v1#bib.bib2288 "Vector autoregressions"))). Finite-order VARs can also be interpreted as practical approximations to the Wold representation for a broad class of stationary processes, which makes them a convenient parametric device for prediction and impulse-response analysis (LÃ¼tkepohl ([2005](https://arxiv.org/html/2601.21272v1#bib.bib522 "New introduction to multiple time series analysis"))).

PropositionÂ [1](https://arxiv.org/html/2601.21272v1#Thmproposition1 "Proposition 1: â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")(iii) establishes that this canonical VAR(p0p\_{0}) representation is stable. Since {(ğ’št,ğ‘¿tâ€²,ğ’–t)}\{({\mbox{$y$}}\_{t},{\mbox{$X$}}\_{t}^{\prime},{\mbox{$u$}}\_{t})\} is a fixed linear transformation of ğ’›Â¯t\bar{{\mbox{$z$}}}\_{t}, it is also covariance-stationary. Moreover, because ğ’›Â¯t\bar{{\mbox{$z$}}}\_{t} admits a stable VAR(p0p\_{0}) representation driven by {ğœºt}\{{\mbox{$\varepsilon$}}\_{t}\} with finite (4+2â€‹Î´)(4+2\delta)-th moments, ğ’›Â¯t\bar{{\mbox{$z$}}}\_{t} can be written as a linear process with absolutely summable coefficients. Consequently, ğ’št{\mbox{$y$}}\_{t}, ğ‘¿t{\mbox{$X$}}\_{t}, and ğ’–t{\mbox{$u$}}\_{t} also have finite (4+2â€‹Î´)(4+2\delta)-th moments.

###### Proposition 1:

Suppose AssumptionÂ [1](https://arxiv.org/html/2601.21272v1#Thmassumption1 "Assumption 1 (Finite-predictor exactness at lag ğ‘â‚€): â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") holds. Then:

(i)
:   Let ğœºt:=ğ’›Â¯tâˆ’ğ’«tâˆ’1â€‹[ğ’›Â¯t]{\mbox{$\varepsilon$}}\_{t}:=\bar{{\mbox{$z$}}}\_{t}-\mathscr{P}\_{t-1}[\bar{{\mbox{$z$}}}\_{t}]. The centered process {ğ’›Â¯t}\{\bar{{\mbox{$z$}}}\_{t}\} admits the Wold representation

    |  |  |  |  |
    | --- | --- | --- | --- |
    |  | ğ’›Â¯t=âˆ‘i=0âˆğšµiâ€‹ğœºtâˆ’iinÂ â€‹L2,ğšµ0=ğ‘°m,\bar{{\mbox{$z$}}}\_{t}=\sum\_{i=0}^{\infty}{\mbox{$\Xi$}}\_{i}\,{\mbox{$\varepsilon$}}\_{t-i}\quad\text{in }L^{2},\qquad{\mbox{$\Xi$}}\_{0}={\mbox{$I$}}\_{m}, |  | (3) |

    where {ğœºt}\{{\mbox{$\varepsilon$}}\_{t}\} is (second-order) white noise: ğ”¼â€‹[ğœºt]=ğŸ{\mathbb{E}}[{\mbox{$\varepsilon$}}\_{t}]={\mbox{$0$}}, ğ”¼â€‹[ğœºtâ€‹ğœºsâ€²]=ğŸ{\mathbb{E}}[{\mbox{$\varepsilon$}}\_{t}{\mbox{$\varepsilon$}}\_{s}^{\prime}]={\mbox{$0$}} for sâ‰ ts\neq t, and ğ”¼â€‹[ğœºtâ€‹ğœºtâ€²]=ğšº{\mathbb{E}}[{\mbox{$\varepsilon$}}\_{t}{\mbox{$\varepsilon$}}\_{t}^{\prime}]={\mbox{$\Sigma$}} (constant in tt). Each ğšµiâˆˆâ„mÃ—m{\mbox{$\Xi$}}\_{i}\in\mathbb{R}^{m\times m} admits the block partition

    |  |  |  |
    | --- | --- | --- |
    |  | ğšµi=[ğšµxâ€‹x,iğšµxâ€‹u,iğšµuâ€‹x,iğšµuâ€‹u,i],ğšµxâ€‹x,iâˆˆâ„kÃ—k,ğšµxâ€‹u,iâˆˆâ„kÃ—N,ğšµuâ€‹x,iâˆˆâ„NÃ—k,ğšµuâ€‹u,iâˆˆâ„NÃ—N.{\mbox{$\Xi$}}\_{i}=\begin{bmatrix}{\mbox{$\Xi$}}\_{xx,i}&{\mbox{$\Xi$}}\_{xu,i}\\ {\mbox{$\Xi$}}\_{ux,i}&{\mbox{$\Xi$}}\_{uu,i}\end{bmatrix},\quad{\mbox{$\Xi$}}\_{xx,i}\in\mathbb{R}^{k\times k},\ {\mbox{$\Xi$}}\_{xu,i}\in\mathbb{R}^{k\times N},\ {\mbox{$\Xi$}}\_{ux,i}\in\mathbb{R}^{N\times k},\ {\mbox{$\Xi$}}\_{uu,i}\in\mathbb{R}^{N\times N}. |  |

(ii)
:   There exist unique matrices {ğš¿j}j=1p0âŠ‚â„mÃ—m\{{\mbox{$\Psi$}}\_{j}\}\_{j=1}^{p\_{0}}\subset\mathbb{R}^{m\times m} such that the centered process admits the canonical VAR(p0p\_{0}) innovations representation

    |  |  |  |  |
    | --- | --- | --- | --- |
    |  | ğ’›Â¯t=âˆ‘j=1p0ğš¿jâ€‹ğ’›Â¯tâˆ’j+ğœºtinÂ â€‹L2.\bar{{\mbox{$z$}}}\_{t}=\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{j}\bar{{\mbox{$z$}}}\_{t-j}+{\mbox{$\varepsilon$}}\_{t}\quad\text{in }L^{2}. |  | (4) |

    Then the VAR and VMA coefficients {ğš¿j}\{{\mbox{$\Psi$}}\_{j}\} and {ğšµn}\{{\mbox{$\Xi$}}\_{n}\} satisfy, for nâ‰¥1n\geq 1,

    |  |  |  |
    | --- | --- | --- |
    |  | ğšµ0=ğ‘°m,ğšµn=âˆ‘j=1minâ¡{p0,n}ğš¿jâ€‹ğšµnâˆ’j,nâ‰¥1,{\mbox{$\Xi$}}\_{0}={\mbox{$I$}}\_{m},\qquad{\mbox{$\Xi$}}\_{n}=\sum\_{j=1}^{\min\{p\_{0},n\}}{\mbox{$\Psi$}}\_{j}{\mbox{$\Xi$}}\_{n-j},\quad n\geq 1, |  |

    equivalently ğš¿â€‹(L)â€‹ğšµâ€‹(L)=ğ‘°m{\mbox{$\Psi$}}(L){\mbox{$\Xi$}}(L)={\mbox{$I$}}\_{m} with ğš¿â€‹(L):=ğ‘°mâˆ’âˆ‘j=1p0ğš¿jâ€‹Lj{\mbox{$\Psi$}}(L):={\mbox{$I$}}\_{m}-\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{j}L^{j} and ğšµâ€‹(L):=ğ‘°m+âˆ‘n=1âˆğšµnâ€‹Ln{\mbox{$\Xi$}}(L):={\mbox{$I$}}\_{m}+\sum\_{n=1}^{\infty}{\mbox{$\Xi$}}\_{n}L^{n}.

(iii)
:   The characteristic matrix polynomial ğ‘·â€‹(z):=ğ‘°mâˆ’âˆ‘j=1p0ğš¿jâ€‹zj{\mbox{$P$}}(z):={\mbox{$I$}}\_{m}-\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{j}z^{j} has no zeros in or on the unit disk, that is, detğ‘·â€‹(z)â‰ 0\det{\mbox{$P$}}(z)\neq 0 for all |z|â‰¤1|z|\leq 1. Hence the inverse filter admits the absolutely summable power series expansion

    |  |  |  |
    | --- | --- | --- |
    |  | ğ‘·â€‹(L)âˆ’1=ğ‘°m+âˆ‘i=1âˆğšµiâ€‹Li,âˆ‘i=0âˆâ€–ğšµiâ€–<âˆ.{\mbox{$P$}}(L)^{-1}={\mbox{$I$}}\_{m}+\sum\_{i=1}^{\infty}{\mbox{$\Xi$}}\_{i}L^{i},\qquad\sum\_{i=0}^{\infty}\|{\mbox{$\Xi$}}\_{i}\|<\infty. |  |

(iv)
:   The vector process {(ğ’št,ğ‘¿tâ€²,ğ’–t)}\{({\mbox{$y$}}\_{t},{\mbox{$X$}}\_{t}^{\prime},{\mbox{$u$}}\_{t})\} is covariance-stationary as a fixed linear transform of {ğ’›t}\{{\mbox{$z$}}\_{t}\}. Moreover, it is strongly mixing with coefficients Î±(y,X,u)â€‹(â„“)\alpha\_{(y,X,u)}(\ell) satisfying

    |  |  |  |
    | --- | --- | --- |
    |  | âˆ‘â„“=1âˆÎ±(y,X,u)â€‹(â„“)Î´/(2+Î´)<âˆ,\sum\_{\ell=1}^{\infty}\alpha\_{(y,X,u)}(\ell)^{\delta/(2+\delta)}<\infty, |  |

    and hence is ergodic. In addition, its components have finite (4+2â€‹Î´)(4+2\delta)-th moments:

    |  |  |  |
    | --- | --- | --- |
    |  | ğ”¼â€‹â€–ğ‘¿tâ€–4+2â€‹Î´<âˆ,ğ”¼â€‹â€–ğ’–tâ€–4+2â€‹Î´<âˆ,ğ”¼â€‹â€–ğ’štâ€–4+2â€‹Î´<âˆ.{\mathbb{E}}\|{\mbox{$X$}}\_{t}\|^{4+2\delta}<\infty,\quad{\mathbb{E}}\|{\mbox{$u$}}\_{t}\|^{4+2\delta}<\infty,\quad{\mathbb{E}}\|{\mbox{$y$}}\_{t}\|^{4+2\delta}<\infty. |  |

###### Proof.

See the Appendix.
âˆ

Under AssumptionÂ [1](https://arxiv.org/html/2601.21272v1#Thmassumption1 "Assumption 1 (Finite-predictor exactness at lag ğ‘â‚€): â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"), the innovation covariance matrix ğšº\Sigma can be expressed in block form as follows:

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğšº=[ğšºxâ€‹xğšºxâ€‹uğšºuâ€‹xğšºuâ€‹u],ğšºxâ€‹xâˆˆâ„kÃ—k,ğšºuâ€‹uâˆˆâ„NÃ—N,ğšºxâ€‹uâˆˆâ„kÃ—N,ğšºuâ€‹xâˆˆâ„NÃ—k.{\mbox{$\Sigma$}}=\begin{bmatrix}{\mbox{$\Sigma$}}\_{xx}&{\mbox{$\Sigma$}}\_{xu}\\ {\mbox{$\Sigma$}}\_{ux}&{\mbox{$\Sigma$}}\_{uu}\end{bmatrix},\quad{\mbox{$\Sigma$}}\_{xx}\in\mathbb{R}^{k\times k},\quad{\mbox{$\Sigma$}}\_{uu}\in\mathbb{R}^{N\times N},\quad{\mbox{$\Sigma$}}\_{xu}\in\mathbb{R}^{k\times N},\quad{\mbox{$\Sigma$}}\_{ux}\in\mathbb{R}^{N\times k}. |  | (5) |

In addition, the VAR coefficient matrix ğš¿jâˆˆâ„mÃ—m{\mbox{$\Psi$}}\_{j}\in\mathbb{R}^{m\times m} for each j=1,â€¦,p0j=1,\ldots,p\_{0} admits the following block partition:

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğš¿j=[ğš¿xâ€‹x,jğš¿xâ€‹u,jğš¿uâ€‹x,jğš¿uâ€‹u,j],ğš¿xâ€‹x,jâˆˆâ„kÃ—k,ğš¿xâ€‹u,jâˆˆâ„kÃ—N,ğš¿uâ€‹x,jâˆˆâ„NÃ—k,ğš¿uâ€‹u,jâˆˆâ„NÃ—N.{\mbox{$\Psi$}}\_{j}=\begin{bmatrix}{\mbox{$\Psi$}}\_{xx,j}&{\mbox{$\Psi$}}\_{xu,j}\\ {\mbox{$\Psi$}}\_{ux,j}&{\mbox{$\Psi$}}\_{uu,j}\end{bmatrix},\ {\mbox{$\Psi$}}\_{xx,j}\in\mathbb{R}^{k\times k},\ {\mbox{$\Psi$}}\_{xu,j}\in\mathbb{R}^{k\times N},\ {\mbox{$\Psi$}}\_{ux,j}\in\mathbb{R}^{N\times k},\ {\mbox{$\Psi$}}\_{uu,j}\in\mathbb{R}^{N\times N}. |  | (6) |

Under the stable VAR(p0p\_{0}) (equivalently, absolutely summable impulse responses) representation for ğ’›Â¯t=[(ğ’™tâˆ’ğx)â€²,ğ’–tâ€²]â€²\bar{{\mbox{$z$}}}\_{t}=[({\mbox{$x$}}\_{t}-{\mbox{$\mu$}}\_{x})^{\prime},{\mbox{$u$}}\_{t}^{\prime}]^{\prime}, the pair {{ğš¿j}j=1p0,ğšº}\big\{\{{\mbox{$\Psi$}}\_{j}\}\_{j=1}^{p\_{0}},{\mbox{$\Sigma$}}\big\} fully characterizes the second-order properties of {ğ’›Â¯t}\{\bar{{\mbox{$z$}}}\_{t}\}. The innovation covariance ğšº\Sigma governs the contemporaneous (innovation) covariance between Îµx,t\varepsilon\_{x,t} and Îµu,t\varepsilon\_{u,t}, whereas dynamic cross-dependence is generated by the off-diagonal VAR blocks {ğš¿xâ€‹u,j,ğš¿uâ€‹x,j}\{{\mbox{$\Psi$}}\_{xu,j},{\mbox{$\Psi$}}\_{ux,j}\} and propagates through the impulse responses {ğšµi}\{{\mbox{$\Xi$}}\_{i}\} defined by ğšµâ€‹(L)=ğ‘·â€‹(L)âˆ’1{\mbox{$\Xi$}}(L)={\mbox{$P$}}(L)^{-1} with ğ‘·â€‹(L)=ğ‘°mâˆ’âˆ‘j=1p0ğš¿jâ€‹Lj{\mbox{$P$}}(L)={\mbox{$I$}}\_{m}-\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{j}L^{j}. For any lag â„“âˆˆâ„¤\ell\in\mathbb{Z},

|  |  |  |
| --- | --- | --- |
|  | ğšªzâ€‹(â„“):=â„‚â€‹ovâ€‹(ğ’›Â¯t,ğ’›Â¯tâˆ’â„“)={âˆ‘j=0âˆğšµâ„“+jâ€‹ğšºğšµjâ€²,â„“â‰¥0,ğšªzâ€‹(âˆ’â„“)â€²,â„“<0,{\mbox{$\Gamma$}}\_{z}(\ell):={\mathbb{C}\rm{ov}}(\bar{{\mbox{$z$}}}\_{t},\bar{{\mbox{$z$}}}\_{t-\ell})=\begin{cases}\displaystyle\sum\_{j=0}^{\infty}{\mbox{$\Xi$}}\_{\ell+j}{\mbox{$\Sigma$}}{\mbox{$\Xi$}}\_{j}^{\prime},&\ell\geq 0,\\ {\mbox{$\Gamma$}}\_{z}(-\ell)^{\prime},&\ell<0,\end{cases} |  |

so every contemporaneous and lead-lag covariance (in particular â„‚â€‹ovâ€‹(ğ’™t,ğ’–tâˆ’â„“){\mathbb{C}\rm{ov}}({\mbox{$x$}}\_{t},{\mbox{$u$}}\_{t-\ell})) is a deterministic function of {ğš¿j}\{{\mbox{$\Psi$}}\_{j}\} and ğšº\Sigma.

In asset-pricing applications, it is common to consider a common set of regressors shared across all equations. Leading examples include the CAPM of Sharpe ([1964](https://arxiv.org/html/2601.21272v1#bib.bib1915 "Capital asset prices: a theory of market equilibrium under conditions of risk")) and Lintner ([1965](https://arxiv.org/html/2601.21272v1#bib.bib1898 "The valuation of risky assets and the selection of risky investments in stock portfolios and budget constraints")), the arbitrage pricing theory (APT) of Ross ([1976](https://arxiv.org/html/2601.21272v1#bib.bib1912 "The arbitrage theory of capital asset pricing")), and the multi-factor models of Fama and French ([1993](https://arxiv.org/html/2601.21272v1#bib.bib1868 "Common risk factors in the returns on stocks and bonds"), [2015](https://arxiv.org/html/2601.21272v1#bib.bib1869 "A five-factor asset pricing model"), [2016](https://arxiv.org/html/2601.21272v1#bib.bib1870 "Dissecting anomalies with a five-factor model")). In these cases, ğ’™i,t=ğ’™t{\mbox{$x$}}\_{i,t}={\mbox{$x$}}\_{t} for all ii. This common-regressor case is nested in our general setup, and all results under AssumptionÂ [1](https://arxiv.org/html/2601.21272v1#Thmassumption1 "Assumption 1 (Finite-predictor exactness at lag ğ‘â‚€): â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") remain valid after a notational simplification.

###### Remark 1 (Common regressors across equations):

Suppose that the regressor vector is common across equations, i.e.,

|  |  |  |
| --- | --- | --- |
|  | ğ’™i,t:=ğ’™tâˆˆâ„rfor allÂ â€‹i=1,â€¦,N,{\mbox{$x$}}\_{i,t}:={\mbox{$x$}}\_{t}\in\mathbb{R}^{r}\quad\text{for all }i=1,\ldots,N, |  |

so that ki:=rk\_{i}:=r and hence the total regressor dimension in the general setup becomes k:=âˆ‘i=1Nki=Nâ€‹rk:=\sum\_{i=1}^{N}k\_{i}=Nr. Then the multi-equation regression ([1](https://arxiv.org/html/2601.21272v1#S2.E1 "In 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"))â€“([2](https://arxiv.org/html/2601.21272v1#S2.E2 "In 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) can be written as

|  |  |  |
| --- | --- | --- |
|  | ğ’št=ğœ¶+ğ‘¿tâ€²â€‹ğœ·+ğ’–t,{\mbox{$y$}}\_{t}={\mbox{$\alpha$}}+{\mbox{$X$}}\_{t}^{\prime}{\mbox{$\beta$}}+{\mbox{$u$}}\_{t}, |  |

where ğ‘¿tâ€²=(ğ‘°NâŠ—ğ’™tâ€²)âˆˆâ„NÃ—k{\mbox{$X$}}\_{t}^{\prime}=({\mbox{$I$}}\_{N}\otimes{\mbox{$x$}}\_{t}^{\prime})\in\mathbb{R}^{N\times k} and ğœ·=(ğœ·1â€²,â€¦,ğœ·Nâ€²)â€²âˆˆâ„k{\mbox{$\beta$}}=({\mbox{$\beta$}}\_{1}^{\prime},\ldots,{\mbox{$\beta$}}\_{N}^{\prime})^{\prime}\in\mathbb{R}^{k} with k=Nâ€‹rk=Nr. Define ğ’›t:=[(ğ’™tâˆ’ğx)â€²,ğ’–tâ€²]â€²âˆˆâ„m{\mbox{$z$}}\_{t}:=[({\mbox{$x$}}\_{t}-{\mbox{$\mu$}}\_{x})^{\prime},\ {\mbox{$u$}}\_{t}^{\prime}]^{\prime}\in\mathbb{R}^{m} with m:=r+Nm:=r+N and ğ’›Â¯t:=ğ’›tâˆ’ğz\bar{{\mbox{$z$}}}\_{t}:={\mbox{$z$}}\_{t}-{\mbox{$\mu$}}\_{z} as before. Under AssumptionÂ [1](https://arxiv.org/html/2601.21272v1#Thmassumption1 "Assumption 1 (Finite-predictor exactness at lag ğ‘â‚€): â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"), {ğ’›Â¯t}\{\bar{{\mbox{$z$}}}\_{t}\} admits the stable VAR(p0p\_{0}) and Wold VMA(âˆ\infty) representations

|  |  |  |
| --- | --- | --- |
|  | ğ’›Â¯t=âˆ‘j=1p0ğš¿jâ€‹ğ’›Â¯tâˆ’j+ğœºt,ğ’›Â¯t=âˆ‘i=0âˆğšµiâ€‹ğœºtâˆ’i,ğšµâ€‹(L)=ğ‘·â€‹(L)âˆ’1,\bar{{\mbox{$z$}}}\_{t}=\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{j}\bar{{\mbox{$z$}}}\_{t-j}+{\mbox{$\varepsilon$}}\_{t},\qquad\bar{{\mbox{$z$}}}\_{t}=\sum\_{i=0}^{\infty}{\mbox{$\Xi$}}\_{i}\,{\mbox{$\varepsilon$}}\_{t-i},\quad{\mbox{$\Xi$}}(L)={\mbox{$P$}}(L)^{-1}, |  |

where ğ‘·â€‹(L)=ğ‘°mâˆ’âˆ‘j=1p0ğš¿jâ€‹Lj{\mbox{$P$}}(L)={\mbox{$I$}}\_{m}-\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{j}L^{j}. The innovation covariance and VAR
coefficients admit the block partitions

|  |  |  |
| --- | --- | --- |
|  | ğšº=ğ”¼â€‹[ğœºtâ€‹ğœºtâ€²]=[ğšºxâ€‹xğšºxâ€‹uğšºuâ€‹xğšºuâ€‹u],ğš¿j=[ğš¿xâ€‹x,jğš¿xâ€‹u,jğš¿uâ€‹x,jğš¿uâ€‹u,j],{\mbox{$\Sigma$}}={\mathbb{E}}[{\mbox{$\varepsilon$}}\_{t}{\mbox{$\varepsilon$}}\_{t}^{\prime}]=\begin{bmatrix}{\mbox{$\Sigma$}}\_{xx}&{\mbox{$\Sigma$}}\_{xu}\\ {\mbox{$\Sigma$}}\_{ux}&{\mbox{$\Sigma$}}\_{uu}\end{bmatrix},\qquad{\mbox{$\Psi$}}\_{j}=\begin{bmatrix}{\mbox{$\Psi$}}\_{xx,j}&{\mbox{$\Psi$}}\_{xu,j}\\ {\mbox{$\Psi$}}\_{ux,j}&{\mbox{$\Psi$}}\_{uu,j}\end{bmatrix}, |  |

with ğšºxâ€‹xâˆˆâ„rÃ—r{\mbox{$\Sigma$}}\_{xx}\in\mathbb{R}^{r\times r}, ğšºuâ€‹uâˆˆâ„NÃ—N{\mbox{$\Sigma$}}\_{uu}\in\mathbb{R}^{N\times N},
ğšºxâ€‹uâˆˆâ„rÃ—N{\mbox{$\Sigma$}}\_{xu}\in\mathbb{R}^{r\times N}, and ğšºuâ€‹xâˆˆâ„NÃ—r{\mbox{$\Sigma$}}\_{ux}\in\mathbb{R}^{N\times r}, and likewise
ğš¿xâ€‹x,jâˆˆâ„rÃ—r{\mbox{$\Psi$}}\_{xx,j}\in\mathbb{R}^{r\times r}, ğš¿xâ€‹u,jâˆˆâ„rÃ—N{\mbox{$\Psi$}}\_{xu,j}\in\mathbb{R}^{r\times N},
ğš¿uâ€‹x,jâˆˆâ„NÃ—r{\mbox{$\Psi$}}\_{ux,j}\in\mathbb{R}^{N\times r}, and ğš¿uâ€‹u,jâˆˆâ„NÃ—N{\mbox{$\Psi$}}\_{uu,j}\in\mathbb{R}^{N\times N}.

Hereafter, we establish conditions for the consistency and asymptotic normality of estimators of (ğœ¶â€²,ğœ·â€²)â€²({\mbox{$\alpha$}}^{\prime},{\mbox{$\beta$}}^{\prime})^{\prime} across a variety of data-generating processes (DGPs), characterized by restrictions on {ğš¿j}j=1p0\{{\mbox{$\Psi$}}\_{j}\}\_{j=1}^{p\_{0}} and ğšº\Sigma.

### 2.2 Relationships among exogeneity conditions

In linear regression analysis, exogeneity assumptions play a central role in determining how the stochastic relationship between the regressors and the disturbances affects the consistency and efficiency of estimators and the validity of associated test statistics. In cross-sectional settings, it is often enough to require a contemporaneous orthogonality condition. In time-series settings, however, it is no longer sufficient to require that ğ’–t{\mbox{$u$}}\_{t} be uncorrelated with the contemporaneous regressors only, (i.e., ğ”¼â€‹[ğ’™tâ€‹ğ’–tâ€²]=ğŸ{\mathbb{E}}[{\mbox{$x$}}\_{t}{\mbox{$u$}}\_{t}^{\prime}]={\mbox{$0$}}). One must specify how ğ’–t{\mbox{$u$}}\_{t} relates to the entire temporal path of {ğ’™s}\{{\mbox{$x$}}\_{s}\}, which leads to various dynamic exogeneity concepts.

Different strands of the literature formalize these orthogonality requirements in different ways. The traditional approach, following Stock and Watson ([2019](https://arxiv.org/html/2601.21272v1#bib.bib2217 "Introduction to econometrics")), imposes conditional mean-independence restrictions such as strict exogeneity and present-and-past exogeneity. Strict exogeneity requires ğ”¼â€‹[ğ’–tâˆ£â€¦,ğ’™t+2,ğ’™t+1,ğ’™t,ğ’™tâˆ’1,ğ’™tâˆ’2,â€¦]=ğŸ{\mathbb{E}}[{\mbox{$u$}}\_{t}\mid\ldots,{\mbox{$x$}}\_{t+2},{\mbox{$x$}}\_{t+1},{\mbox{$x$}}\_{t},{\mbox{$x$}}\_{t-1},{\mbox{$x$}}\_{t-2},\ldots]={\mbox{$0$}} for all tt, that is, the disturbance is mean-independent of the entire history and future path of the regressors. Present-and-past exogeneity relaxes strict exogeneity by conditioning only on current and past regressors: ğ”¼â€‹[ğ’–tâˆ£ğ’™t,ğ’™tâˆ’1,ğ’™tâˆ’2,â€¦]=ğŸ{\mathbb{E}}[{\mbox{$u$}}\_{t}\mid{\mbox{$x$}}\_{t},{\mbox{$x$}}\_{t-1},{\mbox{$x$}}\_{t-2},\ldots]={\mbox{$0$}} for all tt. These exogeneity conditions preclude any correlation between the disturbance ğ’–t{\mbox{$u$}}\_{t} and the relevant history of the regressors. In particular, under strict exogeneity we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â€‹[ğ’–tâ€‹ğ’™sâ€²]=ğŸfor allÂ â€‹t,s,{\mathbb{E}}[{\mbox{$u$}}\_{t}{\mbox{$x$}}\_{s}^{\prime}]={\mbox{$0$}}\quad\text{for all }t,s, |  | (7) |

whereas under present-and-past exogeneity we obtain

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â€‹[ğ’–tâ€‹ğ’™sâ€²]=ğŸfor allÂ â€‹tâ€‹Â and allÂ â€‹sâ‰¤t.{\mathbb{E}}[{\mbox{$u$}}\_{t}{\mbox{$x$}}\_{s}^{\prime}]={\mbox{$0$}}\quad\text{for all }t\text{ and all }s\leq t. |  | (8) |

Hereafter, we refer to ([7](https://arxiv.org/html/2601.21272v1#S2.E7 "In 2.2 Relationships among exogeneity conditions â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) and ([8](https://arxiv.org/html/2601.21272v1#S2.E8 "In 2.2 Relationships among exogeneity conditions â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) as the strict exogeneity and present-and-past exogeneity conditions, respectively. In particular, strict exogeneity implies present-and-past exogeneity, and the inclusion is strict: strict exogeneity âŠŠ\subsetneq present-and-past exogeneity. In what follows, we use these terms in the covariance sense implied by the conditional mean restrictions (i.e., ğ”¼â€‹[ğ’–tâ€‹ğ’™sâ€²]=ğŸ{\mathbb{E}}[{\mbox{$u$}}\_{t}{\mbox{$x$}}\_{s}^{\prime}]={\mbox{$0$}}) because our joint VAR/VMA framework is stated in second-order terms.

In single-equation environments, Perron and GonzÃ¡lez-Coya ([2022](https://arxiv.org/html/2601.21272v1#bib.bib2158 "Feasible gls for time series regression")) define exogeneity conditions in terms of the Wold innovations of the disturbance process. Let {ğœºu,t}\{{\mbox{$\varepsilon$}}\_{u,t}\} denote the (vector) innovation sequence in the Wold decomposition of {ğ’–t}\{{\mbox{$u$}}\_{t}\} induced by the joint representation in PropositionÂ [1](https://arxiv.org/html/2601.21272v1#Thmproposition1 "Proposition 1: â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"). The same definitions extend naturally to multiple-equation systems. The regressors are said to be pre-determined if

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â€‹[ğ’™tâ€‹(ğœºu,t,ğœºu,t+1,â€¦)â€²]=ğŸ{\mathbb{E}}[{\mbox{$x$}}\_{t}({\mbox{$\varepsilon$}}\_{u,t},{\mbox{$\varepsilon$}}\_{u,t+1},\ldots)^{\prime}]={\mbox{$0$}} |  | (9) |

that is, ğ’™t{\mbox{$x$}}\_{t} is uncorrelated with the present and all future innovations.222Under the joint Wold representation in PropositionÂ [1](https://arxiv.org/html/2601.21272v1#Thmproposition1 "Proposition 1: â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")(i), ğ’™Â¯t:=ğ’™tâˆ’ğx\bar{{\mbox{$x$}}}\_{t}:={\mbox{$x$}}\_{t}-{\mbox{$\mu$}}\_{x} is a linear function of current and past innovations. Since ğœºt{{\mbox{$\varepsilon$}}\_{t}} is white noise, ğ’™Â¯t\bar{{\mbox{$x$}}}\_{t} (and hence ğ’™t{\mbox{$x$}}\_{t}) is automatically orthogonal to future ğœºu,t+h{\mbox{$\varepsilon$}}\_{u,t+h} for hâ‰¥1h\geq 1. Hence, the only nontrivial restriction in ([9](https://arxiv.org/html/2601.21272v1#S2.E9 "In 2.2 Relationships among exogeneity conditions â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) is ğ”¼â€‹[ğ’™Â¯tâ€‹ğœºu,tâ€²]=ğŸ{\mathbb{E}}[\bar{{\mbox{$x$}}}\_{t}{\mbox{$\varepsilon$}}\_{u,t}^{\prime}]={\mbox{$0$}}, i.e., ğšºxâ€‹u=ğŸ{\mbox{$\Sigma$}}\_{xu}={\mbox{$0$}}. The regressors are said to be exogenous if

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â€‹[ğ’™tâ€‹(ğœºu,t,ğœºu,tâˆ’1,â€¦)â€²]=ğŸ{\mathbb{E}}[{\mbox{$x$}}\_{t}({\mbox{$\varepsilon$}}\_{u,t},{\mbox{$\varepsilon$}}\_{u,t-1},\ldots)^{\prime}]={\mbox{$0$}} |  | (10) |

so that ğ’™t{\mbox{$x$}}\_{t} is uncorrelated with present and all past innovations. In Perron and GonzÃ¡lez-Coya ([2022](https://arxiv.org/html/2601.21272v1#bib.bib2158 "Feasible gls for time series regression")), ğ”¼â€‹[ğ’™tâ€‹ğœºu,tâ€²]=ğŸ{\mathbb{E}}[{\mbox{$x$}}\_{t}{\mbox{$\varepsilon$}}\_{u,t}^{\prime}]={\mbox{$0$}} is imposed as a standing condition. We incorporate it into ([9](https://arxiv.org/html/2601.21272v1#S2.E9 "In 2.2 Relationships among exogeneity conditions â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"))â€“([10](https://arxiv.org/html/2601.21272v1#S2.E10 "In 2.2 Relationships among exogeneity conditions â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) for notational convenience; the definitions are unchanged.

By contrast, Baillie et al. ([2024](https://arxiv.org/html/2601.21272v1#bib.bib2090 "On robust inference in time-series regression")) propose a different set of exogeneity conditions in a single-equation framework. In our multiple-equation setting, these conditions can be conveniently expressed in terms of the VAR representation for ğ’›Â¯t=(ğ’™Â¯tâ€²,ğ’–tâ€²)â€²\bar{{\mbox{$z$}}}\_{t}=(\bar{{\mbox{$x$}}}\_{t}^{\prime},{\mbox{$u$}}\_{t}^{\prime})^{\prime}. Under the Block-Diagonal (Bâ€‹DBD) condition, the dynamics and innovations are block-diagonal:

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ’›Â¯t=âˆ‘j=1p0[ğš¿xâ€‹x,jğŸğŸğš¿uâ€‹u,j]â€‹ğ’›Â¯tâˆ’j+ğœºt,ğšº=[ğšºxâ€‹xğŸğŸğšºuâ€‹u].\bar{{\mbox{$z$}}}\_{t}=\sum\_{j=1}^{p\_{0}}\begin{bmatrix}{\mbox{$\Psi$}}\_{xx,j}&{\mbox{$0$}}\\ {\mbox{$0$}}&{\mbox{$\Psi$}}\_{uu,j}\\ \end{bmatrix}\bar{{\mbox{$z$}}}\_{t-j}+{\mbox{$\varepsilon$}}\_{t},\qquad{\mbox{$\Sigma$}}=\begin{bmatrix}{\mbox{$\Sigma$}}\_{xx}&{\mbox{$0$}}\\ {\mbox{$0$}}&{\mbox{$\Sigma$}}\_{uu}\\ \end{bmatrix}. |  | (11) |

The GLS-Exogeneity (Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG) condition relaxes the Bâ€‹DBD condition by allowing feedback from past disturbances to the regressors, while keeping the innovation covariance block-diagonal:

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ’›Â¯t=âˆ‘j=1p0[ğš¿xâ€‹x,jğš¿xâ€‹u,jğŸğš¿uâ€‹u,j]â€‹ğ’›Â¯tâˆ’j+ğœºt,ğšº=[ğšºxâ€‹xğŸğŸğšºuâ€‹u].\bar{{\mbox{$z$}}}\_{t}=\sum\_{j=1}^{p\_{0}}\begin{bmatrix}{\mbox{$\Psi$}}\_{xx,j}&{\mbox{$\Psi$}}\_{xu,j}\\ {\mbox{$0$}}&{\mbox{$\Psi$}}\_{uu,j}\\ \end{bmatrix}\bar{{\mbox{$z$}}}\_{t-j}+{\mbox{$\varepsilon$}}\_{t},\qquad{\mbox{$\Sigma$}}=\begin{bmatrix}{\mbox{$\Sigma$}}\_{xx}&{\mbox{$0$}}\\ {\mbox{$0$}}&{\mbox{$\Sigma$}}\_{uu}\\ \end{bmatrix}. |  | (12) |

Finally, under the Error-Block-Diagonal (Eâ€‹Bâ€‹DEBD) condition, both directions of dynamic interaction are allowed through the VAR coefficients, while the innovation covariance matrix remains block-diagonal:

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ’›Â¯t=âˆ‘j=1p0[ğš¿xâ€‹x,jğš¿xâ€‹u,jğš¿uâ€‹x,jğš¿uâ€‹u,j]â€‹ğ’›Â¯tâˆ’j+ğœºt,ğšº=[ğšºxâ€‹xğŸğŸğšºuâ€‹u].\bar{{\mbox{$z$}}}\_{t}=\sum\_{j=1}^{p\_{0}}\begin{bmatrix}{\mbox{$\Psi$}}\_{xx,j}&{\mbox{$\Psi$}}\_{xu,j}\\ {\mbox{$\Psi$}}\_{ux,j}&{\mbox{$\Psi$}}\_{uu,j}\\ \end{bmatrix}\bar{{\mbox{$z$}}}\_{t-j}+{\mbox{$\varepsilon$}}\_{t},\qquad{\mbox{$\Sigma$}}=\begin{bmatrix}{\mbox{$\Sigma$}}\_{xx}&{\mbox{$0$}}\\ {\mbox{$0$}}&{\mbox{$\Sigma$}}\_{uu}\\ \end{bmatrix}. |  | (13) |

As is clear from these representations, the conditions satisfy the nesting relationship Bâ€‹DBD âŠŠ\subsetneq Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG âŠŠ\subsetneq Eâ€‹Bâ€‹DEBD.

However, different strands of the literature rely on different notions of exogeneity, and it is unclear how these conditions are related to one another or how strong they are in a comparative sense. Because AssumptionÂ [1](https://arxiv.org/html/2601.21272v1#Thmassumption1 "Assumption 1 (Finite-predictor exactness at lag ğ‘â‚€): â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") guarantees a joint VAR/VMA representation for ((ğ’™tâˆ’ğx)â€²,ğ’–tâ€²)â€²(({\mbox{$x$}}\_{t}-{\mbox{$\mu$}}\_{x})^{\prime},{\mbox{$u$}}\_{t}^{\prime})^{\prime}, it is natural to study these exogeneity concepts through the lens of the VAR coefficients {ğš¿j}\{{\mbox{$\Psi$}}\_{j}\} and the innovation covariance matrix ğšº\Sigma. In particular, the Bâ€‹DBD, Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG, and Eâ€‹Bâ€‹DEBD conditions can be interpreted as structural restrictions on (ğš¿j,ğšº)({\mbox{$\Psi$}}\_{j},{\mbox{$\Sigma$}}) that delimit the range of dynamic feedback between regressors and disturbances.

The following proposition makes these relationships explicit under AssumptionÂ [1](https://arxiv.org/html/2601.21272v1#Thmassumption1 "Assumption 1 (Finite-predictor exactness at lag ğ‘â‚€): â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").

###### Proposition 2:

Under AssumptionÂ [1](https://arxiv.org/html/2601.21272v1#Thmassumption1 "Assumption 1 (Finite-predictor exactness at lag ğ‘â‚€): â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"), the following statements hold:

(i)
:   The strict exogeneity and the Bâ€‹DBD condition are equivalent;

(ii)
:   The pre-determined condition and the Eâ€‹Bâ€‹DEBD condition coincide;

(iii)
:   Within the Eâ€‹Bâ€‹DEBD class, the subclass of DGPs that satisfy covariance-based present-and-past exogeneity lies strictly between Bâ€‹DBD and Eâ€‹Bâ€‹DEBD; that is, Bâ€‹DBD âŠŠ\subsetneq {\{present-and-past exogeneity}\} âˆ©\cap Eâ€‹Bâ€‹DEBD âŠŠ\subsetneq Eâ€‹Bâ€‹DEBD.

PropositionÂ [2](https://arxiv.org/html/2601.21272v1#Thmproposition2 "Proposition 2: â€£ 2.2 Relationships among exogeneity conditions â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") provides a unified characterization of exogeneity concepts from different strands of the literature and will serve as a reference point when we discuss the consistency of OLS, GLS-type, and Durbin-type estimators.

The equivalences in part (i) show that two seemingly different notions of exogeneity in the literature actually characterize the same class of data-generating processes under AssumptionÂ [1](https://arxiv.org/html/2601.21272v1#Thmassumption1 "Assumption 1 (Finite-predictor exactness at lag ğ‘â‚€): â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"). This class corresponds to environments in which the regressors and the disturbance are separated at the second-order level: the marginal dynamics of {ğ’™t}\{{\mbox{$x$}}\_{t}\} and {ğ’–t}\{{\mbox{$u$}}\_{t}\} follow their own VAR laws of motion with no dynamic cross-effects. Moreover, the two innovation blocks are contemporaneously orthogonal; combined with block-diagonal dynamics, this implies â„‚â€‹ovâ€‹(ğ’™t,ğ’–tâˆ’â„“)=ğŸ{\mathbb{C}\rm{ov}}({\mbox{$x$}}\_{t},{\mbox{$u$}}\_{t-\ell})={\mbox{$0$}} for all â„“âˆˆâ„¤\ell\in\mathbb{Z}. Within this class of DGPs, conventional estimators such as OLS and GLS-type procedures that model the error dynamics independently of the regressors (e.g. Cochrane and Orcutt ([1949](https://arxiv.org/html/2601.21272v1#bib.bib1857 "Application of least squares regression to relationships containing auto-correlated error terms")); Nagakura ([2024](https://arxiv.org/html/2601.21272v1#bib.bib1985 "Cochraneâ€“orcutt type estimator for multivariate linear regression model with serially correlated errors"))) are consistent. Furthermore, the OLS-based GLS estimator is asymptotically efficient within this class of DGPs.

The equivalence in part (ii) shows that the innovation-based pre-determined condition and the Eâ€‹Bâ€‹DEBD condition describe exactly the same class of data-generating processes under AssumptionÂ [1](https://arxiv.org/html/2601.21272v1#Thmassumption1 "Assumption 1 (Finite-predictor exactness at lag ğ‘â‚€): â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"). In Perron and GonzÃ¡lez-Coya ([2022](https://arxiv.org/html/2601.21272v1#bib.bib2158 "Feasible gls for time series regression")), the proposed FGLS-D estimator in a single-equation model is claimed to be consistent under pre-determined regressors. However, within our joint VAR/VMA framework, this statement is valid only for a narrower subclass of pre-determined DGPs. In their analysis, the innovation sequence is defined from the marginal Wold decomposition of utu\_{t}, and lagged xtx\_{t} is excluded from the error law of motion. In our block-VAR notation, this corresponds to imposing Î¨uâ€‹x,j=0\Psi\_{ux,j}=0 for all jj, while maintaining ğšºxâ€‹u=ğŸ{\mbox{$\Sigma$}}\_{xu}={\mbox{$0$}}. Hence, their maintained setting corresponds to the Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG region (upper block-triangular), which is strictly contained in the full pre-determined/Eâ€‹Bâ€‹DEBD class. At this point, it is also useful to clarify how the innovation-based exogenous condition relates to our joint VAR/VMA framework. The next remark records its precise location relative to Bâ€‹DBD, Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG, and Eâ€‹Bâ€‹DEBD.

###### Remark 2 (Innovation-based exogeneity and its location):

In addition to the pre-determined condition in PropositionÂ [2](https://arxiv.org/html/2601.21272v1#Thmproposition2 "Proposition 2: â€£ 2.2 Relationships among exogeneity conditions â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")(ii), one may consider the innovation-based exogenous condition

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[ğ’™tâ€‹(ğœºu,t,ğœºu,tâˆ’1,â€¦)â€²]=ğŸ,{\mathbb{E}}\big[{\mbox{$x$}}\_{t}({\mbox{$\varepsilon$}}\_{u,t},{\mbox{$\varepsilon$}}\_{u,t-1},\ldots)^{\prime}\big]={\mbox{$0$}}, |  |

where {ğœºu,t}\{{\mbox{$\varepsilon$}}\_{u,t}\} is the uu-block of the joint Wold innovation ğœºt{\mbox{$\varepsilon$}}\_{t} of ğ’›Â¯t\bar{{\mbox{$z$}}}\_{t} in PropositionÂ [1](https://arxiv.org/html/2601.21272v1#Thmproposition1 "Proposition 1: â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").333In Perron and GonzÃ¡lez-Coya ([2022](https://arxiv.org/html/2601.21272v1#bib.bib2158 "Feasible gls for time series regression")), the innovation sequence is defined from the marginal Wold decomposition of utu\_{t} alone. Since our innovations are defined jointly for (ğ’™t,ğ’–t)({\mbox{$x$}}\_{t},{\mbox{$u$}}\_{t}), the two notions need not coincide when ğ’™t{\mbox{$x$}}\_{t} helps predict ğ’–t{\mbox{$u$}}\_{t}.

Under AssumptionÂ [1](https://arxiv.org/html/2601.21272v1#Thmassumption1 "Assumption 1 (Finite-predictor exactness at lag ğ‘â‚€): â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"), the exogenous condition is strictly stronger than pre-determined/Eâ€‹Bâ€‹DEBD. Indeed, writing the joint Wold representation as

|  |  |  |
| --- | --- | --- |
|  | ğ’™Â¯t=âˆ‘i=0âˆğšµxâ€‹x,iâ€‹ğœºx,tâˆ’i+âˆ‘i=0âˆğšµxâ€‹u,iâ€‹ğœºu,tâˆ’i,\bar{{\mbox{$x$}}}\_{t}=\sum\_{i=0}^{\infty}{\mbox{$\Xi$}}\_{xx,i}{\mbox{$\varepsilon$}}\_{x,t-i}+\sum\_{i=0}^{\infty}{\mbox{$\Xi$}}\_{xu,i}{\mbox{$\varepsilon$}}\_{u,t-i}, |  |

the restriction ğšµxâ€‹u,i=ğŸ{\mbox{$\Xi$}}\_{xu,i}={\mbox{$0$}} for all iâ‰¥0i\geq 0 (i.e., past uu-innovations do not enter the linear representation of ğ’™t{\mbox{$x$}}\_{t}) is sufficient for the exogenous condition to hold.444Under the regularity conditions in PropositionÂ [1](https://arxiv.org/html/2601.21272v1#Thmproposition1 "Proposition 1: â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") (in particular, ğšºuâ€‹uâ‰»0{\mbox{$\Sigma$}}\_{uu}\succ 0), this restriction is also equivalent to the exogenous condition. In particular, since pre-determined/Eâ€‹Bâ€‹DEBD is equivalent to ğšºxâ€‹u=ğŸ{\mbox{$\Sigma$}}\_{xu}={\mbox{$0$}} (PropositionÂ [2](https://arxiv.org/html/2601.21272v1#Thmproposition2 "Proposition 2: â€£ 2.2 Relationships among exogeneity conditions â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")(ii)), we obtain the strict nesting

|  |  |  |
| --- | --- | --- |
|  | Bâ€‹DâŠŠ{exogenous}âŠŠ{pre-determined}=Eâ€‹Bâ€‹D.BD\subsetneq\{\text{exogenous}\}\subsetneq\{\text{pre-determined}\}=EBD. |  |

The first inclusion holds because Bâ€‹DBD imposes block-diagonality of both the VAR dynamics and the innovation covariance, whereas the exogenous condition only rules out the uâ†’xu\to x channel, namely ğšµxâ€‹u,i=ğŸ{\mbox{$\Xi$}}\_{xu,i}={\mbox{$0$}} for all iâ‰¥0i\geq 0 (equivalently, under invertibility, ğš¿xâ€‹u,j=ğŸ{\mbox{$\Psi$}}\_{xu,j}={\mbox{$0$}} for all jj), while allowing xâ†’ux\to u feedback.

Finally, the exogenous condition is generally not nested with the Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG condition. The exogenous condition eliminates feedback from uu to xx, while the Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG condition eliminates feedback from xx to uu; hence neither condition implies the other in general.

With these innovation-based notions in place, part (iii) locates the covariance-based present-and-past exogeneity condition within the Bâ€‹DBD and Eâ€‹Bâ€‹DEBD classes. Within the Eâ€‹Bâ€‹DEBD class, present-and-past exogeneity is weaker than strict exogeneity/Bâ€‹DBD, but still stronger than the full Eâ€‹Bâ€‹DEBD condition. In this sense, the [Stock and Watson](https://arxiv.org/html/2601.21272v1#bib.bib2217 "Introduction to econometrics")â€™s ([2019](https://arxiv.org/html/2601.21272v1#bib.bib2217 "Introduction to econometrics")) restriction based on present-and-past exogeneity can be interpreted as a nontrivial strengthening of the innovation-orthogonality restriction ğšºxâ€‹u=ğŸ{\mbox{$\Sigma$}}\_{xu}={\mbox{$0$}}, because it further restricts the lagged covariances ğ”¼â€‹[ğ’–tâ€‹ğ’™tâˆ’â„“â€²]{\mathbb{E}}[{\mbox{$u$}}\_{t}{\mbox{$x$}}\_{t-\ell}^{\prime}] for â„“â‰¥0\ell\geq 0, while still allowing richer dynamic feedback than in the Bâ€‹DBD case. At the same time, there is no simple nesting relationship between present-and-past exogeneity and Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG: some Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG DGPs satisfy covariance-based present-and-past exogeneity, while others do not, and conversely.

Among the joint VAR-based exogeneity conditions considered in this paper, namely Bâ€‹DBD, Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG, and Eâ€‹Bâ€‹DEBD, the Eâ€‹Bâ€‹DEBD class is the weakest (least restrictive). An estimator that remains consistent under Eâ€‹Bâ€‹DEBD is particularly valuable in empirical applications, where richer dynamic feedback between regressors and disturbances is likely. In what follows, we develop a generalized Durbin estimator for multiple-equation systems, together with its associated test statistics, providing a robust alternative to conventional OLS/GLS-type procedures.

### 2.3 The inconsistency of OLS and OLS-based GLS estimators in multi-equation models

In empirical applications, the ordinary least squares (OLS) is routinely used to estimate the parameter vector of ([2](https://arxiv.org/html/2601.21272v1#S2.E2 "In 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")). For notational convenience, rewrite ([2](https://arxiv.org/html/2601.21272v1#S2.E2 "In 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) as

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ’št=ğ’tâ€²â€‹ğœ¿+ğ’–t,t=1,â€¦,T,{\mbox{$y$}}\_{t}={\mbox{$Z$}}\_{t}^{\prime}{\mbox{$\kappa$}}+{\mbox{$u$}}\_{t},\quad t=1,\ldots,T, |  | (14) |

where ğ’tâ€²=[ğ‘°N,ğ‘¿tâ€²]âˆˆâ„NÃ—(N+k){\mbox{$Z$}}\_{t}^{\prime}=[{\mbox{$I$}}\_{N},{\mbox{$X$}}\_{t}^{\prime}]\in\mathbb{R}^{N\times(N+k)} and ğœ¿=(ğœ¶â€²,ğœ·â€²)â€²âˆˆâ„N+k{\mbox{$\kappa$}}=({\mbox{$\alpha$}}^{\prime},{\mbox{$\beta$}}^{\prime})^{\prime}\in\mathbb{R}^{N+k}. Then the OLS estimator of ([14](https://arxiv.org/html/2601.21272v1#S2.E14 "In 2.3 The inconsistency of OLS and OLS-based GLS estimators in multi-equation models â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) is

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğœ¿^Oâ€‹Lâ€‹S:=(âˆ‘t=1Tğ’tâ€‹ğ’tâ€²)âˆ’1â€‹(âˆ‘t=1Tğ’tâ€‹ğ’št).\widehat{{\mbox{$\kappa$}}}^{OLS}:=\Big(\sum\_{t=1}^{T}{\mbox{$Z$}}\_{t}{\mbox{$Z$}}\_{t}^{\prime}\Big)^{-1}\Big(\sum\_{t=1}^{T}{\mbox{$Z$}}\_{t}{\mbox{$y$}}\_{t}\Big). |  | (15) |

While the OLS estimator of ([14](https://arxiv.org/html/2601.21272v1#S2.E14 "In 2.3 The inconsistency of OLS and OLS-based GLS estimators in multi-equation models â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) is often used to estimate ğœ¿\kappa in time-series regressions, consistency typically requires not only a full-rank condition on the regressors but also nontrivial exogeneity restrictions. We begin with a standard identification condition.

###### Assumption 2:

ğ‘¸Z:=ğ”¼â€‹[ğ’tâ€‹ğ’tâ€²]{\mbox{$Q$}}\_{Z}:={\mathbb{E}}[{\mbox{$Z$}}\_{t}{\mbox{$Z$}}\_{t}^{\prime}] is positive definite.

AssumptionÂ [2](https://arxiv.org/html/2601.21272v1#Thmassumption2 "Assumption 2: â€£ 2.3 The inconsistency of OLS and OLS-based GLS estimators in multi-equation models â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") is a standard full-rank condition on the regressors. It rules out exact multicollinearity and guarantees that the parameter vector ğœ¿\kappa is identified from the second-moment matrix of the regressors so that the population normal equations admit a unique solution.

Baillie et al. ([2024](https://arxiv.org/html/2601.21272v1#bib.bib2090 "On robust inference in time-series regression")) state that the Bâ€‹DBD condition is required for the consistency of the OLS estimator. In contrast, Perron and GonzÃ¡lez-Coya ([2022](https://arxiv.org/html/2601.21272v1#bib.bib2158 "Feasible gls for time series regression")) formulate innovation-based exogeneity conditionsâ€“defined from the marginal Wold decomposition of utu\_{t}â€“as convenient sufficient conditions in their consistency analysis. As shown in PropositionÂ [2](https://arxiv.org/html/2601.21272v1#Thmproposition2 "Proposition 2: â€£ 2.2 Relationships among exogeneity conditions â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")(i), the Bâ€‹DBD requirement coincides with (covariance-based) strict exogeneity in our joint VAR/VMA framework. These conditions, however, are stronger than is strictly necessary. Likewise, Stock and Watson ([2019](https://arxiv.org/html/2601.21272v1#bib.bib2217 "Introduction to econometrics")) treat present-and-past exogeneity as a key assumption for OLS consistency. Present-and-past exogeneity implies the contemporaneous orthogonality condition ğ”¼â€‹[ğ’–tâ€‹ğ’™tâ€²]=ğŸ{\mathbb{E}}[{\mbox{$u$}}\_{t}{\mbox{$x$}}\_{t}^{\prime}]={\mbox{$0$}}, but it is not necessary for OLS consistency. In this sense, the exogeneity assumptions imposed in these previous studies lie (generally strictly) inside a larger class of data-generating processes under which OLS can still be consistent. The next proposition summarizes the exact condition for OLS consistency in our setting and clarifies how it relates to the exogeneity classes discussed above.

###### Proposition 3:

Suppose that AssumptionÂ [1](https://arxiv.org/html/2601.21272v1#Thmassumption1 "Assumption 1 (Finite-predictor exactness at lag ğ‘â‚€): â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")-[2](https://arxiv.org/html/2601.21272v1#Thmassumption2 "Assumption 2: â€£ 2.3 The inconsistency of OLS and OLS-based GLS estimators in multi-equation models â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") hold. The OLS estimator ğ›‹^Oâ€‹Lâ€‹S\widehat{{\mbox{$\kappa$}}}^{OLS} in ([15](https://arxiv.org/html/2601.21272v1#S2.E15 "In 2.3 The inconsistency of OLS and OLS-based GLS estimators in multi-equation models â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) is consistent for ğ›‹\kappa if and only if ğ”¼â€‹[ğ™tâ€‹ğ®t]=ğŸ{\mathbb{E}}[{\mbox{$Z$}}\_{t}{\mbox{$u$}}\_{t}]={\mbox{$0$}}, equivalently (using ğ”¼â€‹[ğ®t]=ğŸ{\mathbb{E}}[{\mbox{$u$}}\_{t}]={\mbox{$0$}}) if and only if the contemporaneous orthogonality condition ğ”¼â€‹[ğ—tâ€‹ğ®t]=ğŸ{\mathbb{E}}[{\mbox{$X$}}\_{t}{\mbox{$u$}}\_{t}]={\mbox{$0$}} holds.

###### Proof.

See the Appendix.
âˆ

PropositionÂ [3](https://arxiv.org/html/2601.21272v1#Thmproposition3 "Proposition 3: â€£ 2.3 The inconsistency of OLS and OLS-based GLS estimators in multi-equation models â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") shows that OLS consistency hinges on the contemporaneous orthogonality condition ğ”¼â€‹[ğ‘¿tâ€‹ğ’–t]=ğŸ{\mathbb{E}}[{\mbox{$X$}}\_{t}{\mbox{$u$}}\_{t}]={\mbox{$0$}} (equivalently, since ğ”¼â€‹[ğ’–t]=ğŸ{\mathbb{E}}[{\mbox{$u$}}\_{t}]={\mbox{$0$}}, ğ”¼â€‹[ğ’tâ€‹ğ’–t]=ğŸ{\mathbb{E}}[{\mbox{$Z$}}\_{t}{\mbox{$u$}}\_{t}]={\mbox{$0$}}). The key issue, therefore, is the mechanism through which this orthogonality can arise in dynamic environments.

Under AssumptionÂ [1](https://arxiv.org/html/2601.21272v1#Thmassumption1 "Assumption 1 (Finite-predictor exactness at lag ğ‘â‚€): â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"), the joint VAR/VMA representation of ğ’›Â¯t=((ğ’™tâˆ’ğx)â€²,ğ’–tâ€²)â€²\bar{{\mbox{$z$}}}\_{t}=(({\mbox{$x$}}\_{t}-{\mbox{$\mu$}}\_{x})^{\prime},{\mbox{$u$}}\_{t}^{\prime})^{\prime} implies that contemporaneous orthogonality is generally not guaranteed by innovation orthogonality alone. In particular, even if ğšºxâ€‹u=ğŸ{\mbox{$\Sigma$}}\_{xu}={\mbox{$0$}} holds, dynamic feedback captured by the off-diagonal blocks of {ğš¿j}\{{\mbox{$\Psi$}}\_{j}\} (equivalently, by the impulse responses {ğšµi}\{{\mbox{$\Xi$}}\_{i}\}) can propagate past shocks across blocks and generate â„‚â€‹ovâ€‹(ğ’™Â¯t,ğ’–t)â‰ ğŸ{\mathbb{C}\rm{ov}}(\bar{{\mbox{$x$}}}\_{t},{\mbox{$u$}}\_{t})\neq{\mbox{$0$}}. This observation motivates characterizing when ğ”¼â€‹[ğ‘¿tâ€‹ğ’–t]=ğŸ{\mathbb{E}}[{\mbox{$X$}}\_{t}{\mbox{$u$}}\_{t}]={\mbox{$0$}} holds in terms of structural restrictions on ({ğš¿j},ğšº)(\{{\mbox{$\Psi$}}\_{j}\},{\mbox{$\Sigma$}}), which naturally leads to the exogeneity classes Bâ€‹DBD, Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG, and Eâ€‹Bâ€‹DEBD.

To make this point concrete, we next locate the moment condition ğ”¼â€‹[ğ‘¿tâ€‹ğ’–t]=ğŸ{\mathbb{E}}[{\mbox{$X$}}\_{t}{\mbox{$u$}}\_{t}]={\mbox{$0$}} within the joint VAR-based exogeneity classes. We show that Bâ€‹DBD is sufficient for contemporaneous orthogonality, whereas under Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG (and hence within the broader Eâ€‹Bâ€‹DEBD class) the condition typically fails because ğ’™t{\mbox{$x$}}\_{t} may load on lagged disturbances. This establishes the precise sense in which OLS (and OLS-based GLS procedures) can be inconsistent in dynamic multi-equation environments.

###### Proposition 4:

Suppose AssumptionÂ [1](https://arxiv.org/html/2601.21272v1#Thmassumption1 "Assumption 1 (Finite-predictor exactness at lag ğ‘â‚€): â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") holds.

(i)
:   Under Bâ€‹DBD, the contemporaneous orthogonality condition holds:
    ğ”¼â€‹[ğ‘¿tâ€‹ğ’–t]=ğŸ{\mathbb{E}}[{\mbox{$X$}}\_{t}{\mbox{$u$}}\_{t}]={\mbox{$0$}} (equivalently, â„‚â€‹ovâ€‹(ğ’™Â¯t,ğ’–t)=ğŸ{\mathbb{C}\rm{ov}}(\bar{{\mbox{$x$}}}\_{t},{\mbox{$u$}}\_{t})={\mbox{$0$}}).

(ii)
:   Under Gâ€‹Eâ€‹Xâ€‹Oâ€‹Gâˆ–Bâ€‹DGEXOG\setminus BD, the contemporaneous orthogonality condition ğ”¼â€‹[ğ‘¿tâ€‹ğ’–t]=ğŸ{\mathbb{E}}[{\mbox{$X$}}\_{t}{\mbox{$u$}}\_{t}]={\mbox{$0$}} fails for generic parameter values, except under additional restrictions such as ğš¿uâ€‹u,j=0â€‹âˆ€j{\mbox{$\Psi$}}\_{uu,j}=0\ \forall j (no serial correlation in ğ’–t{\mbox{$u$}}\_{t}) or other knife-edge cancellations. Consequently, the OLS estimator ğœ¿^Oâ€‹Lâ€‹S\widehat{{\mbox{$\kappa$}}}^{OLS} is generically inconsistent under Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG.

(iii)
:   Since Gâ€‹Eâ€‹Xâ€‹Oâ€‹GâŠŠEâ€‹Bâ€‹DGEXOG\subsetneq EBD, the same generic inconsistency can arise within Eâ€‹Bâ€‹DEBD.

###### Proof.

See the Appendix.
âˆ

The mechanism in part (ii) is straightforward in the joint VAR/VMA framework. Although Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG maintains innovation orthogonality ğšºxâ€‹u=ğŸ{\mbox{$\Sigma$}}\_{xu}={\mbox{$0$}}, it permits ğ’™t{\mbox{$x$}}\_{t} to depend on lagged disturbances through {ğš¿xâ€‹u,j}\{{\mbox{$\Psi$}}\_{xu,j}\} (equivalently through {ğšµxâ€‹u,i}\{{\mbox{$\Xi$}}\_{xu,i}\}). When {ğš¿uâ€‹u,j}\{{\mbox{$\Psi$}}\_{uu,j}\} generates serial dependence in ğ’–t{\mbox{$u$}}\_{t}, this feedback creates a nonzero contemporaneous covariance between ğ’™t{\mbox{$x$}}\_{t} and ğ’–t{\mbox{$u$}}\_{t}, so that the population orthogonality condition fails and the OLS probability limit differs from ğœ¿\kappa.

A common response to serial correlation in ğ’–t{\mbox{$u$}}\_{t} is to adopt GLS-type procedures that model and remove the error dynamics. Such procedures do not, by themselves, resolve the endogeneity problem highlighted above when ğ”¼â€‹[ğ‘¿tâ€‹ğ’–t]â‰ ğŸ{\mathbb{E}}[{\mbox{$X$}}\_{t}{\mbox{$u$}}\_{t}]\neq{\mbox{$0$}}; nevertheless, they remain the workhorse approach in empirical time-series regressions and provide a useful benchmark. In practice, one typically proceeds in two steps: (i) estimate ([14](https://arxiv.org/html/2601.21272v1#S2.E14 "In 2.3 The inconsistency of OLS and OLS-based GLS estimators in multi-equation models â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) by OLS and obtain residuals {ğ’–^tOâ€‹Lâ€‹S}\{\widehat{{\mbox{$u$}}}\_{t}^{OLS}\}, and (ii) fit a finite-order VAR model to the residual vector process, apply the [Cochrane and Orcutt](https://arxiv.org/html/2601.21272v1#bib.bib1857 "Application of least squares regression to relationships containing auto-correlated error terms")â€™s ([1949](https://arxiv.org/html/2601.21272v1#bib.bib1857 "Application of least squares regression to relationships containing auto-correlated error terms")) transformation, and obtain an estimator (hereafter, the CO-type estimators). In multiple-equation models, Nagakura ([2024](https://arxiv.org/html/2601.21272v1#bib.bib1985 "Cochraneâ€“orcutt type estimator for multivariate linear regression model with serially correlated errors")) propose a CO-type estimator.

The multivariate CO-type estimator suppose that the (true) disturbance admits the VAR(p0p\_{0}) representation

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ’–t=âˆ‘j=1p0ğš¿uâ€‹u,jâ€‹ğ’–tâˆ’j+ğœºu,t,ğ”¼â€‹[ğœºu,t]=ğŸ,ğ”¼â€‹[ğœºu,tâ€‹ğœºu,tâ€²]=ğšºuâ€‹u>0,{\mbox{$u$}}\_{t}=\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{uu,j}{\mbox{$u$}}\_{t-j}+{\mbox{$\varepsilon$}}\_{u,t},\qquad{\mathbb{E}}[{\mbox{$\varepsilon$}}\_{u,t}]={\mbox{$0$}},\quad{\mathbb{E}}[{\mbox{$\varepsilon$}}\_{u,t}{\mbox{$\varepsilon$}}\_{u,t}^{\prime}]={\mbox{$\Sigma$}}\_{uu}>0, |  | (16) |

with a stable characteristic polynomial ğ‘·uâ€‹uâ€‹(z):=ğ‘°Nâˆ’âˆ‘j=1p0ğš¿uâ€‹u,jâ€‹zj{\mbox{$P$}}\_{uu}(z):={\mbox{$I$}}\_{N}-\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{uu,j}z^{j} having no zeros in or on the unit disk. Define the lag polynomial

|  |  |  |
| --- | --- | --- |
|  | ğ‘¨â€‹(L):=ğ‘°Nâˆ’âˆ‘j=1p0ğš¿uâ€‹u,jâ€‹Lj,{\mbox{$A$}}(L):={\mbox{$I$}}\_{N}-\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{uu,j}L^{j}, |  |

and the transformed variables

|  |  |  |
| --- | --- | --- |
|  | ğ’š~t:=ğ‘¨â€‹(L)â€‹ğ’št,ğ’~tâ€²:=ğ‘¨â€‹(L)â€‹ğ’tâ€²,ğ’–~t:=ğ‘¨â€‹(L)â€‹ğ’–t.\tilde{{\mbox{$y$}}}\_{t}:={\mbox{$A$}}(L){\mbox{$y$}}\_{t},\qquad\tilde{{\mbox{$Z$}}}\_{t}^{\prime}:={\mbox{$A$}}(L){\mbox{$Z$}}\_{t}^{\prime},\qquad\tilde{{\mbox{$u$}}}\_{t}:={\mbox{$A$}}(L){\mbox{$u$}}\_{t}. |  |

Under ([16](https://arxiv.org/html/2601.21272v1#S2.E16 "In 2.3 The inconsistency of OLS and OLS-based GLS estimators in multi-equation models â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")), ğ’–~t=ğœºu,t\tilde{{\mbox{$u$}}}\_{t}={\mbox{$\varepsilon$}}\_{u,t}. The infeasible GLS estimator that uses (ğ‘¨â€‹(L),ğšºuâ€‹u)({\mbox{$A$}}(L),{\mbox{$\Sigma$}}\_{uu}) is

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğœ¿^GLSâ€‹-â€‹CO:=(âˆ‘t=p0+1Tğ’~tâ€‹ğšºuâ€‹uâˆ’1â€‹ğ’~tâ€²)âˆ’1â€‹(âˆ‘t=p0+1Tğ’~tâ€‹ğšºuâ€‹uâˆ’1â€‹ğ’š~t).\widehat{{\mbox{$\kappa$}}}^{\tiny\mathrm{GLS\mbox{-}CO}}:=\Big(\sum\_{t=p\_{0}+1}^{T}\tilde{{\mbox{$Z$}}}\_{t}\,{\mbox{$\Sigma$}}\_{uu}^{-1}\tilde{{\mbox{$Z$}}}\_{t}^{\prime}\Big)^{-1}\Big(\sum\_{t=p\_{0}+1}^{T}\tilde{{\mbox{$Z$}}}\_{t}\,{\mbox{$\Sigma$}}\_{uu}^{-1}\tilde{{\mbox{$y$}}}\_{t}\Big). |  | (17) |

The feasible CO-type estimator ğœ¿^FGLSâ€‹-â€‹CO\widehat{{\mbox{$\kappa$}}}^{\tiny\mathrm{FGLS\mbox{-}CO}} is obtained by replacing (ğš¿uâ€‹u,1,â€¦,ğš¿uâ€‹u,p0,ğšºuâ€‹u)({\mbox{$\Psi$}}\_{uu,1},\ldots,{\mbox{$\Psi$}}\_{uu,p\_{0}},{\mbox{$\Sigma$}}\_{uu}) by estimates (ğš¿^uâ€‹u,1,â€¦,ğš¿^uâ€‹u,p0,ğšº^uâ€‹u)(\widehat{{\mbox{$\Psi$}}}\_{uu,1},\ldots,\widehat{{\mbox{$\Psi$}}}\_{uu,p\_{0}},\widehat{{\mbox{$\Sigma$}}}\_{uu}) computed from the first-step OLS residuals, and then applying the same formula ([17](https://arxiv.org/html/2601.21272v1#S2.E17 "In 2.3 The inconsistency of OLS and OLS-based GLS estimators in multi-equation models â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) with the corresponding estimated filter ğ‘¨^â€‹(L):=ğ‘°Nâˆ’âˆ‘j=1p0ğš¿^uâ€‹u,jâ€‹Lj\widehat{{\mbox{$A$}}}(L):={\mbox{$I$}}\_{N}-\sum\_{j=1}^{p\_{0}}\widehat{{\mbox{$\Psi$}}}\_{uu,j}L^{j} and ğšº^uâ€‹u\widehat{{\mbox{$\Sigma$}}}\_{uu}. To ensure that the infeasible GLS estimator in ([17](https://arxiv.org/html/2601.21272v1#S2.E17 "In 2.3 The inconsistency of OLS and OLS-based GLS estimators in multi-equation models â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) is well defined, we impose a standard full-rank condition on the transformed regressors ğ’~tâ€²:=ğ‘¨â€‹(L)â€‹ğ’tâ€²\tilde{{\mbox{$Z$}}}\_{t}^{\prime}:={\mbox{$A$}}(L){\mbox{$Z$}}\_{t}^{\prime}.

###### Assumption 3:

ğ‘¸~Z:=ğ”¼â€‹[ğ’~tâ€‹ğšºuâ€‹uâˆ’1â€‹ğ’~tâ€²]\tilde{{\mbox{$Q$}}}\_{Z}:={\mathbb{E}}[\tilde{{\mbox{$Z$}}}\_{t}{\mbox{$\Sigma$}}\_{uu}^{-1}\tilde{{\mbox{$Z$}}}\_{t}^{\prime}] is positive definite.

This condition rules out degeneracy created by the Cochraneâ€“Orcutt transformation; it is satisfied, for example, when the stacked vector (ğ’tâ€²,ğ’tâˆ’1â€²,â€¦,ğ’tâˆ’p0â€²)â€²({\mbox{$Z$}}\_{t}^{\prime},{\mbox{$Z$}}\_{t-1}^{\prime},\ldots,{\mbox{$Z$}}\_{t-p\_{0}}^{\prime})^{\prime} has a nonsingular second-moment matrix and ğšºuâ€‹uâ‰»0{\mbox{$\Sigma$}}\_{uu}\succ 0.

###### Proposition 5 (Consistency region of CO-type estimators):

Suppose that AssumptionsÂ [1](https://arxiv.org/html/2601.21272v1#Thmassumption1 "Assumption 1 (Finite-predictor exactness at lag ğ‘â‚€): â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")â€“[3](https://arxiv.org/html/2601.21272v1#Thmassumption3 "Assumption 3: â€£ 2.3 The inconsistency of OLS and OLS-based GLS estimators in multi-equation models â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") hold and consider a two-step OLS-based GLS procedure (CO-type estimator) defined as follows:

1. 1.

   Estimate ([14](https://arxiv.org/html/2601.21272v1#S2.E14 "In 2.3 The inconsistency of OLS and OLS-based GLS estimators in multi-equation models â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) by OLS and obtain residuals ğ’–^tOâ€‹Lâ€‹S:=ğ’štâˆ’ğ’tâ€²â€‹ğœ¿^Oâ€‹Lâ€‹S\widehat{{\mbox{$u$}}}\_{t}^{\tiny OLS}:={\mbox{$y$}}\_{t}-{\mbox{$Z$}}\_{t}^{\prime}\widehat{{\mbox{$\kappa$}}}^{\tiny OLS}.
2. 2.

   Fit a VAR(p0p\_{0}) model (or an order selected consistently for p0p\_{0}) to {ğ’–^tOâ€‹Lâ€‹S}\{\widehat{{\mbox{$u$}}}\_{t}^{\tiny OLS}\}, construct the corresponding Cochraneâ€“Orcutt transformation, and re-estimate ([14](https://arxiv.org/html/2601.21272v1#S2.E14 "In 2.3 The inconsistency of OLS and OLS-based GLS estimators in multi-equation models â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) by OLS on the transformed system, yielding ğœ¿^FGLSâ€‹-â€‹CO\widehat{{\mbox{$\kappa$}}}^{\tiny\mathrm{FGLS\mbox{-}CO}}.

Then the following statements hold.

(i)
:   If the Bâ€‹DBD condition holds (equivalently, covariance-based strict exogeneity holds), then ğœ¿^FGLSâ€‹-â€‹CO\widehat{{\mbox{$\kappa$}}}^{\tiny\mathrm{FGLS\mbox{-}CO}} is consistent for ğ›‹\kappa. Moreover, ğœ¿^FGLSâ€‹-â€‹CO\widehat{{\mbox{$\kappa$}}}^{\tiny\mathrm{FGLS\mbox{-}CO}} is asymptotically equivalent to the infeasible GLS estimator that uses the true VAR law of motion for {ğ’–t}\{{\mbox{$u$}}\_{t}\}, and hence attains asymptotic efficiency within the Bâ€‹DBD class.

(ii)
:   Outside the Bâ€‹DBD class, consistency of the CO-type estimator is not guaranteed and generically fails. In particular, under Gâ€‹Eâ€‹Xâ€‹Oâ€‹Gâˆ–Bâ€‹DGEXOG\setminus BD the first-step OLS residuals do not consistently recover the true disturbance process, so the second-step GLS transformation is misspecified and the resulting estimator typically fails to converge to ğ›‹\kappa (except under knife-edge parameter restrictions). Since Gâ€‹Eâ€‹Xâ€‹Oâ€‹GâŠŠEâ€‹Bâ€‹DGEXOG\subsetneq EBD, the same phenomenon can arise within Eâ€‹Bâ€‹Dâˆ–Bâ€‹DEBD\setminus BD.

###### Proof.

See the Appendix.
âˆ

PropositionÂ [5](https://arxiv.org/html/2601.21272v1#Thmproposition5 "Proposition 5 (Consistency region of CO-type estimators): â€£ 2.3 The inconsistency of OLS and OLS-based GLS estimators in multi-equation models â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") also clarifies why innovation-based notions such as pre-determinedness and exogeneity, emphasized by Perron and GonzÃ¡lez-Coya ([2022](https://arxiv.org/html/2601.21272v1#bib.bib2158 "Feasible gls for time series regression")), become relevant when evaluating OLS-based GLS procedures. CO-type estimators first construct OLS residuals and then model the disturbance dynamics alone in the second step. In the joint VAR/VMA framework, this corresponds to maintaining an error model in which the law of motion for ğ’–t{\mbox{$u$}}\_{t} depends only on its own lags, that is, it excludes lagged regressors from the error dynamics (so it is correctly specified when ğš¿uâ€‹x,j=ğŸ{\mbox{$\Psi$}}\_{ux,j}={\mbox{$0$}} for all jj), while treating the regressor process {ğ’™t}\{{\mbox{$x$}}\_{t}\} as given when estimating the error law of motion. When this maintained restriction is violated, innovation orthogonality by itself does not prevent the first-step residuals from being contaminated, and the resulting GLS transformation becomes misspecified.

Taken together with PropositionsÂ [4](https://arxiv.org/html/2601.21272v1#Thmproposition4 "Proposition 4: â€£ 2.3 The inconsistency of OLS and OLS-based GLS estimators in multi-equation models â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") and [5](https://arxiv.org/html/2601.21272v1#Thmproposition5 "Proposition 5 (Consistency region of CO-type estimators): â€£ 2.3 The inconsistency of OLS and OLS-based GLS estimators in multi-equation models â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"), these results show that once the DGP departs from Bâ€‹DBD, neither OLS nor OLS-based GLS procedures are reliably consistent in general. This motivates estimators that explicitly model the joint second-order dynamics of regressors and disturbances and are designed to remain consistent beyond the Bâ€‹DBD class.

### 2.4 The inconsistency of FGLS-D estimator

A related but conceptually distinct approach is the FGLS procedure proposed by Perron and GonzÃ¡lez-Coya ([2022](https://arxiv.org/html/2601.21272v1#bib.bib2158 "Feasible gls for time series regression")) for a single-equation time-series regression model. Their analysis introduces innovation-based notions such as pre-determinedness and exogeneity, formulated in terms of the Wold innovations of the disturbance process.

Within our joint VAR/VMA framework, however, PropositionÂ [2](https://arxiv.org/html/2601.21272v1#Thmproposition2 "Proposition 2: â€£ 2.2 Relationships among exogeneity conditions â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")(ii) shows that pre-determined condition (defined using the joint Wold innovations of (ğ’™t,ğ’–t)({\mbox{$x$}}\_{t},{\mbox{$u$}}\_{t})) coincides with the Eâ€‹Bâ€‹DEBD class. This highlights an important distinction: pre-determined condition controls how ğ’™t{\mbox{$x$}}\_{t} relates to the contemporaneous uu-innovation, but it does not by itself restrict the dynamic feedback channels encoded in the off-diagonal VAR blocks. Consequently, an estimator that models the disturbance dynamics alone may still be misspecified on parts of the Eâ€‹Bâ€‹DEBD region.

The implementation in Perron and GonzÃ¡lez-Coya ([2022](https://arxiv.org/html/2601.21272v1#bib.bib2158 "Feasible gls for time series regression")) proceeds by fitting a finite-order model to the disturbance process and then quasi-differencing the regression using the estimated disturbance dynamics. In our block-VAR notation for ğ’›Â¯t=((ğ’™tâˆ’ğx)â€²,ğ’–tâ€²)â€²\bar{{\mbox{$z$}}}\_{t}=(({\mbox{$x$}}\_{t}-{\mbox{$\mu$}}\_{x})^{\prime},{\mbox{$u$}}\_{t}^{\prime})^{\prime}, this corresponds to working under a maintained disturbance law of motion in which ğ’–t{\mbox{$u$}}\_{t} depends only on its own lags, namely

|  |  |  |
| --- | --- | --- |
|  | ğ’–t=âˆ‘j=1p0ğš¿uâ€‹u,jâ€‹ğ’–tâˆ’j+ğœºu,t,equivalentlyÂ â€‹ğš¿uâ€‹x,j=ğŸâ€‹Â for allÂ â€‹j,{\mbox{$u$}}\_{t}=\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{uu,j}{\mbox{$u$}}\_{t-j}+{\mbox{$\varepsilon$}}\_{u,t},\qquad\text{equivalently }\ {\mbox{$\Psi$}}\_{ux,j}={\mbox{$0$}}\ \text{ for all }j, |  |

while allowing ğ’™t{\mbox{$x$}}\_{t} to load on lagged disturbances through {ğš¿xâ€‹u,j}\{{\mbox{$\Psi$}}\_{xu,j}\}. This maintained restriction aligns with the upper block-triangular region that we call Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG. Accordingly, in what follows we derive the multivariate analogue of the FGLS-D procedure under Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG and characterize its consistency region relative to Bâ€‹DBD, Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG, and Eâ€‹Bâ€‹DEBD.

Under Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG, the joint VAR(p0p\_{0}) law of motion allows feedback from lagged disturbances to the regressors but rules out feedback from lagged regressors to the disturbance. That is, ğš¿uâ€‹x,j=ğŸ{\mbox{$\Psi$}}\_{ux,j}={\mbox{$0$}} for all jj, while ğš¿xâ€‹u,j{\mbox{$\Psi$}}\_{xu,j} may be nonzero. Imposing this restriction on ([5](https://arxiv.org/html/2601.21272v1#S2.E5 "In 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"))â€“([6](https://arxiv.org/html/2601.21272v1#S2.E6 "In 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) yields

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | ğ’™tâˆ’ğx=\displaystyle{\mbox{$x$}}\_{t}-{\mbox{$\mu$}}\_{x}= | âˆ‘j=1p0ğš¿xâ€‹x,jâ€‹(ğ’™tâˆ’jâˆ’ğx)+âˆ‘j=1p0ğš¿xâ€‹u,jâ€‹ğ’–tâˆ’j+ğœºx,t,\displaystyle\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{xx,j}\big({\mbox{$x$}}\_{t-j}-{\mbox{$\mu$}}\_{x}\big)+\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{xu,j}{\mbox{$u$}}\_{t-j}+{\mbox{$\varepsilon$}}\_{x,t}, |  | (18) |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | ğ’–t=\displaystyle{\mbox{$u$}}\_{t}= | âˆ‘j=1p0ğš¿uâ€‹u,jâ€‹ğ’–tâˆ’j+ğœºu,t,\displaystyle\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{uu,j}{\mbox{$u$}}\_{t-j}+{\mbox{$\varepsilon$}}\_{u,t}, |  | (19) |

where the dynamic feedback from ğ’™t{\mbox{$x$}}\_{t} to ğ’–t{\mbox{$u$}}\_{t} is ruled out by imposing ğš¿uâ€‹x,j=ğŸ{\mbox{$\Psi$}}\_{ux,j}={\mbox{$0$}} for all jj, while feedback from past disturbances to the regressors is allowed through ğš¿xâ€‹u,j{\mbox{$\Psi$}}\_{xu,j}.

Substituting ([19](https://arxiv.org/html/2601.21272v1#S2.E19 "In 2.4 The inconsistency of FGLS-D estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) for ğ’–t{\mbox{$u$}}\_{t} into ([2](https://arxiv.org/html/2601.21272v1#S2.E2 "In 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) yields the following regression

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ’št=\displaystyle{\mbox{$y$}}\_{t}= | ğœ¶+ğ‘¿tâ€²â€‹ğœ·+âˆ‘j=1p0ğš¿uâ€‹u,jâ€‹ğ’–tâˆ’j+ğœºu,t\displaystyle{\mbox{$\alpha$}}+{\mbox{$X$}}\_{t}^{\prime}{\mbox{$\beta$}}+\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{uu,j}{\mbox{$u$}}\_{t-j}+{\mbox{$\varepsilon$}}\_{u,t} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | =\displaystyle= | ğœ¶+ğ‘¿tâ€²â€‹ğœ·+âˆ‘j=1p0ğš¿uâ€‹u,jâ€‹(ğ’štâˆ’jâˆ’ğ‘¿tâˆ’jâ€²â€‹ğœ·âˆ’ğœ¶)+ğœºu,t\displaystyle{\mbox{$\alpha$}}+{\mbox{$X$}}\_{t}^{\prime}{\mbox{$\beta$}}+\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{uu,j}\bigl({\mbox{$y$}}\_{t-j}-{\mbox{$X$}}\_{t-j}^{\prime}{\mbox{$\beta$}}-{\mbox{$\alpha$}}\bigr)+{\mbox{$\varepsilon$}}\_{u,t} |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | =\displaystyle= | (ğ‘°Nâˆ’âˆ‘j=1p0ğš¿uâ€‹u,j)â€‹ğœ¶+ğ‘¿tâ€²â€‹ğœ·âˆ’âˆ‘j=1p0ğš¿uâ€‹u,jâ€‹ğ‘¿tâˆ’jâ€²â€‹ğœ·+âˆ‘j=1p0ğš¿uâ€‹u,jâ€‹ğ’štâˆ’j+ğœºu,t.\displaystyle\Bigl({\mbox{$I$}}\_{N}-\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{uu,j}\Bigr){\mbox{$\alpha$}}+{\mbox{$X$}}\_{t}^{\prime}{\mbox{$\beta$}}-\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{uu,j}{\mbox{$X$}}\_{t-j}^{\prime}{\mbox{$\beta$}}+\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{uu,j}{\mbox{$y$}}\_{t-j}+{\mbox{$\varepsilon$}}\_{u,t}. |  | (20) |

Using ğ‘¿tâ€²â€‹ğœ·=ğ‘©ğ’™t{\mbox{$X$}}\_{t}^{\prime}{\mbox{$\beta$}}={\mbox{$B$}}{\mbox{$x$}}\_{t} with ğ‘©:=blkdiagâ€‹(ğœ·1â€²,â€¦,ğœ·Nâ€²)âˆˆâ„NÃ—K{\mbox{$B$}}:=\mathrm{blkdiag}({\mbox{$\beta$}}\_{1}^{\prime},\ldots,{\mbox{$\beta$}}\_{N}^{\prime})\in\mathbb{R}^{N\times K}, the term âˆ’âˆ‘j=1p0ğš¿uâ€‹u,jâ€‹ğ‘¿tâˆ’jâ€²â€‹ğœ·-\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{uu,j}{\mbox{$X$}}\_{t-j}^{\prime}{\mbox{$\beta$}} in ([20](https://arxiv.org/html/2601.21272v1#S2.E20 "In 2.4 The inconsistency of FGLS-D estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) can be rewritten as

|  |  |  |
| --- | --- | --- |
|  | âˆ’âˆ‘j=1p0ğš¿uâ€‹u,jâ€‹ğ‘¿tâˆ’jâ€²â€‹ğœ·=âˆ’âˆ‘j=1p0ğš¿uâ€‹u,jâ€‹ğ‘©â€‹ğ’™tâˆ’j=âˆ‘j=1p0ğš«jâ€‹ğ’™tâˆ’j,-\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{uu,j}{\mbox{$X$}}\_{t-j}^{\prime}{\mbox{$\beta$}}=-\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{uu,j}{\mbox{$B$}}\,{\mbox{$x$}}\_{t-j}=\sum\_{j=1}^{p\_{0}}{\mbox{$\Delta$}}\_{j}\,{\mbox{$x$}}\_{t-j}, |  |

where, at the population level,

|  |  |  |
| --- | --- | --- |
|  | ğš«j:=âˆ’ğš¿uâ€‹u,jâ€‹ğ‘©,j=1,â€¦,p0.{\mbox{$\Delta$}}\_{j}:=-{\mbox{$\Psi$}}\_{uu,j}{\mbox{$B$}},\qquad j=1,\ldots,p\_{0}. |  |

For estimation, however, we treat {ğš«j}j=1p0\{{\mbox{$\Delta$}}\_{j}\}\_{j=1}^{p\_{0}} as unrestricted nuisance parameters so that the augmented regression remains linear in the unknown coefficients. This reparameterization does not change the population regression implied by Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG; it merely avoids imposing the bilinear cross-parameter restriction ğš«j=âˆ’ğš¿uâ€‹u,jâ€‹ğ‘©{\mbox{$\Delta$}}\_{j}=-{\mbox{$\Psi$}}\_{uu,j}{\mbox{$B$}} during estimation.

With this linear reparameterization, ([20](https://arxiv.org/html/2601.21272v1#S2.E20 "In 2.4 The inconsistency of FGLS-D estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) becomes a linear regression model in which ğ’„c, ğœ·\beta, {ğš¿uâ€‹u,j}j=1p0\{{\mbox{$\Psi$}}\_{uu,j}\}\_{j=1}^{p\_{0}}, and {ğš«j}j=1p0\{{\mbox{$\Delta$}}\_{j}\}\_{j=1}^{p\_{0}} enter linearly. Hence the first step of the FGLS-D procedure can be implemented by equation-by-equation OLS: for each i=1,â€¦,Ni=1,\ldots,N, regress yi,ty\_{i,t} on the contemporaneous regressors in equation ii, the lagged dependent vectors {ğ’štâˆ’j}j=1p0\{{\mbox{$y$}}\_{t-j}\}\_{j=1}^{p\_{0}}, and the chosen lagged regressor vector(s) (e.g. {ğ’™tâˆ’j}j=1p0\{{\mbox{$x$}}\_{t-j}\}\_{j=1}^{p\_{0}} or {ğ’™i,tâˆ’j}j=1p0\{{\mbox{$x$}}\_{i,t-j}\}\_{j=1}^{p\_{0}}). Collecting the estimated coefficients on {ğ’štâˆ’j}j=1p0\{{\mbox{$y$}}\_{t-j}\}\_{j=1}^{p\_{0}} across ii yields ğš¿^uâ€‹u,1,â€¦,ğš¿^uâ€‹u,p0\widehat{{\mbox{$\Psi$}}}\_{uu,1},\ldots,\widehat{{\mbox{$\Psi$}}}\_{uu,p\_{0}}, and collecting those on the lagged regressor terms yields ğš«^1,â€¦,ğš«^p0\widehat{{\mbox{$\Delta$}}}\_{1},\ldots,\widehat{{\mbox{$\Delta$}}}\_{p\_{0}}, together with residuals ğœº^u,t\widehat{{\mbox{$\varepsilon$}}}\_{u,t} used to estimate ğšºuâ€‹u{\mbox{$\Sigma$}}\_{uu}. This leads to the following two-step FGLS-D procedure.

(Step 1) OLS estimation of the augmented regression and VAR error dynamics.
:   For each equation i=1,â€¦,Ni=1,\ldots,N, run OLS on ([20](https://arxiv.org/html/2601.21272v1#S2.E20 "In 2.4 The inconsistency of FGLS-D estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")):

    |  |  |  |
    | --- | --- | --- |
    |  | yi,t=ci+ğ’™i,tâ€²â€‹ğœ·i+âˆ‘j=1p0ğ’štâˆ’jâ€²â€‹ğuâ€‹u,j,i+âˆ‘j=1p0ğ’™tâˆ’jâ€²â€‹ğœ¹j,i+Îµu,i,t,y\_{i,t}=c\_{i}+{\mbox{$x$}}\_{i,t}^{\prime}{\mbox{$\beta$}}\_{i}+\sum\_{j=1}^{p\_{0}}{\mbox{$y$}}\_{t-j}^{\prime}{\mbox{$\psi$}}\_{uu,j,i}+\sum\_{j=1}^{p\_{0}}{\mbox{$x$}}\_{t-j}^{\prime}{\mbox{$\delta$}}\_{j,i}+\varepsilon\_{u,i,t}, |  |

    where ğuâ€‹u,j,iâ€²{\mbox{$\psi$}}\_{uu,j,i}^{\prime} is the ii-th row of ğš¿uâ€‹u,j{\mbox{$\Psi$}}\_{uu,j} and ğœ¹j,iâ€²{\mbox{$\delta$}}\_{j,i}^{\prime} is the ii-th row of ğš«j{\mbox{$\Delta$}}\_{j}. Collecting {ğ^uâ€‹u,j,iâ€²}i=1N\{\widehat{{\mbox{$\psi$}}}\_{uu,j,i}^{\prime}\}\_{i=1}^{N} yields ğš¿^uâ€‹u,jâˆˆâ„NÃ—N\widehat{{\mbox{$\Psi$}}}\_{uu,j}\in\mathbb{R}^{N\times N} for each j=1,â€¦,p0j=1,\ldots,p\_{0}. Using the OLS residuals

    |  |  |  |
    | --- | --- | --- |
    |  | ğœº^u,t(1):=ğ’štâˆ’ğ’„^âˆ’ğ‘¿tâ€²â€‹ğœ·^âˆ’âˆ‘j=1p0ğš¿^uâ€‹u,jâ€‹ğ’štâˆ’jâˆ’âˆ‘j=1p0ğš«^jâ€‹ğ’™tâˆ’j,\widehat{{\mbox{$\varepsilon$}}}\_{u,t}^{(1)}:={\mbox{$y$}}\_{t}-\widehat{{\mbox{$c$}}}-{\mbox{$X$}}\_{t}^{\prime}\widehat{{\mbox{$\beta$}}}-\sum\_{j=1}^{p\_{0}}\widehat{{\mbox{$\Psi$}}}\_{uu,j}{\mbox{$y$}}\_{t-j}-\sum\_{j=1}^{p\_{0}}\widehat{{\mbox{$\Delta$}}}\_{j}{\mbox{$x$}}\_{t-j}, |  |

    estimate the innovation covariance matrix by

    |  |  |  |
    | --- | --- | --- |
    |  | ğšº^uâ€‹u:=1Tâˆ’p0â€‹âˆ‘t=p0+1Tğœº^u,t(1)â€‹(ğœº^u,t(1))â€².\widehat{{\mbox{$\Sigma$}}}\_{uu}:=\frac{1}{T-p\_{0}}\sum\_{t=p\_{0}+1}^{T}\widehat{{\mbox{$\varepsilon$}}}\_{u,t}^{(1)}(\widehat{{\mbox{$\varepsilon$}}}\_{u,t}^{(1)})^{\prime}. |  |

(Step 2) Quasi-differencing and GLS on the filtered system.
:   Using ğš¿^uâ€‹u,1,â€¦,ğš¿^uâ€‹u,p0\widehat{{\mbox{$\Psi$}}}\_{uu,1},\ldots,\widehat{{\mbox{$\Psi$}}}\_{uu,p\_{0}} from
    Step 1, construct the filtered (or â€œquasi-differencedâ€) series

    |  |  |  |
    | --- | --- | --- |
    |  | ğ’šFD,t:=ğ’štâˆ’âˆ‘j=1p0ğš¿^uâ€‹u,jâ€‹ğ’štâˆ’j,ğ‘¿FD,tâ€²:=ğ‘¿tâ€²âˆ’âˆ‘j=1p0ğš¿^uâ€‹u,jâ€‹ğ‘¿tâˆ’jâ€²,{\mbox{$y$}}\_{\mathrm{FD},t}:={\mbox{$y$}}\_{t}-\sum\_{j=1}^{p\_{0}}\widehat{{\mbox{$\Psi$}}}\_{uu,j}{\mbox{$y$}}\_{t-j},\qquad{\mbox{$X$}}\_{\mathrm{FD},t}^{\prime}:={\mbox{$X$}}\_{t}^{\prime}-\sum\_{j=1}^{p\_{0}}\widehat{{\mbox{$\Psi$}}}\_{uu,j}{\mbox{$X$}}\_{t-j}^{\prime}, |  |

    and define

    |  |  |  |
    | --- | --- | --- |
    |  | ğ’FD,tâ€²:=[(ğ‘°Nâˆ’âˆ‘j=1p0ğš¿^uâ€‹u,j),ğ‘¿FD,tâ€²],ğœ¿:=(ğœ¶â€²,ğœ·â€²)â€².{\mbox{$Z$}}\_{\mathrm{FD},t}^{\prime}:=\biggl[\Bigl({\mbox{$I$}}\_{N}-\sum\_{j=1}^{p\_{0}}\widehat{{\mbox{$\Psi$}}}\_{uu,j}\Bigr),\ \ {\mbox{$X$}}\_{\mathrm{FD},t}^{\prime}\biggr],\qquad{\mbox{$\kappa$}}:=({\mbox{$\alpha$}}^{\prime},{\mbox{$\beta$}}^{\prime})^{\prime}. |  |

    Under the Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG restriction and AssumptionsÂ [1](https://arxiv.org/html/2601.21272v1#Thmassumption1 "Assumption 1 (Finite-predictor exactness at lag ğ‘â‚€): â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")â€“[2](https://arxiv.org/html/2601.21272v1#Thmassumption2 "Assumption 2: â€£ 2.3 The inconsistency of OLS and OLS-based GLS estimators in multi-equation models â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"), the filtered regression satisfies

    |  |  |  |
    | --- | --- | --- |
    |  | ğ’šFD,t=ğ’FD,tâ€²â€‹ğœ¿+ğœºu,t.{\mbox{$y$}}\_{\mathrm{FD},t}={\mbox{$Z$}}\_{\mathrm{FD},t}^{\prime}{\mbox{$\kappa$}}+{\mbox{$\varepsilon$}}\_{u,t}. |  |

    The multivariate FGLS-D estimator is then given by

    |  |  |  |  |
    | --- | --- | --- | --- |
    |  | ğœ¿^FGLSâˆ’D:=(âˆ‘t=p0+1Tğ’FD,tâ€‹ğšº^uâ€‹uâˆ’1â€‹ğ’FD,tâ€²)âˆ’1â€‹(âˆ‘t=p0+1Tğ’FD,tâ€‹ğšº^uâ€‹uâˆ’1â€‹ğ’šFD,t).\widehat{{\mbox{$\kappa$}}}^{\mathrm{FGLS\!-\!D}}:=\Biggl(\sum\_{t=p\_{0}+1}^{T}{\mbox{$Z$}}\_{\mathrm{FD},t}\,\widehat{{\mbox{$\Sigma$}}}\_{uu}^{-1}\,{\mbox{$Z$}}\_{\mathrm{FD},t}^{\prime}\Biggr)^{-1}\Biggl(\sum\_{t=p\_{0}+1}^{T}{\mbox{$Z$}}\_{\mathrm{FD},t}\,\widehat{{\mbox{$\Sigma$}}}\_{uu}^{-1}\,{\mbox{$y$}}\_{\mathrm{FD},t}\Biggr). |  | (21) |

The above algorithm defines the multivariate FGLS-D estimator as a two-step procedure that (i) estimates an augmented regression implied by the restricted joint VAR structure under Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG, and (ii) applies a quasi-differencing based on the estimated error dynamics, followed by GLS with respect to the innovation covariance ğšºuâ€‹u{\mbox{$\Sigma$}}\_{uu}. Intuitively, this construction exploits GLS-type corrections for serial correlation and cross-equation innovation covariance while relaxing the full block-diagonality of the Bâ€‹DBD condition by allowing lagged disturbances to affect the regressors (i.e., ğš¿xâ€‹u,jâ‰ ğŸ{\mbox{$\Psi$}}\_{xu,j}\neq{\mbox{$0$}}), yet maintaining the Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG restriction ğš¿uâ€‹x,j=ğŸ{\mbox{$\Psi$}}\_{ux,j}={\mbox{$0$}} and innovation orthogonality. To ensure that the second-step GLS estimator in ([21](https://arxiv.org/html/2601.21272v1#S2.E21 "In item (Step 2) Quasi-differencing and GLS on the filtered system. â€£ 2.4 The inconsistency of FGLS-D estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) is well defined, we impose a full-rank condition on the filtered regressors.

###### Assumption 4:

ğ‘¸FD:=ğ”¼â€‹[ğ’FD,tâ€‹ğšºuâ€‹uâˆ’1â€‹ğ’FD,tâ€²]{\mbox{$Q$}}\_{\mathrm{FD}}:={\mathbb{E}}[{\mbox{$Z$}}\_{\mathrm{FD},t}{\mbox{$\Sigma$}}\_{uu}^{-1}{\mbox{$Z$}}\_{\mathrm{FD},t}^{\prime}] is positive definite.

AssumptionÂ [4](https://arxiv.org/html/2601.21272v1#Thmassumption4 "Assumption 4: â€£ 2.4 The inconsistency of FGLS-D estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") guarantees that the population GLS information matrix is nonsingular. In addition, under consistency of ğšº^uâ€‹u\widehat{{\mbox{$\Sigma$}}}\_{uu} (shown below), we have ğšº^uâ€‹uâˆ’1â†’ğ‘ğšºuâ€‹uâˆ’1\widehat{{\mbox{$\Sigma$}}}\_{uu}^{-1}\xrightarrow{p}{\mbox{$\Sigma$}}\_{uu}^{-1}, and hence 1/(Tâˆ’p0)â€‹âˆ‘t=p0+1Tğ’FD,tâ€‹ğšº^uâ€‹uâˆ’1â€‹ğ’FD,tâ€²â†’ğ‘ğ‘¸FDâ‰»01/(T-p\_{0})\sum\_{t=p\_{0}+1}^{T}{\mbox{$Z$}}\_{\mathrm{FD},t}\widehat{{\mbox{$\Sigma$}}}\_{uu}^{-1}{\mbox{$Z$}}\_{\mathrm{FD},t}^{\prime}\xrightarrow{p}{\mbox{$Q$}}\_{\mathrm{FD}}\succ 0, so the sample matrix in ([21](https://arxiv.org/html/2601.21272v1#S2.E21 "In item (Step 2) Quasi-differencing and GLS on the filtered system. â€£ 2.4 The inconsistency of FGLS-D estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) is nonsingular with probability approaching one.

The next proposition establishes the asymptotic properties of the multivariate FGLS-D estimator within the Bâ€‹DBD/Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG/Eâ€‹Bâ€‹DEBD framework and clarifies the extent to which it extends the consistency region relative to OLS- and CO-type estimators.

###### Proposition 6 (Consistency region of the FGLS-D estimator):

Suppose that AssumptionsÂ [1](https://arxiv.org/html/2601.21272v1#Thmassumption1 "Assumption 1 (Finite-predictor exactness at lag ğ‘â‚€): â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")â€“[2](https://arxiv.org/html/2601.21272v1#Thmassumption2 "Assumption 2: â€£ 2.3 The inconsistency of OLS and OLS-based GLS estimators in multi-equation models â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"), [4](https://arxiv.org/html/2601.21272v1#Thmassumption4 "Assumption 4: â€£ 2.4 The inconsistency of FGLS-D estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") hold and that the VAR order p0p\_{0} in the disturbance dynamics is fixed and correctly specified. Let ğ›‹^FGLSâˆ’D\widehat{{\mbox{$\kappa$}}}^{\mathrm{FGLS\!-\!D}} be the multivariate FGLS-D estimator defined by ([20](https://arxiv.org/html/2601.21272v1#S2.E20 "In 2.4 The inconsistency of FGLS-D estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"))â€“([21](https://arxiv.org/html/2601.21272v1#S2.E21 "In item (Step 2) Quasi-differencing and GLS on the filtered system. â€£ 2.4 The inconsistency of FGLS-D estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")). Then:

(i)
:   If the data-generating process satisfies the Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG condition, then ğœ¿^FGLSâˆ’Dâ†’ğ‘ğœ¿=(ğœ¶â€²,ğœ·â€²)â€²\widehat{{\mbox{$\kappa$}}}^{\mathrm{FGLS\!-\!D}}\xrightarrow{p}{\mbox{$\kappa$}}=({\mbox{$\alpha$}}^{\prime},{\mbox{$\beta$}}^{\prime})^{\prime}. Moreover, ğœ¿^FGLSâˆ’D\widehat{{\mbox{$\kappa$}}}^{\mathrm{FGLS\!-\!D}} is asymptotically equivalent to the infeasible GLS estimator that quasi-differences the system using the true lag polynomial ğ‘¨â€‹(L):=ğ‘°Nâˆ’âˆ‘j=1p0ğš¿uâ€‹u,jâ€‹Lj{\mbox{$A$}}(L):={\mbox{$I$}}\_{N}-\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{uu,j}L^{j} and uses the true innovation covariance matrix ğšºuâ€‹u{\mbox{$\Sigma$}}\_{uu}; in particular, Tâ€‹(ğœ¿^FGLSâˆ’Dâˆ’ğœ¿)\sqrt{T}\big(\widehat{{\mbox{$\kappa$}}}^{\mathrm{FGLS\!-\!D}}-{\mbox{$\kappa$}}\big) has the same limiting distribution as its infeasible GLS counterpart. Under the Bâ€‹DBD condition, this implies that ğœ¿^FGLSâˆ’D\widehat{{\mbox{$\kappa$}}}^{\mathrm{FGLS\!-\!D}} is asymptotically equivalent to the usual Cochraneâ€“Orcutt-type GLS estimator based on the marginal VAR for {ğ’–t}\{{\mbox{$u$}}\_{t}\}.

(ii)
:   If the data-generating process lies in Eâ€‹Bâ€‹Dâˆ–Gâ€‹Eâ€‹Xâ€‹Oâ€‹GEBD\setminus GEXOG (i.e., ğš¿uâ€‹x,jâ‰ ğŸ{\mbox{$\Psi$}}\_{ux,j}\neq{\mbox{$0$}} for some jj), then ğœ¿^FGLSâˆ’D\widehat{{\mbox{$\kappa$}}}^{\mathrm{FGLS\!-\!D}} is generically inconsistent for ğ›‹\kappa (except under knife-edge parameter cancellations). In this region, {ğ’–t}\{{\mbox{$u$}}\_{t}\} does not follow a closed VAR driven only by its own lags, so the quasi-differencing filter based solely on {ğš¿uâ€‹u,j}\{{\mbox{$\Psi$}}\_{uu,j}\} is misspecified, and the second-step GLS regression in ([21](https://arxiv.org/html/2601.21272v1#S2.E21 "In item (Step 2) Quasi-differencing and GLS on the filtered system. â€£ 2.4 The inconsistency of FGLS-D estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) is constructed under an incorrect error structure.

###### Proof.

See the Appendix.
âˆ

PropositionÂ [6](https://arxiv.org/html/2601.21272v1#Thmproposition6 "Proposition 6 (Consistency region of the FGLS-D estimator): â€£ 2.4 The inconsistency of FGLS-D estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") clarifies the role of the FGLS-D estimator within the Bâ€‹DBD/Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG/Eâ€‹Bâ€‹DEBD framework. While OLS-based CO-type procedures are only guaranteed to be consistent under the narrow Bâ€‹DBD class (and typically fail outside Bâ€‹DBD), FGLS-D remains consistent on the larger Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG region and is asymptotically equivalent to the corresponding infeasible GLS estimator within that class. In particular, even when the regressors load on lagged disturbances so that contemporaneous orthogonalityâ€”and hence OLS consistencyâ€”fails generically, the one-sided feedback structure of Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG ensures that {ğ’–t}\{{\mbox{$u$}}\_{t}\} admits a closed VAR(p0p\_{0}) representation driven by {ğœºu,t}\{{\mbox{$\varepsilon$}}\_{u,t}\}. Consequently, the quasi-differencing step based on the estimated {ğš¿uâ€‹u,j}\{{\mbox{$\Psi$}}\_{uu,j}\} is asymptotically correctly specified, delivering an infeasible-GLS-equivalent estimator.

At the same time, PropositionÂ [6](https://arxiv.org/html/2601.21272v1#Thmproposition6 "Proposition 6 (Consistency region of the FGLS-D estimator): â€£ 2.4 The inconsistency of FGLS-D estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")(ii) highlights a structural limitation of FGLS-D that stems from its reliance on the restricted VAR for {ğ’–t}\{{\mbox{$u$}}\_{t}\}. Once the DGP moves beyond Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG into the more general Eâ€‹Bâ€‹Dâˆ–Gâ€‹Eâ€‹Xâ€‹Oâ€‹GEBD\setminus GEXOG region, {ğ’–t}\{{\mbox{$u$}}\_{t}\} no longer follows a closed VAR driven solely by its own lags, and the quasi-differencing filter based only on {ğš¿uâ€‹u,j}\{{\mbox{$\Psi$}}\_{uu,j}\} becomes misspecified. In this sense, OLS, CO-type GLS, and FGLS-D each relyâ€”in different waysâ€”on variants of the Bâ€‹DBD/Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG structure and do not provide a fully robust solution when bidirectional dynamic feedback between regressors and disturbances is present. This motivates the generalized Durbin-type estimators developed in the next subsection, which are designed to retain consistency under the weakest Eâ€‹Bâ€‹DEBD condition.

### 2.5 Asymptotic properties of generalized Durbin estimator

We extend Durbin regression to a multiple-equation framework and derive an estimator that remains consistent under the Bâ€‹DBD, Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG, and Eâ€‹Bâ€‹DEBD conditions. Starting from ([2](https://arxiv.org/html/2601.21272v1#S2.E2 "In 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")), we obtain a multiple-equation specification by exploiting the blockwise VAR(p0p\_{0}) representation of ğ’›Â¯t\bar{{\mbox{$z$}}}\_{t} given in PropositionÂ [1](https://arxiv.org/html/2601.21272v1#Thmproposition1 "Proposition 1: â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"):

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | ğ’™tâˆ’ğx=\displaystyle{\mbox{$x$}}\_{t}-{\mbox{$\mu$}}\_{x}= | âˆ‘j=1p0ğš¿xâ€‹x,jâ€‹(ğ’™tâˆ’jâˆ’ğx)+âˆ‘j=1p0ğš¿xâ€‹u,jâ€‹ğ’–tâˆ’j+ğœºx,t,\displaystyle\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{xx,j}\big({\mbox{$x$}}\_{t-j}-{\mbox{$\mu$}}\_{x}\big)+\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{xu,j}{\mbox{$u$}}\_{t-j}+{\mbox{$\varepsilon$}}\_{x,t}, |  | (22) |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | ğ’–t=\displaystyle{\mbox{$u$}}\_{t}= | âˆ‘j=1p0ğš¿uâ€‹x,jâ€‹(ğ’™tâˆ’jâˆ’ğx)+âˆ‘j=1p0ğš¿uâ€‹u,jâ€‹ğ’–tâˆ’j+ğœºu,t.\displaystyle\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{ux,j}\big({\mbox{$x$}}\_{t-j}-{\mbox{$\mu$}}\_{x}\big)+\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{uu,j}{\mbox{$u$}}\_{t-j}+{\mbox{$\varepsilon$}}\_{u,t}. |  | (23) |

Substituting ([23](https://arxiv.org/html/2601.21272v1#S2.E23 "In 2.5 Asymptotic properties of generalized Durbin estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) for ğ’–t{\mbox{$u$}}\_{t} into ([2](https://arxiv.org/html/2601.21272v1#S2.E2 "In 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) yields the following regression model:

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ’št=ğœ¸+ğ‘¿tâ€²â€‹ğœ·+âˆ‘j=1p0ğš¿uâ€‹u,jâ€‹ğ’štâˆ’j+âˆ‘j=1p0ğš²jâ€‹ğ’™tâˆ’j+ğœºu,t,{\mbox{$y$}}\_{t}={\mbox{$\gamma$}}+{\mbox{$X$}}\_{t}^{\prime}{\mbox{$\beta$}}+\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{uu,j}{\mbox{$y$}}\_{t-j}+\sum\_{j=1}^{p\_{0}}{\mbox{$\Lambda$}}\_{j}{\mbox{$x$}}\_{t-j}+{\mbox{$\varepsilon$}}\_{u,t}, |  | (24) |

where

|  |  |  |
| --- | --- | --- |
|  | ğœ¸=(ğ‘°Nâˆ’âˆ‘j=1p0ğš¿uâ€‹u,j)â€‹ğœ¶âˆ’âˆ‘j=1p0ğš¿uâ€‹x,jâ€‹ğx,ğš²j=ğš¿uâ€‹x,jâˆ’ğš¿uâ€‹u,jâ€‹ğ‘©,{\mbox{$\gamma$}}=\bigl({\mbox{$I$}}\_{N}-\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{uu,j}\bigr){\mbox{$\alpha$}}-\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{ux,j}{\mbox{$\mu$}}\_{x},\quad{\mbox{$\Lambda$}}\_{j}={\mbox{$\Psi$}}\_{ux,j}-{\mbox{$\Psi$}}\_{uu,j}{\mbox{$B$}}, |  |

and ğ‘©=blkdiagâ€‹(ğœ·1â€²,ğœ·2â€²,â€¦,ğœ·Nâ€²){\mbox{$B$}}=\mathrm{blkdiag}\big({\mbox{$\beta$}}\_{1}^{\prime},{\mbox{$\beta$}}\_{2}^{\prime},\ldots,{\mbox{$\beta$}}\_{N}^{\prime}\big).

For estimation, we treat {ğš²j}j=1p0\{{\mbox{$\Lambda$}}\_{j}\}\_{j=1}^{p\_{0}} as unrestricted nuisance parameters so that the augmented regression remains linear in the unknown coefficients. This reparameterization does not alter the population regression implied by ([23](https://arxiv.org/html/2601.21272v1#S2.E23 "In 2.5 Asymptotic properties of generalized Durbin estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")); it simply avoids imposing the bilinear cross-parameter restriction ğš²j=ğš¿uâ€‹x,jâˆ’ğš¿uâ€‹u,jâ€‹ğ‘©{\mbox{$\Lambda$}}\_{j}={\mbox{$\Psi$}}\_{ux,j}-{\mbox{$\Psi$}}\_{uu,j}{\mbox{$B$}} during estimation.

With this linear reparameterization, ([24](https://arxiv.org/html/2601.21272v1#S2.E24 "In 2.5 Asymptotic properties of generalized Durbin estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) can be estimated equation by equation. Specifically, for each i=1,â€¦,Ni=1,\ldots,N we consider the ii-th equation

|  |  |  |  |
| --- | --- | --- | --- |
|  | yi,t=Î³i+ğ’™i,tâ€²â€‹ğœ·i+âˆ‘j=1p0ğ’štâˆ’jâ€²â€‹ğuâ€‹u,j,i+âˆ‘j=1p0ğ’™tâˆ’jâ€²â€‹ğ€j,i+Îµu,i,t,y\_{i,t}=\gamma\_{i}+{\mbox{$x$}}\_{i,t}^{\prime}{\mbox{$\beta$}}\_{i}+\sum\_{j=1}^{p\_{0}}{\mbox{$y$}}\_{t-j}^{\prime}{\mbox{$\psi$}}\_{uu,j,i}+\sum\_{j=1}^{p\_{0}}{\mbox{$x$}}\_{t-j}^{\prime}{\mbox{$\lambda$}}\_{j,i}+\varepsilon\_{u,i,t}, |  | (25) |

where ğuâ€‹u,j,iâ€²{\mbox{$\psi$}}\_{uu,j,i}^{\prime} denotes the ii-th row of ğš¿uâ€‹u,j{\mbox{$\Psi$}}\_{uu,j} (so that ğuâ€‹u,j,iâˆˆâ„N{\mbox{$\psi$}}\_{uu,j,i}\in\mathbb{R}^{N}), and ğ€j,i{\mbox{$\lambda$}}\_{j,i} denotes the coefficient vector associated with ğ’™tâˆ’j{\mbox{$x$}}\_{t-j} in the ii-th equation (equivalently, ğ€j,iâ€²{\mbox{$\lambda$}}\_{j,i}^{\prime} is the ii-th row of ğš²j{\mbox{$\Lambda$}}\_{j}). Importantly, while the lagged dependent vectors {ğ’štâˆ’j}\{{\mbox{$y$}}\_{t-j}\} and the stacked lagged regressor vectors {ğ’™tâˆ’j}\{{\mbox{$x$}}\_{t-j}\} are common across equations by construction, the contemporaneous regressors ğ’™i,t{\mbox{$x$}}\_{i,t} and the slope parameters ğœ·i{\mbox{$\beta$}}\_{i} may differ across ii.

For each i=1,â€¦,Ni=1,\ldots,N, define the equation-specific regressor vector

|  |  |  |
| --- | --- | --- |
|  | ğ’˜i,t:=[1,ğ’™i,tâ€²,ğ’štâˆ’1â€²,â€¦ğ’štâˆ’p0â€²,ğ’™tâˆ’1â€²,â€¦ğ’™tâˆ’p0â€²,]â€²âˆˆâ„di,di:=1+ki+p0â€‹N+p0â€‹k,{\mbox{$w$}}\_{i,t}:=\begin{bmatrix}1,&{\mbox{$x$}}\_{i,t}^{\prime},&{\mbox{$y$}}\_{t-1}^{\prime},&\ldots&{\mbox{$y$}}\_{t-p\_{0}}^{\prime},&{\mbox{$x$}}\_{t-1}^{\prime},&\ldots&{\mbox{$x$}}\_{t-p\_{0}}^{\prime},&\end{bmatrix}^{\prime}\in\mathbb{R}^{d\_{i}},\quad d\_{i}:=1+k\_{i}+p\_{0}N+p\_{0}k, |  |

and the corresponding parameter vector

|  |  |  |
| --- | --- | --- |
|  | ğœ½i:=[Î³iğœ·iâ€²ğuâ€‹u,1,iâ€²â€‹â€¦ğuâ€‹u,p0,iâ€²ğ€1,iâ€²â€¦ğ€p0,iâ€²]â€²âˆˆâ„di.{\mbox{$\theta$}}\_{i}:=\begin{bmatrix}\gamma\_{i}&{\mbox{$\beta$}}\_{i}^{\prime}&{\mbox{$\psi$}}\_{uu,1,i}^{\prime}\ldots&{\mbox{$\psi$}}\_{uu,p\_{0},i}^{\prime}&{\mbox{$\lambda$}}\_{1,i}^{\prime}&\ldots&{\mbox{$\lambda$}}\_{p\_{0},i}^{\prime}\end{bmatrix}^{\prime}\in\mathbb{R}^{d\_{i}}. |  |

Then the ii-th equation ([25](https://arxiv.org/html/2601.21272v1#S2.E25 "In 2.5 Asymptotic properties of generalized Durbin estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) can be written compactly as

|  |  |  |  |
| --- | --- | --- | --- |
|  | yi,t=ğ’˜i,tâ€²â€‹ğœ½i+Îµu,i,t.y\_{i,t}={\mbox{$w$}}\_{i,t}^{\prime}{\mbox{$\theta$}}\_{i}+\varepsilon\_{u,i,t}. |  | (26) |

###### Lemma 1 (Equation-by-equation properties of the Durbin regression):

Suppose AssumptionsÂ [1](https://arxiv.org/html/2601.21272v1#Thmassumption1 "Assumption 1 (Finite-predictor exactness at lag ğ‘â‚€): â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")â€“[2](https://arxiv.org/html/2601.21272v1#Thmassumption2 "Assumption 2: â€£ 2.3 The inconsistency of OLS and OLS-based GLS estimators in multi-equation models â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") hold, and let
yi,t=ğ°i,tâ€²â€‹ğ›‰i+Îµu,i,ty\_{i,t}={\mbox{$w$}}\_{i,t}^{\prime}{\mbox{$\theta$}}\_{i}+\varepsilon\_{u,i,t} be defined as above.
Then, for each i=1,â€¦,Ni=1,\ldots,N, the following statements hold.

(i)
:   There exist an â„±tâˆ’1:=Ïƒ(ğ’›s:sâ‰¤tâˆ’1)\mathscr{F}\_{t-1}:=\sigma({\mbox{$z$}}\_{s}:s\leq t-1)-measurable vector ğ’‰i,tâˆˆâ„di{\mbox{$h$}}\_{i,t}\in\mathbb{R}^{d\_{i}} and a deterministic linear map ğ’i:â„kâ†’â„di{\mbox{$m$}}\_{i}:\mathbb{R}^{k}\to\mathbb{R}^{d\_{i}} such that

    |  |  |  |
    | --- | --- | --- |
    |  | ğ’˜i,t=ğ’‰i,t+ğ’iâ€‹ğœºx,t.{\mbox{$w$}}\_{i,t}={\mbox{$h$}}\_{i,t}+{\mbox{$m$}}\_{i}\,{\mbox{$\varepsilon$}}\_{x,t}. |  |

    Hence ğ’˜i,t{\mbox{$w$}}\_{i,t} is measurable with respect to Ïƒâ€‹(â„±tâˆ’1,ğœºx,t)\sigma(\mathscr{F}\_{t-1},{\mbox{$\varepsilon$}}\_{x,t}). Moreover, if the innovation orthogonality condition

    |  |  |  |
    | --- | --- | --- |
    |  | ğšºxâ€‹u:=ğ”¼â€‹[ğœºx,tâ€‹ğœºu,tâ€²]=ğŸ{\mbox{$\Sigma$}}\_{xu}:={\mathbb{E}}[{\mbox{$\varepsilon$}}\_{x,t}{\mbox{$\varepsilon$}}\_{u,t}^{\prime}]={\mbox{$0$}} |  |

    holds (as it does under Bâ€‹DBD, Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG, and Eâ€‹Bâ€‹DEBD), then

    |  |  |  |
    | --- | --- | --- |
    |  | ğ”¼â€‹[ğ’˜i,tâ€‹Îµu,i,t]=ğŸ.{\mathbb{E}}[{\mbox{$w$}}\_{i,t}\,\varepsilon\_{u,i,t}]={\mbox{$0$}}. |  |

(ii)
:   ğ”¼â€‹â€–ğ’˜i,tâ€–2+Î´<âˆ{\mathbb{E}}\|{\mbox{$w$}}\_{i,t}\|^{2+\delta}<\infty and ğ”¼â€‹â€–ğ’˜i,tâ€‹Îµu,i,tâ€–2+Î´<âˆ{\mathbb{E}}\|{\mbox{$w$}}\_{i,t}\varepsilon\_{u,i,t}\|^{2+\delta}<\infty.

(iii)
:   The second-moment matrices ğ‘¸w,i:=ğ”¼â€‹[ğ’˜i,tâ€‹ğ’˜i,tâ€²]{\mbox{$Q$}}\_{w,i}:={\mathbb{E}}[{\mbox{$w$}}\_{i,t}{\mbox{$w$}}\_{i,t}^{\prime}] is positive definite.

###### Proof.

See the Appendix.
âˆ

LemmaÂ [1](https://arxiv.org/html/2601.21272v1#Thmlemma1 "Lemma 1 (Equation-by-equation properties of the Durbin regression): â€£ 2.5 Asymptotic properties of generalized Durbin estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") summarizes the key properties of the augmented regressor vector ğ’˜i,t{\mbox{$w$}}\_{i,t} that are used to analyze the generalized Durbin estimator. PartÂ (i) shows that, under AssumptionÂ [1](https://arxiv.org/html/2601.21272v1#Thmassumption1 "Assumption 1 (Finite-predictor exactness at lag ğ‘â‚€): â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") and the innovation orthogonality condition ğšºxâ€‹u=ğ”¼â€‹[ğœºx,tâ€‹ğœºu,tâ€²]=ğŸ{\mbox{$\Sigma$}}\_{xu}={\mathbb{E}}[{\mbox{$\varepsilon$}}\_{x,t}{\mbox{$\varepsilon$}}\_{u,t}^{\prime}]={\mbox{$0$}} (satisfied by all Bâ€‹DBD/Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG/Eâ€‹Bâ€‹DEBD conditions), the population orthogonality condition ğ”¼â€‹[ğ’˜i,tâ€‹Îµu,i,t]=0{\mathbb{E}}[{\mbox{$w$}}\_{i,t}\varepsilon\_{u,i,t}]=0 holds. Therefore, although {ğ’™t}\{{\mbox{$x$}}\_{t}\} and {ğ’–t}\{{\mbox{$u$}}\_{t}\} may exhibit rich dynamic feedback through the VAR coefficients {ğš¿j}\{{\mbox{$\Psi$}}\_{j}\}, the population normal equations for the first-step regression yi,t=ğ’˜i,tâ€²â€‹ğœ½i+Îµu,i,ty\_{i,t}={\mbox{$w$}}\_{i,t}^{\prime}{\mbox{$\theta$}}\_{i}+\varepsilon\_{u,i,t} remain correctly specified. PartsÂ (ii) and (iii) provide the moment and nonsingularity conditions (together with the weak-dependence regularity implied by AssumptionÂ [1](https://arxiv.org/html/2601.21272v1#Thmassumption1 "Assumption 1 (Finite-predictor exactness at lag ğ‘â‚€): â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) that ensure a law of large numbers and a central limit theorem apply to the sample moments involving ğ’˜i,t{\mbox{$w$}}\_{i,t}. These results will be used to establish consistency and derive the asymptotic distribution of the proposed equation-by-equation estimator.

Before deriving the asymptotic results, we first show that the Bayesian information criterion (BIC) of Schwarz ([1978](https://arxiv.org/html/2601.21272v1#bib.bib1913 "Estimating the dimension of a model")) consistently selects the true lag order. Consequently, we may replace the unknown p0p\_{0} with the data-driven choice p^BIC\widehat{p}\_{\mathrm{BIC}} without affecting the limiting distribution of the estimators.

###### Lemma 2:

Suppose AssumptionÂ [1](https://arxiv.org/html/2601.21272v1#Thmassumption1 "Assumption 1 (Finite-predictor exactness at lag ğ‘â‚€): â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")â€“[2](https://arxiv.org/html/2601.21272v1#Thmassumption2 "Assumption 2: â€£ 2.3 The inconsistency of OLS and OLS-based GLS estimators in multi-equation models â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") and the Eâ€‹Bâ€‹DEBD condition hold. Let the candidate set be ğ’«={1,â€¦,pmax}\mathcal{P}=\{1,\dots,p\_{\max}\} with fixed pmaxâ‰¥p0p\_{\max}\geq p\_{0}. For each pâˆˆğ’«p\in\mathcal{P}, estimate ([26](https://arxiv.org/html/2601.21272v1#S2.E26 "In 2.5 Asymptotic properties of generalized Durbin estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) equation by equation with pp lags and obtain residuals {Îµ^u,i,tâ€‹(p)}\{\widehat{\varepsilon}\_{u,i,t}(p)\}. Stack them as ğ›†^u,tâ€‹(p):=(Îµ^u,1,tâ€‹(p),â€¦,Îµ^u,N,tâ€‹(p))â€²\widehat{{\mbox{$\varepsilon$}}}\_{u,t}(p):=(\widehat{\varepsilon}\_{u,1,t}(p),\ldots,\widehat{\varepsilon}\_{u,N,t}(p))^{\prime}, and define

|  |  |  |
| --- | --- | --- |
|  | ğšº^uâ€‹uâ€‹(p):=1Tâˆ’pâ€‹âˆ‘t=p+1Tğœº^u,tâ€‹(p)â€‹ğœº^u,tâ€‹(p)â€².\widehat{{\mbox{$\Sigma$}}}\_{uu}(p):=\frac{1}{T-p}\sum\_{t=p+1}^{T}\widehat{{\mbox{$\varepsilon$}}}\_{u,t}(p)\widehat{{\mbox{$\varepsilon$}}}\_{u,t}(p)^{\prime}. |  |

Define

|  |  |  |
| --- | --- | --- |
|  | BICTâ¡(p):=logâ€‹detğšº^uâ€‹uâ€‹(p)+Îºâ€‹(p)â€‹logâ¡TT,Îºâ€‹(p):=N+k+pâ€‹N2+pâ€‹Nâ€‹k,\operatorname{BIC}\_{T}(p):=\log\det\widehat{{\mbox{$\Sigma$}}}\_{uu}(p)+\frac{\kappa(p)\log T}{T},\qquad\kappa(p):=N+k+p\,N^{2}+p\,Nk, |  |

where Îºâ€‹(p)\kappa(p) is the total number of regression coefficients in the system of ([24](https://arxiv.org/html/2601.21272v1#S2.E24 "In 2.5 Asymptotic properties of generalized Durbin estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"))
(the additional Nâ€‹(N+1)/2N(N+1)/2 covariance parameters do not depend on pp and hence are omitted).
Then

|  |  |  |
| --- | --- | --- |
|  | p^BIC:=minâ¡argâ¡minpâˆˆğ’«â¡BICTâ¡(p)â†’ğ‘p0.\widehat{p}\_{\mathrm{BIC}}:=\min\arg\min\_{p\in\mathcal{P}}\operatorname{BIC}\_{T}(p)\ \xrightarrow{p}\ p\_{0}. |  |

###### Proof.

See the Appendix.
âˆ

LemmaÂ [2](https://arxiv.org/html/2601.21272v1#Thmlemma2 "Lemma 2: â€£ 2.5 Asymptotic properties of generalized Durbin estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") shows that the BIC consistently selects the true lag order p0p\_{0} from a fixed candidate set. Therefore, in the subsequent asymptotic analysis we may treat p0p\_{0} as known without loss of generality. Given a lag order p0p\_{0}, consider the augmented regression implied by ([24](https://arxiv.org/html/2601.21272v1#S2.E24 "In 2.5 Asymptotic properties of generalized Durbin estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) under the linear reparameterization, and estimate it equation by equation. The next lemma establishes consistency and asymptotic normality of this first-step (generalized Durbin) estimator.

###### Lemma 3:

Fix p0p\_{0} and consider the equation-by-equation augmented regression ([25](https://arxiv.org/html/2601.21272v1#S2.E25 "In 2.5 Asymptotic properties of generalized Durbin estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) for each i=1,â€¦,Ni=1,\ldots,N on the effective sample t=p0+1,â€¦,Tt=p\_{0}+1,\ldots,T, with Teff:=Tâˆ’p0T\_{\mathrm{eff}}:=T-p\_{0}. Suppose AssumptionÂ [1](https://arxiv.org/html/2601.21272v1#Thmassumption1 "Assumption 1 (Finite-predictor exactness at lag ğ‘â‚€): â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")â€“[2](https://arxiv.org/html/2601.21272v1#Thmassumption2 "Assumption 2: â€£ 2.3 The inconsistency of OLS and OLS-based GLS estimators in multi-equation models â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") hold and one of the exogeneity conditions Bâ€‹DBD, Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG, or Eâ€‹Bâ€‹DEBD holds. Define the equation-by-equation Durbin estimator

|  |  |  |
| --- | --- | --- |
|  | ğœ½^iD:=(1Teffâ€‹âˆ‘t=p0+1Tğ’˜i,tâ€‹ğ’˜i,tâ€²)âˆ’1â€‹(1Teffâ€‹âˆ‘t=p0+1Tğ’˜i,tâ€‹yi,t).\widehat{{\mbox{$\theta$}}}\_{i}^{\mathrm{D}}:=\Bigl(\frac{1}{T\_{\mathrm{eff}}}\sum\_{t=p\_{0}+1}^{T}{\mbox{$w$}}\_{i,t}{\mbox{$w$}}\_{i,t}^{\prime}\Bigr)^{-1}\Bigl(\frac{1}{T\_{\mathrm{eff}}}\sum\_{t=p\_{0}+1}^{T}{\mbox{$w$}}\_{i,t}y\_{i,t}\Bigr). |  |

Then, for each i=1,â€¦,Ni=1,\ldots,N,

|  |  |  |
| --- | --- | --- |
|  | ğœ½^iDâ†’ğ‘ğœ½i.\widehat{{\mbox{$\theta$}}}\_{i}^{\mathrm{D}}\xrightarrow{p}{\mbox{$\theta$}}\_{i}. |  |

###### Proof.

See the Appendix.
âˆ

LemmaÂ [3](https://arxiv.org/html/2601.21272v1#Thmlemma3 "Lemma 3: â€£ 2.5 Asymptotic properties of generalized Durbin estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") establishes consistency of the Durbin estimators from the augmented regressions. However, these estimators do not exploit contemporaneous cross-equation dependence in ğœºu,t{\mbox{$\varepsilon$}}\_{u,t} and are therefore generally inefficient. Moreover, the augmented regression delivers ğœ¸\gamma rather than the original intercept vector ğœ¶\alpha. Indeed, from ([23](https://arxiv.org/html/2601.21272v1#S2.E23 "In 2.5 Asymptotic properties of generalized Durbin estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"))â€“([24](https://arxiv.org/html/2601.21272v1#S2.E24 "In 2.5 Asymptotic properties of generalized Durbin estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")),

|  |  |  |
| --- | --- | --- |
|  | ğœ¸=(ğ‘°Nâˆ’âˆ‘j=1p0ğš¿uâ€‹u,j)â€‹ğœ¶âˆ’âˆ‘j=1p0ğš¿uâ€‹x,jâ€‹ğx,{\mbox{$\gamma$}}=\Bigl({\mbox{$I$}}\_{N}-\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{uu,j}\Bigr){\mbox{$\alpha$}}-\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{ux,j}{\mbox{$\mu$}}\_{x}, |  |

so recovering ğœ¶\alpha requires consistent estimates of {ğš¿uâ€‹u,j}\{{\mbox{$\Psi$}}\_{uu,j}\}, {ğš¿uâ€‹x,j}\{{\mbox{$\Psi$}}\_{ux,j}\}, and ğx{\mbox{$\mu$}}\_{x}.

To obtain an asymptotically efficient estimator and recover ğœ¶\alpha, we now construct a feasible GLS transformation. Using the population identity ğš²j=ğš¿uâ€‹x,jâˆ’ğš¿uâ€‹u,jâ€‹ğ‘©{\mbox{$\Lambda$}}\_{j}={\mbox{$\Psi$}}\_{ux,j}-{\mbox{$\Psi$}}\_{uu,j}{\mbox{$B$}}, we define the plug-in estimator

|  |  |  |
| --- | --- | --- |
|  | ğš¿^uâ€‹x,j:=ğš²^j+ğš¿^uâ€‹u,jâ€‹ğ‘©^,j=1,â€¦,p0.\widehat{{\mbox{$\Psi$}}}\_{ux,j}:=\widehat{{\mbox{$\Lambda$}}}\_{j}+\widehat{{\mbox{$\Psi$}}}\_{uu,j}\widehat{{\mbox{$B$}}},\qquad j=1,\ldots,p\_{0}. |  |

Since the first-step estimators are consistent, we have, for each fixed jj (and hence uniformly over j=1,â€¦,p0j=1,\ldots,p\_{0}), ğš¿^uâ€‹u,j=ğš¿uâ€‹u,j+opâ€‹(1)\widehat{{\mbox{$\Psi$}}}\_{uu,j}={\mbox{$\Psi$}}\_{uu,j}+o\_{p}(1), ğš¿^uâ€‹x,j=ğš¿uâ€‹x,j+opâ€‹(1)\widehat{{\mbox{$\Psi$}}}\_{ux,j}={\mbox{$\Psi$}}\_{ux,j}+o\_{p}(1), and ğ^xâ†’ğ‘ğx\widehat{{\mbox{$\mu$}}}\_{x}\xrightarrow{p}{\mbox{$\mu$}}\_{x}. Consequently, after replacing the unknown nuisance parameters in the population transformation by their Durbin estimates, the resulting feasible transformed regression is equal to its population counterpart up to a remainder term that is asymptotically negligible (in the sense specified below). Hence, up to opâ€‹(1)o\_{p}(1) perturbations, the following regression holds:

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ’šGD,t=ğ’GD,tâ€²â€‹ğœ¿+ğœºu,t,{\mbox{$y$}}\_{\mathrm{GD},t}={\mbox{$Z$}}\_{\mathrm{GD},t}^{\prime}{\mbox{$\kappa$}}+{\mbox{$\varepsilon$}}\_{u,t}, |  | (27) |

where ğ’šGD,t=ğ’štâˆ’âˆ‘j=1p0ğš¿^uâ€‹u,jâ€‹ğ’štâˆ’jâˆ’âˆ‘j=1p0ğš¿^uâ€‹x,jâ€‹(ğ’™tâˆ’jâˆ’ğ^x){\mbox{$y$}}\_{\mathrm{GD},t}={\mbox{$y$}}\_{t}-\sum\_{j=1}^{p\_{0}}\widehat{{\mbox{$\Psi$}}}\_{uu,j}{\mbox{$y$}}\_{t-j}-\sum\_{j=1}^{p\_{0}}\widehat{{\mbox{$\Psi$}}}\_{ux,j}\big({\mbox{$x$}}\_{t-j}-\widehat{{\mbox{$\mu$}}}\_{x}\big) and ğ’GD,tâ€²=[(ğ‘°Nâˆ’âˆ‘j=1p0ğš¿^uâ€‹u,j),ğ‘¿tâ€²âˆ’âˆ‘j=1p0ğš¿^uâ€‹u,jâ€‹ğ‘¿tâˆ’jâ€²]{\mbox{$Z$}}\_{\mathrm{GD},t}^{\prime}=[({\mbox{$I$}}\_{N}-\sum\_{j=1}^{p\_{0}}\widehat{{\mbox{$\Psi$}}}\_{uu,j}),{\mbox{$X$}}\_{t}^{\prime}-\sum\_{j=1}^{p\_{0}}\widehat{{\mbox{$\Psi$}}}\_{uu,j}{\mbox{$X$}}\_{t-j}^{\prime}], and ğ^x=1/Teâ€‹fâ€‹fâ€‹âˆ‘t=p0+1Tğ’™t\widehat{{\mbox{$\mu$}}}\_{x}=1/T\_{eff}\sum\_{t=p\_{0}+1}^{T}{\mbox{$x$}}\_{t}, which is consistent by the ergodic theorem.

###### Assumption 5:

(A5.1)
:   ğ‘¸GD:=ğ”¼â€‹[ğ’GD,tâ€‹ğšºuâ€‹uâˆ’1â€‹ğ’GD,tâ€²]{\mbox{$Q$}}\_{\mathrm{GD}}:={\mathbb{E}}[{\mbox{$Z$}}\_{\mathrm{GD},t}{\mbox{$\Sigma$}}\_{uu}^{-1}{\mbox{$Z$}}\_{\mathrm{GD},t}^{\prime}] is positive definite.

(A5.2)
:   ğ”¼â€‹[ğœºu,tâ€‹ğœºu,tâ€²âˆ£Ïƒâ€‹(ğ’GD,t)]=ğšºuâ€‹u{\mathbb{E}}[{\mbox{$\varepsilon$}}\_{u,t}{\mbox{$\varepsilon$}}\_{u,t}^{\prime}\mid\sigma({\mbox{$Z$}}\_{\mathrm{GD},t})]={\mbox{$\Sigma$}}\_{uu}, where ğšºuâ€‹u{\mbox{$\Sigma$}}\_{uu} is positive definite.

###### Theorem 1:

Let ğ²GD,t=ğ™GD,tâ€²â€‹ğ›‹+ğ›†u,t{\mbox{$y$}}\_{\mathrm{GD},t}={\mbox{$Z$}}\_{\mathrm{GD},t}^{\prime}{\mbox{$\kappa$}}+{\mbox{$\varepsilon$}}\_{u,t} as in ([27](https://arxiv.org/html/2601.21272v1#S2.E27 "In 2.5 Asymptotic properties of generalized Durbin estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")), with ğ™GD,tâˆˆâ„mÃ—N{\mbox{$Z$}}\_{\mathrm{GD},t}\in\mathbb{R}^{m\times N} and ğ›‹âˆˆâ„m{\mbox{$\kappa$}}\in\mathbb{R}^{m}. Suppose AssumptionÂ [1](https://arxiv.org/html/2601.21272v1#Thmassumption1 "Assumption 1 (Finite-predictor exactness at lag ğ‘â‚€): â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")-[2](https://arxiv.org/html/2601.21272v1#Thmassumption2 "Assumption 2: â€£ 2.3 The inconsistency of OLS and OLS-based GLS estimators in multi-equation models â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"), [5](https://arxiv.org/html/2601.21272v1#Thmassumption5 "Assumption 5: â€£ 2.5 Asymptotic properties of generalized Durbin estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") and one of the exogeneity conditions BD, GEXOG, or EBD holds. Let the effective sample be t=p0+1,â€¦,Tt=p\_{0}+1,\ldots,T with Teâ€‹fâ€‹f:=Tâˆ’p0T\_{eff}:=T-p\_{0}. Define

|  |  |  |
| --- | --- | --- |
|  | ğšº^uâ€‹u=1Teâ€‹fâ€‹fâ€‹âˆ‘t=p0+1Tğœº^u,tâ€‹ğœº^u,tâ€²,ğœº^u,t:=ğ’štâˆ’ğ‘¾tâ€²â€‹ğœ½^Dâˆ’OLS.\widehat{{\mbox{$\Sigma$}}}\_{uu}=\frac{1}{T\_{eff}}\sum\_{t=p\_{0}+1}^{T}\widehat{{\mbox{$\varepsilon$}}}\_{u,t}\widehat{{\mbox{$\varepsilon$}}}\_{u,t}^{\prime},\qquad\widehat{{\mbox{$\varepsilon$}}}\_{u,t}:={\mbox{$y$}}\_{t}-{\mbox{$W$}}\_{t}^{\prime}\widehat{{\mbox{$\theta$}}}^{\mathrm{D\!-\!OLS}}. |  |

Then ğšº^uâ€‹uâ†’ğ‘ğšºuâ€‹u>0\widehat{{\mbox{$\Sigma$}}}\_{uu}\xrightarrow{p}{\mbox{$\Sigma$}}\_{uu}>0 and is invertible with probability approaching one. The feasible GLS estimator

|  |  |  |
| --- | --- | --- |
|  | ğœ¿^GD=(1Teâ€‹fâ€‹fâ€‹âˆ‘t=p0+1Tğ’GD,tâ€‹ğšº^uâ€‹uâˆ’1â€‹ğ’GD,tâ€²)âˆ’1â€‹(1Teâ€‹fâ€‹fâ€‹âˆ‘t=p0+1Tğ’GD,tâ€‹ğšº^uâ€‹uâˆ’1â€‹ğ’šGD,t)\widehat{{\mbox{$\kappa$}}}^{\mathrm{GD}}=\Big(\frac{1}{T\_{eff}}\sum\_{t=p\_{0}+1}^{T}{\mbox{$Z$}}\_{\mathrm{GD},t}\,\widehat{{\mbox{$\Sigma$}}}\_{uu}^{-1}\,{\mbox{$Z$}}\_{\mathrm{GD},t}^{\prime}\Big)^{-1}\Big(\frac{1}{T\_{eff}}\sum\_{t=p\_{0}+1}^{T}{\mbox{$Z$}}\_{\mathrm{GD},t}\,\widehat{{\mbox{$\Sigma$}}}\_{uu}^{-1}\,{\mbox{$y$}}\_{\mathrm{GD},t}\Big) |  |

satisfies ğ›‹^GDâ†’ğ‘ğ›‹\widehat{{\mbox{$\kappa$}}}^{\mathrm{GD}}\xrightarrow{p}{\mbox{$\kappa$}} and

|  |  |  |
| --- | --- | --- |
|  | Teâ€‹fâ€‹fâ€‹(ğœ¿^GDâˆ’ğœ¿)â†’ğ‘‘ğ’©â€‹(ğŸ,ğ‘½),ğ‘½:=(ğ”¼â€‹[ğ’GD,tâ€‹ğšºuâ€‹uâˆ’1â€‹ğ’GD,tâ€²])âˆ’1.\sqrt{T\_{eff}}\big(\widehat{{\mbox{$\kappa$}}}^{\mathrm{GD}}-{\mbox{$\kappa$}}\big)\ \xrightarrow{d}\ \mathcal{N}\big({\mbox{$0$}},\ {\mbox{$V$}}\big),\quad{\mbox{$V$}}:=\Big({\mathbb{E}}[{\mbox{$Z$}}\_{\mathrm{GD},t}\,{\mbox{$\Sigma$}}\_{uu}^{-1}\,{\mbox{$Z$}}\_{\mathrm{GD},t}^{\prime}]\Big)^{-1}. |  |

Moreover, ğ›‹^GD\widehat{{\mbox{$\kappa$}}}^{\mathrm{GD}} is asymptotically efficient under Eâ€‹Bâ€‹DEBD, in the sense that it is asymptotically equivalent to the infeasible GLS estimator that uses the true ğšºuâ€‹u{\mbox{$\Sigma$}}\_{uu}.

###### Proof.

See the Appendix.
âˆ

As a direct consequence of TheoremÂ [1](https://arxiv.org/html/2601.21272v1#Thmtheorem1 "Theorem 1: â€£ 2.5 Asymptotic properties of generalized Durbin estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"), we can conduct Wald-type tests for linear restrictions on ğœ¿\kappa under any of the exogeneity conditions Bâ€‹DBD, Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG, or Eâ€‹Bâ€‹DEBD. The next corollary states the Wald statistic and its limiting null distribution.

###### Corollary 1:

Let ğ²GD,t=ğ™GD,tâ€²â€‹ğ›‹+ğ›†u,t{\mbox{$y$}}\_{\mathrm{GD},t}={\mbox{$Z$}}\_{\mathrm{GD},t}^{\prime}{\mbox{$\kappa$}}+{\mbox{$\varepsilon$}}\_{u,t} be as in ([27](https://arxiv.org/html/2601.21272v1#S2.E27 "In 2.5 Asymptotic properties of generalized Durbin estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")). Suppose AssumptionsÂ [1](https://arxiv.org/html/2601.21272v1#Thmassumption1 "Assumption 1 (Finite-predictor exactness at lag ğ‘â‚€): â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")â€“[2](https://arxiv.org/html/2601.21272v1#Thmassumption2 "Assumption 2: â€£ 2.3 The inconsistency of OLS and OLS-based GLS estimators in multi-equation models â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"), [5](https://arxiv.org/html/2601.21272v1#Thmassumption5 "Assumption 5: â€£ 2.5 Asymptotic properties of generalized Durbin estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") hold and that one of the exogeneity conditions Bâ€‹DBD, Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG, or Eâ€‹Bâ€‹DEBD holds. Let ğ‘R be a given qÃ—mq\times m matrix with rankâ€‹(ğ‘)=qâ‰¤m\mathrm{rank}({\mbox{$R$}})=q\leq m, and let ğ«âˆˆâ„q{\mbox{$r$}}\in\mathbb{R}^{q}. Consider testing the null hypothesis H0:ğ‘ğ›‹=ğ«H\_{0}:{\mbox{$R$}}{\mbox{$\kappa$}}={\mbox{$r$}}. Then, under H0H\_{0}, the Wald statistic

|  |  |  |
| --- | --- | --- |
|  | ğ’²GD:=Teffâ€‹(ğ‘¹â€‹ğœ¿^GDâˆ’ğ’“)â€²â€‹[ğ‘¹â€‹ğ‘½^â€‹ğ‘¹â€²]âˆ’1â€‹(ğ‘¹â€‹ğœ¿^GDâˆ’ğ’“)â†’ğ‘‘Ï‡q2,\mathcal{W}^{\mathrm{GD}}:=T\_{\mathrm{eff}}({\mbox{$R$}}\widehat{{\mbox{$\kappa$}}}^{\mathrm{GD}}-{\mbox{$r$}})^{\prime}\big[{\mbox{$R$}}\widehat{{\mbox{$V$}}}{\mbox{$R$}}^{\prime}\big]^{-1}({\mbox{$R$}}\widehat{{\mbox{$\kappa$}}}^{\mathrm{GD}}-{\mbox{$r$}})\ \xrightarrow{d}\ \chi^{2}\_{q}, |  |

where

|  |  |  |
| --- | --- | --- |
|  | ğ‘½^:=(1Teffâ€‹âˆ‘t=p0+1Tğ’GD,tâ€‹ğšº^uâ€‹uâˆ’1â€‹ğ’GD,tâ€²)âˆ’1.\widehat{{\mbox{$V$}}}:=\Big(\frac{1}{T\_{\mathrm{eff}}}\sum\_{t=p\_{0}+1}^{T}{\mbox{$Z$}}\_{\mathrm{GD},t}\widehat{{\mbox{$\Sigma$}}}\_{uu}^{-1}{\mbox{$Z$}}\_{\mathrm{GD},t}^{\prime}\Big)^{-1}. |  |

###### Proof.

See the Appendix.
âˆ

### 2.6 Bootstrap-corrected Wald test based on the generalized Durbin estimator

TheoremÂ [1](https://arxiv.org/html/2601.21272v1#Thmtheorem1 "Theorem 1: â€£ 2.5 Asymptotic properties of generalized Durbin estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") establishes Teff\sqrt{T\_{\mathrm{eff}}}-asymptotic normality of the generalized Durbin estimator ğœ¿^GD\widehat{{\mbox{$\kappa$}}}^{\mathrm{GD}}, which motivates Wald-type inference. In practice, however, the associated Wald test can exhibit non-negligible size distortions in finite samples. One practical source of distortion is that the generalized Durbin regression is implemented using transformed variables ğ’šGD,t{\mbox{$y$}}\_{\mathrm{GD},t} and ğ’GD,t{\mbox{$Z$}}\_{\mathrm{GD},t} that are not directly observed. These quantities are generated by plugging in first-step estimates of nuisance parameters governing the joint second-order dynamics (e.g., {ğš¿uâ€‹u,j}j=1p0\{{\mbox{$\Psi$}}\_{uu,j}\}\_{j=1}^{p\_{0}}, {ğš¿uâ€‹x,j}j=1p0\{{\mbox{$\Psi$}}\_{ux,j}\}\_{j=1}^{p\_{0}}, and ğx{\mbox{$\mu$}}\_{x}), and the resulting generated-regressor effect can materially affect the finite-sample distribution of the Wald statistic.

To address these finite-sample distortions, we adopt a bootstrap-based approach. Bootstrap tests are known to deliver asymptotic refinements for a broad class of (asymptotically) pivotal statistics, often yielding more accurate rejection probabilities than purely asymptotic approximations (Davidson and MacKinnon [1999](https://arxiv.org/html/2601.21272v1#bib.bib2282 "The size distortion of bootstrap tests"); MacKinnon [2006](https://arxiv.org/html/2601.21272v1#bib.bib2281 "Bootstrap methods in econometrics")). In our setting, because the relevant objects inherit serial dependence through the estimated joint dynamics, we employ the sieve bootstrap of BÃ¼hlmann ([1997](https://arxiv.org/html/2601.21272v1#bib.bib2283 "Sieve bootstrap for time series")), which approximates the dependence structure by a finite-order VAR(pp) and resamples the estimated innovations to generate pseudo-samples.

Even when inference relies on a single bootstrap pp value, the resulting test may still exhibit finite-sample level (size) distortions. A natural device to further improve accuracy is to calibrate the bootstrap pp value itself by an additional layer of resampling. This idea goes back to Beran ([1988](https://arxiv.org/html/2601.21272v1#bib.bib2284 "Prepivoting test statistics: a bootstrap view of asymptotic refinements")) and forms the basis of the iterated (double) bootstrap, which seeks higher-order improvements in level accuracy by applying a bootstrap-based transformation to an approximately pivotal quantity.

A practical drawback of the full double bootstrap is its computational burden, because it requires a nested resampling scheme with B1B\_{1} outer and B2B\_{2} inner bootstrap replications, leading to a cost proportional to B1Ã—B2B\_{1}\times B\_{2} (e.g., Booth and Hall [1994](https://arxiv.org/html/2601.21272v1#bib.bib2285 "Monte carlo approximation and the iterated boostrap")). To retain much of the accuracy gain at a fraction of the cost, we therefore employ the fast double bootstrap (FDB) of Davidson and MacKinnon ([2007](https://arxiv.org/html/2601.21272v1#bib.bib2286 "Improving the reliability of bootstrap tests with the fast double bootstrap")). The FDB is closely related to the double bootstrap but far less computationally demanding, and it typically requires only about twice the computational effort of a single bootstrap pp value.

We now describe how the FDB is implemented for the Wald test based on the generalized Durbin estimator. The procedure combines a VAR-sieve outer bootstrap under H0H\_{0} with a single inner bootstrap draw per outer replication (i.e., B2=1B\_{2}=1), while re-estimating the nuisance components and reconstructing the GD transforms in each resample.

##### Fast double bootstrap (FDB) algorithm for the GDâ€“Wald test.

Let H0:ğ‘¹ğœ¿=ğ’“H\_{0}:{\mbox{$R$}}{\mbox{$\kappa$}}={\mbox{$r$}} be the null restriction and let ğ’²Gâ€‹D\mathcal{W}^{GD} denote the observed Wald statistic computed from the original sample using the generalized Durbin (GD) estimator.

1. 1.

   Original-sample estimation.
   Compute the unrestricted GD estimator ğœ¿^GD\widehat{{\mbox{$\kappa$}}}^{\mathrm{GD}} and the plug-in covariance matrix ğ‘½^\widehat{{\mbox{$V$}}}, and form the observed Wald statistic ğ’²Gâ€‹D\mathcal{W}^{GD}. Construct the restricted GD estimator under H0H\_{0} by

   |  |  |  |
   | --- | --- | --- |
   |  | ğœ¿~GD=ğœ¿^GDâˆ’ğ‘½^â€‹ğ‘¹â€²â€‹(ğ‘¹â€‹ğ‘½^â€‹ğ‘¹â€²)âˆ’1â€‹(ğ‘¹â€‹ğœ¿^GDâˆ’ğ’“).\widetilde{{\mbox{$\kappa$}}}^{\mathrm{GD}}=\widehat{{\mbox{$\kappa$}}}^{\mathrm{GD}}-\widehat{{\mbox{$V$}}}\,{\mbox{$R$}}^{\prime}\bigl({\mbox{$R$}}\widehat{{\mbox{$V$}}}{\mbox{$R$}}^{\prime}\bigr)^{-1}\bigl({\mbox{$R$}}\widehat{{\mbox{$\kappa$}}}^{\mathrm{GD}}-{\mbox{$r$}}\bigr). |  |

   The restricted estimator ğœ¿~GD\widetilde{{\mbox{$\kappa$}}}^{\mathrm{GD}} is used only to generate bootstrap samples under H0H\_{0}.
2. 2.

   Outer bootstrap (b=1,â€¦,B1b=1,\dots,B\_{1}): VAR-sieve under H0H\_{0}.
   For each bb, generate a pseudo-sample using a VAR-sieve bootstrap under H0H\_{0} (based on ğœ¿~GD\widetilde{{\mbox{$\kappa$}}}^{\mathrm{GD}}), re-estimate the nuisance parameters, reconstruct {ğ’šGD,tâˆ—(b),ğ’GD,tâˆ—(b)}\{{\mbox{$y$}}\_{\mathrm{GD},t}^{\*(b)},{\mbox{$Z$}}\_{\mathrm{GD},t}^{\*(b)}\}, and compute the outer Wald statistic Wbâˆ—W\_{b}^{\ast} using the unrestricted GD estimator on the pseudo-sample.
3. 3.

   Inner fast step (one draw per outer replication).
   For each bb, construct a restricted generator under H0H\_{0} based on the bb-th outer pseudo-sample, generate one inner pseudo-sample, and compute Wbâˆ—âˆ—W\_{b}^{\ast\ast} using the unrestricted GD estimator.
4. 4.

   Single-bootstrap pp value.
   Compute

   |  |  |  |
   | --- | --- | --- |
   |  | p^âˆ—=1B1âˆ‘b=1B1ğŸ{Wbâˆ—â‰¥ğ’²Gâ€‹D}.\widehat{p}^{\ast}=\frac{1}{B\_{1}}\sum\_{b=1}^{B\_{1}}\mathbf{1}\!\mathopen{}\left\{W\_{b}^{\ast}\geq\mathcal{W}^{GD}\mathclose{}\right\}. |  |
5. 5.

   FDB calibration and pp value.
   Let q1âˆ’p^âˆ—âˆ—âˆ—q^{\ast\ast}\_{1-\widehat{p}^{\ast}} denote the empirical (1âˆ’p^âˆ—)(1-\widehat{p}^{\ast}) quantile of {Wbâˆ—âˆ—}b=1B1\{W\_{b}^{\ast\ast}\}\_{b=1}^{B\_{1}}.
   Define the FDB-corrected pp value by

   |  |  |  |
   | --- | --- | --- |
   |  | p^FDB=1B1âˆ‘b=1B1ğŸ{Wbâˆ—â‰¥q1âˆ’p^âˆ—âˆ—âˆ—},\widehat{p}\_{\mathrm{FDB}}=\frac{1}{B\_{1}}\sum\_{b=1}^{B\_{1}}\mathbf{1}\!\mathopen{}\left\{W\_{b}^{\ast}\geq q^{\ast\ast}\_{1-\widehat{p}^{\ast}}\mathclose{}\right\}, |  |

   and reject H0H\_{0} at level Î±\alpha if p^FDB<Î±\widehat{p}\_{\mathrm{FDB}}<\alpha.

## 3 Simulation Experiment

In this section, we examine the finite-sample properties of the multivariate estimators and related test statistics under the Bâ€‹DBD, Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG, and Eâ€‹Bâ€‹DEBD conditions through Monte Carlo experiments. We are interested in using our proposed estimators to test the validity of asset pricing models. To this end, we consider a multiple-equation regression model in which the regressors are common across all equations.

### 3.1 Simulation Design

In asset pricing models, it is typically assumed that each portfolio ii is exposed to the same set of factors. Classic examples include the multifactor models of [Fama and French](https://arxiv.org/html/2601.21272v1#bib.bib1868 "Common risk factors in the returns on stocks and bonds")â€™s ([1993](https://arxiv.org/html/2601.21272v1#bib.bib1868 "Common risk factors in the returns on stocks and bonds"); [2015](https://arxiv.org/html/2601.21272v1#bib.bib1869 "A five-factor asset pricing model")) and the arbitrage pricing theory of [Ross](https://arxiv.org/html/2601.21272v1#bib.bib1912 "The arbitrage theory of capital asset pricing")â€™s ([1976](https://arxiv.org/html/2601.21272v1#bib.bib1912 "The arbitrage theory of capital asset pricing")). In the simulation experiments, we consider a multiple-equation regression model with regressors common across all equations:

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ’št=ğœ¶+(ğ‘°NâŠ—ğ’™tâ€²)â€‹ğœ·+ğ’–t=ğ’tâ€²â€‹ğœ¿+ğ’–t,t=1,â€¦,T,{\mbox{$y$}}\_{t}={\mbox{$\alpha$}}+\big({\mbox{$I$}}\_{N}\otimes{\mbox{$x$}}\_{t}^{\prime}\big){\mbox{$\beta$}}+{\mbox{$u$}}\_{t}={\mbox{$Z$}}\_{t}^{\prime}{\mbox{$\kappa$}}+{\mbox{$u$}}\_{t},\quad t=1,\ldots,T, |  | (28) |

where ğ’št=(y1,t,â€¦,yN,t)â€²âˆˆâ„N{\mbox{$y$}}\_{t}=(y\_{1,t},\ldots,y\_{N,t})^{\prime}\in\mathbb{R}^{N}, ğ’™t{\mbox{$x$}}\_{t} is a kÃ—1k\times 1 vector of common factors (k=kik=k\_{i} for all ii), and ğœ·=(ğœ·1â€²,â€¦,ğœ·Nâ€²)â€²âˆˆâ„kâ€‹N{\mbox{$\beta$}}=({\mbox{$\beta$}}\_{1}^{\prime},\ldots,{\mbox{$\beta$}}\_{N}^{\prime})^{\prime}\in\mathbb{R}^{kN}. Define ğ‘¿tâ€²=(ğ‘°NâŠ—ğ’™tâ€²)âˆˆâ„NÃ—kâ€‹N{\mbox{$X$}}\_{t}^{\prime}=({\mbox{$I$}}\_{N}\otimes{\mbox{$x$}}\_{t}^{\prime})\in\mathbb{R}^{N\times kN} and ğ’tâ€²:=[ğ‘°N,ğ‘¿tâ€²]âˆˆâ„NÃ—(k+1)â€‹N{\mbox{$Z$}}\_{t}^{\prime}:=[{\mbox{$I$}}\_{N},{\mbox{$X$}}\_{t}^{\prime}]\in\mathbb{R}^{N\times(k+1)N}, so that ğœ¿=(ğœ¶â€²,ğœ·â€²)â€²âˆˆâ„(k+1)â€‹N{\mbox{$\kappa$}}=({\mbox{$\alpha$}}^{\prime},{\mbox{$\beta$}}^{\prime})^{\prime}\in\mathbb{R}^{(k+1)N} collects the intercepts and slope coefficients. Note that, as stated in RemarkÂ [2](https://arxiv.org/html/2601.21272v1#Thmremark2 "Remark 2 (Innovation-based exogeneity and its location): â€£ 2.2 Relationships among exogeneity conditions â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"), the specification Â ([28](https://arxiv.org/html/2601.21272v1#S3.E28 "In 3.1 Simulation Design â€£ 3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) is a special case of Â ([2](https://arxiv.org/html/2601.21272v1#S2.E2 "In 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")).

To generate ğ’›Â¯t=(ğ’™tâ€²âˆ’ğxâ€²,ğ’–tâ€²)â€²\bar{{\mbox{$z$}}}\_{t}=({\mbox{$x$}}\_{t}^{\prime}-{\mbox{$\mu$}}\_{x}^{\prime},{\mbox{$u$}}\_{t}^{\prime})^{\prime}, we consider the following three VAR(1) processes corresponding to the Bâ€‹DBD, Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG, and Eâ€‹Bâ€‹DEBD conditions. In all cases, we impose ğšºxâ€‹u=ğŸ{\mbox{$\Sigma$}}\_{xu}={\mbox{$0$}} so that the contemporaneous correlation between ğœºx,t{\mbox{$\varepsilon$}}\_{x,t} and ğœºu,t{\mbox{$\varepsilon$}}\_{u,t} is absent.

(1)
:   Block-diagonal vector autoregression (VAR) with ğšºxâ€‹u=ğŸ{\mbox{$\Sigma$}}\_{xu}={\mbox{$0$}} (Bâ€‹DBD)

    |  |  |  |  |
    | --- | --- | --- | --- |
    |  | ğ’št=ğœ¶+(ğ‘°NâŠ—ğ’™tâ€²)â€‹ğœ·+ğ’–t,[ğ’™tâˆ’ğxğ’–t]=[ğš¿xâ€‹x,1ğŸğŸğš¿uâ€‹u,1]â€‹[ğ’™tâˆ’1âˆ’ğxğ’–tâˆ’1]+[ğœºx,tğœºu,t],[ğœºx,tğœºu,t]âˆ¼ğ’©([ğŸğŸ],[ğšºxâ€‹xğŸğŸğšºuâ€‹u]).\begin{split}{\mbox{$y$}}\_{t}=&\;{\mbox{$\alpha$}}+({\mbox{$I$}}\_{N}\otimes{\mbox{$x$}}\_{t}^{\prime}){\mbox{$\beta$}}+{\mbox{$u$}}\_{t},\\ \begin{bmatrix}{\mbox{$x$}}\_{t}-{\mbox{$\mu$}}\_{x}\\ {\mbox{$u$}}\_{t}\\ \end{bmatrix}=&\;\begin{bmatrix}{\mbox{$\Psi$}}\_{xx,1}&{\mbox{$0$}}\\ {\mbox{$0$}}&{\mbox{$\Psi$}}\_{uu,1}\\ \end{bmatrix}\begin{bmatrix}{\mbox{$x$}}\_{t-1}-{\mbox{$\mu$}}\_{x}\\ {\mbox{$u$}}\_{t-1}\\ \end{bmatrix}+\begin{bmatrix}{\mbox{$\varepsilon$}}\_{x,t}\\ {\mbox{$\varepsilon$}}\_{u,t}\\ \end{bmatrix},\\ \begin{bmatrix}{\mbox{$\varepsilon$}}\_{x,t}\\ {\mbox{$\varepsilon$}}\_{u,t}\\ \end{bmatrix}\sim&\;\mathcal{N}\mathopen{}\left(\begin{bmatrix}{\mbox{$0$}}\\ {\mbox{$0$}}\\ \end{bmatrix},\begin{bmatrix}{\mbox{$\Sigma$}}\_{xx}&{\mbox{$0$}}\\ {\mbox{$0$}}&{\mbox{$\Sigma$}}\_{uu}\\ \end{bmatrix}\mathclose{}\right).\end{split} |  | (29) |

(2)
:   Triangular VAR with ğšºxâ€‹u=ğŸ{\mbox{$\Sigma$}}\_{xu}={\mbox{$0$}} (Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG)

    |  |  |  |  |
    | --- | --- | --- | --- |
    |  | ğ’št=ğœ¶+(ğ‘°NâŠ—ğ’™tâ€²)â€‹ğœ·+ğ’–t,[ğ’™tâˆ’ğxğ’–t]=[ğš¿xâ€‹x,1ğš¿xâ€‹u,1ğŸğš¿uâ€‹u,1]â€‹[ğ’™tâˆ’1âˆ’ğxğ’–tâˆ’1]+[ğœºx,tğœºu,t],[ğœºx,tğœºu,t]âˆ¼ğ’©([ğŸğŸ],[ğšºxâ€‹xğŸğŸğšºuâ€‹u]).\begin{split}{\mbox{$y$}}\_{t}=&\;{\mbox{$\alpha$}}+({\mbox{$I$}}\_{N}\otimes{\mbox{$x$}}\_{t}^{\prime}){\mbox{$\beta$}}+{\mbox{$u$}}\_{t},\\ \begin{bmatrix}{\mbox{$x$}}\_{t}-{\mbox{$\mu$}}\_{x}\\ {\mbox{$u$}}\_{t}\\ \end{bmatrix}=&\;\begin{bmatrix}{\mbox{$\Psi$}}\_{xx,1}&{\mbox{$\Psi$}}\_{xu,1}\\ {\mbox{$0$}}&{\mbox{$\Psi$}}\_{uu,1}\\ \end{bmatrix}\begin{bmatrix}{\mbox{$x$}}\_{t-1}-{\mbox{$\mu$}}\_{x}\\ {\mbox{$u$}}\_{t-1}\\ \end{bmatrix}+\begin{bmatrix}{\mbox{$\varepsilon$}}\_{x,t}\\ {\mbox{$\varepsilon$}}\_{u,t}\\ \end{bmatrix},\\ \begin{bmatrix}{\mbox{$\varepsilon$}}\_{x,t}\\ {\mbox{$\varepsilon$}}\_{u,t}\\ \end{bmatrix}\sim&\;\mathcal{N}\mathopen{}\left(\begin{bmatrix}{\mbox{$0$}}\\ {\mbox{$0$}}\\ \end{bmatrix},\begin{bmatrix}{\mbox{$\Sigma$}}\_{xx}&{\mbox{$0$}}\\ {\mbox{$0$}}&{\mbox{$\Sigma$}}\_{uu}\\ \end{bmatrix}\mathclose{}\right).\end{split} |  | (30) |

(3)
:   Unrestricted VAR with ğšºxâ€‹u=ğŸ{\mbox{$\Sigma$}}\_{xu}={\mbox{$0$}} (Eâ€‹Bâ€‹DEBD)

    |  |  |  |  |
    | --- | --- | --- | --- |
    |  | ğ’št=ğœ¶+(ğ‘°NâŠ—ğ’™tâ€²)â€‹ğœ·+ğ’–t,[ğ’™tâˆ’ğxğ’–t]=[ğš¿xâ€‹x,1ğš¿xâ€‹u,1ğš¿uâ€‹x,1ğš¿uâ€‹u,1]â€‹[ğ’™tâˆ’1âˆ’ğxğ’–tâˆ’1]+[ğœºx,tğœºu,t],[ğœºx,tğœºu,t]âˆ¼ğ’©([ğŸğŸ],[ğšºxâ€‹xğŸğŸğšºuâ€‹u]).\begin{split}{\mbox{$y$}}\_{t}=&\;{\mbox{$\alpha$}}+({\mbox{$I$}}\_{N}\otimes{\mbox{$x$}}\_{t}^{\prime}){\mbox{$\beta$}}+{\mbox{$u$}}\_{t},\\ \begin{bmatrix}{\mbox{$x$}}\_{t}-{\mbox{$\mu$}}\_{x}\\ {\mbox{$u$}}\_{t}\\ \end{bmatrix}=&\;\begin{bmatrix}{\mbox{$\Psi$}}\_{xx,1}&{\mbox{$\Psi$}}\_{xu,1}\\ {\mbox{$\Psi$}}\_{ux,1}&{\mbox{$\Psi$}}\_{uu,1}\\ \end{bmatrix}\begin{bmatrix}{\mbox{$x$}}\_{t-1}-{\mbox{$\mu$}}\_{x}\\ {\mbox{$u$}}\_{t-1}\\ \end{bmatrix}+\begin{bmatrix}{\mbox{$\varepsilon$}}\_{x,t}\\ {\mbox{$\varepsilon$}}\_{u,t}\\ \end{bmatrix},\\ \begin{bmatrix}{\mbox{$\varepsilon$}}\_{x,t}\\ {\mbox{$\varepsilon$}}\_{u,t}\\ \end{bmatrix}\sim&\;\mathcal{N}\mathopen{}\left(\begin{bmatrix}{\mbox{$0$}}\\ {\mbox{$0$}}\\ \end{bmatrix},\begin{bmatrix}{\mbox{$\Sigma$}}\_{xx}&{\mbox{$0$}}\\ {\mbox{$0$}}&{\mbox{$\Sigma$}}\_{uu}\\ \end{bmatrix}\mathclose{}\right).\end{split} |  | (31) |

In Baillie et al. ([2024](https://arxiv.org/html/2601.21272v1#bib.bib2090 "On robust inference in time-series regression")), lagged dependent variables are explicitly included in the estimated regression. In contrast, our setup does not require pre-specifying any finite lag structure for ğ’št{\mbox{$y$}}\_{t}. Since the Wold/VAR representation of ğ’›t=(ğ’™tâ€²âˆ’ğxâ€²,ğ’–tâ€²)â€²{\mbox{$z$}}\_{t}=({\mbox{$x$}}\_{t}^{\prime}-{\mbox{$\mu$}}\_{x}^{\prime},{\mbox{$u$}}\_{t}^{\prime})^{\prime} yields an equivalent Durbin-type representation, the dynamics of ğ’št{\mbox{$y$}}\_{t} are absorbed implicitly and need not be imposed a priori.

In all cases, we consider sample sizes Tâˆˆ{100,200,400,800}T\in\{100,200,400,800\}. To investigate how the number of factors and the number of assets (equations) affect the finite-sample accuracy of the estimators and the performance of the tests, we consider kâˆˆ{2,4}k\in\{2,4\} and Nâˆˆ{5,10}N\in\{5,10\}. In each replication, we simulate T+500T+500 observations and discard the first 500 observations as burn-in. For all DGPs, we perform 1,000 Monte Carlo replications.

For the VAR(1) coefficient matrix, the block restrictions depend on the DGP (Bâ€‹DBD/ Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG/ Eâ€‹Bâ€‹DEBD) as in ([29](https://arxiv.org/html/2601.21272v1#S3.E29 "In item (1) â€£ 3.1 Simulation Design â€£ 3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"))â€“([31](https://arxiv.org/html/2601.21272v1#S3.E31 "In item (3) â€£ 3.1 Simulation Design â€£ 3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")). To control persistence in a unified manner across blocks, we calibrate the magnitude of each block using its spectral norm âˆ¥â‹…âˆ¥2\|\cdot\|\_{2}. Specifically, we draw random orthogonal matrices ğ‘¸xâˆˆâ„kÃ—k{\mbox{$Q$}}\_{x}\in\mathbb{R}^{k\times k} and ğ‘¸uâˆˆâ„NÃ—N{\mbox{$Q$}}\_{u}\in\mathbb{R}^{N\times N} (via QR decomposition of i.i.d. Gaussian matrices) and set

|  |  |  |
| --- | --- | --- |
|  | ğš¿xâ€‹x,1=cxâ€‹xâ€‹ğ‘¸x,ğš¿uâ€‹u,1=cuâ€‹uâ€‹ğ‘¸u,{\mbox{$\Psi$}}\_{xx,1}=c\_{xx}\,{\mbox{$Q$}}\_{x},\qquad{\mbox{$\Psi$}}\_{uu,1}=c\_{uu}\,{\mbox{$Q$}}\_{u}, |  |

so that â€–ğš¿xâ€‹x,1â€–2=cxâ€‹x\|{\mbox{$\Psi$}}\_{xx,1}\|\_{2}=c\_{xx} and â€–ğš¿uâ€‹u,1â€–2=cuâ€‹u\|{\mbox{$\Psi$}}\_{uu,1}\|\_{2}=c\_{uu}.

Let râ‰¤minâ¡(k,N)r\leq\min(k,N) denote a pre-specified rank (we use r=1r=1 in the baseline). We generate matrices ğ‘¨âˆˆâ„kÃ—r{\mbox{$A$}}\in\mathbb{R}^{k\times r} and ğ‘©âˆˆâ„NÃ—r{\mbox{$B$}}\in\mathbb{R}^{N\times r} with orthonormal columns (again via QR decomposition), so that ğ‘¨â€²â€‹ğ‘¨=ğ‘°r{\mbox{$A$}}^{\prime}{\mbox{$A$}}={\mbox{$I$}}\_{r} and ğ‘©â€²â€‹ğ‘©=ğ‘°r{\mbox{$B$}}^{\prime}{\mbox{$B$}}={\mbox{$I$}}\_{r}. We then define the low-rank cross blocks

|  |  |  |
| --- | --- | --- |
|  | ğš¿xâ€‹u,1=cxâ€‹uâ€‹ğ‘¨ğ‘©â€²,ğš¿uâ€‹x,1=cuâ€‹xâ€‹ğ‘©ğ‘¨â€²,{\mbox{$\Psi$}}\_{xu,1}=c\_{xu}\,{\mbox{$A$}}{\mbox{$B$}}^{\prime},\qquad{\mbox{$\Psi$}}\_{ux,1}=c\_{ux}\,{\mbox{$B$}}{\mbox{$A$}}^{\prime}, |  |

which implies â€–ğ‘¨ğ‘©â€²â€–2=1\|{\mbox{$A$}}{\mbox{$B$}}^{\prime}\|\_{2}=1 and hence â€–ğš¿xâ€‹u,1â€–2=cxâ€‹u\|{\mbox{$\Psi$}}\_{xu,1}\|\_{2}=c\_{xu} and â€–ğš¿uâ€‹x,1â€–2=cuâ€‹x\|{\mbox{$\Psi$}}\_{ux,1}\|\_{2}=c\_{ux}. This construction aligns the singular directions of ğš¿xâ€‹u,1{\mbox{$\Psi$}}\_{xu,1} and ğš¿uâ€‹x,1{\mbox{$\Psi$}}\_{ux,1}.

Following the spirit of Baillie et al. ([2024](https://arxiv.org/html/2601.21272v1#bib.bib2090 "On robust inference in time-series regression")), we construct VAR(1) data-generating processes that represent the Bâ€‹DBD, Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG, and Eâ€‹Bâ€‹DEBD conditions. Our design can be viewed as a multi-equation extension of their single-equation setup, allowing for cross-equation dependence in ğ’–t{\mbox{$u$}}\_{t} and joint dynamics in (ğ’™tâ€²,ğ’–tâ€²)â€²({\mbox{$x$}}\_{t}^{\prime},{\mbox{$u$}}\_{t}^{\prime})^{\prime}.

For the block-diagonal DGP (Bâ€‹DBD), we set ğš¿xâ€‹u,1=ğŸ{\mbox{$\Psi$}}\_{xu,1}={\mbox{$0$}} and ğš¿uâ€‹x,1=ğŸ{\mbox{$\Psi$}}\_{ux,1}={\mbox{$0$}}. For the triangular DGP (Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG), we set ğš¿uâ€‹x,1=ğŸ{\mbox{$\Psi$}}\_{ux,1}={\mbox{$0$}} while keeping ğš¿xâ€‹u,1{\mbox{$\Psi$}}\_{xu,1} unrestricted. For the unrestricted DGP (Eâ€‹Bâ€‹DEBD), we keep both ğš¿xâ€‹u,1{\mbox{$\Psi$}}\_{xu,1} and ğš¿uâ€‹x,1{\mbox{$\Psi$}}\_{ux,1} unrestricted. Let Ïâ€‹(ğš¿)\rho({\mbox{$\Psi$}}) denote the spectral radius of ğš¿\Psi. If Ïâ€‹(ğš¿)\rho({\mbox{$\Psi$}}) exceeds a target level ÏÂ¯<1\bar{\rho}<1, we scale all blocks by a common factor,

|  |  |  |
| --- | --- | --- |
|  | ğš¿â†Î³â€‹ğš¿,Î³=ÏÂ¯/Ïâ€‹(ğš¿),{\mbox{$\Psi$}}\leftarrow\gamma\,{\mbox{$\Psi$}},\qquad\gamma=\bar{\rho}/\rho({\mbox{$\Psi$}}), |  |

so that the resulting VAR(1) satisfies Ïâ€‹(ğš¿)=ÏÂ¯\rho({\mbox{$\Psi$}})=\bar{\rho} (up to numerical tolerance). In the baseline, we set (cxâ€‹x,cxâ€‹u,cuâ€‹x,cuâ€‹u)=(0.4,0.7,0.3,0.5)(c\_{xx},c\_{xu},c\_{ux},c\_{uu})=(0.4,0.7,0.3,0.5) and ÏÂ¯=0.91\bar{\rho}=0.91, which allow the sinle-equation settins of Baillie et al. ([2024](https://arxiv.org/html/2601.21272v1#bib.bib2090 "On robust inference in time-series regression")) to generalize in muple-equations.

We assume that the VAR innovations satisfy ğšºxâ€‹u=ğŸ{\mbox{$\Sigma$}}\_{xu}={\mbox{$0$}} and draw ğœºx,t{\mbox{$\varepsilon$}}\_{x,t} and ğœºu,t{\mbox{$\varepsilon$}}\_{u,t} independently as

|  |  |  |
| --- | --- | --- |
|  | [ğœºx,tğœºu,t]âˆ¼ğ’©([ğŸğŸ],[ğšºxâ€‹xğŸğŸğšºuâ€‹u]).\begin{bmatrix}{\mbox{$\varepsilon$}}\_{x,t}\\ {\mbox{$\varepsilon$}}\_{u,t}\end{bmatrix}\sim\mathcal{N}\!\mathopen{}\left(\begin{bmatrix}{\mbox{$0$}}\\ {\mbox{$0$}}\end{bmatrix},\begin{bmatrix}{\mbox{$\Sigma$}}\_{xx}&{\mbox{$0$}}\\ {\mbox{$0$}}&{\mbox{$\Sigma$}}\_{uu}\end{bmatrix}\mathclose{}\right). |  |

Both ğšºxâ€‹x{\mbox{$\Sigma$}}\_{xx} and ğšºuâ€‹u{\mbox{$\Sigma$}}\_{uu} are generated as random symmetric positive definite matrices via a Cholesky-type construction: for dimension mm, we draw a lower-triangular matrix ğ‘³L with unit diagonal and i.i.d. off-diagonal entries Unif(âˆ’Î´/m,,Î´/m)\mathrm{Unif}\big(-\delta/\sqrt{m},,\delta/\sqrt{m}\big), and set ğšº=ğ‘³ğ‘³â€²{\mbox{$\Sigma$}}={\mbox{$L$}}{\mbox{$L$}}^{\prime}. In the implementation we use Î´=0.1\delta=0.1 for ğšºxâ€‹x{\mbox{$\Sigma$}}\_{xx} (m=km=k) and Î´=0.5\delta=0.5 for ğšºuâ€‹u{\mbox{$\Sigma$}}\_{uu} (m=Nm=N). After simulating ğ’™tâˆ’ğx{\mbox{$x$}}\_{t}-{\mbox{$\mu$}}\_{x}, we add back a constant mean ğx=Î¼xâ€‹ğŸk{\mbox{$\mu$}}\_{x}=\mu\_{x}{\mbox{$1$}}\_{k} (baseline Î¼x=0.3\mu\_{x}=0.3) to obtain ğ’™t{\mbox{$x$}}\_{t}.

### 3.2 Estimation Accuracy

Following Baillie et al. ([2024](https://arxiv.org/html/2601.21272v1#bib.bib2090 "On robust inference in time-series regression")), we examine the estimation accuracy of multiple-equation estimators (OLS, FGLS-CO, FGLS-D, GD and BC-GD: generalized durbin estimator with bootstrap bias correction). Specifically, we compute and compare the bias and mean squared error (MSE) of these estimators. For ease of interpretation, we summarize the bias and MSE of vector parameters using scalar measures based on the Euclidean norm.

Let ğœ¿0âˆˆâ„(k+1)â€‹N{\mbox{$\kappa$}}\_{0}\in\mathbb{R}^{(k+1)N} denote the true parameter vector and let ğœ¿^T\widehat{{\mbox{$\kappa$}}}\_{T} be an estimator of ğœ¿0{\mbox{$\kappa$}}\_{0} computed from a sample of size TT. Define the bias vector as

|  |  |  |
| --- | --- | --- |
|  | ğ’ƒT:=ğ”¼â€‹[ğœ¿^T]âˆ’ğœ¿0âˆˆâ„(k+1)â€‹N,{\mbox{$b$}}\_{T}:={\mathbb{E}}[\widehat{{\mbox{$\kappa$}}}\_{T}]-{\mbox{$\kappa$}}\_{0}\in\mathbb{R}^{(k+1)N}, |  |

where the expectation is taken under the data-generating process at sample size TT. We report the scalar bias as the Euclidean norm of ğ’ƒT{\mbox{$b$}}\_{T},

|  |  |  |
| --- | --- | --- |
|  | biasâ€‹(ğœ¿^T):=â€–ğ’ƒTâ€–2=â€–ğ”¼â€‹[ğœ¿^T]âˆ’ğœ¿0â€–2.\mathrm{bias}(\widehat{{\mbox{$\kappa$}}}\_{T}):=\|{\mbox{$b$}}\_{T}\|\_{2}=\|{\mathbb{E}}[\widehat{{\mbox{$\kappa$}}}\_{T}]-{\mbox{$\kappa$}}\_{0}\|\_{2}. |  |

We further define the scalar MSE under quadratic loss by

|  |  |  |
| --- | --- | --- |
|  | MSE(ğœ¿^T):=ğ”¼[âˆ¥ğœ¿^Tâˆ’ğœ¿0âˆ¥22]=ğ”¼[(ğœ¿^Tâˆ’ğœ¿0)â€²(ğœ¿^Tâˆ’ğœ¿0)].\mathrm{MSE}(\widehat{{\mbox{$\kappa$}}}\_{T}):={\mathbb{E}}\mathopen{}\left[\|\widehat{{\mbox{$\kappa$}}}\_{T}-{\mbox{$\kappa$}}\_{0}\|\_{2}^{2}\mathclose{}\right]={\mathbb{E}}\mathopen{}\left[(\widehat{{\mbox{$\kappa$}}}\_{T}-{\mbox{$\kappa$}}\_{0})^{\prime}(\widehat{{\mbox{$\kappa$}}}\_{T}-{\mbox{$\kappa$}}\_{0})\mathclose{}\right]. |  |

Then the biasâ€“variance decomposition holds:

|  |  |  |
| --- | --- | --- |
|  | MSE(ğœ¿^T)=bias(ğœ¿^T)2+tr(ğ•ar(ğœ¿^T)),\mathrm{MSE}(\widehat{{\mbox{$\kappa$}}}\_{T})=\mathrm{bias}(\widehat{{\mbox{$\kappa$}}}\_{T})^{2}+\mathrm{tr}\mathopen{}\left({\mathbb{V}\rm{ar}}(\widehat{{\mbox{$\kappa$}}}\_{T})\mathclose{}\right), |  |

where

|  |  |  |
| --- | --- | --- |
|  | ğ•ar(ğœ¿^T)=ğ”¼[(ğœ¿^Tâˆ’ğ”¼[ğœ¿^T])(ğœ¿^Tâˆ’ğ”¼[ğœ¿^T])â€²].{\mathbb{V}\rm{ar}}(\widehat{{\mbox{$\kappa$}}}\_{T})={\mathbb{E}}\mathopen{}\left[(\widehat{{\mbox{$\kappa$}}}\_{T}-{\mathbb{E}}[\widehat{{\mbox{$\kappa$}}}\_{T}])(\widehat{{\mbox{$\kappa$}}}\_{T}-{\mathbb{E}}[\widehat{{\mbox{$\kappa$}}}\_{T}])^{\prime}\mathclose{}\right]. |  |

We adopt this decomposition to compute the MSE and assess the finite-sample accuracy of the estimators.

In Monte Carlo experiments with MM replications, let ğœ¿^T(m)\widehat{{\mbox{$\kappa$}}}\_{T}^{(m)} denote the estimator computed in replication mm and define the replication error ğ’†T(m):=ğœ¿^T(m)âˆ’ğœ¿0{\mbox{$e$}}\_{T}^{(m)}:=\widehat{{\mbox{$\kappa$}}}\_{T}^{(m)}-{\mbox{$\kappa$}}\_{0}. Let 1Mâ€‹âˆ‘m=1Mğœ¿^T(m)\frac{1}{M}\sum\_{m=1}^{M}\widehat{{\mbox{$\kappa$}}}\_{T}^{(m)} and ğ•â€‹ar^â€‹(ğœ¿^T)=1Mâ€‹âˆ‘m=1M{(ğœ¿^T(m)âˆ’ğ”¼â€‹[ğœ¿^T]^)â€‹(ğœ¿^T(m)âˆ’ğ”¼â€‹[ğœ¿^T]^)â€²}\widehat{{\mathbb{V}\rm{ar}}}(\widehat{{\mbox{$\kappa$}}}\_{T})=\frac{1}{M}\sum\_{m=1}^{M}\{(\widehat{{\mbox{$\kappa$}}}\_{T}^{(m)}-\widehat{{\mathbb{E}}[\widehat{{\mbox{$\kappa$}}}\_{T}]})(\widehat{{\mbox{$\kappa$}}}\_{T}^{(m)}-\widehat{{\mathbb{E}}[\widehat{{\mbox{$\kappa$}}}\_{T}]})^{\prime}\}denote the emprical expectation and variance of ğœ¿^T\widehat{{\mbox{$\kappa$}}}\_{T}. Then, the Monte Carlo analogues of the scalar bias and MSE are given by

|  |  |  |
| --- | --- | --- |
|  | bias^(ğœ¿^T):=âˆ¥1Mâˆ‘m=1Mğœ¿^T(m)âˆ’ğœ¿0âˆ¥2,MSE^(ğœ¿^T):=bias^(ğœ¿^T)2+ğ•â€‹ar^(ğœ¿^T).\widehat{\mathrm{bias}}(\widehat{{\mbox{$\kappa$}}}\_{T}):=\mathopen{}\left\|\frac{1}{M}\sum\_{m=1}^{M}\widehat{{\mbox{$\kappa$}}}\_{T}^{(m)}-{\mbox{$\kappa$}}\_{0}\mathclose{}\right\|\_{2},\quad\widehat{\mathrm{MSE}}(\widehat{{\mbox{$\kappa$}}}\_{T}):=\widehat{\mathrm{bias}}(\widehat{{\mbox{$\kappa$}}}\_{T})^{2}+\widehat{{\mathbb{V}\rm{ar}}}(\widehat{{\mbox{$\kappa$}}}\_{T}). |  |

In what follows, we compare the estimation accuracy of the system estimators under the DGPs in ([29](https://arxiv.org/html/2601.21272v1#S3.E29 "In item (1) â€£ 3.1 Simulation Design â€£ 3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"))â€“([31](https://arxiv.org/html/2601.21272v1#S3.E31 "In item (3) â€£ 3.1 Simulation Design â€£ 3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) described in SubsectionÂ [3.2](https://arxiv.org/html/2601.21272v1#S3.SS2 "3.2 Estimation Accuracy â€£ 3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"). We set the true parameters to ğœ¶=ğŸN{\mbox{$\alpha$}}={\mbox{$0$}}\_{N} and ğœ·=ğŸNâ€‹k{\mbox{$\beta$}}={\mbox{$1$}}\_{Nk} (where ğœ·âˆˆâ„Nâ€‹k{\mbox{$\beta$}}\in\mathbb{R}^{Nk}), so that ğœ¿0=(ğŸNâ€²,ğŸNâ€‹kâ€²)â€²{\mbox{$\kappa$}}\_{0}=({\mbox{$0$}}\_{N}^{\prime},{\mbox{$1$}}\_{Nk}^{\prime})^{\prime}.

(Table [1](https://arxiv.org/html/2601.21272v1#Sx1.T1 "Table 1 â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") around here)

Table [1](https://arxiv.org/html/2601.21272v1#Sx1.T1 "Table 1 â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") reports the scalar bias and MSE under the Bâ€‹DBD condition. Under the Bâ€‹DBD, theoretically all estimators are consistent. Hence, the bias of each estimator is expected to decrease toward zero as the sample size TT increases. In terms of efficiency, the FGLS-CO estimator is expected to be the most efficient under Bâ€‹DBD, and therefore it is expected to attain the smallest MSE among the estimators considered. Consistent with this expectation, the reported bias decline with TT, in line with the consistency results established in the theory. Furthermore, the FGLS-CO and FGLS-D estimator attains the smallest MSE regardless of the number of equations NN.

(Table [2](https://arxiv.org/html/2601.21272v1#Sx1.T2 "Table 2 â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") around here)

Table [2](https://arxiv.org/html/2601.21272v1#Sx1.T2 "Table 2 â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") shows the results under the Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG condition. As shown in SectionÂ [2](https://arxiv.org/html/2601.21272v1#S2 "2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"), the OLS and FGLS-CO estimators are theoretically inconsistent under the Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG. The resulting distortion is particularly severe for OLS: both the bias and the MSE are substantially larger than under Bâ€‹DBD. By contrast, the FGLS-CO estimator mitigates the distortion relative to OLS, yielding relatively smaller bias and MSE. This finding is consistent with the simulation evidence in Perron and GonzÃ¡lez-Coya ([2022](https://arxiv.org/html/2601.21272v1#bib.bib2158 "Feasible gls for time series regression")), who argue that quasi-differencing can reduce finite-sample bias even when conventional FGLS-CO procedures remain inconsistent. Moreover, under Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG, the FGLS-D and generalized Durbin estimators are consistent. Consistent with theory, their biases decrease as TT increases. Furthermore, the FGLS-D estimator is expected to be the most efficient under Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG. Consistent with this prediction, it attains the smallest MSE across the values of NN and TT examined in our simulations and also exhibits the smallest bias in most cases.

(Table [3](https://arxiv.org/html/2601.21272v1#Sx1.T3 "Table 3 â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") around here)

TableÂ [3](https://arxiv.org/html/2601.21272v1#Sx1.T3 "Table 3 â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") reports the results under the Eâ€‹Bâ€‹DEBD condition. The Eâ€‹Bâ€‹DEBD design violates the exogeneity requirements necessary for the OLS, FGLS-CO, and FGLS-D estimators to be consistent; however, the GD estimator remains consistent. Simulation evidence supports this prediction. While the biases of all estimators except the GD and BC-GD estimators do not decrease as TT increases, the GD and BC-GD estimators exhibit a bias that decreases with TT and is the smallest among the estimators considered. In terms of MSE, the OLS, FGLS-CO, and FGLS-D estimators have large MSEs. In contrast, the MSEs of the GD and BC-GD estimators are the smallest. This is consistent with the theoretical requirement that the GD estimator is an efficient estimator under the Eâ€‹Bâ€‹DEBD condition.

### 3.3 Size and power of tests

In this subsection, we examine the finite-sample size and power of the proposed Wald tests for model specification. In linear asset-pricing applications, the joint null hypothesis H0:ğœ¶=ğŸH\_{0}:{\mbox{$\alpha$}}={\mbox{$0$}} is commonly assessed using the GRS test (Gibbons et al. [1989](https://arxiv.org/html/2601.21272v1#bib.bib1878 "A test of the efficiency of a given portfolio")), the HAR test of Lazarus et al. ([2018](https://arxiv.org/html/2601.21272v1#bib.bib2118 "HAR inference: recommendations for practice"), [2021](https://arxiv.org/html/2601.21272v1#bib.bib1946 "The size-power tradeoff in har inference")), as well as Wald tests based on FGLS-CO (Nagakura [2024](https://arxiv.org/html/2601.21272v1#bib.bib1985 "Cochraneâ€“orcutt type estimator for multivariate linear regression model with serially correlated errors")) and FGLS-D (Perron and GonzÃ¡lez-Coya [2022](https://arxiv.org/html/2601.21272v1#bib.bib2158 "Feasible gls for time series regression")). We use these established methods as benchmarks and compare them with (i) the Wald test based on the GD estimator and (ii) its bootstrap bias-corrected variants proposed in this paper.

#### 3.3.1 GRS test

The GRS test statistic of Gibbons et al. ([1989](https://arxiv.org/html/2601.21272v1#bib.bib1878 "A test of the efficiency of a given portfolio")) is one of the most widely used statistical tests for evaluating asset pricing models (see, e.g., Fama and French ([1993](https://arxiv.org/html/2601.21272v1#bib.bib1868 "Common risk factors in the returns on stocks and bonds"), [2015](https://arxiv.org/html/2601.21272v1#bib.bib1869 "A five-factor asset pricing model"), [2016](https://arxiv.org/html/2601.21272v1#bib.bib1870 "Dissecting anomalies with a five-factor model"), [2017](https://arxiv.org/html/2601.21272v1#bib.bib1871 "International tests of a five-factor asset pricing model"), [2018](https://arxiv.org/html/2601.21272v1#bib.bib1872 "Choosing factors"), [2020](https://arxiv.org/html/2601.21272v1#bib.bib2161 "Comparing cross-section and time-seriesfactor models")); Cakici et al. ([2013](https://arxiv.org/html/2601.21272v1#bib.bib2160 "Size, value, and momentum in emerging market stock returns"))). Define the following quantities:

|  |  |  |
| --- | --- | --- |
|  | ğ’™Â¯=1Tâ€‹âˆ‘t=1Tğ’™t,ğ‘ºx=1Tâˆ’1â€‹âˆ‘t=1T(ğ’™tâˆ’ğ’™Â¯)â€‹(ğ’™tâˆ’ğ’™Â¯)â€²,ğšº^=1Tâˆ’kâˆ’1â€‹âˆ‘t=1Tğ’–^tâ€‹ğ’–^tâ€²\bar{{\mbox{$x$}}}=\frac{1}{T}\sum\_{t=1}^{T}{\mbox{$x$}}\_{t},\quad{\mbox{$S$}}\_{x}=\frac{1}{T-1}\sum\_{t=1}^{T}({\mbox{$x$}}\_{t}-\bar{{\mbox{$x$}}})({\mbox{$x$}}\_{t}-\bar{{\mbox{$x$}}})^{\prime},\quad\widehat{{\mbox{$\Sigma$}}}=\frac{1}{T-k-1}\sum\_{t=1}^{T}\widehat{{\mbox{$u$}}}\_{t}\widehat{{\mbox{$u$}}}\_{t}^{\prime} |  |

where ğ’–^t\widehat{{\mbox{$u$}}}\_{t} denotes the OLS residuals. The GRS test statistic is then given by

|  |  |  |  |
| --- | --- | --- | --- |
|  | Gâ€‹Râ€‹S=Tâ€‹(Tâˆ’Nâˆ’k)Nâ€‹(Tâˆ’kâˆ’1)â€‹(1+ğ’™Â¯â€‹ğ‘ºxâˆ’1â€‹ğ’™Â¯)âˆ’1â€‹ğœ¶^OLSâ£â€²â€‹ğšº^âˆ’1â€‹ğœ¶^OLSGRS=\frac{T(T-N-k)}{N(T-k-1)}(1+\bar{{\mbox{$x$}}}{\mbox{$S$}}\_{x}^{-1}\bar{{\mbox{$x$}}})^{-1}\widehat{{\mbox{$\alpha$}}}^{\mathrm{OLS}\prime}\widehat{{\mbox{$\Sigma$}}}^{-1}\widehat{{\mbox{$\alpha$}}}^{\mathrm{OLS}} |  | (32) |

Under the assumption that ğ’–t{\mbox{$u$}}\_{t} is independently and identically distributed according to a multivariate normal distribution, the statistic Gâ€‹Râ€‹SGRS follows an Fâ€‹(N,Tâˆ’Nâˆ’k)F(N,T-N-k) distribution under the null hypothesis H0:ğœ¶=ğŸH\_{0}:{\mbox{$\alpha$}}={\mbox{$0$}}. 555Kamstra and Shi ([2024](https://arxiv.org/html/2601.21272v1#bib.bib1935 "Testing and ranking of asset pricing models using the grs statistic")) propose a modified version of the GRS test to improve its small-sample properties. However, the performance of the original and modified GRS tests does not differ substantially once the sample size TT exceeds 200. For this reason, we rely exclusively on the original GRS test as a benchmark. This statistic jointly tests whether all pricing errors are equal to zero across portfolios.

#### 3.3.2 HAR test

When regression errors exhibit heteroskedasticity and autocorrelation, specification tests that ignore these features are invalid. A prominent approach that accounts for both heteroskedasticity and autocorrelation is the heteroskedasticity and autocorrelation robust (HAR) test based on the heteroskedasticity and autocorrelation consistent (HAC) estimators of Newey and West ([1987](https://arxiv.org/html/2601.21272v1#bib.bib233 "A simple, positive semi-definite, heteroskedasticity and autocorrelation consistent covariance matrix")). Under kernel-based nonparametric estimation, the long-run covariance matrix

|  |  |  |
| --- | --- | --- |
|  | ğ›€=ğšª0+âˆ‘j=1âˆ(ğšªj+ğšªjâ€²),ğšªj=ğ”¼â€‹[ğ’—tâ€‹ğ’—tâˆ’jâ€²],ğ’—t=ğ’tâ€‹ğ’–t,{\mbox{$\Omega$}}={\mbox{$\Gamma$}}\_{0}+\sum\_{j=1}^{\infty}\big({\mbox{$\Gamma$}}\_{j}+{\mbox{$\Gamma$}}\_{j}^{\prime}\big),\quad{\mbox{$\Gamma$}}\_{j}={\mathbb{E}}[{\mbox{$v$}}\_{t}{\mbox{$v$}}\_{t-j}^{\prime}],\quad{\mbox{$v$}}\_{t}={\mbox{$Z$}}\_{t}{\mbox{$u$}}\_{t}, |  |

is estimated as

|  |  |  |
| --- | --- | --- |
|  | ğ›€^â€‹(M)=âˆ‘j=âˆ’(Tâˆ’1)Tâˆ’1kâ€‹(jM)â€‹ğš²^â€‹(j),ğš²^â€‹(j)=1Tâ€‹âˆ‘t=j+1Tğ’—^tâ€‹ğ’—^tâˆ’jâ€²,ğ’—^t=ğ’tâ€‹ğ’–^t.\widehat{{\mbox{$\Omega$}}}(M)=\sum\_{j=-(T-1)}^{T-1}k\Big(\frac{j}{M}\Big)\widehat{{\mbox{$\Lambda$}}}(j),\quad\widehat{{\mbox{$\Lambda$}}}(j)=\frac{1}{T}\sum\_{t=j+1}^{T}\widehat{{\mbox{$v$}}}\_{t}\widehat{{\mbox{$v$}}}\_{t-j}^{\prime},\quad\widehat{{\mbox{$v$}}}\_{t}={\mbox{$Z$}}\_{t}\widehat{{\mbox{$u$}}}\_{t}. |  |

Under the conditions Mâ†’âˆM\to\infty and M/Tâ†’0M/T\to 0 as Tâ†’âˆT\to\infty, we can consistently estimate ğ›€\Omega. The corresponding Wald statistic is given by

|  |  |  |
| --- | --- | --- |
|  | ğ’²HAC=(ğ‘¹ğœ¿^OLSâˆ’ğ’“)â€²[ğ‘¹(ğ’ğ’â€²/T)âˆ’1ğ›€^(M)(ğ’ğ’â€²/T)âˆ’1ğ‘¹â€²](ğ‘¹ğœ¿^OLSâˆ’ğ’“)\mathcal{W}^{\mathrm{HAC}}=({\mbox{$R$}}\widehat{{\mbox{$\kappa$}}}^{\mathrm{OLS}}-{\mbox{$r$}})^{\prime}\mathopen{}\left[{\mbox{$R$}}({\mbox{$Z$}}{\mbox{$Z$}}^{\prime}/T)^{-1}\widehat{{\mbox{$\Omega$}}}(M)({\mbox{$Z$}}{\mbox{$Z$}}^{\prime}/T)^{-1}{\mbox{$R$}}^{\prime}\mathclose{}\right]({\mbox{$R$}}\widehat{{\mbox{$\kappa$}}}^{\mathrm{OLS}}-{\mbox{$r$}}) |  |

which, under the null, converges in distribution to either the standard normal or a Ï‡2\chi^{2} distribution depending on the normalization.

A key practical issue is the choice of the bandwidth parameter MM in the kernel estimator. In small samples, such choices can lead to substantial size distortions, often producing over-rejection. To address this problem, Kiefer et al. ([2000](https://arxiv.org/html/2601.21272v1#bib.bib1937 "Simple robust testing of regression hypotheses")) introduce a nonsingular data-dependent stochastic transformation that eliminates the need for consistent estimation of ğ›€\Omega. Specifically, they proposed the fixed-bb approach, in which the bandwidth is proportional to the sample size,

|  |  |  |
| --- | --- | --- |
|  | M=bâ€‹T,bâˆˆ(0,1].M=bT,\quad b\in(0,1]. |  |

By fixing M/T=bM/T=b, the long-run covariance estimator ğ›€^â€‹(bâ€‹T)\widehat{{\mbox{$\Omega$}}}(bT) converges to a random limit, sacrificing consistency in favor of a nonstandard limiting distribution that depends on the kernel and bb. This approach improves finite-sample size control relative to conventional HAC-based HAR tests (e.g., Sun ([2014](https://arxiv.org/html/2601.21272v1#bib.bib2163 "Letâ€™s fix it: fixed-b aymptotics versus small-b asymptotics in heteroskedasticity and autocorrelation robust inference"))).

Kiefer and Vogelsang ([2002a](https://arxiv.org/html/2601.21272v1#bib.bib1936 "Heteroskedasticity-autocorrelation robust standard errors using the bartlett kernel without truncation")) show that setting b=1b=1 (i.e., M=TM=T) yields a HAR statistic based on the Bartlett kernel with no truncation, which follows a nonstandard limiting distribution. Kiefer and Vogelsang ([2005](https://arxiv.org/html/2601.21272v1#bib.bib2115 "A new asymptotic theory for heteroskedasticity-autocorrelation robust tests")) generalize this framework by developing fixed-bb asymptotics for any bâˆˆ(0,1]b\in(0,1], thereby unifying the theory. Subsequent work (e.g., Jansson [2004](https://arxiv.org/html/2601.21272v1#bib.bib2162 "The error in rejection probability of simple autocorrelation robust tests")) demonstrates that fixed-bb asymptotics incorporate the sampling uncertainty of long-run covariance estimation into the first-order asymptotics, mitigating the size distortions inherent in the conventional small-bb framework. Building on these insights, Lazarus et al. ([2018](https://arxiv.org/html/2601.21272v1#bib.bib2118 "HAR inference: recommendations for practice"), [2021](https://arxiv.org/html/2601.21272v1#bib.bib1946 "The size-power tradeoff in har inference")) provide a comprehensive comparison of fixed-bb methods, established the size-power frontier, and propose loss-function-based rules for selecting bb. They show that the frontier is achieved with the quadratic spectral (QS) kernel. Against this background, we adopt the fixed-bb HAR test with the QS kernel developed by Lazarus et al. ([2018](https://arxiv.org/html/2601.21272v1#bib.bib2118 "HAR inference: recommendations for practice"), [2021](https://arxiv.org/html/2601.21272v1#bib.bib1946 "The size-power tradeoff in har inference")).

#### 3.3.3 Wald test based on the FGLS-CO estimator

Nagakura ([2024](https://arxiv.org/html/2601.21272v1#bib.bib1985 "Cochraneâ€“orcutt type estimator for multivariate linear regression model with serially correlated errors")) extends [Cochrane and Orcutt](https://arxiv.org/html/2601.21272v1#bib.bib1857 "Application of least squares regression to relationships containing auto-correlated error terms")â€™s ([1949](https://arxiv.org/html/2601.21272v1#bib.bib1857 "Application of least squares regression to relationships containing auto-correlated error terms")) autoregressive GLS procedure to a multiple-equation system by assuming that the disturbance vector follows a VAR(pp) process. Since the FGLS-CO estimator has the following asymptotic variance

|  |  |  |
| --- | --- | --- |
|  | Tâ€‹(ğœ¿^COâˆ’ğœ¿)â†’ğ‘‘ğ’©â€‹(ğŸ,ğ‘½CO),ğ‘½CO:=ğ”¼â€‹[ğ’tâˆ—â€‹ğ›€âˆ’1â€‹ğ’tâˆ—â£â€²]âˆ’1,\sqrt{T}\big(\widehat{{\mbox{$\kappa$}}}^{\mathrm{CO}}-{\mbox{$\kappa$}}\big)\xrightarrow{d}\mathcal{N}\big({\mbox{$0$}},\ {\mbox{$V$}}^{\mathrm{CO}}\big),\quad{\mbox{$V$}}^{\mathrm{CO}}:={\mathbb{E}}[{\mbox{$Z$}}\_{t}^{\*}{\mbox{$\Omega$}}^{-1}{\mbox{$Z$}}\_{t}^{\*\prime}]^{-1}, |  |

under the null hypothesis H0:ğ‘¹ğœ¿=ğ’“H\_{0}:{\mbox{$R$}}{\mbox{$\kappa$}}={\mbox{$r$}} the corresponding Wald statistic is given by

|  |  |  |
| --- | --- | --- |
|  | ğ’²CO=(ğ‘¹â€‹ğœ¿^COâˆ’ğ’“)â€²â€‹[ğ‘¹â€‹ğ‘½^COâ€‹ğ‘¹â€²]âˆ’1â€‹(ğ‘¹â€‹ğœ¿^COâˆ’ğ’“)â†’ğ‘‘Ï‡d2.\mathcal{W}^{\mathrm{CO}}=\big({\mbox{$R$}}\widehat{{\mbox{$\kappa$}}}^{\mathrm{CO}}-{\mbox{$r$}}\big)^{\prime}\Big[{\mbox{$R$}}\widehat{{\mbox{$V$}}}^{\mathrm{CO}}{\mbox{$R$}}^{\prime}\Big]^{-1}\big({\mbox{$R$}}\widehat{{\mbox{$\kappa$}}}^{\mathrm{CO}}-{\mbox{$r$}}\big)\xrightarrow{d}\chi\_{d}^{2}. |  |

In contrast to the OLS-based GRS test, the Wald test based on the FGLS-CO estimator can deliver more precise inference under the Bâ€‹DBD condition. However, its validity is fragile: under weaker exogeneity conditions such as Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG or Eâ€‹Bâ€‹DEBD, the FGLS-CO estimator loses consistency, highlighting the trade-off between efficiency gains and robustness (e.g., see PropositionÂ [5](https://arxiv.org/html/2601.21272v1#Thmproposition5 "Proposition 5 (Consistency region of CO-type estimators): â€£ 2.3 The inconsistency of OLS and OLS-based GLS estimators in multi-equation models â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")).

#### 3.3.4 Wald test based on the FGLS-D estimator

Perron and GonzÃ¡lez-Coya ([2022](https://arxiv.org/html/2601.21272v1#bib.bib2158 "Feasible gls for time series regression")) propose an FGLS-D estimator. The extention to the multiple-equation system is shown as in ([21](https://arxiv.org/html/2601.21272v1#S2.E21 "In item (Step 2) Quasi-differencing and GLS on the filtered system. â€£ 2.4 The inconsistency of FGLS-D estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")). The asymptotic normality of the FGLS-D estimator is given as

|  |  |  |
| --- | --- | --- |
|  | Tâ€‹(ğœ¿^FGLSâˆ’Dâˆ’ğœ¿)â†’ğ‘‘ğ’©â€‹(ğŸ,ğ‘½FGLSâˆ’D),ğ‘½FGLSâˆ’D:=(ğ”¼â€‹[ğ’Qâ€‹D,tâ€²â€‹ğšºâˆ’1â€‹ğ’Qâ€‹D,t])âˆ’1.\sqrt{T}\big(\widehat{{\mbox{$\kappa$}}}^{\mathrm{FGLS-D}}-{\mbox{$\kappa$}}\big)\xrightarrow{d}\mathcal{N}\big({\mbox{$0$}},\ {\mbox{$V$}}^{\mathrm{FGLS-D}}\big),\quad{\mbox{$V$}}^{\mathrm{FGLS-D}}:=\big({\mathbb{E}}[{\mbox{$Z$}}\_{QD,t}^{\prime}{\mbox{$\Sigma$}}^{-1}{\mbox{$Z$}}\_{QD,t}]\big)^{-1}. |  |

Hence, under the null hypothesis H0:ğ‘¹ğœ¿=ğ’“H\_{0}:{\mbox{$R$}}{\mbox{$\kappa$}}={\mbox{$r$}}, the Wald statistic is

|  |  |  |
| --- | --- | --- |
|  | ğ’²FGLSâˆ’D=(ğ‘¹â€‹ğœ¿^FGLSâˆ’Dâˆ’ğ’“)â€²â€‹[ğ‘¹â€‹ğ‘½^FGLSâˆ’Dâ€‹ğ‘¹â€²]â€‹(ğ‘¹â€‹ğœ¿^FGLSâˆ’Dâˆ’ğ’“)â†’ğ‘‘Ï‡d2.\mathcal{W}^{\mathrm{FGLS-D}}=\big({\mbox{$R$}}\widehat{{\mbox{$\kappa$}}}^{\mathrm{FGLS-D}}-{\mbox{$r$}}\big)^{\prime}\Big[{\mbox{$R$}}\widehat{{\mbox{$V$}}}^{\mathrm{FGLS-D}}{\mbox{$R$}}^{\prime}\Big]\big({\mbox{$R$}}\widehat{{\mbox{$\kappa$}}}^{\mathrm{FGLS-D}}-{\mbox{$r$}}\big)\xrightarrow{d}\chi\_{d}^{2}. |  |

FGLS-D differs from the conventional FGLS-CO estimator only in the prefiltering step: instead of filtering with VAR parameters fitted to OLS residuals, it estimates the lag polynomial from a Durbin regression and uses that filter to quasi-difference the system. This can deliver efficiency gains when the Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG condition holds as shown in sectionÂ [2](https://arxiv.org/html/2601.21272v1#S2 "2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"). However, under Eâ€‹Bâ€‹DEBD, both FGLS-CO and FGLS-D lose consistency, whereas Durbin remains consistent (and efficient).

### 3.4 Simulation Results

In this subsection, we report results from Monte Carlo experiments. First, we examine size by checking whether each testâ€™s empirical rejection frequency under the null matches the nominal significance level. Second, we assess power, that is, the ability of each test to detect departures from the null. Together, these results illustrate how the competing procedures trade off size (Type I error) and test power (Type II error) across the three DGPs, Bâ€‹DBD, Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG, and Eâ€‹Bâ€‹DEBD.

Throughout, ğ’²Bâ€‹Câ€‹-â€‹Gâ€‹D\mathcal{W}^{BC\text{-}GD} denotes the bootstrap bias-corrected Wald test based on the GD estimator, while ğ’²Gâ€‹D\mathcal{W}^{GD}, ğ’²Fâ€‹Gâ€‹Lâ€‹Sâ€‹-â€‹D\mathcal{W}^{FGLS\text{-}D}, and ğ’²Fâ€‹Gâ€‹Lâ€‹Sâ€‹-â€‹Câ€‹O\mathcal{W}^{FGLS\text{-}CO} denote Wald tests based on the GD, FGLS-D, and FGLS-CO estimators, respectively. We also consider the Hâ€‹Aâ€‹RHAR test constructed using the fixed-bb HAC estimator of Lazarus et al. ([2018](https://arxiv.org/html/2601.21272v1#bib.bib2118 "HAR inference: recommendations for practice"), [2021](https://arxiv.org/html/2601.21272v1#bib.bib1946 "The size-power tradeoff in har inference")), and the Gâ€‹Râ€‹SGRS test of Gibbons et al. ([1989](https://arxiv.org/html/2601.21272v1#bib.bib1878 "A test of the efficiency of a given portfolio")). Under the null hypothesis H0:ğœ¶0=ğŸN,1H\_{0}:{\mbox{$\alpha$}}\_{0}={\mbox{$0$}}\_{N,1}, we report empirical rejection frequencies at the nominal 10%, 5%, and 1% significance levels.

#### 3.4.1 Rejection Frequencies under the Null Hypothesis

(Table [4](https://arxiv.org/html/2601.21272v1#Sx1.T4 "Table 4 â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") around here)

TableÂ [4](https://arxiv.org/html/2601.21272v1#Sx1.T4 "Table 4 â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") reports rejection frequencies under the Bâ€‹DBD condition and the null. The HAR test delivers rejection frequencies close to nominal levels regardless of the cross-sectional dimension. However, the Wald tests based on GD, FGLS-D, and FGLS-CO tends to over reject the null hypothesis in finite samples. While FGLS-CO is asymptotically efficient under Bâ€‹DBD, its associated Wald statistic does not dominate in finite samples. In contrast, the Wald test based on the BC-GD performs well. Its rejection frequencies are close to nominal levels, reflecting the consistency of the underlying estimators.

(Table [5](https://arxiv.org/html/2601.21272v1#Sx1.T5 "Table 5 â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") around here)

Next, we examine the size distortion of each test under the Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG condition in TalbeÂ [5](https://arxiv.org/html/2601.21272v1#Sx1.T5 "Table 5 â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"). Under the Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG, the OLS estimators of ğœ¶\alpha and ğœ·\beta in ([2](https://arxiv.org/html/2601.21272v1#S2.E2 "In 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) are inconsistent due to dynamic regressor-error dependence. Consequently, test statistics that rely on OLS estimator theoretically do not converge to their expected distributions. In line with this, the GRS test and the HAR test severely over-reject the null across all cross-sectional dimensions NN and factor counts kk, missing the nominal 10%, 5%, and 1% levels by wide margins. By contrast, the Wald test based on the FGLS-CO estimator exhibits smaller size distortions (though it is not exact). This pattern accords with Perron and GonzÃ¡lez-Coya ([2022](https://arxiv.org/html/2601.21272v1#bib.bib2158 "Feasible gls for time series regression")), who show, in a single-equation setting, that inference based on the FGLS-D estimator is more robust to misspecification than OLS-based procedures. As in the case of the Bâ€‹DBD condition, the Wald test based on the BC-GD estimator deliver rejection frequencies close to nominal levels across NN and kk.

(Table [6](https://arxiv.org/html/2601.21272v1#Sx1.T6 "Table 6 â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") around here)

Finally, we investigate the size performance under the Eâ€‹Bâ€‹DEBD condition in TableÂ [6](https://arxiv.org/html/2601.21272v1#Sx1.T6 "Table 6 â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"). Theory implies that only the GD estimator is consistent among the procedures considered. Accordingly, tests that depend on OLS estimator, the GRS and the HAR tests, are severely oversized. Furthermore, the Wald tests based on the FGLS-D and FGLS-CO estimators also exhibit severe size distrotions. While the GD estimator is consistent, its Wald test tend to over reject the null hypothesis. In contrast, the rejection frequencies of the Wald test based on the BC-GD estimator are close to nominal level regardless of NN and kk.

#### 3.4.2 Rejection Frequencies under the Alternative Hypothesis

Next, we examine the size-adjusted power of each test under Bâ€‹DBD, Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG, and Eâ€‹Bâ€‹DEBD conditions. Since finite-sample size distortions can substantially affect rejection frequencies, raw power is not directly comparable across tests. We therefore report size-adjusted power to ensure that differences in rejection frequencies reflect each testâ€™s genuine ability to detect departures from the null. Here, we consider the alternative hypothesis H1:Î±1=0.2H\_{1}:\alpha\_{1}=0.2 and Î±j=0\alpha\_{j}=0 for jâ‰ 1j\neq 1.

(Table [7](https://arxiv.org/html/2601.21272v1#Sx1.T7 "Table 7 â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") around here)

TableÂ [7](https://arxiv.org/html/2601.21272v1#Sx1.T7 "Table 7 â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") reports rejection frequencies under the fixed alternative for the Bâ€‹DBD condition. We evaluate size-adjusted power by examining whether rejection frequencies increase with the sample size TT and converge to one. Under Bâ€‹DBD, all estimators are consistent, implying that their associated tests are consistent and should achieve power against fixed alternatives as TT grows. However, the Gâ€‹Râ€‹SGRS test exhibits noticeably lower power, regardless of NN. This pattern is consistent with Affleck-Graves and McDonald ([1989](https://arxiv.org/html/2601.21272v1#bib.bib1501 "Nonnormalities and tests of asset pricing theories")), who show that departures from normality can substantially reduce the power of the Gâ€‹Râ€‹SGRS test.

The HAR test also delivers reasonably good power under the Bâ€‹DBD, in line with Lazarus et al. ([2021](https://arxiv.org/html/2601.21272v1#bib.bib1946 "The size-power tradeoff in har inference")), who show that the fixed-bb Hâ€‹Aâ€‹RHAR test with the QS kernel attains the asymptotic sizeâ€“power frontier. Furthermore, the GLS-based Wald tests, those based on Gâ€‹Dâ€‹-â€‹BGD\text{-}B, Gâ€‹DGD, Fâ€‹Gâ€‹Lâ€‹Sâ€‹-â€‹DFGLS\text{-}D, and Fâ€‹Gâ€‹Lâ€‹Sâ€‹-â€‹Câ€‹OFGLS\text{-}CO, exhibit strong power across all cross-sectional dimensions considered.

(Table [8](https://arxiv.org/html/2601.21272v1#Sx1.T8 "Table 8 â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") around here)

TableÂ [8](https://arxiv.org/html/2601.21272v1#Sx1.T8 "Table 8 â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") reports size-adjusted power under the Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG condition. Since the OLS estimator in ([2](https://arxiv.org/html/2601.21272v1#S2.E2 "In 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) is inconsistent under the Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG condition, the OLS-based tests (GRS and HAR) exhibit low test power. While the FGLS-CO estimator is based on the OLS estimator, it exhibits reasonably good power. This result aligns with Perron and GonzÃ¡lez-Coya ([2022](https://arxiv.org/html/2601.21272v1#bib.bib2158 "Feasible gls for time series regression")), who demonstrated that the FGLS-CO estimator is robust to moderate model misspecifications. The Wald tests based on BC-GD, GD, and FGLS-D estimators demonstrate significant power under the alternative hypothesis, with rejection frequencies increasing with TT.

(Table [9](https://arxiv.org/html/2601.21272v1#Sx1.T9 "Table 9 â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") around here)

Finally, we examine the size-adjusted power under the Eâ€‹Bâ€‹DEBD condition, which is reported in TableÂ [9](https://arxiv.org/html/2601.21272v1#Sx1.T9 "Table 9 â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"). Under the Eâ€‹Bâ€‹DEBD, only the GD-based estimators are consistent, whereas the remaining estimators are misspecified and hence their associated tests lose power as TT increases. Consistent with this implication, the size-adjusted power of all competing procedures, except for Gâ€‹Dâ€‹-â€‹BGD\text{-}B and Gâ€‹DGD, declines markedly under Eâ€‹Bâ€‹DEBD. The deterioration is particularly pronounced for the OLS-based tests (Gâ€‹Râ€‹SGRS and Hâ€‹Aâ€‹RHAR).

Taken together, the size and size-adjusted power results across the Bâ€‹DBD, Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG, and Eâ€‹Bâ€‹DEBD designs indicate that the Wald test based on Gâ€‹DGD tends to over-reject the null in finite samples. In contrast, the Wald test based on the bootstrap bias-corrected Gâ€‹DGD estimator (Gâ€‹Dâ€‹-â€‹BGD\text{-}B) delivers stable size control and competitive power across all DGPs.

## 4 Empirical Application

We examine whether the Wald test based on the generalized Durbin estimator proposed in this paper can mitigate the over-rejection problem observed for Wald tests based on the FGLS-D and FGLS-CO estimators. In SectionÂ [2](https://arxiv.org/html/2601.21272v1#S2 "2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"), we show that, under the Eâ€‹Bâ€‹DEBD condition, the generalized Durbin estimator is consistent, whereas the FGLS-D and FGLS-CO estimators are not. Our simulation results in SectionÂ [3](https://arxiv.org/html/2601.21272v1#S3 "3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"), however, indicate that the Wald test based on the generalized Durbin estimator can still exhibit non-negligible size distortions in finite samples. We therefore employ a bootstrap bias correction to improve finite-sample size control.

To illustrate the empirical relevance of our approach, we revisit the Famaâ€“French multifactor models studied by Fama and French ([1993](https://arxiv.org/html/2601.21272v1#bib.bib1868 "Common risk factors in the returns on stocks and bonds"), [2015](https://arxiv.org/html/2601.21272v1#bib.bib1869 "A five-factor asset pricing model")). As noted by Moriya and Noda ([2024](https://arxiv.org/html/2601.21272v1#bib.bib1743 "Time instability of the fama-french multifactor models: an international evidence")), there remains ongoing debate over the validity of the FF multifactor models. In empirical studies (e.g., Fama and French ([1993](https://arxiv.org/html/2601.21272v1#bib.bib1868 "Common risk factors in the returns on stocks and bonds"), [2015](https://arxiv.org/html/2601.21272v1#bib.bib1869 "A five-factor asset pricing model"), [2016](https://arxiv.org/html/2601.21272v1#bib.bib1870 "Dissecting anomalies with a five-factor model"), [2017](https://arxiv.org/html/2601.21272v1#bib.bib1871 "International tests of a five-factor asset pricing model"), [2018](https://arxiv.org/html/2601.21272v1#bib.bib1872 "Choosing factors"), [2020](https://arxiv.org/html/2601.21272v1#bib.bib2161 "Comparing cross-section and time-seriesfactor models")); Cakici et al. ([2013](https://arxiv.org/html/2601.21272v1#bib.bib2160 "Size, value, and momentum in emerging market stock returns"))), it has become standard practice to apply the GRS test to assess the validity of FF multifactor models. However, our simulation evidence in SectionÂ [3](https://arxiv.org/html/2601.21272v1#S3 "3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") shows that both the GRS test and the HAR test can suffer severe size distortions when the Bâ€‹DBD condition fails. Because Bâ€‹DBD is more restrictive, and thus less empirically plausible, than Eâ€‹Bâ€‹DEBD, and because the GRS and HAR tests are consistent under Bâ€‹DBD but not under Eâ€‹Bâ€‹DEBD, we do not employ these procedures in our empirical analysis. Instead, we compare the bootstrap bias-corrected Wald test based on the generalized Durbin estimator with Wald tests based on the FGLS-D and FGLS-CO estimators.

### 4.1 Famaâ€“French Multifactor Models

Following Fama and French ([1993](https://arxiv.org/html/2601.21272v1#bib.bib1868 "Common risk factors in the returns on stocks and bonds"), [2015](https://arxiv.org/html/2601.21272v1#bib.bib1869 "A five-factor asset pricing model")), we introduce the FF multifactor models. Equation ([33](https://arxiv.org/html/2601.21272v1#S4.E33 "In 4.1 Famaâ€“French Multifactor Models â€£ 4 Empirical Application â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) presents [Fama and French](https://arxiv.org/html/2601.21272v1#bib.bib1868 "Common risk factors in the returns on stocks and bonds")â€™s ([1993](https://arxiv.org/html/2601.21272v1#bib.bib1868 "Common risk factors in the returns on stocks and bonds")) three-factor (hereafter referred to as FF3) model, which is the most widely known multifactor model:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ri,tâˆ’Rf,t=Î±i+Î²iMâ€‹kâ€‹tâ€‹(Rm,tâˆ’Rf,t)+Î²iSâ€‹Mâ€‹Bâ€‹Sâ€‹Mâ€‹Bt+Î²iHâ€‹Mâ€‹Lâ€‹Hâ€‹Mâ€‹Lt+Îµi,t,R\_{i,t}-R\_{f,t}=\alpha\_{i}+\beta^{Mkt}\_{i}(R\_{m,t}-R\_{f,t})+\beta^{SMB}\_{i}SMB\_{t}+\beta^{HML}\_{i}HML\_{t}+\varepsilon\_{i,t}, |  | (33) |

where Ri,tR\_{i,t} denotes the return on the portfolio ii at time tt, Rf,tR\_{f,t} is the risk-free rate at time tt, Rm,tR\_{m,t} is the returns on the market portfolio at time tt, and Îµi,t\varepsilon\_{i,t} is the error term for portfolio ii at time tt. The FF3 model expands the CAPM by adding size and value risk factors to capture market anomalies. The size risk factor (Sâ€‹Mâ€‹BtSMB\_{t}) reflects the empirical regularity that stocks with smaller (or larger) market capitalizations tend to earn higher returns, while the value risk factor (Hâ€‹Mâ€‹LtHML\_{t}) accounts for the superior performance of stocks with low (or high) price-to-book ratios. Building on the FF3 model, Fama and French ([2015](https://arxiv.org/html/2601.21272v1#bib.bib1869 "A five-factor asset pricing model")) propose a five-factor model (hereafter referred to as FF5), as shown in Equation ([34](https://arxiv.org/html/2601.21272v1#S4.E34 "In 4.1 Famaâ€“French Multifactor Models â€£ 4 Empirical Application â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")):

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ri,tâˆ’Rf,t=Î±i+Î²iMâ€‹kâ€‹tâ€‹(Rm,tâˆ’Rf,t)+Î²iSâ€‹Mâ€‹Bâ€‹Sâ€‹Mâ€‹Bt+Î²iHâ€‹Mâ€‹Lâ€‹Hâ€‹Mâ€‹Lt+Î²iRâ€‹Mâ€‹Wâ€‹Râ€‹Mâ€‹Wt+Î²iCâ€‹Mâ€‹Aâ€‹Câ€‹Mâ€‹At+Îµi,t.\begin{split}R\_{i,t}-R\_{f,t}=\alpha\_{i}+\beta^{Mkt}\_{i}(R\_{m,t}-R\_{f,t})+\beta^{SMB}\_{i}SMB\_{t}+\beta^{HML}\_{i}HML\_{t}\\ +\beta^{RMW}\_{i}RMW\_{t}+\beta^{CMA}\_{i}CMA\_{t}+\varepsilon\_{i,t}.\end{split} |  | (34) |

This model adds two risk factors to FF3: the profitability risk factor (Râ€‹Mâ€‹WtRMW\_{t}) and the investment risk factor (Câ€‹Mâ€‹AtCMA\_{t}). The models are said to correctly capture the behavior of stock returns if all intercept terms Î±i\alpha\_{i} for each portfolio in Equations ([33](https://arxiv.org/html/2601.21272v1#S4.E33 "In 4.1 Famaâ€“French Multifactor Models â€£ 4 Empirical Application â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) and ([34](https://arxiv.org/html/2601.21272v1#S4.E34 "In 4.1 Famaâ€“French Multifactor Models â€£ 4 Empirical Application â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) are not significantly different from zero. Therefore, to assess the validity of the FF multifactor models, we test the following null hypothesis:

|  |  |  |
| --- | --- | --- |
|  | H0:ğœ¶=ğŸ,H1:notâ€‹H0H\_{0}:{\mbox{$\alpha$}}={\mbox{$0$}},\ \ H\_{1}:{\rm{not}}\ H\_{0} |  |

where ğœ¶={Î±i}i=1N{\mbox{$\alpha$}}=\mathopen{}\left\{\alpha\_{i}\mathclose{}\right\}\_{i=1}^{N} in each of Equations ([33](https://arxiv.org/html/2601.21272v1#S4.E33 "In 4.1 Famaâ€“French Multifactor Models â€£ 4 Empirical Application â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) and ([34](https://arxiv.org/html/2601.21272v1#S4.E34 "In 4.1 Famaâ€“French Multifactor Models â€£ 4 Empirical Application â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")).

### 4.2 Dataset

Fama and French ([1993](https://arxiv.org/html/2601.21272v1#bib.bib1868 "Common risk factors in the returns on stocks and bonds"), [2015](https://arxiv.org/html/2601.21272v1#bib.bib1869 "A five-factor asset pricing model")) provide the returns on the benchmark portfolios and risk factors for examining the FF multifactor models. In these models, the benchmark portfolios are formed by classifying all stocks in the market into either 2Ã—3=6â€‹orâ€‹ 5Ã—5=252\times 3=6\ {\rm{or}}\ 5\times 5=25 categories using two criteria (e.g., â€œmarket capitalization (firm size) and book-to-market ratio (MB)â€ as proposed by Fama and French ([1993](https://arxiv.org/html/2601.21272v1#bib.bib1868 "Common risk factors in the returns on stocks and bonds"))). The average return of each portfolio is regarded as the representative portfolio in the market. Other sorting criteria have also been proposed, such as â€œmarket capitalization and profitability (MO)â€ and â€œmarket capitalization and investment growth rate (MI)â€ (Fama and French ([2015](https://arxiv.org/html/2601.21272v1#bib.bib1869 "A five-factor asset pricing model"))).

Lewellen et al. ([2010](https://arxiv.org/html/2601.21272v1#bib.bib1717 "A skeptical appraisal of asset pricing tests")) point out that many studies test the FF multifactor models using portfolios sorted by â€œmarket capitalization (firm size)â€ and â€œbook-to-market ratio,â€ but the models may not perform as well when using portfolios sorted by other criteria. To consider the possibility that different sorting methods may affect the results, we use three types of benchmark portfolios to compare the performance of the Wald test based on the generalized Durbin estimator proposed in this paper with the GRS test.

Furthermore, the FF multifactor models are often rejected when U.S. data are used, as documented in Fama and French ([2012](https://arxiv.org/html/2601.21272v1#bib.bib1874 "Size, value, and momentum in international stock returns"), [2017](https://arxiv.org/html/2601.21272v1#bib.bib1871 "International tests of a five-factor asset pricing model")). In this study, following much of the previous literature, we use U.S. monthly data to examine whether such rejections may be driven by the use of unsuitable test statistics, such as the GRS statistic. All datasets are available from [Professor Kenneth Frenchâ€™s website](http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html). For the U.S. monthly data used in this study, the available sample spans July 1990 to November 2025. Our estimation period is restricted to the post-global financial crisis (GFC) period, from October 2008 (the month following the Lehman Brothers bankruptcy) to November 2025. We adopt this window because the estimators we employ are time-invariant and do not explicitly accommodate structural breaks.

A large literature documents that the GFC coincided with shifts in equity-market comovements and factor sensitivities. In particular, Bekaert et al. ([2014](https://arxiv.org/html/2601.21272v1#bib.bib2206 "The global crisis and equity market contagion")) report crisis-period changes in CAPM betas and increases in residual correlations around the Lehman episode. In addition, Lehkonen ([2015](https://arxiv.org/html/2601.21272v1#bib.bib2207 "Stock market integration and the globalfinancial crisis")) document regime-dependent changes in international stock-market integration during the GFC, suggesting that coefficients in FF multi-factor models may have shifted in that period. While the COVID-19 pandemic is widely viewed as a potential source of structural change in global equity markets, Ndako et al. ([2025](https://arxiv.org/html/2601.21272v1#bib.bib2198 "Structural breaks in global stock markets: are they caused by pandemics, protests or other factors?")) report that, using [Bai and Perron](https://arxiv.org/html/2601.21272v1#bib.bib1847 "Estimating and testing linear models with multiple structural changes")â€™s ([1998](https://arxiv.org/html/2601.21272v1#bib.bib1847 "Estimating and testing linear models with multiple structural changes"); [2003a](https://arxiv.org/html/2601.21272v1#bib.bib1848 "Computation and analysis of multiple structural change models"); [2003b](https://arxiv.org/html/2601.21272v1#bib.bib2211 "Critical values for multiple structural change tests")) multiple structural break tests, no additional break aligned with the onset of COVID-19 is detected across the countries and regions they analyze. Instead, their evidence points to the GFC as the dominant common event driving structural change in global stock market, and they conclude that the GFC generated the most pronounced breaks in those markets. Accordingly, our baseline sample spans October 2008 to November 2025.

(Table [10](https://arxiv.org/html/2601.21272v1#Sx1.T10 "Table 10 â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") around here)

Table [10](https://arxiv.org/html/2601.21272v1#Sx1.T10 "Table 10 â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") shows the descriptive statistics and the results of the unit root test for the returns on risk factors in the FF multifactor models. The descriptive statistics show that for all datasets, there are no inconsistent data in the perspective of the traditional mean-variance approach. In the estimations, each variable that appeared in the moment conditions should be stationary. We apply the augmented Dickeyâ€“Fuller (ADF) test of Said and Dickey ([1984](https://arxiv.org/html/2601.21272v1#bib.bib1426 "Testing for unit roots in autoregressive-moving average models of unknown order")) to check whether the variables satisfy the stationarity condition. The ADF test rejects the null hypothesis that each variable contains a unit root at the 5% significance level.666The ADF test also rejects the null hypothesis that the returns on the 6 and 25 benchmark portfolios contain a unit root at the 5% significance level, regardless of portfolio sorting method.

### 4.3 Empirical Results

In this subsection, we assess the FF multifactor models using a Wald test based on the generalized Durbin estimator and compare its performance with competing Wald tests based on the FGLS-Durbin and FGLS-CO estimators. To improve finite-sample size control, we also report a bootstrap bias-corrected Wald test based on the generalized Durbin estimator. Overall, Table [11](https://arxiv.org/html/2601.21272v1#Sx1.T11 "Table 11 â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") indicates that inference on H0:ğœ¶=ğŸH\_{0}:{\mbox{$\alpha$}}={\mbox{$0$}} can depend materially on the choice of the estimator underlying the Wald statistic, and that the competing FGLS-based Wald tests tend to reject more frequently, particularly when the number of benchmark portfolios is large, than the bootstrap bias-corrected Wald test based on the generalized Durbin estimator.

(Table [11](https://arxiv.org/html/2601.21272v1#Sx1.T11 "Table 11 â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") around here)

Table [11](https://arxiv.org/html/2601.21272v1#Sx1.T11 "Table 11 â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") reports the test results for the FF3 model. For MB6, none of the reported Wald statistics rejects the null hypothesis at the 1% significance level. For MB25, the conclusion depends on the test statistic: the bootstrap bias-corrected Wald test based on the generalized Durbin estimator rejects the null at the 5% level but not at the 1% level, whereas the FGLS-based Wald tests yield much smaller pp-values and reject decisively. This contrast indicates that, as the cross-sectional dimension increases, the strength of evidence against the FF3 model becomes more sensitive to the construction of the Wald test.

Table [11](https://arxiv.org/html/2601.21272v1#Sx1.T11 "Table 11 â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") also reports the results for the FF5 model and highlights an important interaction between portfolio sorting and test behavior. For benchmark portfolios sorted by MB, none of the statistics rejects the null at the 1% level for MB6, whereas for MB25 the bootstrap bias-corrected Wald test based on the generalized Durbin estimator rejects only at the 10% level while the competing FGLS-based Wald tests reject decisively. In contrast, when portfolios are sorted by MI, the bootstrap bias-corrected Wald test based on the generalized Durbin estimator does not reject the null for either MI6 or MI25, whereas the FGLS-based Wald tests reject for MI25. A similar pattern arises for portfolios sorted by MO: the null is not rejected at the 1% level for MO6 by any statistic, but for MO25 it is not rejected by the bootstrap bias-corrected Wald test based on the generalized Durbin estimator, whereas it is rejected by the FGLS-based Wald tests.

Taken together, these empirical patterns suggest that the bootstrap bias-corrected Wald test based on the generalized Durbin estimator delivers comparatively stable inference across portfolio sorts and is less prone to frequent rejections in higher-dimensional systems. By contrast, the FGLS-based Wald tests can produce substantially more aggressive rejections in the same settings. This contrast is consistent with our simulation evidence that misspecification in the dynamic dependence between regressors and errors can lead to size distortions for competing procedures, whereas the generalized Durbin approach remains reliable for multi-equation specification testing.

## 5 Conclusion

In this paper, we develop a new model specification test for multivariate regression. Our motivation is that commonly used procedures in empirical asset-pricing applications, such as the GRS test and the HAR test, can exhibit substantial size distortions and yield unreliable inference in empirically relevant settings, especially when the data depart from idealized distributional assumptions or when the system dimension is large.

Building on [Durbin](https://arxiv.org/html/2601.21272v1#bib.bib2135 "Testing for serial correlation in least-squares regression when some of the regressorsare lagged dependent variables")â€™s ([1970](https://arxiv.org/html/2601.21272v1#bib.bib2135 "Testing for serial correlation in least-squares regression when some of the regressorsare lagged dependent variables")) regression framework, we generalize the Durbin approach in two key respects. First, we reformulate it for multivariate seemingly unrelated regressions (SUR) so that cross-equation dependence is explicitly accommodated. This multivariate reformulation allows the test to exploit information in the full system and to remain well-defined in the presence of correlated disturbances across equations. Second, we reinterpret the Durbin correction not as an OLS-based auxiliary regression device, but as a GLS-class estimator. In particular, the proposed generalized Durbin estimator exploits cross-equation covariance and explicitly incorporates the joint second-order dynamics of regressors and disturbances, thereby targeting environments in which regressor-error dependence is dynamic rather than purely exogenous.

These features deliver two central theoretical implications. The generalized Durbin estimator remains consistent under the EBD condition, which allows dynamic dependence between regressors and errors, whereas competing estimators need not be consistent in this environment. Under stronger conditions, it attains GLS efficiency when the cross-equation covariance structure is correctly specified. Based on this estimator, we construct a Wald test for multi-equation specification testing. Because size distortions can still be non-negligible in finite samples, we also consider a bootstrap-based bias-corrected version of the Wald statistic to improve size control in practice.

Our Monte Carlo experiments provide systematic evidence on finite-sample size and test power. The proposed generalized Durbin based Wald test achieves near-nominal size with competitive power across a range of designs, and the bootstrap correction further improves finite-sample reliability. In contrast, conventional procedures can substantially over-reject: the GRS test is particularly sensitive to violations of normality, and HAR-type tests tend to become increasingly conservative or aggressive depending on bandwidth choices and, more importantly, to over-reject as the cross-sectional dimension grows. Taken together, these results underscore the value of explicitly modeling regressor-error dynamic dependence and cross-equation dependence when conducting multivariate specification tests.

Our empirical application illustrates the practical implications of these findings for evaluating the [Fama and French](https://arxiv.org/html/2601.21272v1#bib.bib1868 "Common risk factors in the returns on stocks and bonds")â€™s ([1993](https://arxiv.org/html/2601.21272v1#bib.bib1868 "Common risk factors in the returns on stocks and bonds"); [2015](https://arxiv.org/html/2601.21272v1#bib.bib1869 "A five-factor asset pricing model")) multifactor models. We show that inference on the pricing errors can depend materially on the choice of test statistic, especially in higher-dimensional systems and for certain portfolio sorts. In particular, the bootstrap-corrected generalized Durbin based Wald test delivers comparatively stable inference, while competing procedures can yield more aggressive rejections in the same settings. Overall, the proposed approach provides a reliable and practically useful tool for multi-equation specification testing in the presence of cross-equation dependence and regressor-error dynamic dependence, and it offers a principled alternative to conventional procedures that may be prone to size inflation.

## Acknowledgments

The authors thank Tirthatanmoy Das, Katsuhito Iwai, Akitada Kasahara, Genya Kobayashi, Daisuke Nagakura, Yohei Yamamoto, Tatsuma Wada, and participants at the 100th Annual Conference of the Western Economic Association International and the Japan Society of Monetary Economics 2025 Autumn Meeting for helpful comments and suggestions. The author also gratefully acknowledges financial support from the Japan Society for the Promotion of Science, Grant-in-Aid for Scientific Research (Grant Nos. 23H00838 and 23K25535), and from the Japan Science and Technology Agency, Moonshot Research and Development Program (Grant No. JPMJMS2215). All data and programs used in this paper are available upon request.

## References

* J. Affleck-Graves and B. McDonald (1989)
  Nonnormalities and tests of asset pricing theories.
  Journal of Finance 44 (4),  pp.Â 889â€“908.
  Cited by: [Â§1](https://arxiv.org/html/2601.21272v1#S1.p2.1 "1 Introduction â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§3.4.2](https://arxiv.org/html/2601.21272v1#S3.SS4.SSS2.p3.7 "3.4.2 Rejection Frequencies under the Alternative Hypothesis â€£ 3.4 Simulation Results â€£ 3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* D. W. K. Andrews (1991)
  Heteroskedasticity and autocorrelation consistent covariance matrix estimation.
  Econometrica 59 (3),  pp.Â 817â€“858.
  Cited by: [Â§1](https://arxiv.org/html/2601.21272v1#S1.p3.2 "1 Introduction â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* J. Bai and P. Perron (1998)
  Estimating and testing linear models with multiple structural changes.
  Econometrica 66 (1),  pp.Â 47â€“78.
  Cited by: [Â§4.2](https://arxiv.org/html/2601.21272v1#S4.SS2.p4.1 "4.2 Dataset â€£ 4 Empirical Application â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* J. Bai and P. Perron (2003a)
  Computation and analysis of multiple structural change models.
  Journal of Applied Econometrics 18 (1),  pp.Â 1â€“22.
  Cited by: [Â§4.2](https://arxiv.org/html/2601.21272v1#S4.SS2.p4.1 "4.2 Dataset â€£ 4 Empirical Application â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* J. Bai and P. Perron (2003b)
  Critical values for multiple structural change tests.
  Econometrics Journal 6 (1),  pp.Â 72â€“78.
  Cited by: [Â§4.2](https://arxiv.org/html/2601.21272v1#S4.SS2.p4.1 "4.2 Dataset â€£ 4 Empirical Application â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* R. T. Baillie, F. X. Diebold, G. Kapetanios, K. H. Kim, and A. Mora (2024)
  On robust inference in time-series regression.
  Econometrics Journal 28 (2),  pp.Â 131â€“173.
  Cited by: [Â§1](https://arxiv.org/html/2601.21272v1#S1.p4.1 "1 Introduction â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§2.2](https://arxiv.org/html/2601.21272v1#S2.SS2.p4.2 "2.2 Relationships among exogeneity conditions â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§2.3](https://arxiv.org/html/2601.21272v1#S2.SS3.p3.4 "2.3 The inconsistency of OLS and OLS-based GLS estimators in multi-equation models â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§2](https://arxiv.org/html/2601.21272v1#S2.p2.1 "2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§3.1](https://arxiv.org/html/2601.21272v1#S3.SS1.p4.3 "3.1 Simulation Design â€£ 3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§3.1](https://arxiv.org/html/2601.21272v1#S3.SS1.p8.5 "3.1 Simulation Design â€£ 3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§3.1](https://arxiv.org/html/2601.21272v1#S3.SS1.p9.16 "3.1 Simulation Design â€£ 3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§3.2](https://arxiv.org/html/2601.21272v1#S3.SS2.p1.1 "3.2 Estimation Accuracy â€£ 3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* B. H. Baltagi and G. Bresson (2011)
  Maximum likelihood estimation and lagrange multiplier tests for panelseemingly unrelated regressions with spatial lag and spatial errors: anapplication to hedonic housing prices in paris.
  Journal of Urban Economics 69 (1),  pp.Â 24â€“42.
  Cited by: [Â§1](https://arxiv.org/html/2601.21272v1#S1.p1.1 "1 Introduction â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* S. Barrios, L. Bertinelli, and E. Strobl (2010)
  Global inflation.
  Review of Economics and Statistics 92 (3),  pp.Â 524â€“535.
  Cited by: [Â§1](https://arxiv.org/html/2601.21272v1#S1.p2.1 "1 Introduction â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* M. Bekaert, M. Fratzscher, and A. Mehl (2014)
  The global crisis and equity market contagion.
  Journal of Finance 69 (6),  pp.Â 2597â€“2649.
  Cited by: [Â§4.2](https://arxiv.org/html/2601.21272v1#S4.SS2.p4.1 "4.2 Dataset â€£ 4 Empirical Application â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* R. Beran (1988)
  Prepivoting test statistics: a bootstrap view of asymptotic refinements.
  Journal of the American Statistical Association 83 (403),  pp.Â 687â€“697.
  Cited by: [Â§2.6](https://arxiv.org/html/2601.21272v1#S2.SS6.p3.2 "2.6 Bootstrap-corrected Wald test based on the generalized Durbin estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* J. Bernstein and M. I. Nadiri (1989)
  Research and development and intra-industry spillovers: an empirical application of dynamic duality.
  Review of Economic Studies 56 (2),  pp.Â 249â€“267.
  Cited by: [Â§1](https://arxiv.org/html/2601.21272v1#S1.p1.1 "1 Introduction â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* M. Billio, M. Getmansky, A. W. Lo, and L. Pelizzon (2012)
  Econometric measures of connectedness and systemic risk in the finance and insurance sectors.
  Journal of Financial Economics 104 (3),  pp.Â 535â€“559.
  Cited by: [Â§1](https://arxiv.org/html/2601.21272v1#S1.p2.1 "1 Introduction â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* S. B. Blomberg, G. D. Hess, and A. Orphanides (2004)
  The macroeconomic consequences of terrorism.
  Journal of Monetary Economics 51 (5),  pp.Â 1007â€“1032.
  Cited by: [Â§1](https://arxiv.org/html/2601.21272v1#S1.p2.1 "1 Introduction â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* J. G. Booth and P. Hall (1994)
  Monte carlo approximation and the iterated boostrap.
  Biometrika 81 (2),  pp.Â 331â€“340.
  Cited by: [Â§2.6](https://arxiv.org/html/2601.21272v1#S2.SS6.p4.4 "2.6 Bootstrap-corrected Wald test based on the generalized Durbin estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* T. S. Breusch and A. R. Pagan (1980)
  The lagrange multiplier test and its applications to model specification in econometrics.
  Review of Economic Studies 47 (1),  pp.Â 239â€“253.
  Cited by: [Â§1](https://arxiv.org/html/2601.21272v1#S1.p1.1 "1 Introduction â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§2](https://arxiv.org/html/2601.21272v1#S2.p1.1 "2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* P. J. Brockwell and R. A. Davis (1991)
  Time series: theory and methods.
  2nd edition, Springer.
  Cited by: [Â§A.1](https://arxiv.org/html/2601.21272v1#Sx2.SS1.p1.21 "A.1 Proof of Proposition 1 â€£ Appendix â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* P. BÃ¼hlmann (1997)
  Sieve bootstrap for time series.
  Bernoulli 3 (2),  pp.Â 123â€“148.
  Cited by: [Â§2.6](https://arxiv.org/html/2601.21272v1#S2.SS6.p2.1 "2.6 Bootstrap-corrected Wald test based on the generalized Durbin estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* N. Cakici, F. J. Fabozzi, and S. Tan (2013)
  Size, value, and momentum in emerging market stock returns.
  Emerging Markets Review 16,  pp.Â 46â€“65.
  Cited by: [Â§3.3.1](https://arxiv.org/html/2601.21272v1#S3.SS3.SSS1.p1.6 "3.3.1 GRS test â€£ 3.3 Size and power of tests â€£ 3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§4](https://arxiv.org/html/2601.21272v1#S4.p2.5 "4 Empirical Application â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* D. Cochrane and G.H. Orcutt (1949)
  Application of least squares regression to relationships containing auto-correlated error terms.
  Journal of the American Statistical Association 44 (245),  pp.Â 32â€“61.
  Cited by: [Â§2.2](https://arxiv.org/html/2601.21272v1#S2.SS2.p8.4 "2.2 Relationships among exogeneity conditions â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§2.3](https://arxiv.org/html/2601.21272v1#S2.SS3.p8.3 "2.3 The inconsistency of OLS and OLS-based GLS estimators in multi-equation models â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§3.3.3](https://arxiv.org/html/2601.21272v1#S3.SS3.SSS3.p1.1 "3.3.3 Wald test based on the FGLS-CO estimator â€£ 3.3 Size and power of tests â€£ 3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* R. Cont (2001)
  Empirical properties of asset returns: stylized facts and statistical issues.
  Quantitative Finance 1 (2),  pp.Â 223â€“236.
  Cited by: [Â§1](https://arxiv.org/html/2601.21272v1#S1.p2.1 "1 Introduction â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* R. Davidson and J. G. MacKinnon (1999)
  The size distortion of bootstrap tests.
  Econometric Theory 15 (3),  pp.Â 361â€“376.
  Cited by: [Â§2.6](https://arxiv.org/html/2601.21272v1#S2.SS6.p2.1 "2.6 Bootstrap-corrected Wald test based on the generalized Durbin estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* R. Davidson and J. G. MacKinnon (2007)
  Improving the reliability of bootstrap tests with the fast double bootstrap.
  Computational Statistics & Data Analysis 51 (7),  pp.Â 3259â€“3281.
  Cited by: [Â§2.6](https://arxiv.org/html/2601.21272v1#S2.SS6.p4.4 "2.6 Bootstrap-corrected Wald test based on the generalized Durbin estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* W. J. Den Haan and A. Levin (1997)
  A practitionerâ€™s guide to robust covariance matrix estimation,.
  In Robust Inference,
  Handbook of Statistics, Vol. 15,  pp.Â 299â€“342.
  Cited by: [Â§1](https://arxiv.org/html/2601.21272v1#S1.p3.2 "1 Introduction â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* F. X. Diebold and K. Yilmaz (2009)
  Measuring financial asset return and volatility spillovers, with application to global equity markets.
  Economic Journal 119 (534),  pp.Â 158â€“171.
  Cited by: [Â§1](https://arxiv.org/html/2601.21272v1#S1.p2.1 "1 Introduction â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* J. Durbin (1970)
  Testing for serial correlation in least-squares regression when some of the regressorsare lagged dependent variables.
  Econometrica 38 (3),  pp.Â 410â€“421.
  Cited by: [Â§1](https://arxiv.org/html/2601.21272v1#S1.p4.1 "1 Introduction â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§2](https://arxiv.org/html/2601.21272v1#S2.p2.1 "2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§5](https://arxiv.org/html/2601.21272v1#S5.p2.1 "5 Conclusion â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* E. F. Fama and K. R. French (1993)
  Common risk factors in the returns on stocks and bonds.
  Journal of Financial Economics 33 (1),  pp.Â 3â€“56.
  Cited by: [Â§1](https://arxiv.org/html/2601.21272v1#S1.p4.1 "1 Introduction â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§1](https://arxiv.org/html/2601.21272v1#S1.p5.1 "1 Introduction â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§2.1](https://arxiv.org/html/2601.21272v1#S2.SS1.p8.2 "2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§3.1](https://arxiv.org/html/2601.21272v1#S3.SS1.p1.1 "3.1 Simulation Design â€£ 3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§3.3.1](https://arxiv.org/html/2601.21272v1#S3.SS3.SSS1.p1.6 "3.3.1 GRS test â€£ 3.3 Size and power of tests â€£ 3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§4.1](https://arxiv.org/html/2601.21272v1#S4.SS1.p1.17 "4.1 Famaâ€“French Multifactor Models â€£ 4 Empirical Application â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§4.2](https://arxiv.org/html/2601.21272v1#S4.SS2.p1.1 "4.2 Dataset â€£ 4 Empirical Application â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§4](https://arxiv.org/html/2601.21272v1#S4.p2.5 "4 Empirical Application â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§5](https://arxiv.org/html/2601.21272v1#S5.p5.1 "5 Conclusion â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* E. F. Fama and K. R. French (2012)
  Size, value, and momentum in international stock returns.
  Journal of Financial Economics 105 (3),  pp.Â 457â€“472.
  Cited by: [Â§4.2](https://arxiv.org/html/2601.21272v1#S4.SS2.p3.1 "4.2 Dataset â€£ 4 Empirical Application â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* E. F. Fama and K. R. French (2015)
  A five-factor asset pricing model.
  Journal of Financial Economics 116 (1),  pp.Â 1â€“22.
  Cited by: [Â§1](https://arxiv.org/html/2601.21272v1#S1.p5.1 "1 Introduction â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§2.1](https://arxiv.org/html/2601.21272v1#S2.SS1.p8.2 "2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§3.1](https://arxiv.org/html/2601.21272v1#S3.SS1.p1.1 "3.1 Simulation Design â€£ 3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§3.3.1](https://arxiv.org/html/2601.21272v1#S3.SS3.SSS1.p1.6 "3.3.1 GRS test â€£ 3.3 Size and power of tests â€£ 3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§4.1](https://arxiv.org/html/2601.21272v1#S4.SS1.p1.12 "4.1 Famaâ€“French Multifactor Models â€£ 4 Empirical Application â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§4.1](https://arxiv.org/html/2601.21272v1#S4.SS1.p1.17 "4.1 Famaâ€“French Multifactor Models â€£ 4 Empirical Application â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§4.2](https://arxiv.org/html/2601.21272v1#S4.SS2.p1.1 "4.2 Dataset â€£ 4 Empirical Application â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§4](https://arxiv.org/html/2601.21272v1#S4.p2.5 "4 Empirical Application â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§5](https://arxiv.org/html/2601.21272v1#S5.p5.1 "5 Conclusion â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* E. F. Fama and K. R. French (2016)
  Dissecting anomalies with a five-factor model.
  Review of Financial Studies 29 (1),  pp.Â 69â€“103.
  Cited by: [Â§2.1](https://arxiv.org/html/2601.21272v1#S2.SS1.p8.2 "2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§3.3.1](https://arxiv.org/html/2601.21272v1#S3.SS3.SSS1.p1.6 "3.3.1 GRS test â€£ 3.3 Size and power of tests â€£ 3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§4](https://arxiv.org/html/2601.21272v1#S4.p2.5 "4 Empirical Application â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* E. F. Fama and K. R. French (2017)
  International tests of a five-factor asset pricing model.
  Journal of Financial Economics 123 (3),  pp.Â 441â€“463.
  Cited by: [Â§3.3.1](https://arxiv.org/html/2601.21272v1#S3.SS3.SSS1.p1.6 "3.3.1 GRS test â€£ 3.3 Size and power of tests â€£ 3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§4.2](https://arxiv.org/html/2601.21272v1#S4.SS2.p3.1 "4.2 Dataset â€£ 4 Empirical Application â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§4](https://arxiv.org/html/2601.21272v1#S4.p2.5 "4 Empirical Application â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* E. F. Fama and K. R. French (2018)
  Choosing factors.
  Journal of Financial Economics 128 (2),  pp.Â 234â€“252.
  Cited by: [Â§3.3.1](https://arxiv.org/html/2601.21272v1#S3.SS3.SSS1.p1.6 "3.3.1 GRS test â€£ 3.3 Size and power of tests â€£ 3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§4](https://arxiv.org/html/2601.21272v1#S4.p2.5 "4 Empirical Application â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* E. F. Fama and K. R. French (2020)
  Comparing cross-section and time-seriesfactor models.
  Review of Financial Studies 33 (5),  pp.Â 1891â€“1926.
  Cited by: [Â§3.3.1](https://arxiv.org/html/2601.21272v1#S3.SS3.SSS1.p1.6 "3.3.1 GRS test â€£ 3.3 Size and power of tests â€£ 3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§4](https://arxiv.org/html/2601.21272v1#S4.p2.5 "4 Empirical Application â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* M. R. Gibbons, S. A. Ross, and J. Shanken (1989)
  A test of the efficiency of a given portfolio.
  Econometrica 57 (5),  pp.Â 1121â€“1152.
  Cited by: [Â§1](https://arxiv.org/html/2601.21272v1#S1.p2.1 "1 Introduction â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§3.3.1](https://arxiv.org/html/2601.21272v1#S3.SS3.SSS1.p1.6 "3.3.1 GRS test â€£ 3.3 Size and power of tests â€£ 3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§3.3](https://arxiv.org/html/2601.21272v1#S3.SS3.p1.1 "3.3 Size and power of tests â€£ 3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§3.4](https://arxiv.org/html/2601.21272v1#S3.SS4.p2.8 "3.4 Simulation Results â€£ 3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* B. Hansen (2022)
  Econometrics.
   Princeton University Press, New Jersey, U.S..
  Cited by: [footnote 1](https://arxiv.org/html/2601.21272v1#footnote1 "In item (A1.2) â€£ Assumption 1 (Finite-predictor exactness at lag ğ‘â‚€): â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* C. R. Harvey and A. Siddique (2002)
  Conditional skewness in asset pricing tests.
  Journal of Finance 55 (3),  pp.Â 1263â€“1295.
  Cited by: [Â§1](https://arxiv.org/html/2601.21272v1#S1.p2.1 "1 Introduction â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* W. Huang, Q. Liu, G. Rhee, and F. Wu (2012)
  Extreme downside risk and expected stock returns.
  Journal of Banking & Finance 36 (5),  pp.Â 1492â€“1502.
  Cited by: [Â§1](https://arxiv.org/html/2601.21272v1#S1.p2.1 "1 Introduction â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* A. B. Jaffe (1989)
  Technological opportunity and spillovers of r&d: evidence from firmsâ€™ patents,profits, and market value.
  American Economic Review 76 (5),  pp.Â 984â€“1001.
  Cited by: [Â§1](https://arxiv.org/html/2601.21272v1#S1.p1.1 "1 Introduction â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* M. Jansson (2004)
  The error in rejection probability of simple autocorrelation robust tests.
  Econometrica 72 (3),  pp.Â 937â€“946.
  Cited by: [Â§3.3.2](https://arxiv.org/html/2601.21272v1#S3.SS3.SSS2.p3.9 "3.3.2 HAR test â€£ 3.3 Size and power of tests â€£ 3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* M. J. Kamstra and R. Shi (2024)
  Testing and ranking of asset pricing models using the grs statistic.
  Journal of Risk and Financial Management 17 (4).
  Note: 168
  Cited by: [footnote 5](https://arxiv.org/html/2601.21272v1#footnote5 "In 3.3.1 GRS test â€£ 3.3 Size and power of tests â€£ 3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* N. M. Kiefer, T. J. Vogelsang, and H. Bunzel (2000)
  Simple robust testing of regression hypotheses.
  Econometrica 68 (3),  pp.Â 695â€“714.
  Cited by: [Â§1](https://arxiv.org/html/2601.21272v1#S1.p3.2 "1 Introduction â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§3.3.2](https://arxiv.org/html/2601.21272v1#S3.SS3.SSS2.p2.3 "3.3.2 HAR test â€£ 3.3 Size and power of tests â€£ 3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* N. M. Kiefer and T. J. Vogelsang (2002a)
  Heteroskedasticity-autocorrelation robust standard errors using the bartlett kernel without truncation.
  Econometrica 70 (5),  pp.Â 2093â€“2095.
  Cited by: [Â§1](https://arxiv.org/html/2601.21272v1#S1.p3.2 "1 Introduction â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§3.3.2](https://arxiv.org/html/2601.21272v1#S3.SS3.SSS2.p3.9 "3.3.2 HAR test â€£ 3.3 Size and power of tests â€£ 3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* N. M. Kiefer and T. J. Vogelsang (2002b)
  Heteroskedasticity-autocorrelation robust testing using bandwidth equal to sample size.
  Econometric Theory 18 (6),  pp.Â 1350â€“1366.
  Cited by: [Â§1](https://arxiv.org/html/2601.21272v1#S1.p3.2 "1 Introduction â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* N. M. Kiefer and T. J. Vogelsang (2005)
  A new asymptotic theory for heteroskedasticity-autocorrelation robust tests.
  Econometric Theory 21 (6),  pp.Â 1130â€“1164.
  Cited by: [Â§1](https://arxiv.org/html/2601.21272v1#S1.p3.2 "1 Introduction â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§3.3.2](https://arxiv.org/html/2601.21272v1#S3.SS3.SSS2.p3.9 "3.3.2 HAR test â€£ 3.3 Size and power of tests â€£ 3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* E. Lazarus, D. J. Lewis, J. H. Stock, and M. W. Watson (2018)
  HAR inference: recommendations for practice.
  Journal of Business & Economic Statistics 36 (4),  pp.Â 541â€“559.
  Cited by: [Â§1](https://arxiv.org/html/2601.21272v1#S1.p3.2 "1 Introduction â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§3.3.2](https://arxiv.org/html/2601.21272v1#S3.SS3.SSS2.p3.9 "3.3.2 HAR test â€£ 3.3 Size and power of tests â€£ 3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§3.3](https://arxiv.org/html/2601.21272v1#S3.SS3.p1.1 "3.3 Size and power of tests â€£ 3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§3.4](https://arxiv.org/html/2601.21272v1#S3.SS4.p2.8 "3.4 Simulation Results â€£ 3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* E. Lazarus, D. J. Lewis, and J. H. Stock (2021)
  The size-power tradeoff in har inference.
  Econometrica 89 (5),  pp.Â 2497â€“2516.
  Cited by: [Â§1](https://arxiv.org/html/2601.21272v1#S1.p3.2 "1 Introduction â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§3.3.2](https://arxiv.org/html/2601.21272v1#S3.SS3.SSS2.p3.9 "3.3.2 HAR test â€£ 3.3 Size and power of tests â€£ 3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§3.3](https://arxiv.org/html/2601.21272v1#S3.SS3.p1.1 "3.3 Size and power of tests â€£ 3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§3.4.2](https://arxiv.org/html/2601.21272v1#S3.SS4.SSS2.p4.7 "3.4.2 Rejection Frequencies under the Alternative Hypothesis â€£ 3.4 Simulation Results â€£ 3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§3.4](https://arxiv.org/html/2601.21272v1#S3.SS4.p2.8 "3.4 Simulation Results â€£ 3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* H. Lehkonen (2015)
  Stock market integration and the globalfinancial crisis.
  Review of Finance 19 (5),  pp.Â 2039â€“2094.
  Cited by: [Â§4.2](https://arxiv.org/html/2601.21272v1#S4.SS2.p4.1 "4.2 Dataset â€£ 4 Empirical Application â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* J. Lewellen, S. Nagel, and J. Shanken (2010)
  A skeptical appraisal of asset pricing tests.
  Journal of Financial Economics 96 (2),  pp.Â 175â€“194.
  Cited by: [Â§4.2](https://arxiv.org/html/2601.21272v1#S4.SS2.p2.1 "4.2 Dataset â€£ 4 Empirical Application â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* J. Lintner (1965)
  The valuation of risky assets and the selection of risky investments in stock portfolios and budget constraints.
  Review of Economics and Statistics 47 (1),  pp.Â 13â€“37.
  Cited by: [Â§1](https://arxiv.org/html/2601.21272v1#S1.p4.1 "1 Introduction â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§2.1](https://arxiv.org/html/2601.21272v1#S2.SS1.p8.2 "2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* H. LÃ¼tkepohl (2005)
  New introduction to multiple time series analysis.
   Springer, Berlin, Germany.
  Cited by: [Â§2.1](https://arxiv.org/html/2601.21272v1#S2.SS1.p4.7 "2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* J. D. MacKinnon (2006)
  Bootstrap methods in econometrics.
  The Economic Record 82 (s1),  pp.Â S2â€“S18.
  Cited by: [Â§2.6](https://arxiv.org/html/2601.21272v1#S2.SS6.p2.1 "2.6 Bootstrap-corrected Wald test based on the generalized Durbin estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* S. Martin, N. Rice, R. Jacobs, and P. Smith (2007)
  The market for elective surgery: joint estimation of supply and demand.
  Journal of Health Economics 26 (2),  pp.Â 263â€“285.
  Cited by: [Â§1](https://arxiv.org/html/2601.21272v1#S1.p1.1 "1 Introduction â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* K. Moriya and A. Noda (2024)
  Time instability of the fama-french multifactor models: an international evidence.
  Note: [arXiv:2208.01270], Available at https://arxiv.org/abs/2208.01270
  Cited by: [Â§4](https://arxiv.org/html/2601.21272v1#S4.p2.5 "4 Empirical Application â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* U. MÃ¼ller (2014)
  HAC corrections for strongly autocorrelatedtime series.
  Journal of Business & Economic Statistics 32 (3),  pp.Â 311â€“322.
  Cited by: [Â§1](https://arxiv.org/html/2601.21272v1#S1.p3.2 "1 Introduction â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* D. Nagakura (2024)
  Cochraneâ€“orcutt type estimator for multivariate linear regression model with serially correlated errors.
  Note: Available at SSRN: https://ssrn.com/abstract=4951695
  Cited by: [Â§2.2](https://arxiv.org/html/2601.21272v1#S2.SS2.p8.4 "2.2 Relationships among exogeneity conditions â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§2.3](https://arxiv.org/html/2601.21272v1#S2.SS3.p8.3 "2.3 The inconsistency of OLS and OLS-based GLS estimators in multi-equation models â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§3.3.3](https://arxiv.org/html/2601.21272v1#S3.SS3.SSS3.p1.1 "3.3.3 Wald test based on the FGLS-CO estimator â€£ 3.3 Size and power of tests â€£ 3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§3.3](https://arxiv.org/html/2601.21272v1#S3.SS3.p1.1 "3.3 Size and power of tests â€£ 3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* J. A. Ndako, T. T. Kumeka, F. F. Adedoyin, and A. Asongu (2025)
  Structural breaks in global stock markets: are they caused by pandemics, protests or other factors?.
  Transnational Corporations Review 17.
  Note: 200147
  Cited by: [Â§4.2](https://arxiv.org/html/2601.21272v1#S4.SS2.p4.1 "4.2 Dataset â€£ 4 Empirical Application â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* W. K. Newey and K. D. West (1987)
  A simple, positive semi-definite, heteroskedasticity and autocorrelation consistent covariance matrix.
  Econometrica 55 (3),  pp.Â 703â€“708.
  Cited by: [Â§1](https://arxiv.org/html/2601.21272v1#S1.p3.2 "1 Introduction â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§2](https://arxiv.org/html/2601.21272v1#S2.p1.1 "2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§3.3.2](https://arxiv.org/html/2601.21272v1#S3.SS3.SSS2.p1.6 "3.3.2 HAR test â€£ 3.3 Size and power of tests â€£ 3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* W. K. Newey and K. D. West (1994)
  Automatic lag selection in covariance matrix estimation.
  Review of Economic Studies 61 (4),  pp.Â 631â€“653.
  Cited by: [Â§1](https://arxiv.org/html/2601.21272v1#S1.p3.2 "1 Introduction â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§2](https://arxiv.org/html/2601.21272v1#S2.p1.1 "2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* K. Peremans and S. V. Aelst (2018)
  Robust iference for semingly unrelated regression models.
  Journal of Multivariate Analysis 167,  pp.Â 212â€“224.
  Cited by: [Â§1](https://arxiv.org/html/2601.21272v1#S1.p1.1 "1 Introduction â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* P. Perron and E. GonzÃ¡lez-Coya (2022)
  Feasible gls for time series regression.
  Note: Working Paper. Department of Economics, Boston University.
  Cited by: [Â§2.2](https://arxiv.org/html/2601.21272v1#S2.SS2.p3.2 "2.2 Relationships among exogeneity conditions â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§2.2](https://arxiv.org/html/2601.21272v1#S2.SS2.p3.5 "2.2 Relationships among exogeneity conditions â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§2.2](https://arxiv.org/html/2601.21272v1#S2.SS2.p9.11 "2.2 Relationships among exogeneity conditions â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§2.3](https://arxiv.org/html/2601.21272v1#S2.SS3.p12.4 "2.3 The inconsistency of OLS and OLS-based GLS estimators in multi-equation models â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§2.3](https://arxiv.org/html/2601.21272v1#S2.SS3.p3.4 "2.3 The inconsistency of OLS and OLS-based GLS estimators in multi-equation models â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§2.4](https://arxiv.org/html/2601.21272v1#S2.SS4.p1.1 "2.4 The inconsistency of FGLS-D estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§2.4](https://arxiv.org/html/2601.21272v1#S2.SS4.p3.2 "2.4 The inconsistency of FGLS-D estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§3.2](https://arxiv.org/html/2601.21272v1#S3.SS2.p8.8 "3.2 Estimation Accuracy â€£ 3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§3.3.4](https://arxiv.org/html/2601.21272v1#S3.SS3.SSS4.p1.4 "3.3.4 Wald test based on the FGLS-D estimator â€£ 3.3 Size and power of tests â€£ 3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§3.3](https://arxiv.org/html/2601.21272v1#S3.SS3.p1.1 "3.3 Size and power of tests â€£ 3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§3.4.1](https://arxiv.org/html/2601.21272v1#S3.SS4.SSS1.p4.9 "3.4.1 Rejection Frequencies under the Null Hypothesis â€£ 3.4 Simulation Results â€£ 3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§3.4.2](https://arxiv.org/html/2601.21272v1#S3.SS4.SSS2.p6.3 "3.4.2 Rejection Frequencies under the Alternative Hypothesis â€£ 3.4 Simulation Results â€£ 3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [footnote 3](https://arxiv.org/html/2601.21272v1#footnote3 "In Remark 2 (Innovation-based exogeneity and its location): â€£ 2.2 Relationships among exogeneity conditions â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* M. H. Pesaran, T. Schuermann, and S. M. Weiner (2004)
  Modeling regional interdependencies using a global error-correcting macroeconometric model.
  Journal of Business & Economic Statistics 22 (2),  pp.Â 129â€“162.
  Cited by: [Â§1](https://arxiv.org/html/2601.21272v1#S1.p2.1 "1 Introduction â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* M. H. Pesaran and T. Yamagata (2008)
  Testing slope homogeneity in large panels.
  Journal of Econometrics 142 (1),  pp.Â 50â€“93.
  Cited by: [Â§1](https://arxiv.org/html/2601.21272v1#S1.p1.1 "1 Introduction â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§2](https://arxiv.org/html/2601.21272v1#S2.p1.1 "2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* M. H. Pesaran (2006)
  Estimation and inference in large heterogeneous panels with a multifactor error structure.
  Econometrica 74 (4),  pp.Â 863â€“1159.
  Cited by: [Â§1](https://arxiv.org/html/2601.21272v1#S1.p1.1 "1 Introduction â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* P. C. B. Phillips, Y. Sun, and S. Jin (2006)
  Spectral density estimation and robust hypothesis testing using steep origin kernels without truncation.
  International Economic Review 47 (3),  pp.Â 837â€“894.
  Cited by: [Â§1](https://arxiv.org/html/2601.21272v1#S1.p3.2 "1 Introduction â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* P. C. B. Phillips, Y. Sun, and S. Jin (2007)
  Long run variance estimation and robust regression testing using sharp origin kernels with no truncation.
  Journal of Statistical Planning and Inference 137 (3),  pp.Â 985â€“1023.
  Cited by: [Â§1](https://arxiv.org/html/2601.21272v1#S1.p3.2 "1 Introduction â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* S. Ray, N. E. Savin, and A. Tiwari (2009)
  Testing the capm revisited.
  Journal of Empirical Finance 16 (5),  pp.Â 721â€“733.
  Cited by: [Â§1](https://arxiv.org/html/2601.21272v1#S1.p4.1 "1 Introduction â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§2](https://arxiv.org/html/2601.21272v1#S2.p1.1 "2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* S. Ray and N. E. Savin (2008)
  The performance of heteroskedasticity and autocorrelation robust tests: a monte carlo study with an application to the three-factor fama: french asset-pricing model.
  Journal of Applied Econometrics 23 (1),  pp.Â 91â€“109.
  Cited by: [Â§1](https://arxiv.org/html/2601.21272v1#S1.p4.1 "1 Introduction â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§2](https://arxiv.org/html/2601.21272v1#S2.p1.1 "2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* S.A. Ross (1976)
  The arbitrage theory of capital asset pricing.
  Journal of Economic Theory 13 (3),  pp.Â 341â€“360.
  Cited by: [Â§2.1](https://arxiv.org/html/2601.21272v1#S2.SS1.p8.2 "2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§3.1](https://arxiv.org/html/2601.21272v1#S3.SS1.p1.1 "3.1 Simulation Design â€£ 3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* S.E. Said and D.A. Dickey (1984)
  Testing for unit roots in autoregressive-moving average models of unknown order.
  Biometrika 71 (3),  pp.Â 599â€“607.
  Cited by: [Â§4.2](https://arxiv.org/html/2601.21272v1#S4.SS2.p6.1 "4.2 Dataset â€£ 4 Empirical Application â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* G. Schwarz (1978)
  Estimating the dimension of a model.
  Annals of Statistics 6 (2),  pp.Â 461â€“464.
  Cited by: [Â§2.5](https://arxiv.org/html/2601.21272v1#S2.SS5.p6.2 "2.5 Asymptotic properties of generalized Durbin estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* W. F. Sharpe (1964)
  Capital asset prices: a theory of market equilibrium under conditions of risk.
  Journal of Finance 19 (3),  pp.Â 425â€“442.
  Cited by: [Â§1](https://arxiv.org/html/2601.21272v1#S1.p4.1 "1 Introduction â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§2.1](https://arxiv.org/html/2601.21272v1#S2.SS1.p8.2 "2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* C. A. Sims (1980)
  Macroeconomics and reality.
  Econometrica 48 (1),  pp.Â 1â€“48.
  Cited by: [Â§2.1](https://arxiv.org/html/2601.21272v1#S2.SS1.p4.7 "2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* J. H. Stock and M. W. Watson (2001)
  Vector autoregressions.
  Journal of Economic Perspectives 15 (4),  pp.Â 101â€“115.
  Cited by: [Â§2.1](https://arxiv.org/html/2601.21272v1#S2.SS1.p4.7 "2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* J. H. Stock and M. W. Watson (2019)
  Introduction to econometrics.
  Fourth Edition edition, Pearson.
  Cited by: [Â§2.2](https://arxiv.org/html/2601.21272v1#S2.SS2.p10.11 "2.2 Relationships among exogeneity conditions â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§2.2](https://arxiv.org/html/2601.21272v1#S2.SS2.p2.5 "2.2 Relationships among exogeneity conditions â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§2.3](https://arxiv.org/html/2601.21272v1#S2.SS3.p3.4 "2.3 The inconsistency of OLS and OLS-based GLS estimators in multi-equation models â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* Y. Sun, P. C. B. Phillips, and S. Jin (2008)
  Optimal bandwidth selection in heteroskedasticity-autocorrelation robust testing.
  Econometrica 76 (1),  pp.Â 175â€“194.
  Cited by: [Â§1](https://arxiv.org/html/2601.21272v1#S1.p3.2 "1 Introduction â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* Y. Sun (2014)
  Letâ€™s fix it: fixed-b aymptotics versus small-b asymptotics in heteroskedasticity and autocorrelation robust inference.
  Journal of Econometrics 178 (3),  pp.Â 659â€“677.
  Cited by: [Â§3.3.2](https://arxiv.org/html/2601.21272v1#S3.SS3.SSS2.p2.6 "3.3.2 HAR test â€£ 3.3 Size and power of tests â€£ 3 Simulation Experiment â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* A. Zellner (1962)
  An efficient method of estimating seemingly unrelated regressions and tests foraggregation bias.
  Journal of the American Statistical Association 57 (298),  pp.Â 348â€“368.
  Cited by: [Â§1](https://arxiv.org/html/2601.21272v1#S1.p5.1 "1 Introduction â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§2](https://arxiv.org/html/2601.21272v1#S2.p1.1 "2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* D. Zhang, M. Hu, and Q. Ji (2020)
  Financial markets under the global pandemic of covid-19.
  Finance Research Letters 36.
  Note: 101528
  Cited by: [Â§1](https://arxiv.org/html/2601.21272v1#S1.p2.1 "1 Introduction â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* G. Zhou (1991)
  Small sample tests of portfolio efficiency.
  Journal of Financial Economics 30 (1),  pp.Â 165â€“191.
  Cited by: [Â§1](https://arxiv.org/html/2601.21272v1#S1.p1.1 "1 Introduction â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),
  [Â§2](https://arxiv.org/html/2601.21272v1#S2.p1.1 "2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").
* G. Zhou (1993)
  Asset-pricing tests under alternative distributions.
  Journal of Finance 48 (5),  pp.Â 1927â€“1942.
  Cited by: [Â§1](https://arxiv.org/html/2601.21272v1#S1.p2.1 "1 Introduction â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").

Table 1: Bias and MSE under the Bâ€‹DBD condition

|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
|  |  |  | bias^â€‹(ğœ¿^T)\widehat{\mathrm{bias}}(\widehat{{\mbox{$\kappa$}}}\_{T}) | | | | |  | MSE^â€‹(ğœ¿^T)\widehat{\mathrm{MSE}}(\widehat{{\mbox{$\kappa$}}}\_{T}) | | | | |  |
|  |  |  | BC-GD | GD | FGLS-D | FGLS-CO | OLS |  | BC-GD | GD | FGLS-D | FGLS-CO | OLS |  |
| N=5/K=2N=5/K=2 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  | T=100T=100 |  | 0.01170.0117 | 0.01190.0119 | 0.01110.0111 | 0.01100.0110 | 0.01260.0126 |  | 0.20090.2009 | 0.20230.2023 | 0.16580.1658 | 0.16470.1647 | 0.20450.2045 |  |
|  | T=200T=200 |  | 0.00790.0079 | 0.01010.0101 | 0.00950.0095 | 0.00950.0095 | 0.01220.0122 |  | 0.09780.0978 | 0.09670.0967 | 0.07800.0780 | 0.07780.0778 | 0.10100.1010 |  |
|  | T=400T=400 |  | 0.00560.0056 | 0.00510.0051 | 0.00490.0049 | 0.00490.0049 | 0.00520.0052 |  | 0.04640.0464 | 0.04610.0461 | 0.03770.0377 | 0.03770.0377 | 0.04930.0493 |  |
|  | T=800T=800 |  | 0.00380.0038 | 0.00370.0037 | 0.00340.0034 | 0.00340.0034 | 0.00510.0051 |  | 0.02390.0239 | 0.02390.0239 | 0.01920.0192 | 0.01920.0192 | 0.02560.0256 |  |
| N=5/K=4N=5/K=4 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  | T=100T=100 |  | 0.01880.0188 | 0.01680.0168 | 0.01440.0144 | 0.01410.0141 | 0.01570.0157 |  | 0.33830.3383 | 0.33960.3396 | 0.25840.2584 | 0.25710.2571 | 0.34660.3466 |  |
|  | T=200T=200 |  | 0.01180.0118 | 0.01080.0108 | 0.00920.0092 | 0.00910.0091 | 0.01000.0100 |  | 0.15880.1588 | 0.15840.1584 | 0.12180.1218 | 0.12160.1216 | 0.16690.1669 |  |
|  | T=400T=400 |  | 0.00900.0090 | 0.00930.0093 | 0.00780.0078 | 0.00780.0078 | 0.00940.0094 |  | 0.07490.0749 | 0.07500.0750 | 0.05850.0585 | 0.05840.0584 | 0.08200.0820 |  |
|  | T=800T=800 |  | 0.00860.0086 | 0.00750.0075 | 0.00660.0066 | 0.00660.0066 | 0.00790.0079 |  | 0.03800.0380 | 0.03790.0379 | 0.02910.0291 | 0.02910.0291 | 0.04150.0415 |  |
| N=10/K=2N=10/K=2 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  | T=100T=100 |  | 0.01670.0167 | 0.02290.0229 | 0.01730.0173 | 0.01710.0171 | 0.01740.0174 |  | 0.39470.3947 | 0.39060.3906 | 0.31580.3158 | 0.30960.3096 | 0.38390.3839 |  |
|  | T=200T=200 |  | 0.01390.0139 | 0.01460.0146 | 0.01460.0146 | 0.01440.0144 | 0.01460.0146 |  | 0.18180.1818 | 0.18050.1805 | 0.14380.1438 | 0.14330.1433 | 0.18680.1868 |  |
|  | T=400T=400 |  | 0.01010.0101 | 0.00970.0097 | 0.00830.0083 | 0.00830.0083 | 0.00980.0098 |  | 0.08770.0877 | 0.08720.0872 | 0.06980.0698 | 0.06970.0697 | 0.09220.0922 |  |
|  | T=800T=800 |  | 0.00650.0065 | 0.00610.0061 | 0.00510.0051 | 0.00510.0051 | 0.00700.0070 |  | 0.04430.0443 | 0.04400.0440 | 0.03510.0351 | 0.03510.0351 | 0.04670.0467 |  |
| N=10/K=4N=10/K=4 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  | T=100T=100 |  | 0.02110.0211 | 0.02600.0260 | 0.02270.0227 | 0.02270.0227 | 0.02480.0248 |  | 0.70760.7076 | 0.70580.7058 | 0.54010.5401 | 0.52410.5241 | 0.67000.6700 |  |
|  | T=200T=200 |  | 0.01930.0193 | 0.01800.0180 | 0.01660.0166 | 0.01650.0165 | 0.01730.0173 |  | 0.31210.3121 | 0.31080.3108 | 0.23690.2369 | 0.23530.2353 | 0.31970.3197 |  |
|  | T=400T=400 |  | 0.01310.0131 | 0.01320.0132 | 0.01030.0103 | 0.01040.0104 | 0.01200.0120 |  | 0.14970.1497 | 0.14920.1492 | 0.11340.1134 | 0.11320.1132 | 0.15830.1583 |  |
|  | T=800T=800 |  | 0.00700.0070 | 0.00810.0081 | 0.00700.0070 | 0.00700.0070 | 0.00800.0080 |  | 0.07340.0734 | 0.07320.0732 | 0.05590.0559 | 0.05590.0559 | 0.07970.0797 |  |

Note: R version 4.5.2 was used to compute the statistics.

Table 2: Bias and MSE under the Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG condition

|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
|  |  |  | bias^â€‹(ğœ¿^T)\widehat{\mathrm{bias}}(\widehat{{\mbox{$\kappa$}}}\_{T}) | | | | |  | MSE^â€‹(ğœ¿^T)\widehat{\mathrm{MSE}}(\widehat{{\mbox{$\kappa$}}}\_{T}) | | | | |  |
|  |  |  | BC-GD | GD | FGLS-D | FGLS-CO | OLS |  | BC-GD | GD | FGLS-D | FGLS-CO | OLS |  |
| N=5/K=2N=5/K=2 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  | T=100T=100 |  | 0.01180.0118 | 0.01160.0116 | 0.00970.0097 | 0.01360.0136 | 0.02250.0225 |  | 0.20160.2016 | 0.20270.2027 | 0.16130.1613 | 0.16140.1614 | 0.24520.2452 |  |
|  | T=200T=200 |  | 0.00810.0081 | 0.01010.0101 | 0.00900.0090 | 0.01200.0120 | 0.02220.0222 |  | 0.09810.0981 | 0.09670.0967 | 0.07660.0766 | 0.07990.0799 | 0.15580.1558 |  |
|  | T=400T=400 |  | 0.00560.0056 | 0.00500.0050 | 0.00470.0047 | 0.00840.0084 | 0.02000.0200 |  | 0.04650.0465 | 0.04610.0461 | 0.03710.0371 | 0.04300.0430 | 0.11540.1154 |  |
|  | T=800T=800 |  | 0.00380.0038 | 0.00380.0038 | 0.00350.0035 | 0.00970.0097 | 0.02270.0227 |  | 0.02390.0239 | 0.02390.0239 | 0.01900.0190 | 0.02530.0253 | 0.09440.0944 |  |
| N=5/K=4N=5/K=4 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  | T=100T=100 |  | 0.01910.0191 | 0.01710.0171 | 0.01380.0138 | 0.01600.0160 | 0.02030.0203 |  | 0.33840.3384 | 0.33990.3399 | 0.25320.2532 | 0.25730.2573 | 0.38600.3860 |  |
|  | T=200T=200 |  | 0.01170.0117 | 0.01090.0109 | 0.00930.0093 | 0.01110.0111 | 0.01780.0178 |  | 0.15880.1588 | 0.15840.1584 | 0.12060.1206 | 0.12740.1274 | 0.22960.2296 |  |
|  | T=400T=400 |  | 0.00900.0090 | 0.00930.0093 | 0.00800.0080 | 0.00910.0091 | 0.01570.0157 |  | 0.07480.0748 | 0.07500.0750 | 0.05770.0577 | 0.06470.0647 | 0.14870.1487 |  |
|  | T=800T=800 |  | 0.00860.0086 | 0.00740.0074 | 0.00630.0063 | 0.00840.0084 | 0.01630.0163 |  | 0.03800.0380 | 0.03790.0379 | 0.02870.0287 | 0.03600.0360 | 0.11370.1137 |  |
| N=10/K=2N=10/K=2 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  | T=100T=100 |  | 0.01610.0161 | 0.02250.0225 | 0.01660.0166 | 0.01740.0174 | 0.01990.0199 |  | 0.39690.3969 | 0.39200.3920 | 0.30630.3063 | 0.29280.2928 | 0.40140.4014 |  |
|  | T=200T=200 |  | 0.01400.0140 | 0.01490.0149 | 0.01410.0141 | 0.01630.0163 | 0.02160.0216 |  | 0.18220.1822 | 0.18090.1809 | 0.14050.1405 | 0.14120.1412 | 0.23350.2335 |  |
|  | T=400T=400 |  | 0.01000.0100 | 0.00970.0097 | 0.00840.0084 | 0.00960.0096 | 0.01610.0161 |  | 0.08760.0876 | 0.08720.0872 | 0.06820.0682 | 0.07260.0726 | 0.15330.1533 |  |
|  | T=800T=800 |  | 0.00650.0065 | 0.00610.0061 | 0.00520.0052 | 0.00750.0075 | 0.01390.0139 |  | 0.04430.0443 | 0.04400.0440 | 0.03440.0344 | 0.04020.0402 | 0.11410.1141 |  |
| N=10/K=4N=10/K=4 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  | T=100T=100 |  | 0.02110.0211 | 0.02500.0250 | 0.02140.0214 | 0.02210.0221 | 0.02540.0254 |  | 0.70940.7094 | 0.70710.7071 | 0.52950.5295 | 0.51000.5100 | 0.68620.6862 |  |
|  | T=200T=200 |  | 0.01910.0191 | 0.01800.0180 | 0.01710.0171 | 0.01710.0171 | 0.02010.0201 |  | 0.31240.3124 | 0.31110.3111 | 0.23340.2334 | 0.23440.2344 | 0.36840.3684 |  |
|  | T=400T=400 |  | 0.01320.0132 | 0.01330.0133 | 0.00980.0098 | 0.01040.0104 | 0.01440.0144 |  | 0.14970.1497 | 0.14930.1493 | 0.11180.1118 | 0.11680.1168 | 0.22050.2205 |  |
|  | T=800T=800 |  | 0.00700.0070 | 0.00820.0082 | 0.00720.0072 | 0.00760.0076 | 0.01140.0114 |  | 0.07340.0734 | 0.07320.0732 | 0.05510.0551 | 0.06160.0616 | 0.15050.1505 |  |

Note: As for Table [1](https://arxiv.org/html/2601.21272v1#Sx1.T1 "Table 1 â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").

Table 3: Bias and MSE under the Eâ€‹Bâ€‹DEBD condition

|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
|  |  |  | bias^â€‹(ğœ¿^T)\widehat{\mathrm{bias}}(\widehat{{\mbox{$\kappa$}}}\_{T}) | | | | |  | MSE^â€‹(ğœ¿^T)\widehat{\mathrm{MSE}}(\widehat{{\mbox{$\kappa$}}}\_{T}) | | | | |  |
|  |  |  | BC-GD | GD | FGLS-D | FGLS-CO | OLS |  | BC-GD | GD | FGLS-D | FGLS-CO | OLS |  |
| N=5/K=2N=5/K=2 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  | T=100T=100 |  | 0.01280.0128 | 0.01270.0127 | 0.01880.0188 | 0.04360.0436 | 0.05670.0567 |  | 0.22840.2284 | 0.22960.2296 | 0.21500.2150 | 0.24380.2438 | 0.32090.3209 |  |
|  | T=200T=200 |  | 0.00900.0090 | 0.01040.0104 | 0.01680.0168 | 0.04050.0405 | 0.05580.0558 |  | 0.11330.1133 | 0.11160.1116 | 0.11880.1188 | 0.15190.1519 | 0.22660.2266 |  |
|  | T=400T=400 |  | 0.00600.0060 | 0.00520.0052 | 0.01500.0150 | 0.03880.0388 | 0.05600.0560 |  | 0.05350.0535 | 0.05270.0527 | 0.07350.0735 | 0.11170.1117 | 0.18520.1852 |  |
|  | T=800T=800 |  | 0.00380.0038 | 0.00390.0039 | 0.01720.0172 | 0.04020.0402 | 0.05840.0584 |  | 0.02810.0281 | 0.02780.0278 | 0.05220.0522 | 0.09150.0915 | 0.16320.1632 |  |
| N=5/K=4N=5/K=4 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  | T=100T=100 |  | 0.01940.0194 | 0.01730.0173 | 0.01570.0157 | 0.02620.0262 | 0.03320.0332 |  | 0.35270.3527 | 0.35360.3536 | 0.30200.3020 | 0.33320.3332 | 0.46520.4652 |  |
|  | T=200T=200 |  | 0.01260.0126 | 0.01130.0113 | 0.01260.0126 | 0.02300.0230 | 0.03230.0323 |  | 0.16680.1668 | 0.16610.1661 | 0.16090.1609 | 0.19490.1949 | 0.30680.3068 |  |
|  | T=400T=400 |  | 0.00920.0092 | 0.00940.0094 | 0.00900.0090 | 0.01890.0189 | 0.02950.0295 |  | 0.07840.0784 | 0.07830.0783 | 0.09160.0916 | 0.12400.1240 | 0.22070.2207 |  |
|  | T=800T=800 |  | 0.00910.0091 | 0.00760.0076 | 0.00850.0085 | 0.02000.0200 | 0.03080.0308 |  | 0.04010.0401 | 0.04000.0400 | 0.05980.0598 | 0.09280.0928 | 0.18520.1852 |  |
| N=10/K=2N=10/K=2 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  | T=100T=100 |  | 0.01670.0167 | 0.02270.0227 | 0.02440.0244 | 0.03650.0365 | 0.03960.0396 |  | 0.41380.4138 | 0.40820.4082 | 0.34850.3485 | 0.36500.3650 | 0.45510.4551 |  |
|  | T=200T=200 |  | 0.01420.0142 | 0.01500.0150 | 0.02390.0239 | 0.03660.0366 | 0.04120.0412 |  | 0.19000.1900 | 0.18900.1890 | 0.17630.1763 | 0.20460.2046 | 0.28720.2872 |  |
|  | T=400T=400 |  | 0.01020.0102 | 0.01010.0101 | 0.01660.0166 | 0.03020.0302 | 0.03820.0382 |  | 0.09120.0912 | 0.09080.0908 | 0.10170.1017 | 0.13410.1341 | 0.20730.2073 |  |
|  | T=800T=800 |  | 0.00660.0066 | 0.00600.0060 | 0.01600.0160 | 0.02990.0299 | 0.03720.0372 |  | 0.04650.0465 | 0.04620.0462 | 0.06660.0666 | 0.10050.1005 | 0.16900.1690 |  |
| N=10/K=4N=10/K=4 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  | T=100T=100 |  | 0.02150.0215 | 0.02500.0250 | 0.02310.0231 | 0.02550.0255 | 0.02790.0279 |  | 0.72400.7240 | 0.72100.7210 | 0.57970.5797 | 0.58210.5821 | 0.75010.7501 |  |
|  | T=200T=200 |  | 0.01950.0195 | 0.01860.0186 | 0.01830.0183 | 0.02040.0204 | 0.02430.0243 |  | 0.31920.3192 | 0.31690.3169 | 0.27170.2717 | 0.29310.2931 | 0.42600.4260 |  |
|  | T=400T=400 |  | 0.01330.0133 | 0.01320.0132 | 0.01110.0111 | 0.01470.0147 | 0.02000.0200 |  | 0.15290.1529 | 0.15250.1525 | 0.14590.1459 | 0.17020.1702 | 0.27670.2767 |  |
|  | T=800T=800 |  | 0.00710.0071 | 0.00820.0082 | 0.01070.0107 | 0.01390.0139 | 0.01810.0181 |  | 0.07490.0749 | 0.07480.0748 | 0.08700.0870 | 0.11280.1128 | 0.20620.2062 |  |

Note: As for Table [1](https://arxiv.org/html/2601.21272v1#Sx1.T1 "Table 1 â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").

Table 4: Rejection rates (percentages) under the null hypothesis H0:ğœ¶=ğŸN,1H\_{0}:\mbox{{$\alpha$}}=\mathbf{0}\_{N,1} when the Bâ€‹DBD holds

|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
|  |  |  | ğ’²Bâ€‹Câ€‹-â€‹Gâ€‹D\mathcal{W}^{BC\text{-}GD} | | |  | ğ’²Gâ€‹D\mathcal{W}^{GD} | | |  | ğ’²Fâ€‹Gâ€‹Lâ€‹Sâ€‹-â€‹D\mathcal{W}^{FGLS\text{-}D} | | |  | ğ’²Fâ€‹Gâ€‹Lâ€‹Sâˆ’Câ€‹O\mathcal{W}^{FGLS-CO} | | |  | Hâ€‹Aâ€‹RHAR | | |  | Gâ€‹Râ€‹SGRS | | |  |
|  |  |  | 10% | 5% | 1% |  | 10% | 5% | 1% |  | 10% | 5% | 1% |  | 10% | 5% | 1% |  | 10% | 5% | 1% |  | 10% | 5% | 1% |  |
| N=5/K=2N=5/K=2 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  | T=100T=100 |  | 0.11600.1160 | 0.06700.0670 | 0.01000.0100 |  | 0.24300.2430 | 0.18000.1800 | 0.07600.0760 |  | 0.21100.2110 | 0.15000.1500 | 0.06000.0600 |  | 0.19300.1930 | 0.13800.1380 | 0.05600.0560 |  | 0.11700.1170 | 0.04800.0480 | 0.01800.0180 |  | 0.14100.1410 | 0.10300.1030 | 0.04400.0440 |  |
|  | T=200T=200 |  | 0.12600.1260 | 0.06500.0650 | 0.01700.0170 |  | 0.21000.2100 | 0.13400.1340 | 0.03800.0380 |  | 0.17700.1770 | 0.10600.1060 | 0.03400.0340 |  | 0.16700.1670 | 0.09900.0990 | 0.03100.0310 |  | 0.11600.1160 | 0.06100.0610 | 0.01300.0130 |  | 0.14400.1440 | 0.09200.0920 | 0.03300.0330 |  |
|  | T=400T=400 |  | 0.11400.1140 | 0.06900.0690 | 0.02300.0230 |  | 0.14600.1460 | 0.10000.1000 | 0.03300.0330 |  | 0.12100.1210 | 0.07800.0780 | 0.02500.0250 |  | 0.11900.1190 | 0.07700.0770 | 0.02300.0230 |  | 0.10300.1030 | 0.06300.0630 | 0.02000.0200 |  | 0.11700.1170 | 0.08200.0820 | 0.03100.0310 |  |
|  | T=800T=800 |  | 0.11400.1140 | 0.06200.0620 | 0.01900.0190 |  | 0.15000.1500 | 0.08200.0820 | 0.02500.0250 |  | 0.12500.1250 | 0.06900.0690 | 0.01600.0160 |  | 0.12300.1230 | 0.06900.0690 | 0.01500.0150 |  | 0.11600.1160 | 0.06600.0660 | 0.01800.0180 |  | 0.13400.1340 | 0.10200.1020 | 0.04300.0430 |  |
| N=5/K=4N=5/K=4 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  | T=100T=100 |  | 0.10100.1010 | 0.06300.0630 | 0.01300.0130 |  | 0.27900.2790 | 0.19700.1970 | 0.08400.0840 |  | 0.23600.2360 | 0.15900.1590 | 0.04800.0480 |  | 0.19400.1940 | 0.12600.1260 | 0.04000.0400 |  | 0.10000.1000 | 0.05600.0560 | 0.01200.0120 |  | 0.11600.1160 | 0.07600.0760 | 0.02800.0280 |  |
|  | T=200T=200 |  | 0.10300.1030 | 0.05100.0510 | 0.01100.0110 |  | 0.19600.1960 | 0.11600.1160 | 0.04000.0400 |  | 0.15800.1580 | 0.09500.0950 | 0.03100.0310 |  | 0.14700.1470 | 0.08200.0820 | 0.02700.0270 |  | 0.09100.0910 | 0.04900.0490 | 0.01100.0110 |  | 0.11700.1170 | 0.07200.0720 | 0.02800.0280 |  |
|  | T=400T=400 |  | 0.09000.0900 | 0.05000.0500 | 0.00800.0080 |  | 0.14700.1470 | 0.08300.0830 | 0.02200.0220 |  | 0.11100.1110 | 0.06000.0600 | 0.01500.0150 |  | 0.10500.1050 | 0.05700.0570 | 0.01300.0130 |  | 0.08300.0830 | 0.04700.0470 | 0.00500.0050 |  | 0.09200.0920 | 0.05400.0540 | 0.02400.0240 |  |
|  | T=800T=800 |  | 0.10200.1020 | 0.05800.0580 | 0.01300.0130 |  | 0.17300.1730 | 0.08700.0870 | 0.02700.0270 |  | 0.11900.1190 | 0.06800.0680 | 0.02100.0210 |  | 0.11500.1150 | 0.06700.0670 | 0.01800.0180 |  | 0.10500.1050 | 0.05200.0520 | 0.01900.0190 |  | 0.12800.1280 | 0.07400.0740 | 0.03100.0310 |  |
| N=10/K=2N=10/K=2 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  | T=100T=100 |  | 0.09500.0950 | 0.04800.0480 | 0.00800.0080 |  | 0.42200.4220 | 0.31200.3120 | 0.16200.1620 |  | 0.34900.3490 | 0.25600.2560 | 0.12400.1240 |  | 0.32100.3210 | 0.22800.2280 | 0.09500.0950 |  | 0.06800.0680 | 0.03300.0330 | 0.00800.0080 |  | 0.07100.0710 | 0.04200.0420 | 0.01200.0120 |  |
|  | T=200T=200 |  | 0.11800.1180 | 0.06300.0630 | 0.01600.0160 |  | 0.26100.2610 | 0.17300.1730 | 0.07200.0720 |  | 0.20700.2070 | 0.13200.1320 | 0.03900.0390 |  | 0.19700.1970 | 0.12500.1250 | 0.03800.0380 |  | 0.06700.0670 | 0.03300.0330 | 0.00900.0090 |  | 0.07700.0770 | 0.04800.0480 | 0.01600.0160 |  |
|  | T=400T=400 |  | 0.10300.1030 | 0.05100.0510 | 0.01400.0140 |  | 0.18900.1890 | 0.11000.1100 | 0.03500.0350 |  | 0.14300.1430 | 0.08900.0890 | 0.02200.0220 |  | 0.13700.1370 | 0.08600.0860 | 0.01900.0190 |  | 0.08700.0870 | 0.04500.0450 | 0.01100.0110 |  | 0.06100.0610 | 0.04000.0400 | 0.01300.0130 |  |
|  | T=800T=800 |  | 0.11900.1190 | 0.06200.0620 | 0.01400.0140 |  | 0.18300.1830 | 0.10300.1030 | 0.03500.0350 |  | 0.13600.1360 | 0.07500.0750 | 0.02900.0290 |  | 0.13500.1350 | 0.07500.0750 | 0.02800.0280 |  | 0.11000.1100 | 0.05200.0520 | 0.01200.0120 |  | 0.09100.0910 | 0.05700.0570 | 0.01800.0180 |  |
| N=10/K=4N=10/K=4 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  | T=100T=100 |  | 0.10800.1080 | 0.05600.0560 | 0.00800.0080 |  | 0.52700.5270 | 0.42900.4290 | 0.27200.2720 |  | 0.44900.4490 | 0.34600.3460 | 0.18600.1860 |  | 0.36700.3670 | 0.26200.2620 | 0.12300.1230 |  | 0.09600.0960 | 0.05100.0510 | 0.01000.0100 |  | 0.06300.0630 | 0.03800.0380 | 0.01100.0110 |  |
|  | T=200T=200 |  | 0.09600.0960 | 0.04900.0490 | 0.01200.0120 |  | 0.30100.3010 | 0.20300.2030 | 0.08700.0870 |  | 0.23600.2360 | 0.16100.1610 | 0.05600.0560 |  | 0.20800.2080 | 0.13100.1310 | 0.04200.0420 |  | 0.08000.0800 | 0.04200.0420 | 0.00800.0080 |  | 0.07300.0730 | 0.04100.0410 | 0.01400.0140 |  |
|  | T=400T=400 |  | 0.11000.1100 | 0.06100.0610 | 0.01100.0110 |  | 0.22600.2260 | 0.14400.1440 | 0.04900.0490 |  | 0.15200.1520 | 0.09000.0900 | 0.02500.0250 |  | 0.14700.1470 | 0.07900.0790 | 0.02400.0240 |  | 0.10100.1010 | 0.05600.0560 | 0.01800.0180 |  | 0.07900.0790 | 0.04000.0400 | 0.01200.0120 |  |
|  | T=800T=800 |  | 0.08800.0880 | 0.04900.0490 | 0.00600.0060 |  | 0.17700.1770 | 0.10400.1040 | 0.03300.0330 |  | 0.11800.1180 | 0.06600.0660 | 0.01900.0190 |  | 0.11600.1160 | 0.06500.0650 | 0.01800.0180 |  | 0.09400.0940 | 0.04700.0470 | 0.01600.0160 |  | 0.07800.0780 | 0.04200.0420 | 0.01500.0150 |  |

Note: R version 4.5.2 was used to compute the statistics..

Table 5: Rejection rates (percentages) under the null hypothesis H0:ğœ¶=ğŸN,1H\_{0}:\mbox{{$\alpha$}}=\mathbf{0}\_{N,1} when the Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG holds

|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
|  |  |  | ğ’²Bâ€‹Câ€‹-â€‹Gâ€‹D\mathcal{W}^{BC\text{-}GD} | | |  | ğ’²Gâ€‹D\mathcal{W}^{GD} | | |  | ğ’²Fâ€‹Gâ€‹Lâ€‹Sâ€‹-â€‹D\mathcal{W}^{FGLS\text{-}D} | | |  | ğ’²Fâ€‹Gâ€‹Lâ€‹Sâˆ’Câ€‹O\mathcal{W}^{FGLS-CO} | | |  | Hâ€‹Aâ€‹RHAR | | |  | Gâ€‹Râ€‹SGRS | | |  |
|  |  |  | 10% | 5% | 1% |  | 10% | 5% | 1% |  | 10% | 5% | 1% |  | 10% | 5% | 1% |  | 10% | 5% | 1% |  | 10% | 5% | 1% |  |
| N=5/K=2N=5/K=2 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  | T=100T=100 |  | 0.11700.1170 | 0.07100.0710 | 0.00800.0080 |  | 0.25900.2590 | 0.20100.2010 | 0.09200.0920 |  | 0.24200.2420 | 0.16800.1680 | 0.06400.0640 |  | 0.20700.2070 | 0.14400.1440 | 0.05600.0560 |  | 0.12600.1260 | 0.06200.0620 | 0.02300.0230 |  | 0.15400.1540 | 0.11000.1100 | 0.05000.0500 |  |
|  | T=200T=200 |  | 0.13000.1300 | 0.06400.0640 | 0.01600.0160 |  | 0.21400.2140 | 0.14600.1460 | 0.04700.0470 |  | 0.18900.1890 | 0.11600.1160 | 0.04300.0430 |  | 0.18200.1820 | 0.11400.1140 | 0.03300.0330 |  | 0.17900.1790 | 0.11500.1150 | 0.02700.0270 |  | 0.19100.1910 | 0.11900.1190 | 0.05400.0540 |  |
|  | T=400T=400 |  | 0.10900.1090 | 0.06800.0680 | 0.02100.0210 |  | 0.15600.1560 | 0.10300.1030 | 0.04200.0420 |  | 0.13200.1320 | 0.08700.0870 | 0.02700.0270 |  | 0.13600.1360 | 0.08800.0880 | 0.03400.0340 |  | 0.25000.2500 | 0.17400.1740 | 0.07300.0730 |  | 0.21900.2190 | 0.14400.1440 | 0.05800.0580 |  |
|  | T=800T=800 |  | 0.11300.1130 | 0.06000.0600 | 0.01700.0170 |  | 0.16900.1690 | 0.08900.0890 | 0.03000.0300 |  | 0.13600.1360 | 0.08100.0810 | 0.02000.0200 |  | 0.18400.1840 | 0.10300.1030 | 0.03000.0300 |  | 0.43700.4370 | 0.34100.3410 | 0.19800.1980 |  | 0.35400.3540 | 0.27200.2720 | 0.15300.1530 |  |
| N=5/K=4N=5/K=4 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  | T=100T=100 |  | 0.10400.1040 | 0.05500.0550 | 0.01200.0120 |  | 0.28800.2880 | 0.21600.2160 | 0.09100.0910 |  | 0.24300.2430 | 0.16600.1660 | 0.05700.0570 |  | 0.19800.1980 | 0.12400.1240 | 0.04300.0430 |  | 0.13800.1380 | 0.06200.0620 | 0.02200.0220 |  | 0.13200.1320 | 0.08100.0810 | 0.03300.0330 |  |
|  | T=200T=200 |  | 0.10300.1030 | 0.04800.0480 | 0.01100.0110 |  | 0.20900.2090 | 0.12700.1270 | 0.04700.0470 |  | 0.17800.1780 | 0.09200.0920 | 0.03600.0360 |  | 0.15500.1550 | 0.08800.0880 | 0.03100.0310 |  | 0.16200.1620 | 0.09200.0920 | 0.03500.0350 |  | 0.17000.1700 | 0.10700.1070 | 0.04600.0460 |  |
|  | T=400T=400 |  | 0.09200.0920 | 0.05400.0540 | 0.01000.0100 |  | 0.15900.1590 | 0.09400.0940 | 0.03000.0300 |  | 0.12000.1200 | 0.07200.0720 | 0.01700.0170 |  | 0.12500.1250 | 0.06800.0680 | 0.02100.0210 |  | 0.23700.2370 | 0.14300.1430 | 0.05000.0500 |  | 0.18900.1890 | 0.13400.1340 | 0.05700.0570 |  |
|  | T=800T=800 |  | 0.10000.1000 | 0.05400.0540 | 0.01100.0110 |  | 0.17700.1770 | 0.09800.0980 | 0.03100.0310 |  | 0.12600.1260 | 0.07800.0780 | 0.02300.0230 |  | 0.15400.1540 | 0.09000.0900 | 0.02800.0280 |  | 0.37400.3740 | 0.30200.3020 | 0.15600.1560 |  | 0.34000.3400 | 0.26300.2630 | 0.13200.1320 |  |
| N=10/K=2N=10/K=2 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  | T=100T=100 |  | 0.09800.0980 | 0.04800.0480 | 0.00700.0070 |  | 0.44200.4420 | 0.35400.3540 | 0.19000.1900 |  | 0.37900.3790 | 0.28100.2810 | 0.14000.1400 |  | 0.31200.3120 | 0.23300.2330 | 0.10500.1050 |  | 0.07700.0770 | 0.03300.0330 | 0.00500.0050 |  | 0.08700.0870 | 0.04700.0470 | 0.01400.0140 |  |
|  | T=200T=200 |  | 0.12400.1240 | 0.05800.0580 | 0.01300.0130 |  | 0.28100.2810 | 0.19200.1920 | 0.09000.0900 |  | 0.23600.2360 | 0.16200.1620 | 0.05500.0550 |  | 0.20600.2060 | 0.12800.1280 | 0.04400.0440 |  | 0.11600.1160 | 0.05900.0590 | 0.01000.0100 |  | 0.10800.1080 | 0.05800.0580 | 0.02200.0220 |  |
|  | T=400T=400 |  | 0.10200.1020 | 0.05200.0520 | 0.01400.0140 |  | 0.20800.2080 | 0.12800.1280 | 0.04100.0410 |  | 0.16100.1610 | 0.09800.0980 | 0.03000.0300 |  | 0.15300.1530 | 0.09000.0900 | 0.03300.0330 |  | 0.20200.2020 | 0.12900.1290 | 0.03600.0360 |  | 0.13600.1360 | 0.08900.0890 | 0.03700.0370 |  |
|  | T=800T=800 |  | 0.12000.1200 | 0.06300.0630 | 0.01500.0150 |  | 0.20200.2020 | 0.11700.1170 | 0.04300.0430 |  | 0.15200.1520 | 0.08400.0840 | 0.03200.0320 |  | 0.16500.1650 | 0.10300.1030 | 0.03300.0330 |  | 0.36300.3630 | 0.26000.2600 | 0.12000.1200 |  | 0.25800.2580 | 0.16700.1670 | 0.06800.0680 |  |
| N=10/K=4N=10/K=4 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  | T=100T=100 |  | 0.10700.1070 | 0.05500.0550 | 0.01200.0120 |  | 0.56000.5600 | 0.45600.4560 | 0.29600.2960 |  | 0.46500.4650 | 0.36400.3640 | 0.21400.2140 |  | 0.36100.3610 | 0.27700.2770 | 0.13200.1320 |  | 0.11600.1160 | 0.05600.0560 | 0.01000.0100 |  | 0.09600.0960 | 0.04500.0450 | 0.01600.0160 |  |
|  | T=200T=200 |  | 0.08700.0870 | 0.04600.0460 | 0.01300.0130 |  | 0.32600.3260 | 0.22200.2220 | 0.09800.0980 |  | 0.25000.2500 | 0.15800.1580 | 0.06200.0620 |  | 0.20500.2050 | 0.13700.1370 | 0.04300.0430 |  | 0.11200.1120 | 0.05800.0580 | 0.00900.0090 |  | 0.09600.0960 | 0.05300.0530 | 0.02100.0210 |  |
|  | T=400T=400 |  | 0.11100.1110 | 0.06100.0610 | 0.01200.0120 |  | 0.24600.2460 | 0.15900.1590 | 0.05900.0590 |  | 0.17400.1740 | 0.10400.1040 | 0.02900.0290 |  | 0.17100.1710 | 0.09500.0950 | 0.03300.0330 |  | 0.20800.2080 | 0.12900.1290 | 0.03100.0310 |  | 0.14800.1480 | 0.08000.0800 | 0.02800.0280 |  |
|  | T=800T=800 |  | 0.09500.0950 | 0.04700.0470 | 0.00700.0070 |  | 0.20200.2020 | 0.12100.1210 | 0.04000.0400 |  | 0.13200.1320 | 0.07700.0770 | 0.02100.0210 |  | 0.15100.1510 | 0.08600.0860 | 0.02200.0220 |  | 0.31800.3180 | 0.22200.2220 | 0.11200.1120 |  | 0.21800.2180 | 0.14700.1470 | 0.05300.0530 |  |

Note: As for Table [4](https://arxiv.org/html/2601.21272v1#Sx1.T4 "Table 4 â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").

Table 6: Rejection rates (percentages) under the null hypothesis H0:ğœ¶=ğŸN,1H\_{0}:\mbox{{$\alpha$}}=\mathbf{0}\_{N,1} when the Eâ€‹Bâ€‹DEBD holds

|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
|  |  |  | ğ’²Bâ€‹Câ€‹-â€‹Gâ€‹D\mathcal{W}^{BC\text{-}GD} | | |  | ğ’²Gâ€‹D\mathcal{W}^{GD} | | |  | ğ’²Fâ€‹Gâ€‹Lâ€‹Sâ€‹-â€‹D\mathcal{W}^{FGLS\text{-}D} | | |  | ğ’²Fâ€‹Gâ€‹Lâ€‹Sâˆ’Câ€‹O\mathcal{W}^{FGLS-CO} | | |  | Hâ€‹Aâ€‹RHAR | | |  | Gâ€‹Râ€‹SGRS | | |  |
|  |  |  | 10% | 5% | 1% |  | 10% | 5% | 1% |  | 10% | 5% | 1% |  | 10% | 5% | 1% |  | 10% | 5% | 1% |  | 10% | 5% | 1% |  |
| N=5/K=2N=5/K=2 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  | T=100T=100 |  | 0.12300.1230 | 0.06500.0650 | 0.00700.0070 |  | 0.33800.3380 | 0.26100.2610 | 0.15300.1530 |  | 0.31600.3160 | 0.23100.2310 | 0.12000.1200 |  | 0.27900.2790 | 0.19400.1940 | 0.08900.0890 |  | 0.15300.1530 | 0.08400.0840 | 0.03300.0330 |  | 0.19500.1950 | 0.14100.1410 | 0.08100.0810 |  |
|  | T=200T=200 |  | 0.13200.1320 | 0.06500.0650 | 0.00900.0090 |  | 0.31300.3130 | 0.21600.2160 | 0.10400.1040 |  | 0.30600.3060 | 0.22400.2240 | 0.09500.0950 |  | 0.28800.2880 | 0.21700.2170 | 0.09500.0950 |  | 0.24700.2470 | 0.16100.1610 | 0.05200.0520 |  | 0.26200.2620 | 0.19200.1920 | 0.09700.0970 |  |
|  | T=400T=400 |  | 0.11600.1160 | 0.07400.0740 | 0.02400.0240 |  | 0.24700.2470 | 0.17300.1730 | 0.08000.0800 |  | 0.31300.3130 | 0.21900.2190 | 0.08800.0880 |  | 0.34300.3430 | 0.25800.2580 | 0.11100.1110 |  | 0.35700.3570 | 0.26400.2640 | 0.11900.1190 |  | 0.35400.3540 | 0.25800.2580 | 0.13100.1310 |  |
|  | T=800T=800 |  | 0.12100.1210 | 0.06800.0680 | 0.02000.0200 |  | 0.25000.2500 | 0.16300.1630 | 0.06900.0690 |  | 0.36800.3680 | 0.26600.2660 | 0.13800.1380 |  | 0.44600.4460 | 0.35800.3580 | 0.20400.2040 |  | 0.54200.5420 | 0.44600.4460 | 0.28900.2890 |  | 0.49000.4900 | 0.41000.4100 | 0.28500.2850 |  |
| N=5/K=4N=5/K=4 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  | T=100T=100 |  | 0.09100.0910 | 0.05800.0580 | 0.01000.0100 |  | 0.34200.3420 | 0.24800.2480 | 0.12700.1270 |  | 0.31200.3120 | 0.21700.2170 | 0.10800.1080 |  | 0.24500.2450 | 0.16800.1680 | 0.07200.0720 |  | 0.16100.1610 | 0.08400.0840 | 0.03200.0320 |  | 0.18000.1800 | 0.11600.1160 | 0.05000.0500 |  |
|  | T=200T=200 |  | 0.09000.0900 | 0.05100.0510 | 0.01500.0150 |  | 0.27800.2780 | 0.17900.1790 | 0.06700.0670 |  | 0.26100.2610 | 0.17300.1730 | 0.06300.0630 |  | 0.24500.2450 | 0.15800.1580 | 0.06200.0620 |  | 0.21100.2110 | 0.13300.1330 | 0.04500.0450 |  | 0.22700.2270 | 0.15500.1550 | 0.08000.0800 |  |
|  | T=400T=400 |  | 0.08700.0870 | 0.04300.0430 | 0.00500.0050 |  | 0.21000.2100 | 0.13000.1300 | 0.05000.0500 |  | 0.26500.2650 | 0.17600.1760 | 0.07000.0700 |  | 0.26600.2660 | 0.16500.1650 | 0.07900.0790 |  | 0.29800.2980 | 0.21500.2150 | 0.09900.0990 |  | 0.28800.2880 | 0.21000.2100 | 0.10300.1030 |  |
|  | T=800T=800 |  | 0.11200.1120 | 0.06100.0610 | 0.01500.0150 |  | 0.22500.2250 | 0.14900.1490 | 0.06000.0600 |  | 0.34900.3490 | 0.25100.2510 | 0.11500.1150 |  | 0.39000.3900 | 0.30300.3030 | 0.15700.1570 |  | 0.47300.4730 | 0.38600.3860 | 0.26900.2690 |  | 0.47100.4710 | 0.39100.3910 | 0.25500.2550 |  |
| N=10/K=2N=10/K=2 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  | T=100T=100 |  | 0.09800.0980 | 0.04900.0490 | 0.01100.0110 |  | 0.49400.4940 | 0.40600.4060 | 0.23900.2390 |  | 0.44400.4440 | 0.34400.3440 | 0.17700.1770 |  | 0.37000.3700 | 0.28400.2840 | 0.13300.1330 |  | 0.08700.0870 | 0.04400.0440 | 0.00800.0080 |  | 0.09700.0970 | 0.06300.0630 | 0.02000.0200 |  |
|  | T=200T=200 |  | 0.11600.1160 | 0.06000.0600 | 0.00900.0090 |  | 0.34300.3430 | 0.24200.2420 | 0.12400.1240 |  | 0.30600.3060 | 0.21800.2180 | 0.08300.0830 |  | 0.28100.2810 | 0.19100.1910 | 0.07400.0740 |  | 0.13500.1350 | 0.06700.0670 | 0.01100.0110 |  | 0.14100.1410 | 0.08600.0860 | 0.03200.0320 |  |
|  | T=400T=400 |  | 0.09400.0940 | 0.05500.0550 | 0.01400.0140 |  | 0.25600.2560 | 0.17900.1790 | 0.06400.0640 |  | 0.27500.2750 | 0.19000.1900 | 0.07600.0760 |  | 0.29400.2940 | 0.18600.1860 | 0.08200.0820 |  | 0.27100.2710 | 0.18000.1800 | 0.06400.0640 |  | 0.20400.2040 | 0.13900.1390 | 0.06500.0650 |  |
|  | T=800T=800 |  | 0.11200.1120 | 0.06300.0630 | 0.00900.0090 |  | 0.27400.2740 | 0.17200.1720 | 0.07700.0770 |  | 0.31900.3190 | 0.21400.2140 | 0.10200.1020 |  | 0.39900.3990 | 0.28800.2880 | 0.14700.1470 |  | 0.49400.4940 | 0.36700.3670 | 0.20900.2090 |  | 0.39800.3980 | 0.28800.2880 | 0.15800.1580 |  |
| N=10/K=4N=10/K=4 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  | T=100T=100 |  | 0.10700.1070 | 0.05700.0570 | 0.00700.0070 |  | 0.59800.5980 | 0.48800.4880 | 0.31800.3180 |  | 0.51300.5130 | 0.41000.4100 | 0.24400.2440 |  | 0.40900.4090 | 0.31500.3150 | 0.16400.1640 |  | 0.12300.1230 | 0.05500.0550 | 0.01200.0120 |  | 0.11800.1180 | 0.05900.0590 | 0.02000.0200 |  |
|  | T=200T=200 |  | 0.09400.0940 | 0.05200.0520 | 0.01200.0120 |  | 0.35900.3590 | 0.25800.2580 | 0.11900.1190 |  | 0.31800.3180 | 0.22000.2200 | 0.09500.0950 |  | 0.27900.2790 | 0.18100.1810 | 0.07900.0790 |  | 0.13500.1350 | 0.07200.0720 | 0.02000.0200 |  | 0.12100.1210 | 0.07500.0750 | 0.03300.0330 |  |
|  | T=400T=400 |  | 0.10900.1090 | 0.06100.0610 | 0.01200.0120 |  | 0.28200.2820 | 0.19300.1930 | 0.07400.0740 |  | 0.26500.2650 | 0.18700.1870 | 0.06700.0670 |  | 0.27400.2740 | 0.17800.1780 | 0.07600.0760 |  | 0.25600.2560 | 0.19300.1930 | 0.05500.0550 |  | 0.21700.2170 | 0.13500.1350 | 0.05700.0570 |  |
|  | T=800T=800 |  | 0.10100.1010 | 0.05100.0510 | 0.00500.0050 |  | 0.24500.2450 | 0.14300.1430 | 0.06200.0620 |  | 0.28600.2860 | 0.19800.1980 | 0.09200.0920 |  | 0.30400.3040 | 0.22300.2230 | 0.09400.0940 |  | 0.40100.4010 | 0.30200.3020 | 0.16600.1660 |  | 0.32200.3220 | 0.23600.2360 | 0.12000.1200 |  |

Note: As for Table [4](https://arxiv.org/html/2601.21272v1#Sx1.T4 "Table 4 â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").

Table 7: Rejection rates (percentages) under the alternative hypothesis H1:Î±1=0.2H\_{1}:\alpha\_{1}=0.2 and Î±j=0\alpha\_{j}=0 (j=2,â€¦,Nj=2,\ldots,N) when the Bâ€‹DBD holds

|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
|  |  |  | ğ’²Bâ€‹Câ€‹-â€‹Gâ€‹D\mathcal{W}^{BC\text{-}GD} | | |  | ğ’²Gâ€‹D\mathcal{W}^{GD} | | |  | ğ’²Fâ€‹Gâ€‹Lâ€‹Sâ€‹-â€‹D\mathcal{W}^{FGLS\text{-}D} | | |  | ğ’²Fâ€‹Gâ€‹Lâ€‹Sâˆ’Câ€‹O\mathcal{W}^{FGLS-CO} | | |  | Hâ€‹Aâ€‹RHAR | | |  | Gâ€‹Râ€‹SGRS | | |  |
|  |  |  | 10% | 5% | 1% |  | 10% | 5% | 1% |  | 10% | 5% | 1% |  | 10% | 5% | 1% |  | 10% | 5% | 1% |  | 10% | 5% | 1% |  |
| N=5/K=2N=5/K=2 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  | T=100T=100 |  | 0.41000.4100 | 0.27000.2700 | 0.12500.1250 |  | 0.40800.4080 | 0.28000.2800 | 0.10100.1010 |  | 0.41400.4140 | 0.28200.2820 | 0.09100.0910 |  | 0.40900.4090 | 0.27700.2770 | 0.08600.0860 |  | 0.34800.3480 | 0.23900.2390 | 0.05800.0580 |  | 0.17600.1760 | 0.09200.0920 | 0.03200.0320 |  |
|  | T=200T=200 |  | 0.68900.6890 | 0.57200.5720 | 0.31800.3180 |  | 0.69200.6920 | 0.62100.6210 | 0.38400.3840 |  | 0.72900.7290 | 0.59300.5930 | 0.43200.4320 |  | 0.72400.7240 | 0.59300.5930 | 0.44500.4450 |  | 0.62100.6210 | 0.48500.4850 | 0.26500.2650 |  | 0.37600.3760 | 0.23600.2360 | 0.07000.0700 |  |
|  | T=400T=400 |  | 0.96000.9600 | 0.91200.9120 | 0.69400.6940 |  | 0.96500.9650 | 0.92300.9230 | 0.80000.8000 |  | 0.97400.9740 | 0.93800.9380 | 0.78800.7880 |  | 0.97300.9730 | 0.93700.9370 | 0.78700.7870 |  | 0.94600.9460 | 0.88800.8880 | 0.63300.6330 |  | 0.79500.7950 | 0.55100.5510 | 0.10600.1060 |  |
|  | T=800T=800 |  | 0.99900.9990 | 0.99800.9980 | 0.88600.8860 |  | 0.99900.9990 | 0.99900.9990 | 0.99500.9950 |  | 0.99900.9990 | 0.99900.9990 | 0.99800.9980 |  | 0.99900.9990 | 0.99900.9990 | 0.99800.9980 |  | 0.99900.9990 | 0.99700.9970 | 0.99100.9910 |  | 0.98500.9850 | 0.93700.9370 | 0.50500.5050 |  |
| N=5/K=4N=5/K=4 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  | T=100T=100 |  | 0.36100.3610 | 0.20800.2080 | 0.09100.0910 |  | 0.35700.3570 | 0.23200.2320 | 0.08600.0860 |  | 0.37900.3790 | 0.28600.2860 | 0.09100.0910 |  | 0.38100.3810 | 0.28800.2880 | 0.10300.1030 |  | 0.32700.3270 | 0.20100.2010 | 0.06200.0620 |  | 0.23800.2380 | 0.11900.1190 | 0.02400.0240 |  |
|  | T=200T=200 |  | 0.66600.6660 | 0.55900.5590 | 0.30400.3040 |  | 0.70200.7020 | 0.56200.5620 | 0.24500.2450 |  | 0.73400.7340 | 0.60000.6000 | 0.23400.2340 |  | 0.73600.7360 | 0.60500.6050 | 0.22000.2200 |  | 0.60400.6040 | 0.45600.4560 | 0.20400.2040 |  | 0.41300.4130 | 0.25900.2590 | 0.04000.0400 |  |
|  | T=400T=400 |  | 0.92300.9230 | 0.85800.8580 | 0.70300.7030 |  | 0.93900.9390 | 0.88100.8810 | 0.73900.7390 |  | 0.95600.9560 | 0.92100.9210 | 0.77800.7780 |  | 0.95600.9560 | 0.91900.9190 | 0.77000.7700 |  | 0.90700.9070 | 0.83100.8310 | 0.65700.6570 |  | 0.79900.7990 | 0.62000.6200 | 0.13400.1340 |  |
|  | T=800T=800 |  | 1.00001.0000 | 0.99100.9910 | 0.96400.9640 |  | 1.00001.0000 | 0.99500.9950 | 0.97200.9720 |  | 1.00001.0000 | 0.99700.9970 | 0.98300.9830 |  | 1.00001.0000 | 0.99700.9970 | 0.98200.9820 |  | 0.99200.9920 | 0.98600.9860 | 0.92400.9240 |  | 0.96300.9630 | 0.89200.8920 | 0.57400.5740 |  |
| N=10/K=2N=10/K=2 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  | T=100T=100 |  | 0.30100.3010 | 0.19600.1960 | 0.07600.0760 |  | 0.30800.3080 | 0.20700.2070 | 0.06200.0620 |  | 0.31900.3190 | 0.20600.2060 | 0.07200.0720 |  | 0.35400.3540 | 0.22100.2210 | 0.08400.0840 |  | 0.28200.2820 | 0.13800.1380 | 0.03400.0340 |  | 0.21500.2150 | 0.11800.1180 | 0.02600.0260 |  |
|  | T=200T=200 |  | 0.58600.5860 | 0.44000.4400 | 0.22400.2240 |  | 0.58500.5850 | 0.44400.4440 | 0.21000.2100 |  | 0.61200.6120 | 0.49200.4920 | 0.23400.2340 |  | 0.62600.6260 | 0.48500.4850 | 0.20900.2090 |  | 0.54100.5410 | 0.42200.4220 | 0.11700.1170 |  | 0.34800.3480 | 0.19200.1920 | 0.05300.0530 |  |
|  | T=400T=400 |  | 0.88800.8880 | 0.81300.8130 | 0.55900.5590 |  | 0.89200.8920 | 0.84200.8420 | 0.64000.6400 |  | 0.91500.9150 | 0.85600.8560 | 0.69000.6900 |  | 0.91200.9120 | 0.85500.8550 | 0.68300.6830 |  | 0.86300.8630 | 0.75600.7560 | 0.49200.4920 |  | 0.71700.7170 | 0.54300.5430 | 0.17800.1780 |  |
|  | T=800T=800 |  | 0.99700.9970 | 0.99400.9940 | 0.95300.9530 |  | 0.99400.9940 | 0.98700.9870 | 0.96400.9640 |  | 0.99800.9980 | 0.99300.9930 | 0.97200.9720 |  | 0.99800.9980 | 0.99300.9930 | 0.97200.9720 |  | 0.99100.9910 | 0.97900.9790 | 0.92300.9230 |  | 0.94200.9420 | 0.87200.8720 | 0.67400.6740 |  |
| N=10/K=4N=10/K=4 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  | T=100T=100 |  | 0.25500.2550 | 0.16300.1630 | 0.06100.0610 |  | 0.26500.2650 | 0.17300.1730 | 0.06600.0660 |  | 0.28500.2850 | 0.18800.1880 | 0.05800.0580 |  | 0.30100.3010 | 0.18300.1830 | 0.05000.0500 |  | 0.22500.2250 | 0.12500.1250 | 0.03700.0370 |  | 0.21600.2160 | 0.11600.1160 | 0.02200.0220 |  |
|  | T=200T=200 |  | 0.49100.4910 | 0.35100.3510 | 0.11700.1170 |  | 0.49200.4920 | 0.35400.3540 | 0.14900.1490 |  | 0.54700.5470 | 0.38300.3830 | 0.16100.1610 |  | 0.54500.5450 | 0.38400.3840 | 0.14300.1430 |  | 0.41000.4100 | 0.26400.2640 | 0.11000.1100 |  | 0.30700.3070 | 0.18700.1870 | 0.04700.0470 |  |
|  | T=400T=400 |  | 0.82400.8240 | 0.70800.7080 | 0.47400.4740 |  | 0.83800.8380 | 0.74400.7440 | 0.46900.4690 |  | 0.86700.8670 | 0.81100.8110 | 0.57400.5740 |  | 0.86600.8660 | 0.81000.8100 | 0.59700.5970 |  | 0.77600.7760 | 0.64900.6490 | 0.32500.3250 |  | 0.64200.6420 | 0.49900.4990 | 0.16800.1680 |  |
|  | T=800T=800 |  | 0.98800.9880 | 0.97600.9760 | 0.93400.9340 |  | 0.98800.9880 | 0.97400.9740 | 0.93100.9310 |  | 0.99600.9960 | 0.98600.9860 | 0.94600.9460 |  | 0.99600.9960 | 0.98500.9850 | 0.94600.9460 |  | 0.98200.9820 | 0.94600.9460 | 0.83300.8330 |  | 0.94100.9410 | 0.86800.8680 | 0.48900.4890 |  |

Note: R version 4.5.2 was used to compute the statistics..

Table 8: Rejection rates (percentages) under the alternative hypothesis H1:Î±1=0.2H\_{1}:\alpha\_{1}=0.2 and Î±j=0\alpha\_{j}=0 (j=2,â€¦,Nj=2,\ldots,N) when the Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG holds

|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
|  |  |  | ğ’²Bâ€‹Câ€‹-â€‹Gâ€‹D\mathcal{W}^{BC\text{-}GD} | | |  | ğ’²Gâ€‹D\mathcal{W}^{GD} | | |  | ğ’²Fâ€‹Gâ€‹Lâ€‹Sâ€‹-â€‹D\mathcal{W}^{FGLS\text{-}D} | | |  | ğ’²Fâ€‹Gâ€‹Lâ€‹Sâˆ’Câ€‹O\mathcal{W}^{FGLS-CO} | | |  | Hâ€‹Aâ€‹RHAR | | |  | Gâ€‹Râ€‹SGRS | | |  |
|  |  |  | 10% | 5% | 1% |  | 10% | 5% | 1% |  | 10% | 5% | 1% |  | 10% | 5% | 1% |  | 10% | 5% | 1% |  | 10% | 5% | 1% |  |
| N=5/K=2N=5/K=2 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  | T=100T=100 |  | 0.39100.3910 | 0.26300.2630 | 0.13900.1390 |  | 0.37600.3760 | 0.28100.2810 | 0.10900.1090 |  | 0.42600.4260 | 0.29900.2990 | 0.11200.1120 |  | 0.40700.4070 | 0.28000.2800 | 0.12200.1220 |  | 0.39700.3970 | 0.24700.2470 | 0.06600.0660 |  | 0.21200.2120 | 0.10700.1070 | 0.03700.0370 |  |
|  | T=200T=200 |  | 0.69100.6910 | 0.58400.5840 | 0.32300.3230 |  | 0.68900.6890 | 0.58800.5880 | 0.36800.3680 |  | 0.73400.7340 | 0.58700.5870 | 0.40500.4050 |  | 0.72700.7270 | 0.61200.6120 | 0.41800.4180 |  | 0.57000.5700 | 0.46000.4600 | 0.26000.2600 |  | 0.39500.3950 | 0.22600.2260 | 0.06500.0650 |  |
|  | T=400T=400 |  | 0.96200.9620 | 0.92400.9240 | 0.71500.7150 |  | 0.96700.9670 | 0.92500.9250 | 0.80000.8000 |  | 0.97800.9780 | 0.94000.9400 | 0.84500.8450 |  | 0.97400.9740 | 0.93400.9340 | 0.79900.7990 |  | 0.86200.8620 | 0.76900.7690 | 0.50200.5020 |  | 0.72700.7270 | 0.54700.5470 | 0.18400.1840 |  |
|  | T=800T=800 |  | 1.00001.0000 | 0.99800.9980 | 0.77600.7760 |  | 0.99900.9990 | 0.99900.9990 | 0.99600.9960 |  | 0.99900.9990 | 0.99900.9990 | 0.99800.9980 |  | 0.99900.9990 | 0.99900.9990 | 0.99500.9950 |  | 0.96200.9620 | 0.92400.9240 | 0.80800.8080 |  | 0.88900.8890 | 0.75800.7580 | 0.40900.4090 |  |
| N=5/K=4N=5/K=4 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  | T=100T=100 |  | 0.35900.3590 | 0.22300.2230 | 0.08600.0860 |  | 0.35700.3570 | 0.21800.2180 | 0.06300.0630 |  | 0.37900.3790 | 0.27600.2760 | 0.09700.0970 |  | 0.37000.3700 | 0.27500.2750 | 0.08400.0840 |  | 0.30200.3020 | 0.21000.2100 | 0.04400.0440 |  | 0.24100.2410 | 0.12100.1210 | 0.02000.0200 |  |
|  | T=200T=200 |  | 0.66500.6650 | 0.55700.5570 | 0.21100.2110 |  | 0.72200.7220 | 0.56600.5660 | 0.21300.2130 |  | 0.73500.7350 | 0.61300.6130 | 0.25800.2580 |  | 0.71400.7140 | 0.56300.5630 | 0.21800.2180 |  | 0.54100.5410 | 0.35200.3520 | 0.17700.1770 |  | 0.38500.3850 | 0.24000.2400 | 0.06600.0660 |  |
|  | T=400T=400 |  | 0.92300.9230 | 0.84800.8480 | 0.67900.6790 |  | 0.93200.9320 | 0.87800.8780 | 0.71600.7160 |  | 0.95600.9560 | 0.91900.9190 | 0.72800.7280 |  | 0.95000.9500 | 0.90000.9000 | 0.67600.6760 |  | 0.77100.7710 | 0.66700.6670 | 0.38200.3820 |  | 0.65000.6500 | 0.45900.4590 | 0.14000.1400 |  |
|  | T=800T=800 |  | 0.99900.9990 | 0.99300.9930 | 0.95700.9570 |  | 1.00001.0000 | 0.99600.9960 | 0.97100.9710 |  | 1.00001.0000 | 0.99900.9990 | 0.99000.9900 |  | 0.99900.9990 | 0.99600.9960 | 0.96800.9680 |  | 0.90500.9050 | 0.82300.8230 | 0.58400.5840 |  | 0.83400.8340 | 0.68200.6820 | 0.35700.3570 |  |
| N=10/K=2N=10/K=2 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  | T=100T=100 |  | 0.29500.2950 | 0.19800.1980 | 0.08000.0800 |  | 0.29900.2990 | 0.19600.1960 | 0.05300.0530 |  | 0.31800.3180 | 0.19600.1960 | 0.08200.0820 |  | 0.33400.3340 | 0.21300.2130 | 0.08800.0880 |  | 0.28300.2830 | 0.14400.1440 | 0.03800.0380 |  | 0.22100.2210 | 0.12800.1280 | 0.03400.0340 |  |
|  | T=200T=200 |  | 0.57400.5740 | 0.43200.4320 | 0.20500.2050 |  | 0.56900.5690 | 0.42800.4280 | 0.20600.2060 |  | 0.60900.6090 | 0.48000.4800 | 0.23800.2380 |  | 0.61400.6140 | 0.49400.4940 | 0.22300.2230 |  | 0.49500.4950 | 0.34500.3450 | 0.16500.1650 |  | 0.32800.3280 | 0.21600.2160 | 0.05600.0560 |  |
|  | T=400T=400 |  | 0.88100.8810 | 0.81900.8190 | 0.53300.5330 |  | 0.89600.8960 | 0.84300.8430 | 0.62300.6230 |  | 0.91900.9190 | 0.85400.8540 | 0.67400.6740 |  | 0.92400.9240 | 0.85100.8510 | 0.69000.6900 |  | 0.76000.7600 | 0.64100.6410 | 0.40500.4050 |  | 0.62000.6200 | 0.44200.4420 | 0.20600.2060 |  |
|  | T=800T=800 |  | 0.99900.9990 | 0.99300.9930 | 0.94000.9400 |  | 0.99500.9950 | 0.98900.9890 | 0.95700.9570 |  | 0.99700.9970 | 0.99300.9930 | 0.96700.9670 |  | 0.99700.9970 | 0.99200.9920 | 0.96000.9600 |  | 0.95200.9520 | 0.90300.9030 | 0.69400.6940 |  | 0.87700.8770 | 0.76900.7690 | 0.52600.5260 |  |
| N=10/K=4N=10/K=4 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  | T=100T=100 |  | 0.25000.2500 | 0.15300.1530 | 0.05400.0540 |  | 0.25600.2560 | 0.17000.1700 | 0.05700.0570 |  | 0.29700.2970 | 0.17300.1730 | 0.06100.0610 |  | 0.29900.2990 | 0.18500.1850 | 0.06100.0610 |  | 0.20900.2090 | 0.13200.1320 | 0.05300.0530 |  | 0.21200.2120 | 0.13200.1320 | 0.02600.0260 |  |
|  | T=200T=200 |  | 0.50700.5070 | 0.35000.3500 | 0.13400.1340 |  | 0.49200.4920 | 0.33700.3370 | 0.12300.1230 |  | 0.53600.5360 | 0.37300.3730 | 0.14300.1430 |  | 0.54000.5400 | 0.40100.4010 | 0.19100.1910 |  | 0.40300.4030 | 0.27900.2790 | 0.12500.1250 |  | 0.34200.3420 | 0.21700.2170 | 0.05100.0510 |  |
|  | T=400T=400 |  | 0.82600.8260 | 0.70800.7080 | 0.46100.4610 |  | 0.83700.8370 | 0.73200.7320 | 0.46500.4650 |  | 0.87800.8780 | 0.80100.8010 | 0.60800.6080 |  | 0.87600.8760 | 0.80300.8030 | 0.58500.5850 |  | 0.67200.6720 | 0.57700.5770 | 0.25500.2550 |  | 0.59200.5920 | 0.45500.4550 | 0.15100.1510 |  |
|  | T=800T=800 |  | 0.98800.9880 | 0.97600.9760 | 0.92700.9270 |  | 0.98800.9880 | 0.97500.9750 | 0.93600.9360 |  | 0.99600.9960 | 0.98800.9880 | 0.94900.9490 |  | 0.99500.9950 | 0.98500.9850 | 0.95500.9550 |  | 0.89300.8930 | 0.79800.7980 | 0.58800.5880 |  | 0.85100.8510 | 0.74900.7490 | 0.35000.3500 |  |

Note: As for Table [7](https://arxiv.org/html/2601.21272v1#Sx1.T7 "Table 7 â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").

Table 9: Rejection rates (percentages) under the alternative hypothesis H1:Î±1=0.2H\_{1}:\alpha\_{1}=0.2 and Î±j=0\alpha\_{j}=0 (j=2,â€¦,Nj=2,\ldots,N) when the Eâ€‹Bâ€‹DEBD holds

|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
|  |  |  | ğ’²Bâ€‹Câ€‹-â€‹Gâ€‹D\mathcal{W}^{BC\text{-}GD} | | |  | ğ’²Gâ€‹D\mathcal{W}^{GD} | | |  | ğ’²Fâ€‹Gâ€‹Lâ€‹Sâ€‹-â€‹D\mathcal{W}^{FGLS\text{-}D} | | |  | ğ’²Fâ€‹Gâ€‹Lâ€‹Sâˆ’Câ€‹O\mathcal{W}^{FGLS-CO} | | |  | Hâ€‹Aâ€‹RHAR | | |  | Gâ€‹Râ€‹SGRS | | |  |
|  |  |  | 10% | 5% | 1% |  | 10% | 5% | 1% |  | 10% | 5% | 1% |  | 10% | 5% | 1% |  | 10% | 5% | 1% |  | 10% | 5% | 1% |  |
| N=5/K=2N=5/K=2 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  | T=100T=100 |  | 0.31400.3140 | 0.19500.1950 | 0.09600.0960 |  | 0.32200.3220 | 0.20600.2060 | 0.05100.0510 |  | 0.34600.3460 | 0.23800.2380 | 0.07400.0740 |  | 0.38600.3860 | 0.25400.2540 | 0.08100.0810 |  | 0.35200.3520 | 0.18300.1830 | 0.05100.0510 |  | 0.19300.1930 | 0.10100.1010 | 0.03300.0330 |  |
|  | T=200T=200 |  | 0.59200.5920 | 0.47600.4760 | 0.31500.3150 |  | 0.61300.6130 | 0.49400.4940 | 0.21100.2110 |  | 0.65400.6540 | 0.49400.4940 | 0.30000.3000 |  | 0.62200.6220 | 0.48600.4860 | 0.26000.2600 |  | 0.52300.5230 | 0.39700.3970 | 0.22400.2240 |  | 0.32900.3290 | 0.20800.2080 | 0.05800.0580 |  |
|  | T=400T=400 |  | 0.91700.9170 | 0.83300.8330 | 0.49100.4910 |  | 0.92700.9270 | 0.84900.8490 | 0.57600.5760 |  | 0.92100.9210 | 0.85700.8570 | 0.66500.6650 |  | 0.89200.8920 | 0.82900.8290 | 0.60200.6020 |  | 0.78000.7800 | 0.65600.6560 | 0.37500.3750 |  | 0.61500.6150 | 0.42600.4260 | 0.15800.1580 |  |
|  | T=800T=800 |  | 0.99800.9980 | 0.99400.9940 | 0.72900.7290 |  | 0.99900.9990 | 0.99600.9960 | 0.94000.9400 |  | 0.99400.9940 | 0.98200.9820 | 0.93000.9300 |  | 0.97600.9760 | 0.96000.9600 | 0.83900.8390 |  | 0.90300.9030 | 0.82300.8230 | 0.63400.6340 |  | 0.76500.7650 | 0.58800.5880 | 0.28700.2870 |  |
| N=5/K=4N=5/K=4 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  | T=100T=100 |  | 0.34000.3400 | 0.20000.2000 | 0.06700.0670 |  | 0.34500.3450 | 0.21400.2140 | 0.05200.0520 |  | 0.33400.3340 | 0.21400.2140 | 0.07500.0750 |  | 0.33900.3390 | 0.21100.2110 | 0.07500.0750 |  | 0.28800.2880 | 0.17600.1760 | 0.03600.0360 |  | 0.21700.2170 | 0.12000.1200 | 0.02400.0240 |  |
|  | T=200T=200 |  | 0.64200.6420 | 0.49000.4900 | 0.18400.1840 |  | 0.65700.6570 | 0.51200.5120 | 0.16800.1680 |  | 0.65700.6570 | 0.54000.5400 | 0.22300.2230 |  | 0.61700.6170 | 0.44800.4480 | 0.15000.1500 |  | 0.48000.4800 | 0.31900.3190 | 0.09900.0990 |  | 0.34200.3420 | 0.17900.1790 | 0.03800.0380 |  |
|  | T=400T=400 |  | 0.89800.8980 | 0.82800.8280 | 0.62200.6220 |  | 0.91100.9110 | 0.84400.8440 | 0.64000.6400 |  | 0.88700.8870 | 0.81400.8140 | 0.61400.6140 |  | 0.85200.8520 | 0.73200.7320 | 0.49000.4900 |  | 0.67300.6730 | 0.55500.5550 | 0.23000.2300 |  | 0.55000.5500 | 0.33900.3390 | 0.14400.1440 |  |
|  | T=800T=800 |  | 0.99400.9940 | 0.98800.9880 | 0.91700.9170 |  | 0.99700.9970 | 0.98300.9830 | 0.94200.9420 |  | 0.97600.9760 | 0.95700.9570 | 0.87100.8710 |  | 0.95600.9560 | 0.91100.9110 | 0.72400.7240 |  | 0.82000.8200 | 0.64400.6440 | 0.31300.3130 |  | 0.60500.6050 | 0.45900.4590 | 0.23100.2310 |  |
| N=10/K=2N=10/K=2 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  | T=100T=100 |  | 0.26600.2660 | 0.17200.1720 | 0.04500.0450 |  | 0.28800.2880 | 0.18000.1800 | 0.03900.0390 |  | 0.29200.2920 | 0.19000.1900 | 0.07500.0750 |  | 0.32500.3250 | 0.19200.1920 | 0.09300.0930 |  | 0.27800.2780 | 0.12600.1260 | 0.04400.0440 |  | 0.22300.2230 | 0.11800.1180 | 0.03400.0340 |  |
|  | T=200T=200 |  | 0.52400.5240 | 0.39200.3920 | 0.19600.1960 |  | 0.49300.4930 | 0.37400.3740 | 0.17000.1700 |  | 0.57100.5710 | 0.44500.4450 | 0.21300.2130 |  | 0.59600.5960 | 0.47400.4740 | 0.22300.2230 |  | 0.48000.4800 | 0.34500.3450 | 0.16300.1630 |  | 0.32600.3260 | 0.21200.2120 | 0.06000.0600 |  |
|  | T=400T=400 |  | 0.86900.8690 | 0.77100.7710 | 0.41000.4100 |  | 0.87100.8710 | 0.79300.7930 | 0.54100.5410 |  | 0.87000.8700 | 0.80000.8000 | 0.59800.5980 |  | 0.86500.8650 | 0.75400.7540 | 0.51000.5100 |  | 0.71200.7120 | 0.58300.5830 | 0.37100.3710 |  | 0.57000.5700 | 0.40700.4070 | 0.14600.1460 |  |
|  | T=800T=800 |  | 0.99500.9950 | 0.98700.9870 | 0.94000.9400 |  | 0.99100.9910 | 0.98100.9810 | 0.93300.9330 |  | 0.99200.9920 | 0.98000.9800 | 0.92300.9230 |  | 0.98200.9820 | 0.95900.9590 | 0.86700.8670 |  | 0.90600.9060 | 0.84800.8480 | 0.57800.5780 |  | 0.78900.7890 | 0.65500.6550 | 0.36100.3610 |  |
| N=10/K=4N=10/K=4 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  | T=100T=100 |  | 0.23300.2330 | 0.13200.1320 | 0.06400.0640 |  | 0.24400.2440 | 0.15000.1500 | 0.04600.0460 |  | 0.25600.2560 | 0.16500.1650 | 0.05700.0570 |  | 0.26500.2650 | 0.16700.1670 | 0.05300.0530 |  | 0.21700.2170 | 0.12800.1280 | 0.04200.0420 |  | 0.19300.1930 | 0.11700.1170 | 0.02800.0280 |  |
|  | T=200T=200 |  | 0.47300.4730 | 0.30600.3060 | 0.11900.1190 |  | 0.49100.4910 | 0.33200.3320 | 0.11500.1150 |  | 0.49700.4970 | 0.34200.3420 | 0.15400.1540 |  | 0.48700.4870 | 0.36300.3630 | 0.20000.2000 |  | 0.38000.3800 | 0.27200.2720 | 0.09000.0900 |  | 0.34200.3420 | 0.17400.1740 | 0.06100.0610 |  |
|  | T=400T=400 |  | 0.80600.8060 | 0.63600.6360 | 0.42100.4210 |  | 0.81300.8130 | 0.71300.7130 | 0.44700.4470 |  | 0.82100.8210 | 0.71700.7170 | 0.38500.3850 |  | 0.80600.8060 | 0.68000.6800 | 0.44800.4480 |  | 0.63200.6320 | 0.50200.5020 | 0.23200.2320 |  | 0.51500.5150 | 0.35600.3560 | 0.15100.1510 |  |
|  | T=800T=800 |  | 0.98500.9850 | 0.96700.9670 | 0.92000.9200 |  | 0.98500.9850 | 0.96900.9690 | 0.90500.9050 |  | 0.98100.9810 | 0.95100.9510 | 0.84100.8410 |  | 0.97400.9740 | 0.93900.9390 | 0.76800.7680 |  | 0.81500.8150 | 0.67100.6710 | 0.45300.4530 |  | 0.76500.7650 | 0.59700.5970 | 0.19800.1980 |  |

Note: As for Table [7](https://arxiv.org/html/2601.21272v1#Sx1.T7 "Table 7 â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models").

Table 10: Descriptive statistics and unit root tests

|  |  |  |  |  |  |  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
|  |  |  | Mean | SD | Min | Max |  | ADF | Lags |  | TT |  |
| FF3 |  |  |  |  |  |  |  |  |  |  |  |  |
|  | Rmâˆ’RfR\_{m}-R\_{f} |  | 0.01030.0103 | 0.04640.0464 | âˆ’0.1720-0.1720 | 0.13600.1360 |  | âˆ’4.9877-4.9877 | 77 |  | 206206 |  |
|  | Sâ€‹Mâ€‹BSMB |  | âˆ’0.0005-0.0005 | 0.02600.0260 | âˆ’0.0593-0.0593 | 0.07140.0714 |  | âˆ’14.9473-14.9473 | 0 |  | 206206 |  |
|  | Hâ€‹Mâ€‹LHML |  | âˆ’0.0015-0.0015 | 0.03410.0341 | âˆ’0.1383-0.1383 | 0.12860.1286 |  | âˆ’12.1516-12.1516 | 0 |  | 206206 |  |
| FF5 |  |  |  |  |  |  |  |  |  |  |  |  |
|  | Rmâˆ’RfR\_{m}-R\_{f} |  | 0.01030.0103 | 0.04640.0464 | âˆ’0.1720-0.1720 | 0.13580.1358 |  | âˆ’4.9904-4.9904 | 77 |  | 206206 |  |
|  | Sâ€‹Mâ€‹BSMB |  | âˆ’0.0008-0.0008 | 0.02780.0278 | âˆ’0.0818-0.0818 | 0.08340.0834 |  | âˆ’10.4293-10.4293 | 11 |  | 206206 |  |
|  | Hâ€‹Mâ€‹LHML |  | âˆ’0.0015-0.0015 | 0.03410.0341 | âˆ’0.1383-0.1383 | 0.12860.1286 |  | âˆ’12.1516-12.1516 | 0 |  | 206206 |  |
|  | Wâ€‹Mâ€‹LWML |  | 0.00240.0024 | 0.01990.0199 | âˆ’0.0522-0.0522 | 0.07190.0719 |  | âˆ’12.0900-12.0900 | 0 |  | 206206 |  |
|  | Câ€‹Mâ€‹ACMA |  | 0.00010.0001 | 0.02060.0206 | âˆ’0.0708-0.0708 | 0.07730.0773 |  | âˆ’12.7320-12.7320 | 0 |  | 206206 |  |

Notes:

* (1)

  â€œRmâˆ’RfR\_{m}-R\_{f},â€ â€œSâ€‹Mâ€‹BSMB,â€ â€œHâ€‹Mâ€‹LHML,â€ â€œRâ€‹Mâ€‹WRMW,â€ and â€œCâ€‹Mâ€‹ACMAâ€ denote the returns on each risk factor, wich correspond to Fama-French multi-factor models.
* (2)

  â€œADFâ€ denotes the ADF test statistics and â€œLagsâ€ denotes the lag order selected by the BIC.
* (3)

  In computing the ADF test, a model with a time trend and a constant is assumed. The critical value at the 5% significance level for the ADF test is â€œâˆ’3.41-3.41.â€
* (4)

  R version 4.5.2 was used to compute the statistics.

Table 11: Test statistics for null hypothesis ğœ¶0=ğŸ\mbox{{$\alpha$}}\_{0}=\mathbf{0} (FF3/FF5 models)

|  |  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- | --- |
|  |  |  | ğ’²Bâ€‹Câ€‹-â€‹Gâ€‹D\mathcal{W}^{BC\text{-}GD} | ğ’²Gâ€‹D\mathcal{W}^{GD} | ğ’²Fâ€‹Gâ€‹Lâ€‹Sâ€‹-â€‹D\mathcal{W}^{FGLS\text{-}D} | ğ’²Fâ€‹Gâ€‹Lâ€‹Sâ€‹-â€‹Câ€‹O\mathcal{W}^{FGLS\text{-}CO} |  |
| FF3 |  |  |  |  |  |  |  |
|  | MB6 |  | 7.49647.4964 | 16.411016.4110 | 16.134316.1343 | 16.011416.0114 |  |
|  |  | (0.0210)(0.0210) | (0.0117)(0.0117) | (0.0131)(0.0131) | (0.0137)(0.0137) |  |
|  | MB25 |  | 50.035850.0358 | 79.271179.2711 | 74.349474.3494 | 70.340070.3400 |  |
|  |  | (0.0420)(0.0420) | (0.0000)(0.0000) | (0.0000)(0.0000) | (0.0000)(0.0000) |  |
| FF5 |  |  |  |  |  |  |  |
|  | MB6 |  | 7.76177.7617 | 13.744813.7448 | 13.899913.8999 | 13.918913.9189 |  |
|  |  | (0.0390)(0.0390) | (0.0326)(0.0326) | (0.0308)(0.0308) | (0.0306)(0.0306) |  |
|  | MB25 |  | 55.851855.8518 | 78.966078.9660 | 72.992072.9920 | 67.688567.6885 |  |
|  |  | (0.0601)(0.0601) | (0.0000)(0.0000) | (0.0000)(0.0000) | (0.0000)(0.0000) |  |
|  | MI6 |  | 7.83357.8335 | 4.10014.1001 | 4.16574.1657 | 3.90033.9003 |  |
|  |  | (0.7237)(0.7237) | (0.6631)(0.6631) | (0.6543)(0.6543) | (0.6902)(0.6902) |  |
|  | MI25 |  | 55.516255.5162 | 61.267861.2678 | 60.837860.8378 | 56.617756.6177 |  |
|  |  | (0.1381)(0.1381) | (0.0001)(0.0001) | (0.0001)(0.0001) | (0.0003)(0.0003) |  |
|  | MO6 |  | 7.84677.8467 | 7.67727.6772 | 8.34478.3447 | 7.88407.8840 |  |
|  |  | (0.3283)(0.3283) | (0.2627)(0.2627) | (0.2139)(0.2139) | (0.2467)(0.2467) |  |
|  | MO25 |  | 54.996654.9966 | 51.712551.7125 | 49.809349.8093 | 45.779645.7796 |  |
|  |  | (0.2475)(0.2475) | (0.0013)(0.0013) | (0.0022)(0.0022) | (0.0068)(0.0068) |  |

Notes:

* (1)

  â€œğ’²Bâ€‹Câ€‹-â€‹Gâ€‹D\mathcal{W}^{BC\text{-}GD},â€ â€œğ’²Gâ€‹D\mathcal{W}^{GD},â€ â€œğ’²Fâ€‹Gâ€‹Lâ€‹Sâ€‹-â€‹D\mathcal{W}^{FGLS\text{-}D},â€ and â€œğ’²Fâ€‹Gâ€‹Lâ€‹Sâˆ’Câ€‹O\mathcal{W}^{FGLS-CO}â€ denote the Wald statistics based on the bootstrap-based bias-corrected GD estimator, the GD estimator, the FGLS-D estimator, and the FGLS-CO estimator, respectively.
* (2)

  â€œMB,â€ â€œMO,â€ and â€œMIâ€ denotes the portfolios sorted by â€œmarket capitalization (size of firms) and book-to-market ratio,â€ â€œmarket capitalization and profitability,â€ and â€œmarket capitalization and investment growth rate,â€ respectively.
* (3)

  pp-values for the test statistics under the null hypothesis are in parentheses.
* (4)

  R version 4.5.2 was used to compute the statistics.

## Appendix

### A.1 Proof of PropositionÂ [1](https://arxiv.org/html/2601.21272v1#Thmproposition1 "Proposition 1: â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")

(i) Define the innovation

|  |  |  |
| --- | --- | --- |
|  | ğœºt:=ğ’›Â¯tâˆ’ğ’«tâˆ’1â€‹[ğ’›Â¯t],{\mbox{$\varepsilon$}}\_{t}:=\bar{{\mbox{$z$}}}\_{t}-\mathscr{P}\_{t-1}[\bar{{\mbox{$z$}}}\_{t}], |  |

where ğ’«tâˆ’1:L2â†’â„‹tâˆ’1\mathscr{P}\_{t-1}:L^{2}\to\mathscr{H}\_{t-1} is the L2L^{2}-orthogonal projection onto â„‹tâˆ’1:=spanÂ¯â€‹{ğ’›Â¯tâˆ’1,ğ’›Â¯tâˆ’2,â€¦}\mathscr{H}\_{t-1}:=\overline{\mathrm{span}}\{\bar{{\mbox{$z$}}}\_{t-1},\bar{{\mbox{$z$}}}\_{t-2},\ldots\}. By the projection theorem (e.g. Theorem 2.3.1 in Brockwell and Davis ([1991](https://arxiv.org/html/2601.21272v1#bib.bib580 "Time series: theory and methods"))), ğœºtâˆˆL2{\mbox{$\varepsilon$}}\_{t}\in L^{2} and ğœºtâŸ‚â„‹tâˆ’1{\mbox{$\varepsilon$}}\_{t}\perp\mathscr{H}\_{t-1}, i.e., ğ”¼â€‹[ğœºtâ€²â€‹ğ‘½]=0{\mathbb{E}}[{\mbox{$\varepsilon$}}\_{t}^{\prime}{\mbox{$V$}}]=0 for all ğ‘½âˆˆâ„‹tâˆ’1{\mbox{$V$}}\in\mathscr{H}\_{t-1}. In particular, ğ”¼â€‹[ğœºtâ€‹ğ’›Â¯tâˆ’â„“â€²]=ğŸ{\mathbb{E}}[{\mbox{$\varepsilon$}}\_{t}\bar{{\mbox{$z$}}}\_{t-\ell}^{\prime}]={\mbox{$0$}} for all â„“â‰¥1\ell\geq 1, and since ğœºtâˆ’hâˆˆâ„‹tâˆ’1{\mbox{$\varepsilon$}}\_{t-h}\in\mathscr{H}\_{t-1} for all hâ‰¥1h\geq 1 (because ğœºtâˆ’hâˆˆâ„‹tâˆ’hâŠ†â„‹tâˆ’1{\mbox{$\varepsilon$}}\_{t-h}\in\mathscr{H}\_{t-h}\subseteq\mathscr{H}\_{t-1}), we have ğ”¼â€‹[ğœºtâ€‹ğœºtâˆ’hâ€²]=ğŸ{\mathbb{E}}[{\mbox{$\varepsilon$}}\_{t}{\mbox{$\varepsilon$}}\_{t-h}^{\prime}]={\mbox{$0$}} for hâ‰¥1h\geq 1. Moreover, ğ”¼â€‹[ğ’›Â¯t]=ğŸ{\mathbb{E}}[\bar{{\mbox{$z$}}}\_{t}]={\mbox{$0$}} and every element of â„‹tâˆ’1\mathscr{H}\_{t-1} has mean zero, hence ğ”¼â€‹[ğœºt]=ğŸ{\mathbb{E}}[{\mbox{$\varepsilon$}}\_{t}]={\mbox{$0$}}. Because {ğ’›Â¯t}\{\bar{{\mbox{$z$}}}\_{t}\} is covariance-stationary, the variance ğšº:=ğ”¼â€‹[ğœºtâ€‹ğœºtâ€²]{\mbox{$\Sigma$}}:={\mathbb{E}}[{\mbox{$\varepsilon$}}\_{t}{\mbox{$\varepsilon$}}\_{t}^{\prime}] is constant in tt. Thus {ğœºt}\{{\mbox{$\varepsilon$}}\_{t}\} is (second-order) white noise.

By definition,

|  |  |  |
| --- | --- | --- |
|  | ğ’›Â¯t=ğ’«tâˆ’1â€‹[ğ’›Â¯t]+ğœºt,ğ’«tâˆ’1â€‹[ğ’›Â¯t]âˆˆâ„‹tâˆ’1,ğœºtâŸ‚â„‹tâˆ’1.\bar{{\mbox{$z$}}}\_{t}=\mathscr{P}\_{t-1}[\bar{{\mbox{$z$}}}\_{t}]+{\mbox{$\varepsilon$}}\_{t},\qquad\mathscr{P}\_{t-1}[\bar{{\mbox{$z$}}}\_{t}]\in\mathscr{H}\_{t-1},\quad{\mbox{$\varepsilon$}}\_{t}\perp\mathscr{H}\_{t-1}. |  |

Since â„‹tâˆ’1âŠ‚â„‹t\mathscr{H}\_{t-1}\subset\mathscr{H}\_{t} and ğœºtâˆˆâ„‹t{\mbox{$\varepsilon$}}\_{t}\in\mathscr{H}\_{t}, we obtain the orthogonal direct sum

|  |  |  |
| --- | --- | --- |
|  | â„‹t=â„‹tâˆ’1âŠ•spanâ€‹{ğœºt}.\mathscr{H}\_{t}=\mathscr{H}\_{t-1}\ \oplus\ \mathrm{span}\{{\mbox{$\varepsilon$}}\_{t}\}. |  |

Iterating this identity yields, for any mâ‰¥0m\geq 0,

|  |  |  |
| --- | --- | --- |
|  | â„‹t=â„‹tâˆ’mâˆ’1âŠ•â¨j=0mspanâ€‹{ğœºtâˆ’j}.\mathscr{H}\_{t}=\mathscr{H}\_{t-m-1}\ \oplus\ \bigoplus\_{j=0}^{m}\mathrm{span}\{{\mbox{$\varepsilon$}}\_{t-j}\}. |  |

Taking mâ†’âˆm\to\infty and using that orthogonal direct sums of closed subspaces are closed, we get

|  |  |  |
| --- | --- | --- |
|  | â„‹t=(â‹‚mâ‰¥0â„‹tâˆ’m)âŠ•spanÂ¯â€‹{ğœºt,ğœºtâˆ’1,â€¦}.\mathscr{H}\_{t}=\Big(\bigcap\_{m\geq 0}\mathscr{H}\_{t-m}\Big)\ \oplus\ \overline{\mathrm{span}}\{{\mbox{$\varepsilon$}}\_{t},{\mbox{$\varepsilon$}}\_{t-1},\ldots\}. |  |

By pure nondeterminism in AssumptionÂ [1](https://arxiv.org/html/2601.21272v1#Thmassumption1 "Assumption 1 (Finite-predictor exactness at lag ğ‘â‚€): â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"), â‹‚mâ‰¥0â„‹tâˆ’m={ğŸ}\bigcap\_{m\geq 0}\mathscr{H}\_{t-m}=\{{\mbox{$0$}}\}, hence

|  |  |  |
| --- | --- | --- |
|  | â„‹t=spanÂ¯â€‹{ğœºt,ğœºtâˆ’1,â€¦}.\mathscr{H}\_{t}=\overline{\mathrm{span}}\{{\mbox{$\varepsilon$}}\_{t},{\mbox{$\varepsilon$}}\_{t-1},\ldots\}. |  |

Because {ğœºtâˆ’h}hâ‰¥0\{{\mbox{$\varepsilon$}}\_{t-h}\}\_{h\geq 0} is a mutually orthogonal family that densely spans
â„‹t\mathscr{H}\_{t}, every element of â„‹t\mathscr{H}\_{t} admits a unique L2L^{2}-orthogonal expansion.
In particular, there exists a unique sequence of deterministic matrices
{ğšµh}hâ‰¥0âŠ‚â„mÃ—m\{{\mbox{$\Xi$}}\_{h}\}\_{h\geq 0}\subset\mathbb{R}^{m\times m} such that

|  |  |  |
| --- | --- | --- |
|  | ğ’›Â¯t=âˆ‘h=0âˆğšµhâ€‹ğœºtâˆ’hinÂ â€‹L2.\bar{{\mbox{$z$}}}\_{t}=\sum\_{h=0}^{\infty}{\mbox{$\Xi$}}\_{h}\,{\mbox{$\varepsilon$}}\_{t-h}\quad\text{in }L^{2}. |  |

Moreover, the normal equations give the coefficient formula

|  |  |  |
| --- | --- | --- |
|  | ğšµh=ğ”¼â€‹[ğ’›Â¯tâ€‹ğœºtâˆ’hâ€²]â€‹ğšºâˆ’1(hâ‰¥0),{\mbox{$\Xi$}}\_{h}={\mathbb{E}}[\bar{{\mbox{$z$}}}\_{t}{\mbox{$\varepsilon$}}\_{t-h}^{\prime}]\,{\mbox{$\Sigma$}}^{-1}\qquad(h\geq 0), |  |

provided ğšº>0{\mbox{$\Sigma$}}>0 (as assumed). In particular, ğšµ0=ğ”¼â€‹[ğ’›Â¯tâ€‹ğœºtâ€²]â€‹ğšºâˆ’1=ğšºğšºâˆ’1=ğ‘°m{\mbox{$\Xi$}}\_{0}={\mathbb{E}}[\bar{{\mbox{$z$}}}\_{t}{\mbox{$\varepsilon$}}\_{t}^{\prime}]\,{\mbox{$\Sigma$}}^{-1}={\mbox{$\Sigma$}}{\mbox{$\Sigma$}}^{-1}={\mbox{$I$}}\_{m}. Adding back the mean yields the Wold representation

|  |  |  |
| --- | --- | --- |
|  | ğ’›t=ğz+âˆ‘h=0âˆğšµhâ€‹ğœºtâˆ’hinÂ â€‹L2.{\mbox{$z$}}\_{t}={\mbox{$\mu$}}\_{z}+\sum\_{h=0}^{\infty}{\mbox{$\Xi$}}\_{h}\,{\mbox{$\varepsilon$}}\_{t-h}\quad\text{in }L^{2}. |  |

âˆ

(ii) Finite-predictor exactness at lag p0p\_{0} implies that there exist matrices ğ‘¨j(p){\mbox{$A$}}\_{j}^{(p)} such that

|  |  |  |
| --- | --- | --- |
|  | ğ’«tâˆ’1â€‹[ğ’›Â¯t]=ğ’«tâˆ’1(p)â€‹[ğ’›Â¯t]=âˆ‘j=1pğ‘¨j(p)â€‹ğ’›Â¯tâˆ’j.\mathscr{P}\_{t-1}[\bar{{\mbox{$z$}}}\_{t}]=\mathscr{P}^{(p)}\_{t-1}[\bar{{\mbox{$z$}}}\_{t}]=\sum\_{j=1}^{p}{\mbox{$A$}}\_{j}^{(p)}\bar{{\mbox{$z$}}}\_{t-j}. |  |

Since ğœºt:=ğ’›Â¯tâˆ’ğ’«tâˆ’1â€‹[ğ’›Â¯t]{\mbox{$\varepsilon$}}\_{t}:=\bar{{\mbox{$z$}}}\_{t}-\mathscr{P}\_{t-1}[\bar{{\mbox{$z$}}}\_{t}], we obtain

|  |  |  |
| --- | --- | --- |
|  | ğ’›Â¯t=âˆ‘j=1pğ‘¨j(p)â€‹ğ’›Â¯tâˆ’j+ğœºtinâ€‹L2.\bar{{\mbox{$z$}}}\_{t}=\sum\_{j=1}^{p}{\mbox{$A$}}\_{j}^{(p)}\bar{{\mbox{$z$}}}\_{t-j}+{\mbox{$\varepsilon$}}\_{t}\quad\text{in}\,L^{2}. |  |

Setting ğš¿j:=ğ‘¨j(p){\mbox{$\Psi$}}\_{j}:={\mbox{$A$}}\_{j}^{(p)} for j=1,â€¦,pj=1,\dots,p yields ([4](https://arxiv.org/html/2601.21272v1#S2.E4 "In item (ii) â€£ Proposition 1: â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")). Substituting the Wold expansions ğ’›Â¯tâˆ’j=âˆ‘h=0âˆğšµhâ€‹ğœºtâˆ’jâˆ’h\bar{{\mbox{$z$}}}\_{t-j}=\sum\_{h=0}^{\infty}{\mbox{$\Xi$}}\_{h}{\mbox{$\varepsilon$}}\_{t-j-h} and reindexing by i=j+hi=j+h (first for finite truncations, then letting the truncation â†’âˆ\to\infty in L2L^{2}) gives

|  |  |  |
| --- | --- | --- |
|  | ğ’›Â¯t=ğœºt+âˆ‘i=1âˆ(âˆ‘j=1minâ¡{p,i}ğ‘¨j(p)ğšµiâˆ’j)ğœºtâˆ’i=:âˆ‘i=0âˆğ‘«i(p)ğœºtâˆ’i,\bar{{\mbox{$z$}}}\_{t}={\mbox{$\varepsilon$}}\_{t}+\sum\_{i=1}^{\infty}\Big(\sum\_{j=1}^{\min\{p,i\}}{\mbox{$A$}}\_{j}^{(p)}{\mbox{$\Xi$}}\_{i-j}\Big){\mbox{$\varepsilon$}}\_{t-i}=:\sum\_{i=0}^{\infty}{\mbox{$D$}}\_{i}^{(p)}{\mbox{$\varepsilon$}}\_{t-i}, |  |

where ğ‘«0(p)=ğ‘°m{\mbox{$D$}}\_{0}^{(p)}={\mbox{$I$}}\_{m} and ğ‘«i(p):=âˆ‘j=1minâ¡{p,i}ğ‘¨j(p)â€‹ğšµiâˆ’j{\mbox{$D$}}\_{i}^{(p)}:=\sum\_{j=1}^{\min\{p,i\}}{\mbox{$A$}}\_{j}^{(p)}{\mbox{$\Xi$}}\_{i-j} for iâ‰¥1i\geq 1. On the other hand, (i) gives ğ’›Â¯t=âˆ‘i=0âˆğšµiâ€‹ğœºtâˆ’i\bar{{\mbox{$z$}}}\_{t}=\sum\_{i=0}^{\infty}{\mbox{$\Xi$}}\_{i}{\mbox{$\varepsilon$}}\_{t-i}, and the innovations are mutually orthogonal. Hence coefficients are unique, and for all iâ‰¥1i\geq 1,

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğšµi=âˆ‘j=1minâ¡{p,i}ğ‘¨j(p)â€‹ğšµiâˆ’j.{\mbox{$\Xi$}}\_{i}=\sum\_{j=1}^{\min\{p,i\}}{\mbox{$A$}}\_{j}^{(p)}{\mbox{$\Xi$}}\_{i-j}. |  | (35) |

Define {ğš¿j}jâ‰¥1\{{\mbox{$\Psi$}}\_{j}\}\_{j\geq 1} recursively by

|  |  |  |
| --- | --- | --- |
|  | ğš¿1:=ğšµ1,ğš¿n:=ğšµnâˆ’âˆ‘j=1nâˆ’1ğš¿jâ€‹ğšµnâˆ’j(nâ‰¥2).{\mbox{$\Psi$}}\_{1}:={\mbox{$\Xi$}}\_{1},\qquad{\mbox{$\Psi$}}\_{n}:={\mbox{$\Xi$}}\_{n}-\sum\_{j=1}^{n-1}{\mbox{$\Psi$}}\_{j}{\mbox{$\Xi$}}\_{n-j}\quad(n\geq 2). |  |

A simple induction shows that, for every iâ‰¥1i\geq 1,

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğšµi=âˆ‘j=1iğš¿jâ€‹ğšµiâˆ’j.{\mbox{$\Xi$}}\_{i}=\sum\_{j=1}^{i}{\mbox{$\Psi$}}\_{j}{\mbox{$\Xi$}}\_{i-j}. |  | (36) |

Comparing ([35](https://arxiv.org/html/2601.21272v1#Sx2.E35 "In A.1 Proof of Proposition 1 â€£ Appendix â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) and ([36](https://arxiv.org/html/2601.21272v1#Sx2.E36 "In A.1 Proof of Proposition 1 â€£ Appendix â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) successively for i=1,2,â€¦i=1,2,\dots and using ğšµ0=ğ‘°m{\mbox{$\Xi$}}\_{0}={\mbox{$I$}}\_{m} gives

|  |  |  |
| --- | --- | --- |
|  | ğš¿j=ğ‘¨j(p)(j=1,â€¦,p),ğš¿j=ğŸ(j>p),{\mbox{$\Psi$}}\_{j}={\mbox{$A$}}\_{j}^{(p)}\quad(j=1,\ldots,p),\qquad{\mbox{$\Psi$}}\_{j}={\mbox{$0$}}\quad(j>p), |  |

so the VAR(pp) coefficients in ([4](https://arxiv.org/html/2601.21272v1#S2.E4 "In item (ii) â€£ Proposition 1: â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) are unique. Finally, from ([36](https://arxiv.org/html/2601.21272v1#Sx2.E36 "In A.1 Proof of Proposition 1 â€£ Appendix â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) we have, for nâ‰¥1n\geq 1,

|  |  |  |
| --- | --- | --- |
|  | ğšµ0=ğ‘°m,ğšµn=âˆ‘j=1nğš¿jâ€‹ğšµnâˆ’j,{\mbox{$\Xi$}}\_{0}={\mbox{$I$}}\_{m},\qquad{\mbox{$\Xi$}}\_{n}=\sum\_{j=1}^{n}{\mbox{$\Psi$}}\_{j}{\mbox{$\Xi$}}\_{n-j}, |  |

which is equivalent to ğš¿â€‹(L)â€‹ğšµâ€‹(L)=ğ‘°m{\mbox{$\Psi$}}(L){\mbox{$\Xi$}}(L)={\mbox{$I$}}\_{m} with
ğš¿â€‹(L):=ğ‘°mâˆ’âˆ‘j=1pğš¿jâ€‹Lj{\mbox{$\Psi$}}(L):={\mbox{$I$}}\_{m}-\sum\_{j=1}^{p}{\mbox{$\Psi$}}\_{j}L^{j} and
ğšµâ€‹(L):=ğ‘°m+âˆ‘n=1âˆğšµnâ€‹Ln{\mbox{$\Xi$}}(L):={\mbox{$I$}}\_{m}+\sum\_{n=1}^{\infty}{\mbox{$\Xi$}}\_{n}L^{n}. âˆ

(iii) Define the companion state ğ’”t:=[ğ’›Â¯tâ€²,ğ’›Â¯tâˆ’1â€²,â€¦,ğ’›Â¯tâˆ’p+1â€²]â€²âˆˆâ„mâ€‹p{\mbox{$s$}}\_{t}:=[\bar{{\mbox{$z$}}}\_{t}^{\prime},\bar{{\mbox{$z$}}}\_{t-1}^{\prime},\ldots,\bar{{\mbox{$z$}}}\_{t-p+1}^{\prime}]^{\prime}\in\mathbb{R}^{mp} and matrices

|  |  |  |
| --- | --- | --- |
|  | ğ‘­:=[ğš¿1ğš¿2â‹¯ğš¿pğ‘°mğŸâ‹¯ğŸâ‹®â‹±â‹±â‹®ğŸâ‹¯ğ‘°mğŸ],ğ‘®:=[ğ‘°mğŸâ‹®ğŸ].{\mbox{$F$}}:=\begin{bmatrix}{\mbox{$\Psi$}}\_{1}&{\mbox{$\Psi$}}\_{2}&\cdots&{\mbox{$\Psi$}}\_{p}\\ {\mbox{$I$}}\_{m}&{\mbox{$0$}}&\cdots&{\mbox{$0$}}\\ \vdots&\ddots&\ddots&\vdots\\ {\mbox{$0$}}&\cdots&{\mbox{$I$}}\_{m}&{\mbox{$0$}}\end{bmatrix},\qquad{\mbox{$G$}}:=\begin{bmatrix}{\mbox{$I$}}\_{m}\\ {\mbox{$0$}}\\ \vdots\\ {\mbox{$0$}}\end{bmatrix}. |  |

Then ğ’”t=ğ‘­ğ’”tâˆ’1+ğ‘®ğœºt{\mbox{$s$}}\_{t}={\mbox{$F$}}{\mbox{$s$}}\_{t-1}+{\mbox{$G$}}{\mbox{$\varepsilon$}}\_{t} in L2L^{2}. Let ğšºs:=ğ”¼â€‹[ğ’”tâ€‹ğ’”tâ€²]{\mbox{$\Sigma$}}\_{s}:={\mathbb{E}}[{\mbox{$s$}}\_{t}{\mbox{$s$}}\_{t}^{\prime}] denote the (finite) state covariance (stationarity).

Suppose by contradiction that ğ‘­F has an eigenvalue Î»\lambda with |Î»|â‰¥1|\lambda|\geq 1 and a corresponding left eigenvector ğ’—â‰ ğŸ{\mbox{$v$}}\neq{\mbox{$0$}} such that ğ’—â€²â€‹ğ‘­=Î»â€‹ğ’—â€²{\mbox{$v$}}^{\prime}{\mbox{$F$}}=\lambda\,{\mbox{$v$}}^{\prime}. Set the scalar process wt:=ğ’—â€²â€‹ğ’”tw\_{t}:={\mbox{$v$}}^{\prime}{\mbox{$s$}}\_{t} and Î·t:=ğ’—â€²â€‹ğ‘®ğœºt\eta\_{t}:={\mbox{$v$}}^{\prime}{\mbox{$G$}}{\mbox{$\varepsilon$}}\_{t}. Then wt=Î»â€‹wtâˆ’1+Î·tw\_{t}=\lambda w\_{t-1}+\eta\_{t} and, since ğœºtâŸ‚â„‹tâˆ’1{\mbox{$\varepsilon$}}\_{t}\perp\mathscr{H}\_{t-1}, Î·t\eta\_{t} is uncorrelated with wtâˆ’1w\_{t-1}, so

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[wt2]=|Î»|2â€‹ğ”¼â€‹[wtâˆ’12]+ğ”¼â€‹[Î·t2].{\mathbb{E}}[w\_{t}^{2}]=|\lambda|^{2}{\mathbb{E}}[w\_{t-1}^{2}]+{\mathbb{E}}[\eta\_{t}^{2}]. |  |

By stationarity ğ”¼[wt2]=ğ”¼[wtâˆ’12]=:Ïƒw2{\mathbb{E}}[w\_{t}^{2}]={\mathbb{E}}[w\_{t-1}^{2}]=:\sigma\_{w}^{2}, hence

|  |  |  |  |
| --- | --- | --- | --- |
|  | (1âˆ’|Î»|2)â€‹Ïƒw2=ğ”¼â€‹[Î·t2]=ğ’—â€²â€‹ğ‘®â€‹ğšºâ€‹ğ‘®â€²â€‹ğ’—.(1-|\lambda|^{2})\,\sigma\_{w}^{2}={\mathbb{E}}[\eta\_{t}^{2}]={\mbox{$v$}}^{\prime}{\mbox{$G$}}\,{\mbox{$\Sigma$}}\,{\mbox{$G$}}^{\prime}{\mbox{$v$}}. |  | (37) |

Write ğ’—â€²=(ğ’—1â€²,â€¦,ğ’—pâ€²){\mbox{$v$}}^{\prime}=({\mbox{$v$}}\_{1}^{\prime},\ldots,{\mbox{$v$}}\_{p}^{\prime}) with ğ’—jâˆˆâ„m{\mbox{$v$}}\_{j}\in\mathbb{R}^{m}. From ğ’—â€²â€‹ğ‘­=Î»â€‹ğ’—â€²{\mbox{$v$}}^{\prime}{\mbox{$F$}}=\lambda{\mbox{$v$}}^{\prime} one checks the block recursions

|  |  |  |
| --- | --- | --- |
|  | ğ’—1â€²â€‹ğš¿1+ğ’—2â€²=Î»â€‹ğ’—1â€²,ğ’—1â€²â€‹ğš¿2+ğ’—3â€²=Î»â€‹ğ’—2â€²,â€¦,ğ’—1â€²â€‹ğš¿p=Î»â€‹ğ’—pâ€².{\mbox{$v$}}\_{1}^{\prime}{\mbox{$\Psi$}}\_{1}+{\mbox{$v$}}\_{2}^{\prime}=\lambda{\mbox{$v$}}\_{1}^{\prime},\quad{\mbox{$v$}}\_{1}^{\prime}{\mbox{$\Psi$}}\_{2}+{\mbox{$v$}}\_{3}^{\prime}=\lambda{\mbox{$v$}}\_{2}^{\prime},\ \ldots,\ {\mbox{$v$}}\_{1}^{\prime}{\mbox{$\Psi$}}\_{p}=\lambda{\mbox{$v$}}\_{p}^{\prime}. |  |

If ğ’—1â€²=ğŸ{\mbox{$v$}}\_{1}^{\prime}={\mbox{$0$}}, then the block recursions imply successively ğ’—pâ€²=â‹¯=ğ’—2â€²=ğŸ{\mbox{$v$}}\_{p}^{\prime}=\cdots={\mbox{$v$}}\_{2}^{\prime}={\mbox{$0$}}, hence ğ’—=ğŸ{\mbox{$v$}}={\mbox{$0$}}, a contradiction. Therefore ğ’—1â‰ ğŸ{\mbox{$v$}}\_{1}\neq{\mbox{$0$}}. Since ğ‘®=[ğ‘°m,ğŸ,â€¦,ğŸ]â€²{\mbox{$G$}}=[{\mbox{$I$}}\_{m},\ {\mbox{$0$}},\ldots,{\mbox{$0$}}]^{\prime}, we have Î·t=ğ’—â€²â€‹ğ‘®â€‹ğœºt=ğ’—1â€²â€‹ğœºt\eta\_{t}={\mbox{$v$}}^{\prime}{\mbox{$G$}}\,{\mbox{$\varepsilon$}}\_{t}={\mbox{$v$}}\_{1}^{\prime}{\mbox{$\varepsilon$}}\_{t} and ğ”¼â€‹[Î·t2]=ğ’—1â€²â€‹ğšºğ’—1>0{\mathbb{E}}[\eta\_{t}^{2}]={\mbox{$v$}}\_{1}^{\prime}{\mbox{$\Sigma$}}{\mbox{$v$}}\_{1}>0.

Moreover ğ’”tâˆ’1âˆˆâ„‹tâˆ’1{\mbox{$s$}}\_{t-1}\in\mathscr{H}\_{t-1} and ğœºtâŸ‚â„‹tâˆ’1{\mbox{$\varepsilon$}}\_{t}\perp\mathscr{H}\_{t-1}, so

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[wtâˆ’1â€‹Î·t]=ğ’—â€²â€‹ğ”¼â€‹[ğ’”tâˆ’1â€‹ğœºtâ€²]â€‹ğ‘®â€²â€‹ğ’—=0.{\mathbb{E}}[w\_{t-1}\eta\_{t}]={\mbox{$v$}}^{\prime}\,{\mathbb{E}}[{\mbox{$s$}}\_{t-1}{\mbox{$\varepsilon$}}\_{t}^{\prime}]\,{\mbox{$G$}}^{\prime}{\mbox{$v$}}=0. |  |

From wt=Î»â€‹wtâˆ’1+Î·tw\_{t}=\lambda w\_{t-1}+\eta\_{t} we get (using stationarity ğ”¼[wt2]=ğ”¼[wtâˆ’12]=:Ïƒw2<âˆ{\mathbb{E}}[w\_{t}^{2}]={\mathbb{E}}[w\_{t-1}^{2}]=:\sigma\_{w}^{2}<\infty) Ïƒw2=ğ”¼â€‹[wt2]=|Î»|2â€‹Ïƒw2+ğ”¼â€‹[Î·t2]\sigma\_{w}^{2}={\mathbb{E}}[w\_{t}^{2}]=|\lambda|^{2}\sigma\_{w}^{2}+{\mathbb{E}}[\eta\_{t}^{2}] and (1âˆ’|Î»|2)â€‹Ïƒw2=ğ”¼â€‹[Î·t2](1-|\lambda|^{2})\sigma\_{w}^{2}={\mathbb{E}}[\eta\_{t}^{2}]. The right-hand side is strictly positive, hence 1âˆ’|Î»|2>01-|\lambda|^{2}>0, i.e. |Î»|<1|\lambda|<1. This contradicts the assumption |Î»|â‰¥1|\lambda|\geq 1. Therefore the companion matrix ğ‘­F has no eigenvalue with modulus â‰¥1\geq 1; in particular Ïâ€‹(ğ‘­)<1\rho({\mbox{$F$}})<1.

Next, by forward iteration,

|  |  |  |
| --- | --- | --- |
|  | ğ’”t=ğ‘­kâ€‹ğ’”tâˆ’k+âˆ‘j=0kâˆ’1ğ‘­jâ€‹ğ‘®â€‹ğœºtâˆ’j.{\mbox{$s$}}\_{t}={\mbox{$F$}}^{k}{\mbox{$s$}}\_{t-k}+\sum\_{j=0}^{k-1}{\mbox{$F$}}^{\,j}{\mbox{$G$}}\,{\mbox{$\varepsilon$}}\_{t-j}. |  |

Since Ïâ€‹(ğ‘­)<1\rho({\mbox{$F$}})<1, by Gelfandâ€™s formula there exist constants M<âˆM<\infty and Ïâˆˆ(0,1)\rho\in(0,1) such that
â€–ğ‘­jâ€–â‰¤Mâ€‹Ïj\|{\mbox{$F$}}^{\,j}\|\leq M\rho^{\,j} for all jâ‰¥0j\geq 0. Stationarity implies ğ”¼â€‹â€–ğ’”tâ€–2<âˆ{\mathbb{E}}\|{\mbox{$s$}}\_{t}\|^{2}<\infty, hence

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹â€–ğ‘­kâ€‹ğ’”tâˆ’kâ€–2â‰¤â€–ğ‘­kâ€–2â€‹ğ”¼â€‹â€–ğ’”tâˆ’kâ€–2â‰¤M2â€‹Ïâ€‰2â€‹kâ€‹ğ”¼â€‹â€–ğ’”tâ€–2â†’0,asâ€‹kâ†’âˆ,{\mathbb{E}}\|{\mbox{$F$}}^{k}{\mbox{$s$}}\_{t-k}\|^{2}\leq\|{\mbox{$F$}}^{k}\|^{2}\,{\mathbb{E}}\|{\mbox{$s$}}\_{t-k}\|^{2}\leq M^{2}\rho^{\,2k}\,{\mathbb{E}}\|{\mbox{$s$}}\_{t}\|^{2}\ \rightarrow 0,\quad\text{as}\ k\to\infty, |  |

so ğ‘­kâ€‹ğ’”tâˆ’kâ†’ğŸ{\mbox{$F$}}^{k}{\mbox{$s$}}\_{t-k}\to{\mbox{$0$}} in L2L^{2}. Letting kâ†’âˆk\to\infty gives the L2L^{2} expansion

|  |  |  |
| --- | --- | --- |
|  | ğ’”t=âˆ‘i=0âˆğ‘­iâ€‹ğ‘®â€‹ğœºtâˆ’i.{\mbox{$s$}}\_{t}=\sum\_{i=0}^{\infty}{\mbox{$F$}}^{\,i}{\mbox{$G$}}\,{\mbox{$\varepsilon$}}\_{t-i}. |  |

Let ğ‘±:=[ğ‘°m,ğŸ,â€¦,ğŸ]âˆˆâ„mÃ—mâ€‹p{\mbox{$J$}}:=[{\mbox{$I$}}\_{m},\ {\mbox{$0$}},\ \ldots,\ {\mbox{$0$}}]\in\mathbb{R}^{m\times mp} be the selector of the first mm-block. Then

|  |  |  |
| --- | --- | --- |
|  | ğ’›Â¯t=ğ‘±ğ’”t=âˆ‘i=0âˆ(ğ‘±ğ‘­iğ‘®)ğœºtâˆ’i=:âˆ‘i=0âˆğšµiğœºtâˆ’i,ğšµi:=ğ‘±ğ‘­iğ‘®.\bar{{\mbox{$z$}}}\_{t}={\mbox{$J$}}{\mbox{$s$}}\_{t}=\sum\_{i=0}^{\infty}\big({\mbox{$J$}}{\mbox{$F$}}^{\,i}{\mbox{$G$}}\big)\,{\mbox{$\varepsilon$}}\_{t-i}=:\sum\_{i=0}^{\infty}{\mbox{$\Xi$}}\_{i}\,{\mbox{$\varepsilon$}}\_{t-i},\qquad{\mbox{$\Xi$}}\_{i}:={\mbox{$J$}}{\mbox{$F$}}^{\,i}{\mbox{$G$}}. |  |

Using a submultiplicative operator norm,

|  |  |  |
| --- | --- | --- |
|  | â€–ğšµiâ€–=â€–ğ‘±ğ‘­iâ€‹ğ‘®â€–â‰¤â€–ğ‘±â€–â€‹â€–ğ‘­iâ€–â€‹â€–ğ‘®â€–â‰¤(â€–ğ‘±â€–â€‹â€–ğ‘®â€–â€‹M)â€‹Ïi=Mâ€‹Ïi,\|{\mbox{$\Xi$}}\_{i}\|=\|{\mbox{$J$}}{\mbox{$F$}}^{i}{\mbox{$G$}}\|\ \leq\ \|{\mbox{$J$}}\|\,\|{\mbox{$F$}}^{\,i}\|\,\|{\mbox{$G$}}\|\ \leq\ (\|{\mbox{$J$}}\|\,\|{\mbox{$G$}}\|\,M)\,\rho^{\,i}=M\rho^{i}, |  |

where â€–ğ‘­iâ€–â‰¤Mâ€‹Ïi\|{\mbox{$F$}}^{i}\|\leq M\rho^{i} with 0<Ï<10<\rho<1. Since ğšµ0=ğ‘±ğ‘®=ğ‘°m{\mbox{$\Xi$}}\_{0}={\mbox{$J$}}{\mbox{$G$}}={\mbox{$I$}}\_{m},

|  |  |  |
| --- | --- | --- |
|  | âˆ‘i=0âˆâ€–ğšµiâ€–=â€–ğšµ0â€–+âˆ‘i=1âˆâ€–ğšµiâ€–â‰¤â€–ğ‘°mâ€–+Mâ€‹âˆ‘i=1âˆÏi=1+Mâ€‹Ï1âˆ’Ï<âˆ.\sum\_{i=0}^{\infty}\|{\mbox{$\Xi$}}\_{i}\|=\|{\mbox{$\Xi$}}\_{0}\|+\sum\_{i=1}^{\infty}\|{\mbox{$\Xi$}}\_{i}\|\ \leq\ \|{\mbox{$I$}}\_{m}\|+M\sum\_{i=1}^{\infty}\rho^{\,i}=1+M\,\frac{\rho}{1-\rho}\ <\ \infty. |  |

Thus the coefficients are absolutely summable. âˆ

(iv) By partsÂ (i)â€“(iii) (from AssumptionÂ [1](https://arxiv.org/html/2601.21272v1#Thmassumption1 "Assumption 1 (Finite-predictor exactness at lag ğ‘â‚€): â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")-(A1.1)), the centered process admits the Wold representation

|  |  |  |
| --- | --- | --- |
|  | ğ’›Â¯t=âˆ‘i=0âˆğšµiâ€‹ğœºtâˆ’iinÂ â€‹L2,âˆ‘i=0âˆâ€–ğšµiâ€–<âˆ,\bar{{\mbox{$z$}}}\_{t}=\sum\_{i=0}^{\infty}{\mbox{$\Xi$}}\_{i}{\mbox{$\varepsilon$}}\_{t-i}\quad\text{in }L^{2},\qquad\sum\_{i=0}^{\infty}\|{\mbox{$\Xi$}}\_{i}\|<\infty, |  |

where {ğœºt}\{{\mbox{$\varepsilon$}}\_{t}\} is the innovation sequence. Fix r:=4+2â€‹Î´>2r:=4+2\delta>2 and set Xi:=ğšµiâ€‹ğœºtâˆ’iX\_{i}:={\mbox{$\Xi$}}\_{i}{\mbox{$\varepsilon$}}\_{t-i}. Using a submultiplicative operator norm and stationarity of {ğœºt}\{{\mbox{$\varepsilon$}}\_{t}\},

|  |  |  |
| --- | --- | --- |
|  | â€–Xiâ€–Lr=(ğ”¼â€‹â€–ğšµiâ€‹ğœºtâˆ’iâ€–r)1/râ‰¤â€–ğšµiâ€–â€‹(ğ”¼â€‹â€–ğœºtâ€–r)1/r.\|X\_{i}\|\_{L^{r}}=\big({\mathbb{E}}\|{\mbox{$\Xi$}}\_{i}{\mbox{$\varepsilon$}}\_{t-i}\|^{r}\big)^{1/r}\leq\|{\mbox{$\Xi$}}\_{i}\|\big({\mathbb{E}}\|{\mbox{$\varepsilon$}}\_{t}\|^{r}\big)^{1/r}. |  |

AssumptionÂ [1](https://arxiv.org/html/2601.21272v1#Thmassumption1 "Assumption 1 (Finite-predictor exactness at lag ğ‘â‚€): â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")-(A1.2) gives ğ”¼â€‹â€–ğœºtâ€–r<âˆ{\mathbb{E}}\|{\mbox{$\varepsilon$}}\_{t}\|^{r}<\infty, and partÂ (iii) gives âˆ‘i=0âˆâ€–ğšµiâ€–<âˆ\sum\_{i=0}^{\infty}\|{\mbox{$\Xi$}}\_{i}\|<\infty. By Minkowskiâ€™s inequality,

|  |  |  |
| --- | --- | --- |
|  | â€–âˆ‘i=0âˆğšµiâ€‹ğœºtâˆ’iâ€–Lrâ‰¤âˆ‘i=0âˆâ€–ğšµiâ€‹ğœºtâˆ’iâ€–Lrâ‰¤(ğ”¼â€‹â€–ğœºtâ€–r)1/râ€‹âˆ‘i=0âˆâ€–ğšµiâ€–<âˆ,\Big\|\sum\_{i=0}^{\infty}{\mbox{$\Xi$}}\_{i}\,{\mbox{$\varepsilon$}}\_{t-i}\Big\|\_{L^{r}}\leq\sum\_{i=0}^{\infty}\|{\mbox{$\Xi$}}\_{i}\,{\mbox{$\varepsilon$}}\_{t-i}\|\_{L^{r}}\leq\big({\mathbb{E}}\|{\mbox{$\varepsilon$}}\_{t}\|^{r}\big)^{1/r}\sum\_{i=0}^{\infty}\|{\mbox{$\Xi$}}\_{i}\|<\infty, |  |

so the series âˆ‘i=0âˆğšµiâ€‹ğœºtâˆ’i\sum\_{i=0}^{\infty}{\mbox{$\Xi$}}\_{i}{\mbox{$\varepsilon$}}\_{t-i} converges in LrL^{r} and

|  |  |  |
| --- | --- | --- |
|  | â€–ğ’›Â¯tâ€–Lr=â€–âˆ‘i=0âˆğšµiâ€‹ğœºtâˆ’iâ€–Lrâ‰¤(ğ”¼â€‹â€–ğœºtâ€–r)1/râ€‹âˆ‘i=0âˆâ€–ğšµiâ€–<âˆ.\|\bar{{\mbox{$z$}}}\_{t}\|\_{L^{r}}=\Big\|\sum\_{i=0}^{\infty}{\mbox{$\Xi$}}\_{i}\,{\mbox{$\varepsilon$}}\_{t-i}\Big\|\_{L^{r}}\leq\big({\mathbb{E}}\|{\mbox{$\varepsilon$}}\_{t}\|^{r}\big)^{1/r}\sum\_{i=0}^{\infty}\|{\mbox{$\Xi$}}\_{i}\|<\infty. |  |

With the selection matrices ğ‘±x=[ğ‘°k,ğŸ]{\mbox{$J$}}\_{x}=[{\mbox{$I$}}\_{k},\ {\mbox{$0$}}] and ğ‘±u=[ğŸ,ğ‘°N]{\mbox{$J$}}\_{u}=[{\mbox{$0$}},\ {\mbox{$I$}}\_{N}],

|  |  |  |
| --- | --- | --- |
|  | ğ’™tâˆ’ğ”¼â€‹[ğ’™t]=ğ‘±xâ€‹ğ’›Â¯t=âˆ‘i=0âˆ(ğ‘±xâ€‹ğšµi)â€‹ğœºtâˆ’i,ğ’–t=ğ‘±uâ€‹ğ’›Â¯t=âˆ‘i=0âˆ(ğ‘±uâ€‹ğšµi)â€‹ğœºtâˆ’i.{\mbox{$x$}}\_{t}-{\mathbb{E}}[{\mbox{$x$}}\_{t}]={\mbox{$J$}}\_{x}\bar{{\mbox{$z$}}}\_{t}=\sum\_{i=0}^{\infty}({\mbox{$J$}}\_{x}{\mbox{$\Xi$}}\_{i}){\mbox{$\varepsilon$}}\_{t-i},\qquad{\mbox{$u$}}\_{t}={\mbox{$J$}}\_{u}\bar{{\mbox{$z$}}}\_{t}=\sum\_{i=0}^{\infty}({\mbox{$J$}}\_{u}{\mbox{$\Xi$}}\_{i}){\mbox{$\varepsilon$}}\_{t-i}. |  |

Since â€–ğ‘±xâ€‹ğšµiâ€–â‰¤â€–ğ‘±xâ€–â€‹â€–ğšµiâ€–\|{\mbox{$J$}}\_{x}{\mbox{$\Xi$}}\_{i}\|\leq\|{\mbox{$J$}}\_{x}\|\,\|{\mbox{$\Xi$}}\_{i}\| and similarly for ğ‘±u{\mbox{$J$}}\_{u}, the same argument yields

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹â€–ğ’™tâˆ’ğ”¼â€‹[ğ’™t]â€–r<âˆ,ğ”¼â€‹â€–ğ’–tâ€–r<âˆ.{\mathbb{E}}\|{\mbox{$x$}}\_{t}-{\mathbb{E}}[{\mbox{$x$}}\_{t}]\|^{r}<\infty,\qquad{\mathbb{E}}\|{\mbox{$u$}}\_{t}\|^{r}<\infty. |  |

Since adding constants does not affect finiteness of rr-th moments, we conclude
ğ”¼â€‹â€–ğ’™tâ€–r<âˆ{\mathbb{E}}\|{\mbox{$x$}}\_{t}\|^{r}<\infty and ğ”¼â€‹â€–ğ’–tâ€–r<âˆ{\mathbb{E}}\|{\mbox{$u$}}\_{t}\|^{r}<\infty.

Write ğ’št=ğœ¶+ğ‘¿tâ€²â€‹ğœ·+ğ’–t{\mbox{$y$}}\_{t}={\mbox{$\alpha$}}+{\mbox{$X$}}\_{t}^{\prime}{\mbox{$\beta$}}+{\mbox{$u$}}\_{t} and define ğ‘©:=diag(ğœ·1â€²,â€¦,ğœ·Nâ€²){\mbox{$B$}}:=\operatorname\*{diag}({\mbox{$\beta$}}\_{1}^{\prime},\ldots,{\mbox{$\beta$}}\_{N}^{\prime}) so that ğ‘¿tâ€²â€‹ğœ·=ğ‘©ğ’™t{\mbox{$X$}}\_{t}^{\prime}{\mbox{$\beta$}}={\mbox{$B$}}{\mbox{$x$}}\_{t}. Then

|  |  |  |
| --- | --- | --- |
|  | â€–ğ’štâ€–â‰¤â€–ğœ¶â€–+â€–ğ‘©â€–â€‹â€–ğ’™tâ€–+â€–ğ’–tâ€–.\|{\mbox{$y$}}\_{t}\|\leq\|{\mbox{$\alpha$}}\|+\|{\mbox{$B$}}\|\|{\mbox{$x$}}\_{t}\|+\|{\mbox{$u$}}\_{t}\|. |  |

By the HÃ¶lderâ€™s inequality,

|  |  |  |
| --- | --- | --- |
|  | â€–ğ’štâ€–râ‰¤3râˆ’1â€‹(â€–ğœ¶â€–r+â€–ğ‘©â€–râ€‹â€–ğ’™tâ€–r+â€–ğ’–tâ€–r)\|{\mbox{$y$}}\_{t}\|^{r}\leq 3^{r-1}\big(\|{\mbox{$\alpha$}}\|^{r}+\|{\mbox{$B$}}\|^{r}\|{\mbox{$x$}}\_{t}\|^{r}+\|{\mbox{$u$}}\_{t}\|^{r}\big) |  |

so ğ”¼â€‹â€–ğ’štâ€–r<âˆ{\mathbb{E}}\|{\mbox{$y$}}\_{t}\|^{r}<\infty follows from the already established finiteness of ğ”¼â€‹â€–ğ’™tâ€–r{\mathbb{E}}\|{\mbox{$x$}}\_{t}\|^{r} and ğ”¼â€‹â€–ğ’–tâ€–r{\mathbb{E}}\|{\mbox{$u$}}\_{t}\|^{r}.

For â„“â‰¥0\ell\geq 0,

|  |  |  |
| --- | --- | --- |
|  | â„‚â€‹ovâ€‹(ğ’›t,ğ’›tâˆ’â„“)=âˆ‘i=0âˆâˆ‘j=0âˆğšµiâ€‹ğ”¼â€‹[ğœºtâˆ’iâ€‹ğœºtâˆ’â„“âˆ’jâ€²]â€‹ğšµjâ€²=âˆ‘j=0âˆğšµâ„“+jâ€‹ğšºğšµjâ€²,{\mathbb{C}\rm{ov}}({\mbox{$z$}}\_{t},{\mbox{$z$}}\_{t-\ell})=\sum\_{i=0}^{\infty}\sum\_{j=0}^{\infty}{\mbox{$\Xi$}}\_{i}{\mathbb{E}}[{\mbox{$\varepsilon$}}\_{t-i}{\mbox{$\varepsilon$}}\_{t-\ell-j}^{\prime}]{\mbox{$\Xi$}}\_{j}^{\prime}=\sum\_{j=0}^{\infty}{\mbox{$\Xi$}}\_{\ell+j}{\mbox{$\Sigma$}}{\mbox{$\Xi$}}\_{j}^{\prime}, |  |

which depends only on â„“\ell. Moreover,

|  |  |  |
| --- | --- | --- |
|  | â€–â„‚â€‹ovâ€‹(ğ’›t,ğ’›tâˆ’â„“)â€–â‰¤â€–ğšºâ€–â€‹âˆ‘j=0âˆâ€–ğšµâ„“+jâ€–â€‹â€–ğšµjâ€–â‰¤â€–ğšºâ€–â€‹(âˆ‘n=0âˆâ€–ğšµnâ€–)2<âˆ.\big\|{\mathbb{C}\rm{ov}}({\mbox{$z$}}\_{t},{\mbox{$z$}}\_{t-\ell})\big\|\leq\|{\mbox{$\Sigma$}}\|\sum\_{j=0}^{\infty}\|{\mbox{$\Xi$}}\_{\ell+j}\|\|{\mbox{$\Xi$}}\_{j}\|\leq\|{\mbox{$\Sigma$}}\|\Big(\sum\_{n=0}^{\infty}\|{\mbox{$\Xi$}}\_{n}\|\Big)^{2}<\infty. |  |

Hence {ğ’›Â¯t}\{\bar{{\mbox{$z$}}}\_{t}\} is covariance-stationary. Since ğ’™t=ğ‘±xâ€‹ğ’›t{\mbox{$x$}}\_{t}={\mbox{$J$}}\_{x}{\mbox{$z$}}\_{t} and ğ’–t=ğ‘±uâ€‹ğ’›t{\mbox{$u$}}\_{t}={\mbox{$J$}}\_{u}{\mbox{$z$}}\_{t}, both {ğ’™t}\{{\mbox{$x$}}\_{t}\} and {ğ’–t}\{{\mbox{$u$}}\_{t}\} (and thus {ğ’št}\{{\mbox{$y$}}\_{t}\}) are covariance-stationary as fixed linear images of {ğ’›t}\{{\mbox{$z$}}\_{t}\}.

By AssumptionÂ [1](https://arxiv.org/html/2601.21272v1#Thmassumption1 "Assumption 1 (Finite-predictor exactness at lag ğ‘â‚€): â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")-(A1.2), the innovation sequence {ğœºt}\{{\mbox{$\varepsilon$}}\_{t}\} is strongly mixing with coefficients Î±Îµâ€‹(â„“)\alpha\_{\varepsilon}(\ell) such that âˆ‘â„“â‰¥1Î±Îµâ€‹(â„“)Î´/(2+Î´)<âˆ\sum\_{\ell\geq 1}\alpha\_{\varepsilon}(\ell)^{\delta/(2+\delta)}<\infty and has finite (4+2â€‹Î´)(4+2\delta)-th moments. Consider the mm-truncated linear process

|  |  |  |
| --- | --- | --- |
|  | ğ’›Â¯t(m):=âˆ‘i=0mğšµiâ€‹ğœºtâˆ’i.\bar{{\mbox{$z$}}}\_{t}^{(m)}:=\sum\_{i=0}^{m}{\mbox{$\Xi$}}\_{i}{\mbox{$\varepsilon$}}\_{t-i}. |  |

Then ğ’›Â¯t(m)\bar{{\mbox{$z$}}}\_{t}^{(m)} is a measurable function of (ğœºt,â€¦,ğœºtâˆ’m)({\mbox{$\varepsilon$}}\_{t},\ldots,{\mbox{$\varepsilon$}}\_{t-m}), and hence is strongly mixing with

|  |  |  |
| --- | --- | --- |
|  | Î±z(m)â€‹(â„“)â‰¤Î±Îµâ€‹(â„“âˆ’m)(â„“>m),\alpha\_{z^{(m)}}(\ell)\leq\alpha\_{\varepsilon}(\ell-m)\quad(\ell>m), |  |

because Ïƒâ€‹(ğ’›Â¯âˆ’âˆ(m),0)âŠ‚Ïƒâ€‹(ğœºâˆ’âˆ0)\sigma(\bar{{\mbox{$z$}}}\_{-\infty}^{(m),0})\subset\sigma({\mbox{$\varepsilon$}}\_{-\infty}^{0}) and Ïƒâ€‹(ğ’›Â¯â„“(m),âˆ)âŠ‚Ïƒâ€‹(ğœºâ„“âˆ’mâˆ)\sigma(\bar{{\mbox{$z$}}}\_{\ell}^{(m),\infty})\subset\sigma({\mbox{$\varepsilon$}}\_{\ell-m}^{\infty}). Moreover, by Minkowskiâ€™s inequality and âˆ‘i=0âˆâ€–ğšµiâ€–<âˆ\sum\_{i=0}^{\infty}\|{\mbox{$\Xi$}}\_{i}\|<\infty,

|  |  |  |
| --- | --- | --- |
|  | â€–ğ’›Â¯tâˆ’ğ’›Â¯t(m)â€–L4+2â€‹Î´â‰¤âˆ‘i>mâ€–ğšµiâ€–â€‹(ğ”¼â€‹â€–ğœºtâ€–4+2â€‹Î´)1/(4+2â€‹Î´)â†’mâ†’âˆ 0.\|\bar{{\mbox{$z$}}}\_{t}-\bar{{\mbox{$z$}}}\_{t}^{(m)}\|\_{L^{4+2\delta}}\leq\sum\_{i>m}\|{\mbox{$\Xi$}}\_{i}\|\big({\mathbb{E}}\|{\mbox{$\varepsilon$}}\_{t}\|^{4+2\delta}\big)^{1/(4+2\delta)}\ \xrightarrow{m\to\infty}\ 0. |  |

Standard approximation arguments for strongly mixing sequences (via mm-dependent truncation and comparison inequalities for mixing coefficients) yield: there exist constants C>0C>0 and Î¸âˆˆ(0,1)\theta\in(0,1) (depending only on 4+2â€‹Î´4+2\delta) such that, for â„“>2â€‹m\ell>2m,

|  |  |  |
| --- | --- | --- |
|  | Î±zâ€‹(â„“)â‰¤Î±z(m)â€‹(â„“âˆ’m)+Câ€‹â€–ğ’›Â¯tâˆ’ğ’›Â¯t(m)â€–L4+2â€‹Î´Î¸.\alpha\_{z}(\ell)\ \leq\ \alpha\_{z^{(m)}}(\ell-m)\ +\ C\,\|\bar{{\mbox{$z$}}}\_{t}-\bar{{\mbox{$z$}}}\_{t}^{(m)}\|\_{L^{4+2\delta}}^{\theta}. |  |

Choosing m=âŒŠâ„“/4âŒ‹m=\lfloor\ell/4\rfloor gives

|  |  |  |
| --- | --- | --- |
|  | Î±zâ€‹(â„“)â‰¤Î±Îµâ€‹(â„“/2)+Câ€‹(âˆ‘i>â„“/4âˆâ€–ğšµiâ€–)Î¸.\alpha\_{z}(\ell)\ \leq\ \alpha\_{\varepsilon}(\ell/2)\ +\ C\,\Big(\sum\_{i>\ell/4}^{\infty}\|{\mbox{$\Xi$}}\_{i}\|\Big)^{\theta}. |  |

By partÂ (iii), the stability of ğ‘·â€‹(L){\mbox{$P$}}(L) implies the existence of Câ€²>0C^{\prime}>0 and Ïâˆˆ(0,1)\rho\in(0,1) such that â€–ğšµiâ€–â‰¤Câ€²â€‹Ïi\|{\mbox{$\Xi$}}\_{i}\|\leq C^{\prime}\rho^{i} (geometric decay of the impulse responses). Hence

|  |  |  |
| --- | --- | --- |
|  | âˆ‘â„“=1âˆÎ±zâ€‹(â„“)Î´/(2+Î´)â‰¤Câ€‹âˆ‘â„“=1âˆÎ±Îµâ€‹(â„“/2)Î´/(2+Î´)+Câ€‹âˆ‘â„“=1âˆ(Câ€²â€‹Ïâ„“/4)Î¸â€‹Î´/(2+Î´)<âˆ.\sum\_{\ell=1}^{\infty}\alpha\_{z}(\ell)^{\delta/(2+\delta)}\ \leq\ C\sum\_{\ell=1}^{\infty}\alpha\_{\varepsilon}(\ell/2)^{\delta/(2+\delta)}\ +\ C\sum\_{\ell=1}^{\infty}\big(C^{\prime}\rho^{\ell/4}\big)^{\theta\delta/(2+\delta)}\ <\ \infty. |  |

Finally, (ğ’št,ğ’™t,ğ’–t)({\mbox{$y$}}\_{t},{\mbox{$x$}}\_{t},{\mbox{$u$}}\_{t}) are fixed linear transforms of ğ’›t{\mbox{$z$}}\_{t}, and ğ‘¿t{\mbox{$X$}}\_{t} is a deterministic function of ğ’™t{\mbox{$x$}}\_{t}. Hence,

|  |  |  |
| --- | --- | --- |
|  | Î±(y,X,u)â€‹(â„“)â‰¤Î±zâ€‹(â„“)andâˆ‘â„“=1âˆÎ±(y,X,u)â€‹(â„“)Î´/(2+Î´)<âˆ.\alpha\_{(y,X,u)}(\ell)\ \leq\ \alpha\_{z}(\ell)\qquad\text{and}\qquad\sum\_{\ell=1}^{\infty}\alpha\_{(y,X,u)}(\ell)^{\delta/(2+\delta)}<\infty. |  |

Since strong mixing implies ergodicity, the claim follows. âˆ

### A.2 Proof of Proposition2

(i) strict exogeneity â‡”\Leftrightarrow Bâ€‹DBD

#### strict exogeneity â‡’\Rightarrow Bâ€‹DBD

We first show that (a covariance-based notion of) strict exogeneity implies the Bâ€‹DBD condition. Throughout this subsection, by strict exogeneity we mean

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[ğ’–tâ€‹ğ’™sâ€²]=ğŸfor allÂ â€‹t,sâˆˆâ„¤.{\mathbb{E}}[{\mbox{$u$}}\_{t}{\mbox{$x$}}\_{s}^{\prime}]={\mbox{$0$}}\quad\text{for all }t,s\in\mathbb{Z}. |  |

Let ğ’™Â¯t:=ğ’™tâˆ’ğx\bar{{\mbox{$x$}}}\_{t}:={\mbox{$x$}}\_{t}-{\mbox{$\mu$}}\_{x} and ğ’›Â¯t:=(ğ’™Â¯tâ€²,ğ’–tâ€²)â€²\bar{{\mbox{$z$}}}\_{t}:=(\bar{{\mbox{$x$}}}\_{t}^{\prime},{\mbox{$u$}}\_{t}^{\prime})^{\prime}.

##### Step 1: Orthogonality of the closed linear spans.

Define the closed linear subspaces of L2â€‹(Î©,â„±,â„™;â„m)L^{2}(\Omega,\mathscr{F},\mathbb{P};\mathbb{R}^{m}) by

|  |  |  |
| --- | --- | --- |
|  | â„‹x:=spanÂ¯â€‹{ğ’™Â¯t:tâˆˆâ„¤},â„‹u:=spanÂ¯â€‹{ğ’–t:tâˆˆâ„¤},â„‹z:=spanÂ¯â€‹{ğ’›Â¯t:tâˆˆâ„¤}.\mathscr{H}\_{x}:=\overline{\operatorname{span}}\{\bar{{\mbox{$x$}}}\_{t}:t\in\mathbb{Z}\},\quad\mathscr{H}\_{u}:=\overline{\operatorname{span}}\{{\mbox{$u$}}\_{t}:t\in\mathbb{Z}\},\quad\mathscr{H}\_{z}:=\overline{\operatorname{span}}\{\bar{{\mbox{$z$}}}\_{t}:t\in\mathbb{Z}\}. |  |

For finite index sets S,TâŠ‚â„¤S,T\subset\mathbb{Z}, consider the finite linear combinations

|  |  |  |
| --- | --- | --- |
|  | hx=âˆ‘sâˆˆSğ’‚sâ€²â€‹ğ’™Â¯s,hu=âˆ‘tâˆˆTğ’ƒtâ€²â€‹ğ’–t.h\_{x}=\sum\_{s\in S}{\mbox{$a$}}\_{s}^{\prime}\bar{{\mbox{$x$}}}\_{s},\quad h\_{u}=\sum\_{t\in T}{\mbox{$b$}}\_{t}^{\prime}{\mbox{$u$}}\_{t}. |  |

Then

|  |  |  |
| --- | --- | --- |
|  | âŸ¨hx,huâŸ©:=ğ”¼â€‹[hxâ€‹hu]=âˆ‘sâˆˆSâˆ‘tâˆˆTğ’‚sâ€²â€‹ğ”¼â€‹[ğ’™Â¯sâ€‹ğ’–tâ€²]â€‹ğ’ƒt.\langle h\_{x},h\_{u}\rangle:={\mathbb{E}}[h\_{x}h\_{u}]=\sum\_{s\in S}\sum\_{t\in T}{\mbox{$a$}}\_{s}^{\prime}\,{\mathbb{E}}[\bar{{\mbox{$x$}}}\_{s}{\mbox{$u$}}\_{t}^{\prime}]\,{\mbox{$b$}}\_{t}. |  |

Since ğ”¼â€‹[ğ’™Â¯sâ€‹ğ’–tâ€²]=ğ”¼â€‹[(ğ’™sâˆ’ğx)â€‹ğ’–tâ€²]=ğ”¼â€‹[ğ’™sâ€‹ğ’–tâ€²]âˆ’ğxâ€‹ğ”¼â€‹[ğ’–tâ€²]=ğŸ{\mathbb{E}}[\bar{{\mbox{$x$}}}\_{s}{\mbox{$u$}}\_{t}^{\prime}]={\mathbb{E}}[({\mbox{$x$}}\_{s}-{\mbox{$\mu$}}\_{x}){\mbox{$u$}}\_{t}^{\prime}]={\mathbb{E}}[{\mbox{$x$}}\_{s}{\mbox{$u$}}\_{t}^{\prime}]-{\mbox{$\mu$}}\_{x}{\mathbb{E}}[{\mbox{$u$}}\_{t}^{\prime}]={\mbox{$0$}} under strict exogeneity and ğ”¼â€‹[ğ’–t]=ğŸ{\mathbb{E}}[{\mbox{$u$}}\_{t}]={\mbox{$0$}}, we obtain âŸ¨hx,huâŸ©=0\langle h\_{x},h\_{u}\rangle=0 for all such finite linear combinations. By continuity of the inner product with respect to the L2L^{2}-norm, this orthogonality extends to the closures, so that

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„‹xâŸ‚â„‹u.\mathscr{H}\_{x}\perp\mathscr{H}\_{u}. |  | (38) |

Moreover, since ğ’›Â¯t=(ğ’™Â¯tâ€²,ğ’–tâ€²)â€²\bar{{\mbox{$z$}}}\_{t}=(\bar{{\mbox{$x$}}}\_{t}^{\prime},{\mbox{$u$}}\_{t}^{\prime})^{\prime}, we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„‹z=spanÂ¯â€‹(â„‹xâˆªâ„‹u)=â„‹xâŠ•â„‹u.\mathscr{H}\_{z}=\overline{\operatorname{span}}(\mathscr{H}\_{x}\cup\mathscr{H}\_{u})=\mathscr{H}\_{x}\oplus\mathscr{H}\_{u}. |  | (39) |

##### Step 2: Innovation decomposition and block-diagonal innovation covariance.

For each tt, define the past closed linear spans (consistent with PropositionÂ [1](https://arxiv.org/html/2601.21272v1#Thmproposition1 "Proposition 1: â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) by

|  |  |  |
| --- | --- | --- |
|  | â„‹tâˆ’1:=spanÂ¯â€‹{ğ’›Â¯tâˆ’1,ğ’›Â¯tâˆ’2,â€¦},â„‹x,tâˆ’1:=spanÂ¯â€‹{ğ’™Â¯tâˆ’1,ğ’™Â¯tâˆ’2,â€¦},â„‹u,tâˆ’1:=spanÂ¯â€‹{ğ’–tâˆ’1,ğ’–tâˆ’2,â€¦}.\mathscr{H}\_{t-1}:=\overline{\operatorname{span}}\{\bar{{\mbox{$z$}}}\_{t-1},\bar{{\mbox{$z$}}}\_{t-2},\ldots\},\quad\mathscr{H}\_{x,t-1}:=\overline{\operatorname{span}}\{\bar{{\mbox{$x$}}}\_{t-1},\bar{{\mbox{$x$}}}\_{t-2},\ldots\},\quad\mathscr{H}\_{u,t-1}:=\overline{\operatorname{span}}\{{\mbox{$u$}}\_{t-1},{\mbox{$u$}}\_{t-2},\ldots\}. |  |

Then ([39](https://arxiv.org/html/2601.21272v1#Sx2.E39 "In Step 1: Orthogonality of the closed linear spans. â€£ strict exogeneity â‡’ ğµâ¢ğ· â€£ A.2 Proof of Proposition2 â€£ Appendix â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) implies

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„‹tâˆ’1=â„‹x,tâˆ’1âŠ•â„‹u,tâˆ’1.\mathscr{H}\_{t-1}=\mathscr{H}\_{x,t-1}\oplus\mathscr{H}\_{u,t-1}. |  | (40) |

Let ğ’«tâˆ’1:L2â†’â„‹tâˆ’1\mathscr{P}\_{t-1}:L^{2}\to\mathscr{H}\_{t-1} be the L2L^{2}-orthogonal projection and define the innovation

|  |  |  |
| --- | --- | --- |
|  | ğœºt:=ğ’›Â¯tâˆ’ğ’«tâˆ’1â€‹[ğ’›Â¯t].{\mbox{$\varepsilon$}}\_{t}:=\bar{{\mbox{$z$}}}\_{t}-\mathscr{P}\_{t-1}[\bar{{\mbox{$z$}}}\_{t}]. |  |

Because ([40](https://arxiv.org/html/2601.21272v1#Sx2.E40 "In Step 2: Innovation decomposition and block-diagonal innovation covariance. â€£ strict exogeneity â‡’ ğµâ¢ğ· â€£ A.2 Proof of Proposition2 â€£ Appendix â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) is an orthogonal direct sum, the projection respects this decomposition:

|  |  |  |
| --- | --- | --- |
|  | ğ’«tâˆ’1=ğ’«x,tâˆ’1âŠ•ğ’«u,tâˆ’1,\mathscr{P}\_{t-1}=\mathscr{P}\_{x,t-1}\oplus\mathscr{P}\_{u,t-1}, |  |

where ğ’«x,tâˆ’1:L2â†’â„‹x,tâˆ’1\mathscr{P}\_{x,t-1}:L^{2}\to\mathscr{H}\_{x,t-1} and ğ’«u,tâˆ’1:L2â†’â„‹u,tâˆ’1\mathscr{P}\_{u,t-1}:L^{2}\to\mathscr{H}\_{u,t-1} are the corresponding orthogonal projections. Hence

|  |  |  |
| --- | --- | --- |
|  | ğœºt=[ğ’™Â¯tâˆ’ğ’«x,tâˆ’1â€‹[ğ’™Â¯t]ğ’–tâˆ’ğ’«u,tâˆ’1â€‹[ğ’–t]]=:[ğœºx,tğœºu,t],{\mbox{$\varepsilon$}}\_{t}=\begin{bmatrix}\bar{{\mbox{$x$}}}\_{t}-\mathscr{P}\_{x,t-1}[\bar{{\mbox{$x$}}}\_{t}]\\ {\mbox{$u$}}\_{t}-\mathscr{P}\_{u,t-1}[{\mbox{$u$}}\_{t}]\end{bmatrix}=:\begin{bmatrix}{\mbox{$\varepsilon$}}\_{x,t}\\ {\mbox{$\varepsilon$}}\_{u,t}\end{bmatrix}, |  |

with ğœºx,tâˆˆâ„‹x{\mbox{$\varepsilon$}}\_{x,t}\in\mathscr{H}\_{x} and ğœºu,tâˆˆâ„‹u{\mbox{$\varepsilon$}}\_{u,t}\in\mathscr{H}\_{u}. By ([38](https://arxiv.org/html/2601.21272v1#Sx2.E38 "In Step 1: Orthogonality of the closed linear spans. â€£ strict exogeneity â‡’ ğµâ¢ğ· â€£ A.2 Proof of Proposition2 â€£ Appendix â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")),

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[ğœºx,tâ€‹ğœºu,tâ€²]=ğŸfor allÂ â€‹t,{\mathbb{E}}[{\mbox{$\varepsilon$}}\_{x,t}{\mbox{$\varepsilon$}}\_{u,t}^{\prime}]={\mbox{$0$}}\quad\text{for all }t, |  |

so the innovation covariance matrix is block diagonal:

|  |  |  |
| --- | --- | --- |
|  | ğšº:=ğ”¼â€‹[ğœºtâ€‹ğœºtâ€²]=[ğšºxâ€‹xğŸğŸğšºuâ€‹u].{\mbox{$\Sigma$}}:={\mathbb{E}}[{\mbox{$\varepsilon$}}\_{t}{\mbox{$\varepsilon$}}\_{t}^{\prime}]=\begin{bmatrix}{\mbox{$\Sigma$}}\_{xx}&{\mbox{$0$}}\\ {\mbox{$0$}}&{\mbox{$\Sigma$}}\_{uu}\end{bmatrix}. |  |

##### Step 3: Block-diagonality of the VMA coefficients.

By PropositionÂ [1](https://arxiv.org/html/2601.21272v1#Thmproposition1 "Proposition 1: â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")(i), ğ’›Â¯t\bar{{\mbox{$z$}}}\_{t} admits the unique Wold representation

|  |  |  |
| --- | --- | --- |
|  | ğ’›Â¯t=âˆ‘i=0âˆğšµiâ€‹ğœºtâˆ’iinÂ â€‹L2,ğšµi=[ğšµxâ€‹x,iğšµxâ€‹u,iğšµuâ€‹x,iğšµuâ€‹u,i].\bar{{\mbox{$z$}}}\_{t}=\sum\_{i=0}^{\infty}{\mbox{$\Xi$}}\_{i}{\mbox{$\varepsilon$}}\_{t-i}\quad\text{in }L^{2},\qquad{\mbox{$\Xi$}}\_{i}=\begin{bmatrix}{\mbox{$\Xi$}}\_{xx,i}&{\mbox{$\Xi$}}\_{xu,i}\\ {\mbox{$\Xi$}}\_{ux,i}&{\mbox{$\Xi$}}\_{uu,i}\end{bmatrix}. |  |

Then

|  |  |  |
| --- | --- | --- |
|  | ğ’™Â¯t=âˆ‘i=0âˆ(ğšµxâ€‹x,iâ€‹ğœºx,tâˆ’i+ğšµxâ€‹u,iâ€‹ğœºu,tâˆ’i),ğ’–t=âˆ‘i=0âˆ(ğšµuâ€‹x,iâ€‹ğœºx,tâˆ’i+ğšµuâ€‹u,iâ€‹ğœºu,tâˆ’i).\bar{{\mbox{$x$}}}\_{t}=\sum\_{i=0}^{\infty}\big({\mbox{$\Xi$}}\_{xx,i}{\mbox{$\varepsilon$}}\_{x,t-i}+{\mbox{$\Xi$}}\_{xu,i}{\mbox{$\varepsilon$}}\_{u,t-i}\big),\qquad{\mbox{$u$}}\_{t}=\sum\_{i=0}^{\infty}\big({\mbox{$\Xi$}}\_{ux,i}{\mbox{$\varepsilon$}}\_{x,t-i}+{\mbox{$\Xi$}}\_{uu,i}{\mbox{$\varepsilon$}}\_{u,t-i}\big). |  |

Define

|  |  |  |
| --- | --- | --- |
|  | ğ’—t:=âˆ‘i=0âˆğšµxâ€‹u,iâ€‹ğœºu,tâˆ’i.{\mbox{$v$}}\_{t}:=\sum\_{i=0}^{\infty}{\mbox{$\Xi$}}\_{xu,i}{\mbox{$\varepsilon$}}\_{u,t-i}. |  |

Since each ğœºu,tâˆ’iâˆˆâ„‹u{\mbox{$\varepsilon$}}\_{u,t-i}\in\mathscr{H}\_{u} and â„‹u\mathscr{H}\_{u} is a closed linear subspace, we have ğ’—tâˆˆâ„‹u{\mbox{$v$}}\_{t}\in\mathscr{H}\_{u}. On the other hand, ğ’™Â¯tâˆˆâ„‹x\bar{{\mbox{$x$}}}\_{t}\in\mathscr{H}\_{x} and âˆ‘i=0âˆğšµxâ€‹x,iâ€‹ğœºx,tâˆ’iâˆˆâ„‹x\sum\_{i=0}^{\infty}{\mbox{$\Xi$}}\_{xx,i}{\mbox{$\varepsilon$}}\_{x,t-i}\in\mathscr{H}\_{x}, hence

|  |  |  |
| --- | --- | --- |
|  | ğ’—t=ğ’™Â¯tâˆ’âˆ‘i=0âˆğšµxâ€‹x,iâ€‹ğœºx,tâˆ’iâˆˆâ„‹x.{\mbox{$v$}}\_{t}=\bar{{\mbox{$x$}}}\_{t}-\sum\_{i=0}^{\infty}{\mbox{$\Xi$}}\_{xx,i}{\mbox{$\varepsilon$}}\_{x,t-i}\in\mathscr{H}\_{x}. |  |

Therefore ğ’—tâˆˆâ„‹xâˆ©â„‹u{\mbox{$v$}}\_{t}\in\mathscr{H}\_{x}\cap\mathscr{H}\_{u}. By ([38](https://arxiv.org/html/2601.21272v1#Sx2.E38 "In Step 1: Orthogonality of the closed linear spans. â€£ strict exogeneity â‡’ ğµâ¢ğ· â€£ A.2 Proof of Proposition2 â€£ Appendix â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")), â„‹xâˆ©â„‹u={ğŸ}\mathscr{H}\_{x}\cap\mathscr{H}\_{u}=\{{\mbox{$0$}}\}, so ğ’—t=ğŸ{\mbox{$v$}}\_{t}={\mbox{$0$}} in L2L^{2}, i.e. ğ’—t=ğŸ{\mbox{$v$}}\_{t}={\mbox{$0$}} a.s.

For any hâ‰¥0h\geq 0, using that {ğœºt}\{{\mbox{$\varepsilon$}}\_{t}\} is white noise and ğ”¼â€‹[ğœºu,tâ€‹ğœºu,tâ€²]=ğšºuâ€‹u{\mathbb{E}}[{\mbox{$\varepsilon$}}\_{u,t}{\mbox{$\varepsilon$}}\_{u,t}^{\prime}]={\mbox{$\Sigma$}}\_{uu}, we obtain

|  |  |  |
| --- | --- | --- |
|  | ğŸ=â„‚â€‹ovâ€‹(ğ’—t,ğœºu,tâˆ’h)=âˆ‘i=0âˆğšµxâ€‹u,iâ€‹â„‚â€‹ovâ€‹(ğœºu,tâˆ’i,ğœºu,tâˆ’h)=ğšµxâ€‹u,hâ€‹ğšºuâ€‹u.{\mbox{$0$}}={\mathbb{C}\rm{ov}}({\mbox{$v$}}\_{t},{\mbox{$\varepsilon$}}\_{u,t-h})=\sum\_{i=0}^{\infty}{\mbox{$\Xi$}}\_{xu,i}\,{\mathbb{C}\rm{ov}}({\mbox{$\varepsilon$}}\_{u,t-i},{\mbox{$\varepsilon$}}\_{u,t-h})={\mbox{$\Xi$}}\_{xu,h}{\mbox{$\Sigma$}}\_{uu}. |  |

Since ğšºuâ€‹uâ‰»0{\mbox{$\Sigma$}}\_{uu}\succ 0 (as a principal submatrix of ğšºâ‰»0{\mbox{$\Sigma$}}\succ 0), it follows that ğšµxâ€‹u,h=ğŸ{\mbox{$\Xi$}}\_{xu,h}={\mbox{$0$}} for all hâ‰¥0h\geq 0. By the same argument applied to âˆ‘i=0âˆğšµuâ€‹x,iâ€‹ğœºx,tâˆ’i\sum\_{i=0}^{\infty}{\mbox{$\Xi$}}\_{ux,i}{\mbox{$\varepsilon$}}\_{x,t-i}, we also have ğšµuâ€‹x,i=ğŸ{\mbox{$\Xi$}}\_{ux,i}={\mbox{$0$}} for all iâ‰¥0i\geq 0.
Hence each ğšµi{\mbox{$\Xi$}}\_{i} is block diagonal.

##### Step 4: Block-diagonality of the VAR coefficients.

By PropositionÂ [1](https://arxiv.org/html/2601.21272v1#Thmproposition1 "Proposition 1: â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")(ii)â€“(iii), ğ’›Â¯t\bar{{\mbox{$z$}}}\_{t} admits a stable VAR(p0p\_{0}) representation and

|  |  |  |
| --- | --- | --- |
|  | ğš¿â€‹(L)â€‹ğšµâ€‹(L)=ğ‘°m,ğš¿â€‹(L):=ğ‘°mâˆ’âˆ‘j=1p0ğš¿jâ€‹Lj,ğšµâ€‹(L):=ğ‘°m+âˆ‘i=1âˆğšµiâ€‹Li.{\mbox{$\Psi$}}(L){\mbox{$\Xi$}}(L)={\mbox{$I$}}\_{m},\qquad{\mbox{$\Psi$}}(L):={\mbox{$I$}}\_{m}-\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{j}L^{j},\quad{\mbox{$\Xi$}}(L):={\mbox{$I$}}\_{m}+\sum\_{i=1}^{\infty}{\mbox{$\Xi$}}\_{i}L^{i}. |  |

Since each ğšµi{\mbox{$\Xi$}}\_{i} is block diagonal, ğšµâ€‹(L){\mbox{$\Xi$}}(L) is block diagonal, and so is its inverse ğš¿â€‹(L)=ğšµâ€‹(L)âˆ’1{\mbox{$\Psi$}}(L)={\mbox{$\Xi$}}(L)^{-1}. Therefore each VAR coefficient ğš¿j{\mbox{$\Psi$}}\_{j} is block diagonal. Together with the block-diagonal innovation covariance matrix ğšº\Sigma, we conclude that strict exogeneity implies the Bâ€‹DBD condition. âˆ

#### Bâ€‹DBD â‡’\Rightarrow strict exogeneity

We next show that the Bâ€‹DBD condition implies strict exogeneity in the covariance sense, that is,

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[ğ’–tâ€‹ğ’™sâ€²]=ğŸfor allÂ â€‹s,tâˆˆâ„¤.{\mathbb{E}}[{\mbox{$u$}}\_{t}{\mbox{$x$}}\_{s}^{\prime}]={\mbox{$0$}}\quad\text{for all }s,t\in\mathbb{Z}. |  |

Suppose that the Bâ€‹DBD condition holds. Then, by AssumptionÂ [1](https://arxiv.org/html/2601.21272v1#Thmassumption1 "Assumption 1 (Finite-predictor exactness at lag ğ‘â‚€): â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") and PropositionÂ [1](https://arxiv.org/html/2601.21272v1#Thmproposition1 "Proposition 1: â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"), the marginal processes {ğ’™Â¯t}\{\bar{{\mbox{$x$}}}\_{t}\} and {ğ’–t}\{{\mbox{$u$}}\_{t}\} are covariance-stationary and purely nondeterministic, and (since the VAR coefficients and innovation covariance matrix are block diagonal) each admits its own Wold VMA(âˆ\infty) representation driven by the corresponding innovation block. In particular, there exist coefficient matrices {ğšµxâ€‹x,i}iâ‰¥0\{{\mbox{$\Xi$}}\_{xx,i}\}\_{i\geq 0} and {ğšµuâ€‹u,i}iâ‰¥0\{{\mbox{$\Xi$}}\_{uu,i}\}\_{i\geq 0} such that

|  |  |  |
| --- | --- | --- |
|  | ğ’™Â¯t=âˆ‘i=0âˆğšµxâ€‹x,iâ€‹ğœºx,tâˆ’i,ğ’–t=âˆ‘i=0âˆğšµuâ€‹u,iâ€‹ğœºu,tâˆ’i,\bar{{\mbox{$x$}}}\_{t}=\sum\_{i=0}^{\infty}{\mbox{$\Xi$}}\_{xx,i}\,{\mbox{$\varepsilon$}}\_{x,t-i},\qquad{\mbox{$u$}}\_{t}=\sum\_{i=0}^{\infty}{\mbox{$\Xi$}}\_{uu,i}\,{\mbox{$\varepsilon$}}\_{u,t-i}, |  |

where âˆ‘i=0âˆâ€–ğšµxâ€‹x,iâ€–<âˆ\sum\_{i=0}^{\infty}\|{\mbox{$\Xi$}}\_{xx,i}\|<\infty, âˆ‘i=0âˆâ€–ğšµuâ€‹u,iâ€–<âˆ\sum\_{i=0}^{\infty}\|{\mbox{$\Xi$}}\_{uu,i}\|<\infty, and
ğšµxâ€‹x,0=ğ‘°k{\mbox{$\Xi$}}\_{xx,0}={\mbox{$I$}}\_{k}, ğšµuâ€‹u,0=ğ‘°N{\mbox{$\Xi$}}\_{uu,0}={\mbox{$I$}}\_{N}.

The innovation process {ğœºt}\{{\mbox{$\varepsilon$}}\_{t}\} is (second-order) white noise, so

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[ğœºtâ€‹ğœºsâ€²]=ğŸfor allÂ â€‹sâ‰ t.{\mathbb{E}}[{\mbox{$\varepsilon$}}\_{t}{\mbox{$\varepsilon$}}\_{s}^{\prime}]={\mbox{$0$}}\quad\text{for all }s\neq t. |  |

Writing

|  |  |  |
| --- | --- | --- |
|  | ğœºt=[ğœºx,tğœºu,t],{\mbox{$\varepsilon$}}\_{t}=\begin{bmatrix}{\mbox{$\varepsilon$}}\_{x,t}\\ {\mbox{$\varepsilon$}}\_{u,t}\end{bmatrix}, |  |

the Bâ€‹DBD condition further implies that the innovation covariance matrix is block diagonal,

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[ğœºtâ€‹ğœºtâ€²]=[ğšºxâ€‹xğŸğŸğšºuâ€‹u],{\mathbb{E}}[{\mbox{$\varepsilon$}}\_{t}{\mbox{$\varepsilon$}}\_{t}^{\prime}]=\begin{bmatrix}{\mbox{$\Sigma$}}\_{xx}&{\mbox{$0$}}\\ {\mbox{$0$}}&{\mbox{$\Sigma$}}\_{uu}\end{bmatrix}, |  |

so that

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[ğœºu,tâ€‹ğœºx,tâ€²]=ğŸ.{\mathbb{E}}[{\mbox{$\varepsilon$}}\_{u,t}{\mbox{$\varepsilon$}}\_{x,t}^{\prime}]={\mbox{$0$}}. |  |

Combining whiteness across time and block diagonality at each tt, we obtain

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â€‹[ğœºu,râ€‹ğœºx,qâ€²]=ğŸfor allÂ â€‹r,qâˆˆâ„¤.{\mathbb{E}}[{\mbox{$\varepsilon$}}\_{u,r}{\mbox{$\varepsilon$}}\_{x,q}^{\prime}]={\mbox{$0$}}\quad\text{for all }r,q\in\mathbb{Z}. |  | (41) |

Now fix arbitrary t,sâˆˆâ„¤t,s\in\mathbb{Z}. Using the VMA representations and absolute summability of the coefficient sequences, we can interchange sums and expectations to write

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â€‹[ğ’–tâ€‹ğ’™sâ€²]\displaystyle{\mathbb{E}}[{\mbox{$u$}}\_{t}{\mbox{$x$}}\_{s}^{\prime}] | =ğ”¼â€‹[(âˆ‘i=0âˆğšµuâ€‹u,iâ€‹ğœºu,tâˆ’i)â€‹(âˆ‘j=0âˆğšµxâ€‹x,jâ€‹ğœºx,sâˆ’j)â€²]\displaystyle={\mathbb{E}}\Bigg[\Big(\sum\_{i=0}^{\infty}{\mbox{$\Xi$}}\_{uu,i}\,{\mbox{$\varepsilon$}}\_{u,t-i}\Big)\Big(\sum\_{j=0}^{\infty}{\mbox{$\Xi$}}\_{xx,j}\,{\mbox{$\varepsilon$}}\_{x,s-j}\Big)^{\prime}\Bigg] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =âˆ‘i=0âˆâˆ‘j=0âˆğšµuâ€‹u,iâ€‹ğ”¼â€‹[ğœºu,tâˆ’iâ€‹ğœºx,sâˆ’jâ€²]â€‹ğšµxâ€‹x,jâ€².\displaystyle=\sum\_{i=0}^{\infty}\sum\_{j=0}^{\infty}{\mbox{$\Xi$}}\_{uu,i}\,{\mathbb{E}}\big[{\mbox{$\varepsilon$}}\_{u,t-i}{\mbox{$\varepsilon$}}\_{x,s-j}^{\prime}\big]\,{\mbox{$\Xi$}}\_{xx,j}^{\prime}. |  |

By ([41](https://arxiv.org/html/2601.21272v1#Sx2.E41 "In ğµâ¢ğ· â‡’ strict exogeneity â€£ A.2 Proof of Proposition2 â€£ Appendix â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")), each inner expectation vanishes:

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[ğœºu,tâˆ’iâ€‹ğœºx,sâˆ’jâ€²]=ğŸfor allÂ â€‹i,j,{\mathbb{E}}\big[{\mbox{$\varepsilon$}}\_{u,t-i}{\mbox{$\varepsilon$}}\_{x,s-j}^{\prime}\big]={\mbox{$0$}}\quad\text{for all }i,j, |  |

so every term in the double series is zero and hence

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[ğ’–tâ€‹ğ’™sâ€²]=ğŸfor allÂ â€‹s,tâˆˆâ„¤.{\mathbb{E}}[{\mbox{$u$}}\_{t}{\mbox{$x$}}\_{s}^{\prime}]={\mbox{$0$}}\quad\text{for all }s,t\in\mathbb{Z}. |  |

Recalling that ğ”¼â€‹[ğ’–t]=ğŸ{\mathbb{E}}[{\mbox{$u$}}\_{t}]={\mbox{$0$}} and ğ”¼â€‹[ğ’™s]=ğx{\mathbb{E}}[{\mbox{$x$}}\_{s}]={\mbox{$\mu$}}\_{x}, this is equivalent to

|  |  |  |
| --- | --- | --- |
|  | â„‚â€‹ovâ€‹(ğ’–t,ğ’™s)=ğ”¼â€‹[(ğ’–tâˆ’ğŸ)â€‹(ğ’™sâˆ’ğx)â€²]=ğ”¼â€‹[ğ’–tâ€‹ğ’™sâ€²]=ğŸfor allÂ â€‹s,t,{\mathbb{C}\rm{ov}}({\mbox{$u$}}\_{t},{\mbox{$x$}}\_{s})={\mathbb{E}}[({\mbox{$u$}}\_{t}-{\mbox{$0$}})({\mbox{$x$}}\_{s}-{\mbox{$\mu$}}\_{x})^{\prime}]={\mathbb{E}}[{\mbox{$u$}}\_{t}{\mbox{$x$}}\_{s}^{\prime}]={\mbox{$0$}}\quad\text{for all }s,t, |  |

which is precisely strict exogeneity in the covariance sense. Therefore, the Bâ€‹DBD condition implies strict exogeneity. Combining result of strict exogeneity â‡’\Rightarrow Bâ€‹DBD, the strict exogeneity and Bâ€‹DBD condition are identical. âˆ

(ii) pre-determined = Eâ€‹Bâ€‹DEBD

Recall from PropositionÂ [1](https://arxiv.org/html/2601.21272v1#Thmproposition1 "Proposition 1: â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")(i) that, under AssumptionÂ [1](https://arxiv.org/html/2601.21272v1#Thmassumption1 "Assumption 1 (Finite-predictor exactness at lag ğ‘â‚€): â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"), the joint process {ğ’›t}\{{\mbox{$z$}}\_{t}\} with ğ’›t=(ğ’™tâ€²,ğ’–tâ€²)â€²{\mbox{$z$}}\_{t}=({\mbox{$x$}}\_{t}^{\prime},{\mbox{$u$}}\_{t}^{\prime})^{\prime} admits the Wold representation

|  |  |  |
| --- | --- | --- |
|  | ğ’›Â¯t=âˆ‘i=0âˆğšµiâ€‹ğœºtâˆ’i,ğšµ0=ğ‘°m,\bar{{\mbox{$z$}}}\_{t}=\sum\_{i=0}^{\infty}{\mbox{$\Xi$}}\_{i}{\mbox{$\varepsilon$}}\_{t-i},\qquad{\mbox{$\Xi$}}\_{0}={\mbox{$I$}}\_{m}, |  |

where ğ’›Â¯t:=ğ’›tâˆ’ğ”¼â€‹[ğ’›t]\bar{{\mbox{$z$}}}\_{t}:={\mbox{$z$}}\_{t}-{\mathbb{E}}[{\mbox{$z$}}\_{t}], {ğœºt}\{{\mbox{$\varepsilon$}}\_{t}\} is a white-noise innovation sequence with ğ”¼â€‹[ğœºt]=ğŸ{\mathbb{E}}[{\mbox{$\varepsilon$}}\_{t}]={\mbox{$0$}}, ğ”¼â€‹[ğœºtâ€‹ğœºsâ€²]=ğŸ{\mathbb{E}}[{\mbox{$\varepsilon$}}\_{t}{\mbox{$\varepsilon$}}\_{s}^{\prime}]={\mbox{$0$}} for tâ‰ st\neq s, and ğšº:=ğ”¼â€‹[ğœºtâ€‹ğœºtâ€²]>0{\mbox{$\Sigma$}}:={\mathbb{E}}[{\mbox{$\varepsilon$}}\_{t}{\mbox{$\varepsilon$}}\_{t}^{\prime}]>0. Partition

|  |  |  |
| --- | --- | --- |
|  | ğœºt=[ğœºx,tğœºu,t],ğšº=[ğšºxâ€‹xğšºxâ€‹uğšºuâ€‹xğšºuâ€‹u],ğšµi=[ğšµxâ€‹x,iğšµxâ€‹u,iğšµuâ€‹x,iğšµuâ€‹u,i].{\mbox{$\varepsilon$}}\_{t}=\begin{bmatrix}{\mbox{$\varepsilon$}}\_{x,t}\\ {\mbox{$\varepsilon$}}\_{u,t}\end{bmatrix},\qquad{\mbox{$\Sigma$}}=\begin{bmatrix}{\mbox{$\Sigma$}}\_{xx}&{\mbox{$\Sigma$}}\_{xu}\\ {\mbox{$\Sigma$}}\_{ux}&{\mbox{$\Sigma$}}\_{uu}\end{bmatrix},\qquad{\mbox{$\Xi$}}\_{i}=\begin{bmatrix}{\mbox{$\Xi$}}\_{xx,i}&{\mbox{$\Xi$}}\_{xu,i}\\ {\mbox{$\Xi$}}\_{ux,i}&{\mbox{$\Xi$}}\_{uu,i}\end{bmatrix}. |  |

Then the first block of the Wold representation can be written as

|  |  |  |
| --- | --- | --- |
|  | ğ’™Â¯t=âˆ‘i=0âˆ(ğšµxâ€‹x,iâ€‹ğœºx,tâˆ’i+ğšµxâ€‹u,iâ€‹ğœºu,tâˆ’i),\bar{{\mbox{$x$}}}\_{t}=\sum\_{i=0}^{\infty}\bigl({\mbox{$\Xi$}}\_{xx,i}{\mbox{$\varepsilon$}}\_{x,t-i}+{\mbox{$\Xi$}}\_{xu,i}{\mbox{$\varepsilon$}}\_{u,t-i}\bigr), |  |

where ğ’™Â¯t:=ğ’™tâˆ’ğx\bar{{\mbox{$x$}}}\_{t}:={\mbox{$x$}}\_{t}-{\mbox{$\mu$}}\_{x} and ğx:=ğ”¼â€‹[ğ’™t]{\mbox{$\mu$}}\_{x}:={\mathbb{E}}[{\mbox{$x$}}\_{t}].

We say that the regressors are pre-determined if

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â€‹[ğ’™tâ€‹ğœºu,t+hâ€²]=ğŸfor allÂ â€‹tâˆˆâ„¤,hâ‰¥0,{\mathbb{E}}[{\mbox{$x$}}\_{t}{\mbox{$\varepsilon$}}\_{u,t+h}^{\prime}]={\mbox{$0$}}\quad\text{for all }t\in\mathbb{Z},\ h\geq 0, |  | (42) |

which is equivalent to ([9](https://arxiv.org/html/2601.21272v1#S2.E9 "In 2.2 Relationships among exogeneity conditions â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")). We say that the Eâ€‹Bâ€‹DEBD condition holds if and only if ğšºxâ€‹u=ğŸ{\mbox{$\Sigma$}}\_{xu}={\mbox{$0$}} (that is, the innovation covariance matrix is block diagonal).

We now prove that, under AssumptionÂ [1](https://arxiv.org/html/2601.21272v1#Thmassumption1 "Assumption 1 (Finite-predictor exactness at lag ğ‘â‚€): â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"), ([42](https://arxiv.org/html/2601.21272v1#Sx2.E42 "In ğµâ¢ğ· â‡’ strict exogeneity â€£ A.2 Proof of Proposition2 â€£ Appendix â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) holds if and only if ğšºxâ€‹u=ğŸ{\mbox{$\Sigma$}}\_{xu}={\mbox{$0$}}.

#### Eâ€‹Bâ€‹Dâ‡’EBD\Rightarrow pre-determined.

Suppose that Eâ€‹Bâ€‹DEBD holds, i.e. ğšºxâ€‹u=ğŸ{\mbox{$\Sigma$}}\_{xu}={\mbox{$0$}}. Since ğ’™t=ğx+ğ’™Â¯t{\mbox{$x$}}\_{t}={\mbox{$\mu$}}\_{x}+\bar{{\mbox{$x$}}}\_{t} and ğ”¼â€‹[ğœºu,t+h]=ğŸ{\mathbb{E}}[{\mbox{$\varepsilon$}}\_{u,t+h}]={\mbox{$0$}}, we have ğ”¼â€‹[ğ’™tâ€‹ğœºu,t+hâ€²]=ğ”¼â€‹[ğ’™Â¯tâ€‹ğœºu,t+hâ€²]{\mathbb{E}}[{\mbox{$x$}}\_{t}{\mbox{$\varepsilon$}}\_{u,t+h}^{\prime}]={\mathbb{E}}[\bar{{\mbox{$x$}}}\_{t}{\mbox{$\varepsilon$}}\_{u,t+h}^{\prime}]. Using the Wold expansion for ğ’™Â¯t\bar{{\mbox{$x$}}}\_{t} and the absolute summability of {ğšµi}\{{\mbox{$\Xi$}}\_{i}\}, we can interchange expectation and summation to obtain,
for any hâ‰¥0h\geq 0,

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â€‹[ğ’™tâ€‹ğœºu,t+hâ€²]\displaystyle{\mathbb{E}}[{\mbox{$x$}}\_{t}{\mbox{$\varepsilon$}}\_{u,t+h}^{\prime}] | =ğ”¼â€‹[ğ’™Â¯tâ€‹ğœºu,t+hâ€²]\displaystyle={\mathbb{E}}[\bar{{\mbox{$x$}}}\_{t}{\mbox{$\varepsilon$}}\_{u,t+h}^{\prime}] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =âˆ‘i=0âˆğšµxâ€‹x,iâ€‹ğ”¼â€‹[ğœºx,tâˆ’iâ€‹ğœºu,t+hâ€²]+âˆ‘i=0âˆğšµxâ€‹u,iâ€‹ğ”¼â€‹[ğœºu,tâˆ’iâ€‹ğœºu,t+hâ€²].\displaystyle=\sum\_{i=0}^{\infty}{\mbox{$\Xi$}}\_{xx,i}\,{\mathbb{E}}[{\mbox{$\varepsilon$}}\_{x,t-i}{\mbox{$\varepsilon$}}\_{u,t+h}^{\prime}]+\sum\_{i=0}^{\infty}{\mbox{$\Xi$}}\_{xu,i}\,{\mathbb{E}}[{\mbox{$\varepsilon$}}\_{u,t-i}{\mbox{$\varepsilon$}}\_{u,t+h}^{\prime}]. |  |

Because {ğœºt}\{{\mbox{$\varepsilon$}}\_{t}\} is white noise, we have

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[ğœºx,tâˆ’iâ€‹ğœºu,t+hâ€²]={ğšºxâ€‹u,ifÂ â€‹tâˆ’i=t+h,ğŸ,otherwise,{\mathbb{E}}[{\mbox{$\varepsilon$}}\_{x,t-i}{\mbox{$\varepsilon$}}\_{u,t+h}^{\prime}]=\begin{cases}{\mbox{$\Sigma$}}\_{xu},&\text{if }t-i=t+h,\\ {\mbox{$0$}},&\text{otherwise},\end{cases} |  |

and

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[ğœºu,tâˆ’iâ€‹ğœºu,t+hâ€²]={ğšºuâ€‹u,ifÂ â€‹tâˆ’i=t+h,ğŸ,otherwise.{\mathbb{E}}[{\mbox{$\varepsilon$}}\_{u,t-i}{\mbox{$\varepsilon$}}\_{u,t+h}^{\prime}]=\begin{cases}{\mbox{$\Sigma$}}\_{uu},&\text{if }t-i=t+h,\\ {\mbox{$0$}},&\text{otherwise}.\end{cases} |  |

For h>0h>0, the equalities tâˆ’i=t+ht-i=t+h cannot hold for any iâ‰¥0i\geq 0, so both matrices are zero for all ii, and hence ğ”¼â€‹[ğ’™tâ€‹ğœºu,t+hâ€²]=ğŸ{\mathbb{E}}[{\mbox{$x$}}\_{t}{\mbox{$\varepsilon$}}\_{u,t+h}^{\prime}]={\mbox{$0$}}.

For h=0h=0, the condition tâˆ’i=tt-i=t implies i=0i=0, so only the i=0i=0 terms contribute:

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â€‹[ğ’™tâ€‹ğœºu,tâ€²]\displaystyle{\mathbb{E}}[{\mbox{$x$}}\_{t}{\mbox{$\varepsilon$}}\_{u,t}^{\prime}] | =ğšµxâ€‹x,0â€‹ğšºxâ€‹u+ğšµxâ€‹u,0â€‹ğšºuâ€‹u.\displaystyle={\mbox{$\Xi$}}\_{xx,0}\,{\mbox{$\Sigma$}}\_{xu}+{\mbox{$\Xi$}}\_{xu,0}\,{\mbox{$\Sigma$}}\_{uu}. |  |

From ğšµ0=ğ‘°m{\mbox{$\Xi$}}\_{0}={\mbox{$I$}}\_{m}, we have

|  |  |  |
| --- | --- | --- |
|  | ğšµ0=[ğšµxâ€‹x,0ğšµxâ€‹u,0ğšµuâ€‹x,0ğšµuâ€‹u,0]=[ğ‘°kğŸğŸğ‘°N],{\mbox{$\Xi$}}\_{0}=\begin{bmatrix}{\mbox{$\Xi$}}\_{xx,0}&{\mbox{$\Xi$}}\_{xu,0}\\ {\mbox{$\Xi$}}\_{ux,0}&{\mbox{$\Xi$}}\_{uu,0}\end{bmatrix}=\begin{bmatrix}{\mbox{$I$}}\_{k}&{\mbox{$0$}}\\ {\mbox{$0$}}&{\mbox{$I$}}\_{N}\end{bmatrix}, |  |

so that ğšµxâ€‹x,0=ğ‘°k{\mbox{$\Xi$}}\_{xx,0}={\mbox{$I$}}\_{k} and ğšµxâ€‹u,0=ğŸ{\mbox{$\Xi$}}\_{xu,0}={\mbox{$0$}}. Therefore

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[ğ’™tâ€‹ğœºu,tâ€²]=ğ‘°kâ€‹ğšºxâ€‹u+ğŸâ‹…ğšºuâ€‹u=ğšºxâ€‹u=ğŸ,{\mathbb{E}}[{\mbox{$x$}}\_{t}{\mbox{$\varepsilon$}}\_{u,t}^{\prime}]={\mbox{$I$}}\_{k}\,{\mbox{$\Sigma$}}\_{xu}+{\mbox{$0$}}\cdot{\mbox{$\Sigma$}}\_{uu}={\mbox{$\Sigma$}}\_{xu}={\mbox{$0$}}, |  |

where we used the Eâ€‹Bâ€‹DEBD restriction ğšºxâ€‹u=ğŸ{\mbox{$\Sigma$}}\_{xu}={\mbox{$0$}}. Combining the cases h=0h=0 and h>0h>0 yields

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[ğ’™tâ€‹ğœºu,t+hâ€²]=ğŸfor allÂ â€‹hâ‰¥0,{\mathbb{E}}[{\mbox{$x$}}\_{t}{\mbox{$\varepsilon$}}\_{u,t+h}^{\prime}]={\mbox{$0$}}\quad\text{for all }h\geq 0, |  |

so the pre-determined condition ([42](https://arxiv.org/html/2601.21272v1#Sx2.E42 "In ğµâ¢ğ· â‡’ strict exogeneity â€£ A.2 Proof of Proposition2 â€£ Appendix â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) holds.

#### pre-determined â‡’Eâ€‹Bâ€‹D\Rightarrow EBD.

Conversely, suppose that the pre-determined condition ([42](https://arxiv.org/html/2601.21272v1#Sx2.E42 "In ğµâ¢ğ· â‡’ strict exogeneity â€£ A.2 Proof of Proposition2 â€£ Appendix â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) holds. Taking h=0h=0 and using the same argument as above, we obtain

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â€‹[ğ’™tâ€‹ğœºu,tâ€²]\displaystyle{\mathbb{E}}[{\mbox{$x$}}\_{t}{\mbox{$\varepsilon$}}\_{u,t}^{\prime}] | =âˆ‘i=0âˆğšµxâ€‹x,iâ€‹ğ”¼â€‹[ğœºx,tâˆ’iâ€‹ğœºu,tâ€²]+âˆ‘i=0âˆğšµxâ€‹u,iâ€‹ğ”¼â€‹[ğœºu,tâˆ’iâ€‹ğœºu,tâ€²]\displaystyle=\sum\_{i=0}^{\infty}{\mbox{$\Xi$}}\_{xx,i}\,{\mathbb{E}}[{\mbox{$\varepsilon$}}\_{x,t-i}{\mbox{$\varepsilon$}}\_{u,t}^{\prime}]+\sum\_{i=0}^{\infty}{\mbox{$\Xi$}}\_{xu,i}\,{\mathbb{E}}[{\mbox{$\varepsilon$}}\_{u,t-i}{\mbox{$\varepsilon$}}\_{u,t}^{\prime}] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =ğšµxâ€‹x,0â€‹ğšºxâ€‹u+ğšµxâ€‹u,0â€‹ğšºuâ€‹u,\displaystyle={\mbox{$\Xi$}}\_{xx,0}\,{\mbox{$\Sigma$}}\_{xu}+{\mbox{$\Xi$}}\_{xu,0}\,{\mbox{$\Sigma$}}\_{uu}, |  |

where again only the i=0i=0 terms survive by whiteness of {ğœºt}\{{\mbox{$\varepsilon$}}\_{t}\}. Using ğšµxâ€‹x,0=ğ‘°k{\mbox{$\Xi$}}\_{xx,0}={\mbox{$I$}}\_{k} and ğšµxâ€‹u,0=ğŸ{\mbox{$\Xi$}}\_{xu,0}={\mbox{$0$}} as before, we obtain

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[ğ’™tâ€‹ğœºu,tâ€²]=ğ‘°kâ€‹ğšºxâ€‹u=ğšºxâ€‹u.{\mathbb{E}}[{\mbox{$x$}}\_{t}{\mbox{$\varepsilon$}}\_{u,t}^{\prime}]={\mbox{$I$}}\_{k}\,{\mbox{$\Sigma$}}\_{xu}={\mbox{$\Sigma$}}\_{xu}. |  |

By pre-determinedness, the left-hand side is zero, so ğšºxâ€‹u=ğŸ{\mbox{$\Sigma$}}\_{xu}={\mbox{$0$}}.

Thus, under AssumptionÂ [1](https://arxiv.org/html/2601.21272v1#Thmassumption1 "Assumption 1 (Finite-predictor exactness at lag ğ‘â‚€): â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"), the innovation-based pre-determined condition ([42](https://arxiv.org/html/2601.21272v1#Sx2.E42 "In ğµâ¢ğ· â‡’ strict exogeneity â€£ A.2 Proof of Proposition2 â€£ Appendix â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) holds if and only if the innovation covariance matrix is block diagonal, that is, if and only if ğšºxâ€‹u=ğŸ{\mbox{$\Sigma$}}\_{xu}={\mbox{$0$}}. Equivalently, the pre-determined and Eâ€‹Bâ€‹DEBD conditions characterize the same class of data-generating processes. âˆ

(iii) Bâ€‹DBD âŠŠ\subsetneq (present-and-past exogeneity âˆ©\cap ğšºxâ€‹u=ğŸ{\mbox{$\Sigma$}}\_{xu}={\mbox{$0$}}) âŠŠ\subsetneq Eâ€‹Bâ€‹DEBD

We again work under AssumptionÂ [1](https://arxiv.org/html/2601.21272v1#Thmassumption1 "Assumption 1 (Finite-predictor exactness at lag ğ‘â‚€): â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"), so that PropositionÂ [1](https://arxiv.org/html/2601.21272v1#Thmproposition1 "Proposition 1: â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") applies. Let Bâ€‹DBD, Eâ€‹Bâ€‹DEBD, and â€œpresent-and-past exogeneityâ€ denote, respectively, the sets of data-generating processes (DGPs) that satisfy the Bâ€‹DBD, Eâ€‹Bâ€‹DEBD, and covariance-based
present-and-past exogeneity conditions:

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[ğ’–tâ€‹ğ’™sâ€²]=ğŸfor allÂ â€‹sâ‰¤t.{\mathbb{E}}[{\mbox{$u$}}\_{t}{\mbox{$x$}}\_{s}^{\prime}]={\mbox{$0$}}\quad\text{for all }s\leq t. |  |

Bâ€‹DBD âŠ†\subseteq {present-and-past exogeneity}\bigl\{\text{present-and-past exogeneity}\bigr\} âˆ©\cap {ğšºxâ€‹u=ğŸ}\bigl\{{\mbox{$\Sigma$}}\_{xu}={\mbox{$0$}}\bigr\}

Under AssumptionÂ [1](https://arxiv.org/html/2601.21272v1#Thmassumption1 "Assumption 1 (Finite-predictor exactness at lag ğ‘â‚€): â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"), we have already shown that strict exogeneity in the covariance sense,

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[ğ’–tâ€‹ğ’™sâ€²]=ğŸfor allÂ â€‹s,tâˆˆâ„¤,{\mathbb{E}}[{\mbox{$u$}}\_{t}{\mbox{$x$}}\_{s}^{\prime}]={\mbox{$0$}}\quad\text{for all }s,t\in\mathbb{Z}, |  |

is equivalent to the Bâ€‹DBD condition (and to pre-determined plus exogenous). Hence Bâ€‹DBD satisfies

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[ğ’–tâ€‹ğ’™sâ€²]=ğŸfor allÂ â€‹sâ‰¤t,{\mathbb{E}}[{\mbox{$u$}}\_{t}{\mbox{$x$}}\_{s}^{\prime}]={\mbox{$0$}}\quad\text{for all }s\leq t, |  |

so it belongs to the class of present-and-past exogeneity. Moreover, by definition of Bâ€‹DBD, the innovation covariance is block diagonal,

|  |  |  |
| --- | --- | --- |
|  | ğšº=[ğšºxâ€‹xğŸğŸğšºuâ€‹u],{\mbox{$\Sigma$}}=\begin{bmatrix}{\mbox{$\Sigma$}}\_{xx}&{\mbox{$0$}}\\ {\mbox{$0$}}&{\mbox{$\Sigma$}}\_{uu}\end{bmatrix}, |  |

so ğšºxâ€‹u=ğŸ{\mbox{$\Sigma$}}\_{xu}={\mbox{$0$}} holds as well. Thus Bâ€‹DBD âŠ†\subseteq {present-and-past exogeneity}\bigl\{\text{present-and-past exogeneity}\bigr\} âˆ©\cap {ğšºxâ€‹u=ğŸ}\bigl\{{\mbox{$\Sigma$}}\_{xu}={\mbox{$0$}}\bigr\}.

To see that the inclusion is strict, we construct a scalar (k=N=1k=N=1) example that belongs to
present-and-past exogeneity with Î£xâ€‹u=0\Sigma\_{xu}=0 but fails to satisfy Bâ€‹DBD (equivalently, fails strict exogeneity).

Let {Î·t}\{\eta\_{t}\} and {Îµt}\{\varepsilon\_{t}\} be mutually independent i.i.d. sequences with ğ”¼â€‹[Î·t]=ğ”¼â€‹[Îµt]=0{\mathbb{E}}[\eta\_{t}]={\mathbb{E}}[\varepsilon\_{t}]=0 and strictly positive variances ğ•â€‹arâ€‹(Î·t)=ÏƒÎ·2>0{\mathbb{V}\rm{ar}}(\eta\_{t})=\sigma\_{\eta}^{2}>0, ğ•â€‹arâ€‹(Îµt)=ÏƒÎµ2>0{\mathbb{V}\rm{ar}}(\varepsilon\_{t})=\sigma\_{\varepsilon}^{2}>0. Consider the VAR(1) system

|  |  |  |  |
| --- | --- | --- | --- |
|  | {xt=Î³â€‹utâˆ’1+Î·t,ut=Îµt,\begin{cases}x\_{t}=\gamma u\_{t-1}+\eta\_{t},\\ u\_{t}=\varepsilon\_{t},\end{cases} |  | (43) |

with some Î³â‰ 0\gamma\neq 0. Define ğ’›t=(xt,ut)â€²{\mbox{$z$}}\_{t}=(x\_{t},u\_{t})^{\prime} and ğœºt=(Î·t,Îµt)â€²{\mbox{$\varepsilon$}}\_{t}=(\eta\_{t},\varepsilon\_{t})^{\prime}. Then ([43](https://arxiv.org/html/2601.21272v1#Sx2.E43 "In pre-determined â‡’ğ¸â¢ğµâ¢ğ·. â€£ A.2 Proof of Proposition2 â€£ Appendix â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) can be written as

|  |  |  |
| --- | --- | --- |
|  | ğ’›Â¯t=[0Î³00]âŸ:=ğš¿1â€‹ğ’›Â¯tâˆ’1+ğœºt,\bar{{\mbox{$z$}}}\_{t}=\underbrace{\begin{bmatrix}0&\gamma\\ 0&0\end{bmatrix}}\_{:=\,{\mbox{$\Psi$}}\_{1}}\bar{{\mbox{$z$}}}\_{t-1}+{\mbox{$\varepsilon$}}\_{t}, |  |

with innovation covariance

|  |  |  |
| --- | --- | --- |
|  | ğšº:=ğ”¼â€‹[ğœºtâ€‹ğœºtâ€²]=[ÏƒÎ·200ÏƒÎµ2].{\mbox{$\Sigma$}}:={\mathbb{E}}[{\mbox{$\varepsilon$}}\_{t}{\mbox{$\varepsilon$}}\_{t}^{\prime}]=\begin{bmatrix}\sigma\_{\eta}^{2}&0\\ 0&\sigma\_{\varepsilon}^{2}\end{bmatrix}. |  |

Thus ğšºxâ€‹u=0{\mbox{$\Sigma$}}\_{xu}=0, so this DGP satisfies the innovation block-diagonality required for Bâ€‹DBD, Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG, and Eâ€‹Bâ€‹DEBD.

Next we verify present-and-past exogeneity in the covariance sense. From ([43](https://arxiv.org/html/2601.21272v1#Sx2.E43 "In pre-determined â‡’ğ¸â¢ğµâ¢ğ·. â€£ A.2 Proof of Proposition2 â€£ Appendix â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")),

|  |  |  |
| --- | --- | --- |
|  | ut=Îµt,xs=Î³â€‹usâˆ’1+Î·s=Î³â€‹Îµsâˆ’1+Î·s.u\_{t}=\varepsilon\_{t},\qquad x\_{s}=\gamma u\_{s-1}+\eta\_{s}=\gamma\varepsilon\_{s-1}+\eta\_{s}. |  |

If sâ‰¤ts\leq t, then sâˆ’1â‰¤tâˆ’1<ts-1\leq t-1<t, so Îµsâˆ’1\varepsilon\_{s-1} and Î·s\eta\_{s} are independent of Îµt\varepsilon\_{t}. Hence, for all sâ‰¤ts\leq t,

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[utâ€‹xs]=ğ”¼â€‹[Îµtâ€‹(Î³â€‹Îµsâˆ’1+Î·s)]=Î³â€‹ğ”¼â€‹[Îµtâ€‹Îµsâˆ’1]+ğ”¼â€‹[Îµtâ€‹Î·s]=0,{\mathbb{E}}[u\_{t}x\_{s}]={\mathbb{E}}\big[\varepsilon\_{t}(\gamma\varepsilon\_{s-1}+\eta\_{s})\big]=\gamma\,{\mathbb{E}}[\varepsilon\_{t}\varepsilon\_{s-1}]+{\mathbb{E}}[\varepsilon\_{t}\eta\_{s}]=0, |  |

because {Îµt}\{\varepsilon\_{t}\} is white noise and independent of {Î·t}\{\eta\_{t}\}. Therefore,

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[utâ€‹xs]=0âˆ€sâ‰¤t,{\mathbb{E}}[u\_{t}x\_{s}]=0\quad\forall\,s\leq t, |  |

so present-and-past exogeneity holds.

However, strict exogeneity fails. Indeed,

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[utâ€‹xt+1]=ğ”¼â€‹[Îµtâ€‹(Î³â€‹Îµt+Î·t+1)]=Î³â€‹ğ”¼â€‹[Îµt2]+ğ”¼â€‹[Îµtâ€‹Î·t+1]=Î³â€‹ÏƒÎµ2â‰ 0{\mathbb{E}}[u\_{t}x\_{t+1}]={\mathbb{E}}\big[\varepsilon\_{t}(\gamma\varepsilon\_{t}+\eta\_{t+1})\big]=\gamma\,{\mathbb{E}}[\varepsilon\_{t}^{2}]+{\mathbb{E}}[\varepsilon\_{t}\eta\_{t+1}]=\gamma\,\sigma\_{\varepsilon}^{2}\neq 0 |  |

whenever Î³â‰ 0\gamma\neq 0. Thus the covariance-based strict exogeneity condition ğ”¼â€‹[utâ€‹xs]=0{\mathbb{E}}[u\_{t}x\_{s}]=0 for all s,ts,t does not hold, and by the equivalence shown earlier, Bâ€‹DBD also fails.

Consequently, the DGP ([43](https://arxiv.org/html/2601.21272v1#Sx2.E43 "In pre-determined â‡’ğ¸â¢ğµâ¢ğ·. â€£ A.2 Proof of Proposition2 â€£ Appendix â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) belongs to (present-and-past exogeneity âˆ©{Î£xâ€‹u=0}\cap\{\Sigma\_{xu}=0\}) but not to Bâ€‹DBD, so Bâ€‹DBD âŠŠ\subsetneq {present-and-past exogeneity}\bigl\{\text{present-and-past exogeneity}\bigr\} âˆ©\cap {ğšºxâ€‹u=ğŸ}\bigl\{{\mbox{$\Sigma$}}\_{xu}={\mbox{$0$}}\bigr\}.

{present-and-past exogeneity}\bigl\{\text{present-and-past exogeneity}\bigr\} âˆ©\cap {ğšºxâ€‹u=ğŸ}\bigl\{{\mbox{$\Sigma$}}\_{xu}={\mbox{$0$}}\bigr\} âŠ†\subseteq Eâ€‹Bâ€‹DEBD.

By definition, the Eâ€‹Bâ€‹DEBD condition imposes two requirements on the joint process {ğ’›t}\{{\mbox{$z$}}\_{t}\}:
The innovation covariance is block diagonal:

|  |  |  |
| --- | --- | --- |
|  | ğšº=[ğšºxâ€‹xğŸğŸğšºuâ€‹u],{\mbox{$\Sigma$}}=\begin{bmatrix}{\mbox{$\Sigma$}}\_{xx}&{\mbox{$0$}}\\ {\mbox{$0$}}&{\mbox{$\Sigma$}}\_{uu}\end{bmatrix}, |  |

that is, ğšºxâ€‹u=ğŸ{\mbox{$\Sigma$}}\_{xu}={\mbox{$0$}}. The VAR coefficient matrices admit the general block form

|  |  |  |
| --- | --- | --- |
|  | ğš¿j=[ğš¿xâ€‹x,jğš¿xâ€‹u,jğš¿uâ€‹x,jğš¿uâ€‹u,j],j=1,â€¦,p0,{\mbox{$\Psi$}}\_{j}=\begin{bmatrix}{\mbox{$\Psi$}}\_{xx,j}&{\mbox{$\Psi$}}\_{xu,j}\\ {\mbox{$\Psi$}}\_{ux,j}&{\mbox{$\Psi$}}\_{uu,j}\end{bmatrix},\qquad j=1,\ldots,p\_{0}, |  |

with no additional restrictions on the off-diagonal blocks.

Thus, within the class of DGPs satisfying AssumptionÂ [1](https://arxiv.org/html/2601.21272v1#Thmassumption1 "Assumption 1 (Finite-predictor exactness at lag ğ‘â‚€): â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"), the Eâ€‹Bâ€‹DEBD class is exactly the set of processes for which ğšºxâ€‹u=ğŸ{\mbox{$\Sigma$}}\_{xu}={\mbox{$0$}}, together with the (stable) VAR representation of PropositionÂ [1](https://arxiv.org/html/2601.21272v1#Thmproposition1 "Proposition 1: â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"). In particular, any DGP with ğšºxâ€‹u=ğŸ{\mbox{$\Sigma$}}\_{xu}={\mbox{$0$}} automatically belongs to Eâ€‹Bâ€‹DEBD (regardless of whether present-and-past exogeneity holds). Hence {present-and-past exogeneity}\bigl\{\text{present-and-past exogeneity}\bigr\} âˆ©\cap {ğšºxâ€‹u=ğŸ}\bigl\{{\mbox{$\Sigma$}}\_{xu}={\mbox{$0$}}\bigr\} âŠ†\subseteq Eâ€‹Bâ€‹DEBD.

To show that the inclusion is strict, it suffices to provide a DGP that satisfies Eâ€‹Bâ€‹DEBD but violates present-and-past exogeneity. Consider the scalar (k=N=1k=N=1) VAR(1) system

|  |  |  |  |
| --- | --- | --- | --- |
|  | {xt=Ïxâ€‹xtâˆ’1+Î±â€‹utâˆ’1+Î·t,ut=Ïuâ€‹utâˆ’1+Îµu,t,|Ïx|<1,|Ïu|<1,\begin{cases}x\_{t}=\rho\_{x}x\_{t-1}+\alpha u\_{t-1}+\eta\_{t},\\ u\_{t}=\rho\_{u}u\_{t-1}+\varepsilon\_{u,t},\end{cases}\qquad|\rho\_{x}|<1,\ |\rho\_{u}|<1, |  | (44) |

where {Î·t}\{\eta\_{t}\} and {Îµu,t}\{\varepsilon\_{u,t}\} are mutually independent i.i.d. sequences with zero mean and positive variances. Let ğ’›t=(xt,ut)â€²{\mbox{$z$}}\_{t}=(x\_{t},u\_{t})^{\prime} and ğœºt=(Î·t,Îµu,t)â€²{\mbox{$\varepsilon$}}\_{t}=(\eta\_{t},\varepsilon\_{u,t})^{\prime}. Then ([44](https://arxiv.org/html/2601.21272v1#Sx2.E44 "In pre-determined â‡’ğ¸â¢ğµâ¢ğ·. â€£ A.2 Proof of Proposition2 â€£ Appendix â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) can be written as

|  |  |  |
| --- | --- | --- |
|  | ğ’›Â¯t=[ÏxÎ±0Ïu]â€‹ğ’›Â¯tâˆ’1+ğœºt,\bar{{\mbox{$z$}}}\_{t}=\begin{bmatrix}\rho\_{x}&\alpha\\ 0&\rho\_{u}\end{bmatrix}\bar{{\mbox{$z$}}}\_{t-1}+{\mbox{$\varepsilon$}}\_{t}, |  |

with innovation covariance

|  |  |  |
| --- | --- | --- |
|  | ğšº:=ğ”¼â€‹[ğœºtâ€‹ğœºtâ€²]=[ÏƒÎ·200Ïƒu2].{\mbox{$\Sigma$}}:={\mathbb{E}}[{\mbox{$\varepsilon$}}\_{t}{\mbox{$\varepsilon$}}\_{t}^{\prime}]=\begin{bmatrix}\sigma\_{\eta}^{2}&0\\ 0&\sigma\_{u}^{2}\end{bmatrix}. |  |

Hence ğšºxâ€‹u=0{\mbox{$\Sigma$}}\_{xu}=0, and the VAR coefficient matrix has the general Eâ€‹Bâ€‹DEBD form (with Î¨xâ€‹u,1=Î±â‰ 0\Psi\_{xu,1}=\alpha\neq 0 in general), so this DGP belongs to Eâ€‹Bâ€‹DEBD.

We now show that present-and-past exogeneity fails. Note that utu\_{t} follows an AR(1) driven by Îµu,t\varepsilon\_{u,t}, so {ut}\{u\_{t}\} is serially correlated when Ïuâ‰ 0\rho\_{u}\neq 0. Moreover, xtâˆ’1x\_{t-1} contains the lagged error utâˆ’2u\_{t-2} via the term Î±â€‹utâˆ’2\alpha u\_{t-2}. In the stationary solution of ([44](https://arxiv.org/html/2601.21272v1#Sx2.E44 "In pre-determined â‡’ğ¸â¢ğµâ¢ğ·. â€£ A.2 Proof of Proposition2 â€£ Appendix â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")), utu\_{t} and utâˆ’2u\_{t-2} are correlated whenever Ïuâ‰ 0\rho\_{u}\neq 0, and hence

|  |  |  |
| --- | --- | --- |
|  | â„‚â€‹ovâ€‹(ut,xtâˆ’1)=â„‚â€‹ovâ€‹(ut,Ïxâ€‹xtâˆ’2+Î±â€‹utâˆ’2+Î·tâˆ’1)=Î±â€‹â„‚â€‹ovâ€‹(ut,utâˆ’2)â‰ 0{\mathbb{C}\rm{ov}}(u\_{t},x\_{t-1})={\mathbb{C}\rm{ov}}\big(u\_{t},\rho\_{x}x\_{t-2}+\alpha u\_{t-2}+\eta\_{t-1}\big)=\alpha\,{\mathbb{C}\rm{ov}}(u\_{t},u\_{t-2})\neq 0 |  |

for suitable choices of (Ïu,Î±)(\rho\_{u},\alpha) (e.g. Ïuâ‰ 0\rho\_{u}\neq 0 and Î±â‰ 0\alpha\neq 0). Thus
there exists sâ‰¤ts\leq t (take s=tâˆ’1s=t-1) such that ğ”¼â€‹[utâ€‹xs]â‰ 0{\mathbb{E}}[u\_{t}x\_{s}]\neq 0, so the
present-and-past exogeneity condition does not hold.

Therefore, the DGP ([44](https://arxiv.org/html/2601.21272v1#Sx2.E44 "In pre-determined â‡’ğ¸â¢ğµâ¢ğ·. â€£ A.2 Proof of Proposition2 â€£ Appendix â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) belongs to Eâ€‹Bâ€‹DEBD but not to (present-and-past exogeneity âˆ©{ğšºxâ€‹u=ğŸ}\cap\{{\mbox{$\Sigma$}}\_{xu}={\mbox{$0$}}\}), and we obtain {present-and-past exogeneity}\bigl\{\text{present-and-past exogeneity}\bigr\} âˆ©\cap {ğšºxâ€‹u=ğŸ}\bigl\{{\mbox{$\Sigma$}}\_{xu}={\mbox{$0$}}\bigr\} âŠŠ\subsetneq Eâ€‹Bâ€‹DEBD.

Combining previous results, we have established the strict nesting

|  |  |  |
| --- | --- | --- |
|  | Bâ€‹DâŠŠ{present-and-past exogeneity}âˆ©{ğšºxâ€‹u=ğŸ}âŠŠEâ€‹Bâ€‹D.BD\subsetneq\bigl\{\text{present-and-past exogeneity}\bigr\}\cap\bigl\{{\mbox{$\Sigma$}}\_{xu}={\mbox{$0$}}\bigr\}\subsetneq EBD. |  |

âˆ

### A.3 Proof of Proposition3

We work with the regression system in ([14](https://arxiv.org/html/2601.21272v1#S2.E14 "In 2.3 The inconsistency of OLS and OLS-based GLS estimators in multi-equation models â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")):

|  |  |  |
| --- | --- | --- |
|  | ğ’št=ğ’tâ€²â€‹ğœ¿+ğ’–t,ğ’tâ€²=[ğ‘°N,ğ‘¿tâ€²],ğœ¿=(ğœ¶â€²,ğœ·â€²)â€².{\mbox{$y$}}\_{t}={\mbox{$Z$}}\_{t}^{\prime}{\mbox{$\kappa$}}+{\mbox{$u$}}\_{t},\quad{\mbox{$Z$}}\_{t}^{\prime}=[{\mbox{$I$}}\_{N},{\mbox{$X$}}\_{t}^{\prime}],\quad{\mbox{$\kappa$}}=({\mbox{$\alpha$}}^{\prime},{\mbox{$\beta$}}^{\prime})^{\prime}. |  |

Then the OLS estimator is

|  |  |  |
| --- | --- | --- |
|  | ğœ¿^OLSâˆ’ğœ¿=(1Tâ€‹âˆ‘t=1Tğ’tâ€‹ğ’tâ€²)âˆ’1â€‹(1Tâ€‹âˆ‘t=1Tğ’tâ€‹ğ’–t).\widehat{{\mbox{$\kappa$}}}^{\mathrm{OLS}}-{\mbox{$\kappa$}}=\Big(\frac{1}{T}\sum\_{t=1}^{T}{\mbox{$Z$}}\_{t}{\mbox{$Z$}}\_{t}^{\prime}\Big)^{-1}\Big(\frac{1}{T}\sum\_{t=1}^{T}{\mbox{$Z$}}\_{t}{\mbox{$u$}}\_{t}\Big). |  |

(i) By AssumptionÂ [1](https://arxiv.org/html/2601.21272v1#Thmassumption1 "Assumption 1 (Finite-predictor exactness at lag ğ‘â‚€): â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")-(A1.1) and PropositionÂ [1](https://arxiv.org/html/2601.21272v1#Thmproposition1 "Proposition 1: â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") (iii)-(iv), the processes ğ’t{{\mbox{$Z$}}\_{t}} and ğ’–t{{\mbox{$u$}}\_{t}} are ergodic and have finite (2+Î´)(2+\delta)-th moments. Hence the ergodic LLN yields

|  |  |  |
| --- | --- | --- |
|  | 1Tâ€‹âˆ‘t=1Tğ’tâ€‹ğ’tâ€²â†’ğ‘ğ‘¸Z:=ğ”¼â€‹[ğ’tâ€‹ğ’tâ€²]and1Tâ€‹âˆ‘t=1Tğ’tâ€‹ğ’–tâ†’ğ‘ğ”¼â€‹[ğ’tâ€‹ğ’–t],\frac{1}{T}\sum\_{t=1}^{T}{\mbox{$Z$}}\_{t}{\mbox{$Z$}}\_{t}^{\prime}\xrightarrow{p}{\mbox{$Q$}}\_{Z}:={\mathbb{E}}[{\mbox{$Z$}}\_{t}{\mbox{$Z$}}\_{t}^{\prime}]\quad\text{and}\quad\frac{1}{T}\sum\_{t=1}^{T}{\mbox{$Z$}}\_{t}{\mbox{$u$}}\_{t}\xrightarrow{p}{\mathbb{E}}[{\mbox{$Z$}}\_{t}{\mbox{$u$}}\_{t}], |  |

where ğ‘¸Z{\mbox{$Q$}}\_{Z} is positive definite by AssumptionÂ [2](https://arxiv.org/html/2601.21272v1#Thmassumption2 "Assumption 2: â€£ 2.3 The inconsistency of OLS and OLS-based GLS estimators in multi-equation models â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"). Hence

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğœ¿^OLSâ†’ğ‘ğœ¿+ğ‘¸Zâˆ’1â€‹ğ”¼â€‹[ğ’tâ€‹ğ’–t].\widehat{{\mbox{$\kappa$}}}^{\mathrm{OLS}}\xrightarrow{p}{\mbox{$\kappa$}}+{\mbox{$Q$}}\_{Z}^{-1}{\mathbb{E}}[{\mbox{$Z$}}\_{t}{\mbox{$u$}}\_{t}]. |  | (45) |

Using

|  |  |  |
| --- | --- | --- |
|  | ğ’t=[ğ‘°Nğ‘¿t]andğ”¼â€‹[ğ’–t]=ğŸ,{\mbox{$Z$}}\_{t}=\begin{bmatrix}{\mbox{$I$}}\_{N}\\ {\mbox{$X$}}\_{t}\end{bmatrix}\quad\text{and}\quad{\mathbb{E}}[{\mbox{$u$}}\_{t}]={\mbox{$0$}}, |  |

we obtain

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[ğ’tâ€‹ğ’–t]=[ğ”¼â€‹[ğ’–t]ğ”¼â€‹[ğ‘¿tâ€‹ğ’–t]]=[ğŸğ”¼â€‹[ğ‘¿tâ€‹ğ’–t]].{\mathbb{E}}[{\mbox{$Z$}}\_{t}{\mbox{$u$}}\_{t}]=\begin{bmatrix}{\mathbb{E}}[{\mbox{$u$}}\_{t}]\\ {\mathbb{E}}[{\mbox{$X$}}\_{t}{\mbox{$u$}}\_{t}]\\ \end{bmatrix}=\begin{bmatrix}{\mbox{$0$}}\\ {\mathbb{E}}[{\mbox{$X$}}\_{t}{\mbox{$u$}}\_{t}]\\ \end{bmatrix}. |  |

Substituting this into ([45](https://arxiv.org/html/2601.21272v1#Sx2.E45 "In A.3 Proof of Proposition3 â€£ Appendix â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) yields

|  |  |  |
| --- | --- | --- |
|  | plimTâ†’âˆğœ¿^OLS=ğœ¿+ğ‘¸Zâˆ’1â€‹[ğŸğ”¼â€‹[ğ‘¿tâ€‹ğ’–t]].\operatorname\*{plim}\_{T\to\infty}\widehat{{\mbox{$\kappa$}}}^{\mathrm{OLS}}={\mbox{$\kappa$}}+{\mbox{$Q$}}\_{Z}^{-1}\begin{bmatrix}{\mbox{$0$}}\\ {\mathbb{E}}[{\mbox{$X$}}\_{t}{\mbox{$u$}}\_{t}]\end{bmatrix}. |  |

Since ğ‘¸Z{\mbox{$Q$}}\_{Z} is nonsingular, this limit equals ğœ¿\kappa if and only if

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[ğ‘¿tâ€‹ğ’–t]=ğŸ,{\mathbb{E}}[{\mbox{$X$}}\_{t}{\mbox{$u$}}\_{t}]={\mbox{$0$}}, |  |

that is, if and only if the present exogeneity condition holds. âˆ

### A.4 Proof of Proposition4

###### Proof.

Under AssumptionÂ [1](https://arxiv.org/html/2601.21272v1#Thmassumption1 "Assumption 1 (Finite-predictor exactness at lag ğ‘â‚€): â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"), PropositionÂ [1](https://arxiv.org/html/2601.21272v1#Thmproposition1 "Proposition 1: â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") yields the joint Wold representation

|  |  |  |
| --- | --- | --- |
|  | ğ’›Â¯t=âˆ‘i=0âˆğšµiâ€‹ğœºtâˆ’i,ğœºt=(ğœºx,tâ€²,ğœºu,tâ€²)â€²,\bar{{\mbox{$z$}}}\_{t}=\sum\_{i=0}^{\infty}{\mbox{$\Xi$}}\_{i}{\mbox{$\varepsilon$}}\_{t-i},\quad{\mbox{$\varepsilon$}}\_{t}=({\mbox{$\varepsilon$}}\_{x,t}^{\prime},{\mbox{$\varepsilon$}}\_{u,t}^{\prime})^{\prime}, |  |

with ğœºt{{\mbox{$\varepsilon$}}\_{t}} second-order white noise and ğšµ0=ğ‘°m{\mbox{$\Xi$}}\_{0}={\mbox{$I$}}\_{m}. Partition ğšµi{\mbox{$\Xi$}}\_{i} and ğšº:=ğ”¼â€‹[ğœºtâ€‹ğœºtâ€²]{\mbox{$\Sigma$}}:={\mathbb{E}}[{\mbox{$\varepsilon$}}\_{t}{\mbox{$\varepsilon$}}\_{t}^{\prime}] conformably as in PropositionÂ [1](https://arxiv.org/html/2601.21272v1#Thmproposition1 "Proposition 1: â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")(i). Then, by whiteness and absolute summability, we have the identity

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â€‹[ğ’™Â¯tâ€‹ğ’–tâ€²]=âˆ‘i=0âˆ(ğšµxâ€‹x,iâ€‹ğšºxâ€‹xâ€‹ğšµuâ€‹x,iâ€²+ğšµxâ€‹x,iâ€‹ğšºxâ€‹uâ€‹ğšµuâ€‹u,iâ€²+ğšµxâ€‹u,iâ€‹ğšºuâ€‹xâ€‹ğšµuâ€‹x,iâ€²+ğšµxâ€‹u,iâ€‹ğšºuâ€‹uâ€‹ğšµuâ€‹u,iâ€²).{\mathbb{E}}[\bar{{\mbox{$x$}}}\_{t}{\mbox{$u$}}\_{t}^{\prime}]=\sum\_{i=0}^{\infty}\Big({\mbox{$\Xi$}}\_{xx,i}{\mbox{$\Sigma$}}\_{xx}{\mbox{$\Xi$}}\_{ux,i}^{\prime}+{\mbox{$\Xi$}}\_{xx,i}{\mbox{$\Sigma$}}\_{xu}{\mbox{$\Xi$}}\_{uu,i}^{\prime}+{\mbox{$\Xi$}}\_{xu,i}{\mbox{$\Sigma$}}\_{ux}{\mbox{$\Xi$}}\_{ux,i}^{\prime}+{\mbox{$\Xi$}}\_{xu,i}{\mbox{$\Sigma$}}\_{uu}{\mbox{$\Xi$}}\_{uu,i}^{\prime}\Big). |  | (46) |

Moreover, since ğšµ0=ğ‘°m{\mbox{$\Xi$}}\_{0}={\mbox{$I$}}\_{m}, ğšµxâ€‹x,0=ğ‘°k{\mbox{$\Xi$}}\_{xx,0}={\mbox{$I$}}\_{k}, ğšµuâ€‹u,0=ğ‘°N{\mbox{$\Xi$}}\_{uu,0}={\mbox{$I$}}\_{N} and ğšµxâ€‹u,0=ğšµuâ€‹x,0=ğŸ{\mbox{$\Xi$}}\_{xu,0}={\mbox{$\Xi$}}\_{ux,0}={\mbox{$0$}}, we can rewrite ([46](https://arxiv.org/html/2601.21272v1#Sx2.E46 "In Proof. â€£ A.4 Proof of Proposition4 â€£ Appendix â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) as

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â€‹[ğ’™Â¯tâ€‹ğ’–tâ€²]=ğšºxâ€‹u+âˆ‘i=1âˆ(ğšµxâ€‹x,iâ€‹ğšºxâ€‹xâ€‹ğšµuâ€‹x,iâ€²+ğšµxâ€‹x,iâ€‹ğšºxâ€‹uâ€‹ğšµuâ€‹u,iâ€²+ğšµxâ€‹u,iâ€‹ğšºuâ€‹xâ€‹ğšµuâ€‹x,iâ€²+ğšµxâ€‹u,iâ€‹ğšºuâ€‹uâ€‹ğšµuâ€‹u,iâ€²).{\mathbb{E}}[\bar{{\mbox{$x$}}}\_{t}{\mbox{$u$}}\_{t}^{\prime}]={\mbox{$\Sigma$}}\_{xu}+\sum\_{i=1}^{\infty}\Big({\mbox{$\Xi$}}\_{xx,i}{\mbox{$\Sigma$}}\_{xx}{\mbox{$\Xi$}}\_{ux,i}^{\prime}+{\mbox{$\Xi$}}\_{xx,i}{\mbox{$\Sigma$}}\_{xu}{\mbox{$\Xi$}}\_{uu,i}^{\prime}+{\mbox{$\Xi$}}\_{xu,i}{\mbox{$\Sigma$}}\_{ux}{\mbox{$\Xi$}}\_{ux,i}^{\prime}+{\mbox{$\Xi$}}\_{xu,i}{\mbox{$\Sigma$}}\_{uu}{\mbox{$\Xi$}}\_{uu,i}^{\prime}\Big). |  | (47) |

(i) BD.
Under Bâ€‹DBD, both the VAR coefficients and the innovation covariance are block diagonal. Equivalently, the impulse responses are block diagonal, i.e. ğšµxâ€‹u,i=ğšµuâ€‹x,i=ğŸ{\mbox{$\Xi$}}\_{xu,i}={\mbox{$\Xi$}}\_{ux,i}={\mbox{$0$}} for all iâ‰¥0i\geq 0, and ğšºxâ€‹u=ğšºuâ€‹x=ğŸ{\mbox{$\Sigma$}}\_{xu}={\mbox{$\Sigma$}}\_{ux}={\mbox{$0$}}. Substituting these restrictions into ([46](https://arxiv.org/html/2601.21272v1#Sx2.E46 "In Proof. â€£ A.4 Proof of Proposition4 â€£ Appendix â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) yields ğ”¼â€‹[ğ’™Â¯tâ€‹ğ’–tâ€²]=ğŸ{\mathbb{E}}[\bar{{\mbox{$x$}}}\_{t}{\mbox{$u$}}\_{t}^{\prime}]={\mbox{$0$}}, hence ğ”¼â€‹[ğ‘¿tâ€‹ğ’–t]=ğŸ{\mathbb{E}}[{\mbox{$X$}}\_{t}{\mbox{$u$}}\_{t}]={\mbox{$0$}}.

(ii) GEXOGâˆ–\setminusBD.
Under Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG, ğšºxâ€‹u=ğšºuâ€‹x=ğŸ{\mbox{$\Sigma$}}\_{xu}={\mbox{$\Sigma$}}\_{ux}={\mbox{$0$}} and ğš¿uâ€‹x,j=ğŸ{\mbox{$\Psi$}}\_{ux,j}={\mbox{$0$}} for all jj, so the characteristic polynomial is upper block-triangular. Therefore ğšµâ€‹(L)=ğ‘·â€‹(L)âˆ’1{\mbox{$\Xi$}}(L)={\mbox{$P$}}(L)^{-1} is also upper block-triangular, implying ğšµuâ€‹x,i=ğŸ{\mbox{$\Xi$}}\_{ux,i}={\mbox{$0$}} for all iâ‰¥0i\geq 0. In contrast, Gâ€‹Eâ€‹Xâ€‹Oâ€‹Gâˆ–Bâ€‹DGEXOG\setminus BD means that ğš¿xâ€‹u,jâ‰ ğŸ{\mbox{$\Psi$}}\_{xu,j}\neq{\mbox{$0$}} for some jj, which generically implies ğšµxâ€‹u,iâ‰ ğŸ{\mbox{$\Xi$}}\_{xu,i}\neq{\mbox{$0$}} for at least one iâ‰¥1i\geq 1. Hence ([46](https://arxiv.org/html/2601.21272v1#Sx2.E46 "In Proof. â€£ A.4 Proof of Proposition4 â€£ Appendix â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) reduces to

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â€‹[ğ’™Â¯tâ€‹ğ’–tâ€²]=âˆ‘i=1âˆğšµxâ€‹u,iâ€‹ğšºuâ€‹uâ€‹ğšµuâ€‹u,iâ€².{\mathbb{E}}[\bar{{\mbox{$x$}}}\_{t}{\mbox{$u$}}\_{t}^{\prime}]=\sum\_{i=1}^{\infty}{\mbox{$\Xi$}}\_{xu,i}{\mbox{$\Sigma$}}\_{uu}{\mbox{$\Xi$}}\_{uu,i}^{\prime}. |  | (48) |

If ğ’–t{\mbox{$u$}}\_{t} is serially correlated (equivalently, ğšµuâ€‹u,iâ‰ ğŸ{\mbox{$\Xi$}}\_{uu,i}\neq{\mbox{$0$}} for some iâ‰¥1i\geq 1), then the right-hand side of ([48](https://arxiv.org/html/2601.21272v1#Sx2.E48 "In Proof. â€£ A.4 Proof of Proposition4 â€£ Appendix â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) is nonzero for generic parameter values because it is a sum of nontrivial cross-block impulse-response terms weighted by ğšºuâ€‹u>0{\mbox{$\Sigma$}}\_{uu}>0.
The only ways for ([48](https://arxiv.org/html/2601.21272v1#Sx2.E48 "In Proof. â€£ A.4 Proof of Proposition4 â€£ Appendix â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) to vanish are additional restrictions such as ğš¿uâ€‹u,j=ğŸâ€‹âˆ€j{\mbox{$\Psi$}}\_{uu,j}={\mbox{$0$}}\ \forall j (so that ğšµuâ€‹u,i=ğŸ{\mbox{$\Xi$}}\_{uu,i}={\mbox{$0$}} for all iâ‰¥1i\geq 1) or other knife-edge cancellations across lags.
Therefore, under Gâ€‹Eâ€‹Xâ€‹Oâ€‹Gâˆ–Bâ€‹DGEXOG\setminus BD, contemporaneous orthogonality typically fails and, by PropositionÂ [3](https://arxiv.org/html/2601.21272v1#Thmproposition3 "Proposition 3: â€£ 2.3 The inconsistency of OLS and OLS-based GLS estimators in multi-equation models â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"), ğœ¿^Oâ€‹Lâ€‹S\widehat{{\mbox{$\kappa$}}}^{OLS} is generically inconsistent.

(iii) EBD.
Since Gâ€‹Eâ€‹Xâ€‹Oâ€‹GâŠŠEâ€‹Bâ€‹DGEXOG\subsetneq EBD, the same mechanism applies within Eâ€‹Bâ€‹DEBD: there exist DGPs in Eâ€‹Bâ€‹DEBD (namely those in Gâ€‹Eâ€‹Xâ€‹Oâ€‹Gâˆ–Bâ€‹DGEXOG\setminus BD) for which ğ”¼â€‹[ğ‘¿tâ€‹ğ’–t]â‰ ğŸ{\mathbb{E}}[{\mbox{$X$}}\_{t}{\mbox{$u$}}\_{t}]\neq{\mbox{$0$}} generically, so OLS can be inconsistent within the Eâ€‹Bâ€‹DEBD class as well.
âˆ

### A.5 Proof of PropositionÂ [5](https://arxiv.org/html/2601.21272v1#Thmproposition5 "Proposition 5 (Consistency region of CO-type estimators): â€£ 2.3 The inconsistency of OLS and OLS-based GLS estimators in multi-equation models â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")

Throughout, let Tâˆ—:=Tâˆ’p0T^{\ast}:=T-p\_{0} and implicitly restrict sums to t=p0+1,â€¦,Tt=p\_{0}+1,\ldots,T.

(i) Consistency and asymptotic equivalence under Bâ€‹DBD.
Assume the Bâ€‹DBD condition holds. By PropositionÂ [2](https://arxiv.org/html/2601.21272v1#Thmproposition2 "Proposition 2: â€£ 2.2 Relationships among exogeneity conditions â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")(i), Bâ€‹DBD is equivalent to covariance-based strict exogeneity, which implies ğ”¼â€‹[ğ’tâ€‹ğ’–t]=ğŸ{\mathbb{E}}[{\mbox{$Z$}}\_{t}{\mbox{$u$}}\_{t}]={\mbox{$0$}} and hence ğ”¼â€‹[ğ‘¿tâ€‹ğ’–t]=ğŸ{\mathbb{E}}[{\mbox{$X$}}\_{t}{\mbox{$u$}}\_{t}]={\mbox{$0$}}.
Therefore, by PropositionÂ [3](https://arxiv.org/html/2601.21272v1#Thmproposition3 "Proposition 3: â€£ 2.3 The inconsistency of OLS and OLS-based GLS estimators in multi-equation models â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğœ¿^Oâ€‹Lâ€‹Sâ†’ğ‘ğœ¿.\widehat{{\mbox{$\kappa$}}}^{\tiny OLS}\xrightarrow{p}{\mbox{$\kappa$}}. |  | (49) |

Write the OLS residuals as

|  |  |  |
| --- | --- | --- |
|  | ğ’–^tOâ€‹Lâ€‹S=ğ’štâˆ’ğ’tâ€²â€‹ğœ¿^Oâ€‹Lâ€‹S=ğ’–tâˆ’ğ’tâ€²â€‹(ğœ¿^Oâ€‹Lâ€‹Sâˆ’ğœ¿).\widehat{{\mbox{$u$}}}\_{t}^{\tiny OLS}={\mbox{$y$}}\_{t}-{\mbox{$Z$}}\_{t}^{\prime}\widehat{{\mbox{$\kappa$}}}^{\tiny OLS}={\mbox{$u$}}\_{t}-{\mbox{$Z$}}\_{t}^{\prime}(\widehat{{\mbox{$\kappa$}}}^{\tiny OLS}-{\mbox{$\kappa$}}). |  |

Since {ğ’t}\{{\mbox{$Z$}}\_{t}\} is ergodic with finite second moments (PropositionÂ [1](https://arxiv.org/html/2601.21272v1#Thmproposition1 "Proposition 1: â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")(iv)), ([49](https://arxiv.org/html/2601.21272v1#Sx2.E49 "In A.5 Proof of Proposition 5 â€£ Appendix â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) yields

|  |  |  |  |
| --- | --- | --- | --- |
|  | 1Tâ€‹âˆ‘t=1Tâ€–ğ’–^tOâ€‹Lâ€‹Sâˆ’ğ’–tâ€–2â‰¤â€–ğœ¿^Oâ€‹Lâ€‹Sâˆ’ğœ¿â€–2â‹…1Tâ€‹âˆ‘t=1Tâ€–ğ’tâ€–2â†’ğ‘0.\frac{1}{T}\sum\_{t=1}^{T}\|\widehat{{\mbox{$u$}}}\_{t}^{\tiny OLS}-{\mbox{$u$}}\_{t}\|^{2}\leq\|\widehat{{\mbox{$\kappa$}}}^{\tiny OLS}-{\mbox{$\kappa$}}\|^{2}\cdot\frac{1}{T}\sum\_{t=1}^{T}\|{\mbox{$Z$}}\_{t}\|^{2}\xrightarrow{p}0. |  | (50) |

Under Bâ€‹DBD, the joint VAR(p0p\_{0}) representation in PropositionÂ [1](https://arxiv.org/html/2601.21272v1#Thmproposition1 "Proposition 1: â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")(ii) is block diagonal, so in particular the uu-block follows a stable VAR(p0p\_{0}) of the form ([16](https://arxiv.org/html/2601.21272v1#S2.E16 "In 2.3 The inconsistency of OLS and OLS-based GLS estimators in multi-equation models â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) with innovation covariance ğšºuâ€‹uâ‰»0{\mbox{$\Sigma$}}\_{uu}\succ 0.
Let ğš¿^uâ€‹u:=(ğš¿^uâ€‹u,1,â€¦,ğš¿^uâ€‹u,p0)\widehat{{\mbox{$\Psi$}}}\_{uu}:=(\widehat{{\mbox{$\Psi$}}}\_{uu,1},\ldots,\widehat{{\mbox{$\Psi$}}}\_{uu,p\_{0}}) denote the VAR(p0p\_{0}) OLS estimator obtained by regressing ğ’–^tOâ€‹Lâ€‹S\widehat{{\mbox{$u$}}}\_{t}^{\tiny OLS} on (ğ’–^tâˆ’1Oâ€‹Lâ€‹S,â€¦,ğ’–^tâˆ’p0Oâ€‹Lâ€‹S)(\widehat{{\mbox{$u$}}}\_{t-1}^{\tiny OLS},\ldots,\widehat{{\mbox{$u$}}}\_{t-p\_{0}}^{\tiny OLS}).
By standard LLN/continuous-mapping arguments and ([50](https://arxiv.org/html/2601.21272v1#Sx2.E50 "In A.5 Proof of Proposition 5 â€£ Appendix â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) (a generated-regressor argument),

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğš¿^uâ€‹u,jâ†’ğ‘ğš¿uâ€‹u,j(j=1,â€¦,p0),ğšº^uâ€‹uâ†’ğ‘ğšºuâ€‹u.\widehat{{\mbox{$\Psi$}}}\_{uu,j}\xrightarrow{p}{\mbox{$\Psi$}}\_{uu,j}\quad(j=1,\ldots,p\_{0}),\qquad\widehat{{\mbox{$\Sigma$}}}\_{uu}\xrightarrow{p}{\mbox{$\Sigma$}}\_{uu}. |  | (51) |

Consequently, ğ‘¨^â€‹(L)â†’ğ‘¨â€‹(L)\widehat{{\mbox{$A$}}}(L)\to{\mbox{$A$}}(L) coefficientwise in probability.

Define the estimated transformed variables

|  |  |  |
| --- | --- | --- |
|  | ğ’š~^t:=ğ‘¨^â€‹(L)â€‹ğ’št,ğ’~^tâ€²:=ğ‘¨^â€‹(L)â€‹ğ’tâ€²,\widehat{\tilde{{\mbox{$y$}}}}\_{t}:=\widehat{{\mbox{$A$}}}(L){\mbox{$y$}}\_{t},\qquad\widehat{\tilde{{\mbox{$Z$}}}}\_{t}^{\prime}:=\widehat{{\mbox{$A$}}}(L){\mbox{$Z$}}\_{t}^{\prime}, |  |

and recall the infeasible transforms ğ’š~t:=ğ‘¨â€‹(L)â€‹ğ’št\tilde{{\mbox{$y$}}}\_{t}:={\mbox{$A$}}(L){\mbox{$y$}}\_{t} and ğ’~tâ€²:=ğ‘¨â€‹(L)â€‹ğ’tâ€²\tilde{{\mbox{$Z$}}}\_{t}^{\prime}:={\mbox{$A$}}(L){\mbox{$Z$}}\_{t}^{\prime}.
Using ([51](https://arxiv.org/html/2601.21272v1#Sx2.E51 "In A.5 Proof of Proposition 5 â€£ Appendix â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) together with ergodicity and finite second moments of (ğ’št,ğ’t)({\mbox{$y$}}\_{t},{\mbox{$Z$}}\_{t}), we obtain

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | 1Tâˆ—â€‹âˆ‘t=p0+1Tğ’~^tâ€‹ğšº^uâ€‹uâˆ’1â€‹ğ’~^tâ€²\displaystyle\frac{1}{T^{\ast}}\sum\_{t=p\_{0}+1}^{T}\widehat{\tilde{{\mbox{$Z$}}}}\_{t}\,\widehat{{\mbox{$\Sigma$}}}\_{uu}^{-1}\widehat{\tilde{{\mbox{$Z$}}}}\_{t}^{\prime} | =1Tâˆ—â€‹âˆ‘t=p0+1Tğ’~tâ€‹ğšºuâ€‹uâˆ’1â€‹ğ’~tâ€²+opâ€‹(1),\displaystyle=\frac{1}{T^{\ast}}\sum\_{t=p\_{0}+1}^{T}\tilde{{\mbox{$Z$}}}\_{t}\,{\mbox{$\Sigma$}}\_{uu}^{-1}\tilde{{\mbox{$Z$}}}\_{t}^{\prime}+o\_{p}(1), |  | (52) |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | 1Tâˆ—â€‹âˆ‘t=p0+1Tğ’~^tâ€‹ğšº^uâ€‹uâˆ’1â€‹ğ’š~^t\displaystyle\frac{1}{T^{\ast}}\sum\_{t=p\_{0}+1}^{T}\widehat{\tilde{{\mbox{$Z$}}}}\_{t}\,\widehat{{\mbox{$\Sigma$}}}\_{uu}^{-1}\widehat{\tilde{{\mbox{$y$}}}}\_{t} | =1Tâˆ—â€‹âˆ‘t=p0+1Tğ’~tâ€‹ğšºuâ€‹uâˆ’1â€‹ğ’š~t+opâ€‹(1).\displaystyle=\frac{1}{T^{\ast}}\sum\_{t=p\_{0}+1}^{T}\tilde{{\mbox{$Z$}}}\_{t}\,{\mbox{$\Sigma$}}\_{uu}^{-1}\tilde{{\mbox{$y$}}}\_{t}+o\_{p}(1). |  | (53) |

Moreover, by AssumptionÂ [3](https://arxiv.org/html/2601.21272v1#Thmassumption3 "Assumption 3: â€£ 2.3 The inconsistency of OLS and OLS-based GLS estimators in multi-equation models â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") and the ergodic LLN,

|  |  |  |
| --- | --- | --- |
|  | 1Tâˆ—â€‹âˆ‘t=p0+1Tğ’~tâ€‹ğšºuâ€‹uâˆ’1â€‹ğ’~tâ€²â†’ğ‘ğ‘¸~Z:=ğ”¼â€‹[ğ’~tâ€‹ğšºuâ€‹uâˆ’1â€‹ğ’~tâ€²]â‰»0,\frac{1}{T^{\ast}}\sum\_{t=p\_{0}+1}^{T}\tilde{{\mbox{$Z$}}}\_{t}\,{\mbox{$\Sigma$}}\_{uu}^{-1}\tilde{{\mbox{$Z$}}}\_{t}^{\prime}\xrightarrow{p}\tilde{{\mbox{$Q$}}}\_{Z}:={\mathbb{E}}\!\big[\tilde{{\mbox{$Z$}}}\_{t}\,{\mbox{$\Sigma$}}\_{uu}^{-1}\tilde{{\mbox{$Z$}}}\_{t}^{\prime}\big]\succ 0, |  |

so the matrix on the left-hand side is nonsingular with probability approaching one.
Since the map (M,b)â†¦Mâˆ’1â€‹b(M,b)\mapsto M^{-1}b is continuous on the set of nonsingular MM,
([52](https://arxiv.org/html/2601.21272v1#Sx2.E52 "In A.5 Proof of Proposition 5 â€£ Appendix â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"))â€“([53](https://arxiv.org/html/2601.21272v1#Sx2.E53 "In A.5 Proof of Proposition 5 â€£ Appendix â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) imply

|  |  |  |
| --- | --- | --- |
|  | ğœ¿^Câ€‹Oâˆ’ğœ¿^GLS=opâ€‹(1),ğœ¿^Câ€‹Oâ†’ğ‘ğœ¿,\widehat{{\mbox{$\kappa$}}}^{\tiny CO}-\widehat{{\mbox{$\kappa$}}}^{\mathrm{GLS}}=o\_{p}(1),\qquad\widehat{{\mbox{$\kappa$}}}^{\tiny CO}\xrightarrow{p}{\mbox{$\kappa$}}, |  |

where ğœ¿^GLS\widehat{{\mbox{$\kappa$}}}^{\mathrm{GLS}} is the infeasible GLS estimator in ([17](https://arxiv.org/html/2601.21272v1#S2.E17 "In 2.3 The inconsistency of OLS and OLS-based GLS estimators in multi-equation models â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")).
Moreover, under standard regularity conditions for stable VAR estimation (yielding T\sqrt{T}-consistency of ğš¿^uâ€‹u,j\widehat{{\mbox{$\Psi$}}}\_{uu,j} and ğšº^uâ€‹u\widehat{{\mbox{$\Sigma$}}}\_{uu}), the same argument strengthens to

|  |  |  |
| --- | --- | --- |
|  | Tâ€‹(ğœ¿^Câ€‹Oâˆ’ğœ¿^GLS)=opâ€‹(1),\sqrt{T}\big(\widehat{{\mbox{$\kappa$}}}^{\tiny CO}-\widehat{{\mbox{$\kappa$}}}^{\mathrm{GLS}}\big)=o\_{p}(1), |  |

so ğœ¿^Câ€‹O\widehat{{\mbox{$\kappa$}}}^{\tiny CO} is asymptotically equivalent to the infeasible GLS estimator, and hence attains the GLS efficiency bound within the Bâ€‹DBD class.
This proves (i).

(ii) Generic failure outside Bâ€‹DBD, in particular under Gâ€‹Eâ€‹Xâ€‹Oâ€‹Gâˆ–Bâ€‹DGEXOG\setminus BD.
Assume now that the DGP lies in Gâ€‹Eâ€‹Xâ€‹Oâ€‹Gâˆ–Bâ€‹DGEXOG\setminus BD. By PropositionÂ [4](https://arxiv.org/html/2601.21272v1#Thmproposition4 "Proposition 4: â€£ 2.3 The inconsistency of OLS and OLS-based GLS estimators in multi-equation models â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")(ii),
the contemporaneous orthogonality condition ğ”¼â€‹[ğ‘¿tâ€‹ğ’–t]=ğŸ{\mathbb{E}}[{\mbox{$X$}}\_{t}{\mbox{$u$}}\_{t}]={\mbox{$0$}} fails for generic parameter values in Gâ€‹Eâ€‹Xâ€‹Oâ€‹Gâˆ–Bâ€‹DGEXOG\setminus BD as soon as the error has serial correlation (e.g. âˆƒj:ğš¿uâ€‹u,jâ‰ ğŸ\exists j:{\mbox{$\Psi$}}\_{uu,j}\neq{\mbox{$0$}}).
Hence, by PropositionÂ [3](https://arxiv.org/html/2601.21272v1#Thmproposition3 "Proposition 3: â€£ 2.3 The inconsistency of OLS and OLS-based GLS estimators in multi-equation models â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğœ¿^Oâ€‹Lâ€‹Sâ†’ğ‘ğœ¿+ğ’ƒ,ğ’ƒâ‰ ğŸâ€‹(generic).\widehat{{\mbox{$\kappa$}}}^{\tiny OLS}\xrightarrow{p}{\mbox{$\kappa$}}+{\mbox{$b$}},\qquad{\mbox{$b$}}\neq{\mbox{$0$}}\ \text{(generic)}. |  | (54) |

Then the first-step residuals satisfy

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ’–^tOâ€‹Lâ€‹S=ğ’–tâˆ’ğ’tâ€²â€‹(ğœ¿^Oâ€‹Lâ€‹Sâˆ’ğœ¿)=ğ’–tâˆ’ğ’tâ€²â€‹ğ’ƒ+opâ€‹(1),\widehat{{\mbox{$u$}}}\_{t}^{\tiny OLS}={\mbox{$u$}}\_{t}-{\mbox{$Z$}}\_{t}^{\prime}(\widehat{{\mbox{$\kappa$}}}^{\tiny OLS}-{\mbox{$\kappa$}})={\mbox{$u$}}\_{t}-{\mbox{$Z$}}\_{t}^{\prime}{\mbox{$b$}}+o\_{p}(1), |  | (55) |

so the residual process converges to a different process

|  |  |  |
| --- | --- | --- |
|  | ğ’–tâˆ—:=ğ’–tâˆ’ğ’tâ€²â€‹ğ’ƒ,{\mbox{$u$}}\_{t}^{\ast}:={\mbox{$u$}}\_{t}-{\mbox{$Z$}}\_{t}^{\prime}{\mbox{$b$}}, |  |

not to the true disturbance ğ’–t{\mbox{$u$}}\_{t} (unless ğ’ƒ=ğŸ{\mbox{$b$}}={\mbox{$0$}}, which is non-generic).

In the second step, the VAR(p0p\_{0}) is fitted to {ğ’–^tOâ€‹Lâ€‹S}\{\widehat{{\mbox{$u$}}}\_{t}^{\tiny OLS}\}, which is asymptotically equivalent to fitting it to {ğ’–tâˆ—}\{{\mbox{$u$}}\_{t}^{\ast}\}. Therefore the estimated VAR coefficients converge to the pseudo-true coefficients ğš¿uâ€‹u,jâˆ—{\mbox{$\Psi$}}\_{uu,j}^{\ast} that solve the population least-squares projection of ğ’–tâˆ—{\mbox{$u$}}\_{t}^{\ast} on (ğ’–tâˆ’1âˆ—,â€¦,ğ’–tâˆ’p0âˆ—)({\mbox{$u$}}\_{t-1}^{\ast},\ldots,{\mbox{$u$}}\_{t-p\_{0}}^{\ast}):

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğš¿^uâ€‹u,jâ†’ğ‘ğš¿uâ€‹u,jâˆ—â‰ ğš¿uâ€‹u,j(generic),ğšº^uâ€‹uâ†’ğ‘ğšºuâ€‹uâˆ—,\widehat{{\mbox{$\Psi$}}}\_{uu,j}\xrightarrow{p}{\mbox{$\Psi$}}\_{uu,j}^{\ast}\neq{\mbox{$\Psi$}}\_{uu,j}\quad\text{(generic)},\qquad\widehat{{\mbox{$\Sigma$}}}\_{uu}\xrightarrow{p}{\mbox{$\Sigma$}}\_{uu}^{\ast}, |  | (56) |

where the inequality is generic because ğ’–tâˆ—{\mbox{$u$}}\_{t}^{\ast} has a different second-order structure from ğ’–t{\mbox{$u$}}\_{t} whenever ğ’ƒâ‰ ğŸ{\mbox{$b$}}\neq{\mbox{$0$}} and ğ’t{\mbox{$Z$}}\_{t} is non-degenerate (AssumptionÂ [2](https://arxiv.org/html/2601.21272v1#Thmassumption2 "Assumption 2: â€£ 2.3 The inconsistency of OLS and OLS-based GLS estimators in multi-equation models â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")).

Let ğ‘¨âˆ—â€‹(L):=ğ‘°Nâˆ’âˆ‘j=1p0ğš¿uâ€‹u,jâˆ—â€‹Lj{\mbox{$A$}}^{\ast}(L):={\mbox{$I$}}\_{N}-\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{uu,j}^{\ast}L^{j} be the corresponding pseudo-true filter. Then the feasible CO transformation converges to applying ğ‘¨âˆ—â€‹(L){\mbox{$A$}}^{\ast}(L) rather than the true ğ‘¨â€‹(L){\mbox{$A$}}(L). As a result, the second-step estimator converges to the pseudo-true coefficient vector

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğœ¿^Câ€‹Oâ†’ğ‘ğœ¿âˆ—,ğœ¿âˆ—=argâ¡minğœ¿â¡ğ”¼â€‹[â€–ğšºuâ€‹uâˆ—âˆ’1/2â€‹ğ‘¨âˆ—â€‹(L)â€‹(ğ’štâˆ’ğ’tâ€²â€‹ğœ¿)â€–2],\widehat{{\mbox{$\kappa$}}}^{\tiny CO}\xrightarrow{p}{\mbox{$\kappa$}}^{\ast},\qquad{\mbox{$\kappa$}}^{\ast}=\arg\min\_{{\mbox{$\kappa$}}}\ {\mathbb{E}}\Big[\big\|{\mbox{$\Sigma$}}\_{uu}^{\ast-1/2}{\mbox{$A$}}^{\ast}(L)({\mbox{$y$}}\_{t}-{\mbox{$Z$}}\_{t}^{\prime}{\mbox{$\kappa$}})\big\|^{2}\Big], |  | (57) |

which in general satisfies ğœ¿âˆ—â‰ ğœ¿{\mbox{$\kappa$}}^{\ast}\neq{\mbox{$\kappa$}} (generic misspecification), except under knife-edge cancellations (e.g. no serial correlation ğš¿uâ€‹u,j=ğŸ{\mbox{$\Psi$}}\_{uu,j}={\mbox{$0$}} for all jj, or parameter restrictions making ğ’ƒ=ğŸ{\mbox{$b$}}={\mbox{$0$}} despite Gâ€‹Eâ€‹Xâ€‹Oâ€‹Gâˆ–Bâ€‹DGEXOG\setminus BD).
Hence, outside the Bâ€‹DBD class, and in particular under Gâ€‹Eâ€‹Xâ€‹Oâ€‹Gâˆ–Bâ€‹DGEXOG\setminus BD, consistency of the CO-type estimator is not guaranteed and generically fails.

Finally, since Gâ€‹Eâ€‹Xâ€‹Oâ€‹GâŠŠEâ€‹Bâ€‹DGEXOG\subsetneq EBD, the same generic failure can arise within Eâ€‹Bâ€‹Dâˆ–Bâ€‹DEBD\setminus BD by considering any DGP in Gâ€‹Eâ€‹Xâ€‹Oâ€‹Gâˆ–Bâ€‹DâŠ‚Eâ€‹Bâ€‹Dâˆ–Bâ€‹DGEXOG\setminus BD\subset EBD\setminus BD.
This completes the proof. âˆ

### A.6 Proof of PropositionÂ [6](https://arxiv.org/html/2601.21272v1#Thmproposition6 "Proposition 6 (Consistency region of the FGLS-D estimator): â€£ 2.4 The inconsistency of FGLS-D estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")

Throughout, fix p0p\_{0} and write Tâˆ—:=Tâˆ’p0T^{\ast}:=T-p\_{0}, implicitly restricting sums to
t=p0+1,â€¦,Tt=p\_{0}+1,\ldots,T (initial observations are asymptotically negligible).
Define

|  |  |  |
| --- | --- | --- |
|  | ğ‘¨â€‹(L):=ğ‘°Nâˆ’âˆ‘j=1p0ğš¿uâ€‹u,jâ€‹Lj,ğ‘¨â€‹(1):=ğ‘°Nâˆ’âˆ‘j=1p0ğš¿uâ€‹u,j.{\mbox{$A$}}(L):={\mbox{$I$}}\_{N}-\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{uu,j}L^{j},\qquad{\mbox{$A$}}(1):={\mbox{$I$}}\_{N}-\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{uu,j}. |  |

Define the infeasible filtered variables

|  |  |  |
| --- | --- | --- |
|  | ğ’šFD,t:=ğ‘¨â€‹(L)â€‹ğ’št,ğ‘¿FD,tâ€²:=ğ‘¿tâ€²âˆ’âˆ‘j=1p0ğš¿uâ€‹u,jâ€‹ğ‘¿tâˆ’jâ€²,{\mbox{$y$}}\_{\mathrm{FD},t}:={\mbox{$A$}}(L){\mbox{$y$}}\_{t},\qquad{\mbox{$X$}}\_{\mathrm{FD},t}^{\prime}:={\mbox{$X$}}\_{t}^{\prime}-\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{uu,j}{\mbox{$X$}}\_{t-j}^{\prime}, |  |

and

|  |  |  |
| --- | --- | --- |
|  | ğ’FD,tâ€²:=[ğ‘¨â€‹(1),ğ‘¿FD,tâ€²],ğœ¿:=(ğœ¶â€²,ğœ·â€²)â€².{\mbox{$Z$}}\_{\mathrm{FD},t}^{\prime}:=\bigl[\ {\mbox{$A$}}(1),\ \ {\mbox{$X$}}\_{\mathrm{FD},t}^{\prime}\ \bigr],\qquad{\mbox{$\kappa$}}:=({\mbox{$\alpha$}}^{\prime},{\mbox{$\beta$}}^{\prime})^{\prime}. |  |

(i) Consistency and asymptotic equivalence under Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG.

Assume the DGP satisfies Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG, i.e. ğš¿uâ€‹x,j=ğŸ{\mbox{$\Psi$}}\_{ux,j}={\mbox{$0$}} for all jj and
ğšºxâ€‹u=ğŸ{\mbox{$\Sigma$}}\_{xu}={\mbox{$0$}}. Then the disturbance law of motion is closed:

|  |  |  |
| --- | --- | --- |
|  | ğ’–t=âˆ‘j=1p0ğš¿uâ€‹u,jâ€‹ğ’–tâˆ’j+ğœºu,t,{\mbox{$u$}}\_{t}=\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{uu,j}{\mbox{$u$}}\_{t-j}+{\mbox{$\varepsilon$}}\_{u,t}, |  |

so ğ‘¨â€‹(L)â€‹ğ’–t=ğœºu,t{\mbox{$A$}}(L){\mbox{$u$}}\_{t}={\mbox{$\varepsilon$}}\_{u,t}. Applying ğ‘¨â€‹(L){\mbox{$A$}}(L) to
ğ’št=ğ’tâ€²â€‹ğœ¿+ğ’–t{\mbox{$y$}}\_{t}={\mbox{$Z$}}\_{t}^{\prime}{\mbox{$\kappa$}}+{\mbox{$u$}}\_{t} yields the correctly specified infeasible filtered regression

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ’šFD,t=ğ’FD,tâ€²â€‹ğœ¿+ğœºu,t.{\mbox{$y$}}\_{\mathrm{FD},t}={\mbox{$Z$}}\_{\mathrm{FD},t}^{\prime}{\mbox{$\kappa$}}+{\mbox{$\varepsilon$}}\_{u,t}. |  | (58) |

Let â„‹tâˆ’1:=spanÂ¯â€‹{ğ’›Â¯s:sâ‰¤tâˆ’1}\mathscr{H}\_{t-1}:=\overline{\mathrm{span}}\{\bar{{\mbox{$z$}}}\_{s}:\ s\leq t-1\} be the closed linear span
generated by the past of ğ’›Â¯t=((ğ’™tâˆ’ğx)â€²,ğ’–tâ€²)â€²\bar{{\mbox{$z$}}}\_{t}=(({\mbox{$x$}}\_{t}-{\mbox{$\mu$}}\_{x})^{\prime},{\mbox{$u$}}\_{t}^{\prime})^{\prime}.
By PropositionÂ [1](https://arxiv.org/html/2601.21272v1#Thmproposition1 "Proposition 1: â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")(i), the joint innovation
ğœºt=(ğœºx,tâ€²,ğœºu,tâ€²)â€²{\mbox{$\varepsilon$}}\_{t}=({\mbox{$\varepsilon$}}\_{x,t}^{\prime},{\mbox{$\varepsilon$}}\_{u,t}^{\prime})^{\prime}
satisfies ğœºtâŸ‚â„‹tâˆ’1{\mbox{$\varepsilon$}}\_{t}\perp\mathscr{H}\_{t-1} in L2L^{2}, and ğšºxâ€‹u=ğŸ{\mbox{$\Sigma$}}\_{xu}={\mbox{$0$}} implies
ğœºu,tâŸ‚ğœºx,t{\mbox{$\varepsilon$}}\_{u,t}\perp{\mbox{$\varepsilon$}}\_{x,t} in L2L^{2}. Under Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG, ğ’™Â¯t\bar{{\mbox{$x$}}}\_{t} is a linear
function of (â„‹tâˆ’1,ğœºx,t)(\mathscr{H}\_{t-1},{\mbox{$\varepsilon$}}\_{x,t}) and does not load on ğœºu,t{\mbox{$\varepsilon$}}\_{u,t}
contemporaneously. Hence ğ’FD,t{\mbox{$Z$}}\_{\mathrm{FD},t}â€”being a fixed linear function of
{ğ‘¿tâˆ’â„“}â„“=0p0\{{\mbox{$X$}}\_{t-\ell}\}\_{\ell=0}^{p\_{0}} and constantsâ€”is measurable with respect to the closed linear space
spanÂ¯â€‹(â„‹tâˆ’1âˆª{ğœºx,t})\overline{\mathrm{span}}(\mathscr{H}\_{t-1}\cup\{{\mbox{$\varepsilon$}}\_{x,t}\}), and therefore

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â€‹[ğ’FD,tâ€‹ğœºu,t]=ğŸ.{\mathbb{E}}[{\mbox{$Z$}}\_{\mathrm{FD},t}{\mbox{$\varepsilon$}}\_{u,t}]={\mbox{$0$}}. |  | (59) |

Consider the infeasible GLS estimator based on ([58](https://arxiv.org/html/2601.21272v1#Sx2.E58 "In A.6 Proof of Proposition 6 â€£ Appendix â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")):

|  |  |  |
| --- | --- | --- |
|  | ğœ¿^FDGLS:=(âˆ‘ğ’FD,tâ€‹ğšºuâ€‹uâˆ’1â€‹ğ’FD,tâ€²)âˆ’1â€‹(âˆ‘ğ’FD,tâ€‹ğšºuâ€‹uâˆ’1â€‹ğ’šFD,t).\widehat{{\mbox{$\kappa$}}}^{\,\mathrm{GLS}}\_{\mathrm{FD}}:=\Big(\sum{\mbox{$Z$}}\_{\mathrm{FD},t}{\mbox{$\Sigma$}}\_{uu}^{-1}{\mbox{$Z$}}\_{\mathrm{FD},t}^{\prime}\Big)^{-1}\Big(\sum{\mbox{$Z$}}\_{\mathrm{FD},t}{\mbox{$\Sigma$}}\_{uu}^{-1}{\mbox{$y$}}\_{\mathrm{FD},t}\Big). |  |

Using ([58](https://arxiv.org/html/2601.21272v1#Sx2.E58 "In A.6 Proof of Proposition 6 â€£ Appendix â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")),

|  |  |  |
| --- | --- | --- |
|  | ğœ¿^FDGLSâˆ’ğœ¿=(1Tâˆ—â€‹âˆ‘ğ’FD,tâ€‹ğšºuâ€‹uâˆ’1â€‹ğ’FD,tâ€²)âˆ’1â€‹(1Tâˆ—â€‹âˆ‘ğ’FD,tâ€‹ğšºuâ€‹uâˆ’1â€‹ğœºu,t).\widehat{{\mbox{$\kappa$}}}^{\,\mathrm{GLS}}\_{\mathrm{FD}}-{\mbox{$\kappa$}}=\Big(\frac{1}{T^{\ast}}\sum{\mbox{$Z$}}\_{\mathrm{FD},t}{\mbox{$\Sigma$}}\_{uu}^{-1}{\mbox{$Z$}}\_{\mathrm{FD},t}^{\prime}\Big)^{-1}\Big(\frac{1}{T^{\ast}}\sum{\mbox{$Z$}}\_{\mathrm{FD},t}{\mbox{$\Sigma$}}\_{uu}^{-1}{\mbox{$\varepsilon$}}\_{u,t}\Big). |  |

By PropositionÂ [1](https://arxiv.org/html/2601.21272v1#Thmproposition1 "Proposition 1: â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")(iv), {(ğ’št,ğ‘¿tâ€²,ğ’–t)}\{({\mbox{$y$}}\_{t},{\mbox{$X$}}\_{t}^{\prime},{\mbox{$u$}}\_{t})\} is strictly stationary,
strongly mixing with finite moments, and so is {(ğ’FD,t,ğœºu,t)}\{({\mbox{$Z$}}\_{\mathrm{FD},t},{\mbox{$\varepsilon$}}\_{u,t})\} as a fixed
linear transform of finitely many lags. Hence a LLN for strongly mixing sequences yields

|  |  |  |
| --- | --- | --- |
|  | 1Tâˆ—â€‹âˆ‘ğ’FD,tâ€‹ğšºuâ€‹uâˆ’1â€‹ğ’FD,tâ€²â†’ğ‘ğ”¼â€‹[ğ’FD,tâ€‹ğšºuâ€‹uâˆ’1â€‹ğ’FD,tâ€²]=ğ‘¸FD,1Tâˆ—â€‹âˆ‘ğ’FD,tâ€‹ğšºuâ€‹uâˆ’1â€‹ğœºu,tâ†’ğ‘ğŸ,\frac{1}{T^{\ast}}\sum{\mbox{$Z$}}\_{\mathrm{FD},t}{\mbox{$\Sigma$}}\_{uu}^{-1}{\mbox{$Z$}}\_{\mathrm{FD},t}^{\prime}\xrightarrow{p}{\mathbb{E}}[{\mbox{$Z$}}\_{\mathrm{FD},t}{\mbox{$\Sigma$}}\_{uu}^{-1}{\mbox{$Z$}}\_{\mathrm{FD},t}^{\prime}]={\mbox{$Q$}}\_{\mathrm{FD}},\qquad\frac{1}{T^{\ast}}\sum{\mbox{$Z$}}\_{\mathrm{FD},t}{\mbox{$\Sigma$}}\_{uu}^{-1}{\mbox{$\varepsilon$}}\_{u,t}\xrightarrow{p}{\mbox{$0$}}, |  |

where the second convergence uses ([59](https://arxiv.org/html/2601.21272v1#Sx2.E59 "In A.6 Proof of Proposition 6 â€£ Appendix â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")). Since ğ‘¸FDâ‰»0{\mbox{$Q$}}\_{\mathrm{FD}}\succ 0 by
AssumptionÂ [4](https://arxiv.org/html/2601.21272v1#Thmassumption4 "Assumption 4: â€£ 2.4 The inconsistency of FGLS-D estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"), it follows that
ğœ¿^FDGLSâ†’ğ‘ğœ¿\widehat{{\mbox{$\kappa$}}}^{\,\mathrm{GLS}}\_{\mathrm{FD}}\xrightarrow{p}{\mbox{$\kappa$}}.

Under Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG, StepÂ 1 is a correctly specified linear regression with error ğœºu,t{\mbox{$\varepsilon$}}\_{u,t}, and
its regressors are functions of {ğ’štâˆ’j,ğ’™tâˆ’j,ğ’™t}j=1p0\{{\mbox{$y$}}\_{t-j},{\mbox{$x$}}\_{t-j},{\mbox{$x$}}\_{t}\}\_{j=1}^{p\_{0}} and constants, which lie in
spanÂ¯â€‹(â„‹tâˆ’1âˆª{ğœºx,t})\overline{\mathrm{span}}(\mathscr{H}\_{t-1}\cup\{{\mbox{$\varepsilon$}}\_{x,t}\}).
Therefore the StepÂ 1 OLS estimators satisfy (by the same LLN/continuous-mapping arguments)

|  |  |  |
| --- | --- | --- |
|  | ğš¿^uâ€‹u,jâ†’ğ‘ğš¿uâ€‹u,j(j=1,â€¦,p0),ğšº^uâ€‹uâ†’ğ‘ğšºuâ€‹u,\widehat{{\mbox{$\Psi$}}}\_{uu,j}\xrightarrow{p}{\mbox{$\Psi$}}\_{uu,j}\quad(j=1,\ldots,p\_{0}),\qquad\widehat{{\mbox{$\Sigma$}}}\_{uu}\xrightarrow{p}{\mbox{$\Sigma$}}\_{uu}, |  |

and hence ğ‘¨^â€‹(L):=ğ‘°Nâˆ’âˆ‘j=1p0ğš¿^uâ€‹u,jâ€‹Lj\widehat{{\mbox{$A$}}}(L):={\mbox{$I$}}\_{N}-\sum\_{j=1}^{p\_{0}}\widehat{{\mbox{$\Psi$}}}\_{uu,j}L^{j} converges coefficientwise
to ğ‘¨â€‹(L){\mbox{$A$}}(L) and ğšº^uâ€‹uâˆ’1â†’ğ‘ğšºuâ€‹uâˆ’1\widehat{{\mbox{$\Sigma$}}}\_{uu}^{-1}\xrightarrow{p}{\mbox{$\Sigma$}}\_{uu}^{-1}.
Let ğ’š^FD,t\widehat{{\mbox{$y$}}}\_{\mathrm{FD},t} and ğ’^FD,t\widehat{{\mbox{$Z$}}}\_{\mathrm{FD},t} denote the feasible filtered series
constructed from (ğ‘¨^â€‹(L),ğšº^uâ€‹u)(\widehat{{\mbox{$A$}}}(L),\widehat{{\mbox{$\Sigma$}}}\_{uu}). Then, by ergodicity and finite moments,

|  |  |  |
| --- | --- | --- |
|  | 1Tâˆ—â€‹âˆ‘ğ’^FD,tâ€‹ğšº^uâ€‹uâˆ’1â€‹ğ’^FD,tâ€²=1Tâˆ—â€‹âˆ‘ğ’FD,tâ€‹ğšºuâ€‹uâˆ’1â€‹ğ’FD,tâ€²+opâ€‹(1),1Tâˆ—â€‹âˆ‘ğ’^FD,tâ€‹ğšº^uâ€‹uâˆ’1â€‹ğ’š^FD,t=1Tâˆ—â€‹âˆ‘ğ’FD,tâ€‹ğšºuâ€‹uâˆ’1â€‹ğ’šFD,t+opâ€‹(1).\frac{1}{T^{\ast}}\sum\widehat{{\mbox{$Z$}}}\_{\mathrm{FD},t}\widehat{{\mbox{$\Sigma$}}}\_{uu}^{-1}\widehat{{\mbox{$Z$}}}\_{\mathrm{FD},t}^{\prime}=\frac{1}{T^{\ast}}\sum{\mbox{$Z$}}\_{\mathrm{FD},t}{\mbox{$\Sigma$}}\_{uu}^{-1}{\mbox{$Z$}}\_{\mathrm{FD},t}^{\prime}+o\_{p}(1),\qquad\frac{1}{T^{\ast}}\sum\widehat{{\mbox{$Z$}}}\_{\mathrm{FD},t}\widehat{{\mbox{$\Sigma$}}}\_{uu}^{-1}\widehat{{\mbox{$y$}}}\_{\mathrm{FD},t}=\frac{1}{T^{\ast}}\sum{\mbox{$Z$}}\_{\mathrm{FD},t}{\mbox{$\Sigma$}}\_{uu}^{-1}{\mbox{$y$}}\_{\mathrm{FD},t}+o\_{p}(1). |  |

Since ğ‘¸FDâ‰»0{\mbox{$Q$}}\_{\mathrm{FD}}\succ 0 (AssumptionÂ [4](https://arxiv.org/html/2601.21272v1#Thmassumption4 "Assumption 4: â€£ 2.4 The inconsistency of FGLS-D estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")), the map (M,b)â†¦Mâˆ’1â€‹b(M,b)\mapsto M^{-1}b is continuous
at (ğ‘¸FD,ğ”¼â€‹[ğ’FD,tâ€‹ğšºuâ€‹uâˆ’1â€‹ğ’šFD,t])({\mbox{$Q$}}\_{\mathrm{FD}},{\mathbb{E}}[{\mbox{$Z$}}\_{\mathrm{FD},t}{\mbox{$\Sigma$}}\_{uu}^{-1}{\mbox{$y$}}\_{\mathrm{FD},t}]), so

|  |  |  |
| --- | --- | --- |
|  | ğœ¿^FGLSâˆ’Dâˆ’ğœ¿^FDGLS=opâ€‹(1),ğœ¿^FGLSâˆ’Dâ†’ğ‘ğœ¿.\widehat{{\mbox{$\kappa$}}}^{\mathrm{FGLS\!-\!D}}-\widehat{{\mbox{$\kappa$}}}^{\,\mathrm{GLS}}\_{\mathrm{FD}}=o\_{p}(1),\qquad\widehat{{\mbox{$\kappa$}}}^{\mathrm{FGLS\!-\!D}}\xrightarrow{p}{\mbox{$\kappa$}}. |  |

Under standard regularity conditions ensuring T\sqrt{T}-consistency of
(ğš¿^uâ€‹u,â‹…,ğšº^uâ€‹u)(\widehat{{\mbox{$\Psi$}}}\_{uu,\cdot},\widehat{{\mbox{$\Sigma$}}}\_{uu}), the same argument strengthens to
Tâ€‹(ğœ¿^FGLSâˆ’Dâˆ’ğœ¿^FDGLS)=opâ€‹(1)\sqrt{T}\big(\widehat{{\mbox{$\kappa$}}}^{\mathrm{FGLS\!-\!D}}-\widehat{{\mbox{$\kappa$}}}^{\,\mathrm{GLS}}\_{\mathrm{FD}}\big)=o\_{p}(1),
so the feasible estimator inherits the limiting distribution of its infeasible GLS counterpart.
Under Bâ€‹DBD, the Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG restriction holds trivially, so the same conclusion applies.

(ii) Generic inconsistency on Eâ€‹Bâ€‹Dâˆ–Gâ€‹Eâ€‹Xâ€‹Oâ€‹GEBD\setminus GEXOG.

Now assume the DGP lies in Eâ€‹Bâ€‹Dâˆ–Gâ€‹Eâ€‹Xâ€‹Oâ€‹GEBD\setminus GEXOG, i.e. ğšºxâ€‹u=ğŸ{\mbox{$\Sigma$}}\_{xu}={\mbox{$0$}} but
ğš¿uâ€‹x,jâ‰ ğŸ{\mbox{$\Psi$}}\_{ux,j}\neq{\mbox{$0$}} for some jj. Then the true disturbance dynamics is

|  |  |  |
| --- | --- | --- |
|  | ğ’–t=âˆ‘j=1p0ğš¿uâ€‹u,jâ€‹ğ’–tâˆ’j+âˆ‘j=1p0ğš¿uâ€‹x,jâ€‹(ğ’™tâˆ’jâˆ’ğx)+ğœºu,t.{\mbox{$u$}}\_{t}=\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{uu,j}{\mbox{$u$}}\_{t-j}+\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{ux,j}({\mbox{$x$}}\_{t-j}-{\mbox{$\mu$}}\_{x})+{\mbox{$\varepsilon$}}\_{u,t}. |  |

Applying ğ‘¨â€‹(L){\mbox{$A$}}(L) to ğ’št=ğ’tâ€²â€‹ğœ¿+ğ’–t{\mbox{$y$}}\_{t}={\mbox{$Z$}}\_{t}^{\prime}{\mbox{$\kappa$}}+{\mbox{$u$}}\_{t} yields

|  |  |  |
| --- | --- | --- |
|  | ğ’šFD,t=ğ’FD,tâ€²ğœ¿+âˆ‘j=1p0ğš¿uâ€‹x,j(ğ’™tâˆ’jâˆ’ğx)+ğœºu,t=:ğ’FD,tâ€²ğœ¿+ğœº~u,t,{\mbox{$y$}}\_{\mathrm{FD},t}={\mbox{$Z$}}\_{\mathrm{FD},t}^{\prime}{\mbox{$\kappa$}}+\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{ux,j}({\mbox{$x$}}\_{t-j}-{\mbox{$\mu$}}\_{x})+{\mbox{$\varepsilon$}}\_{u,t}=:{\mbox{$Z$}}\_{\mathrm{FD},t}^{\prime}{\mbox{$\kappa$}}+\tilde{{\mbox{$\varepsilon$}}}\_{u,t}, |  |

where

|  |  |  |
| --- | --- | --- |
|  | ğœº~u,t:=ğœºu,t+âˆ‘j=1p0ğš¿uâ€‹x,jâ€‹(ğ’™tâˆ’jâˆ’ğx).\tilde{{\mbox{$\varepsilon$}}}\_{u,t}:={\mbox{$\varepsilon$}}\_{u,t}+\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{ux,j}({\mbox{$x$}}\_{t-j}-{\mbox{$\mu$}}\_{x}). |  |

As before, ğ”¼â€‹[ğ’FD,tâ€‹ğœºu,t]=ğŸ{\mathbb{E}}[{\mbox{$Z$}}\_{\mathrm{FD},t}{\mbox{$\varepsilon$}}\_{u,t}]={\mbox{$0$}} holds (innovation orthogonality and
ğšºxâ€‹u=ğŸ{\mbox{$\Sigma$}}\_{xu}={\mbox{$0$}}). However,

|  |  |  |
| --- | --- | --- |
|  | ğ”¼[ğ’FD,tğœº~u,t]=âˆ‘j=1p0ğš¿uâ€‹x,jğ”¼[ğ’FD,t(ğ’™tâˆ’jâˆ’ğx)].{\mathbb{E}}[{\mbox{$Z$}}\_{\mathrm{FD},t}\tilde{{\mbox{$\varepsilon$}}}\_{u,t}]=\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{ux,j}\ {\mathbb{E}}\!\mathopen{}\left[{\mbox{$Z$}}\_{\mathrm{FD},t}({\mbox{$x$}}\_{t-j}-{\mbox{$\mu$}}\_{x})\mathclose{}\right]. |  |

The constant block ğ‘¨â€‹(1){\mbox{$A$}}(1) contributes ğ‘¨â€‹(1)â€‹ğ”¼â€‹[ğ’™tâˆ’jâˆ’ğx]=ğŸ{\mbox{$A$}}(1){\mathbb{E}}[{\mbox{$x$}}\_{t-j}-{\mbox{$\mu$}}\_{x}]={\mbox{$0$}}, but the regressor block
ğ‘¿FD,tâ€²{\mbox{$X$}}\_{\mathrm{FD},t}^{\prime} is a nontrivial linear function of ğ’™t,ğ’™tâˆ’1,â€¦,ğ’™tâˆ’p0{\mbox{$x$}}\_{t},{\mbox{$x$}}\_{t-1},\ldots,{\mbox{$x$}}\_{t-p\_{0}}.
In particular, for any jâˆˆ{1,â€¦,p0}j\in\{1,\ldots,p\_{0}\},

|  |  |  |
| --- | --- | --- |
|  | ğ”¼[ğ‘¿FD,tâ€²(ğ’™tâˆ’jâˆ’ğx)]=ğ”¼[ğ‘¿tâ€²(ğ’™tâˆ’jâˆ’ğx)]âˆ’âˆ‘â„“=1p0ğš¿uâ€‹u,â„“ğ”¼[ğ‘¿tâˆ’â„“â€²(ğ’™tâˆ’jâˆ’ğx)],{\mathbb{E}}\!\mathopen{}\left[{\mbox{$X$}}\_{\mathrm{FD},t}^{\prime}({\mbox{$x$}}\_{t-j}-{\mbox{$\mu$}}\_{x})\mathclose{}\right]={\mathbb{E}}\!\mathopen{}\left[{\mbox{$X$}}\_{t}^{\prime}({\mbox{$x$}}\_{t-j}-{\mbox{$\mu$}}\_{x})\mathclose{}\right]-\sum\_{\ell=1}^{p\_{0}}{\mbox{$\Psi$}}\_{uu,\ell}{\mathbb{E}}\!\mathopen{}\left[{\mbox{$X$}}\_{t-\ell}^{\prime}({\mbox{$x$}}\_{t-j}-{\mbox{$\mu$}}\_{x})\mathclose{}\right], |  |

which is generically nonzero under AssumptionÂ [1](https://arxiv.org/html/2601.21272v1#Thmassumption1 "Assumption 1 (Finite-predictor exactness at lag ğ‘â‚€): â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") and AssumptionÂ [2](https://arxiv.org/html/2601.21272v1#Thmassumption2 "Assumption 2: â€£ 2.3 The inconsistency of OLS and OLS-based GLS estimators in multi-equation models â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")
(except under knife-edge cancellations, e.g. degenerate regressor dynamics or special parameter values).
Since ğš¿uâ€‹x,â‹…â‰ ğŸ{\mbox{$\Psi$}}\_{ux,\cdot}\neq{\mbox{$0$}} in Eâ€‹Bâ€‹Dâˆ–Gâ€‹Eâ€‹Xâ€‹Oâ€‹GEBD\setminus GEXOG, we obtain

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[ğ’FD,tâ€‹ğœº~u,t]â‰ ğŸ(generic).{\mathbb{E}}[{\mbox{$Z$}}\_{\mathrm{FD},t}\tilde{{\mbox{$\varepsilon$}}}\_{u,t}]\neq{\mbox{$0$}}\qquad\text{(generic)}. |  |

Therefore the population normal equations for the filtered regression are violated and the probability limit
of even the infeasible GLS based on (ğ‘¨â€‹(L),ğšºuâ€‹u)({\mbox{$A$}}(L),{\mbox{$\Sigma$}}\_{uu}) equals

|  |  |  |
| --- | --- | --- |
|  | ğœ¿âˆ—=ğœ¿+(ğ”¼â€‹[ğ’FD,tâ€‹ğšºuâ€‹uâˆ’1â€‹ğ’FD,tâ€²])âˆ’1â€‹ğ”¼â€‹[ğ’FD,tâ€‹ğšºuâ€‹uâˆ’1â€‹ğœº~u,t]â‰ ğœ¿(generic).{\mbox{$\kappa$}}^{\ast}={\mbox{$\kappa$}}+\Big({\mathbb{E}}[{\mbox{$Z$}}\_{\mathrm{FD},t}{\mbox{$\Sigma$}}\_{uu}^{-1}{\mbox{$Z$}}\_{\mathrm{FD},t}^{\prime}]\Big)^{-1}{\mathbb{E}}[{\mbox{$Z$}}\_{\mathrm{FD},t}{\mbox{$\Sigma$}}\_{uu}^{-1}\tilde{{\mbox{$\varepsilon$}}}\_{u,t}]\neq{\mbox{$\kappa$}}\qquad\text{(generic)}. |  |

Since the feasible FGLS-D estimator is obtained by replacing (ğ‘¨â€‹(L),ğšºuâ€‹u)({\mbox{$A$}}(L),{\mbox{$\Sigma$}}\_{uu}) with estimators
constructed under the maintained Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG-type disturbance specification, it converges to the same pseudo-true
limit and cannot eliminate this misspecification bias. Hence ğœ¿^FGLSâˆ’D\widehat{{\mbox{$\kappa$}}}^{\mathrm{FGLS\!-\!D}} is
generically inconsistent on Eâ€‹Bâ€‹Dâˆ–Gâ€‹Eâ€‹Xâ€‹Oâ€‹GEBD\setminus GEXOG. âˆ

### A.7 Proof of Lemma1

Fix iâˆˆ{1,â€¦,N}i\in\{1,\ldots,N\}. Recall â„±tâˆ’1:=Ïƒ(ğ’›s:sâ‰¤tâˆ’1)\mathscr{F}\_{t-1}:=\sigma({\mbox{$z$}}\_{s}:\,s\leq t-1) and ğ’›t=(ğ’™tâ€²,ğ’–tâ€²)â€²{\mbox{$z$}}\_{t}=({\mbox{$x$}}\_{t}^{\prime},{\mbox{$u$}}\_{t}^{\prime})^{\prime}.

(i)
By PropositionÂ [1](https://arxiv.org/html/2601.21272v1#Thmproposition1 "Proposition 1: â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")(ii)â€“(iii) under AssumptionÂ [1](https://arxiv.org/html/2601.21272v1#Thmassumption1 "Assumption 1 (Finite-predictor exactness at lag ğ‘â‚€): â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"), ğ’›Â¯t=((ğ’™tâˆ’ğx)â€²,ğ’–tâ€²)â€²\bar{{\mbox{$z$}}}\_{t}=(({\mbox{$x$}}\_{t}-{\mbox{$\mu$}}\_{x})^{\prime},{\mbox{$u$}}\_{t}^{\prime})^{\prime} admits the stable VAR(p0p\_{0}) representation, and in particular the xx-block satisfies

|  |  |  |
| --- | --- | --- |
|  | ğ’™t=ğx+âˆ‘j=1p0ğš¿xâ€‹x,jâ€‹(ğ’™tâˆ’jâˆ’ğx)+âˆ‘j=1p0ğš¿xâ€‹u,jâ€‹ğ’–tâˆ’j+ğœºx,t.{\mbox{$x$}}\_{t}={\mbox{$\mu$}}\_{x}+\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{xx,j}({\mbox{$x$}}\_{t-j}-{\mbox{$\mu$}}\_{x})+\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{xu,j}{\mbox{$u$}}\_{t-j}+{\mbox{$\varepsilon$}}\_{x,t}. |  |

Let ğ‘±iâˆˆâ„kiÃ—k{\mbox{$J$}}\_{i}\in\mathbb{R}^{k\_{i}\times k} be the selection matrix extracting the iith block
(ğ‘±iâ€‹ğ’™t=ğ’™i,t{\mbox{$J$}}\_{i}{\mbox{$x$}}\_{t}={\mbox{$x$}}\_{i,t}). Define the â„±tâˆ’1\mathscr{F}\_{t-1}-measurable vector

|  |  |  |
| --- | --- | --- |
|  | ğ’‰x,i,t:=ğ‘±iâ€‹ğx+âˆ‘j=1p0ğ‘±iâ€‹ğš¿xâ€‹x,jâ€‹(ğ’™tâˆ’jâˆ’ğx)+âˆ‘j=1p0ğ‘±iâ€‹ğš¿xâ€‹u,jâ€‹ğ’–tâˆ’j.{\mbox{$h$}}\_{x,i,t}:={\mbox{$J$}}\_{i}{\mbox{$\mu$}}\_{x}+\sum\_{j=1}^{p\_{0}}{\mbox{$J$}}\_{i}{\mbox{$\Psi$}}\_{xx,j}({\mbox{$x$}}\_{t-j}-{\mbox{$\mu$}}\_{x})+\sum\_{j=1}^{p\_{0}}{\mbox{$J$}}\_{i}{\mbox{$\Psi$}}\_{xu,j}{\mbox{$u$}}\_{t-j}. |  |

Since ğ’™tâˆ’j{\mbox{$x$}}\_{t-j} and ğ’–tâˆ’j{\mbox{$u$}}\_{t-j} (jâ‰¥1j\geq 1) are â„±tâˆ’1\mathscr{F}\_{t-1}-measurable, we have ğ’‰x,i,t{\mbox{$h$}}\_{x,i,t} â„±tâˆ’1\mathscr{F}\_{t-1}-measurable and

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ’™i,t=ğ’‰x,i,t+ğ‘±iâ€‹ğœºx,t.{\mbox{$x$}}\_{i,t}={\mbox{$h$}}\_{x,i,t}+{\mbox{$J$}}\_{i}{\mbox{$\varepsilon$}}\_{x,t}. |  | (60) |

Next note that, by construction, ğ’štâˆ’j{\mbox{$y$}}\_{t-j} and ğ’™tâˆ’j{\mbox{$x$}}\_{t-j} (jâ‰¥1j\geq 1) are â„±tâˆ’1\mathscr{F}\_{t-1}-measurable. Moreover 11 is deterministic. Hence the equation-specific regressor vector

|  |  |  |
| --- | --- | --- |
|  | ğ’˜i,t=[â€‰1,ğ’™i,tâ€²,ğ’štâˆ’1â€²,â€¦,ğ’štâˆ’p0â€²,ğ’™tâˆ’1â€²,â€¦,ğ’™tâˆ’p0â€²]â€²{\mbox{$w$}}\_{i,t}=\bigl[\,1,\ {\mbox{$x$}}\_{i,t}^{\prime},\ {\mbox{$y$}}\_{t-1}^{\prime},\ldots,{\mbox{$y$}}\_{t-p\_{0}}^{\prime},\ {\mbox{$x$}}\_{t-1}^{\prime},\ldots,{\mbox{$x$}}\_{t-p\_{0}}^{\prime}\,\bigr]^{\prime} |  |

can be written, using ([60](https://arxiv.org/html/2601.21272v1#Sx2.E60 "In A.7 Proof of Lemma1 â€£ Appendix â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")), as

|  |  |  |
| --- | --- | --- |
|  | ğ’˜i,t=[â€‰1,ğ’‰x,i,tâ€²,ğ’štâˆ’1â€²,â€¦,ğ’štâˆ’p0â€²,ğ’™tâˆ’1â€²,â€¦,ğ’™tâˆ’p0â€²]â€²âŸ=â£:ğ’‰i,t+[ğŸğ‘±iğŸâ‹®ğŸ]âŸ=â£:ğ’iâ€‹ğœºx,t.{\mbox{$w$}}\_{i,t}=\underbrace{\bigl[\,1,\ {\mbox{$h$}}\_{x,i,t}^{\prime},\ {\mbox{$y$}}\_{t-1}^{\prime},\ldots,{\mbox{$y$}}\_{t-p\_{0}}^{\prime},\ {\mbox{$x$}}\_{t-1}^{\prime},\ldots,{\mbox{$x$}}\_{t-p\_{0}}^{\prime}\,\bigr]^{\prime}}\_{=:~{\mbox{$h$}}\_{i,t}}\;+\;\underbrace{\begin{bmatrix}{\mbox{$0$}}\\ {\mbox{$J$}}\_{i}\\ {\mbox{$0$}}\\ \vdots\\ {\mbox{$0$}}\end{bmatrix}}\_{=:~{\mbox{$m$}}\_{i}}{\mbox{$\varepsilon$}}\_{x,t}. |  |

Clearly, ğ’‰i,t{\mbox{$h$}}\_{i,t} is â„±tâˆ’1\mathscr{F}\_{t-1}-measurable and ğ’i{\mbox{$m$}}\_{i} is deterministic, so this yields the desired decomposition

|  |  |  |
| --- | --- | --- |
|  | ğ’˜i,t=ğ’‰i,t+ğ’iâ€‹ğœºx,t,{\mbox{$w$}}\_{i,t}={\mbox{$h$}}\_{i,t}+{\mbox{$m$}}\_{i}{\mbox{$\varepsilon$}}\_{x,t}, |  |

and in particular ğ’˜i,t{\mbox{$w$}}\_{i,t} is measurable with respect to Ïƒâ€‹(â„±tâˆ’1,ğœºx,t)\sigma(\mathscr{F}\_{t-1},{\mbox{$\varepsilon$}}\_{x,t}).

Finally, since ğœºt{\mbox{$\varepsilon$}}\_{t} is the innovation sequence in the Wold/VAR representation, we have ğ”¼â€‹[ğœºu,tâˆ£â„±tâˆ’1]=ğŸ{\mathbb{E}}[{\mbox{$\varepsilon$}}\_{u,t}\mid\mathscr{F}\_{t-1}]={\mbox{$0$}} and thus ğ”¼â€‹[Îµu,i,tâˆ£â„±tâˆ’1]=0{\mathbb{E}}[\varepsilon\_{u,i,t}\mid\mathscr{F}\_{t-1}]=0. Therefore,

|  |  |  |
| --- | --- | --- |
|  | ğ”¼[ğ’‰i,tÎµu,i,t]=ğ”¼[ğ’‰i,tğ”¼[Îµu,i,tâˆ£â„±tâˆ’1]]=ğŸ.{\mathbb{E}}[{\mbox{$h$}}\_{i,t}\,\varepsilon\_{u,i,t}]={\mathbb{E}}\!\mathopen{}\left[\ {\mbox{$h$}}\_{i,t}\,{\mathbb{E}}[\varepsilon\_{u,i,t}\mid\mathscr{F}\_{t-1}]\ \mathclose{}\right]={\mbox{$0$}}. |  |

Moreover, if ğšºxâ€‹u=ğ”¼â€‹[ğœºx,tâ€‹ğœºu,tâ€²]=ğŸ{\mbox{$\Sigma$}}\_{xu}={\mathbb{E}}[{\mbox{$\varepsilon$}}\_{x,t}{\mbox{$\varepsilon$}}\_{u,t}^{\prime}]={\mbox{$0$}}, then
ğ”¼â€‹[ğœºx,tâ€‹Îµu,i,t]=ğŸ{\mathbb{E}}[{\mbox{$\varepsilon$}}\_{x,t}\varepsilon\_{u,i,t}]={\mbox{$0$}} and hence

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[ğ’iâ€‹ğœºx,tâ€‹Îµu,i,t]=ğ’iâ€‹ğ”¼â€‹[ğœºx,tâ€‹Îµu,i,t]=ğŸ.{\mathbb{E}}[{\mbox{$m$}}\_{i}{\mbox{$\varepsilon$}}\_{x,t}\,\varepsilon\_{u,i,t}]={\mbox{$m$}}\_{i}\,{\mathbb{E}}[{\mbox{$\varepsilon$}}\_{x,t}\varepsilon\_{u,i,t}]={\mbox{$0$}}. |  |

Combining the two displays gives ğ”¼â€‹[ğ’˜i,tâ€‹Îµu,i,t]=ğŸ{\mathbb{E}}[{\mbox{$w$}}\_{i,t}\,\varepsilon\_{u,i,t}]={\mbox{$0$}}.

(ii)
By PropositionÂ [1](https://arxiv.org/html/2601.21272v1#Thmproposition1 "Proposition 1: â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")(iv) (finite (4+2â€‹Î´)(4+2\delta)-moments of ğ’›t{\mbox{$z$}}\_{t}), ğ”¼â€‹â€–ğ’™tâ€–4+2â€‹Î´<âˆ{\mathbb{E}}\|{\mbox{$x$}}\_{t}\|^{4+2\delta}<\infty and ğ”¼â€‹â€–ğ’štâ€–4+2â€‹Î´<âˆ{\mathbb{E}}\|{\mbox{$y$}}\_{t}\|^{4+2\delta}<\infty, and also ğ”¼â€‹â€–ğœºu,tâ€–4+2â€‹Î´<âˆ{\mathbb{E}}\|{\mbox{$\varepsilon$}}\_{u,t}\|^{4+2\delta}<\infty. Since ğ’˜i,t{\mbox{$w$}}\_{i,t} is a finite stacking of the blocks 11, ğ’™i,t{\mbox{$x$}}\_{i,t}, {ğ’štâˆ’j}j=1p0\{{\mbox{$y$}}\_{t-j}\}\_{j=1}^{p\_{0}}, and {ğ’™tâˆ’j}j=1p0\{{\mbox{$x$}}\_{t-j}\}\_{j=1}^{p\_{0}},
there exists a constant Ci>0C\_{i}>0 such that

|  |  |  |
| --- | --- | --- |
|  | â€–ğ’˜i,tâ€–â‰¤Ciâ€‹(1+â€–ğ’™i,tâ€–+âˆ‘j=1p0â€–ğ’štâˆ’jâ€–+âˆ‘j=1p0â€–ğ’™tâˆ’jâ€–).\|{\mbox{$w$}}\_{i,t}\|\leq C\_{i}\Bigl(1+\|{\mbox{$x$}}\_{i,t}\|+\sum\_{j=1}^{p\_{0}}\|{\mbox{$y$}}\_{t-j}\|+\sum\_{j=1}^{p\_{0}}\|{\mbox{$x$}}\_{t-j}\|\Bigr). |  |

Hence ğ”¼â€‹â€–ğ’˜i,tâ€–2+Î´<âˆ{\mathbb{E}}\|{\mbox{$w$}}\_{i,t}\|^{2+\delta}<\infty. Moreover, using â€–ğ’˜i,tâ€‹Îµu,i,tâ€–â‰¤â€–ğ’˜i,tâ€–â€‹|Îµu,i,t|\|{\mbox{$w$}}\_{i,t}\varepsilon\_{u,i,t}\|\leq\|{\mbox{$w$}}\_{i,t}\|\,|\varepsilon\_{u,i,t}| and HÃ¶lderâ€™s inequality,

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹â€–ğ’˜i,tâ€‹Îµu,i,tâ€–2+Î´â‰¤(ğ”¼â€‹â€–ğ’˜i,tâ€–4+2â€‹Î´)1/2â€‹(ğ”¼â€‹|Îµu,i,t|4+2â€‹Î´)1/2<âˆ.{\mathbb{E}}\|{\mbox{$w$}}\_{i,t}\varepsilon\_{u,i,t}\|^{2+\delta}\leq\Big({\mathbb{E}}\|{\mbox{$w$}}\_{i,t}\|^{4+2\delta}\Big)^{1/2}\Big({\mathbb{E}}|\varepsilon\_{u,i,t}|^{4+2\delta}\Big)^{1/2}<\infty. |  |

(iii)
Let ğ‘¸w,i:=ğ”¼â€‹[ğ’˜i,tâ€‹ğ’˜i,tâ€²]{\mbox{$Q$}}\_{w,i}:={\mathbb{E}}[{\mbox{$w$}}\_{i,t}{\mbox{$w$}}\_{i,t}^{\prime}]. For any ğ’‚âˆˆâ„di{\mbox{$a$}}\in\mathbb{R}^{d\_{i}}, ğ’‚â‰ ğŸ{\mbox{$a$}}\neq{\mbox{$0$}},

|  |  |  |
| --- | --- | --- |
|  | ğ’‚â€²â€‹ğ‘¸w,iâ€‹ğ’‚=ğ”¼â€‹[(ğ’‚â€²â€‹ğ’˜i,t)2]â‰¥0.{\mbox{$a$}}^{\prime}{\mbox{$Q$}}\_{w,i}{\mbox{$a$}}={\mathbb{E}}[({\mbox{$a$}}^{\prime}{\mbox{$w$}}\_{i,t})^{2}]\geq 0. |  |

If ğ’‚â€²â€‹ğ‘¸w,iâ€‹ğ’‚=0{\mbox{$a$}}^{\prime}{\mbox{$Q$}}\_{w,i}{\mbox{$a$}}=0, then ğ’‚â€²â€‹ğ’˜i,t=0{\mbox{$a$}}^{\prime}{\mbox{$w$}}\_{i,t}=0 a.s., which implies an exact linear dependence among the regressors in the iith equation. This is ruled out by AssumptionÂ [2](https://arxiv.org/html/2601.21272v1#Thmassumption2 "Assumption 2: â€£ 2.3 The inconsistency of OLS and OLS-based GLS estimators in multi-equation models â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")(A2.1) (full-rank/identification for the stacked regressor system) together with the definition of ğ’˜i,t{\mbox{$w$}}\_{i,t} as the ii-specific subvector of the stacked regressor ğ’t{\mbox{$Z$}}\_{t} augmented by finitely many lags. Hence ğ‘¸w,i{\mbox{$Q$}}\_{w,i} is positive definite. âˆ

### A.8 Proof of Lemma2

Let p0p\_{0} be the minimal lag order in AssumptionÂ [1](https://arxiv.org/html/2601.21272v1#Thmassumption1 "Assumption 1 (Finite-predictor exactness at lag ğ‘â‚€): â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") (finite-predictor exactness). Fix a candidate set ğ’«={1,â€¦,pmax}\mathcal{P}=\{1,\ldots,p\_{\max}\} with pmaxâ‰¥p0p\_{\max}\geq p\_{0}.

For each pâˆˆğ’«p\in\mathcal{P}, define the equation-by-equation augmented regression
(obtained from ([25](https://arxiv.org/html/2601.21272v1#S2.E25 "In 2.5 Asymptotic properties of generalized Durbin estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) by replacing p0p\_{0} with pp):

|  |  |  |
| --- | --- | --- |
|  | yi,t=ğ’˜i,tâ€‹(p)â€²â€‹ğœ½iâ€‹(p)+Îµu,i,t,t=p+1,â€¦,T,i=1,â€¦,N,y\_{i,t}={\mbox{$w$}}\_{i,t}(p)^{\prime}{\mbox{$\theta$}}\_{i}(p)+\varepsilon\_{u,i,t},\qquad t=p+1,\ldots,T,\quad i=1,\ldots,N, |  |

where ğ’˜i,tâ€‹(p){\mbox{$w$}}\_{i,t}(p) stacks the contemporaneous regressors in equation ii and the lagged vectors {ğ’štâˆ’j}j=1p\{{\mbox{$y$}}\_{t-j}\}\_{j=1}^{p} and {ğ’™tâˆ’j}j=1p\{{\mbox{$x$}}\_{t-j}\}\_{j=1}^{p}. Let ğœ½^iâ€‹(p)\widehat{{\mbox{$\theta$}}}\_{i}(p) be the OLS estimator and define residuals

|  |  |  |
| --- | --- | --- |
|  | Îµ^u,i,tâ€‹(p):=yi,tâˆ’ğ’˜i,tâ€‹(p)â€²â€‹ğœ½^iâ€‹(p),ğœº^u,tâ€‹(p):=(Îµ^u,1,tâ€‹(p),â€¦,Îµ^u,N,tâ€‹(p))â€².\widehat{\varepsilon}\_{u,i,t}(p):=y\_{i,t}-{\mbox{$w$}}\_{i,t}(p)^{\prime}\widehat{{\mbox{$\theta$}}}\_{i}(p),\qquad\widehat{{\mbox{$\varepsilon$}}}\_{u,t}(p):=\big(\widehat{\varepsilon}\_{u,1,t}(p),\ldots,\widehat{\varepsilon}\_{u,N,t}(p)\big)^{\prime}. |  |

Under AssumptionsÂ [1](https://arxiv.org/html/2601.21272v1#Thmassumption1 "Assumption 1 (Finite-predictor exactness at lag ğ‘â‚€): â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")â€“[2](https://arxiv.org/html/2601.21272v1#Thmassumption2 "Assumption 2: â€£ 2.3 The inconsistency of OLS and OLS-based GLS estimators in multi-equation models â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") and Eâ€‹Bâ€‹DEBD, LemmaÂ [1](https://arxiv.org/html/2601.21272v1#Thmlemma1 "Lemma 1 (Equation-by-equation properties of the Durbin regression): â€£ 2.5 Asymptotic properties of generalized Durbin estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") applied with pp lags implies that, for each fixed pp and each ii, {ğ’˜i,tâ€‹(p)}\{{\mbox{$w$}}\_{i,t}(p)\} has finite (2+Î´)(2+\delta)-moments and is strictly stationary and strongly mixing, and

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[ğ’˜i,tâ€‹(p)â€‹Îµu,i,t]=0,ğ‘¸w,iâ€‹(p):=ğ”¼â€‹[ğ’˜i,tâ€‹(p)â€‹ğ’˜i,tâ€‹(p)â€²]>0.{\mathbb{E}}\!\big[{\mbox{$w$}}\_{i,t}(p)\,\varepsilon\_{u,i,t}\big]=0,\qquad{\mbox{$Q$}}\_{w,i}(p):={\mathbb{E}}[{\mbox{$w$}}\_{i,t}(p){\mbox{$w$}}\_{i,t}(p)^{\prime}]>0. |  |

Hence the usual OLS consistency argument for strongly mixing regressors yields

|  |  |  |
| --- | --- | --- |
|  | ğœ½^iâ€‹(p)â†’ğ‘ğœ½iâˆ—â€‹(p),ğœ½iâˆ—â€‹(p):=argâ¡minğœ½â¡ğ”¼â€‹(yi,tâˆ’ğ’˜i,tâ€‹(p)â€²â€‹ğœ½)2,\widehat{{\mbox{$\theta$}}}\_{i}(p)\xrightarrow{p}{\mbox{$\theta$}}\_{i}^{\ast}(p),\qquad{\mbox{$\theta$}}\_{i}^{\ast}(p):=\arg\min\_{{\mbox{$\theta$}}}{\mathbb{E}}\!\big(y\_{i,t}-{\mbox{$w$}}\_{i,t}(p)^{\prime}{\mbox{$\theta$}}\big)^{2}, |  |

for every pâˆˆğ’«p\in\mathcal{P} and i=1,â€¦,Ni=1,\ldots,N.

Define the pseudo-true residual vector

|  |  |  |
| --- | --- | --- |
|  | ğ’†tâˆ—â€‹(p):=(e1,tâˆ—â€‹(p),â€¦,eN,tâˆ—â€‹(p))â€²,ei,tâˆ—â€‹(p):=yi,tâˆ’ğ’˜i,tâ€‹(p)â€²â€‹ğœ½iâˆ—â€‹(p),{\mbox{$e$}}\_{t}^{\ast}(p):=\big(e\_{1,t}^{\ast}(p),\ldots,e\_{N,t}^{\ast}(p)\big)^{\prime},\qquad e\_{i,t}^{\ast}(p):=y\_{i,t}-{\mbox{$w$}}\_{i,t}(p)^{\prime}{\mbox{$\theta$}}\_{i}^{\ast}(p), |  |

and its covariance matrix

|  |  |  |
| --- | --- | --- |
|  | ğšºuâ€‹uâ€‹(p):=ğ”¼â€‹[ğ’†tâˆ—â€‹(p)â€‹ğ’†tâˆ—â€‹(p)â€²].{\mbox{$\Sigma$}}\_{uu}(p):={\mathbb{E}}\!\big[{\mbox{$e$}}\_{t}^{\ast}(p){\mbox{$e$}}\_{t}^{\ast}(p)^{\prime}\big]. |  |

Since pmaxp\_{\max} is fixed and ğ’«\mathcal{P} is finite, the above convergences hold uniformly over pâˆˆğ’«p\in\mathcal{P}.

By definition,

|  |  |  |
| --- | --- | --- |
|  | ğšº^uâ€‹u(p):=1Tâˆ’pâˆ‘t=p+1Tğœº^u,t(p)ğœº^u,t(p)â€².\widehat{{\mbox{$\Sigma$}}}\_{uu}(p):=\frac{1}{T-p}\sum\_{t=p+1}^{T}\widehat{{\mbox{$\varepsilon$}}}\_{u,t}(p)\widehat{{\mbox{$\varepsilon$}}}\_{u,t}(p)^{\prime}. |  |

Write ğœº^u,tâ€‹(p)=ğ’†tâˆ—â€‹(p)+ğ’“tâ€‹(p)\widehat{{\mbox{$\varepsilon$}}}\_{u,t}(p)={\mbox{$e$}}\_{t}^{\ast}(p)+{\mbox{$r$}}\_{t}(p) where ğ’“tâ€‹(p){\mbox{$r$}}\_{t}(p) collects the estimation errors from replacing ğœ½iâˆ—â€‹(p){\mbox{$\theta$}}\_{i}^{\ast}(p) by ğœ½^iâ€‹(p)\widehat{{\mbox{$\theta$}}}\_{i}(p). Using ğœ½^iâ€‹(p)â†’ğ‘ğœ½iâˆ—â€‹(p)\widehat{{\mbox{$\theta$}}}\_{i}(p)\xrightarrow{p}{\mbox{$\theta$}}\_{i}^{\ast}(p), finite (2+Î´)(2+\delta)-moments, and a LLN for strongly mixing sequences, we obtain

|  |  |  |
| --- | --- | --- |
|  | ğšº^uâ€‹uâ€‹(p)=1Tâˆ’pâ€‹âˆ‘t=p+1Tğ’†tâˆ—â€‹(p)â€‹ğ’†tâˆ—â€‹(p)â€²+opâ€‹(1)â†’ğ‘ğšºuâ€‹uâ€‹(p),\widehat{{\mbox{$\Sigma$}}}\_{uu}(p)=\frac{1}{T-p}\sum\_{t=p+1}^{T}{\mbox{$e$}}\_{t}^{\ast}(p){\mbox{$e$}}\_{t}^{\ast}(p)^{\prime}+o\_{p}(1)\xrightarrow{p}{\mbox{$\Sigma$}}\_{uu}(p), |  |

for each pâˆˆğ’«p\in\mathcal{P}.

Therefore, by the continuous mapping theorem,

|  |  |  |
| --- | --- | --- |
|  | logâ€‹detğšº^uâ€‹uâ€‹(p)â†’ğ‘logâ€‹detğšºuâ€‹uâ€‹(p),(pâˆˆğ’«).\log\det\widehat{{\mbox{$\Sigma$}}}\_{uu}(p)\xrightarrow{p}\log\det{\mbox{$\Sigma$}}\_{uu}(p),\qquad(p\in\mathcal{P}). |  |

For pâ‰¥p0p\geq p\_{0}, the augmented regression contains all the lags required by finite-predictor exactness, so the population regression is correctly specified with the additional lag coefficients set to zero. Hence ei,tâˆ—â€‹(p)=Îµu,i,te\_{i,t}^{\ast}(p)=\varepsilon\_{u,i,t} and thus

|  |  |  |
| --- | --- | --- |
|  | ğšºuâ€‹uâ€‹(p)=ğšºuâ€‹u(pâ‰¥p0).{\mbox{$\Sigma$}}\_{uu}(p)={\mbox{$\Sigma$}}\_{uu}\qquad(p\geq p\_{0}). |  |

For p<p0p<p\_{0}, write the omitted component in the stacked Durbin regression as

|  |  |  |
| --- | --- | --- |
|  | ğœ¹tâ€‹(p):=âˆ‘j=p+1p0(ğš¿uâ€‹u,jâ€‹ğ’štâˆ’j+ğš²jâ€‹ğ’™tâˆ’j),{\mbox{$\delta$}}\_{t}(p):=\sum\_{j=p+1}^{p\_{0}}\Big({\mbox{$\Psi$}}\_{uu,j}{\mbox{$y$}}\_{t-j}+{\mbox{$\Lambda$}}\_{j}{\mbox{$x$}}\_{t-j}\Big), |  |

so that the (correctly specified) population regression can be written as ğ’št=ğ‘¾tâ€‹(p)â€²â€‹ğœ½â€‹(p)+ğœ¹tâ€‹(p)+ğœºu,t{\mbox{$y$}}\_{t}={\mbox{$W$}}\_{t}(p)^{\prime}{\mbox{$\theta$}}(p)+{\mbox{$\delta$}}\_{t}(p)+{\mbox{$\varepsilon$}}\_{u,t}, where ğ‘¾tâ€‹(p){\mbox{$W$}}\_{t}(p) stacks the same regressors as {ğ’˜i,tâ€‹(p)}i=1N\{{\mbox{$w$}}\_{i,t}(p)\}\_{i=1}^{N}. Let ğ’«p\mathscr{P}\_{p} denote the L2L^{2}-projection onto the closed linear span of ğ‘¾tâ€‹(p){\mbox{$W$}}\_{t}(p). Then we can decompose

|  |  |  |
| --- | --- | --- |
|  | ğœ¹tâ€‹(p)=ğ’«pâ€‹[ğœ¹tâ€‹(p)]+ğœ»tâ€‹(p),ğ”¼â€‹[ğ‘¾tâ€‹(p)â€‹ğœ»tâ€‹(p)â€²]=ğŸ,{\mbox{$\delta$}}\_{t}(p)=\mathscr{P}\_{p}[{\mbox{$\delta$}}\_{t}(p)]+{\mbox{$\zeta$}}\_{t}(p),\qquad{\mathbb{E}}[{\mbox{$W$}}\_{t}(p){\mbox{$\zeta$}}\_{t}(p)^{\prime}]={\mbox{$0$}}, |  |

where ğœ»tâ€‹(p){\mbox{$\zeta$}}\_{t}(p) is the projection residual. By construction, ğœ»tâ€‹(p){\mbox{$\zeta$}}\_{t}(p) is measurable with respect to â„±tâˆ’1\mathscr{F}\_{t-1} (it is built from lags only), and under Eâ€‹Bâ€‹DEBD the innovation ğœºu,t{\mbox{$\varepsilon$}}\_{u,t} satisfies ğ”¼â€‹[ğœºu,tâˆ£â„±tâˆ’1]=ğŸ{\mathbb{E}}[{\mbox{$\varepsilon$}}\_{u,t}\mid\mathscr{F}\_{t-1}]={\mbox{$0$}}. Therefore ğ”¼â€‹[ğœºu,tâ€‹ğœ»tâ€‹(p)â€²]=ğŸ{\mathbb{E}}[{\mbox{$\varepsilon$}}\_{u,t}{\mbox{$\zeta$}}\_{t}(p)^{\prime}]={\mbox{$0$}} and

|  |  |  |
| --- | --- | --- |
|  | ğ’†tâˆ—â€‹(p)=ğœºu,t+ğœ»tâ€‹(p),ğšºuâ€‹uâ€‹(p)=ğšºuâ€‹u+ğš«â€‹(p),ğš«â€‹(p):=ğ”¼â€‹[ğœ»tâ€‹(p)â€‹ğœ»tâ€‹(p)â€²]â‰¥0.{\mbox{$e$}}\_{t}^{\ast}(p)={\mbox{$\varepsilon$}}\_{u,t}+{\mbox{$\zeta$}}\_{t}(p),\qquad{\mbox{$\Sigma$}}\_{uu}(p)={\mbox{$\Sigma$}}\_{uu}+{\mbox{$\Delta$}}(p),\quad{\mbox{$\Delta$}}(p):={\mathbb{E}}[{\mbox{$\zeta$}}\_{t}(p){\mbox{$\zeta$}}\_{t}(p)^{\prime}]\geq 0. |  |

Moreover, the minimality of p0p\_{0} in AssumptionÂ [1](https://arxiv.org/html/2601.21272v1#Thmassumption1 "Assumption 1 (Finite-predictor exactness at lag ğ‘â‚€): â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") implies that for p<p0p<p\_{0} we cannot have ğœ¹tâ€‹(p)âˆˆspanÂ¯â€‹{ğ‘¾tâ€‹(p)}{\mbox{$\delta$}}\_{t}(p)\in\overline{\mathrm{span}}\{{\mbox{$W$}}\_{t}(p)\} (otherwise p0p\_{0} would not be minimal), hence Prâ¡(ğœ»tâ€‹(p)â‰ 0)>0\Pr({\mbox{$\zeta$}}\_{t}(p)\neq 0)>0 and thus ğš«â€‹(p)â‰ 0{\mbox{$\Delta$}}(p)\neq 0. Since ğšºuâ€‹u>0{\mbox{$\Sigma$}}\_{uu}>0, we have the strict Loewner inequality ğšºuâ€‹uâ€‹(p)>ğšºuâ€‹u{\mbox{$\Sigma$}}\_{uu}(p)>{\mbox{$\Sigma$}}\_{uu} for p<p0p<p\_{0}, which implies

|  |  |  |
| --- | --- | --- |
|  | logâ€‹detğšºuâ€‹uâ€‹(p)>logâ€‹detğšºuâ€‹u(p<p0).\log\det{\mbox{$\Sigma$}}\_{uu}(p)>\log\det{\mbox{$\Sigma$}}\_{uu}\qquad(p<p\_{0}). |  |

Consequently, for every fixed p<p0p<p\_{0},

|  |  |  |
| --- | --- | --- |
|  | BICTâ¡(p)âˆ’BICTâ¡(p0)={logâ€‹detğšº^uâ€‹uâ€‹(p)âˆ’logâ€‹detğšº^uâ€‹uâ€‹(p0)}+{Îºâ€‹(p)âˆ’Îºâ€‹(p0)}â€‹logâ¡TTâ†’ğ‘câ€‹(p)>0,\operatorname{BIC}\_{T}(p)-\operatorname{BIC}\_{T}(p\_{0})=\big\{\log\det\widehat{{\mbox{$\Sigma$}}}\_{uu}(p)-\log\det\widehat{{\mbox{$\Sigma$}}}\_{uu}(p\_{0})\big\}+\frac{\{\kappa(p)-\kappa(p\_{0})\}\log T}{T}\xrightarrow{p}c(p)>0, |  |

because the penalty term is oâ€‹(1)o(1) while the first term converges to a strictly positive constant. Hence Prâ¡(p^BIC<p0)â†’0\Pr(\widehat{p}\_{\mathrm{BIC}}<p\_{0})\to 0.

Fix p>p0p>p\_{0}. Since the additional lag coefficients are zero under the true DGP, the fitted model with order pp is a (nested) over-parameterization of the correctly specified order-p0p\_{0} model. Under the maintained regularity conditions (strict stationarity, strong mixing, and finite moments), the Gaussian quasi-likelihood ratio (equivalently, the reduction in the concentrated objective) satisfies the standard chi-square limit:

|  |  |  |
| --- | --- | --- |
|  | (Tâˆ’p)â€‹{logâ€‹detğšº^uâ€‹uâ€‹(p0)âˆ’logâ€‹detğšº^uâ€‹uâ€‹(p)}â†’ğ‘‘Ï‡Îºâ€‹(p)âˆ’Îºâ€‹(p0)2.(T-p)\Big\{\log\det\widehat{{\mbox{$\Sigma$}}}\_{uu}(p\_{0})-\log\det\widehat{{\mbox{$\Sigma$}}}\_{uu}(p)\Big\}\xrightarrow{d}\chi^{2}\_{\kappa(p)-\kappa(p\_{0})}. |  |

In particular,

|  |  |  |
| --- | --- | --- |
|  | logâ€‹detğšº^uâ€‹uâ€‹(p)âˆ’logâ€‹detğšº^uâ€‹uâ€‹(p0)=Opâ€‹(Tâˆ’1),(p>p0).\log\det\widehat{{\mbox{$\Sigma$}}}\_{uu}(p)-\log\det\widehat{{\mbox{$\Sigma$}}}\_{uu}(p\_{0})=O\_{p}(T^{-1}),\qquad(p>p\_{0}). |  |

Therefore,

|  |  |  |
| --- | --- | --- |
|  | BICTâ¡(p)âˆ’BICTâ¡(p0)=Opâ€‹(Tâˆ’1)+{Îºâ€‹(p)âˆ’Îºâ€‹(p0)}â€‹logâ¡TTâ†’ğ‘0+,\operatorname{BIC}\_{T}(p)-\operatorname{BIC}\_{T}(p\_{0})=O\_{p}(T^{-1})+\frac{\{\kappa(p)-\kappa(p\_{0})\}\log T}{T}\xrightarrow{p}0^{+}, |  |

and more precisely the second term dominates the first because logâ¡Tâ†’âˆ\log T\to\infty:

|  |  |  |
| --- | --- | --- |
|  | Prâ¡(BICTâ¡(p)>BICTâ¡(p0))â†’1.\Pr\!\Big(\operatorname{BIC}\_{T}(p)>\operatorname{BIC}\_{T}(p\_{0})\Big)\to 1. |  |

Hence Prâ¡(p^BIC>p0)â†’0\Pr(\widehat{p}\_{\mathrm{BIC}}>p\_{0})\to 0.

Using that ğ’«\mathcal{P} is finite, we obtain

|  |  |  |
| --- | --- | --- |
|  | Prâ¡(p^BIC=p0)â†’1,\Pr(\widehat{p}\_{\mathrm{BIC}}=p\_{0})\to 1, |  |

which proves p^BICâ†’ğ‘p0\widehat{p}\_{\mathrm{BIC}}\xrightarrow{p}p\_{0}. âˆ

### A.9 Proof of Lemma3

Fix iâˆˆ{1,â€¦,N}i\in\{1,\ldots,N\} and write the iith Durbin regression on the effective sample t=p0+1,â€¦,Tt=p\_{0}+1,\ldots,T as

|  |  |  |
| --- | --- | --- |
|  | yi,t=ğ’˜i,tâ€²â€‹ğœ½i+Îµu,i,t,Teff:=Tâˆ’p0.y\_{i,t}={\mbox{$w$}}\_{i,t}^{\prime}{\mbox{$\theta$}}\_{i}+\varepsilon\_{u,i,t},\qquad T\_{\mathrm{eff}}:=T-p\_{0}. |  |

Then

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğœ½^iDâˆ’ğœ½i=(1Teffâ€‹âˆ‘t=p0+1Tğ’˜i,tâ€‹ğ’˜i,tâ€²)âˆ’1â€‹(1Teffâ€‹âˆ‘t=p0+1Tğ’˜i,tâ€‹Îµu,i,t).\widehat{{\mbox{$\theta$}}}\_{i}^{\mathrm{D}}-{\mbox{$\theta$}}\_{i}=\Bigl(\frac{1}{T\_{\mathrm{eff}}}\sum\_{t=p\_{0}+1}^{T}{\mbox{$w$}}\_{i,t}{\mbox{$w$}}\_{i,t}^{\prime}\Bigr)^{-1}\Bigl(\frac{1}{T\_{\mathrm{eff}}}\sum\_{t=p\_{0}+1}^{T}{\mbox{$w$}}\_{i,t}\varepsilon\_{u,i,t}\Bigr). |  | (61) |

Hence it suffices to show

|  |  |  |
| --- | --- | --- |
|  | 1Teffâ€‹âˆ‘t=p0+1Tğ’˜i,tâ€‹ğ’˜i,tâ€²â†’ğ‘ğ‘¸w,iwithÂ ğ‘¸w,i>0,1Teffâ€‹âˆ‘t=p0+1Tğ’˜i,tâ€‹Îµu,i,tâ†’ğ‘ğŸ.\frac{1}{T\_{\mathrm{eff}}}\sum\_{t=p\_{0}+1}^{T}{\mbox{$w$}}\_{i,t}{\mbox{$w$}}\_{i,t}^{\prime}\xrightarrow{p}{\mbox{$Q$}}\_{w,i}\quad\text{with }{\mbox{$Q$}}\_{w,i}>0,\qquad\frac{1}{T\_{\mathrm{eff}}}\sum\_{t=p\_{0}+1}^{T}{\mbox{$w$}}\_{i,t}\varepsilon\_{u,i,t}\xrightarrow{p}{\mbox{$0$}}. |  |

Under AssumptionÂ [1](https://arxiv.org/html/2601.21272v1#Thmassumption1 "Assumption 1 (Finite-predictor exactness at lag ğ‘â‚€): â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"), PropositionÂ [1](https://arxiv.org/html/2601.21272v1#Thmproposition1 "Proposition 1: â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")(iv) implies that {ğ’›t}\{{\mbox{$z$}}\_{t}\} (and hence any finite-dimensional vector formed from finitely many lags/leads of ğ’›t{\mbox{$z$}}\_{t}) is strictly stationary, strongly mixing with summable mixing-rate powers, and has finite (2+Î´)(2+\delta)-moments. Since ğ’˜i,t{\mbox{$w$}}\_{i,t} is a finite stacking of 11, ğ’™i,t{\mbox{$x$}}\_{i,t}, {ğ’štâˆ’j}j=1p0\{{\mbox{$y$}}\_{t-j}\}\_{j=1}^{p\_{0}}, and {ğ’™tâˆ’j}j=1p0\{{\mbox{$x$}}\_{t-j}\}\_{j=1}^{p\_{0}}, it follows that {ğ’˜i,t}\{{\mbox{$w$}}\_{i,t}\} and {ğ’˜i,tâ€‹Îµu,i,t}\{{\mbox{$w$}}\_{i,t}\varepsilon\_{u,i,t}\} are also strictly stationary and strongly mixing, and by LemmaÂ [1](https://arxiv.org/html/2601.21272v1#Thmlemma1 "Lemma 1 (Equation-by-equation properties of the Durbin regression): â€£ 2.5 Asymptotic properties of generalized Durbin estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")(ii),

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹â€–ğ’˜i,tâ€–2+Î´<âˆ,ğ”¼â€‹â€–ğ’˜i,tâ€‹Îµu,i,tâ€–2+Î´<âˆ.{\mathbb{E}}\|{\mbox{$w$}}\_{i,t}\|^{2+\delta}<\infty,\qquad{\mathbb{E}}\|{\mbox{$w$}}\_{i,t}\varepsilon\_{u,i,t}\|^{2+\delta}<\infty. |  |

Therefore, a law of large numbers for strongly mixing sequences applies to {ğ’˜i,tâ€‹ğ’˜i,tâ€²}\{{\mbox{$w$}}\_{i,t}{\mbox{$w$}}\_{i,t}^{\prime}\} and {ğ’˜i,tâ€‹Îµu,i,t}\{{\mbox{$w$}}\_{i,t}\varepsilon\_{u,i,t}\}.

First, by LemmaÂ [1](https://arxiv.org/html/2601.21272v1#Thmlemma1 "Lemma 1 (Equation-by-equation properties of the Durbin regression): â€£ 2.5 Asymptotic properties of generalized Durbin estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")(iii), the second-moment matrix

|  |  |  |
| --- | --- | --- |
|  | ğ‘¸w,i:=ğ”¼â€‹[ğ’˜i,tâ€‹ğ’˜i,tâ€²]{\mbox{$Q$}}\_{w,i}:={\mathbb{E}}[{\mbox{$w$}}\_{i,t}{\mbox{$w$}}\_{i,t}^{\prime}] |  |

is positive definite. Next, we show ğ”¼â€‹[ğ’˜i,tâ€‹Îµu,i,t]=ğŸ{\mathbb{E}}[{\mbox{$w$}}\_{i,t}\varepsilon\_{u,i,t}]={\mbox{$0$}}. By LemmaÂ [1](https://arxiv.org/html/2601.21272v1#Thmlemma1 "Lemma 1 (Equation-by-equation properties of the Durbin regression): â€£ 2.5 Asymptotic properties of generalized Durbin estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")(i), there exist an â„±tâˆ’1\mathscr{F}\_{t-1}-measurable vector ğ’‰i,t{\mbox{$h$}}\_{i,t} and a deterministic matrix ğ’i{\mbox{$m$}}\_{i} such that

|  |  |  |
| --- | --- | --- |
|  | ğ’˜i,t=ğ’‰i,t+ğ’iğœºx,t,â„±tâˆ’1:=Ïƒ(ğ’›s:sâ‰¤tâˆ’1).{\mbox{$w$}}\_{i,t}={\mbox{$h$}}\_{i,t}+{\mbox{$m$}}\_{i}{\mbox{$\varepsilon$}}\_{x,t},\qquad\mathscr{F}\_{t-1}:=\sigma({\mbox{$z$}}\_{s}:\,s\leq t-1). |  |

Moreover, under each of Bâ€‹DBD, Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG, and Eâ€‹Bâ€‹DEBD we have the innovation block-orthogonality ğšºxâ€‹u=ğ”¼â€‹[ğœºx,tâ€‹ğœºu,tâ€²]=ğŸ{\mbox{$\Sigma$}}\_{xu}={\mathbb{E}}[{\mbox{$\varepsilon$}}\_{x,t}{\mbox{$\varepsilon$}}\_{u,t}^{\prime}]={\mbox{$0$}}. Using that ğœºu,t{\mbox{$\varepsilon$}}\_{u,t} is the innovation of the joint process, we also have ğ”¼â€‹[Îµu,i,tâˆ£â„±tâˆ’1]=0{\mathbb{E}}[\varepsilon\_{u,i,t}\mid\mathscr{F}\_{t-1}]=0. Hence,

|  |  |  |
| --- | --- | --- |
|  | ğ”¼[ğ’‰i,tÎµu,i,t]=ğ”¼[ğ’‰i,tğ”¼[Îµu,i,tâˆ£â„±tâˆ’1]]=ğŸ,{\mathbb{E}}[{\mbox{$h$}}\_{i,t}\varepsilon\_{u,i,t}]={\mathbb{E}}\!\mathopen{}\left[{\mbox{$h$}}\_{i,t}\,{\mathbb{E}}[\varepsilon\_{u,i,t}\mid\mathscr{F}\_{t-1}]\mathclose{}\right]={\mbox{$0$}}, |  |

and

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[ğ’iâ€‹ğœºx,tâ€‹Îµu,i,t]=ğ’iâ€‹ğ”¼â€‹[ğœºx,tâ€‹Îµu,i,t]=ğŸ.{\mathbb{E}}[{\mbox{$m$}}\_{i}{\mbox{$\varepsilon$}}\_{x,t}\,\varepsilon\_{u,i,t}]={\mbox{$m$}}\_{i}\,{\mathbb{E}}[{\mbox{$\varepsilon$}}\_{x,t}\varepsilon\_{u,i,t}]={\mbox{$0$}}. |  |

Therefore,

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â€‹[ğ’˜i,tâ€‹Îµu,i,t]=ğŸ.{\mathbb{E}}[{\mbox{$w$}}\_{i,t}\varepsilon\_{u,i,t}]={\mbox{$0$}}. |  | (62) |

By the LLN for strongly mixing sequences,

|  |  |  |
| --- | --- | --- |
|  | 1Teffâ€‹âˆ‘t=p0+1Tğ’˜i,tâ€‹ğ’˜i,tâ€²â†’ğ‘ğ”¼â€‹[ğ’˜i,tâ€‹ğ’˜i,tâ€²]=ğ‘¸w,i,\frac{1}{T\_{\mathrm{eff}}}\sum\_{t=p\_{0}+1}^{T}{\mbox{$w$}}\_{i,t}{\mbox{$w$}}\_{i,t}^{\prime}\xrightarrow{p}{\mathbb{E}}[{\mbox{$w$}}\_{i,t}{\mbox{$w$}}\_{i,t}^{\prime}]={\mbox{$Q$}}\_{w,i}, |  |

and similarly,

|  |  |  |
| --- | --- | --- |
|  | 1Teffâ€‹âˆ‘t=p0+1Tğ’˜i,tâ€‹Îµu,i,tâ†’ğ‘ğ”¼â€‹[ğ’˜i,tâ€‹Îµu,i,t]=ğŸ,\frac{1}{T\_{\mathrm{eff}}}\sum\_{t=p\_{0}+1}^{T}{\mbox{$w$}}\_{i,t}\varepsilon\_{u,i,t}\xrightarrow{p}{\mathbb{E}}[{\mbox{$w$}}\_{i,t}\varepsilon\_{u,i,t}]={\mbox{$0$}}, |  |

where the last equality uses ([62](https://arxiv.org/html/2601.21272v1#Sx2.E62 "In A.9 Proof of Lemma3 â€£ Appendix â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")). (The omission of the first p0p\_{0} observations is asymptotically negligible because p0p\_{0} is fixed and Teff/Tâ†’1T\_{\mathrm{eff}}/T\to 1.) Since ğ‘¸w,i{\mbox{$Q$}}\_{w,i} is positive definite, the continuous mapping theorem yields

|  |  |  |
| --- | --- | --- |
|  | (1Teffâ€‹âˆ‘t=p0+1Tğ’˜i,tâ€‹ğ’˜i,tâ€²)âˆ’1â†’ğ‘ğ‘¸w,iâˆ’1.\Bigl(\frac{1}{T\_{\mathrm{eff}}}\sum\_{t=p\_{0}+1}^{T}{\mbox{$w$}}\_{i,t}{\mbox{$w$}}\_{i,t}^{\prime}\Bigr)^{-1}\xrightarrow{p}{\mbox{$Q$}}\_{w,i}^{-1}. |  |

Combining this with ([61](https://arxiv.org/html/2601.21272v1#Sx2.E61 "In A.9 Proof of Lemma3 â€£ Appendix â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")) and Slutskyâ€™s theorem gives

|  |  |  |
| --- | --- | --- |
|  | ğœ½^iDâˆ’ğœ½iâ†’ğ‘ğŸ,\widehat{{\mbox{$\theta$}}}\_{i}^{\mathrm{D}}-{\mbox{$\theta$}}\_{i}\xrightarrow{p}{\mbox{$0$}}, |  |

i.e. ğœ½^iDâ†’ğ‘ğœ½i\widehat{{\mbox{$\theta$}}}\_{i}^{\mathrm{D}}\xrightarrow{p}{\mbox{$\theta$}}\_{i}. âˆ

### A.10 Proof of TheoremÂ [1](https://arxiv.org/html/2601.21272v1#Thmtheorem1 "Theorem 1: â€£ 2.5 Asymptotic properties of generalized Durbin estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")

Write the effective sample as t=p0+1,â€¦,Tt=p\_{0}+1,\ldots,T with Teff:=Tâˆ’p0T\_{\mathrm{eff}}:=T-p\_{0}.
Throughout, let âˆ¥â‹…âˆ¥\|\cdot\| denote any matrix norm compatible with multiplication.

Define the infeasible (population) generalized-Durbin transformation using the true nuisance parameters:

|  |  |  |
| --- | --- | --- |
|  | ğ’šGD,t0:=ğ’štâˆ’âˆ‘j=1p0ğš¿uâ€‹u,jâ€‹ğ’štâˆ’jâˆ’âˆ‘j=1p0ğš¿uâ€‹x,jâ€‹(ğ’™tâˆ’jâˆ’ğx),{\mbox{$y$}}\_{\mathrm{GD},t}^{0}:={\mbox{$y$}}\_{t}-\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{uu,j}{\mbox{$y$}}\_{t-j}-\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{ux,j}\big({\mbox{$x$}}\_{t-j}-{\mbox{$\mu$}}\_{x}\big), |  |

|  |  |  |
| --- | --- | --- |
|  | ğ’GD,t0â£â€²:=[(ğ‘°Nâˆ’âˆ‘j=1p0ğš¿uâ€‹u,j),ğ‘¿tâ€²âˆ’âˆ‘j=1p0ğš¿uâ€‹u,jâ€‹ğ‘¿tâˆ’jâ€²].{\mbox{$Z$}}\_{\mathrm{GD},t}^{0\prime}:=\biggl[\Bigl({\mbox{$I$}}\_{N}-\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{uu,j}\Bigr),\ {\mbox{$X$}}\_{t}^{\prime}-\sum\_{j=1}^{p\_{0}}{\mbox{$\Psi$}}\_{uu,j}{\mbox{$X$}}\_{t-j}^{\prime}\biggr]. |  |

Then, by construction from ([23](https://arxiv.org/html/2601.21272v1#S2.E23 "In 2.5 Asymptotic properties of generalized Durbin estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"))â€“([24](https://arxiv.org/html/2601.21272v1#S2.E24 "In 2.5 Asymptotic properties of generalized Durbin estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")),

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ’šGD,t0=ğ’GD,t0â£â€²â€‹ğœ¿+ğœºu,t,t=p0+1,â€¦,T.{\mbox{$y$}}\_{\mathrm{GD},t}^{0}={\mbox{$Z$}}\_{\mathrm{GD},t}^{0\prime}{\mbox{$\kappa$}}+{\mbox{$\varepsilon$}}\_{u,t},\qquad t=p\_{0}+1,\ldots,T. |  | (63) |

Let ğšºuâ€‹u:=ğ”¼â€‹[ğœºu,tâ€‹ğœºu,tâ€²]{\mbox{$\Sigma$}}\_{uu}:={\mathbb{E}}[{\mbox{$\varepsilon$}}\_{u,t}{\mbox{$\varepsilon$}}\_{u,t}^{\prime}].
Define the infeasible GLS estimator (using the true ğšºuâ€‹u{\mbox{$\Sigma$}}\_{uu}) by

|  |  |  |
| --- | --- | --- |
|  | ğœ¿^â€‰0:=(1Teffâ€‹âˆ‘t=p0+1Tğ’GD,t0â€‹ğšºuâ€‹uâˆ’1â€‹ğ’GD,t0â£â€²)âˆ’1â€‹(1Teffâ€‹âˆ‘t=p0+1Tğ’GD,t0â€‹ğšºuâ€‹uâˆ’1â€‹ğ’šGD,t0).\widehat{{\mbox{$\kappa$}}}^{\,0}:=\Bigl(\frac{1}{T\_{\mathrm{eff}}}\sum\_{t=p\_{0}+1}^{T}{\mbox{$Z$}}\_{\mathrm{GD},t}^{0}{\mbox{$\Sigma$}}\_{uu}^{-1}{\mbox{$Z$}}\_{\mathrm{GD},t}^{0\prime}\Bigr)^{-1}\Bigl(\frac{1}{T\_{\mathrm{eff}}}\sum\_{t=p\_{0}+1}^{T}{\mbox{$Z$}}\_{\mathrm{GD},t}^{0}{\mbox{$\Sigma$}}\_{uu}^{-1}{\mbox{$y$}}\_{\mathrm{GD},t}^{0}\Bigr). |  |

We prove (i) ğœ¿^GD\widehat{{\mbox{$\kappa$}}}^{\mathrm{GD}} is asymptotically equivalent to ğœ¿^â€‰0\widehat{{\mbox{$\kappa$}}}^{\,0},
(ii) ğœ¿^â€‰0\widehat{{\mbox{$\kappa$}}}^{\,0} is consistent and asymptotically normal with asymptotic variance ğ‘½V,
and (iii) ğœ¿^GD\widehat{{\mbox{$\kappa$}}}^{\mathrm{GD}} is asymptotically efficient under Eâ€‹Bâ€‹DEBD.

Let ğœ½^Dâˆ’OLS\widehat{{\mbox{$\theta$}}}^{\mathrm{D\!-\!OLS}} be the stacked first-step Durbin OLS estimator
and define ğš«:=ğœ½^Dâˆ’OLSâˆ’ğœ½{\mbox{$\Delta$}}:=\widehat{{\mbox{$\theta$}}}^{\mathrm{D\!-\!OLS}}-{\mbox{$\theta$}}.
By definition,

|  |  |  |
| --- | --- | --- |
|  | ğœº^u,t=ğ’štâˆ’ğ‘¾tâ€²â€‹ğœ½^Dâˆ’OLS=ğœºu,tâˆ’ğ‘¾tâ€²â€‹ğš«,\widehat{{\mbox{$\varepsilon$}}}\_{u,t}={\mbox{$y$}}\_{t}-{\mbox{$W$}}\_{t}^{\prime}\widehat{{\mbox{$\theta$}}}^{\mathrm{D\!-\!OLS}}={\mbox{$\varepsilon$}}\_{u,t}-{\mbox{$W$}}\_{t}^{\prime}{\mbox{$\Delta$}}, |  |

where ğœºu,t{\mbox{$\varepsilon$}}\_{u,t} is the innovation in ([23](https://arxiv.org/html/2601.21272v1#S2.E23 "In 2.5 Asymptotic properties of generalized Durbin estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")).
Hence

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğšº^uâ€‹u\displaystyle\widehat{{\mbox{$\Sigma$}}}\_{uu} | =1Teffâ€‹âˆ‘t=p0+1Tğœº^u,tâ€‹ğœº^u,tâ€²\displaystyle=\frac{1}{T\_{\mathrm{eff}}}\sum\_{t=p\_{0}+1}^{T}\widehat{{\mbox{$\varepsilon$}}}\_{u,t}\widehat{{\mbox{$\varepsilon$}}}\_{u,t}^{\prime} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =1Teffâ€‹âˆ‘t=p0+1Tğœºu,tâ€‹ğœºu,tâ€²âˆ’1Teffâ€‹âˆ‘t=p0+1Tğœºu,tâ€‹(ğ‘¾tâ€²â€‹ğš«)â€²âˆ’1Teffâ€‹âˆ‘t=p0+1T(ğ‘¾tâ€²â€‹ğš«)â€‹ğœºu,tâ€²\displaystyle=\frac{1}{T\_{\mathrm{eff}}}\sum\_{t=p\_{0}+1}^{T}{\mbox{$\varepsilon$}}\_{u,t}{\mbox{$\varepsilon$}}\_{u,t}^{\prime}-\frac{1}{T\_{\mathrm{eff}}}\sum\_{t=p\_{0}+1}^{T}{\mbox{$\varepsilon$}}\_{u,t}({\mbox{$W$}}\_{t}^{\prime}{\mbox{$\Delta$}})^{\prime}-\frac{1}{T\_{\mathrm{eff}}}\sum\_{t=p\_{0}+1}^{T}({\mbox{$W$}}\_{t}^{\prime}{\mbox{$\Delta$}}){\mbox{$\varepsilon$}}\_{u,t}^{\prime} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +1Teffâ€‹âˆ‘t=p0+1T(ğ‘¾tâ€²â€‹ğš«)â€‹(ğ‘¾tâ€²â€‹ğš«)â€².\displaystyle\qquad+\frac{1}{T\_{\mathrm{eff}}}\sum\_{t=p\_{0}+1}^{T}({\mbox{$W$}}\_{t}^{\prime}{\mbox{$\Delta$}})({\mbox{$W$}}\_{t}^{\prime}{\mbox{$\Delta$}})^{\prime}. |  |

By AssumptionÂ [1](https://arxiv.org/html/2601.21272v1#Thmassumption1 "Assumption 1 (Finite-predictor exactness at lag ğ‘â‚€): â€£ 2.1 Setup â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") (mixing and finite moments),
a LLN yields

|  |  |  |
| --- | --- | --- |
|  | 1Teffâ€‹âˆ‘t=p0+1Tğœºu,tâ€‹ğœºu,tâ€²â†’ğ‘ğ”¼â€‹[ğœºu,tâ€‹ğœºu,tâ€²]=ğšºuâ€‹u.\frac{1}{T\_{\mathrm{eff}}}\sum\_{t=p\_{0}+1}^{T}{\mbox{$\varepsilon$}}\_{u,t}{\mbox{$\varepsilon$}}\_{u,t}^{\prime}\xrightarrow{p}{\mathbb{E}}[{\mbox{$\varepsilon$}}\_{u,t}{\mbox{$\varepsilon$}}\_{u,t}^{\prime}]={\mbox{$\Sigma$}}\_{uu}. |  |

Moreover, LemmaÂ [3](https://arxiv.org/html/2601.21272v1#Thmlemma3 "Lemma 3: â€£ 2.5 Asymptotic properties of generalized Durbin estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") gives â€–ğš«â€–=opâ€‹(1)\|{\mbox{$\Delta$}}\|=o\_{p}(1), and by Cauchyâ€“Schwarz together with LLN,

|  |  |  |
| --- | --- | --- |
|  | 1Teffâ€‹âˆ‘t=p0+1Tâ€–ğœºu,tâ€–â€‹â€–ğ‘¾tâ€–=Opâ€‹(1),1Teffâ€‹âˆ‘t=p0+1Tâ€–ğ‘¾tâ€–2=Opâ€‹(1).\frac{1}{T\_{\mathrm{eff}}}\sum\_{t=p\_{0}+1}^{T}\|{\mbox{$\varepsilon$}}\_{u,t}\|\,\|{\mbox{$W$}}\_{t}\|=O\_{p}(1),\qquad\frac{1}{T\_{\mathrm{eff}}}\sum\_{t=p\_{0}+1}^{T}\|{\mbox{$W$}}\_{t}\|^{2}=O\_{p}(1). |  |

Hence the two cross terms are opâ€‹(1)o\_{p}(1) and the last quadratic term is also opâ€‹(1)o\_{p}(1).
Therefore,

|  |  |  |
| --- | --- | --- |
|  | ğšº^uâ€‹uâ†’ğ‘ğšºuâ€‹u.\widehat{{\mbox{$\Sigma$}}}\_{uu}\xrightarrow{p}{\mbox{$\Sigma$}}\_{uu}. |  |

Since ğšºuâ€‹u>0{\mbox{$\Sigma$}}\_{uu}>0, eigenvalue continuity implies that ğšº^uâ€‹u\widehat{{\mbox{$\Sigma$}}}\_{uu} is invertible w.p.a.1
and ğšº^uâ€‹uâˆ’1â†’ğ‘ğšºuâ€‹uâˆ’1\widehat{{\mbox{$\Sigma$}}}\_{uu}^{-1}\xrightarrow{p}{\mbox{$\Sigma$}}\_{uu}^{-1}.

From ([63](https://arxiv.org/html/2601.21272v1#Sx2.E63 "In A.10 Proof of Theorem 1 â€£ Appendix â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")),

|  |  |  |
| --- | --- | --- |
|  | ğœ¿^â€‰0âˆ’ğœ¿=(1Teffâ€‹âˆ‘tğ’GD,t0â€‹ğšºuâ€‹uâˆ’1â€‹ğ’GD,t0â£â€²)âˆ’1â€‹(1Teffâ€‹âˆ‘tğ’GD,t0â€‹ğšºuâ€‹uâˆ’1â€‹ğœºu,t).\widehat{{\mbox{$\kappa$}}}^{\,0}-{\mbox{$\kappa$}}=\Bigl(\frac{1}{T\_{\mathrm{eff}}}\sum\_{t}{\mbox{$Z$}}\_{\mathrm{GD},t}^{0}{\mbox{$\Sigma$}}\_{uu}^{-1}{\mbox{$Z$}}\_{\mathrm{GD},t}^{0\prime}\Bigr)^{-1}\Bigl(\frac{1}{T\_{\mathrm{eff}}}\sum\_{t}{\mbox{$Z$}}\_{\mathrm{GD},t}^{0}{\mbox{$\Sigma$}}\_{uu}^{-1}{\mbox{$\varepsilon$}}\_{u,t}\Bigr). |  |

By a LLN for strongly mixing sequences,

|  |  |  |
| --- | --- | --- |
|  | 1Teffâˆ‘tğ’GD,t0ğšºuâ€‹uâˆ’1ğ’GD,t0â£â€²â†’ğ‘ğ”¼[ğ’GD,t0ğšºuâ€‹uâˆ’1ğ’GD,t0â£â€²]=:ğ‘¸GD0.\frac{1}{T\_{\mathrm{eff}}}\sum\_{t}{\mbox{$Z$}}\_{\mathrm{GD},t}^{0}{\mbox{$\Sigma$}}\_{uu}^{-1}{\mbox{$Z$}}\_{\mathrm{GD},t}^{0\prime}\xrightarrow{p}{\mathbb{E}}[{\mbox{$Z$}}\_{\mathrm{GD},t}^{0}{\mbox{$\Sigma$}}\_{uu}^{-1}{\mbox{$Z$}}\_{\mathrm{GD},t}^{0\prime}]=:{\mbox{$Q$}}\_{\mathrm{GD}}^{0}. |  |

By AssumptionÂ [5](https://arxiv.org/html/2601.21272v1#Thmassumption5 "Assumption 5: â€£ 2.5 Asymptotic properties of generalized Durbin estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")(A5.1), ğ‘¸GD0{\mbox{$Q$}}\_{\mathrm{GD}}^{0} is positive definite, hence nonsingular.

Next define the linear information set

|  |  |  |
| --- | --- | --- |
|  | ğ’¢t:=Ïƒ(â„±tâˆ’1,ğœºx,t),â„±tâˆ’1:=Ïƒ(ğ’›s:sâ‰¤tâˆ’1).\mathscr{G}\_{t}:=\sigma(\mathscr{F}\_{t-1},{\mbox{$\varepsilon$}}\_{x,t}),\qquad\mathscr{F}\_{t-1}:=\sigma({\mbox{$z$}}\_{s}:s\leq t-1). |  |

As in LemmaÂ [1](https://arxiv.org/html/2601.21272v1#Thmlemma1 "Lemma 1 (Equation-by-equation properties of the Durbin regression): â€£ 2.5 Asymptotic properties of generalized Durbin estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")(i), ğ’GD,t0{\mbox{$Z$}}\_{\mathrm{GD},t}^{0} is ğ’¢t\mathscr{G}\_{t}-measurable.
Under Bâ€‹DBD, Gâ€‹Eâ€‹Xâ€‹Oâ€‹GGEXOG, or Eâ€‹Bâ€‹DEBD, we have ğšºxâ€‹u=0{\mbox{$\Sigma$}}\_{xu}=0 and, by the defining orthogonality property of the Wold innovations,

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â€‹[ğœºu,tâˆ£ğ’¢t]=ğŸ.{\mathbb{E}}[{\mbox{$\varepsilon$}}\_{u,t}\mid\mathscr{G}\_{t}]={\mbox{$0$}}. |  | (64) |

Hence ğ”¼â€‹[ğ’GD,t0â€‹ğšºuâ€‹uâˆ’1â€‹ğœºu,t]=ğŸ{\mathbb{E}}[{\mbox{$Z$}}\_{\mathrm{GD},t}^{0}{\mbox{$\Sigma$}}\_{uu}^{-1}{\mbox{$\varepsilon$}}\_{u,t}]={\mbox{$0$}} and a LLN yields

|  |  |  |
| --- | --- | --- |
|  | 1Teffâ€‹âˆ‘tğ’GD,t0â€‹ğšºuâ€‹uâˆ’1â€‹ğœºu,tâ†’ğ‘ğŸ,\frac{1}{T\_{\mathrm{eff}}}\sum\_{t}{\mbox{$Z$}}\_{\mathrm{GD},t}^{0}{\mbox{$\Sigma$}}\_{uu}^{-1}{\mbox{$\varepsilon$}}\_{u,t}\xrightarrow{p}{\mbox{$0$}}, |  |

which implies ğœ¿^â€‰0â†’ğ‘ğœ¿\widehat{{\mbox{$\kappa$}}}^{\,0}\xrightarrow{p}{\mbox{$\kappa$}}.

For asymptotic normality, set ğt:=ğ’GD,t0â€‹ğšºuâ€‹uâˆ’1â€‹ğœºu,t{\mbox{$\psi$}}\_{t}:={\mbox{$Z$}}\_{\mathrm{GD},t}^{0}{\mbox{$\Sigma$}}\_{uu}^{-1}{\mbox{$\varepsilon$}}\_{u,t}.
A multivariate CLT for strongly mixing sequences gives

|  |  |  |
| --- | --- | --- |
|  | 1Teffâ€‹âˆ‘tğtâ†’ğ‘‘ğ’©â€‹(ğŸ,ğ‘º),ğ‘º:=âˆ‘hâˆˆâ„¤ğ”¼â€‹[ğtâ€‹ğtâˆ’hâ€²].\frac{1}{\sqrt{T\_{\mathrm{eff}}}}\sum\_{t}{\mbox{$\psi$}}\_{t}\xrightarrow{d}\mathcal{N}({\mbox{$0$}},{\mbox{$S$}}),\qquad{\mbox{$S$}}:=\sum\_{h\in\mathbb{Z}}{\mathbb{E}}[{\mbox{$\psi$}}\_{t}{\mbox{$\psi$}}\_{t-h}^{\prime}]. |  |

We simplify ğ‘ºS.
Fix hâ‰¥1h\geq 1. Since ğtâˆ’h{\mbox{$\psi$}}\_{t-h} is â„±tâˆ’1\mathscr{F}\_{t-1}-measurable, it is ğ’¢t\mathscr{G}\_{t}-measurable.
Thus, by the tower property and ([64](https://arxiv.org/html/2601.21272v1#Sx2.E64 "In A.10 Proof of Theorem 1 â€£ Appendix â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")),

|  |  |  |
| --- | --- | --- |
|  | ğ”¼[ğtğtâˆ’hâ€²]=ğ”¼[ğ”¼(ğtâˆ£ğ’¢t)ğtâˆ’hâ€²]=ğ”¼[ğ’GD,t0ğšºuâ€‹uâˆ’1ğ”¼(ğœºu,tâˆ£ğ’¢t)ğtâˆ’hâ€²]=ğŸ.{\mathbb{E}}[{\mbox{$\psi$}}\_{t}{\mbox{$\psi$}}\_{t-h}^{\prime}]={\mathbb{E}}\!\mathopen{}\left[{\mathbb{E}}({\mbox{$\psi$}}\_{t}\mid\mathscr{G}\_{t})\,{\mbox{$\psi$}}\_{t-h}^{\prime}\mathclose{}\right]={\mathbb{E}}\!\mathopen{}\left[{\mbox{$Z$}}\_{\mathrm{GD},t}^{0}{\mbox{$\Sigma$}}\_{uu}^{-1}{\mathbb{E}}({\mbox{$\varepsilon$}}\_{u,t}\mid\mathscr{G}\_{t})\,{\mbox{$\psi$}}\_{t-h}^{\prime}\mathclose{}\right]={\mbox{$0$}}. |  |

By symmetry, ğ”¼â€‹[ğtâ€‹ğt+hâ€²]=ğŸ{\mathbb{E}}[{\mbox{$\psi$}}\_{t}{\mbox{$\psi$}}\_{t+h}^{\prime}]={\mbox{$0$}} for hâ‰¥1h\geq 1 as well.
Hence ğ‘º=ğ”¼â€‹[ğtâ€‹ğtâ€²]{\mbox{$S$}}={\mathbb{E}}[{\mbox{$\psi$}}\_{t}{\mbox{$\psi$}}\_{t}^{\prime}].

Using AssumptionÂ [5](https://arxiv.org/html/2601.21272v1#Thmassumption5 "Assumption 5: â€£ 2.5 Asymptotic properties of generalized Durbin estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models")(A5.2) with Ïƒâ€‹(ğ’GD,t0)\sigma({\mbox{$Z$}}\_{\mathrm{GD},t}^{0}),

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ‘ºS | =ğ”¼[ğ’GD,t0ğšºuâ€‹uâˆ’1ğœºu,tğœºu,tâ€²ğšºuâ€‹uâˆ’1ğ’GD,t0â£â€²]\displaystyle={\mathbb{E}}\!\mathopen{}\left[{\mbox{$Z$}}\_{\mathrm{GD},t}^{0}{\mbox{$\Sigma$}}\_{uu}^{-1}{\mbox{$\varepsilon$}}\_{u,t}{\mbox{$\varepsilon$}}\_{u,t}^{\prime}{\mbox{$\Sigma$}}\_{uu}^{-1}{\mbox{$Z$}}\_{\mathrm{GD},t}^{0\prime}\mathclose{}\right] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =ğ”¼[ğ’GD,t0ğšºuâ€‹uâˆ’1ğ”¼(ğœºu,tğœºu,tâ€²âˆ£Ïƒ(ğ’GD,t0))ğšºuâ€‹uâˆ’1ğ’GD,t0â£â€²]\displaystyle={\mathbb{E}}\!\mathopen{}\left[{\mbox{$Z$}}\_{\mathrm{GD},t}^{0}{\mbox{$\Sigma$}}\_{uu}^{-1}\,{\mathbb{E}}\!\bigl({\mbox{$\varepsilon$}}\_{u,t}{\mbox{$\varepsilon$}}\_{u,t}^{\prime}\mid\sigma({\mbox{$Z$}}\_{\mathrm{GD},t}^{0})\bigr)\,{\mbox{$\Sigma$}}\_{uu}^{-1}{\mbox{$Z$}}\_{\mathrm{GD},t}^{0\prime}\mathclose{}\right] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =ğ”¼[ğ’GD,t0ğšºuâ€‹uâˆ’1ğšºuâ€‹uğšºuâ€‹uâˆ’1ğ’GD,t0â£â€²]=ğ”¼[ğ’GD,t0ğšºuâ€‹uâˆ’1ğ’GD,t0â£â€²]=ğ‘¸GD0.\displaystyle={\mathbb{E}}\!\mathopen{}\left[{\mbox{$Z$}}\_{\mathrm{GD},t}^{0}{\mbox{$\Sigma$}}\_{uu}^{-1}{\mbox{$\Sigma$}}\_{uu}{\mbox{$\Sigma$}}\_{uu}^{-1}{\mbox{$Z$}}\_{\mathrm{GD},t}^{0\prime}\mathclose{}\right]={\mathbb{E}}[{\mbox{$Z$}}\_{\mathrm{GD},t}^{0}{\mbox{$\Sigma$}}\_{uu}^{-1}{\mbox{$Z$}}\_{\mathrm{GD},t}^{0\prime}]={\mbox{$Q$}}\_{\mathrm{GD}}^{0}. |  |

Therefore, with ğ‘½:=(ğ‘¸GD0)âˆ’1{\mbox{$V$}}:=({\mbox{$Q$}}\_{\mathrm{GD}}^{0})^{-1},

|  |  |  |
| --- | --- | --- |
|  | Teffâ€‹(ğœ¿^â€‰0âˆ’ğœ¿)â†’ğ‘‘ğ’©â€‹(ğŸ,ğ‘½).\sqrt{T\_{\mathrm{eff}}}\,(\widehat{{\mbox{$\kappa$}}}^{\,0}-{\mbox{$\kappa$}})\xrightarrow{d}\mathcal{N}({\mbox{$0$}},{\mbox{$V$}}). |  |

Define the feasible generalized-Durbin transformation as in the theorem and proceed exactly as in your original proof to show
ğœ¿^GDâˆ’ğœ¿^â€‰0=opâ€‹(1)\widehat{{\mbox{$\kappa$}}}^{\mathrm{GD}}-\widehat{{\mbox{$\kappa$}}}^{\,0}=o\_{p}(1) (and under Teff\sqrt{T\_{\mathrm{eff}}}-consistency of first-step pieces,
Teffâ€‹(ğœ¿^GDâˆ’ğœ¿^â€‰0)=opâ€‹(1)\sqrt{T\_{\mathrm{eff}}}(\widehat{{\mbox{$\kappa$}}}^{\mathrm{GD}}-\widehat{{\mbox{$\kappa$}}}^{\,0})=o\_{p}(1)).
Combining yields the stated limit distribution with ğ‘½=(ğ‘¸GD0)âˆ’1{\mbox{$V$}}=({\mbox{$Q$}}\_{\mathrm{GD}}^{0})^{-1}.

The efficiency argument under Eâ€‹Bâ€‹DEBD is unchanged from your original proof. âˆ

### A.11 Proof of Corollary1

By TheoremÂ [1](https://arxiv.org/html/2601.21272v1#Thmtheorem1 "Theorem 1: â€£ 2.5 Asymptotic properties of generalized Durbin estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models"),

|  |  |  |
| --- | --- | --- |
|  | Teffâ€‹(ğœ¿^GDâˆ’ğœ¿)â†’ğ‘‘ğ’©â€‹(ğŸ,ğ‘½),ğ‘½=(ğ”¼â€‹[ğ’GD,tâ€‹ğšºuâ€‹uâˆ’1â€‹ğ’GD,tâ€²])âˆ’1,\sqrt{T\_{\mathrm{eff}}}\,(\widehat{{\mbox{$\kappa$}}}^{\mathrm{GD}}-{\mbox{$\kappa$}})\ \xrightarrow{d}\ \mathcal{N}({\mbox{$0$}},{\mbox{$V$}}),\qquad{\mbox{$V$}}=\Big({\mathbb{E}}[{\mbox{$Z$}}\_{\mathrm{GD},t}{\mbox{$\Sigma$}}\_{uu}^{-1}{\mbox{$Z$}}\_{\mathrm{GD},t}^{\prime}]\Big)^{-1}, |  |

where ğ‘½V is positive definite. Under H0:ğ‘¹ğœ¿=ğ’“H\_{0}:\ {\mbox{$R$}}{\mbox{$\kappa$}}={\mbox{$r$}},

|  |  |  |
| --- | --- | --- |
|  | ğ‘¹â€‹ğœ¿^GDâˆ’ğ’“=ğ‘¹â€‹(ğœ¿^GDâˆ’ğœ¿),{\mbox{$R$}}\widehat{{\mbox{$\kappa$}}}^{\mathrm{GD}}-{\mbox{$r$}}={\mbox{$R$}}(\widehat{{\mbox{$\kappa$}}}^{\mathrm{GD}}-{\mbox{$\kappa$}}), |  |

and hence, by linearity,

|  |  |  |
| --- | --- | --- |
|  | Teffâ€‹(ğ‘¹â€‹ğœ¿^GDâˆ’ğ’“)â†’ğ‘‘ğ’©â€‹(ğŸ,ğ‘¹ğ‘½ğ‘¹â€²).\sqrt{T\_{\mathrm{eff}}}\,({\mbox{$R$}}\widehat{{\mbox{$\kappa$}}}^{\mathrm{GD}}-{\mbox{$r$}})\ \xrightarrow{d}\ \mathcal{N}\big({\mbox{$0$}},\ {\mbox{$R$}}{\mbox{$V$}}{\mbox{$R$}}^{\prime}\big). |  |

Let ğ‘½âˆ—:=ğ‘¹ğ‘½ğ‘¹â€²{\mbox{$V$}}^{\*}:={\mbox{$R$}}{\mbox{$V$}}{\mbox{$R$}}^{\prime}. Since rankâ¡(ğ‘¹)=q\operatorname{rank}({\mbox{$R$}})=q and ğ‘½>0{\mbox{$V$}}>0, it follows that ğ‘½âˆ—>0{\mbox{$V$}}^{\*}>0.

Moreover, by TheoremÂ [1](https://arxiv.org/html/2601.21272v1#Thmtheorem1 "Theorem 1: â€£ 2.5 Asymptotic properties of generalized Durbin estimator â€£ 2 Model and Test â€£ Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models") we have ğ‘½^â†’ğ‘ğ‘½\widehat{{\mbox{$V$}}}\xrightarrow{p}{\mbox{$V$}}, and thus
ğ‘½^âˆ—:=ğ‘¹â€‹ğ‘½^â€‹ğ‘¹â€²â†’ğ‘ğ‘½âˆ—\widehat{{\mbox{$V$}}}^{\*}:={\mbox{$R$}}\widehat{{\mbox{$V$}}}{\mbox{$R$}}^{\prime}\xrightarrow{p}{\mbox{$V$}}^{\*}.
Therefore, using the (symmetric) matrix square root, (ğ‘½^âˆ—)âˆ’1/2â†’ğ‘(ğ‘½âˆ—)âˆ’1/2(\widehat{{\mbox{$V$}}}^{\*})^{-1/2}\xrightarrow{p}({\mbox{$V$}}^{\*})^{-1/2}, and Slutskyâ€™s theorem yields

|  |  |  |
| --- | --- | --- |
|  | (ğ‘½^âˆ—)âˆ’1/2â€‹Teffâ€‹(ğ‘¹â€‹ğœ¿^GDâˆ’ğ’“)â†’ğ‘‘ğ’©â€‹(ğŸ,ğ‘°q).(\widehat{{\mbox{$V$}}}^{\*})^{-1/2}\,\sqrt{T\_{\mathrm{eff}}}\,({\mbox{$R$}}\widehat{{\mbox{$\kappa$}}}^{\mathrm{GD}}-{\mbox{$r$}})\ \xrightarrow{d}\ \mathcal{N}({\mbox{$0$}},{\mbox{$I$}}\_{q}). |  |

Finally, by the continuous mapping theorem applied to the quadratic form,

|  |  |  |
| --- | --- | --- |
|  | ğ’²GD=Teffâ€‹(ğ‘¹â€‹ğœ¿^GDâˆ’ğ’“)â€²â€‹(ğ‘½^âˆ—)âˆ’1â€‹(ğ‘¹â€‹ğœ¿^GDâˆ’ğ’“)=â€–(ğ‘½^âˆ—)âˆ’1/2â€‹Teffâ€‹(ğ‘¹â€‹ğœ¿^GDâˆ’ğ’“)â€–2â†’ğ‘‘Ï‡q2.\mathcal{W}^{\mathrm{GD}}=T\_{\mathrm{eff}}({\mbox{$R$}}\widehat{{\mbox{$\kappa$}}}^{\mathrm{GD}}-{\mbox{$r$}})^{\prime}(\widehat{{\mbox{$V$}}}^{\*})^{-1}({\mbox{$R$}}\widehat{{\mbox{$\kappa$}}}^{\mathrm{GD}}-{\mbox{$r$}})=\big\|(\widehat{{\mbox{$V$}}}^{\*})^{-1/2}\sqrt{T\_{\mathrm{eff}}}({\mbox{$R$}}\widehat{{\mbox{$\kappa$}}}^{\mathrm{GD}}-{\mbox{$r$}})\big\|^{2}\ \xrightarrow{d}\ \chi^{2}\_{q}. |  |

âˆ