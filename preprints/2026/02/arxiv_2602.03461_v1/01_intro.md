---
authors:
- Philipp J. Schneider
- Daniel Kuhn
doc_id: arxiv:2602.03461v1
family_id: arxiv:2602.03461
is_current: true
taxonomy:
  alpha_families: []
  asset_classes: []
  horizons: []
  themes: []
title: Soft-Radial Projection for Constrained End-to-End Learning
url_abs: http://arxiv.org/abs/2602.03461v1
url_html: https://arxiv.org/html/2602.03461v1
venue: arXiv q-fin
version: 1
year: 2026
---


Philipp J.Â Schneider â€ƒDaniel Kuhn
Risk Analytics and Optimization Chair


 EPFL
{philipp.schneider, daniel.kuhn}@epfl.ch

###### Abstract

Integrating hard constraints into deep learning is essential for safety-critical systems. Yet existing constructive layers that project predictions onto constraint boundaries face a fundamental bottleneck: *gradient saturation*. By collapsing exterior points onto lower-dimensional surfaces, standard orthogonal projections induce rank-deficient Jacobians, which nullify gradients orthogonal to active constraints and hinder optimization. We introduce Soft-Radial Projection, a differentiable reparameterization layer that circumvents this issue through a radial mapping from Euclidean space into the *interior* of the feasible set. This construction guarantees strict feasibility while preserving a full-rank Jacobian almost everywhere, thereby preventing the optimization stalls typical of boundary-based methods. We theoretically prove that the architecture retains the universal approximation property and empirically show improved convergence behavior and solution quality over state-of-the-art optimization- and projection-based baselines.

## 1 Introduction

Many decision-making systems operate under hard constraints during both training and deploymentâ€”for example, safety envelopes in autonomous driving, actuator limits in robotics (Brunke et al., [2022](https://arxiv.org/html/2602.03461v1#bib.bib47 "Safe learning in robotics: From learning-based control to safe reinforcement learning")), or budget and capacity constraints in operations. While neural networks are powerful function approximators, they are not inherently constraint-aware, and naive usage can lead to *infeasible* outputs precisely in scenarios where constraint violations have serious consequences. We formalize constrained learning objectives and propose architectural mechanisms that enforce feasibility *by construction*.

Setup. Let ğ’µâŠ†â„d\mathcal{Z}\subseteq\mathbb{R}^{d} be the input space, ğ’´âŠ†â„n\mathcal{Y}\subseteq\mathbb{R}^{n} the target space, and ğ’âŠ†â„n\mathcal{C}\subseteq\mathbb{R}^{n} the feasible set of decisions, assumed closed and convex with a nonempty interior. This assumption is without loss of generality, as we can always restrict the ambient space to the affine hull of the convex set. Data are drawn from a distribution â„™\mathbb{P} on ğ’µÃ—ğ’´\mathcal{Z}\times\mathcal{Y} with marginal â„™Z\mathbb{P}\_{Z} on ğ’µ\mathcal{Z}. We define the policy space Î \Pi as the set of all measurable mappings Ï€:ğ’µâ†’â„n\pi:\mathcal{Z}\to\mathbb{R}^{n} that map an input zz to a decision.

Supervised learning. Let (z,y)âˆ¼â„™(z,y)\sim\mathbb{P}. We learn an optimal policy by solving

|  |  |  |  |
| --- | --- | --- | --- |
|  | minÏ€âˆˆÎ ğ”¼(z,y)âˆ¼â„™â€‹[â„“â€‹(Ï€â€‹(z),y)]s.t.Ï€â€‹(z)âˆˆğ’forÂ â„™Z-a.e.Â â€‹z,\begin{array}[]{cl}\displaystyle\min\_{\pi\in\Pi}&\displaystyle\mathbb{E}\_{(z,y)\sim\mathbb{P}}\big[\ell\big(\pi(z),y\big)\big]\\ \text{s.t.}&\pi(z)\in\mathcal{C}\quad\text{for $\mathbb{P}\_{Z}$-a.e.\ }z,\end{array} |  | (1) |

where â„“:ğ’Ã—ğ’´â†’â„\ell:\mathcal{C}\times\mathcal{Y}\to\mathbb{R} is a chosen loss. This formulation covers standard regression tasks as well as imitation learning, where the label yy represents a hindsight-optimal decision Ï€â‹†â€‹(z)\pi^{\star}(z). In such cases, â„“\ell quantifies the distance to the optimal decision or a regret surrogate.

Task-driven learning. Without labels, we measure performance directly via a task cost c:ğ’µÃ—ğ’â†’â„c:\mathcal{Z}\times\mathcal{C}\to\mathbb{R} and optimize

|  |  |  |  |
| --- | --- | --- | --- |
|  | minÏ€âˆˆÎ ğ”¼zâˆ¼â„™Zâ€‹[câ€‹(z,Ï€â€‹(z))]s.t.Ï€â€‹(z)âˆˆğ’forÂ â„™Z-a.e.Â â€‹z.\begin{array}[]{cl}\displaystyle\min\_{\pi\in\Pi}&\displaystyle\mathbb{E}\_{z\sim\mathbb{P}\_{Z}}\big[c\big(z,\pi(z)\big)\big]\\ \text{s.t.}&\pi(z)\in\mathcal{C}\quad\text{for $\mathbb{P}\_{Z}$-a.e.\ }z.\end{array} |  | (2) |

This formulation aligns the training objective with the downstream operational cost, thereby circumventing the potential objective mismatch inherent in surrogate supervised targetsÂ (Donti et al., [2017](https://arxiv.org/html/2602.03461v1#bib.bib48 "Task-based end-to-end model learning in stochastic optimization"); Elmachtoub and Grigas, [2022](https://arxiv.org/html/2602.03461v1#bib.bib49 "Smart â€œpredict, then optimizeâ€"); Rychener et al., [2023](https://arxiv.org/html/2602.03461v1#bib.bib22 "End-to-end learning for stochastic optimization: A Bayesian perspective"); Chenreddy and Delage, [2024](https://arxiv.org/html/2602.03461v1#bib.bib56 "End-to-end conditional robust optimization")).

Feasibility by construction. Optimization over the space of constrained functions is generally intractable. However, we can simplify the problem by leveraging the *Interchangeability Principle* (Rockafellar and Wets, [2009](https://arxiv.org/html/2602.03461v1#bib.bib37 "Variational Analysis"), Theorem 14.60). Applied to the task-driven formulation ([2](https://arxiv.org/html/2602.03461v1#S1.E2 "Equation 2 â€£ 1 Introduction â€£ Soft-Radial Projection for Constrained End-to-End Learning")), this principle establishes that minimizing the expected cost is equivalent to minimizing the cost pointwise for almost every input zz, formally infÏ€âˆˆÎ ,Ï€â€‹(z)âˆˆğ’ğ”¼zâˆ¼â„™Zâ€‹[câ€‹(z,Ï€â€‹(z))]=ğ”¼zâˆ¼â„™Zâ€‹[infxâˆˆğ’câ€‹(z,x)]\inf\_{\pi\in\Pi,\,\pi(z)\in\mathcal{C}}\mathbb{E}\_{z\sim\mathbb{P}\_{Z}}[c(z,\pi(z))]=\mathbb{E}\_{z\sim\mathbb{P}\_{Z}}\left[\inf\_{x\in\mathcal{C}}c(z,x)\right]. This equivalence allows us to solve the functional problem by finding a pointwise optimal decision for any given input zz. We enforce constraints constructively by reparameterizing the decision policy: we optimize an unconstrained candidate function g:ğ’µâ†’â„ng:\mathcal{Z}\to\mathbb{R}^{n} composed with a fixed projection operator, such that Ï€â€‹(z)=Projâ€‹(gâ€‹(z))\pi(z)=\text{Proj}(g(z)).

The geometric properties of the projection are decisive for the optimization landscape. The standard approach is the *orthogonal projection* of a vector uâˆˆâ„nu\in\mathbb{R}^{n}, denoted Pâ€‹(u)=argâ¡minvâˆˆğ’â¡â€–uâˆ’vâ€–2P(u)=\arg\min\_{v\in\mathcal{C}}\|u-v\|^{2}. While PP guarantees feasibility, it maps the entire exterior space onto the lower-dimensional boundary âˆ‚ğ’\partial\mathcal{C}. In Figure [1(a)](https://arxiv.org/html/2602.03461v1#S1.F1.sf1 "Figure 1(a) â€£ Figure 1 â€£ 1 Introduction â€£ Soft-Radial Projection for Constrained End-to-End Learning"), we demonstrate the phenomenon of gradient saturation. This occurs when the unconstrained output u=gâ€‹(z)u=g(z) falls outside ğ’\mathcal{C} and the level sets of the objective are orthogonal to the boundary. Because PP collapses the exterior space onto the surface âˆ‚ğ’\partial\mathcal{C}, infinitesimal variations of uu orthogonal to the boundary result in zero change to the output Ï€â€‹(z)\pi(z). Consequently, gradient components in these directions are nullified, and the optimization dynamics stall or crawl along the boundary. While the projection is idempotentâ€”preserving the landscape for points already in the interiorâ€”the optimization landscape for exterior points uâˆ‰ğ’u\notin\mathcal{C} becomes rank-deficient.

![Refer to caption](x1.png)


(a) Orthogonal projection Pâ€‹(u)P(u).

![Refer to caption](x2.png)


(b) Soft-radial projection pâ€‹(u)p(u) into Intâ¡(ğ’)\operatorname{Int}(\mathcal{C}).

Figure 1: Impact of projection geometry on optimization. Comparison of (a) Orthogonal and (b) Soft-Radial Projection on a 2D constrained task (target Ã—\times). Main plots visualize the trajectory of the unconstrained candidate (âˆ˜\circ) versus the feasible decision (++). Insets show the coordinate grid warping (A) and training loss (B). Note that soft-radial projection prevents gradient saturation by maintaining descent signals for infeasible inputs.

Soft-Radial Projection to the interior. We instantiate the projection layer as a *Soft-Radial Projection* p:â„nâ†’Intâ¡(ğ’)p:\mathbb{R}^{n}\to\operatorname{Int}(\mathcal{C}) constructed as a radial homeomorphism. Unlike the orthogonal projection, this map keeps every output strictly inside the interior Intâ¡(ğ’)\operatorname{Int}(\mathcal{C}), acting as a smooth reparameterization of the feasible set rather than a distance-minimizing operator. By enforcing a diffeomorphic correspondence almost everywhere, we ensure well-conditioned gradients that guide the optimizer efficiently through the feasible regionâ€”and asymptotically toward the boundary if necessaryâ€”without the saturation artifacts of the orthogonal projection (cf.Â Figure [1](https://arxiv.org/html/2602.03461v1#S1.F1 "Figure 1 â€£ 1 Introduction â€£ Soft-Radial Projection for Constrained End-to-End Learning")).

Related work.
The challenge of obtaining feasibility guarantees for neural networks has attracted broad interest across research domains, ranging from safety-critical control (GarcÄ±a and FernÃ¡ndez, [2015](https://arxiv.org/html/2602.03461v1#bib.bib51 "A comprehensive survey on safe reinforcement learning")) to decision-focused analytics (Kotary et al., [2021](https://arxiv.org/html/2602.03461v1#bib.bib52 "End-to-end constrained optimization learning: A survey")). For an extensive review, we refer to AppendixÂ [A](https://arxiv.org/html/2602.03461v1#A1 "Appendix A Related Work â€£ Soft-Radial Projection for Constrained End-to-End Learning"). Existing methodologies can be broadly categorized into optimization-based layers (e.g., Amos and Kolter, [2017](https://arxiv.org/html/2602.03461v1#bib.bib30 "Optnet: Differentiable optimization as a layer in neural networks"); Agrawal et al., [2019](https://arxiv.org/html/2602.03461v1#bib.bib18 "Differentiable convex optimization layers"); Donti et al., [2021](https://arxiv.org/html/2602.03461v1#bib.bib16 "DC3: A learning method for optimization with hard constraints")), which operate via implicit differentiation through iterative solvers, and constructive layers that enforce feasibility by design via specialized projections (e.g., Konstantinov and Utkin, [2023](https://arxiv.org/html/2602.03461v1#bib.bib3 "A new computationally simple approach for implementing neural networks with output hard constraints"); Liang et al., [2024](https://arxiv.org/html/2602.03461v1#bib.bib7 "Homeomorphic projection to ensure neural-network solution feasibility for constrained optimization")). Crucially, the current research frontier has shifted beyond merely guaranteeing constraint satisfaction (Dalal et al., [2018](https://arxiv.org/html/2602.03461v1#bib.bib21 "Safe exploration in continuous action spaces")) to ensuring smooth optimization landscapes that facilitate superior learning performance.

Contributions. Our main contributions are as follows:

* â€¢

  We introduce Soft-Radial Projection, a closed-form layer for convex sets that ensures strict interior feasibility via a radial transformation (differentiable almost everywhere).
* â€¢

  We provide a theoretical analysis of the gradient dynamics, showing that our construction avoids the rank-deficient Jacobian issues inherent to orthogonal projections.
* â€¢

  We prove that the architecture preserves the Universal Approximation property (TheoremÂ [4](https://arxiv.org/html/2602.03461v1#S4 "4 Constrained Universal Approximation â€£ Soft-Radial Projection for Constrained End-to-End Learning")) for continuous functions mapping into Intâ¡(ğ’)\operatorname{Int}(\mathcal{C}).
* â€¢

  Empirical results demonstrate the effectiveness of our method in end-to-end learning tasks, including portfolio optimization and resource allocation for demand sharing.

We proceed by formally introducing the soft-radial projection mechanism.

Notation.
We use â€–uâ€–\|u\| for the Euclidean norm of vectors and â€–Aâ€–\|A\| for the spectral norm of matrices. For functions Ïˆ:ğ’µâ†’â„n\psi:\mathcal{Z}\to\mathbb{R}^{n}, we denote the uniform norm by â€–Ïˆâ€–âˆâ‰”supzâˆˆğ’µâ€–Ïˆâ€‹(z)â€–\|\psi\|\_{\infty}\coloneqq\sup\_{z\in\mathcal{Z}}\|\psi(z)\|. We write Intâ¡(ğ’)\operatorname{Int}(\mathcal{C}) and âˆ‚ğ’\partial\mathcal{C} for the interior and boundary of the feasible set. The Jacobian of a differentiable map Ï•\phi is denoted by JÏ•J\_{\phi}. We reserve uu for unconstrained vectors (pre-projection) and use xx or Ï€â€‹(z)\pi(z) for feasible vectors in ğ’\mathcal{C}.

## 2 Soft-Radial Projection Layer

Many models output a raw vector in â„n\mathbb{R}^{n} that must satisfy hard constraints at inference timeâ€”e.g., neural networks and decoders, control policies, differentiable optimization layers, or amortized solvers. We place a *soft-radial projection layer* p:â„nâ†’Intâ¡(ğ’)p:\mathbb{R}^{n}\to\operatorname{Int}(\mathcal{C}) downstream of such modules so that, regardless of how the raw output is produced (even if infeasible), the final decision lies in the feasible set.

Concretely, we instantiate the unconstrained candidate gg as a parameterized map gÎ¸:ğ’µâ†’â„ng\_{\theta}:\mathcal{Z}\to\mathbb{R}^{n} (e.g., a neural network). Given the raw output u=gÎ¸â€‹(z)u=g\_{\theta}(z), we set the policy as Ï€Î¸=pâˆ˜gÎ¸\pi\_{\theta}=p\circ g\_{\theta}, which ensures Ï€Î¸â€‹(z)âˆˆIntâ¡(ğ’)\pi\_{\theta}(z)\in\operatorname{Int}(\mathcal{C}) during training and deployment without penalty tuning or post-hoc repair. The learning objectivesÂ ([1](https://arxiv.org/html/2602.03461v1#S1.E1 "Equation 1 â€£ 1 Introduction â€£ Soft-Radial Projection for Constrained End-to-End Learning")) and ([2](https://arxiv.org/html/2602.03461v1#S1.E2 "Equation 2 â€£ 1 Introduction â€£ Soft-Radial Projection for Constrained End-to-End Learning")) are then instantiated by composing with pp (e.g., â„“â€‹(Ï€Î¸â€‹(z),y)\ell(\pi\_{\theta}(z),y) or câ€‹(z,Ï€Î¸â€‹(z))c(z,\pi\_{\theta}(z))). This composition turns any base predictor into a constraint-satisfying model while keeping gradients usable for end-to-end training. The remainder of this section develops the geometry and regularity of pp needed for gradient-based methods. We first formalize the soft-radial projection pp and its raywise components. The construction relies on a smooth contraction toward an anchor point, mapping the entire ambient space into the interior of ğ’\mathcal{C} while retaining differentiability almost everywhere.

###### Definition 2.1 (Soft-Radial Projection).

Let ğ’âŠ†â„n\mathcal{C}\subseteq\mathbb{R}^{n} be a closed, convex set with nonempty interior, and fix an anchor point u0âˆˆIntâ¡(ğ’)u\_{0}\in\operatorname{Int}(\mathcal{C}). Let r:â„+â†’[0,1)r:\mathbb{R}\_{+}\to[0,1) be continuous and strictly increasing with râ€‹(0)>0r(0)>0 and limÏâ†’âˆrâ€‹(Ï)=1\lim\_{\rho\to\infty}r(\rho)=1.

For any uâˆˆâ„nu\in\mathbb{R}^{n}, the *soft-radial projection* of uu onto ğ’\mathcal{C} is

|  |  |  |
| --- | --- | --- |
|  | pâ€‹(u)=u0+râ€‹(â€–uâˆ’u0â€–2)â€‹(qâ€‹(u)âˆ’u0),p(u)\;=\;u\_{0}\;+\;r\!\big(\|u-u\_{0}\|^{2}\big)\,\big(q(u)-u\_{0}\big), |  |

where qâ€‹(u)q(u) denotes the *hard radial projection* of uu onto ğ’\mathcal{C},

|  |  |  |
| --- | --- | --- |
|  | qâ€‹(u)={u,ifÂ â€‹uâˆˆğ’,u0+Î±â‹†â€‹(u)â€‹(uâˆ’u0),otherwise.q(u)\;=\;\begin{cases}u,&\text{if }u\in\mathcal{C},\\[4.0pt] u\_{0}+\alpha^{\star}(u)(u-u\_{0}),&\text{otherwise}.\end{cases} |  |

The scaling factor Î±â‹†â€‹(u)\alpha^{\star}(u) is defined as

|  |  |  |
| --- | --- | --- |
|  | Î±â‹†â€‹(u)=sup{Î±âˆˆ[0,1]:u0+Î±â€‹(uâˆ’u0)âˆˆğ’}âˆˆ[0,1].\alpha^{\star}(u)\;=\;\sup\{\alpha\in[0,1]:\;u\_{0}+\alpha(u-u\_{0})\in\mathcal{C}\}\in[0,1]. |  |

###### Remark 2.2.

The mapping qq projects uu onto ğ’\mathcal{C} along the ray emanating from the anchor u0u\_{0}, in contrast to the usual Euclidean (orthogonal) projection. The soft-radial projection pp then interpolates between u0u\_{0} and qâ€‹(u)q(u), with interpolation factor governed by râ€‹(â€–uâˆ’u0â€–2)r(\|u-u\_{0}\|^{2}) (see Fig.Â [2](https://arxiv.org/html/2602.03461v1#S2.F2 "Figure 2 â€£ 2 Soft-Radial Projection Layer â€£ Soft-Radial Projection for Constrained End-to-End Learning")). If we drop the strict-interior requirement and allow râ‰¡1r\equiv 1, then pâ€‹(u)=qâ€‹(u)p(u)=q(u) and the soft-radial projection takes the form of the hard radial projection.

![Refer to caption](x3.png)


Figure 2: Geometric intuition. A ray from anchor u0u\_{0} through input uu intersects the boundary âˆ‚ğ’\partial\mathcal{C} at the hard radial projection qâ€‹(u)q(u). Our soft-radial mapping pâ€‹(u)p(u) strictly enforces feasibility by smoothly interpolating along the segment [u0,qâ€‹(u)][u\_{0},q(u)] via the radial weight rr.

Coordinate convention. The construction of pp is invariant under translations of both the input and the set: replacing uu by uâˆ’u0u-u\_{0} and ğ’\mathcal{C} by ğ’âˆ’{u0}\mathcal{C}-\{u\_{0}\} leaves all statements below unchanged. In the proofs, we therefore assume, without loss of generality, that the anchor is at the origin, 0âˆˆIntâ¡(ğ’)0\in\operatorname{Int}(\mathcal{C}), so that qâ€‹(u)=uq(u)=u for uâˆˆğ’u\in\mathcal{C}, qâ€‹(u)=Î±â‹†â€‹(u)â€‹uq(u)=\alpha^{\star}(u)\,u otherwise, and the soft-radial projection simply reads pâ€‹(u)=râ€‹(â€–uâ€–2)â€‹qâ€‹(u)p(u)=r(\|u\|^{2})\,q(u). For a general anchor u0u\_{0}, one recovers the global form by applying these statements to ğ’âˆ’{u0}\mathcal{C}-\{u\_{0}\} and uâˆ’u0u-u\_{0}, and then translating back by u0u\_{0}.

Regularity of components.
We first record basic properties of the ray map qq and the scalar Î±â‹†\alpha^{\star}. These facts will be used later to build a raywise parametrization of Intâ¡(ğ’)\operatorname{Int}(\mathcal{C}) and to analyze â„“âˆ˜p\ell\circ p with first-order methods. In particular, continuity and local Lipschitz continuity of qq ensure that composing losses with pp preserves standard first-order optimization guarantees on compact sublevel sets.

{restatable}

lemmaRayRegularity
(Ray intersection and continuity). Let ğ’âŠ‚â„n\mathcal{C}\subset\mathbb{R}^{n} be nonempty, closed, and convex with nonempty interior, and assume 0âˆˆIntâ¡(ğ’)0\in\operatorname{Int}(\mathcal{C}). For any uâˆˆâ„nu\in\mathbb{R}^{n}, the set

|  |  |  |
| --- | --- | --- |
|  | {Î±âˆˆ[0,1]:Î±â€‹uâˆˆğ’}\{\alpha\in[0,1]:\alpha u\in\mathcal{C}\} |  |

is a nonempty closed interval [0,Î±â‹†â€‹(u)][0,\alpha^{\star}(u)] for a unique Î±â‹†â€‹(u)âˆˆ[0,1]\alpha^{\star}(u)\in[0,1], and qâ€‹(u)âˆˆğ’q(u)\in\mathcal{C}. Moreover, Î±â‹†\alpha^{\star} is globally Lipschitz continuous and qq is locally Lipschitz (and therefore differentiable almost everywhere by Rademacherâ€™s theorem; see, e.g.,Â Federer ([1969](https://arxiv.org/html/2602.03461v1#bib.bib26 "Geometric measure theory")), Thm.Â 3.1.6).

Structural properties of pp. We now analyze the global geometry of pp. A critical requirement is that pp must preserve the *representational capacity* of the model gg. Unlike orthogonal projections, which suffer from dimensional collapse by mapping the exterior space onto the boundary surface (a many-to-one mapping), our construction guarantees that pp induces a *one-to-one parametrization* of Intâ¡(ğ’)\operatorname{Int}(\mathcal{C}) by strictly rescaling radii along rays emanating from the origin.

###### Assumption 2.3 (Radial monotonicity).

The function r:â„+â†’[0,1)r:\mathbb{R}\_{+}\to[0,1) is C1C^{1}, strictly increasing with râ€²â€‹(Ï)>0r^{\prime}(\rho)>0 for all Ï>0\rho>0, and satisfies râ€‹(0)>0r(0)>0 and limÏâ†’âˆrâ€‹(Ï)=1\lim\_{\rho\to\infty}r(\rho)=1.

Assumption [2.3](https://arxiv.org/html/2602.03461v1#S2.Thmtheorem3 "Assumption 2.3 (Radial monotonicity). â€£ 2 Soft-Radial Projection Layer â€£ Soft-Radial Projection for Constrained End-to-End Learning") ensures that the soft-radial projection is strictly monotonic along rays, providing the topological foundation for the one-to-one mapping. This prevents distinct points on the same ray from collapsing to the same interior location. Specifically, the scalar map tâ†¦râ€‹(t2)â€‹tt\mapsto r(t^{2})t has derivative ddâ€‹tâ€‹[râ€‹(t2)â€‹t]=râ€‹(t2)+2â€‹t2â€‹râ€²â€‹(t2)\frac{d}{dt}\bigl[r(t^{2})t\bigr]\;=\;r(t^{2})+2t^{2}r^{\prime}(t^{2}).
Since rr and râ€²r^{\prime} are positive, this derivative is strictly positive for all tâ‰¥0t\geq 0, confirming that pp is an injective transformation into Intâ¡(ğ’)\operatorname{Int}(\mathcal{C}).

The mapping pp admits a radial decomposition into angular and radial components, simplifying the analysis. Since the projection preserves the direction of the ray â„+â€‹v\mathbb{R}\_{+}v (the angular component), its behavior is fully characterized by its action on the scalar distance from the originâ€”the radial component.

###### Definition 2.4 (Boundary distance).

Fix a unit direction vâˆˆâ„nv\in\mathbb{R}^{n} (â€–vâ€–=1\|v\|=1). We define the *boundary distance* tÂ¯â€‹(v)âˆˆ(0,âˆ]\bar{t}(v)\in(0,\infty] as the distance from the origin to the boundary of ğ’\mathcal{C} along vv, given by tÂ¯â€‹(v)â‰”sup{tâ‰¥0:tâ€‹vâˆˆğ’}\bar{t}(v)\coloneqq\sup\{t\geq 0:tv\in\mathcal{C}\}.

With the boundary established, the radial profile of the projection is defined by the function Ïˆv\psi\_{v}. This map distinguishes between the interior regime, where the ray remains within ğ’\mathcal{C}, and the exterior regime, where the input is scaled relative to the boundary.

###### Definition 2.5 (Scalar projection map).

For a fixed unit direction vv, define the map Ïˆv:[0,âˆ)â†’[0,âˆ)\psi\_{v}:[0,\infty)\to[0,\infty) by Ïˆvâ€‹(t)â‰”râ€‹(t2)â€‹minâ¡{t,tÂ¯â€‹(v)}.\psi\_{v}(t)\coloneqq r(t^{2})\min\{t,\bar{t}(v)\}.

By construction, the soft-radial projection satisfies pâ€‹(tâ€‹v)=Ïˆvâ€‹(t)â€‹vp(tv)=\psi\_{v}(t)v. With this decomposition, we now establish the regularity of these components. The following lemma verifies that the boundary distance varies continuously with direction and that the scalar projection is strictly monotonicâ€”properties that are essential for proving the soft-radial projection is a global bijection.

{restatable}

lemmaRegComponents
(Regularity of components). The boundary distance map tÂ¯\bar{t} is continuous on the unit sphere {v:â€–vâ€–=1}\{v:\|v\|=1\} (with values in (0,âˆ](0,\infty]). Moreover, under AssumptionÂ [2.3](https://arxiv.org/html/2602.03461v1#S2.Thmtheorem3 "Assumption 2.3 (Radial monotonicity). â€£ 2 Soft-Radial Projection Layer â€£ Soft-Radial Projection for Constrained End-to-End Learning"), for any unit direction vv, the scalar map Ïˆv\psi\_{v} is continuous and strictly increasing, and its range is exactly [0,tÂ¯â€‹(v))[0,\bar{t}(v)).

The raywise analysis in LemmaÂ [2.5](https://arxiv.org/html/2602.03461v1#S2.Thmtheorem5 "Definition 2.5 (Scalar projection map). â€£ 2 Soft-Radial Projection Layer â€£ Soft-Radial Projection for Constrained End-to-End Learning") provides the sufficient conditions to characterize the global topology of pp. We now formally state the main result: pp is a homeomorphism, acting as a reversible deformation of the entire ambient space â„n\mathbb{R}^{n} onto the feasible interior Intâ¡(ğ’)\operatorname{Int}(\mathcal{C}).

{restatable}

theoremHomeomorphism
(Homeomorphism). Under AssumptionÂ [2.3](https://arxiv.org/html/2602.03461v1#S2.Thmtheorem3 "Assumption 2.3 (Radial monotonicity). â€£ 2 Soft-Radial Projection Layer â€£ Soft-Radial Projection for Constrained End-to-End Learning"), the soft-radial projection p:â„nâ†’Intâ¡(ğ’)p:\mathbb{R}^{n}\to\operatorname{Int}(\mathcal{C}) is a homeomorphism.

This theorem confirms that the soft-radial layer preserves the topological structure of the unconstrained function space. Avoiding the dimensional collapse of hard projections, pp maintains a full-dimensional, bijective correspondence between the parameter space and the feasible policy space.

Differential regularity. Finally, to enable gradient-based learning, we require the projection to be differentiable and the resulting optimization landscape to be free of projection-induced vanishing gradients.

{restatable}

theoremDiffInvert
(Differentiability and invertibility). Under AssumptionÂ [2.3](https://arxiv.org/html/2602.03461v1#S2.Thmtheorem3 "Assumption 2.3 (Radial monotonicity). â€£ 2 Soft-Radial Projection Layer â€£ Soft-Radial Projection for Constrained End-to-End Learning"), the soft-radial projection pp is differentiable almost everywhere on â„n\mathbb{R}^{n}. The Jacobian Jpâ€‹(u)J\_{p}(u) is invertible at every point uu where it exists.

The global invertibility of JpJ\_{p} ensures that the mapping is *full-rank* almost everywhere. Consequently, the gradient signal is preserved even when the raw output is far outside ğ’\mathcal{C}, strictly avoiding the rank-deficiency and optimization stalling characteristic of standard orthogonal projections.

## 3 Optimization Guarantees

After establishing the geometry of the projection layer, we now analyze its implications for *unconstrained optimization*. Consider a standard constrained task where we seek to minimize a loss â„“â€‹(x)\ell(x) over the feasible set ğ’\mathcal{C}. By equipping the model with the soft-radial projection, we transform this into the unconstrained minimization of the *composite objective* fâ€‹(u)â‰”â„“â€‹(pâ€‹(u))f(u)\coloneqq\ell(p(u)), where uâˆˆâ„nu\in\mathbb{R}^{n} denotes the unconstrained candidate vector (e.g., the model output u=gâ€‹(z)u=g(z)).

The following discussion translates the geometric properties of pp into concrete optimization guarantees for ff, proceeding from the consistency of optimal values to algorithmic convergence rates.

Optimal value equivalence.
Our primary requirement is that solving the unconstrained formulation fâ€‹(u)f(u) is equivalent to solving the original constrained objective â„“â€‹(x)\ell(x). The homeomorphism property of pp guarantees that the global minima align, meaning no valid solutions are lost and no spurious solutions are created.

{restatable}

theoremEquivOptVal
(Equivalence of optimal values). Let â„“:ğ’â†’â„\ell:\mathcal{C}\to\mathbb{R} be continuous. With fâ‰”â„“âˆ˜pf\coloneqq\ell\circ p, we have

|  |  |  |
| --- | --- | --- |
|  | infuâˆˆâ„nfâ€‹(u)=infxâˆˆIntâ¡(ğ’)â„“â€‹(x)=infxâˆˆğ’â„“â€‹(x).\inf\_{u\in\mathbb{R}^{n}}f(u)\;=\;\inf\_{x\in\operatorname{Int}(\mathcal{C})}\ell(x)\;=\;\inf\_{x\in\mathcal{C}}\ell(x). |  |

Moreover, argâ¡minâ¡fâ‰ âˆ…\arg\min f\neq\emptyset if and only if argâ¡minğ’â¡â„“\arg\min\_{\mathcal{C}}\ell intersects Intâ¡(ğ’)\operatorname{Int}(\mathcal{C}). In that case, uâ‹†âˆˆargâ¡minâ¡fu^{\star}\in\arg\min f if and only if pâ€‹(uâ‹†)âˆˆargâ¡minğ’â¡â„“p(u^{\star})\in\arg\min\_{\mathcal{C}}\ell.

First-order correspondence. Beyond agreement of optimal values, optimization algorithms require that first-order stationarity of the unconstrained objective fâ‰”â„“âˆ˜pf\coloneqq\ell\circ p be interpretable in terms of first-order conditions for the original constrained problem. Since pâ€‹(â„n)âŠ‚Intâ¡(ğ’)p(\mathbb{R}^{n})\subset\operatorname{Int}(\mathcal{C}), the correspondence we establish is necessarily an *interior* one: it relates stationary points of ff to points xâˆˆIntâ¡(ğ’)x\in\operatorname{Int}(\mathcal{C}) satisfying âˆ‡â„“â€‹(x)=0\nabla\ell(x)=0. Boundary minimizers of â„“\ell over ğ’\mathcal{C} do not necessarily satisfy âˆ‡â„“=0\nabla\ell=0 and therefore cannot, in general, be characterized by stationarity of ff. The next statement formalizes the interior correspondence via the chain rule and the fact that JpJ\_{p} is invertible almost everywhere.

{restatable}

propositionCorrStatPoints
(Correspondence of stationary points). Assume â„“\ell is C1C^{1} on Intâ¡(ğ’)\operatorname{Int}(\mathcal{C}). Wherever pp is differentiable, with f=â„“âˆ˜pf=\ell\circ p we have

|  |  |  |
| --- | --- | --- |
|  | âˆ‡fâ€‹(u)=Jpâ€‹(u)âŠ¤â€‹âˆ‡â„“â€‹(pâ€‹(u)).\nabla f(u)\;=\;J\_{p}(u)^{\top}\,\nabla\ell\big(p(u)\big). |  |

Consequently, at any point uu where the Jacobian Jpâ€‹(u)J\_{p}(u) is invertibleâ€”which holds almost everywhere by TheoremÂ [2.5](https://arxiv.org/html/2602.03461v1#S2.Thmtheorem5 "Definition 2.5 (Scalar projection map). â€£ 2 Soft-Radial Projection Layer â€£ Soft-Radial Projection for Constrained End-to-End Learning")â€”the stationary conditions are equivalent:

|  |  |  |
| --- | --- | --- |
|  | âˆ‡fâ€‹(u)=0âŸºâˆ‡â„“â€‹(pâ€‹(u))=0.\nabla f(u)=0\quad\Longleftrightarrow\quad\nabla\ell\big(p(u)\big)=0. |  |

This implies that the unconstrained optimization landscape of ff introduces no spurious stationary points within the interior of ğ’\mathcal{C}.

*Remark.* If a minimizer xâ‹†x^{\star} of â„“\ell lies on the boundary âˆ‚ğ’\partial\mathcal{C} and satisfies âˆ‡â„“â€‹(xâ‹†)â‰ 0\nabla\ell(x^{\star})\neq 0 (as is typical for constrained optima), PropositionÂ [3](https://arxiv.org/html/2602.03461v1#S3 "3 Optimization Guarantees â€£ Soft-Radial Projection for Constrained End-to-End Learning") implies that there is no stationary point uu corresponding to xâ‹†x^{\star}. In this case, minimizing sequences for ff must diverge, â€–ukâ€–â†’âˆ\|u\_{k}\|\to\infty, pushing pâ€‹(uk)p(u\_{k}) toward the boundary.

Global optimality for convex losses. When â„“\ell is convex, any *interior* stationary point is globally optimal. Since pâ€‹(â„n)=Intâ¡(ğ’)p(\mathbb{R}^{n})=\operatorname{Int}(\mathcal{C}) and PropositionÂ [3](https://arxiv.org/html/2602.03461v1#S3 "3 Optimization Guarantees â€£ Soft-Radial Projection for Constrained End-to-End Learning") shows that, at points where JpJ\_{p} is invertible, stationarity of f=â„“âˆ˜pf=\ell\circ p is equivalent to âˆ‡â„“=0\nabla\ell=0 at the corresponding interior point, it follows that pp does not introduce spurious interior local minima.

{restatable}

corollaryGlobOptInt
(Global optimality of interior stationary points for convex losses). Assume â„“\ell is convex and C1C^{1} on Intâ¡(ğ’)\operatorname{Int}(\mathcal{C}), and let fâ‰”â„“âˆ˜pf\coloneqq\ell\circ p. If uu is a local minimizer of ff at which pp is differentiable, then pâ€‹(u)p(u) is a global minimizer of â„“\ell over ğ’\mathcal{C}.

*Remark (Anchor).* At the anchor u0âˆˆIntâ¡(ğ’)u\_{0}\in\operatorname{Int}(\mathcal{C}) we have Jpâ€‹(u0)=râ€‹(0)â€‹IJ\_{p}(u\_{0})=r(0)I, hence

|  |  |  |
| --- | --- | --- |
|  | âˆ‡(â„“âˆ˜p)â¡(u0)=Jpâ€‹(u0)âŠ¤â€‹âˆ‡â„“â€‹(pâ€‹(u0))=râ€‹(0)â€‹âˆ‡â„“â€‹(u0).\nabla(\ell\circ p)(u\_{0})=J\_{p}(u\_{0})^{\top}\nabla\ell(p(u\_{0}))=r(0)\,\nabla\ell(u\_{0}). |  |

Thus, the condition râ€‹(0)>0r(0)>0 (cf.Â Assumption [2.3](https://arxiv.org/html/2602.03461v1#S2.Thmtheorem3 "Assumption 2.3 (Radial monotonicity). â€£ 2 Soft-Radial Projection Layer â€£ Soft-Radial Projection for Constrained End-to-End Learning")) prevents the anchor from becoming an artificial stationary point.

On PL-type conditions. Since the non-linearity of the soft-radial projection pp induces non-convexity in the composite objective f=â„“âˆ˜pf=\ell\circ p, standard global convergence guarantees do not apply directly. A natural question is whether ff satisfies the weaker Polyakâ€“Åojasiewicz (PL) inequality, which would suffice to ensure linear convergence rates. However, even when â„“\ell is well-conditioned on ğ’\mathcal{C}, the saturation râ€‹(Ï)â†’1r(\rho)\to 1 as Ïâ†’âˆ\rho\to\infty can create regions where â€–âˆ‡fâ€–\|\nabla f\| becomes arbitrarily small while the suboptimality fâˆ’fâ‹†f-f^{\star} stays bounded away from 0, ruling out a global PL bound.
{restatable}lemmaAbsencePL
(Absence of a global PL inequality in general).
Even if ğ’â‰”B1â€‹(0)\mathcal{C}\coloneqq B\_{1}(0), â„“â€‹(x)â‰”â€–xâ€–2\ell(x)\coloneqq\|x\|^{2} and u0â‰”0u\_{0}\coloneqq 0, there exists no function rr satisfying AssumptionÂ [2.3](https://arxiv.org/html/2602.03461v1#S2.Thmtheorem3 "Assumption 2.3 (Radial monotonicity). â€£ 2 Soft-Radial Projection Layer â€£ Soft-Radial Projection for Constrained End-to-End Learning") such that f=â„“âˆ˜pf=\ell\circ p satisfies a global PL inequality on â„n\mathbb{R}^{n}.

Gradient methods under bounded iterates. Although a global PL inequality fails in general (LemmaÂ [3](https://arxiv.org/html/2602.03461v1#S3 "3 Optimization Guarantees â€£ Soft-Radial Projection for Constrained End-to-End Learning")), optimization in practice is typically confined to a bounded region, either *by design* (e.g., projected updates, trust regions, gradient clipping/weight decay) or *by stability* of the dynamics. Since the soft-radial projection pp depends on the constraint set ğ’\mathcal{C} through the radial map qq, global smoothness of pp (and hence of f=â„“âˆ˜pf=\ell\circ p) cannot be taken for granted for arbitrary convex ğ’\mathcal{C}. We therefore record standard convergence guarantees under the common assumption that the iterates remain in a bounded region. We consider the minimization of the expected objective Fâ€‹(Î¸)â‰”ğ”¼zâ€‹[fâ€‹(gÎ¸â€‹(z))]F(\theta)\coloneqq\mathbb{E}\_{z}[f(g\_{\theta}(z))], where f=â„“âˆ˜pf=\ell\circ p, and gÎ¸g\_{\theta} is the neural network function parametrized by Î¸âˆˆÎ˜\theta\in\Theta.

{restatable}

propositionConvergence(Convergence of stochastic gradient descent). Let Fâ€‹(Î¸)â‰”ğ”¼zâ€‹[â„“â€‹(pâ€‹(gÎ¸â€‹(z)))]F(\theta)\coloneqq\mathbb{E}\_{z}[\ell(p(g\_{\theta}(z)))] with iterates confined to a compact set KâŠ‚â„pK\subset\mathbb{R}^{p}.

1. 1.

   Smooth Regime: If â„™\mathbb{P} is absolutely continuous, FF is continuously differentiable and LL-smooth on KK. SGD with step size Î·tâˆTâˆ’1/2\eta\_{t}\propto T^{-1/2} satisfies:

   |  |  |  |
   | --- | --- | --- |
   |  | min0â‰¤t<Tâ¡ğ”¼â€‹[â€–âˆ‡Fâ€‹(Î¸t)â€–2]=ğ’ªâ€‹(Tâˆ’1/2).\min\_{0\leq t<T}\mathbb{E}[\|\nabla F(\theta\_{t})\|^{2}]=\mathcal{O}(T^{-1/2}). |  |
2. 2.

   Nonsmooth Regime: For general distributions or non-smooth losses, FF is locally Lipschitz. Stochastic Subgradient Descent with diminishing steps Î·tâ†’0\eta\_{t}\to 0 converges asymptotically to the set of Clarke stationary points:

   |  |  |  |
   | --- | --- | --- |
   |  | limTâ†’âˆğ”¼â€‹[minvâˆˆâˆ‚Fâ€‹(Î¸T)â¡â€–vâ€–]=0.\lim\_{T\to\infty}\mathbb{E}\left[\min\_{v\in\partial F(\theta\_{T})}\|v\|\right]=0. |  |

Regularity and proof intuition. The regularity of the objective Fâ€‹(Î¸)F(\theta) depends on the interplay between the data distribution and the composite function comprising the network gÎ¸g\_{\theta}, the projection pp, and the loss â„“\ell. We identify the boundary of the feasible set âˆ‚ğ’\partial\mathcal{C} (where pp exhibits kinks, see Thm.Â [2.5](https://arxiv.org/html/2602.03461v1#S2.Thmtheorem5 "Definition 2.5 (Scalar projection map). â€£ 2 Soft-Radial Projection Layer â€£ Soft-Radial Projection for Constrained End-to-End Learning")) and the activation functions within gÎ¸g\_{\theta} as the primary sources of non-differentiability. These points constitute lower-dimensional manifolds in the output space. In the *Smooth Regime*, the absolute continuity of â„™\mathbb{P} implies that the pre-activations gÎ¸â€‹(z)g\_{\theta}(z) fall on these *kinks* with probability zero. Consequently, the gradient of the expected objective is well-defined almost everywhere, and standard LL-smoothness holds on compact sets. In the *Nonsmooth Regime*, we rely on the fact that pp is locally Lipschitz. Since the composition of locally Lipschitz functions (pp, gÎ¸g\_{\theta}, â„“\ell) preserves this property, the objective admits bounded Clarke subgradients, ensuring the stability of stochastic subgradient updates.

Robustness to degeneracy. A potential concern is the *degenerate* scenario where the network gÎ¸g\_{\theta} collapses the data distribution onto the non-differentiable boundary âˆ‚ğ’\partial\mathcal{C}. In this case, the optimization effectively switches to subgradient descent. Unlike orthogonal projections where gradients may vanish (due to rank deficiency), the soft-radial projection maintains strict feasibility and non-zero subgradients at the boundary, providing valid descent directions to recover from such collapse.

## 4 Constrained Universal Approximation

We next demonstrate that the soft-radial projection preserves the expressivity of the underlying model. To establish this, let ğ’µâŠ‚â„d\mathcal{Z}\subset\mathbb{R}^{d} be a compact input space and consider a class of continuous functions ğ’¢â‰”{gÎ¸:ğ’µâ†’â„nâˆ£Î¸âˆˆÎ˜}\mathcal{G}\coloneqq\{g\_{\theta}:\mathcal{Z}\to\mathbb{R}^{n}\mid\theta\in\Theta\}, such as sufficiently deep and wide neural networksÂ (Cybenko, [1989](https://arxiv.org/html/2602.03461v1#bib.bib4 "Approximation by superpositions of a sigmoidal function"); Hornik, [1991](https://arxiv.org/html/2602.03461v1#bib.bib5 "Approximation capabilities of multilayer feedforward networks")). We assume ğ’¢\mathcal{G} is a *universal approximator* in the space of continuous functions Câ€‹(ğ’µ,â„n)C(\mathcal{Z},\mathbb{R}^{n}); that is, for any continuous function Ï•âˆˆCâ€‹(ğ’µ,â„n)\phi\in C(\mathcal{Z},\mathbb{R}^{n}) and Î´>0\delta>0, there exists a function gâˆˆğ’¢g\in\mathcal{G} such that

|  |  |  |
| --- | --- | --- |
|  | supzâˆˆğ’µâ€–gâ€‹(z)âˆ’Ï•â€‹(z)â€–â‰¤Î´.\sup\_{z\in\mathcal{Z}}\|g(z)-\phi(z)\|\leq\delta. |  |

With ğ’¢\mathcal{G} defined, we aim to show that the constrained model class {pâˆ˜gâˆ£gâˆˆğ’¢}\{p\circ g\mid g\in\mathcal{G}\} is also a universal approximator for targets in ğ’\mathcal{C}. A topological nuance here is that pp maps strictly into the *open* interior Intâ¡(ğ’)\operatorname{Int}(\mathcal{C}). We must therefore prove that this class can approximate any continuous target h:ğ’µâ†’ğ’h:\mathcal{Z}\to\mathcal{C} arbitrarily well, even when the targetâ€™s image contains points on the boundary âˆ‚ğ’\partial\mathcal{C}.

Recall from TheoremÂ [2.5](https://arxiv.org/html/2602.03461v1#S2.Thmtheorem5 "Definition 2.5 (Scalar projection map). â€£ 2 Soft-Radial Projection Layer â€£ Soft-Radial Projection for Constrained End-to-End Learning") that the soft-radial projection p:â„nâ†’Intâ¡(ğ’)p:\mathbb{R}^{n}\to\operatorname{Int}(\mathcal{C}) is a homeomorphism.

{restatable}

theoremConstUnvAppr
(Universal approximation on ğ’\mathcal{C}). Let ğ’µâŠ‚â„d\mathcal{Z}\subset\mathbb{R}^{d} be compact and let ğ’¢\mathcal{G} be a universal approximator on ğ’µ\mathcal{Z}. For every continuous function h:ğ’µâ†’ğ’h:\mathcal{Z}\to\mathcal{C} and every Îµ>0\varepsilon>0, there exists a function gâˆˆğ’¢g\in\mathcal{G} such that the constrained predictor pâˆ˜gp\circ g satisfies

|  |  |  |
| --- | --- | --- |
|  | supzâˆˆğ’µâ€–pâ€‹(gâ€‹(z))âˆ’hâ€‹(z)â€–â‰¤Îµ.\sup\_{z\in\mathcal{Z}}\|\,p(g(z))-h(z)\,\|\;\leq\;\varepsilon. |  |

Proof intuition. The argument proceeds by density and composition. First, the convexity of ğ’\mathcal{C} ensures that continuous functions mapping to the interior Intâ¡(ğ’)\operatorname{Int}(\mathcal{C}) are dense in the space of all feasible continuous functions Câ€‹(ğ’µ,ğ’)C(\mathcal{Z},\mathcal{C}); thus, any target hh admits a uniformly close approximation hÎµh\_{\varepsilon} that remains strictly interior. Second, because pp is a homeomorphism from â„n\mathbb{R}^{n} onto Intâ¡(ğ’)\operatorname{Int}(\mathcal{C}), there exists a continuous pre-image Ï•:ğ’µâ†’â„n\phi:\mathcal{Z}\to\mathbb{R}^{n} such that pâˆ˜Ï•=hÎµp\circ\phi=h\_{\varepsilon}. The result then follows by applying the universal approximation property of ğ’¢\mathcal{G} to approximate this unconstrained function Ï•\phi.

*Remark (Unbounded sets ğ’\mathcal{C}).* The compactness of the input domain ğ’µ\mathcal{Z} is crucial. It ensures that the image hâ€‹(ğ’µ)h(\mathcal{Z}) is bounded even if the feasible set ğ’\mathcal{C} itself is unbounded. Consequently, the requisite pre-images in â„n\mathbb{R}^{n} remain within a compact subset, where the uniform approximation bounds of ğ’¢\mathcal{G} apply.

Approximation stability.
Finally, we ensure that the error bounds of the base estimator transfer to the constrained model. Let KâŠ‚â„nK\subset\mathbb{R}^{n} be compact and let LKL\_{K} be a Lipschitz constant of pp on KK. For any continuous function Ï•:ğ’µâ†’â„n\phi:\mathcal{Z}\to\mathbb{R}^{n} and any approximator gâˆˆğ’¢g\in\mathcal{G} satisfying Ï•â€‹(ğ’µ)âŠ‚K\phi(\mathcal{Z})\subset K and gâ€‹(ğ’µ)âŠ‚Kg(\mathcal{Z})\subset K, we have

|  |  |  |
| --- | --- | --- |
|  | supzâˆˆğ’µâ€–pâ€‹(gâ€‹(z))âˆ’pâ€‹(Ï•â€‹(z))â€–â‰¤LKâ€‹supzâˆˆğ’µâ€–gâ€‹(z)âˆ’Ï•â€‹(z)â€–.\sup\_{z\in\mathcal{Z}}\|p(g(z))-p(\phi(z))\|\;\leq\;L\_{K}\sup\_{z\in\mathcal{Z}}\|g(z)-\phi(z)\|. |  |

This follows immediately by applying the Lipschitz bound for pp pointwise on KK and taking the supremum over zâˆˆğ’µz\in\mathcal{Z}.

*Implication.* Composition with pp preserves expressivity and transfers uniform approximation guarantees up to a constant factor. Any uniform error bound for the base class ğ’¢\mathcal{G} translates to the constrained class pâˆ˜ğ’¢p\circ\mathcal{G}, scaled by the Lipschitz constant LKL\_{K} of the projection.

## 5 Numerical Experiments

While SectionÂ [1](https://arxiv.org/html/2602.03461v1#S1 "1 Introduction â€£ Soft-Radial Projection for Constrained End-to-End Learning") emphasized safety, strict constraints are equally critical in operations where violations are prohibitive. We evaluate our framework on formulation ([2](https://arxiv.org/html/2602.03461v1#S1.E2 "Equation 2 â€£ 1 Introduction â€£ Soft-Radial Projection for Constrained End-to-End Learning")) via portfolio optimization and ride-sharing dispatch; see AppendixÂ [C](https://arxiv.org/html/2602.03461v1#A3 "Appendix C Extended Numerical Results â€£ Soft-Radial Projection for Constrained End-to-End Learning") for extended experimental details. For our implementation, we treat the projection as a differentiable layer, allowing us to train the model end-to-end by backpropagating gradients directly through the projection operation to the base network gÎ¸g\_{\theta}.111Code: <https://anonymous.4open.science/r/soft-radial-projection-e2e-icml/>

### 5.1 Implementation Details

A critical computational advantage of the soft-radial projection is that the ray-boundary intersection scalar, Î±â‹†â€‹(u)\alpha^{\star}(u), often admits an efficient closed-form solution. This contrasts sharply with standard layers based on orthogonal projection (minvâˆˆğ’â¡â€–uâˆ’vâ€–2\min\_{v\in\mathcal{C}}\|u-v\|^{2}). For feasible sets formed by intersecting constraints (such as the capped simplex), orthogonal projection requires solving a constrained quadratic program (QP) for every sample in the batch. These QPs typically lack closed-form solutions, necessitating iterative numerical solversâ€”such as Dykstraâ€™s algorithm for intersections of convex sets (Dykstra, [1983](https://arxiv.org/html/2602.03461v1#bib.bib42 "An algorithm for restricted least squares regression"); Boyle and Dykstra, [1986](https://arxiv.org/html/2602.03461v1#bib.bib41 "A method for finding projections onto the intersection of convex sets in Hilbert spaces")) or ADMM for splitting complex composite constraints (Boyd et al., [2010](https://arxiv.org/html/2602.03461v1#bib.bib43 "Distributed optimization and statistical learning via the alternating direction method of multipliers"))â€”that significantly slow down the forward pass. Our method circumvents this bottleneck by replacing the iterative QP with a direct, vectorized calculation.

Choice of radial contraction rr.
Recall from DefinitionÂ [2.1](https://arxiv.org/html/2602.03461v1#S2.Thmtheorem1 "Definition 2.1 (Soft-Radial Projection). â€£ 2 Soft-Radial Projection Layer â€£ Soft-Radial Projection for Constrained End-to-End Learning") that the radial contraction rr maps the squared distance Ïâ‰”â€–uâˆ’u0â€–2\rho\coloneqq\|u-u\_{0}\|^{2} to a scaling factor in [0,1)[0,1). We parameterize this contraction using smooth sigmoidal mappings satisfying AssumptionÂ [2.3](https://arxiv.org/html/2602.03461v1#S2.Thmtheorem3 "Assumption 2.3 (Radial monotonicity). â€£ 2 Soft-Radial Projection Layer â€£ Soft-Radial Projection for Constrained End-to-End Learning"). Fixing parameters Îµâˆˆ(0,1)\varepsilon\in(0,1) and Î»>0\lambda>0, we consider three families:

|  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- |
|  | (Rational) | râ€‹(Ï)\displaystyle r(\rho) | =Îµ+(1âˆ’Îµ)â€‹ÏÏ+Î»\displaystyle=\varepsilon+(1-\varepsilon)\frac{\rho}{\rho+\lambda} |  | (3) |
|  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- |
|  | (Exponential) | râ€‹(Ï)\displaystyle r(\rho) | =Îµ+(1âˆ’Îµ)â€‹(1âˆ’eâˆ’Ï/Î»)\displaystyle=\varepsilon+(1-\varepsilon)\big(1-e^{-\rho/\lambda}\big) |  | (4) |
|  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- |
|  | (Hyperbolic) | râ€‹(Ï)\displaystyle r(\rho) | =Îµ+(1âˆ’Îµ)â€‹tanhâ¡(Ï/Î»)\displaystyle=\varepsilon+(1-\varepsilon)\tanh(\rho/\lambda) |  | (5) |

While all three forms guarantee strict interior feasibility, they differ in their asymptotic saturation rate (see AppendixÂ [C.1](https://arxiv.org/html/2602.03461v1#A3.SS1 "C.1 Sensitivity Analysis: Radial Contraction and Scaling â€£ Appendix C Extended Numerical Results â€£ Soft-Radial Projection for Constrained End-to-End Learning")). Note that the term râ€‹(Ï)+2â€‹Ïâ€‹râ€²â€‹(Ï)r(\rho)+2\rho\,r^{\prime}(\rho) remains strictly positive bounded away from zero, ensuring the Jacobian is well-conditioned even at the anchor u0u\_{0}.

Computing the radial map. The evaluation of pâ€‹(u)p(u) reduces to computing the scalar Î±â‹†â€‹(u)\alpha^{\star}(u), defined as the distance from the anchor u0u\_{0} to the boundary âˆ‚ğ’\partial\mathcal{C} along the ray (uâˆ’u0)(u-u\_{0}).

Case 1: Polyhedra (Linear constraints). For sets defined by linear inequalities Aâ€‹xâ‰¤bAx\leq b (e.g., the simplex or capped simplex), the intersection time is computable exactly without iteration. Since u0u\_{0} is strictly interior (bâˆ’Aâ€‹u0>0b-Au\_{0}>0), the distance to the boundary is the minimum positive intersection time across all hyperplanes:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î±â‹†â€‹(u)=mini:aiâŠ¤â€‹(uâˆ’u0)>0â¡biâˆ’aiâŠ¤â€‹u0aiâŠ¤â€‹(uâˆ’u0).\alpha^{\star}(u)\;=\;\min\_{i:\,a\_{i}^{\top}(u-u\_{0})>0}\frac{b\_{i}-a\_{i}^{\top}u\_{0}}{a\_{i}^{\top}(u-u\_{0})}. |  | (6) |

This operation is ğ’ªâ€‹(m)\mathcal{O}(m) per sample and is fully parallelizable.

Case 2: Euclidean balls. For spherical constraints â€–xâˆ’câ€–2â‰¤R\|x-c\|\_{2}\leq R, the scalar Î±â‹†â€‹(u)\alpha^{\star}(u) is the unique positive root of the quadratic equation â€–(u0+tâ€‹(uâˆ’u0))âˆ’câ€–2=R2\|(u\_{0}+t(u-u\_{0}))-c\|^{2}=R^{2}, which admits a simple analytic solution.

Case 3: General convex sets. For general convex sets defined by level sets hâ€‹(x)â‰¤0h(x)\leq 0, finding Î±â‹†\alpha^{\star} is equivalent to finding the root of the scalar function Ï•â€‹(t)=hâ€‹(u0+tâ€‹(uâˆ’u0))\phi(t)=h(u\_{0}+t(u-u\_{0})). Since ğ’\mathcal{C} is convex, Ï•â€‹(t)\phi(t) is convex and monotonic along the ray. The root can therefore be found to machine precision efficiently using Newton-Raphson or Bisection methods.

Baselines. To provide insights with different neural network architectures, we consider for gÎ¸â€‹(z)g\_{\theta}(z) a multi-layer perceptron (MLP) and a Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, [1997](https://arxiv.org/html/2602.03461v1#bib.bib44 "Long short-term memory")) architecture. To benchmark the efficacy of the soft-radial projection, we compare against state-of-the-art constraint enforcement layers (Projâ€‹(gÎ¸â€‹(z))\text{Proj}(g\_{\theta}(z))) across three categories:

* â€¢

  Softmax (Simplex): For standard simplex constraints, we utilize the ubiquitous Softmax function with temperature scaling Ï„\tau. While computationally cheap, it does not generalize to arbitrary convex sets.
* â€¢

  Orthogonal projection: We compare against the standard Euclidean projection. As discussed in SectionÂ [1](https://arxiv.org/html/2602.03461v1#S1 "1 Introduction â€£ Soft-Radial Projection for Constrained End-to-End Learning"), this method is idempotentâ€”preserving the landscape for already-feasible pointsâ€”but suffers from gradient saturation when inputs lie outside ğ’\mathcal{C}.
* â€¢

  Optimization-based layers: We evaluate against *Deep Constraint Completion and Correction* (DC3) (Donti et al., [2021](https://arxiv.org/html/2602.03461v1#bib.bib16 "DC3: A learning method for optimization with hard constraints")), which enforces feasibility via gradient-based corrections, and *HardNet* (Min and Azizan, [2024](https://arxiv.org/html/2602.03461v1#bib.bib9 "HardNet: Hard-constrained neural networks with universal approximation guarantees")), a recent architecture specialized for affine constraints.222App.Â [C.2](https://arxiv.org/html/2602.03461v1#A3.SS2 "C.2 Adaptation of Competitor Methods to Capped Simplex Constraints â€£ Appendix C Extended Numerical Results â€£ Soft-Radial Projection for Constrained End-to-End Learning") describes the implementation of the respective models for the capped simplex.

### 5.2 Portfolio Optimization

We consider a financial market with NN assets over a finite horizon TT. Let (Î©,â„±,â„™)(\Omega,\mathcal{F},\mathbb{P}) be a probability space equipped with a filtration (â„±t)tâ‰¥0(\mathcal{F}\_{t})\_{t\geq 0}. At each time step tt, the agent observes signals ztz\_{t}â€“comprising raw returns augmented with rolling volatility and correlation to the market proxy computed over lookback HHâ€”and selects a portfolio weight vector wtâˆˆğ’âŠ‚â„Nw\_{t}\in\mathcal{C}\subset\mathbb{R}^{N}. Between steps tâˆ’1t-1 and tt, price relatives ytâˆˆâ„+Ny\_{t}\in\mathbb{R}^{N}\_{+} induce a passive drift, evolving weights to wtâˆ’1+=(diagâ¡(yt)â€‹wtâˆ’1)/(ytâŠ¤â€‹wtâˆ’1)w\_{t-1}^{+}=(\operatorname{diag}(y\_{t})\,w\_{t-1})/(y\_{t}^{\top}w\_{t-1}). The goal is to maximize the Sharpe ratio of the net returns RtnetR^{\text{net}}\_{t}

|  |  |  |  |
| --- | --- | --- | --- |
|  | maxğœƒğ”¼â€‹[Rtnet]Varâ¡(Rtnet)s.t.Rtnet=wtâˆ’1âŠ¤â€‹ytâˆ’Î³2â€‹â€–wtâˆ’wtâˆ’1+â€–1,âˆ€t,wt=Projğ’â€‹(gÎ¸â€‹(zt)),âˆ€t.\begin{array}[]{cl}\displaystyle\underset{\theta}{\text{max}}&\displaystyle\frac{\mathbb{E}[R^{\text{net}}\_{t}]}{\sqrt{\operatorname{Var}(R^{\text{net}}\_{t})}}\\[10.00002pt] \text{s.t.}&\displaystyle R^{\text{net}}\_{t}=w\_{t-1}^{\top}y\_{t}-\frac{\gamma}{2}\|w\_{t}-w\_{t-1}^{+}\|\_{1},\quad\forall t,\\[10.00002pt] &\displaystyle w\_{t}=\text{Proj}\_{\mathcal{C}}(g\_{\theta}(z\_{t})),\quad\forall t.\end{array} |  | (7) |

Here, Î³\gamma is the transaction cost parameter and the L1L\_{1} norm captures the turnover cost. As the L1L\_{1} norm is not differentiable at zero, we replace it in the implementation with the Pseudo-Huber loss. We employ the component-wise capped simplex, which generalizes the standard simplex by enforcing asset-specific capacities câˆˆ(Nâˆ’1,1]Nc\in(N^{-1},1]^{N} to control risk concentration
ğ’={wâˆˆâ„+N|â€„1âŠ¤â€‹w=1,wâ‰¤c}\mathcal{C}\;=\;\left\{w\in\mathbb{R}^{N}\_{+}\;\middle|\;\mathds{1}^{\top}w=1,\;w\leq c\right\}.

Results analysis. TableÂ [1](https://arxiv.org/html/2602.03461v1#S5.T1 "Table 1 â€£ 5.2 Portfolio Optimization â€£ 5 Numerical Experiments â€£ Soft-Radial Projection for Constrained End-to-End Learning") compares the net Sharpe ratio (SR) and turnover costs utilizing an LSTM architecture within an end-to-end learning framework across different constraint enforcement methods.
On the standard simplex (Î”\Delta), the soft-radial projection demonstrates superior stability, achieving a Sharpe ratio of 0.90â€‹(Â±0.03)0.90\,(\pm 0.03) and significantly outperforming Softmax (0.630.63). Notably, the standard orthogonal Projection fails to converge to a profitable strategy (0.250.25), yielding high turnover (0.480.48). This suggests that the LSTM struggles to backpropagate effective gradients through the hard projection step, likely due to vanishing gradients at the constraint boundaries.
On the capped simplex (ğ’\mathcal{C}), baseline methods (O-Proj, DC3, HardNet) cluster around a Sharpe ratio of 0.620.62â€“0.640.64 with higher variance, while the soft-radial projection maintains its performance (0.900.90) with minimal turnover (0.050.05). The reduced standard deviation (Â±0.02\pm 0.02) indicates that our method is not only more performant but also significantly more robust to initialization than competitive differentiable optimization layers.

Table 1: Portfolio results for Liquid dataset, aggregated over five seeds (mean Â±\pm std) with N=50N=50 assets, per-asset capacity c=0.05c=0.05, and transaction cost Î³=0.1\gamma=0.1; see App.Â [C.4](https://arxiv.org/html/2602.03461v1#A3.SS4 "C.4 Extended Analysis: Portfolio Optimization â€£ Appendix C Extended Numerical Results â€£ Soft-Radial Projection for Constrained End-to-End Learning").

|  |  |  |  |
| --- | --- | --- | --- |
| Feasible Set | Method | SR (Net) | Turnover |
| Î”\Delta | Softmax | 0.63â€‹(Â±0.19)0.63\,(\pm 0.19) | 0.22â€‹(Â±0.05)0.22\,(\pm 0.05) |
| O-Proj | 0.25â€‹(Â±0.18)0.25\,(\pm 0.18) | 0.48â€‹(Â±0.02)0.48\,(\pm 0.02) |
| Soft-Radial | 0.90â€‹(Â±0.03)0.90\,(\pm 0.03) | 0.06â€‹(Â±0.00)0.06\,(\pm 0.00) |
| ğ’\mathcal{C} | O-Proj | 0.64â€‹(Â±0.09)0.64\,(\pm 0.09) | 0.22â€‹(Â±0.01)0.22\,(\pm 0.01) |
| DC3 | 0.63â€‹(Â±0.08)0.63\,(\pm 0.08) | 0.23â€‹(Â±0.01)0.23\,(\pm 0.01) |
| HardNet | 0.62â€‹(Â±0.11)0.62\,(\pm 0.11) | 0.19â€‹(Â±0.01)0.19\,(\pm 0.01) |
| Soft-Radial | 0.90â€‹(Â±0.02)0.90\,(\pm 0.02) | 0.05â€‹(Â±0.00)0.05\,(\pm 0.00) |

### 5.3 Ride-Sharing Dispatch

To demonstrate generalizability, we apply our framework to ride-sharing dispatch with a *time-varying budget*. Unlike the fixed portfolio sum, the available fleet size Stâˆˆâ„•+S\_{t}\in\mathbb{N}\_{+} fluctuates over time, causing the feasible set ğ’t\mathcal{C}\_{t} to expand and contract. In practice, such constraints enforce per-zone capacity limits (e.g., congestion or emissions control) Alonso-Mora et al. ([2017](https://arxiv.org/html/2602.03461v1#bib.bib39 "On-demand high-capacity ride-sharing via dynamic trip-vehicle assignment")); Zardini et al. ([2022](https://arxiv.org/html/2602.03461v1#bib.bib40 "Analysis and control of autonomous mobility-on-demand systems")). At each time step tt, the agent observes a demand history window ztz\_{t} (augmented with sine/cosine encodings for day and week) and a current supply proxy StS\_{t}. We seek an allocation atâˆˆâ„+Na\_{t}\in\mathbb{R}^{N}\_{+} over NN zones to maximize the served rate for the upcoming interval. We optimize the empirical average:

|  |  |  |  |
| --- | --- | --- | --- |
|  | maxÎ¸1Tâ€‹âˆ‘t=1T[âˆ‘i=1NminÏ„â¡(ai,t,di,t)âˆ‘i=1Ndi,t]s.t.at=Projğ’tâ€‹(gÎ¸â€‹(zt)),âˆ€t,\begin{array}[]{cl}\displaystyle\max\_{\theta}&\displaystyle\frac{1}{T}\sum\_{t=1}^{T}\left[\frac{\sum\_{i=1}^{N}\operatorname{min}\_{\tau}(a\_{i,t},d\_{i,t})}{\sum\_{i=1}^{N}d\_{i,t}}\right]\\[15.00002pt] \text{s.t.}&a\_{t}=\text{Proj}\_{\mathcal{C}\_{t}}(g\_{\theta}(z\_{t})),\quad\forall t,\end{array} |  | (8) |

where dtd\_{t} is the realized demand in the interval following the observation window. We replace the hard minimum with the smooth SoftMin, minÏ„â¡(x,y)=âˆ’Ï„â€‹logâ¡(eâˆ’x/Ï„+eâˆ’y/Ï„)\operatorname{min}\_{\tau}(x,y)=-\tau\log(e^{-x/\tau}+e^{-y/\tau}), to maintain non-zero gradients even when supply exceeds demand (ai,t>di,ta\_{i,t}>d\_{i,t}). The constraint set is a scaled capped simplex
ğ’t={aâˆˆâ„+N|â€„1âŠ¤â€‹a=St,aâ‰¤Îºâ€‹St}\mathcal{C}\_{t}\!=\!\left\{a\in\mathbb{R}^{N}\_{+}\;\middle|\;\mathds{1}^{\top}a=S\_{t},\;a\leq\kappa S\_{t}\right\}. We implement Proj via a scaling transformation (homothety): the network output is normalized by StS\_{t}, projected onto the canonical unit capped simplex, and subsequently rescaled by StS\_{t}. This effectively maps the dynamic constraint geometry to a fixed reference polytope.

Table 2: Ride-sharing dispatch results, aggregated over five random seeds (mean Â±\pm std), for the top N=150N=150 activity zones and zone capacity Îº=0.1\kappa=0.1; see App.Â [C.5](https://arxiv.org/html/2602.03461v1#A3.SS5 "C.5 Extended Analysis: Ride-Sharing Dispatch â€£ Appendix C Extended Numerical Results â€£ Soft-Radial Projection for Constrained End-to-End Learning").

|  |  |  |
| --- | --- | --- |
| Feasible Set | Method | Served Rate |
| Î”t\Delta\_{t} | Softmax | 0.84â€‹(Â±0.00)0.84\,(\pm 0.00) |
| ğ’t\mathcal{C}\_{t} | O-Proj | 0.70â€‹(Â±0.01)0.70\,(\pm 0.01) |
| DC3 | 0.84â€‹(Â±0.00)0.84\,(\pm 0.00) |
| HardNet | 0.84â€‹(Â±0.00)0.84\,(\pm 0.00) |
| Soft-Radial | 0.84â€‹(Â±0.00)0.84\,(\pm 0.00) |

Results analysis. TableÂ [2](https://arxiv.org/html/2602.03461v1#S5.T2 "Table 2 â€£ 5.3 Ride-Sharing Dispatch â€£ 5 Numerical Experiments â€£ Soft-Radial Projection for Constrained End-to-End Learning") summarizes the served rate (demand fulfillment) using an MLP policy. Unlike the stochastic portfolio domain, the ride-sharing environment exhibits stable gradient signals, allowing the unconstrained Softmax baseline to reach a high served rate of 0.840.84. In the constrained setting (ğ’t\mathcal{C}\_{t}), we observe a performance saturation: DC3, HardNet, and our proposed soft-radial projection converge to the same solution quality (0.84Â±0.000.84\pm 0.00). This confirms that the soft-radial projection effectively recovers the optimal feasible policy, matching the performance of specialized differentiable optimization layers. Conversely, the standard orthogonal projection significantly underperforms (0.700.70), again highlighting the detrimental effect of hard boundary constraints on gradient flow during training.

## 6 Conclusion

This paper introduced soft-radial projection to address the gradient saturation inherent in orthogonal projection layers. By constructing a radial homeomorphism to the interior of a convex set, we ensure strict feasibility and model expressivity (TheoremÂ [4](https://arxiv.org/html/2602.03461v1#S4 "4 Constrained Universal Approximation â€£ Soft-Radial Projection for Constrained End-to-End Learning")) without the computational bottleneck of iterative solvers. Empirically, this approach yields superior solution quality compared to optimization-based baselines, effectively balancing theoretical rigor with practical efficiency. These results establish soft-radial projection as a robust, plug-and-play primitive for imposing hard constraints in end-to-end learning.

Limitations. Our framework relies on the convexity of the feasible set to ensure unique radial intersections, preventing immediate application to non-convex manifolds. Additionally, the method requires a known interior anchor point; while straightforward in our tested domains, automating anchor selection for arbitrary high-dimensional sets and analyzing the stability implications of dynamic anchors remain important directions for future work.

## References

* A. Agrawal, B. Amos, S. Barratt, S. Boyd, S. Diamond, and Z. Kolter (2019)
  [Differentiable convex optimization layers](https://papers.nips.cc/paper_files/paper/2019/hash/9ce3c52fc54362e22053399d3181c638-Abstract.html).
  In Advances in Neural Information Processing Systems,
   pp.Â 9562â€“9574.
  Cited by: [Â§A.1](https://arxiv.org/html/2602.03461v1#A1.SS1.p2.1 "A.1 Approximation and Optimization Approaches â€£ Appendix A Related Work â€£ Soft-Radial Projection for Constrained End-to-End Learning"),
  [Â§C.2](https://arxiv.org/html/2602.03461v1#A3.SS2.p2.11 "C.2 Adaptation of Competitor Methods to Capped Simplex Constraints â€£ Appendix C Extended Numerical Results â€£ Soft-Radial Projection for Constrained End-to-End Learning"),
  [Â§1](https://arxiv.org/html/2602.03461v1#S1.p9.1 "1 Introduction â€£ Soft-Radial Projection for Constrained End-to-End Learning").
* J. Alonso-Mora, S. Samaranayake, A. Wallar, E. Frazzoli, and D. Rus (2017)
  [On-demand high-capacity ride-sharing via dynamic trip-vehicle assignment](https://doi.org/10.1073/pnas.1611675114).
  Proceedings of the National Academy of Sciences 114 (3),  pp.Â 462â€“467.
  Cited by: [Â§5.3](https://arxiv.org/html/2602.03461v1#S5.SS3.p1.7 "5.3 Ride-Sharing Dispatch â€£ 5 Numerical Experiments â€£ Soft-Radial Projection for Constrained End-to-End Learning").
* B. Amos and J. Z. Kolter (2017)
  [Optnet: Differentiable optimization as a layer in neural networks](https://proceedings.mlr.press/v70/amos17a.html).
  In International Conference on Machine Learning,
   pp.Â 136â€“145.
  Cited by: [Â§A.1](https://arxiv.org/html/2602.03461v1#A1.SS1.p2.1 "A.1 Approximation and Optimization Approaches â€£ Appendix A Related Work â€£ Soft-Radial Projection for Constrained End-to-End Learning"),
  [Â§1](https://arxiv.org/html/2602.03461v1#S1.p9.1 "1 Introduction â€£ Soft-Radial Projection for Constrained End-to-End Learning").
* S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein (2010)
  Distributed optimization and statistical learning via the alternating direction method of multipliers.
  Foundations and TrendsÂ® in Machine Learning 3 (1),  pp.Â 1â€“122.
  Cited by: [Â§5.1](https://arxiv.org/html/2602.03461v1#S5.SS1.p1.2 "5.1 Implementation Details â€£ 5 Numerical Experiments â€£ Soft-Radial Projection for Constrained End-to-End Learning").
* J. P. Boyle and R. L. Dykstra (1986)
  [A method for finding projections onto the intersection of convex sets in Hilbert spaces](https://doi.org/10.1007/978-1-4613-9940-7_3).
  In Advances in Order Restricted Statistical Inference, R. Dykstra, T. Robertson, and F. T. Wright (Eds.),
  New York, NY,  pp.Â 28â€“47.
  Cited by: [Â§5.1](https://arxiv.org/html/2602.03461v1#S5.SS1.p1.2 "5.1 Implementation Details â€£ 5 Numerical Experiments â€£ Soft-Radial Projection for Constrained End-to-End Learning").
* J. Brodie, I. Daubechies, C. De Mol, D. Giannone, and I. Loris (2009)
  [Sparse and stable Markowitz portfolios](https://doi.org/10.1073/pnas.0904287106).
  Proceedings of the National Academy of Sciences 106 (30),  pp.Â 12267â€“12272.
  Cited by: [Â§C.4.1](https://arxiv.org/html/2602.03461v1#A3.SS4.SSS1.p4.2 "C.4.1 Discussion: Deep Portfolio Optimization vs. Classical Approaches â€£ C.4 Extended Analysis: Portfolio Optimization â€£ Appendix C Extended Numerical Results â€£ Soft-Radial Projection for Constrained End-to-End Learning").
* L.E.J. Brouwer (1912)
  Ãœber Abbildung von Mannigfaltigkeiten.
  Mathematische Annalen 71,  pp.Â 97â€“115.
  Cited by: [Â§B.1](https://arxiv.org/html/2602.03461v1#A2.SS1.10.p4.7 "Proof. â€£ B.1 Proofs of Section 2 â€£ Appendix B Proofs â€£ Soft-Radial Projection for Constrained End-to-End Learning").
* L. Brunke, M. Greeff, A. W. Hall, Z. Yuan, S. Zhou, J. Panerati, and A. P. Schoellig (2022)
  [Safe learning in robotics: From learning-based control to safe reinforcement learning](https://doi.org/10.1146/annurev-control-042920-020211).
  Annual Review of Control, Robotics, and Autonomous Systems 5 (1),  pp.Â 411â€“444.
  Cited by: [Â§1](https://arxiv.org/html/2602.03461v1#S1.p1.1 "1 Introduction â€£ Soft-Radial Projection for Constrained End-to-End Learning").
* P. Charbonnier, L. Blanc-Feraud, G. Aubert, and M. Barlaud (1994)
  [Two deterministic half-quadratic regularization algorithms for computed imaging](https://doi.org/10.1109/ICIP.1994.413553).
  In International Conference on Image Processing,
  Vol. 2,  pp.Â 168â€“172.
  Cited by: [Â§C.4.3](https://arxiv.org/html/2602.03461v1#A3.SS4.SSS3.p2.2 "C.4.3 Implementation Details â€£ C.4 Extended Analysis: Portfolio Optimization â€£ Appendix C Extended Numerical Results â€£ Soft-Radial Projection for Constrained End-to-End Learning").
* A. R. Chenreddy and E. Delage (2024)
  [End-to-end conditional robust optimization](https://openreview.net/forum?id=Oe9ngGi8Gh).
  In Uncertainty in Artificial Intelligence,
   pp.Â 736â€“748.
  Cited by: [Â§1](https://arxiv.org/html/2602.03461v1#S1.p5.2 "1 Introduction â€£ Soft-Radial Projection for Constrained End-to-End Learning").
* G. E. Constante-Flores, H. Chen, and C. Li (2025)
  [Enforcing Hard Linear Constraints in Deep Learning Models with Decision Rules](https://openreview.net/forum?id=gjiCml2CNG).
  In Advances in Neural Information Processing Systems,
  Vol. 38.
  Cited by: [Appendix A](https://arxiv.org/html/2602.03461v1#A1.p2.2 "Appendix A Related Work â€£ Soft-Radial Projection for Constrained End-to-End Learning").
* G. Cybenko (1989)
  [Approximation by superpositions of a sigmoidal function](https://doi.org/10.1007/BF02551274).
  Mathematics of Control, Signals and Systems 2 (4),  pp.Â 303â€“314.
  Cited by: [Â§4](https://arxiv.org/html/2602.03461v1#S4.p1.7 "4 Constrained Universal Approximation â€£ Soft-Radial Projection for Constrained End-to-End Learning").
* G. Dalal, K. Dvijotham, M. Vecerik, T. Hester, C. Paduraru, and Y. Tassa (2018)
  Safe exploration in continuous action spaces.
  arXiv preprint arXiv:1801.08757.
  Cited by: [Â§1](https://arxiv.org/html/2602.03461v1#S1.p9.1 "1 Introduction â€£ Soft-Radial Projection for Constrained End-to-End Learning").
* D. Davis, D. Drusvyatskiy, S. Kakade, and J. D. Lee (2020)
  [Stochastic subgradient method converges on tame functions](https://doi.org/10.1007/s10208-018-09409-5).
  Foundations of Computational Mathematics 20 (1),  pp.Â 119â€“154.
  Cited by: [itemÂ 2](https://arxiv.org/html/2602.03461v1#A2.I1.i2.p1.1 "In Regime II: Nonsmooth Optimization. â€£ B.2 Proofs of Section 3 â€£ Appendix B Proofs â€£ Soft-Radial Projection for Constrained End-to-End Learning"),
  [Â§B.2](https://arxiv.org/html/2602.03461v1#A2.SS2.SSS0.Px2.p1.1 "Regime II: Nonsmooth Optimization. â€£ B.2 Proofs of Section 3 â€£ Appendix B Proofs â€£ Soft-Radial Projection for Constrained End-to-End Learning"),
  [Â§B.2](https://arxiv.org/html/2602.03461v1#A2.SS2.SSS0.Px2.p2.3 "Regime II: Nonsmooth Optimization. â€£ B.2 Proofs of Section 3 â€£ Appendix B Proofs â€£ Soft-Radial Projection for Constrained End-to-End Learning").
* M. H. Davis and A. R. Norman (1990)
  [Portfolio selection with transaction costs](https://doi.org/10.1287/moor.15.4.676).
  Mathematics of Operations Research 15 (4),  pp.Â 676â€“713.
  Cited by: [Â§C.4.1](https://arxiv.org/html/2602.03461v1#A3.SS4.SSS1.p6.1 "C.4.1 Discussion: Deep Portfolio Optimization vs. Classical Approaches â€£ C.4 Extended Analysis: Portfolio Optimization â€£ Appendix C Extended Numerical Results â€£ Soft-Radial Projection for Constrained End-to-End Learning").
* K. Deng, H. Zhang, J. Lu, and H. Sun (2025)
  [HoP: Homeomorphic polar learning for hard constrained optimization](https://arxiv.org/abs/2502.00304).
  arXiv preprint arXiv:2502.00304.
  Cited by: [Â§A.2](https://arxiv.org/html/2602.03461v1#A1.SS2.p3.1 "A.2 Constructive Geometric Layers â€£ Appendix A Related Work â€£ Soft-Radial Projection for Constrained End-to-End Learning"),
  [Table 3](https://arxiv.org/html/2602.03461v1#A1.T3.2.2.2.2 "In Appendix A Related Work â€£ Soft-Radial Projection for Constrained End-to-End Learning").
* P. Donti, B. Amos, and J. Z. Kolter (2017)
  [Task-based end-to-end model learning in stochastic optimization](https://papers.nips.cc/paper_files/paper/2017/hash/3fc2c60b5782f641f76bcefc39fb2392-Abstract.html).
  In Advances in Neural Information Processing Systems,
  Vol. 30,  pp.Â 5490â€“5500.
  Cited by: [Â§1](https://arxiv.org/html/2602.03461v1#S1.p5.2 "1 Introduction â€£ Soft-Radial Projection for Constrained End-to-End Learning").
* P. L. Donti, D. Rolnick, and J. Z. Kolter (2021)
  [DC3: A learning method for optimization with hard constraints](https://openreview.net/forum?id=V1ZHVxJ6dSS).
  In International Conference on Learning Representations,
  Cited by: [Â§A.1](https://arxiv.org/html/2602.03461v1#A1.SS1.p2.1 "A.1 Approximation and Optimization Approaches â€£ Appendix A Related Work â€£ Soft-Radial Projection for Constrained End-to-End Learning"),
  [Table 3](https://arxiv.org/html/2602.03461v1#A1.T3.1.1.1.2 "In Appendix A Related Work â€£ Soft-Radial Projection for Constrained End-to-End Learning"),
  [Â§C.2](https://arxiv.org/html/2602.03461v1#A3.SS2.SSS0.Px1.p1.5 "Symmetric DC3 for the capped simplex. â€£ C.2 Adaptation of Competitor Methods to Capped Simplex Constraints â€£ Appendix C Extended Numerical Results â€£ Soft-Radial Projection for Constrained End-to-End Learning"),
  [Appendix C](https://arxiv.org/html/2602.03461v1#A3.p1.2 "Appendix C Extended Numerical Results â€£ Soft-Radial Projection for Constrained End-to-End Learning"),
  [Â§1](https://arxiv.org/html/2602.03461v1#S1.p9.1 "1 Introduction â€£ Soft-Radial Projection for Constrained End-to-End Learning"),
  [3rd item](https://arxiv.org/html/2602.03461v1#S5.I1.i3.p1.1 "In 5.1 Implementation Details â€£ 5 Numerical Experiments â€£ Soft-Radial Projection for Constrained End-to-End Learning").
* R. L. Dykstra (1983)
  [An algorithm for restricted least squares regression](https://doi.org/10.2307/2288193).
  Journal of the American Statistical Association 78 (384),  pp.Â 837â€“842.
  Cited by: [Â§5.1](https://arxiv.org/html/2602.03461v1#S5.SS1.p1.2 "5.1 Implementation Details â€£ 5 Numerical Experiments â€£ Soft-Radial Projection for Constrained End-to-End Learning").
* A. N. Elmachtoub and P. Grigas (2022)
  [Smart â€œpredict, then optimizeâ€](https://doi.org/10.1287/mnsc.2020.3922).
  Management Science 68 (1),  pp.Â 9â€“26.
  Cited by: [Â§1](https://arxiv.org/html/2602.03461v1#S1.p5.2 "1 Introduction â€£ Soft-Radial Projection for Constrained End-to-End Learning").
* H. Federer (1969)
  Geometric measure theory.
  Grundlehren der mathematischen Wissenschaften, Vol. 153, Springer-Verlag, Berlin, Germany.
  Cited by: [Â§2](https://arxiv.org/html/2602.03461v1#S2.p5.8 "2 Soft-Radial Projection Layer â€£ Soft-Radial Projection for Constrained End-to-End Learning").
* J. GarcÄ±a and F. FernÃ¡ndez (2015)
  [A comprehensive survey on safe reinforcement learning](https://jmlr.org/papers/v16/garcia15a.html).
  Journal of Machine Learning Research 16 (42),  pp.Â 1437â€“1480.
  Cited by: [Â§1](https://arxiv.org/html/2602.03461v1#S1.p9.1 "1 Introduction â€£ Soft-Radial Projection for Constrained End-to-End Learning").
* S. Ghadimi and G. Lan (2013)
  [Stochastic first-and zeroth-order methods for nonconvex stochastic programming](https://doi.org/10.1137/120880811).
  SIAM Journal on Optimization 23 (4),  pp.Â 2341â€“2368.
  Cited by: [Â§B.2](https://arxiv.org/html/2602.03461v1#A2.SS2.SSS0.Px1.p3.8 "Regime I: Smooth Optimization. â€£ B.2 Proofs of Section 3 â€£ Appendix B Proofs â€£ Soft-Radial Projection for Constrained End-to-End Learning").
* P. D. Grontas, A. Terpin, E. C. Balta, R. Dâ€™Andrea, and J. Lygeros (2026)
  [Pinet: Optimizing hard-constrained neural networks with orthogonal projection layers](https://openreview.net/forum?id=EJ680UQeZG).
  In International Conference on Learning Representations,
  Cited by: [Appendix A](https://arxiv.org/html/2602.03461v1#A1.p2.2 "Appendix A Related Work â€£ Soft-Radial Projection for Constrained End-to-End Learning").
* S. Hochreiter and J. Schmidhuber (1997)
  [Long short-term memory](https://doi.org/10.1162/neco.1997.9.8.1735).
  Neural Computation 9 (8),  pp.Â 1735â€“1780.
  Cited by: [Â§5.1](https://arxiv.org/html/2602.03461v1#S5.SS1.p7.2 "5.1 Implementation Details â€£ 5 Numerical Experiments â€£ Soft-Radial Projection for Constrained End-to-End Learning").
* K. Hornik (1991)
  [Approximation capabilities of multilayer feedforward networks](https://doi.org/10.1016/0893-6080(91)90009-T).
  Neural Networks 4 (2),  pp.Â 251â€“257.
  Cited by: [Â§4](https://arxiv.org/html/2602.03461v1#S4.p1.7 "4 Constrained Universal Approximation â€£ Soft-Radial Projection for Constrained End-to-End Learning").
* A. V. Konstantinov and L. V. Utkin (2023)
  [A new computationally simple approach for implementing neural networks with output hard constraints](https://doi.org/10.1134/S1064562423701077).
  In Doklady Mathematics,
  Vol. 108,  pp.Â S233â€“S241.
  Cited by: [Â§A.2](https://arxiv.org/html/2602.03461v1#A1.SS2.p4.4 "A.2 Constructive Geometric Layers â€£ Appendix A Related Work â€£ Soft-Radial Projection for Constrained End-to-End Learning"),
  [Table 3](https://arxiv.org/html/2602.03461v1#A1.T3.3.3.9.1 "In Appendix A Related Work â€£ Soft-Radial Projection for Constrained End-to-End Learning"),
  [Â§1](https://arxiv.org/html/2602.03461v1#S1.p9.1 "1 Introduction â€£ Soft-Radial Projection for Constrained End-to-End Learning").
* J. Kotary, F. Fioretto, P. Van Hentenryck, and B. Wilder (2021)
  [End-to-end constrained optimization learning: A survey](https://doi.org/10.24963/ijcai.2021/610).
  In International Joint Conference on Artificial Intelligence,
   pp.Â 4475â€“4482.
  Note: Survey Track
  Cited by: [Â§1](https://arxiv.org/html/2602.03461v1#S1.p9.1 "1 Introduction â€£ Soft-Radial Projection for Constrained End-to-End Learning").
* E. Liang, M. Chen, and S. H. Low (2024)
  [Homeomorphic projection to ensure neural-network solution feasibility for constrained optimization](http://jmlr.org/papers/v25/23-1577.html).
  Journal of Machine Learning Research 25 (329),  pp.Â 1â€“55.
  Cited by: [Â§A.2](https://arxiv.org/html/2602.03461v1#A1.SS2.p3.1 "A.2 Constructive Geometric Layers â€£ Appendix A Related Work â€£ Soft-Radial Projection for Constrained End-to-End Learning"),
  [Table 3](https://arxiv.org/html/2602.03461v1#A1.T3.3.3.8.1 "In Appendix A Related Work â€£ Soft-Radial Projection for Constrained End-to-End Learning"),
  [Â§1](https://arxiv.org/html/2602.03461v1#S1.p9.1 "1 Introduction â€£ Soft-Radial Projection for Constrained End-to-End Learning").
* H. Markowitz (1952)
  [Portfolio selection](https://doi.org/10.2307/2975974).
  Journal of Finance 7 (1),  pp.Â 77â€“91.
  Cited by: [Â§C.4.1](https://arxiv.org/html/2602.03461v1#A3.SS4.SSS1.p1.1 "C.4.1 Discussion: Deep Portfolio Optimization vs. Classical Approaches â€£ C.4 Extended Analysis: Portfolio Optimization â€£ Appendix C Extended Numerical Results â€£ Soft-Radial Projection for Constrained End-to-End Learning").
* P. MÃ¡rquez-Neila, M. Salzmann, and P. Fua (2017)
  [Imposing hard constraints on deep networks: Promises and limitations](https://infoscience.epfl.ch/entities/publication/e9a398fa-e422-4472-a028-94c90e25e061).
  arXiv preprint arXiv:1706.02025.
  Note: CVPR 2017 Workshop on Negative Results in Computer Vision
  Cited by: [Â§A.1](https://arxiv.org/html/2602.03461v1#A1.SS1.p1.1 "A.1 Approximation and Optimization Approaches â€£ Appendix A Related Work â€£ Soft-Radial Projection for Constrained End-to-End Learning").
* R. O. Michaud (1989)
  [The Markowitz optimization enigma: Is â€˜optimizedâ€™ optimal?](https://doi.org/10.2469/faj.v45.n1.31).
  Financial Analysts Journal 45 (1),  pp.Â 31â€“42.
  Cited by: [Â§C.4.1](https://arxiv.org/html/2602.03461v1#A3.SS4.SSS1.p2.5 "C.4.1 Discussion: Deep Portfolio Optimization vs. Classical Approaches â€£ C.4 Extended Analysis: Portfolio Optimization â€£ Appendix C Extended Numerical Results â€£ Soft-Radial Projection for Constrained End-to-End Learning").
* Y. Min and N. Azizan (2024)
  [HardNet: Hard-constrained neural networks with universal approximation guarantees](https://arxiv.org/abs/2410.10807).
  arXiv preprint arXiv:2410.10807.
  Cited by: [Â§A.2](https://arxiv.org/html/2602.03461v1#A1.SS2.p2.1 "A.2 Constructive Geometric Layers â€£ Appendix A Related Work â€£ Soft-Radial Projection for Constrained End-to-End Learning"),
  [Table 3](https://arxiv.org/html/2602.03461v1#A1.T3.3.3.7.1 "In Appendix A Related Work â€£ Soft-Radial Projection for Constrained End-to-End Learning"),
  [Â§C.2](https://arxiv.org/html/2602.03461v1#A3.SS2.p2.12 "C.2 Adaptation of Competitor Methods to Capped Simplex Constraints â€£ Appendix C Extended Numerical Results â€£ Soft-Radial Projection for Constrained End-to-End Learning"),
  [Â§C.2](https://arxiv.org/html/2602.03461v1#A3.SS2.p2.3 "C.2 Adaptation of Competitor Methods to Capped Simplex Constraints â€£ Appendix C Extended Numerical Results â€£ Soft-Radial Projection for Constrained End-to-End Learning"),
  [Â§C.2](https://arxiv.org/html/2602.03461v1#A3.SS2.p2.7 "C.2 Adaptation of Competitor Methods to Capped Simplex Constraints â€£ Appendix C Extended Numerical Results â€£ Soft-Radial Projection for Constrained End-to-End Learning"),
  [Appendix C](https://arxiv.org/html/2602.03461v1#A3.p1.2 "Appendix C Extended Numerical Results â€£ Soft-Radial Projection for Constrained End-to-End Learning"),
  [3rd item](https://arxiv.org/html/2602.03461v1#S5.I1.i3.p1.1 "In 5.1 Implementation Details â€£ 5 Numerical Experiments â€£ Soft-Radial Projection for Constrained End-to-End Learning").
* R. T. Rockafellar and R. J.-B. Wets (2009)
  [Variational Analysis](https://doi.org/10.1007/978-3-642-02431-3).
   Springer.
  Cited by: [Â§1](https://arxiv.org/html/2602.03461v1#S1.p6.5 "1 Introduction â€£ Soft-Radial Projection for Constrained End-to-End Learning").
* R. T. Rockafellar (1970)
  Convex Analysis.
   Princeton University Press, Princeton, NJ.
  Cited by: [Â§B.1](https://arxiv.org/html/2602.03461v1#A2.SS1.13.p3.13 "Proof. â€£ B.1 Proofs of Section 2 â€£ Appendix B Proofs â€£ Soft-Radial Projection for Constrained End-to-End Learning"),
  [Â§B.1](https://arxiv.org/html/2602.03461v1#A2.SS1.2.p2.12 "Proof. â€£ B.1 Proofs of Section 2 â€£ Appendix B Proofs â€£ Soft-Radial Projection for Constrained End-to-End Learning").
* Y. Rychener, D. Kuhn, and T. Sutter (2023)
  [End-to-end learning for stochastic optimization: A Bayesian perspective](https://proceedings.mlr.press/v202/rychener23a).
  In International Conference on Machine Learning,
   pp.Â 29455â€“29472.
  Cited by: [Â§1](https://arxiv.org/html/2602.03461v1#S1.p5.2 "1 Introduction â€£ Soft-Radial Projection for Constrained End-to-End Learning").
* J. Tordesillas, J. P. How, and M. Hutter (2023)
  [Rayen: Imposition of hard convex constraints on neural networks](https://arxiv.org/abs/2307.08336).
  arXiv preprint arXiv:2307.08336.
  Cited by: [Â§A.2](https://arxiv.org/html/2602.03461v1#A1.SS2.p2.1 "A.2 Constructive Geometric Layers â€£ Appendix A Related Work â€£ Soft-Radial Projection for Constrained End-to-End Learning"),
  [Table 3](https://arxiv.org/html/2602.03461v1#A1.T3.3.3.6.1 "In Appendix A Related Work â€£ Soft-Radial Projection for Constrained End-to-End Learning").
* G. Zardini, N. Lanzetti, M. Pavone, and E. Frazzoli (2022)
  [Analysis and control of autonomous mobility-on-demand systems](https://doi.org/10.1146/annurev-control-042920-012811).
  Annual Review of Control, Robotics, and Autonomous Systems 5 (1),  pp.Â 633â€“658.
  Cited by: [Â§5.3](https://arxiv.org/html/2602.03461v1#S5.SS3.p1.7 "5.3 Ride-Sharing Dispatch â€£ 5 Numerical Experiments â€£ Soft-Radial Projection for Constrained End-to-End Learning").
* Z. Zhang, S. Zohren, and S. Roberts (2020)
  [Deep learning for portfolio optimization](https://doi.org/10.3905/jfds.2020.1.042).
  Journal of Financial Data Science 2 (4),  pp.Â 8â€“20.
  Cited by: [Â§C.4.1](https://arxiv.org/html/2602.03461v1#A3.SS4.SSS1.p1.1 "C.4.1 Discussion: Deep Portfolio Optimization vs. Classical Approaches â€£ C.4 Extended Analysis: Portfolio Optimization â€£ Appendix C Extended Numerical Results â€£ Soft-Radial Projection for Constrained End-to-End Learning").

## Appendix A Related Work

Enforcing hard constraints in neural networks is a fundamental challenge in safety-critical learning, requiring architectures that guarantee output feasibility without compromising optimization stability. The literature can be categorized by the mechanism of enforcementâ€”ranging from soft relaxations and iterative solvers to rigorous geometric constructions. Table [3](https://arxiv.org/html/2602.03461v1#A1.T3 "Table 3 â€£ Appendix A Related Work â€£ Soft-Radial Projection for Constrained End-to-End Learning") summarizes the properties of representative state-of-the-art approaches.

Broadly, constraint layers are distinguished by their dependency on the input context. *Input-independent constraints* involve a feasible set ğ’\mathcal{C} that is fixed across all samples (e.g., static actuator limits or portfolio weights), whereas *input-dependent constraints* involve a dynamic geometry ğ’â€‹(z)\mathcal{C}(z) conditioned on the input. Our work primarily targets the former, establishing a rigorous geometric framework for fixed convex sets. While strict constraint satisfaction is essential, the quality of the learning signalâ€”specifically the ability of backpropagation to provide informative gradients in high-noise regimesâ€”remains under-explored. Existing approaches prioritize inference speed and feasibility during training and execution. However, the convergence behavior toward the true solution and the ultimate optimality gap are less understood, particularly when the enforcement layer significantly alters the unconstrained network output. While this review is not exhaustiveâ€”with ongoing developments ranging from accelerated orthogonal projections Grontas et al. ([2026](https://arxiv.org/html/2602.03461v1#bib.bib54 "Pinet: Optimizing hard-constrained neural networks with orthogonal projection layers")) to the use of decision rules for constraint enforcement Constante-Flores et al. ([2025](https://arxiv.org/html/2602.03461v1#bib.bib53 "Enforcing Hard Linear Constraints in Deep Learning Models with Decision Rules"))â€”different applications and noise profiles often demand alternatives to standard orthogonal projection to maintain superior optimization properties.

Table 3: Comparison with State-of-the-Art Constraint Layers.
We position Soft-Radial Projection among constructive geometric layers that guarantee feasibility without high-dimensional iterative solvers.

|  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- |
| Method | Constraint Scope | Input Dep. | Satisfaction | Computation | Gradient Landscape | Univ. Approx. |
| Soft-Penalty | Any | Yes | No (Penalty) | Closed-Form | Smooth (infeasible) | Yes |
| DC3 Donti et al. ([2021](https://arxiv.org/html/2602.03461v1#bib.bib16 "DC3: A learning method for optimization with hard constraints")) | General | Yes | Approx.â‹† | Iterative (GD) | Step-size sensitive | Unknown |
| RAYEN Tordesillas et al. ([2023](https://arxiv.org/html/2602.03461v1#bib.bib29 "Rayen: Imposition of hard convex constraints on neural networks")) | Convex (Lin/QC/SOC/LMI) | Limited | Strict | Closed-Form | Clipped / Flat | Unknown |
| HardNet-Aff Min and Azizan ([2024](https://arxiv.org/html/2602.03461v1#bib.bib9 "HardNet: Hard-constrained neural networks with universal approximation guarantees")) | Affine | Yes | Strict | Closed-Form | Saturated (Boundary) | Yes |
| HP Liang et al. ([2024](https://arxiv.org/html/2602.03461v1#bib.bib7 "Homeomorphic projection to ensure neural-network solution feasibility for constrained optimization")) | Ball-Homeomorphic | Yes | Strict | Bisection | Iterative search | Yes |
| HoP (Polar) Deng et al. ([2025](https://arxiv.org/html/2602.03461v1#bib.bib8 "HoP: Homeomorphic polar learning for hard constrained optimization")) | Star-Convex | Yes | Strict (Int.) | Closed-Formâ€  | Polar stagnation | Unknown |
| RP Konstantinov and Utkin ([2023](https://arxiv.org/html/2602.03461v1#bib.bib3 "A new computationally simple approach for implementing neural networks with output hard constraints")) | Convex (Lin/Quad) | Limited | Strict (Int.) | Closed-Form | Smooth (Saturating) | Unknown |
| \rowcolorgray!10 Soft-Radial (Ours) | Convex | Noâ€¡ | Strict (Int.) | C.-F. / 1D Root | Full-rank Jacobian a.e. | Yes |

â‹†Asymptotic (linear constraints); heuristic/approximate otherwise; truncated correction in practice.
  
â€ HoP requires a per-sample interior anchor and a rayâ€“boundary intersection query.
  
â€¡Current focus is on fixed sets ğ’\mathcal{C}, though formulation supports dynamic anchors.

### A.1 Approximation and Optimization Approaches

Soft Penalties and Activations.
The most straightforward approach, favored by many industry practitioners, relaxes strict constraints into the training objective. Penalty-based methods augment the loss function with a regularization term proportional to the magnitude of constraint violations MÃ¡rquez-Neila et al. ([2017](https://arxiv.org/html/2602.03461v1#bib.bib50 "Imposing hard constraints on deep networks: Promises and limitations")). While this preserves the smoothness of the optimization landscape, it fundamentally fails to guarantee feasibility during inference, particularly in high-dimensional spaces where the volume of the feasible set relative to the ambient space can become negligible. Similarly, standard activation functions (e.g., Sigmoid, Softmax) offer computationally efficient bounds for elementary box or simplex constraints but lack the topological expressivity to model complex coupled constraints or polyhedral boundaries.

Differentiable Optimization and Iterative Correction. To ensure strict satisfaction, a second family of methods embeds optimization solvers directly into the network architecture. Differentiable optimization layers, such as OptNet Amos and Kolter ([2017](https://arxiv.org/html/2602.03461v1#bib.bib30 "Optnet: Differentiable optimization as a layer in neural networks")) and CVXPYLayers Agrawal et al. ([2019](https://arxiv.org/html/2602.03461v1#bib.bib18 "Differentiable convex optimization layers")), compute the orthogonal projection of a prediction onto ğ’\mathcal{C} by solving a convex program during the forward pass. DC3 Donti et al. ([2021](https://arxiv.org/html/2602.03461v1#bib.bib16 "DC3: A learning method for optimization with hard constraints")) instead utilizes a *completionâ€“correction* scheme, where equality constraints are satisfied via variable completion and inequality violations are reduced through iterative gradient-based correction. While mathematically general, these approaches introduce significant computational overhead. Furthermore, backpropagating through these layers typically relies on implicit differentiation, which assumes the inner solver has reached a stationary point. When truncated for real-time efficiency (e.g., fixed-iteration limits), this assumption is violated, leading to conflict-prone gradient updates where the descent direction of the task loss contradicts the incomplete constraint correction.

### A.2 Constructive Geometric Layers

To circumvent the computational cost of iterative solvers, recent research focuses on â€œFeasibility by Constructionâ€ via closed-form surjective mappings Proj:â„nâ†’ğ’\text{Proj}:\mathbb{R}^{n}\to\mathcal{C}. We classify these geometric approaches into three distinct families:

Gauge and Boundary Projections.
RAYEN Tordesillas et al. ([2023](https://arxiv.org/html/2602.03461v1#bib.bib29 "Rayen: Imposition of hard convex constraints on neural networks")) enforces convex constraints by selecting a fixed interior point and *rescaling* a predicted direction so that the endpoint remains feasible; the mapping is identity when the step stays inside the set, and otherwise returns the first rayâ€“boundary intersection. HardNet-Aff Min and Azizan ([2024](https://arxiv.org/html/2602.03461v1#bib.bib9 "HardNet: Hard-constrained neural networks with universal approximation guarantees")) targets *input-dependent affine* inequalities via a closed-form pseudoinverse-based correction that clamps violated constraints to their bounds while moving in directions parallel to already-satisfied constraint boundaries. While both layers are computationally efficient, their piecewise ray/clamping structure can suppress learning signals when constraints are active (e.g., RAYEN becomes insensitive to step magnitude after rescaling, and HardNet-Aff can attenuate gradient components in constraint-normal directions). This suppression can bias training toward boundary-feasible outputs and reduce interior exploration.

Topological Diffeomorphisms.
To utilize the setâ€™s interior, topological methods reshape the output space. Homeomorphic Projection (HP) Liang et al. ([2024](https://arxiv.org/html/2602.03461v1#bib.bib7 "Homeomorphic projection to ensure neural-network solution feasibility for constrained optimization")) learns a bi-Lipschitz *invertible neural network* (INN) that approximates a minimum-distortion homeomorphism between the constraint set and a unit ball. At inference, HP maps an infeasible prediction into the ball via the INN inverse and then performs a kk-step bisection along the ray to the ball center, requiring repeated INN forward evaluations and feasibility checks; the resulting boundary-feasible point trades computation for accuracy through the bisection depth and depends on the learned mapping distortion.
Polar coordinate methods like HoP Deng et al. ([2025](https://arxiv.org/html/2602.03461v1#bib.bib8 "HoP: Homeomorphic polar learning for hard constrained optimization")) attempt an analytic solution but introduce polar stagnation, singularities at the origin where angular components are undefined, resulting in numerical instability near the geometric center.

Radial Interior Mappings.
Closely related to our work, Konstantinov and Utkin ([2023](https://arxiv.org/html/2602.03461v1#bib.bib3 "A new computationally simple approach for implementing neural networks with output hard constraints")) propose a â€œray-projectionâ€ layer that constructs feasible points by scaling a direction vector rr from an interior anchor. While effective, this requires a specialized architecture that learns separate polar coordinates (gâ€‹(r,s)g(r,s)) rather than a direct map of the ambient Euclidean space. Furthermore, these constructions typically do not furnish a global bijection between â„n\mathbb{R}^{n} and Intâ¡(ğ’)\operatorname{Int}(\mathcal{C}), potentially resulting in saturating radial behavior due to the choice of bounded scaling functions (e.g., sigmoids).

Our Contribution: Soft-Radial Projection. We propose a drop-in reparameterization layer for fixed closed convex sets with a non-empty interior. Our mapping acts directly on the unconstrained ambient prediction uâˆˆâ„nu\in\mathbb{R}^{n} as a unified radial homeomorphism â„nâ†’Intâ¡(ğ’)\mathbb{R}^{n}\to\operatorname{Int}(\mathcal{C}). For standard convex geometriesâ€”such as balls, ellipsoids, and boxesâ€”the mapping admits a closed-form solution. For general convex sets, we utilize an efficient monotonic 1D root-finding procedure. Crucially, we ensure the mapping is differentiable with an invertible Jacobian almost everywhere, preventing the rank-collapse characteristic of boundary projections. This ensures the learning signal remains non-vanishing even for predictions far outside the feasible set, while preserving the Universal Approximation property of the base network (Theorem [4](https://arxiv.org/html/2602.03461v1#S4 "4 Constrained Universal Approximation â€£ Soft-Radial Projection for Constrained End-to-End Learning")).

## Appendix B Proofs

In the following sections, we provide complete proofs for the theoretical results. To facilitate readability, we restate each claim prior to its proof.

### B.1 Proofs of Section [2](https://arxiv.org/html/2602.03461v1#S2 "2 Soft-Radial Projection Layer â€£ Soft-Radial Projection for Constrained End-to-End Learning")

We begin by establishing the regularity of the individual projection components.

\RayRegularity

\*

###### Proof.

*Ray feasibility and intersection.* Fix uâˆˆâ„nu\in\mathbb{R}^{n} and consider scalars Î±âˆˆ[0,1]\alpha\in[0,1] such that Î±â€‹uâˆˆğ’\alpha u\in\mathcal{C}. The map Î±â†¦Î±â€‹u\alpha\mapsto\alpha u is continuous and ğ’\mathcal{C} is closed, so the set of feasible Î±âˆˆ[0,1]\alpha\in[0,1] is closed in [0,1][0,1] and nonempty (since Î±=0\alpha=0 is always feasible). Convexity of ğ’\mathcal{C} implies this set is convex in â„\mathbb{R}, and any nonempty closed convex subset of â„\mathbb{R} is an interval; hence it is of the form [0,Î±â‹†â€‹(u)][0,\alpha^{\star}(u)] for a unique Î±â‹†â€‹(u)âˆˆ[0,1]\alpha^{\star}(u)\in[0,1].

*Global Lipschitz continuity of Î±\alpha.* Let Î³ğ’:â„nâ†’[0,âˆ)\gamma\_{\mathcal{C}}:\mathbb{R}^{n}\to[0,\infty) denote the Minkowski functional (gauge) of ğ’\mathcal{C},

|  |  |  |
| --- | --- | --- |
|  | Î³ğ’â€‹(x)â‰”inf{t>0:xâˆˆtâ€‹ğ’}.\gamma\_{\mathcal{C}}(x)\coloneqq\inf\{t>0:x\in t\mathcal{C}\}. |  |

Standard properties of the gauge (see, e.g., Rockafellar ([1970](https://arxiv.org/html/2602.03461v1#bib.bib25 "Convex Analysis")), Thm.Â 14.5 and Chap.Â 13) imply that Î³ğ’\gamma\_{\mathcal{C}} is sublinear (convex and positively homogeneous) and finite on â„n\mathbb{R}^{n} when 0âˆˆIntâ¡(ğ’)0\in\operatorname{Int}(\mathcal{C}). Because 0âˆˆIntâ¡(ğ’)0\in\operatorname{Int}(\mathcal{C}), there exists Î´>0\delta>0 with Bâ€‹(0,Î´)âŠ‚ğ’B(0,\delta)\subset\mathcal{C}, which yields Î³ğ’â€‹(x)â‰¤â€–xâ€–/Î´\gamma\_{\mathcal{C}}(x)\leq\|x\|/\delta for all xx and, by subadditivity of Î³ğ’\gamma\_{\mathcal{C}},

|  |  |  |
| --- | --- | --- |
|  | Î³ğ’â€‹(x)âˆ’Î³ğ’â€‹(y)â‰¤Î³ğ’â€‹(xâˆ’y)â‰¤â€–xâˆ’yâ€–Î´âˆ€x,yâˆˆâ„n,\gamma\_{\mathcal{C}}(x)-\gamma\_{\mathcal{C}}(y)\leq\gamma\_{\mathcal{C}}(x-y)\leq\frac{\|x-y\|}{\delta}\quad\forall\;x,y\in\mathbb{R}^{n}, |  |

and

|  |  |  |
| --- | --- | --- |
|  | Î³ğ’â€‹(y)âˆ’Î³ğ’â€‹(x)â‰¤Î³ğ’â€‹(yâˆ’x)â‰¤â€–yâˆ’xâ€–Î´âˆ€x,yâˆˆâ„n.\gamma\_{\mathcal{C}}(y)-\gamma\_{\mathcal{C}}(x)\leq\gamma\_{\mathcal{C}}(y-x)\leq\frac{\|y-x\|}{\delta}\quad\forall\;x,y\in\mathbb{R}^{n}. |  |

Taking the maximum of the two inequalities shows

|  |  |  |
| --- | --- | --- |
|  | |Î³ğ’â€‹(x)âˆ’Î³ğ’â€‹(y)|â‰¤â€–xâˆ’yâ€–Î´âˆ€x,yâˆˆâ„n,|\gamma\_{\mathcal{C}}(x)-\gamma\_{\mathcal{C}}(y)|\leq\frac{\|x-y\|}{\delta}\quad\forall\;x,y\in\mathbb{R}^{n}, |  |

so Î³ğ’\gamma\_{\mathcal{C}} is Lipschitz with constant 1/Î´1/\delta.

For uâ‰ 0u\neq 0, the feasibility of Î±â€‹uâˆˆğ’\alpha u\in\mathcal{C} is equivalent to Î±â‰¤1/Î³ğ’â€‹(u)\alpha\leq 1/\gamma\_{\mathcal{C}}(u). Therefore, the feasible Î±âˆˆ[0,1]\alpha\in[0,1] form the interval [0,minâ¡{1,1/Î³ğ’â€‹(u)}][0,\min\{1,1/\gamma\_{\mathcal{C}}(u)\}], implying that Î±â‹†â€‹(u)=minâ¡(1,1/Î³ğ’â€‹(u))\alpha^{\star}(u)=\min\bigl(1,1/\gamma\_{\mathcal{C}}(u)\bigr), for uâ‰ 0u\neq 0, while for u=0u=0 we clearly have Î±â‹†â€‹(u)=1\alpha^{\star}(u)=1. Define a helper function Ïˆ:[0,âˆ)â†’(0,1]\psi:[0,\infty)\to(0,1] by Ïˆâ€‹(t)â‰”minâ¡(1,1/t)\psi(t)\coloneqq\min(1,1/t) for t>0t>0 and Ïˆâ€‹(0)â‰”1\psi(0)\coloneqq 1. The function Ïˆ\psi is 11â€“Lipschitz on [0,âˆ)[0,\infty), so for all u,vâˆˆâ„nu,v\in\mathbb{R}^{n},

|  |  |  |  |
| --- | --- | --- | --- |
|  | |Î±â‹†â€‹(u)âˆ’Î±â‹†â€‹(v)|\displaystyle|\alpha^{\star}(u)-\alpha^{\star}(v)| | =|Ïˆâ€‹(Î³ğ’â€‹(u))âˆ’Ïˆâ€‹(Î³ğ’â€‹(v))|\displaystyle=|\psi(\gamma\_{\mathcal{C}}(u))-\psi(\gamma\_{\mathcal{C}}(v))| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤|Î³ğ’â€‹(u)âˆ’Î³ğ’â€‹(v)|â‰¤â€–uâˆ’vâ€–Î´.\displaystyle\leq|\gamma\_{\mathcal{C}}(u)-\gamma\_{\mathcal{C}}(v)|\leq\frac{\|u-v\|}{\delta}. |  |

Thus Î±â‹†\alpha^{\star} is globally Lipschitz with constant 1/Î´1/\delta.

*Local Lipschitz continuity of qq.* We now bound the variation of qâ€‹(u)=Î±â‹†â€‹(u)â€‹uq(u)=\alpha^{\star}(u)u on bounded sets. Let R>0R>0 and assume â€–vâ€–â‰¤R\|v\|\leq R and â€–uâ€–â‰¤R\|u\|\leq R.
Then

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–qâ€‹(u)âˆ’qâ€‹(v)â€–\displaystyle\|q(u)-q(v)\| | =â€–Î±â‹†â€‹(u)â€‹uâˆ’Î±â‹†â€‹(v)â€‹vâ€–\displaystyle=\|\alpha^{\star}(u)u-\alpha^{\star}(v)v\| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤â€–Î±â‹†â€‹(u)â€‹(uâˆ’v)â€–+â€–vâ€‹(Î±â‹†â€‹(u)âˆ’Î±â‹†â€‹(v))â€–\displaystyle\leq\|\alpha^{\star}(u)(u-v)\|+\|v(\alpha^{\star}(u)-\alpha^{\star}(v))\| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤â€–uâˆ’vâ€–+â€–vâ€–â‹…|Î±â‹†â€‹(u)âˆ’Î±â‹†â€‹(v)|\displaystyle\leq\|u-v\|+\|v\|\cdot|\alpha^{\star}(u)-\alpha^{\star}(v)| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤â€–uâˆ’vâ€–+â€–vâ€–Î´â€‹â€–uâˆ’vâ€–\displaystyle\leq\|u-v\|+\frac{\|v\|}{\delta}\,\|u-v\| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤(1+RÎ´)â€‹â€–uâˆ’vâ€–.\displaystyle\leq\Bigl(1+\frac{R}{\delta}\Bigr)\|u-v\|. |  |

Hence qq is (1+R/Î´)(1+R/\delta)â€“Lipschitz on the ball {u:â€–uâ€–â‰¤R}\{u:\|u\|\leq R\}, which implies that qq is locally Lipschitz on â„n\mathbb{R}^{n}, completing the proof.
âˆ

Next, we analyze the structural properties of the constructed projection map.

\RegComponents

\*

###### Proof.

*Continuity of tÂ¯\bar{t}.* We relate tÂ¯â€‹(v)\bar{t}(v) to the Minkowski functional (gauge) Î³ğ’\gamma\_{\mathcal{C}}. By definition, tÂ¯â€‹(v)=1/Î³ğ’â€‹(v)\bar{t}(v)=1/\gamma\_{\mathcal{C}}(v) (with the convention 1/0=âˆ1/0=\infty). Since Î³ğ’\gamma\_{\mathcal{C}} is continuous on â„n\mathbb{R}^{n} (LemmaÂ [2](https://arxiv.org/html/2602.03461v1#S2.F2 "Figure 2 â€£ 2 Soft-Radial Projection Layer â€£ Soft-Radial Projection for Constrained End-to-End Learning")) and the inversion map is continuous on [0,âˆ)[0,\infty) into (0,âˆ](0,\infty], the composition vâ†¦tÂ¯â€‹(v)v\mapsto\bar{t}(v) is continuous on the unit sphere.

*Properties of Ïˆv\psi\_{v}.* For tâ‰¤tÂ¯â€‹(v)t\leq\bar{t}(v), we have Ïˆvâ€‹(t)=râ€‹(t2)â€‹t\psi\_{v}(t)=r(t^{2})t. Strict monotonicity on this interval was established immediately following AssumptionÂ [2.3](https://arxiv.org/html/2602.03461v1#S2.Thmtheorem3 "Assumption 2.3 (Radial monotonicity). â€£ 2 Soft-Radial Projection Layer â€£ Soft-Radial Projection for Constrained End-to-End Learning"). For t>tÂ¯â€‹(v)t>\bar{t}(v) (which implies tÂ¯â€‹(v)<âˆ\bar{t}(v)<\infty), we have Ïˆvâ€‹(t)=râ€‹(t2)â€‹tÂ¯â€‹(v)\psi\_{v}(t)=r(t^{2})\bar{t}(v), which is strictly increasing simply because rr is increasing. Continuity holds at t=tÂ¯â€‹(v)t=\bar{t}(v) as both expressions coincide. Finally, since Ïˆvâ€‹(0)=0\psi\_{v}(0)=0 and limtâ†’âˆÏˆvâ€‹(t)=tÂ¯â€‹(v)\lim\_{t\to\infty}\psi\_{v}(t)=\bar{t}(v), the range is exactly [0,tÂ¯â€‹(v))[0,\bar{t}(v)).
âˆ

Building on these established properties, we prove that pp is indeed a homeomorphism.

\Homeomorphism

\*

###### Proof.

We first establish that pp is a bijection.

*Injectivity.* Let u1,u2âˆˆâ„nu\_{1},u\_{2}\in\mathbb{R}^{n} with pâ€‹(u1)=pâ€‹(u2)p(u\_{1})=p(u\_{2}). Since pp preserves directions (i.e., pâ€‹(u)âˆˆâ„+â€‹up(u)\in\mathbb{R}\_{+}u), u1u\_{1} and u2u\_{2} must lie on the same ray, so ui=tiâ€‹vu\_{i}=t\_{i}v for some unit vector vv and tiâ‰¥0t\_{i}\geq 0.The scalar map Ïˆv\psi\_{v} from DefinitionÂ [2.5](https://arxiv.org/html/2602.03461v1#S2.Thmtheorem5 "Definition 2.5 (Scalar projection map). â€£ 2 Soft-Radial Projection Layer â€£ Soft-Radial Projection for Constrained End-to-End Learning") satisfies pâ€‹(tiâ€‹v)=Ïˆvâ€‹(ti)â€‹vp(t\_{i}v)=\psi\_{v}(t\_{i})v. By LemmaÂ [2.5](https://arxiv.org/html/2602.03461v1#S2.Thmtheorem5 "Definition 2.5 (Scalar projection map). â€£ 2 Soft-Radial Projection Layer â€£ Soft-Radial Projection for Constrained End-to-End Learning"), Ïˆv\psi\_{v} is strictly increasing, so Ïˆvâ€‹(t1)=Ïˆvâ€‹(t2)\psi\_{v}(t\_{1})=\psi\_{v}(t\_{2}) implies t1=t2t\_{1}=t\_{2}, and hence u1=u2u\_{1}=u\_{2}.

*Surjectivity.* Let xâˆˆIntâ¡(ğ’)x\in\operatorname{Int}(\mathcal{C}). If x=0x=0, x=pâ€‹(0)x=p(0). If xâ‰ 0x\neq 0, set v=x/â€–xâ€–v=x/\|x\| and s=â€–xâ€–s=\|x\|. Since xâˆˆIntâ¡(ğ’)x\in\operatorname{Int}(\mathcal{C}), we strictly have s<tÂ¯â€‹(v)s<\bar{t}(v). By LemmaÂ [2.5](https://arxiv.org/html/2602.03461v1#S2.Thmtheorem5 "Definition 2.5 (Scalar projection map). â€£ 2 Soft-Radial Projection Layer â€£ Soft-Radial Projection for Constrained End-to-End Learning"), the continuous scalar map Ïˆv\psi\_{v} has range [0,tÂ¯â€‹(v))[0,\bar{t}(v)). By the Intermediate Value Theorem, there exists a unique tt such that Ïˆvâ€‹(t)=s\psi\_{v}(t)=s, yielding pâ€‹(tâ€‹v)=xp(tv)=x. Moreover, pp is continuous on â„n\mathbb{R}^{n} as it is composed of the continuous scaling function rr (AssumptionÂ [2.3](https://arxiv.org/html/2602.03461v1#S2.Thmtheorem3 "Assumption 2.3 (Radial monotonicity). â€£ 2 Soft-Radial Projection Layer â€£ Soft-Radial Projection for Constrained End-to-End Learning")) and the continuous hard radial projection qq (LemmaÂ [2](https://arxiv.org/html/2602.03461v1#S2.F2 "Figure 2 â€£ 2 Soft-Radial Projection Layer â€£ Soft-Radial Projection for Constrained End-to-End Learning")). Thus, pp is a continuous bijection from â„n\mathbb{R}^{n} onto Intâ¡(ğ’)\operatorname{Int}(\mathcal{C}).

*Continuity of Inverse.* Since pp is a continuous injection from â„n\mathbb{R}^{n} into an open subset â„n\mathbb{R}^{n}, the Invariance of Domain theorem Brouwer ([1912](https://arxiv.org/html/2602.03461v1#bib.bib33 "Ãœber Abbildung von Mannigfaltigkeiten")) guarantees that pp is an open map. Because pp is a continuous bijection that maps open sets to open sets, it is a homeomorphism between â„n\mathbb{R}^{n} and its image Intâ¡(ğ’)\operatorname{Int}(\mathcal{C}).
âˆ

Finally, we analyze the differentiability of pp, establishing that it is a diffeomorphism almost everywhere.

\DiffInvert

\*

###### Proof.

By LemmaÂ [2](https://arxiv.org/html/2602.03461v1#S2.F2 "Figure 2 â€£ 2 Soft-Radial Projection Layer â€£ Soft-Radial Projection for Constrained End-to-End Learning"), qq is locally Lipschitz and therefore differentiable almost everywhere by Rademacherâ€™s theorem. Since rr is C1C^{1}, the product pâ€‹(u)=râ€‹(â€–uâ€–2)â€‹qâ€‹(u)p(u)=r(\|u\|^{2})q(u) shares the same domain of differentiability. Whenever Jpâ€‹(u)J\_{p}(u) exists, the product rule gives

|  |  |  |  |
| --- | --- | --- | --- |
|  | Jpâ€‹(u)=râ€‹(â€–uâ€–2)â€‹Jqâ€‹(u)+â€„2â€‹râ€²â€‹(â€–uâ€–2)â€‹qâ€‹(u)â€‹uâŠ¤.J\_{p}(u)\;=\;r(\|u\|^{2})\,J\_{q}(u)\;+\;2\,r^{\prime}(\|u\|^{2})\,q(u)\,u^{\top}. |  | (9) |

We analyze the invertibility of this Jacobian in two cases based on the position of uu relative to ğ’\mathcal{C}. Note that pp is generally not differentiable for uâˆˆâˆ‚ğ’u\in\partial\mathcal{C} because the map Î±â‹†â€‹(u)\alpha^{\star}(u) (and thus qq) typically exhibits a kink at the boundary where the active regime switches.

*Interior points.* For uâˆˆIntâ¡(ğ’)u\in\operatorname{Int}(\mathcal{C}) we have qâ€‹(u)=uq(u)=u and Jqâ€‹(u)=IJ\_{q}(u)=I, hence

|  |  |  |
| --- | --- | --- |
|  | Jpâ€‹(u)=râ€‹(â€–uâ€–2)â€‹I+2â€‹râ€²â€‹(â€–uâ€–2)â€‹uâ€‹uâŠ¤.J\_{p}(u)=r(\|u\|^{2})\,I+2\,r^{\prime}(\|u\|^{2})\,uu^{\top}. |  |

For uâ‰ 0u\neq 0, this matrix has eigenvector u/â€–uâ€–u/\|u\|, with corresponding eigenvalue râ€‹(â€–uâ€–2)+2â€‹â€–uâ€–2â€‹râ€²â€‹(â€–uâ€–2)r(\|u\|^{2})+2\,\|u\|^{2}\,r^{\prime}(\|u\|^{2}) and all vectors orthogonal to uu are eigenvectors with eigenvalue râ€‹(â€–uâ€–2)r(\|u\|^{2}). AssumptionÂ [2.3](https://arxiv.org/html/2602.03461v1#S2.Thmtheorem3 "Assumption 2.3 (Radial monotonicity). â€£ 2 Soft-Radial Projection Layer â€£ Soft-Radial Projection for Constrained End-to-End Learning") gives râ€‹(Ï)>0r(\rho)>0 and râ€‹(Ï)+2â€‹Ïâ€‹râ€²â€‹(Ï)>0r(\rho)+2\rho\,r^{\prime}(\rho)>0 for all Ïâ‰¥0\rho\geq 0, so Jpâ€‹(u)â‰»0J\_{p}(u)\succ 0 is invertible for all uâˆˆIntâ¡(ğ’)âˆ–{0}u\in\operatorname{Int}(\mathcal{C})\setminus\{0\}. At the origin, qq coincides with the identity in a neighborhood of 0 (because 0âˆˆIntâ¡(ğ’)0\in\operatorname{Int}(\mathcal{C})), so Jqâ€‹(0)=IJ\_{q}(0)=I and the rank-one term vanishes, giving Jpâ€‹(0)=râ€‹(0)â€‹IJ\_{p}(0)=r(0)\,I, which is invertible since râ€‹(0)>0r(0)>0.

*Exterior points.* For points outside the interior (uâˆ‰Intâ¡(ğ’)u\notin\operatorname{Int}(\mathcal{C})), qâ€‹(u)q(u) lies on the boundary âˆ‚ğ’\partial\mathcal{C}. Using the gauge Î³ğ’\gamma\_{\mathcal{C}} of ğ’\mathcal{C} (the Minkowski functional), the condition uâˆ‰Intâ¡(ğ’)u\notin\operatorname{Int}(\mathcal{C}) implies Î³ğ’â€‹(u)â‰¥1\gamma\_{\mathcal{C}}(u)\geq 1, and the hard radial projection is given by qâ€‹(u)=u/Î³ğ’â€‹(u)q(u)=u/\gamma\_{\mathcal{C}}(u). Now fix a point uu where Î³ğ’\gamma\_{\mathcal{C}} is differentiable (which holds almost everywhere by standard results on convex functions, see, e.g., Rockafellar ([1970](https://arxiv.org/html/2602.03461v1#bib.bib25 "Convex Analysis")), Â§25). To simplify notation, let Î»â‰”Î³ğ’â€‹(u)â‰¥1\lambda\coloneqq\gamma\_{\mathcal{C}}(u)\geq 1. Differentiating qâ€‹(u)q(u) with respect to uu yields the Jacobian

|  |  |  |
| --- | --- | --- |
|  | Jqâ€‹(u)=1Î»â€‹Iâˆ’1Î»2â€‹uâ€‹âˆ‡Î³ğ’â€‹(u)âŠ¤.J\_{q}(u)\;=\;\frac{1}{\lambda}I-\frac{1}{\lambda^{2}}\,u\,\nabla\gamma\_{\mathcal{C}}(u)^{\top}. |  |

We establish that Jpâ€‹(u)J\_{p}(u) is invertible by showing its kernel is trivial. Let hâˆˆâ„nh\in\mathbb{R}^{n} satisfy Jpâ€‹(u)â€‹h=0J\_{p}(u)h=0. We decompose hh with respect to the subspace W={wâˆˆâ„nâˆ£uâŠ¤â€‹w=0}W=\{w\in\mathbb{R}^{n}\mid u^{\top}w=0\}. Any hh can be uniquely written as h=hW+Î²â€‹uh=h\_{W}+\beta u, where hWâˆˆWh\_{W}\in W and Î²âˆˆâ„\beta\in\mathbb{R}.

First, we compute the product of Jqâ€‹(u)J\_{q}(u) with the radial component. Since Î³ğ’\gamma\_{\mathcal{C}} is positively homogeneous of degree 1 by definition, Eulerâ€™s homogeneous function theorem implies âˆ‡Î³ğ’â€‹(u)âŠ¤â€‹u=Î³ğ’â€‹(u)=Î»\nabla\gamma\_{\mathcal{C}}(u)^{\top}u=\gamma\_{\mathcal{C}}(u)=\lambda. Using this relation, we find

|  |  |  |
| --- | --- | --- |
|  | Jqâ€‹(u)â€‹u=1Î»â€‹uâˆ’1Î»2â€‹uâ€‹(âˆ‡Î³ğ’â€‹(u)âŠ¤â€‹u)âŸÎ»=1Î»â€‹uâˆ’1Î»â€‹u=â€„0.J\_{q}(u)u\;=\;\frac{1}{\lambda}u-\frac{1}{\lambda^{2}}u\underbrace{(\nabla\gamma\_{\mathcal{C}}(u)^{\top}u)}\_{\lambda}\;=\;\frac{1}{\lambda}u-\frac{1}{\lambda}u\;=\;0. |  |

For the orthogonal component hWh\_{W}, we have

|  |  |  |
| --- | --- | --- |
|  | Jqâ€‹(u)â€‹hW=1Î»â€‹hWâˆ’âˆ‡Î³ğ’â€‹(u)âŠ¤â€‹hWÎ»2â€‹u.J\_{q}(u)h\_{W}\;=\;\frac{1}{\lambda}h\_{W}-\frac{\nabla\gamma\_{\mathcal{C}}(u)^{\top}h\_{W}}{\lambda^{2}}u. |  |

Substituting h=hW+Î²â€‹uh=h\_{W}+\beta u and the expression for Jpâ€‹(u)J\_{p}(u) ([9](https://arxiv.org/html/2602.03461v1#A2.E9 "Equation 9 â€£ Proof. â€£ B.1 Proofs of Section 2 â€£ Appendix B Proofs â€£ Soft-Radial Projection for Constrained End-to-End Learning")) into the condition Jpâ€‹(u)â€‹h=0J\_{p}(u)h=0 yields

|  |  |  |
| --- | --- | --- |
|  | râ€‹(â€–uâ€–2)â€‹Jqâ€‹(u)â€‹hW+2â€‹Î²â€‹â€–uâ€–2â€‹râ€²â€‹(â€–uâ€–2)â€‹qâ€‹(u)=â€„0.r(\|u\|^{2})J\_{q}(u)h\_{W}+2\beta\|u\|^{2}r^{\prime}(\|u\|^{2})q(u)\;=\;0. |  |

Substituting the expansion of Jqâ€‹(u)â€‹hWJ\_{q}(u)h\_{W} allows us to group terms proportional to uu. The equation becomes

|  |  |  |
| --- | --- | --- |
|  | râ€‹(â€–uâ€–2)Î»â€‹hW+Î¼â€‹u=0,\frac{r(\|u\|^{2})}{\lambda}h\_{W}\;+\;\mu u=0, |  |

where Î¼âˆˆâ„\mu\in\mathbb{R} captures all radial coefficients:

|  |  |  |
| --- | --- | --- |
|  | Î¼=2â€‹Î²â€‹â€–uâ€–2â€‹râ€²â€‹(â€–uâ€–2)â€‹1Î»âˆ’râ€‹(â€–uâ€–2)â€‹âˆ‡Î³ğ’â€‹(u)âŠ¤â€‹hWÎ»2.\mu=2\beta\|u\|^{2}r^{\prime}(\|u\|^{2})\frac{1}{\lambda}-\frac{r(\|u\|^{2})\nabla\gamma\_{\mathcal{C}}(u)^{\top}h\_{W}}{\lambda^{2}}. |  |

Projecting the vector equation onto the subspace WW eliminates Î¼â€‹u\mu u, yielding

|  |  |  |
| --- | --- | --- |
|  | râ€‹(â€–uâ€–2)Î»â€‹hW=0.\frac{r(\|u\|^{2})}{\lambda}h\_{W}=0. |  |

Since r>0r>0 and Î»â‰¥1\lambda\geq 1, we must have hW=0h\_{W}=0. Consequently, the radial term must also vanish, so Î¼â€‹u=0\mu u=0, implying Î¼=0\mu=0. With hW=0h\_{W}=0, the expression for Î¼\mu simplifies to

|  |  |  |
| --- | --- | --- |
|  | 2â€‹Î²â€‹â€–uâ€–2â€‹râ€²â€‹(â€–uâ€–2)â€‹1Î»=0.2\beta\|u\|^{2}r^{\prime}(\|u\|^{2})\frac{1}{\lambda}=0. |  |

AssumptionÂ [2.3](https://arxiv.org/html/2602.03461v1#S2.Thmtheorem3 "Assumption 2.3 (Radial monotonicity). â€£ 2 Soft-Radial Projection Layer â€£ Soft-Radial Projection for Constrained End-to-End Learning") ensures râ€²â€‹(Ï)>0r^{\prime}(\rho)>0 for Ï>0\rho>0, so we must have Î²=0\beta=0. Thus h=0h=0, proving that Jpâ€‹(u)J\_{p}(u) is invertible. Combining the results for interior and exterior points, we conclude that pp is differentiable almost everywhere and Jpâ€‹(u)J\_{p}(u) is invertible wherever it exists.
âˆ

### B.2 Proofs of Section [3](https://arxiv.org/html/2602.03461v1#S3 "3 Optimization Guarantees â€£ Soft-Radial Projection for Constrained End-to-End Learning")

We begin by establishing the equivalence of the optimal values between the unconstrained composite optimization fâ€‹(u)f(u) and the constrained optimization objective.

\EquivOptVal

\*

###### Proof.

By bijectivity of p:â„nâ†’Intâ¡(ğ’)p:\mathbb{R}^{n}\to\operatorname{Int}(\mathcal{C}),

|  |  |  |
| --- | --- | --- |
|  | infuâˆˆâ„nfâ€‹(u)=infxâˆˆIntâ¡(ğ’)â„“â€‹(x).\inf\_{u\in\mathbb{R}^{n}}f(u)=\inf\_{x\in\operatorname{Int}(\mathcal{C})}\ell(x). |  |

Since u0âˆˆIntâ¡(ğ’)u\_{0}\in\operatorname{Int}(\mathcal{C}) and ğ’\mathcal{C} is convex, for any xâˆˆğ’x\in\mathcal{C} and Îµâˆˆ(0,1]\varepsilon\in(0,1],
xÎµâ‰”(1âˆ’Îµ)â€‹x+Îµâ€‹u0âˆˆIntâ¡(ğ’)x\_{\varepsilon}\coloneqq(1-\varepsilon)x+\varepsilon u\_{0}\in\operatorname{Int}(\mathcal{C}) and xÎµâ†’xx\_{\varepsilon}\to x as Îµâ†“0\varepsilon\downarrow 0.
By continuity, â„“â€‹(xÎµ)â†’â„“â€‹(x)\ell(x\_{\varepsilon})\to\ell(x), hence infIntâ¡(ğ’)â„“â‰¤â„“â€‹(x)\inf\_{\operatorname{Int}(\mathcal{C})}\ell\leq\ell(x) for all xâˆˆğ’x\in\mathcal{C},
implying infIntâ¡(ğ’)â„“â‰¤infğ’â„“\inf\_{\operatorname{Int}(\mathcal{C})}\ell\leq\inf\_{\mathcal{C}}\ell.
Since Intâ¡(ğ’)âŠ†ğ’\operatorname{Int}(\mathcal{C})\subseteq\mathcal{C}, we also have infğ’â„“â‰¤infIntâ¡(ğ’)â„“\inf\_{\mathcal{C}}\ell\leq\inf\_{\operatorname{Int}(\mathcal{C})}\ell.
Thus the optimal values coincide.

Finally, argâ¡minâ¡fâ‰ âˆ…\arg\min f\neq\emptyset holds iff there exists xâ‹†âˆˆIntâ¡(ğ’)x^{\star}\in\operatorname{Int}(\mathcal{C}) with
â„“â€‹(xâ‹†)=infxâˆˆğ’â„“â€‹(x)\ell(x^{\star})=\inf\_{x\in\mathcal{C}}\ell(x), in which case uâ‹†=pâˆ’1â€‹(xâ‹†)u^{\star}=p^{-1}(x^{\star}) exists and satisfies uâ‹†âˆˆargâ¡minâ¡fu^{\star}\in\arg\min f.
Conversely, if uâ‹†âˆˆargâ¡minâ¡fu^{\star}\in\arg\min f, then pâ€‹(uâ‹†)âˆˆIntâ¡(ğ’)p(u^{\star})\in\operatorname{Int}(\mathcal{C}) satisfies
â„“â€‹(pâ€‹(uâ‹†))=infxâˆˆğ’â„“â€‹(x)\ell(p(u^{\star}))=\inf\_{x\in\mathcal{C}}\ell(x), i.e., pâ€‹(uâ‹†)âˆˆargâ¡minğ’â¡â„“âˆ©Intâ¡(ğ’)p(u^{\star})\in\arg\min\_{\mathcal{C}}\ell\cap\operatorname{Int}(\mathcal{C}).
âˆ

Next, we establish the equivalence of stationary points between the two objectives.

\CorrStatPoints

\*

###### Proof.

The gradient identity follows from the chain rule, using that pâ€‹(u)âˆˆIntâ¡(ğ’)p(u)\in\operatorname{Int}(\mathcal{C}) for all uu. If Jpâ€‹(u)J\_{p}(u) is invertible and âˆ‡fâ€‹(u)=Jpâ€‹(u)âŠ¤â€‹âˆ‡â„“â€‹(pâ€‹(u))=0\nabla f(u)=J\_{p}(u)^{\top}\nabla\ell(p(u))=0, then âˆ‡â„“â€‹(pâ€‹(u))=0\nabla\ell(p(u))=0. The converse is immediate.
âˆ

We then extend this analysis to show that local minimizers of the composite objective correspond to global minimizers of the constrained problem.

\GlobOptInt

\*

###### Proof.

Let uu be a local minimizer of ff at which pp is differentiable. Then Jpâ€‹(u)J\_{p}(u) is invertible by Theorem [2.5](https://arxiv.org/html/2602.03461v1#S2.Thmtheorem5 "Definition 2.5 (Scalar projection map). â€£ 2 Soft-Radial Projection Layer â€£ Soft-Radial Projection for Constrained End-to-End Learning"), and âˆ‡fâ€‹(u)=0\nabla f(u)=0. By PropositionÂ [3](https://arxiv.org/html/2602.03461v1#S3 "3 Optimization Guarantees â€£ Soft-Radial Projection for Constrained End-to-End Learning"), âˆ‡â„“â€‹(pâ€‹(u))=0\nabla\ell(p(u))=0. Since pâ€‹(u)âˆˆIntâ¡(ğ’)p(u)\in\operatorname{Int}(\mathcal{C}) and â„“\ell is convex, âˆ‡â„“â€‹(pâ€‹(u))=0\nabla\ell(p(u))=0 implies that pâ€‹(u)p(u) is a global minimizer of â„“\ell (over â„n\mathbb{R}^{n}, and therefore also over ğ’\mathcal{C}).
âˆ

We now investigate the convergence rate and demonstrate the absence of a global Polyak-Åojasiewicz (PL) condition, which necessitates a more nuanced convergence analysis.

\AbsencePL

\*

###### Proof.

The unique minimizer of â„“\ell over ğ’\mathcal{C} is xâ‹†=0x^{\star}=0, and the corresponding optimal value â„“â‹†=0\ell^{\star}=0. Fix a unit vector vv and consider uâ€‹(t)â‰”tâ€‹vu(t)\coloneqq tv with tâ‰¥0t\geq 0. For ğ’=B1â€‹(0)\mathcal{C}=B\_{1}(0) the hard radial projection is qâ€‹(u)=uq(u)=u if â€–uâ€–â‰¤1\|u\|\leq 1 and qâ€‹(u)=u/â€–uâ€–q(u)=u/\|u\| otherwise; hence for tâ‰¥1t\geq 1 we have qâ€‹(uâ€‹(t))=vq(u(t))=v and therefore

|  |  |  |
| --- | --- | --- |
|  | pâ€‹(uâ€‹(t))=râ€‹(t2)â€‹v,fâ€‹(uâ€‹(t))=â€–pâ€‹(uâ€‹(t))â€–2=râ€‹(t2)2.p(u(t))\;=\;r(t^{2})\,v,\qquad f(u(t))\;=\;\|p(u(t))\|^{2}\;=\;r(t^{2})^{2}. |  |

Since râ€‹(t2)â†’1r(t^{2})\to 1 as tâ†’âˆt\to\infty, we obtain

|  |  |  |
| --- | --- | --- |
|  | fâ€‹(uâ€‹(t))âˆ’fâ‹†=râ€‹(t2)2â†’â€„1(tâ†’âˆ),f(u(t))-f^{\star}\;=\;r(t^{2})^{2}\;\to\;1\quad(t\to\infty), |  |

so along this ray the suboptimality converges to a strictly positive constant. Next, compute the gradient for t>1t>1. On {â€–uâ€–>1}\{\|u\|>1\}, the hard radial projection qâ€‹(u)=u/â€–uâ€–q(u)=u/\|u\| has Jacobian

|  |  |  |
| --- | --- | --- |
|  | Jqâ€‹(u)=1â€–uâ€–â€‹(Iâˆ’uâ€‹uâŠ¤â€–uâ€–2),J\_{q}(u)\;=\;\frac{1}{\|u\|}\Bigl(I-\frac{uu^{\top}}{\|u\|^{2}}\Bigr), |  |

so at uâ€‹(t)=tâ€‹vu(t)=tv

|  |  |  |
| --- | --- | --- |
|  | Jqâ€‹(uâ€‹(t))=1tâ€‹(Iâˆ’vâ€‹vâŠ¤).J\_{q}(u(t))\;=\;\frac{1}{t}\bigl(I-vv^{\top}\bigr). |  |

Substituting the expression for the Jacobian Jpâ€‹(u)J\_{p}(u) given in ([9](https://arxiv.org/html/2602.03461v1#A2.E9 "Equation 9 â€£ Proof. â€£ B.1 Proofs of Section 2 â€£ Appendix B Proofs â€£ Soft-Radial Projection for Constrained End-to-End Learning")), we obtain for t>1t>1

|  |  |  |
| --- | --- | --- |
|  | Jpâ€‹(uâ€‹(t))=râ€‹(t2)tâ€‹(Iâˆ’vâ€‹vâŠ¤)+â€„2â€‹tâ€‹râ€²â€‹(t2)â€‹vâ€‹vâŠ¤.J\_{p}(u(t))=\frac{r(t^{2})}{t}(I-vv^{\top})\;+\;2t\,r^{\prime}(t^{2})\,vv^{\top}. |  |

By PropositionÂ [3](https://arxiv.org/html/2602.03461v1#S3 "3 Optimization Guarantees â€£ Soft-Radial Projection for Constrained End-to-End Learning"), the gradient is given by âˆ‡fâ€‹(u)=Jpâ€‹(u)âŠ¤â€‹âˆ‡â„“â€‹(pâ€‹(u))\nabla f(u)=J\_{p}(u)^{\top}\nabla\ell(p(u)). Since â„“â€‹(x)=â€–xâ€–2\ell(x)=\|x\|^{2} implies âˆ‡â„“â€‹(x)=2â€‹x\nabla\ell(x)=2x, and specifically pâ€‹(uâ€‹(t))=râ€‹(t2)â€‹vp(u(t))=r(t^{2})v, we obtain

|  |  |  |
| --- | --- | --- |
|  | âˆ‡fâ€‹(uâ€‹(t))=Jpâ€‹(uâ€‹(t))âŠ¤â€‹(2â€‹râ€‹(t2)â€‹v)=â€„2â€‹râ€‹(t2)â€‹Jpâ€‹(uâ€‹(t))â€‹v,\nabla f(u(t))\;=\;J\_{p}(u(t))^{\top}\bigl(2r(t^{2})v\bigr)\;=\;2r(t^{2})J\_{p}(u(t))v, |  |

where the last equality follows from the symmetry of Jpâ€‹(uâ€‹(t))J\_{p}(u(t)). When applying the Jacobian matrix to the vector vv, the tangential term vanishes because (Iâˆ’vâ€‹vâŠ¤)â€‹v=0(I-vv^{\top})v=0. Only the radial term remains

|  |  |  |
| --- | --- | --- |
|  | Jpâ€‹(uâ€‹(t))â€‹v=(2â€‹tâ€‹râ€²â€‹(t2)â€‹vâ€‹vâŠ¤)â€‹v=â€„2â€‹tâ€‹râ€²â€‹(t2)â€‹v.J\_{p}(u(t))v\;=\;\left(2tr^{\prime}(t^{2})vv^{\top}\right)v\;=\;2tr^{\prime}(t^{2})v. |  |

Substituting this back yields the final gradient norm

|  |  |  |
| --- | --- | --- |
|  | â€–âˆ‡fâ€‹(uâ€‹(t))â€–=â€–2â€‹râ€‹(t2)â‹…2â€‹tâ€‹râ€²â€‹(t2)â€‹vâ€–=â€„4â€‹tâ€‹râ€‹(t2)â€‹râ€²â€‹(t2).\|\nabla f(u(t))\|\;=\;\|2r(t^{2})\cdot 2tr^{\prime}(t^{2})v\|\;=\;4\,t\,r(t^{2})\,r^{\prime}(t^{2}). |  |

It remains to show the existence of a sequence tkâ†’âˆt\_{k}\to\infty such that tkâ€‹râ€²â€‹(tk2)â†’0t\_{k}r^{\prime}(t\_{k}^{2})\to 0. Since limÏâ†’âˆrâ€‹(Ï)=1\lim\_{\rho\to\infty}r(\rho)=1, we have âˆ«0âˆrâ€²â€‹(Ï)â€‹ğ‘‘Ï=1âˆ’râ€‹(0)<âˆ\int\_{0}^{\infty}r^{\prime}(\rho)\,d\rho=1-r(0)<\infty. We claim there exists a sequence Ïkâ†’âˆ\rho\_{k}\to\infty such that Ïkâ€‹râ€²â€‹(Ïk)â†’0\sqrt{\rho\_{k}}\,r^{\prime}(\rho\_{k})\to 0. Indeed, if not, then there exist Îµ>0\varepsilon>0 and Ï0â‰¥0\rho\_{0}\geq 0 such that Ïâ€‹râ€²â€‹(Ï)â‰¥Îµ\sqrt{\rho}\,r^{\prime}(\rho)\geq\varepsilon for all Ïâ‰¥Ï0\rho\geq\rho\_{0}, which implies

|  |  |  |
| --- | --- | --- |
|  | âˆ«0âˆrâ€²â€‹(Ï)â€‹ğ‘‘Ïâ‰¥Îµâ€‹âˆ«Ï0âˆdâ€‹ÏÏ=âˆ,\int\_{0}^{\infty}r^{\prime}(\rho)\,d\rho\;\geq\;\varepsilon\int\_{\rho\_{0}}^{\infty}\frac{d\rho}{\sqrt{\rho}}\;=\;\infty, |  |

a contradiction. Setting tkâ‰”Ïkt\_{k}\coloneqq\sqrt{\rho\_{k}} yields tkâ†’âˆt\_{k}\to\infty and tkâ€‹râ€²â€‹(tk2)â†’0t\_{k}r^{\prime}(t\_{k}^{2})\to 0. Along uâ€‹(tk)=tkâ€‹vu(t\_{k})=t\_{k}v we therefore have â€–âˆ‡fâ€‹(uâ€‹(tk))â€–â†’0\|\nabla f(u(t\_{k}))\|\to 0 while fâ€‹(uâ€‹(tk))âˆ’fâ‹†â†’1f(u(t\_{k}))-f^{\star}\to 1. Consequently, no Î¼>0\mu>0 can satisfy a global PL inequality â€–âˆ‡fâ€‹(u)â€–2â‰¥2â€‹Î¼â€‹(fâ€‹(u)âˆ’fâ‹†)\|\nabla f(u)\|^{2}\geq 2\mu\,(f(u)-f^{\star}) for all uâˆˆâ„nu\in\mathbb{R}^{n}.
âˆ

Given the absence of the global PL condition, we provide convergence guarantees for both smooth and nonsmooth regimes.

\Convergence

\*

###### Proof.

We differentiate between the smooth and non-smooth regimes based on the properties of the data distribution and the resulting regularity of the composite objective.

##### Regime I: Smooth Optimization.

Assume the data distribution â„™\mathbb{P} is absolutely continuous. As established in TheoremÂ [2.5](https://arxiv.org/html/2602.03461v1#S2.Thmtheorem5 "Definition 2.5 (Scalar projection map). â€£ 2 Soft-Radial Projection Layer â€£ Soft-Radial Projection for Constrained End-to-End Learning"), the soft-radial projection pp is differentiable almost everywhere. Consequently, the event that a pre-activation u=gÎ¸â€‹(z)u=g\_{\theta}(z) lands exactly on the non-differentiable boundary âˆ‚ğ’\partial\mathcal{C} has measure zero.

While the pointwise function fâ€‹(u)=â„“â€‹(pâ€‹(u))f(u)=\ell(p(u)) is not globally LL-smooth (due to Jacobian discontinuities at âˆ‚ğ’\partial\mathcal{C}), the *expected* objective Fâ€‹(Î¸)F(\theta) becomes smooth due to the smoothing effect of the integration over an absolutely continuous distribution. However, to provide a tractable bound on the gradients, we analyze the smoothness of ff on the dense open set U=â„nâˆ–âˆ‚ğ’U=\mathbb{R}^{n}\setminus\partial\mathcal{C} where pp is twice differentiable. Assume âˆ‡â„“\nabla\ell is Lâ„“L\_{\ell}-Lipschitz on pâ€‹(K)p(K), and on the smooth pieces Uâˆ©KU\cap K, JpJ\_{p} is LpL\_{p}-Lipschitz. Define the constants over these smooth regions:

|  |  |  |
| --- | --- | --- |
|  | Gâ„“â‰”supxâˆˆpâ€‹(K)â€–âˆ‡â„“â€‹(x)â€–,Bpâ‰”supuâˆˆKâˆ–âˆ‚ğ’â€–Jpâ€‹(u)â€–.G\_{\ell}\coloneqq\sup\_{x\in p(K)}\|\nabla\ell(x)\|,\qquad B\_{p}\coloneqq\sup\_{u\in K\setminus\partial\mathcal{C}}\|J\_{p}(u)\|. |  |

For any u,uâ€²u,u^{\prime} lying in the same connected component of Uâˆ©KU\cap K, we apply the chain rule âˆ‡fâ€‹(u)=Jpâ€‹(u)âŠ¤â€‹âˆ‡â„“â€‹(pâ€‹(u))\nabla f(u)=J\_{p}(u)^{\top}\nabla\ell(p(u)). We decouple the variations in the Jacobian and the loss gradient by adding and subtracting the cross-term Jpâ€‹(u)âŠ¤â€‹âˆ‡â„“â€‹(pâ€‹(uâ€²))J\_{p}(u)^{\top}\nabla\ell(p(u^{\prime})), and applying the triangle inequality:

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–âˆ‡fâ€‹(u)âˆ’âˆ‡fâ€‹(uâ€²)â€–\displaystyle\|\nabla f(u)-\nabla f(u^{\prime})\| | =â€–Jpâ€‹(u)âŠ¤â€‹âˆ‡â„“â€‹(pâ€‹(u))âˆ’Jpâ€‹(uâ€²)âŠ¤â€‹âˆ‡â„“â€‹(pâ€‹(uâ€²))â€–\displaystyle=\|J\_{p}(u)^{\top}\nabla\ell(p(u))-J\_{p}(u^{\prime})^{\top}\nabla\ell(p(u^{\prime}))\| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =â€–Jpâ€‹(u)âŠ¤â€‹âˆ‡â„“â€‹(pâ€‹(u))âˆ’Jpâ€‹(u)âŠ¤â€‹âˆ‡â„“â€‹(pâ€‹(uâ€²))+Jpâ€‹(u)âŠ¤â€‹âˆ‡â„“â€‹(pâ€‹(uâ€²))âˆ’Jpâ€‹(uâ€²)âŠ¤â€‹âˆ‡â„“â€‹(pâ€‹(uâ€²))â€–\displaystyle=\|J\_{p}(u)^{\top}\nabla\ell(p(u))-J\_{p}(u)^{\top}\nabla\ell(p(u^{\prime}))+J\_{p}(u)^{\top}\nabla\ell(p(u^{\prime}))-J\_{p}(u^{\prime})^{\top}\nabla\ell(p(u^{\prime}))\| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤â€–Jpâ€‹(u)â€–â€‹â€–âˆ‡â„“â€‹(pâ€‹(u))âˆ’âˆ‡â„“â€‹(pâ€‹(uâ€²))â€–+â€–âˆ‡â„“â€‹(pâ€‹(uâ€²))â€–â€‹â€–Jpâ€‹(u)âˆ’Jpâ€‹(uâ€²)â€–\displaystyle\leq\|J\_{p}(u)\|\|\nabla\ell(p(u))-\nabla\ell(p(u^{\prime}))\|+\|\nabla\ell(p(u^{\prime}))\|\|J\_{p}(u)-J\_{p}(u^{\prime})\| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤Bpâ€‹Lâ„“â€‹â€–pâ€‹(u)âˆ’pâ€‹(uâ€²)â€–+Gâ„“â€‹Lpâ€‹â€–uâˆ’uâ€²â€–\displaystyle\leq B\_{p}L\_{\ell}\|p(u)-p(u^{\prime})\|+G\_{\ell}L\_{p}\|u-u^{\prime}\| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤(Bp2â€‹Lâ„“+Gâ„“â€‹Lp)â€‹â€–uâˆ’uâ€²â€–.\displaystyle\leq(B\_{p}^{2}L\_{\ell}+G\_{\ell}L\_{p})\|u-u^{\prime}\|. |  |

Thus, ff is LL-smooth *almost everywhere* with constant Lâ‰¤Lâ„“â€‹Bp2+Gâ„“â€‹LpL\leq L\_{\ell}B\_{p}^{2}+G\_{\ell}L\_{p}. Under the assumption of absolute continuity of â„™\mathbb{P}, the objective Fâ€‹(Î¸)=ğ”¼â€‹[fâ€‹(gÎ¸â€‹(z))]F(\theta)=\mathbb{E}[f(g\_{\theta}(z))] inherits differentiability, and the variance of the gradients is bounded by the Lipschitz continuity of ff. Applying standard Stochastic Gradient Descent (SGD) with step size Î·tâˆ1/T\eta\_{t}\propto 1/\sqrt{T} guarantees convergence to a stationary point of FF Ghadimi and Lan ([2013](https://arxiv.org/html/2602.03461v1#bib.bib31 "Stochastic first-and zeroth-order methods for nonconvex stochastic programming")):

|  |  |  |
| --- | --- | --- |
|  | min0â‰¤t<Tâ¡ğ”¼â€‹[â€–âˆ‡Fâ€‹(Î¸t)â€–2]=ğ’ªâ€‹(1T).\min\_{0\leq t<T}\mathbb{E}[\|\nabla F(\theta\_{t})\|^{2}]=\mathcal{O}\left(\frac{1}{\sqrt{T}}\right). |  |

##### Regime II: Nonsmooth Optimization.

If the data distribution is discrete or arbitrary, the differentiability of Fâ€‹(Î¸)F(\theta) cannot be guaranteed. However, we can establish convergence by relying on the theory of *tame functions* Davis et al. ([2020](https://arxiv.org/html/2602.03461v1#bib.bib38 "Stochastic subgradient method converges on tame functions")). This framework covers the vast majority of deep learning architectures and ensures that stochastic subgradient methods converge effectively, provided the objective satisfies two structural properties:

1. 1.

   Lipschitz Continuity: As a consequence of LemmaÂ [2](https://arxiv.org/html/2602.03461v1#S2.F2 "Figure 2 â€£ 2 Soft-Radial Projection Layer â€£ Soft-Radial Projection for Constrained End-to-End Learning"), the soft-radial projection pp is locally Lipschitz. Since the loss â„“\ell and the network gÎ¸g\_{\theta} (e.g., with ReLU activations) are also locally Lipschitz, their composition Fâ€‹(Î¸)F(\theta) inherits this property. This guarantees that the Clarke subdifferential âˆ‚Fâ€‹(Î¸)\partial F(\theta) is non-empty and uniformly bounded on compact sets.
2. 2.

   Tame Objective: The convergence results of Davis et al. ([2020](https://arxiv.org/html/2602.03461v1#bib.bib38 "Stochastic subgradient method converges on tame functions")) apply to the class of *tame* functions. This class encompasses functions constructed from finite compositions of piecewise-polynomials and algebraic operations. Since standard activations (like ReLU) are piecewise-polynomial, and our soft-radial projection is constructed from norms and algebraic radial mappings (assuming a semi-algebraic constraint set ğ’\mathcal{C}, such as a polytope or ball), the composite objective satisfies this property. This guarantees that the loss landscape is geometrically well-behaved, preventing pathological infinite oscillations.

Davis et al. ([2020](https://arxiv.org/html/2602.03461v1#bib.bib38 "Stochastic subgradient method converges on tame functions"), Thm.Â 3.2) proved that for any locally Lipschitz, tame function, the Stochastic Subgradient Method with diminishing step sizes Î·tâ†’0\eta\_{t}\to 0 (and âˆ‘Î·t=âˆ\sum\eta\_{t}=\infty, âˆ‘Î·t2<âˆ\sum\eta\_{t}^{2}<\infty) converges almost surely to the set of Clarke stationary points:

|  |  |  |
| --- | --- | --- |
|  | limTâ†’âˆğ”¼â€‹[minvâˆˆâˆ‚Fâ€‹(Î¸T)â¡â€–vâ€–]=0.\lim\_{T\to\infty}\mathbb{E}\left[\min\_{v\in\partial F(\theta\_{T})}\|v\|\right]=0. |  |

This result holds even if the iterates traverse the non-differentiable boundary âˆ‚ğ’\partial\mathcal{C}, provided the standard bounded variance assumption on the subgradients is met. In our case, this is guaranteed by the local Lipschitz property of the objective on the compact set KK.
âˆ

### B.3 Proofs of Section [4](https://arxiv.org/html/2602.03461v1#S4 "4 Constrained Universal Approximation â€£ Soft-Radial Projection for Constrained End-to-End Learning")

Finally, we prove that the constrained model inherits the universal approximation capabilities of the underlying unconstrained architecture.

\ConstUnvAppr

\*

###### Proof.

Without loss of generality, we assume 0âˆˆIntâ¡(ğ’)0\in\operatorname{Int}(\mathcal{C}) serves as the anchor point (see Coordinate Convention in SectionÂ [2](https://arxiv.org/html/2602.03461v1#S2 "2 Soft-Radial Projection Layer â€£ Soft-Radial Projection for Constrained End-to-End Learning")). Throughout this proof, we denote the uniform norm of a function Ïˆ:ğ’µâ†’â„n\psi:\mathcal{Z}\to\mathbb{R}^{n} by â€–Ïˆâ€–âˆâ‰”supzâˆˆğ’µâ€–Ïˆâ€‹(z)â€–\|\psi\|\_{\infty}\coloneqq\sup\_{z\in\mathcal{Z}}\|\psi(z)\|. We separate the interior and boundary cases.

*Interior case.* Assume the target image satisfies hâ€‹(ğ’µ)âŠ‚Intâ¡(ğ’)h(\mathcal{Z})\subset\operatorname{Int}(\mathcal{C}). Since p:â„nâ†’Intâ¡(ğ’)p:\mathbb{R}^{n}\to\operatorname{Int}(\mathcal{C}) is a homeomorphism (TheoremÂ [2.5](https://arxiv.org/html/2602.03461v1#S2.Thmtheorem5 "Definition 2.5 (Scalar projection map). â€£ 2 Soft-Radial Projection Layer â€£ Soft-Radial Projection for Constrained End-to-End Learning")), the function Ï•â‰”pâˆ’1âˆ˜h:ğ’µâ†’â„n\phi\coloneqq p^{-1}\circ h:\mathcal{Z}\to\mathbb{R}^{n} is continuous. Since ğ’µ\mathcal{Z} is compact, the image Ï•â€‹(ğ’µ)\phi(\mathcal{Z}) is compact. Fix any Î´0>0\delta\_{0}>0 (e.g., Î´0=1\delta\_{0}=1) and define the compact Î´0\delta\_{0}-thickening

|  |  |  |
| --- | --- | --- |
|  | Kâ‰”{Î¾âˆˆâ„nâˆ£distâ¡(Î¾,Ï•â€‹(ğ’µ))â‰¤Î´0},K\coloneqq\bigl\{\xi\in\mathbb{R}^{n}\mid\operatorname{dist}\bigl(\xi,\phi(\mathcal{Z})\bigr)\leq\delta\_{0}\bigr\}, |  |

where distâ¡(Î¾,A)â‰”infaâˆˆAâ€–Î¾âˆ’aâ€–\operatorname{dist}(\xi,A)\coloneqq\inf\_{a\in A}\|\xi-a\|. By the local Lipschitz continuity of pp on the compact set KK, there exists a constant LK<âˆL\_{K}<\infty such that â€–pâ€‹(a)âˆ’pâ€‹(b)â€–â‰¤LKâ€‹â€–aâˆ’bâ€–\|p(a)-p(b)\|\leq L\_{K}\|a-b\| for all a,bâˆˆKa,b\in K. By the universality of ğ’¢\mathcal{G}, there exists a function gâˆˆğ’¢g\in\mathcal{G} such that

|  |  |  |
| --- | --- | --- |
|  | â€–gâˆ’Ï•â€–âˆâ‰¤Î´,Î´â‰”minâ¡{Î´0,ÎµLK}.\|g-\phi\|\_{\infty}\;\leq\;\delta,\qquad\delta\coloneqq\min\Bigl\{\delta\_{0},\ \frac{\varepsilon}{L\_{K}}\Bigr\}. |  |

This implies gâ€‹(ğ’µ)âŠ‚Kg(\mathcal{Z})\subset K. Consequently, for all zâˆˆğ’µz\in\mathcal{Z}:

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–pâ€‹(gâ€‹(z))âˆ’hâ€‹(z)â€–\displaystyle\|p(g(z))-h(z)\| | =â€–pâ€‹(gâ€‹(z))âˆ’pâ€‹(Ï•â€‹(z))â€–\displaystyle=\|p(g(z))-p(\phi(z))\| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤LKâ€‹â€–gâ€‹(z)âˆ’Ï•â€‹(z)â€–\displaystyle\leq L\_{K}\,\|g(z)-\phi(z)\| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤LKâ€‹Î´â‰¤Îµ,\displaystyle\leq L\_{K}\,\delta\;\leq\;\varepsilon, |  |

which implies â€–pâˆ˜gâˆ’hâ€–âˆâ‰¤Îµ\|p\circ g-h\|\_{\infty}\leq\varepsilon, proving the claim for the interior case.

*Boundary case.* For a general target h:ğ’µâ†’ğ’h:\mathcal{Z}\to\mathcal{C}, we define a strictly interior approximation hÎµh\_{\varepsilon} by shrinking hh towards the anchor. Define the shrink operator SÏ„â€‹(x)â‰”(1âˆ’Ï„)â€‹xS\_{\tau}(x)\coloneqq(1-\tau)x. Since ğ’\mathcal{C} is convex and 0âˆˆIntâ¡(ğ’)0\in\operatorname{Int}(\mathcal{C}), we have SÏ„â€‹(ğ’)âŠ‚Intâ¡(ğ’)S\_{\tau}(\mathcal{C})\subset\operatorname{Int}(\mathcal{C}) for any Ï„âˆˆ(0,1)\tau\in(0,1).Let Dâ‰”â€–hâ€–âˆ<âˆD\coloneqq\|h\|\_{\infty}<\infty. If D=0D=0, then hâ‰¡0h\equiv 0, and the interior case applies directly. Otherwise, we select a shrinkage factor Ï„\tau sufficiently small to ensure hÎµh\_{\varepsilon} remains within Îµ/2\varepsilon/2 of hh. Specifically, set:

|  |  |  |
| --- | --- | --- |
|  | Ï„â‰”minâ¡{12,Îµ2â€‹D}âˆˆ(0,1),hÎµâ‰”SÏ„âˆ˜h.\tau\coloneqq\min\Bigl\{\tfrac{1}{2},\ \frac{\varepsilon}{2D}\Bigr\}\in(0,1),\qquad h\_{\varepsilon}\coloneqq S\_{\tau}\circ h. |  |

The uniform distance between the original and shrunken target is bounded by

|  |  |  |
| --- | --- | --- |
|  | â€–hÎµâˆ’hâ€–âˆ=supzâˆˆğ’µâ€–(1âˆ’Ï„)â€‹hâ€‹(z)âˆ’hâ€‹(z)â€–=Ï„â€‹Dâ‰¤Îµ2.\|h\_{\varepsilon}-h\|\_{\infty}\;=\;\sup\_{z\in\mathcal{Z}}\|(1-\tau)h(z)-h(z)\|\;=\;\tau D\;\leq\;\frac{\varepsilon}{2}. |  |

Since the new target satisfies hÎµâ€‹(ğ’µ)âŠ‚Intâ¡(ğ’)h\_{\varepsilon}(\mathcal{Z})\subset\operatorname{Int}(\mathcal{C}), we can invoke the *Interior case* result with precision Îµ/2\varepsilon/2. This guarantees the existence of a function gâˆˆğ’¢g\in\mathcal{G} such that

|  |  |  |
| --- | --- | --- |
|  | â€–pâˆ˜gâˆ’hÎµâ€–âˆâ‰¤Îµ2.\|p\circ g-h\_{\varepsilon}\|\_{\infty}\;\leq\;\frac{\varepsilon}{2}. |  |

Finally, applying the triangle inequality yields the total error bound

|  |  |  |
| --- | --- | --- |
|  | â€–pâˆ˜gâˆ’hâ€–âˆâ‰¤â€–pâˆ˜gâˆ’hÎµâ€–âˆâŸâ‰¤Îµ/2+â€–hÎµâˆ’hâ€–âˆâŸâ‰¤Îµ/2â‰¤Îµ.\|p\circ g-h\|\_{\infty}\;\leq\;\underbrace{\|p\circ g-h\_{\varepsilon}\|\_{\infty}}\_{\leq\varepsilon/2}\;+\;\underbrace{\|h\_{\varepsilon}-h\|\_{\infty}}\_{\leq\varepsilon/2}\;\leq\;\varepsilon. |  |

âˆ

## Appendix C Extended Numerical Results

In this section, we first provide additional insights into the model parameters, specifically how the radial contraction function râ€‹(â‹…)r(\cdot) and the scale parameter Î»\lambda influence the effective optimization landscape. Next, we provide implementation details for the competitor algorithms, DC3 (Donti et al., [2021](https://arxiv.org/html/2602.03461v1#bib.bib16 "DC3: A learning method for optimization with hard constraints")) and HardNet (Min and Azizan, [2024](https://arxiv.org/html/2602.03461v1#bib.bib9 "HardNet: Hard-constrained neural networks with universal approximation guarantees")), which were adjusted to accommodate the (capped) simplex constraints considered in our experiments. We then detail the architectures and hyperparameters utilized within our study. Finally, we conclude with a comprehensive description of our experimental setups.

### C.1 Sensitivity Analysis: Radial Contraction and Scaling

In Sec.Â [5.1](https://arxiv.org/html/2602.03461v1#S5.SS1 "5.1 Implementation Details â€£ 5 Numerical Experiments â€£ Soft-Radial Projection for Constrained End-to-End Learning") we introduced radial contraction functions râ€‹(â‹…)r(\cdot) satisfying the properties of AssumptionÂ [2.3](https://arxiv.org/html/2602.03461v1#S2.Thmtheorem3 "Assumption 2.3 (Radial monotonicity). â€£ 2 Soft-Radial Projection Layer â€£ Soft-Radial Projection for Constrained End-to-End Learning"). The *rational* radial contraction decays polynomially (ğ’ªâ€‹(Ïâˆ’1)\mathcal{O}(\rho^{-1})), whereas the *exponential* and *hyperbolic* forms exhibit exponential decay. Consequently, the rational link preserves larger gradient magnitudes for predictions falling far outside the feasible set (Ïâ‰«1\rho\gg 1), which helps mitigate the vanishing gradient problem for extreme outliers.

Similar to the problem illustrated in Fig.Â [1](https://arxiv.org/html/2602.03461v1#S1.F1 "Figure 1 â€£ 1 Introduction â€£ Soft-Radial Projection for Constrained End-to-End Learning"), we evaluate the role of râ€‹(â‹…)r(\cdot) in terms of warping the effective loss landscape (A), influencing the training loss (B), and determining the convergence to the target xâˆ—x^{\*}. FiguresÂ [3](https://arxiv.org/html/2602.03461v1#A3.F3 "Figure 3 â€£ C.1 Sensitivity Analysis: Radial Contraction and Scaling â€£ Appendix C Extended Numerical Results â€£ Soft-Radial Projection for Constrained End-to-End Learning"), [4](https://arxiv.org/html/2602.03461v1#A3.F4 "Figure 4 â€£ C.1 Sensitivity Analysis: Radial Contraction and Scaling â€£ Appendix C Extended Numerical Results â€£ Soft-Radial Projection for Constrained End-to-End Learning"), and [5](https://arxiv.org/html/2602.03461v1#A3.F5 "Figure 5 â€£ C.1 Sensitivity Analysis: Radial Contraction and Scaling â€£ Appendix C Extended Numerical Results â€£ Soft-Radial Projection for Constrained End-to-End Learning") visualize the effective loss landscape for the *rational* ([3](https://arxiv.org/html/2602.03461v1#S5.E3 "Equation 3 â€£ 5.1 Implementation Details â€£ 5 Numerical Experiments â€£ Soft-Radial Projection for Constrained End-to-End Learning")), *exponential* ([4](https://arxiv.org/html/2602.03461v1#S5.E4 "Equation 4 â€£ 5.1 Implementation Details â€£ 5 Numerical Experiments â€£ Soft-Radial Projection for Constrained End-to-End Learning")), and *hyperbolic* ([5](https://arxiv.org/html/2602.03461v1#S5.E5 "Equation 5 â€£ 5.1 Implementation Details â€£ 5 Numerical Experiments â€£ Soft-Radial Projection for Constrained End-to-End Learning")) functions, respectively. Furthermore, we illustrate in Fig.Â [6](https://arxiv.org/html/2602.03461v1#A3.F6 "Figure 6 â€£ C.1 Sensitivity Analysis: Radial Contraction and Scaling â€£ Appendix C Extended Numerical Results â€£ Soft-Radial Projection for Constrained End-to-End Learning") how the minimum scaling threshold Îµ\varepsilon modifies the landscape geometry surrounding the chosen anchor u0u\_{0}.

![Refer to caption](x4.png)


(a) Î»=0.5\lambda=0.5.

![Refer to caption](x5.png)


(b) Î»=1.0\lambda=1.0.

![Refer to caption](x6.png)


(c) Î»=2.0\lambda=2.0

Figure 3: Soft-radial projection pâ€‹(u)p(u) with *rational* radial contraction ([3](https://arxiv.org/html/2602.03461v1#S5.E3 "Equation 3 â€£ 5.1 Implementation Details â€£ 5 Numerical Experiments â€£ Soft-Radial Projection for Constrained End-to-End Learning")).



![Refer to caption](x7.png)


(a) Î»=0.5\lambda=0.5.

![Refer to caption](x8.png)


(b) Î»=1.0\lambda=1.0.

![Refer to caption](x9.png)


(c) Î»=2.0\lambda=2.0.

Figure 4: Soft-radial projection pâ€‹(u)p(u) with *exponential* radial contraction ([4](https://arxiv.org/html/2602.03461v1#S5.E4 "Equation 4 â€£ 5.1 Implementation Details â€£ 5 Numerical Experiments â€£ Soft-Radial Projection for Constrained End-to-End Learning")).



![Refer to caption](x10.png)


(a) Î»=0.5\lambda=0.5.

![Refer to caption](x11.png)


(b) Î»=1.0\lambda=1.0.

![Refer to caption](x12.png)


(c) Î»=2.0\lambda=2.0.

Figure 5: Soft-radial projection pâ€‹(u)p(u) with *hyperbolic* radial contraction ([5](https://arxiv.org/html/2602.03461v1#S5.E5 "Equation 5 â€£ 5.1 Implementation Details â€£ 5 Numerical Experiments â€£ Soft-Radial Projection for Constrained End-to-End Learning")).



![Refer to caption](x13.png)


(a) Îµ=0.001\varepsilon=0.001.

![Refer to caption](x14.png)


(b) Îµ=0.01\varepsilon=0.01.

![Refer to caption](x15.png)


(c) Îµ=0.1\varepsilon=0.1.

Figure 6: Soft-radial projection pâ€‹(u)p(u) for varying minimum scaling threshold Îµ\varepsilon.

### C.2 Adaptation of Competitor Methods to Capped Simplex Constraints

We explain the adaptation of the competitor methods with respect to the capped simplex introduced in the portfolio setting and formally defined as

|  |  |  |
| --- | --- | --- |
|  | ğ’={wâˆˆâ„N| 0â‰¤w, 1âŠ¤â€‹w=1,wâ‰¤c},\mathcal{C}\;=\;\Bigl\{w\in\mathbb{R}^{N}\ \big|\ 0\leq w,\ \mathds{1}^{\top}w=1,\ w\leq c\Bigr\}, |  |

where ww is the portfolio weight and cc is a cap on the weight.

HardNet. The HardNet framework Min and Azizan ([2024](https://arxiv.org/html/2602.03461v1#bib.bib9 "HardNet: Hard-constrained neural networks with universal approximation guarantees")) enforces feasibility by appending a differentiable projection layer to an unconstrained predictor gÎ¸:ğ’µâ†’â„Ng\_{\theta}:\mathcal{Z}\to\mathbb{R}^{N}. Given an input zâˆˆğ’µz\in\mathcal{Z} and raw output uâ‰”gÎ¸â€‹(z)u\coloneqq g\_{\theta}(z), the method considers input-dependent affine constraints of the form

|  |  |  |
| --- | --- | --- |
|  | bÂ¯â€‹(z)â‰¤Aâ€‹(z)â€‹wâ‰¤bÂ¯â€‹(z).\underline{b}(z)\leq A(z)w\leq\bar{b}(z). |  |

Min and Azizan ([2024](https://arxiv.org/html/2602.03461v1#bib.bib9 "HardNet: Hard-constrained neural networks with universal approximation guarantees")) derive a closed-form projection for this case, denoted HardNet-Aff:

|  |  |  |  |
| --- | --- | --- | --- |
|  | PHNâ€‹(u;z)=u+ARâ€ â€‹(z)â€‹(ReLUâ¡(bÂ¯â€‹(z)âˆ’Aâ€‹(z)â€‹u)âˆ’ReLUâ¡(Aâ€‹(z)â€‹uâˆ’bÂ¯â€‹(z))),P\_{\text{HN}}(u;z)\;=\;u\;+\;A^{\dagger}\_{\mathrm{R}}(z)\Big(\operatorname{ReLU}\big(\underline{b}(z)-A(z)u\big)-\operatorname{ReLU}\big(A(z)u-\bar{b}(z)\big)\Big), |  | (10) |

where ARâ€ â€‹(z)â‰”Aâ€‹(z)âŠ¤â€‹(Aâ€‹(z)â€‹Aâ€‹(z)âŠ¤)âˆ’1A^{\dagger}\_{\mathrm{R}}(z)\coloneqq A(z)^{\top}\!\big(A(z)A(z)^{\top}\big)^{-1} is the Mooreâ€“Penrose *right* pseudoinverse. This closed form assumes that Aâ€‹(z)A(z) has full row rank (AssumptionÂ 5 in Min and Azizan ([2024](https://arxiv.org/html/2602.03461v1#bib.bib9 "HardNet: Hard-constrained neural networks with universal approximation guarantees"))), describing an underdetermined system.
In our portfolio setting, the feasible set is a capped simplex; stacking the equality constraint with the 2â€‹N2N bound constraints yields an *overdetermined* affine system. Consequently, the matrix Aâ€‹AâŠ¤AA^{\top} is singular and the right-pseudoinverse expression inÂ ([10](https://arxiv.org/html/2602.03461v1#A3.E10 "Equation 10 â€£ C.2 Adaptation of Competitor Methods to Capped Simplex Constraints â€£ Appendix C Extended Numerical Results â€£ Soft-Radial Projection for Constrained End-to-End Learning")) is not directly applicable. We therefore use a least-squares relaxation of the HardNet correction. This involves defining a (signed) violation vector

|  |  |  |  |
| --- | --- | --- | --- |
|  | vâ€‹(u;z)â‰”ReLUâ¡(bÂ¯â€‹(z)âˆ’Aâ€‹u)âˆ’ReLUâ¡(Aâ€‹uâˆ’bÂ¯â€‹(z)).v(u;z)\;\coloneqq\;\operatorname{ReLU}\big(\underline{b}(z)-Au\big)-\operatorname{ReLU}\big(Au-\bar{b}(z)\big). |  | (11) |

Since the resulting static constraint matrix AA has full column rank in our construction, we compute a least-squares correction direction via the *left* pseudoinverse

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î”â€‹(u;z)â‰”argâ¡minÎ”âˆˆâ„Nâ¡â€–Aâ€‹Î”âˆ’vâ€‹(u;z)â€–2=ALâ€ â€‹vâ€‹(u;z),ALâ€ â‰”(AâŠ¤â€‹A)âˆ’1â€‹AâŠ¤.\Delta(u;z)\;\coloneqq\;\arg\min\_{\Delta\in\mathbb{R}^{N}}\ \|A\Delta-v(u;z)\|^{2}\;=\;A^{\dagger}\_{\mathrm{L}}\,v(u;z),\qquad A^{\dagger}\_{\mathrm{L}}\coloneqq(A^{\top}A)^{-1}A^{\top}. |  | (12) |

We then set w~â‰”u+Î”â€‹(u;z)\tilde{w}\coloneqq u+\Delta(u;z). A conceptually broader but computationally slower alternative proposed by the authors for general convex constraints is to embed the projection as a differentiable convex optimization layer Agrawal et al. ([2019](https://arxiv.org/html/2602.03461v1#bib.bib18 "Differentiable convex optimization layers")), referred to as HardNet-Cvx. To remove any remaining numerical residuals during evaluation, we additionally apply an exact orthogonal projection onto the capped simplex, w=Pâ€‹(w~)w=P(\tilde{w}), to guarantee wâˆˆğ’w\in\mathcal{C}.

##### Symmetric DC3 for the capped simplex.

We adapt the Deep Constraint Completion and Correction (DC3) framework (Donti et al., [2021](https://arxiv.org/html/2602.03461v1#bib.bib16 "DC3: A learning method for optimization with hard constraints")) to ğ’\mathcal{C}. Standard DC3 selects Nâˆ’1N-1 independent variables and defines the remaining coordinate as dependent via the equality constraint. Applied directly to the raw network output u=gÎ¸â€‹(z)u=g\_{\theta}(z), this introduces an asymmetry: the dependent coordinate absorbs the entire sum deviation, often producing unstable magnitudes and gradients. To mitigate this, we employ a *symmetric* initialization that first distributes the sum deviation evenly by orthogonally projecting uu onto the simplex hyperplane
â„‹â‰”{wâˆˆâ„N:ğŸâŠ¤â€‹w=1}.\mathcal{H}\coloneqq\{w\in\mathbb{R}^{N}:\mathbf{1}^{\top}w=1\}.
Concretely, we set

|  |  |  |  |
| --- | --- | --- | --- |
|  | w(0)â‰”Pâ„‹â€‹(u)=uâˆ’1Nâ€‹(ğŸ™âŠ¤â€‹uâˆ’1)â€‹ğŸ™âˆˆâ„‹.w^{(0)}\;\coloneqq\;P\_{\mathcal{H}}(u)\;=\;u-\frac{1}{N}\big(\mathds{1}^{\top}u-1\big)\mathds{1}\;\in\;\mathcal{H}. |  | (13) |

We then take the first Nâˆ’1N-1 coordinates as free variables
Î¾â‰”(w1(0),â€¦,wNâˆ’1(0))âŠ¤âˆˆâ„Nâˆ’1\xi\coloneqq(w^{(0)}\_{1},\ldots,w^{(0)}\_{N-1})^{\top}\in\mathbb{R}^{N-1}
and enforce the equality constraint through a completion map

|  |  |  |  |
| --- | --- | --- | --- |
|  | wâ€‹(Î¾)â‰”[Î¾1âˆ’ğŸ™âŠ¤â€‹Î¾],w(\xi)\;\coloneqq\;\begin{bmatrix}\xi\\ 1-\mathds{1}^{\top}\xi\end{bmatrix}, |  | (14) |

which ensures ğŸ™âŠ¤â€‹wâ€‹(Î¾)=1\mathds{1}^{\top}w(\xi)=1 for all Î¾\xi.
To correct violations of the bound constraints 0â‰¤wâ‰¤c0\leq w\leq c, we minimize a squared hinge-residual energy

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ’±(Î¾)â‰”âˆ‘i=1N(ReLU(âˆ’wi(Î¾))2+ReLU(wi(Î¾)âˆ’ci)2),\mathcal{V}(\xi)\;\coloneqq\;\sum\_{i=1}^{N}\Big(\operatorname{ReLU}\big(-w\_{i}(\xi)\big)^{2}+\operatorname{ReLU}\big(w\_{i}(\xi)-c\_{i}\big)^{2}\Big), |  | (15) |

via an unrolled gradient descent loop on the free variables

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î¾(t+1)=Î¾(t)âˆ’Î·â€‹âˆ‡Î¾ğ’±â€‹(Î¾(t)),t=0,â€¦,Lâˆ’1.\xi^{(t+1)}\;=\;\xi^{(t)}-\eta\,\nabla\_{\xi}\mathcal{V}\big(\xi^{(t)}\big),\qquad t=0,\ldots,L-1. |  | (16) |

The final output is given by w(L)=wâ€‹(Î¾(L))w^{(L)}=w(\xi^{(L)}). Gradients are backpropagated through the completion mapÂ ([14](https://arxiv.org/html/2602.03461v1#A3.E14 "Equation 14 â€£ Symmetric DC3 for the capped simplex. â€£ C.2 Adaptation of Competitor Methods to Capped Simplex Constraints â€£ Appendix C Extended Numerical Results â€£ Soft-Radial Projection for Constrained End-to-End Learning")), properly accounting for the dependence of the dependent coordinate on the free variables. Similar to the HardNet implementation, we apply an orthogonal projection during evaluation to ensure strict feasibility.

### C.3 Hyperparameter Tuning

Table [4](https://arxiv.org/html/2602.03461v1#A3.T4 "Table 4 â€£ C.3 Hyperparameter Tuning â€£ Appendix C Extended Numerical Results â€£ Soft-Radial Projection for Constrained End-to-End Learning") summarizes the hyperparameters used in our experiments, distinguishing between the portfolio optimization and ride-sharing dispatch task. For both domains, we perform a grid search to select the optimal configuration based on validation performance. As noted earlier, for HardNet and DC3, we apply a final orthogonal projection to the candidate solution during evaluation only. This ensures that the evaluation metrics are not penalized by minor infeasibilities caused by numerical rounding errors. Crucially, we omit this strict projection step during training to preserve gradient flow and prevent the deterioration of learning signals.

Table 4: Hyperparameter settings for portfolio optimization and ride-sharing dispatch experiments.

|  |  |  |
| --- | --- | --- |
| Parameter | Portfolio Optimization | Ride-Sharing Dispatch |
| Architecture & Training | | |
| Base Architecture | LSTM | MLP |
| Hidden Units | {32,64}\{32,64\} | |
| Dropout Rate | {0.1,0.2}\{0.1,0.2\} | |
| Optimizer | Adam | |
| Learning Rate | {1Ã—10âˆ’4,5Ã—10âˆ’4}\{1\times 10^{-4},5\times 10^{-4}\} | |
| Lookback Horizon (HH) | {10,30}\{10,30\} | 2424 |
| Batch Size | 6464 | 128128 |
| Training Epochs | 5050 | 100100 |
| Projection Layer Hyperparameters | | |
| Softmax Temperature (Ï„\tau) | {0.1,0.5,1.0,2.0}\{0.1,0.5,1.0,2.0\} | |
| Soft-radial Î»\lambda | {0.5,1.0,2.0,5.0,10.0}\{0.5,1.0,2.0,5.0,10.0\} | |
| Soft-radial râ€‹(â‹…)r(\cdot) | {Fractional, Exponential, Hyperbolic} | |
| HardNet Steps | {1,3,5}\{1,3,5\} | |
| DC3 Steps | {1,3,5}\{1,3,5\} | |
| DC3 Learning Rate | {10âˆ’2,10âˆ’1}\{10^{-2},10^{-1}\} | |
| DC3 Momentum | {0.5,0.9}\{0.5,0.9\} | |

### C.4 Extended Analysis: Portfolio Optimization

#### C.4.1 Discussion: Deep Portfolio Optimization vs. Classical Approaches

The formulation presented in SectionÂ [5.2](https://arxiv.org/html/2602.03461v1#S5.SS2 "5.2 Portfolio Optimization â€£ 5 Numerical Experiments â€£ Soft-Radial Projection for Constrained End-to-End Learning") represents a deep portfolio optimization problem Zhang et al. ([2020](https://arxiv.org/html/2602.03461v1#bib.bib45 "Deep learning for portfolio optimization")), which fundamentally differs from the classical mean-variance optimization framework Markowitz ([1952](https://arxiv.org/html/2602.03461v1#bib.bib35 "Portfolio selection")).

Predict-then-Optimize vs.Â End-to-End Learning.
Classical mean-variance optimization typically operates in a â€œpredict-then-optimizeâ€ framework: one first estimates the expected return vector Î¼âˆˆâ„N\mu\in\mathbb{R}^{N} and the covariance matrix Î£âˆˆğ•Š+N\Sigma\in\mathbb{S}^{N}\_{+}, and then solves a quadratic program to maximize wâŠ¤â€‹Î¼âˆ’Î¶â€‹wâŠ¤â€‹Î£â€‹ww^{\top}\mu-\zeta w^{\top}\Sigma w. This two-step process suffers from two major limitations. First, it separates the prediction error from the optimization cost; a small error in estimating Î£\Sigma can lead to drastically suboptimal portfolios (a phenomenon formally known as the â€œerror maximizationâ€ property of Markowitz Michaud ([1989](https://arxiv.org/html/2602.03461v1#bib.bib36 "The Markowitz optimization enigma: Is â€˜optimizedâ€™ optimal?"))). Second, while standard sample estimators neglect temporal dependencies (implicitly assuming i.i.d.Â returns), more advanced dynamic estimators (e.g., generalized autoregressive conditional heteroskedasticity (GARCH) model) often impose rigid parametric structures that fail to capture the complex, non-linear interactions between assets that recurrent neural network architectures in the form of gÎ¸g\_{\theta} are designed to exploit.

Parameter Sensitivity and Convexity.
A significant hurdle in classical frameworks is the sensitivity to the risk-aversion parameter Î¶\zeta, which requires frequent recalibration. Our formulation bypasses this by directly maximizing the Sharpe ratioâ€”a scale-invariant metric that eliminates the need for a subjective risk-return trade-off coefficient. While the static Sharpe ratio maximization is a quasiconcave problem that can be reformulated as a Second-Order Cone Program (SOCP) for global convergence, parameterizing the weights via a neural network gÎ¸g\_{\theta} introduces non-convexity. We accept this trade-off to leverage the representational power of deep learning; the model can ingest complex state histories to generate non-linear weight mappings, far surpassing the constraints of the linear-quadratic assumptions in traditional finance.

Transaction Cost as Temporal Regularization. Finally, we address the cost of trading. Classical frameworks often use static L2L\_{2} regularization to prevent overfitting or L1L\_{1} regularization to enforce sparsity Brodie et al. ([2009](https://arxiv.org/html/2602.03461v1#bib.bib34 "Sparse and stable Markowitz portfolios")). However, these spatial constraints fail to penalize policy volatility: a network can satisfy strict sparsity constraints while still oscillating wildly between assets over time.

To enforce temporal consistency, we explicitly model transaction costs via the term Î³2â€‹â€–wtâˆ’wtâˆ’1+â€–1\frac{\gamma}{2}\|w\_{t}-w\_{t-1}^{+}\|\_{1}. Unlike static penalties, this targets the velocity of the portfolio turnover. Crucially, costs are computed relative to the drifted portfolio wtâˆ’1+w\_{t-1}^{+}, ensuring that passive price movements do not incur penalties. Let wtâˆ’1w\_{t-1} be the weights at tâˆ’1t-1. Over the interval (tâˆ’1,t](t-1,t], price movements yty\_{t} cause these weights to evolve naturally into the drifted weights wtâˆ’1+=(diagâ€‹(yt)â€‹wtâˆ’1)/(ytâŠ¤â€‹wtâˆ’1)w\_{t-1}^{+}=(\text{diag}(y\_{t})\,w\_{t-1})/(y\_{t}^{\top}w\_{t-1}). The numerator captures asset-specific growth, while the denominator renormalizes the sum.

###### Remark C.1 (Cost normalization).

The factor 1/21/2 normalizes the L1L\_{1} distance to represent one-way turnover (the fraction of portfolio value actively replaced). Thus, Î³\gamma represents the effective cost per unit of turnover.

This formulation has a profound effect on the learning dynamics. By including this cost directly in the objective, the gradient signal penalizes high-frequency control noise. The policy learns a valid economic trade-off: it naturally creates a *hysteresis effect*, where the portfolio weights exhibit inertia against small market fluctuations. This recovers the classical *no-trade zone* behavior predicted by Davis and Norman ([1990](https://arxiv.org/html/2602.03461v1#bib.bib55 "Portfolio selection with transaction costs")): rebalancing is only performed if the expected Sharpe ratio gain strictly outweighs the explicit cost of the action.

#### C.4.2 Descriptive Statistics

In Table [5](https://arxiv.org/html/2602.03461v1#A3.T5 "Table 5 â€£ C.4.2 Descriptive Statistics â€£ C.4 Extended Analysis: Portfolio Optimization â€£ Appendix C Extended Numerical Results â€£ Soft-Radial Projection for Constrained End-to-End Learning"), we provide descriptive statistics for the asset universes included in our analysis. We evaluate performance across three distinct portfolios, each representing a different difficulty level and correlation structure:

* â€¢

  Global: The Global portfolio aggregates a diverse set of multi-asset class indices covering global equities, treasury bonds, and commodities. It represents a macro-allocation problem characterized by lower average correlation.
* â€¢

  Sectors: The Sectors dataset comprises the 11 primary Global Industry Classification Standard (GICS) sectors. It serves as a standard industry benchmark with high inter-asset correlation, effectively representing the broader US economy.
* â€¢

  Liquid: The Liquid collection contains a selection of 50 high-volume individual stocks. This universe captures the higher volatility and idiosyncratic risk associated with individual stock picking.

Returns are derived from Yahoo Finance adjusted closing prices to account for corporate actions, such as splits and dividends, and are partitioned chronologically to strictly prevent look-ahead bias.333We note that survivorship bias is not compensated for within our analysis, and assets with missing data are omitted from the study. The training set spans from 2010/01/01 to 2018/12/31, covering the post-financial-crisis recovery. The validation set, used for hyperparameter tuning, covers 2019/01/01 to 2020/12/31. The test set extends from 2021/01/01 to 2025/12/31.

Table [5](https://arxiv.org/html/2602.03461v1#A3.T5 "Table 5 â€£ C.4.2 Descriptive Statistics â€£ C.4 Extended Analysis: Portfolio Optimization â€£ Appendix C Extended Numerical Results â€£ Soft-Radial Projection for Constrained End-to-End Learning") reports the annualized mean and median returns alongside annualized volatility. To fully characterize the risk profile and market dynamics, we also include the Maximum Drawdown (Max DD), the average pairwise correlation between assets (Ï±Â¯\bar{\varrho}), the lag-1 autocorrelation (AC(1)), and the Kolmogorov-Smirnov pp-value (pKSp\_{\text{KS}}). As shown in the table, the test period exhibits significantly higher volatility and maximum drawdown compared to the training period, reflecting more turbulent market conditions. This distribution shift is quantified by pKSp\_{\text{KS}}: for the Global, Liquid, and Sectors universes, pKS<0.05p\_{\text{KS}}<0.05 in the test set, rejecting the null hypothesis that the test data follows the training distribution. This confirms that our evaluation effectively tests the modelâ€™s generalization to out-of-distribution market regimes.

Table 5: Summary statistics. Returns, Volatility, and Drawdown are in percentages. Ï±Â¯\bar{\varrho} is average correlation. pKSp\_{\text{KS}} tests for regime shift.

|  |  |  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Universe | Split | Î¼(%)\mu\,(\%) | Median (%)(\%) | Ïƒ(%)\sigma\,(\%) | MaxDD (%)(\%) | Ï±Â¯\bar{\varrho} | AC(1) | pKSp\_{\text{KS}} |
| Global | Train | 7.307.30 | 11.8411.84 | 17.4717.47 | âˆ’19.37-19.37 | 0.350.35 | âˆ’0.037-0.037 |  |
| Global | Val | 7.467.46 | 14.2214.22 | 15.6815.68 | âˆ’13.87-13.87 | 0.320.32 | âˆ’0.020-0.020 | 0.6070.607 |
| Global | Test | 9.309.30 | 13.6013.60 | 22.3422.34 | âˆ’28.46-28.46 | 0.460.46 | âˆ’0.045-0.045 | 0.0110.011 |
| Liquid | Train | 19.1219.12 | 14.7214.72 | 24.9824.98 | âˆ’16.11-16.11 | 0.380.38 | âˆ’0.021-0.021 |  |
| Liquid | Val | 19.7119.71 | 30.7630.76 | 25.4025.40 | âˆ’18.11-18.11 | 0.390.39 | âˆ’0.029-0.029 | 0.3190.319 |
| Liquid | Test | 16.9516.95 | 16.9316.93 | 31.4431.44 | âˆ’30.56-30.56 | 0.400.40 | âˆ’0.088-0.088 | 0.0190.019 |
| Sectors | Train | 13.4613.46 | 20.1720.17 | 17.0517.05 | âˆ’18.61-18.61 | 0.700.70 | âˆ’0.037-0.037 |  |
| Sectors | Val | 9.759.75 | 26.7226.72 | 17.1517.15 | âˆ’18.47-18.47 | 0.580.58 | âˆ’0.003-0.003 | 0.8260.826 |
| Sectors | Test | 13.9913.99 | 22.0122.01 | 24.5724.57 | âˆ’36.91-36.91 | 0.650.65 | âˆ’0.107-0.107 | 0.0070.007 |

#### C.4.3 Implementation Details

In the portfolio optimization setting, we augment raw asset returns with rolling volatility and rolling correlation to a cross-sectional market proxy (defined as the mean return across the universe). These features are computed over the lookback horizon HH and normalized using training-set statistics to strictly prevent look-ahead bias. We acknowledge that achieving state-of-the-art financial performance typically requires a broader set of predictive signals (alphas). However, our primary objective is to evaluate the stability of the learning process under the high noise-to-signal ratio characteristic of financial time series, rather than to maximize the Sharpe ratio through signal engineering.

As discussed in Sec.Â [5.2](https://arxiv.org/html/2602.03461v1#S5.SS2 "5.2 Portfolio Optimization â€£ 5 Numerical Experiments â€£ Soft-Radial Projection for Constrained End-to-End Learning"), the L1L\_{1} transaction cost term inÂ ([7](https://arxiv.org/html/2602.03461v1#S5.E7 "Equation 7 â€£ 5.2 Portfolio Optimization â€£ 5 Numerical Experiments â€£ Soft-Radial Projection for Constrained End-to-End Learning")) is non-differentiable at zero, posing a challenge for gradient-based optimization. To address this, we replace the exact L1L\_{1}-norm with the Pseudo-Huber loss, a reparameterization of the smooth penalty introduced byÂ Charbonnier et al. ([1994](https://arxiv.org/html/2602.03461v1#bib.bib46 "Two deterministic half-quadratic regularization algorithms for computed imaging")), defined as

|  |  |  |  |
| --- | --- | --- | --- |
|  | LÎ´â€‹(wt,wtâˆ’1+)=âˆ‘i=1N(Î´2+(wi,tâˆ’wi,tâˆ’1+)2âˆ’Î´)L\_{\delta}(w\_{t},w\_{t-1}^{+})=\sum\_{i=1}^{N}\left(\sqrt{\delta^{2}+(w\_{i,t}-w\_{i,t-1}^{+})^{2}}-\delta\right) |  | (17) |

where Î´>0\delta>0 controls the steepness. As Î´â†’0\delta\to 0, this function converges to â€–wtâˆ’wtâˆ’1+â€–1\|w\_{t}-w\_{t-1}^{+}\|\_{1}, preserving theoretical consistency while enabling stable gradient flow.

#### C.4.4 Additional Empirical Results

In SectionÂ [5](https://arxiv.org/html/2602.03461v1#S5 "5 Numerical Experiments â€£ Soft-Radial Projection for Constrained End-to-End Learning"), we presented results for the Sharpe ratio maximization formulation defined in ([7](https://arxiv.org/html/2602.03461v1#S5.E7 "Equation 7 â€£ 5.2 Portfolio Optimization â€£ 5 Numerical Experiments â€£ Soft-Radial Projection for Constrained End-to-End Learning")). A common concern in practice is that the explicit inclusion of transaction costs may encourage trivial solutions where the network prioritizes weight stability over active performance optimization. To address this, we provide comparative results for portfolio optimization where training is performed without modeling transaction costs (i.e., omitting the Î³2â€‹â€–wtâˆ’wtâˆ’1+â€–1\frac{\gamma}{2}\|w\_{t}-w\_{t-1}^{+}\|\_{1} term). We refer to the formulation excluding these costs from the loss function as *training gross*, and the standard formulation including them as *training net*. Crucially, to ensure a fair comparison, we report the net Sharpe ratio and turnover for both configurations during evaluation. TableÂ [6](https://arxiv.org/html/2602.03461v1#A3.T6 "Table 6 â€£ C.4.4 Additional Empirical Results â€£ C.4 Extended Analysis: Portfolio Optimization â€£ Appendix C Extended Numerical Results â€£ Soft-Radial Projection for Constrained End-to-End Learning") and TableÂ [7](https://arxiv.org/html/2602.03461v1#A3.T7 "Table 7 â€£ C.4.4 Additional Empirical Results â€£ C.4 Extended Analysis: Portfolio Optimization â€£ Appendix C Extended Numerical Results â€£ Soft-Radial Projection for Constrained End-to-End Learning") present the aggregated results for the *training gross* and *training net* settings, respectively. Overall, we observe that explicitly considering transaction costs during training leads to a decrease in test-time turnover across methods. Consistent with our findings in SectionÂ [5](https://arxiv.org/html/2602.03461v1#S5 "5 Numerical Experiments â€£ Soft-Radial Projection for Constrained End-to-End Learning"), our *soft-radial projection* demonstrates superior financial performance and greater robustness to random seed initialization.

Table 6: Portfolio optimization results for the *training gross* objective (costs omitted from loss), aggregated over five random seeds (mean Â±\pm std).

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
| Universe | Feasible Set | Method | Sharpe Ratio (Net) | Turnover |
| Global | Î”\Delta | Softmax | 0.03â€‹(Â±0.29)0.03\,(\pm 0.29) | 0.27â€‹(Â±0.04)0.27\,(\pm 0.04) |
| O-Proj | 0.15â€‹(Â±0.32)0.15\,(\pm 0.32) | 0.23â€‹(Â±0.05)0.23\,(\pm 0.05) |
| Soft-Radial | 0.55â€‹(Â±0.04)0.55\,(\pm 0.04) | 0.06â€‹(Â±0.02)0.06\,(\pm 0.02) |
| Global | c=0.15c=0.15 | O-Proj | 0.33â€‹(Â±0.13)0.33\,(\pm 0.13) | 0.15â€‹(Â±0.01)0.15\,(\pm 0.01) |
| DC3 | 0.34â€‹(Â±0.14)0.34\,(\pm 0.14) | 0.19â€‹(Â±0.01)0.19\,(\pm 0.01) |
| HardNet | 0.43â€‹(Â±0.13)0.43\,(\pm 0.13) | 0.17â€‹(Â±0.02)0.17\,(\pm 0.02) |
| Soft-Radial | 0.60â€‹(Â±0.09)0.60\,(\pm 0.09) | 0.06â€‹(Â±0.02)0.06\,(\pm 0.02) |
| Liquid | Î”\Delta | Softmax | 0.61â€‹(Â±0.32)0.61\,(\pm 0.32) | 0.22â€‹(Â±0.05)0.22\,(\pm 0.05) |
| O-Proj | 0.44â€‹(Â±0.25)0.44\,(\pm 0.25) | 0.44â€‹(Â±0.02)0.44\,(\pm 0.02) |
| Soft-Radial | 0.87â€‹(Â±0.03)0.87\,(\pm 0.03) | 0.09â€‹(Â±0.01)0.09\,(\pm 0.01) |
| Liquid | c=0.05c=0.05 | O-Proj | 0.56â€‹(Â±0.12)0.56\,(\pm 0.12) | 0.20â€‹(Â±0.01)0.20\,(\pm 0.01) |
| DC3 | 0.49â€‹(Â±0.18)0.49\,(\pm 0.18) | 0.23â€‹(Â±0.02)0.23\,(\pm 0.02) |
| HardNet | 0.59â€‹(Â±0.19)0.59\,(\pm 0.19) | 0.20â€‹(Â±0.01)0.20\,(\pm 0.01) |
| Soft-Radial | 0.87â€‹(Â±0.02)0.87\,(\pm 0.02) | 0.07â€‹(Â±0.01)0.07\,(\pm 0.01) |
| Sectors | Î”\Delta | Softmax | 0.39â€‹(Â±0.16)0.39\,(\pm 0.16) | 0.26â€‹(Â±0.04)0.26\,(\pm 0.04) |
| O-Proj | 0.56â€‹(Â±0.14)0.56\,(\pm 0.14) | 0.23â€‹(Â±0.02)0.23\,(\pm 0.02) |
| Soft-Radial | 0.74â€‹(Â±0.04)0.74\,(\pm 0.04) | 0.11â€‹(Â±0.02)0.11\,(\pm 0.02) |
| Sectors | c=0.5c=0.5 | O-Proj | 0.58â€‹(Â±0.08)0.58\,(\pm 0.08) | 0.21â€‹(Â±0.03)0.21\,(\pm 0.03) |
| DC3 | 0.55â€‹(Â±0.07)0.55\,(\pm 0.07) | 0.20â€‹(Â±0.02)0.20\,(\pm 0.02) |
| HardNet | 0.63â€‹(Â±0.07)0.63\,(\pm 0.07) | 0.20â€‹(Â±0.03)0.20\,(\pm 0.03) |
| Soft-Radial | 0.75â€‹(Â±0.05)0.75\,(\pm 0.05) | 0.13â€‹(Â±0.02)0.13\,(\pm 0.02) |
| Sectors | c=0.15c=0.15 | O-Proj | 0.73â€‹(Â±0.10)0.73\,(\pm 0.10) | 0.09â€‹(Â±0.00)0.09\,(\pm 0.00) |
| DC3 | 0.72â€‹(Â±0.02)0.72\,(\pm 0.02) | 0.07â€‹(Â±0.00)0.07\,(\pm 0.00) |
| HardNet | 0.82â€‹(Â±0.05)0.82\,(\pm 0.05) | 0.07â€‹(Â±0.01)0.07\,(\pm 0.01) |
| Soft-Radial | 0.82â€‹(Â±0.03)0.82\,(\pm 0.03) | 0.03â€‹(Â±0.01)0.03\,(\pm 0.01) |




Table 7: Portfolio optimization results for the *training net* objective (costs included in loss), aggregated over five random seeds (mean Â±\pm std).

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
| Universe | Feasible Set | Method | Sharpe Ratio (Net) | Turnover |
| Global | Î”\Delta | Softmax | 0.21â€‹(Â±0.29)0.21\,(\pm 0.29) | 0.31â€‹(Â±0.04)0.31\,(\pm 0.04) |
| O-Proj | 0.27â€‹(Â±0.20)0.27\,(\pm 0.20) | 0.20â€‹(Â±0.03)0.20\,(\pm 0.03) |
| Soft-Radial | 0.68â€‹(Â±0.09)0.68\,(\pm 0.09) | 0.05â€‹(Â±0.03)0.05\,(\pm 0.03) |
| Global | c=0.15c=0.15 | O-Proj | 0.49â€‹(Â±0.18)0.49\,(\pm 0.18) | 0.17â€‹(Â±0.02)0.17\,(\pm 0.02) |
| DC3 | 0.38â€‹(Â±0.15)0.38\,(\pm 0.15) | 0.18â€‹(Â±0.02)0.18\,(\pm 0.02) |
| HardNet | 0.44â€‹(Â±0.13)0.44\,(\pm 0.13) | 0.16â€‹(Â±0.02)0.16\,(\pm 0.02) |
| Soft-Radial | 0.68â€‹(Â±0.05)0.68\,(\pm 0.05) | 0.05â€‹(Â±0.01)0.05\,(\pm 0.01) |
| Liquid | Î”\Delta | Softmax | 0.63â€‹(Â±0.19)0.63\,(\pm 0.19) | 0.22â€‹(Â±0.05)0.22\,(\pm 0.05) |
| O-Proj | 0.25â€‹(Â±0.18)0.25\,(\pm 0.18) | 0.48â€‹(Â±0.02)0.48\,(\pm 0.02) |
| Soft-Radial | 0.90â€‹(Â±0.03)0.90\,(\pm 0.03) | 0.06â€‹(Â±0.00)0.06\,(\pm 0.00) |
| Liquid | c=0.05c=0.05 | O-Proj | 0.64â€‹(Â±0.09)0.64\,(\pm 0.09) | 0.22â€‹(Â±0.01)0.22\,(\pm 0.01) |
| DC3 | 0.63â€‹(Â±0.08)0.63\,(\pm 0.08) | 0.23â€‹(Â±0.01)0.23\,(\pm 0.01) |
| HardNet | 0.62â€‹(Â±0.11)0.62\,(\pm 0.11) | 0.19â€‹(Â±0.01)0.19\,(\pm 0.01) |
| Soft-Radial | 0.90â€‹(Â±0.02)0.90\,(\pm 0.02) | 0.05â€‹(Â±0.00)0.05\,(\pm 0.00) |
| Sectors | Î”\Delta | Softmax | 0.43â€‹(Â±0.24)0.43\,(\pm 0.24) | 0.25â€‹(Â±0.04)0.25\,(\pm 0.04) |
| O-Proj | 0.68â€‹(Â±0.21)0.68\,(\pm 0.21) | 0.21â€‹(Â±0.03)0.21\,(\pm 0.03) |
| Soft-Radial | 0.76â€‹(Â±0.04)0.76\,(\pm 0.04) | 0.09â€‹(Â±0.03)0.09\,(\pm 0.03) |
| Sectors | c=0.5c=0.5 | O-Proj | 0.73â€‹(Â±0.23)0.73\,(\pm 0.23) | 0.18â€‹(Â±0.02)0.18\,(\pm 0.02) |
| DC3 | 0.72â€‹(Â±0.08)0.72\,(\pm 0.08) | 0.17â€‹(Â±0.02)0.17\,(\pm 0.02) |
| HardNet | 0.66â€‹(Â±0.08)0.66\,(\pm 0.08) | 0.19â€‹(Â±0.03)0.19\,(\pm 0.03) |
| Soft-Radial | 0.77â€‹(Â±0.07)0.77\,(\pm 0.07) | 0.12â€‹(Â±0.02)0.12\,(\pm 0.02) |
| Sectors | c=0.15c=0.15 | O-Proj | 0.80â€‹(Â±0.06)0.80\,(\pm 0.06) | 0.09â€‹(Â±0.01)0.09\,(\pm 0.01) |
| DC3 | 0.78â€‹(Â±0.06)0.78\,(\pm 0.06) | 0.07â€‹(Â±0.00)0.07\,(\pm 0.00) |
| HardNet | 0.84â€‹(Â±0.02)0.84\,(\pm 0.02) | 0.06â€‹(Â±0.01)0.06\,(\pm 0.01) |
| Soft-Radial | 0.87â€‹(Â±0.04)0.87\,(\pm 0.04) | 0.02â€‹(Â±0.00)0.02\,(\pm 0.00) |

### C.5 Extended Analysis: Ride-Sharing Dispatch

Our taxi dataset is derived from the publicly available records of the NYC Taxi and Limousine Commission (TLC) for the period of January to June 2024. To ensure a robust evaluation of temporal generalization, the data is split chronologically into 70% training, 15% validation, and 15% testing sets. To capture the multi-scale periodicity of urban mobility, we represent temporal states using cyclical sine-cosine encodings for both daily and weekly cycles. Unlike categorical indicators that create artificial boundaries at midnight, this continuous representation allows the policy network gÎ¸g\_{\theta} to learn smooth, time-dependent transition boundaries. This enables the model to proactively align fleet distribution with anticipated demand peaks, maximizing the global served rate within the constraints of the time-varying supply StS\_{t}.

In SectionÂ [5](https://arxiv.org/html/2602.03461v1#S5 "5 Numerical Experiments â€£ Soft-Radial Projection for Constrained End-to-End Learning") (TableÂ [2](https://arxiv.org/html/2602.03461v1#S5.T2 "Table 2 â€£ 5.3 Ride-Sharing Dispatch â€£ 5 Numerical Experiments â€£ Soft-Radial Projection for Constrained End-to-End Learning")), we presented results for an MLP model subject to the scaled capped simplex with Îº=0.1\kappa=0.1. Here, we introduce a more challenging experimental setting with Îº=0.02\kappa=0.02. This tighter constraint creates a significantly more difficult allocation problem, requiring the distribution of the fleet across all N=150N=150 zones with strictly limited per-zone capacity. For completeness, we also provide results for projection onto the standard simplex. While this baseline serves as a reference point consistent with TableÂ [2](https://arxiv.org/html/2602.03461v1#S5.T2 "Table 2 â€£ 5.3 Ride-Sharing Dispatch â€£ 5 Numerical Experiments â€£ Soft-Radial Projection for Constrained End-to-End Learning"), we observe a clear deterioration in performance for the constrained models under this stricter regime, characterized by a lower volume of served customers (cf. TableÂ [8](https://arxiv.org/html/2602.03461v1#A3.T8 "Table 8 â€£ C.5 Extended Analysis: Ride-Sharing Dispatch â€£ Appendix C Extended Numerical Results â€£ Soft-Radial Projection for Constrained End-to-End Learning")). Consistent with the less constrained objective, most methods perform equivalently, with the notable exception of the orthogonal projection. These findings suggest that across the set of projection methods, performance is heavily influenced by data noise and model complexity.

Table 8: Ride sharing dispatch results aggregated over five random seeds (mean Â±\pm std).

|  |  |  |
| --- | --- | --- |
| Feasible Set | Method | Served Rate |
| Î”\Delta | Softmax | 0.83â€‹(Â±0.00)0.83\,(\pm 0.00) |
| Îº=0.02\kappa=0.02 | O-Proj | 0.64â€‹(Â±0.00)0.64\,(\pm 0.00) |
| DC3 | 0.69â€‹(Â±0.00)0.69\,(\pm 0.00) |
| HardNet | 0.69â€‹(Â±0.00)0.69\,(\pm 0.00) |
| Soft-Radial | 0.69â€‹(Â±0.00)0.69\,(\pm 0.00) |