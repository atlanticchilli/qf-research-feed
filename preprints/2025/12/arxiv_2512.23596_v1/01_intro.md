---
authors:
- Agostino Capponi
- Chengpiao Huang
- J. Antonio Sidaoui
- Kaizheng Wang
- Jiacheng Zou
doc_id: arxiv:2512.23596v1
family_id: arxiv:2512.23596
is_current: true
taxonomy:
  alpha_families: []
  asset_classes: []
  horizons: []
  themes: []
title: The nonstationarity-complexity tradeoff in return prediction
url_abs: http://arxiv.org/abs/2512.23596v1
url_html: https://arxiv.org/html/2512.23596v1
venue: arXiv q-fin
version: 1
year: 2025
---


Agostino Capponi111Department of IEOR and Columbia Business School, Columbia University. Email: [ac3827@columbia.edu](mailto:ac3827@columbia.edu).
â€ƒâ€ƒ
Chengpiao Huang222Department of IEOR, Columbia University. Email: [chengpiao.huang@columbia.edu](mailto:chengpiao.huang@columbia.edu).
â€ƒâ€ƒ
J.Â Antonio Sidaoui333Department of IEOR, Columbia University. Email: [j.sidaoui@columbia.edu](mailto:j.sidaoui@columbia.edu).
â€ƒâ€ƒ
Kaizheng Wang444Department of IEOR and Data Science Institute, Columbia University. Email: [kaizheng.wang@columbia.edu](mailto:kaizheng.wang@columbia.edu).
â€ƒâ€ƒ
Jiacheng Zou555Email: [jiachengzou@alumni.stanford.edu](mailto:jiachengzou@alumni.stanford.edu).

(This version: December 29, 2025)

###### Abstract

We investigate machine learning models for stock return prediction in non-stationary environments, revealing a fundamental nonstationarity-complexity tradeoff: complex models reduce misspecification error but require longer training windows that introduce stronger non-stationarity. We resolve this tension with a novel model selection method that jointly optimizes model class and training window size using a tournament procedure that adaptively evaluates candidates on non-stationary validation data. Our theoretical analysis demonstrates that this approach balances misspecification error, estimation variance, and non-stationarity, performing close to the best model in hindsight.

Applying our method to 17 industry portfolio returns, we consistently outperform standard rolling-window benchmarks, improving out-of-sample R2R^{2} by 14â€“23% on average. During NBER-designated recessions, improvements are substantial: our method achieves positive R2R^{2} during the Gulf War recession while benchmarks are negative, and improves R2R^{2} in absolute terms by at least 80bps during the 2001 recession as well as superior performance during the 2008 Financial Crisis. Economically, a trading strategy based on our selected model generates 31% higher cumulative returns averaged across the industries.

Keywords: Non-stationarity, Model complexity, Return prediction, Model selection, Adaptive window selection

## 1 Introduction

Machine learning (ML) models have emerged as powerful tools for return prediction in financial markets. Recent studies demonstrate that ML models can effectively approximate stochastic discount factor (SDF) by capturing complex nonlinear relationships between risk information carried by characteristics and asset returns (GKX20; FNW20; KXi23; KMZ24). While these studies have shown impressive predictive performance, they implicitly admit a degree of freedom vis-a-vis how historical data is utilized for estimation. The literature typically adopts one of two regimes: either an expanding window that uses all historical data, or a fixed-length rolling window which uses only the most recent observations in a fixed look-back horizon. Both conventions follow the same operating logic: to firstly consider the choice of model class, such as linear regression, random forest, or any other ML model; secondly and conditioned on the model choice, pick a training window. Since financial markets are subject to shocks and exhibit cycles, this separation of model class and training window is ad-hoc rather than built upon rigorous statistical design.

We show that in a non-stationary environment, the complexity of an approximation model of SDF and the estimation window length are deeply intertwined and cannot be optimized independently. They are linked by a fundamental *nonstationarity-complexity tradeoff*. While complex models are effective at reducing model misspecification error, they simultaneously require a larger volume of training data to mitigate their inherent estimation variance. Extending the
training window to meet this data requirement increases the risk of incorporating outdated economic regimes, thereby introducing non-stationarity bias that can degrade a SDF estimatorâ€™s predictive performance. This tension creates a â€œless can be moreâ€ dilemma, where a complex model trained on a long window of past data may be outperformed by a simpler model trained on a shorter, more recent window of data.
Consequently, the optimal model complexity depends on the training window size, and vice versa.

A rapidly growing branch of literature has recently established the â€œvirtue of complexityâ€ in return prediction, demonstrating that complex, high-dimensional models can significantly outperform simpler, parsimonious benchmarks (KellyMalamud2025Understanding; Kelly2022Virtue; KMZ24). Drawing on the universal approximation property of neural networks, these studies prove that models where the number of parameters exceeds the number of observations can better leverage the information content of predictive signals by accurately approximating unknown nonlinear functions that govern asset returns. Our work complements this literature by introducing a new dimension to this framework: the role of non-stationarity in the training environment for return prediction.

The â€œvirtue of complexityâ€ literature demonstrates that complex ML models can effectively capture nonlinear SDF relationships. These studies typically employ expanding windows with all available historical data. They show that approximation gains from model flexibility outweigh the statistical costs of heavy parameterization in the classical bias-variance tradeoff. Our work complements this literature by examining how non-stationarity in financial markets, arising from structural breaks and economic cycles, affects the optimal choice of model complexity and training window. When the data generating process shifts over time, realizing the virtue of complexity requires carefully selecting how much historical data to include in training.

Our findings suggest that model complexity and training window size cannot be optimized independently as fixed hyperparameters; rather, they must be chosen jointly to balance misspecification error, statistical uncertainty, and environmental drift. This is the central problem studied in our paper:

*How to jointly choose the model complexity and training window size?*

We complement the insights of the machine learning asset pricing literature, including the surprising dominance of large factor models (DKK24) and the success of deep learning and complexity in return prediction (Kelly2022Virtue and their extensions), by considering the case of unknown temporal distribution shifts. By proposing a data-driven framework that adaptively selects the optimal model class and training window size simultaneously, we offer a method for navigating the complex relationships between predictors and returns as they evolve over time. Our method adaptively selects validation data tailored to the local non-stationarity, allowing for a near-optimal estimation of a modelâ€™s future performance. Our framework is general: it can compare any candidate models from different model classes trained on different horizons in any manner.

Our main contributions are three-fold. First, we provide empirical and theoretical investigations of a fundamental nonstationarity-complexity tradeoff in return prediction under non-stationarity. In an empirical study on industry portfolio return prediction, we show that models with greater expressive power or longer training windows may underpreform when the environment changes over time. We then formalize this phenomenon through a finite-sample bound that characterizes the prediction error of a model ff in terms of its model class â„±\mathcal{F} and training window size kk:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Prediction Errorâ€‹(f)â‰²Misspecificationâ€‹(â„±)+Uncertaintyâ€‹(â„±,nk)+Non-stationarityâ€‹(k).\textrm{Prediction Error}(f)~\lesssim~\textrm{Misspecification}(\mathcal{F})+\textrm{Uncertainty}(\mathcal{F},n\_{k})+\textrm{Non-stationarity}(k). |  | (1.1) |

The bound decomposes the prediction error into three sources: the model misspecification error of the model class â„±\mathcal{F}, the statistical uncertainty associated with learning the model using nkn\_{k} samples in the training window, and the non-stationarity within the last kk periods. This characterization quantifies how model complexity and training window length jointly influence the modelâ€™s predictive performance.

Second, motivated by this tradeoff, we develop an adaptive model selection approach for jointly choosing the model class and training window length. Our method is a sequential elimination tournament procedure, and uses a pairwise model comparison subroutine that adaptively selects non-stationary validation data to compare two given models. We prove that our algorithm jointly chooses a model class and training window that near-optimally balance the nonstationarity-complexity tradeoff ([1.1](https://arxiv.org/html/2512.23596v1#S1.E1 "Equation 1.1 â€£ 1 Introduction â€£ The nonstationarity-complexity tradeoff in return prediction")), up to logarithmic factors. Furthermore, we develop a variant tailored to the out-of-sample R2R^{2} metric commonly used in asset pricing.

Third, we demonstrate the empirical efficacy of this framework on daily returns of 17 industry portfolios, and show that it adapts to the local non-stationarity and significantly improves the out-of-sample (OOS) R2R^{2} compared to non-adaptive fixed-window baselines. Over the 1990â€“2016 OOS period, our method delivers an average R2R^{2} of 0.049 across all industries, representing a 14% improvement over fixed-horizon training with long-horizon validation, and more than doubling the performance of short-horizon validation.

Our methodâ€™s advantages are most pronounced during recessions, when non-stationarity is most evident. We examine the three recessions identified by National Bureau of Economic Research (NBER) in its [NBER Business Cycle Dating](https://www.nber.org/research/business-cycle-dating) that fall within our OOS period. During the 1990 Gulf War recession, our framework achieves a positive R2R^{2} of 0.027 while all benchmarks produce negative R2R^{2}, demonstrating the critical importance of handling non-stationarity properly. In the 2001 recession, our method attains an R2R^{2} of 0.125, outperforming the cross-validation benchmark which attains 0.071, a 540 basis point improvement, and the long-window validation benchmark which achieves 0.117, an 80 basis point improvement. During the 2008 Financial Crisis, our method again delivers the strongest performance. These gains are robust across all benchmark methods (Table [2](https://arxiv.org/html/2512.23596v1#S5.T2 "Table 2 â€£ Recession Performance Analysis. â€£ 5.3 Empirical Results: 17 Industry Portfolios â€£ 5 Explaining the Cross-Section of Industry Portfolio Returns â€£ The nonstationarity-complexity tradeoff in return prediction")) and persistent across industries (Figure [5](https://arxiv.org/html/2512.23596v1#S5.F5 "Figure 5 â€£ Recession Performance Analysis. â€£ 5.3 Empirical Results: 17 Industry Portfolios â€£ 5 Explaining the Cross-Section of Industry Portfolio Returns â€£ The nonstationarity-complexity tradeoff in return prediction")), confirming that our adaptive approach effectively navigates the nonstationarity-complexity tradeoff.

Economically, our predictive gains translate to meaningful value: a simple trading strategy based on our selected models generates 31% higher returns than the best-performing validation benchmark, averaged across the 17 industries. This confirms that jointly optimizing model complexity and training window size to address non-stationarity yields substantial benefits for investors.

### 1.1 Related Literature

The integration of machine learning into asset pricing was initially driven by the â€œmultidimensional challengeâ€, that is, the need to identify which of the hundreds of proposed firm characteristics provide independent information for expected returns.

Early influential work by GKX20 demonstrated that nonlinear interactions missed by traditional regressions are a primary source of predictive gains, identifying trees and neural networks as superior methods. FNW20 used adaptive group LASSO to show that only a small subset of characteristics provides incremental information when nonlinearities are properly accounted for. ChoiJiangZhang2025 applied machine learning to 32 international markets, concluding that market-specific neural networks achieve stronger results than global models by capturing local return-characteristic relationships. We refer to KXi23 for an excellent survey on financial machine learning.

Building on these empirical successes, a series of theoretical papers have formalized the virtue of complexity, proving that out-of-sample forecast accuracy and portfolio Sharpe ratios can be strictly increasing in model complexity. This phenomenon occurs because high complexity induces â€œimplicit shrinkageâ€, which reduces prediction variance without the heavy bias costs associated with explicit shrinkage. This line of research advocates for the largest approximating model one can compute, because the gains from better approximation of the unknown truth dominate the statistical costs of heavy parameterization. Foundational works in this stream of literature include Kelly2022Virtue; KellyMalamud2025Understanding; KMZ24. They focus on time-series return prediction and market timing, resolving the â€œdouble limitâ€ problem of growing parameters and observations to show that complexity captures unknown nonlinearities that improve Sharpe ratios. A recent study by DKK24 extends these insights to the cross-section of returns, tackling a â€œthree infinitiesâ€ problem involving a simultaneously large number of assets, parameters, and observations. Their work proposes using random Fourier features to generate vast numbers of nonlinear factors, shifting the statistical objective from pure return prediction to minimizing pricing errors and constructing a high-complexity stochastic discount factor that reflects the true drivers of investorsâ€™ marginal rates of substitution.

The above surveyed works provide compelling empirical evidence and theoretical justifications for the superiority of complex machine learning models over simple linear models. However, they typically treat the training window as a fixed hyperparameter, often setting it to an expanding window that includes all available historical data. Furthermore, their theoretical analysis typically assumes that the training data is i.i.d. Our work points out that, as the financial market is in constant motion due to structural breaks, shifting risk regimes, and economic cycles, accounting for non-stationarity beyond complexity may lead to even further improvements.

A rich literature has developed statistical frameworks for detecting structural breaks and change points (BUr05). Foundational works such as Cho60; And93; BPe98; Chi98 established rigorous methods to identify structural changes, which have been applied to financial time series including realized volatility (LMa07) and speculative bubbles (HBr12). While these studies focus on identifying when breaks occur, our work addresses how to optimally use non-stationary data for estimation. Rather than pinpointing change points, we determine the optimal training window to minimize prediction error in
the presence of non-stationarity.

A complementary literature examines optimal training window selection under non-stationarity. PTi07 showed that under structural breaks, optimal window selection should balance the bias from including pre-break data against the variance from using only post-break data. Subsequent work explored various selection criteria, including minimizing estimation loss functions (PTi07; IJR17) and aggregating predictions across multiple windows (PPi11). However, these approaches typically assume linear models and specific non-stationary structures, such as single breaks or random walks. Our contribution differs in two key ways. First, we take a model-free approach that does not impose parametric assumptions on either the prediction function or the non-stationary dynamics, allowing us to handle more general patterns. Second, we extend the bias-variance tradeoff identified by PTi07 to the machine learning context by jointly optimizing model complexity and training window size, whereas prior work typically selects windows for a pre-specified model. This joint selection accounts for the fact that more complex models introduce additional misspecification-variance tradeoffs that interact with the choice of training window.

The rest of the paper is organized as follows. [SectionËœ2](https://arxiv.org/html/2512.23596v1#S2 "2 Problem Setup â€£ The nonstationarity-complexity tradeoff in return prediction") describes the problem setup. [SectionËœ3](https://arxiv.org/html/2512.23596v1#S3 "3 The Nonstationarity-Complexity Tradeoff â€£ The nonstationarity-complexity tradeoff in return prediction") investigates the nonstationarity-complexity tradeoff. [SectionËœ4](https://arxiv.org/html/2512.23596v1#S4 "4 Adaptive Model and Data Selection under Non-Stationarity â€£ The nonstationarity-complexity tradeoff in return prediction") presents the adaptive model selection algorithm. [SectionËœ5](https://arxiv.org/html/2512.23596v1#S5 "5 Explaining the Cross-Section of Industry Portfolio Returns â€£ The nonstationarity-complexity tradeoff in return prediction") illustrates our algorithm on real datasets. [SectionËœ6](https://arxiv.org/html/2512.23596v1#S6 "6 Conclusions â€£ The nonstationarity-complexity tradeoff in return prediction") concludes the paper and discusses future directions. Mathematical proofs are deferred to the supplemental materials.

##### Notation and Terminology.

We introduce the mathematical notation used throughout the paper. Let â„¤+={1,2,â€¦}\mathbb{Z}\_{+}=\{1,2,...\} be the set of positive integers. For nâˆˆâ„¤+n\in\mathbb{Z}\_{+}, define [n]={1,2,â€¦,n}[n]=\{1,2,...,n\}. For a,bâˆˆâ„a,b\in\mathbb{R}, define aâˆ§b=minâ¡{a,b}a\wedge b=\min\{a,b\} and aâˆ¨b=maxâ¡{a,b}a\vee b=\max\{a,b\}. For xâˆˆâ„x\in\mathbb{R}, let x+=xâˆ¨0x\_{+}=x\vee 0. The sign of a real number xâˆˆâ„x\in\mathbb{R} is defined by signâ€‹(x)=1\textrm{sign}(x)=1 if x>0x>0, signâ€‹(x)=0\textrm{sign}(x)=0 if x=0x=0, and signâ€‹(x)=âˆ’1\textrm{sign}(x)=-1 if x<0x<0. For non-negative sequences {an}n=1âˆ\{a\_{n}\}\_{n=1}^{\infty} and {bn}n=1âˆ\{b\_{n}\}\_{n=1}^{\infty}, we write an=Oâ€‹(bn)a\_{n}=O(b\_{n}) if there exists C>0C>0 such that for all nâˆˆâ„¤+n\in\mathbb{Z}\_{+}, anâ‰¤Câ€‹bna\_{n}\leq Cb\_{n}. We write an=Î˜â€‹(bn)a\_{n}=\Theta(b\_{n}) if an=Oâ€‹(bn)a\_{n}=O(b\_{n}) and bn=Oâ€‹(an)b\_{n}=O(a\_{n}). Unless otherwise stated, anâ‰²bna\_{n}\lesssim b\_{n} also represents an=Oâ€‹(bn)a\_{n}=O(b\_{n}). For a finite set SS, we use |S||S| to denote its cardinality.

## 2 Problem Setup

We consider the problem of predicting a response variable yâˆˆâ„y\in\mathbb{R}, such as an asset return, using a vector of covariates ğ’™\bm{x} in a space ğ’³âŠ†â„d\mathcal{X}\subseteq\mathbb{R}^{d}. A key feature in our setting is *non-stationarity*: in each time period t=1,â€¦,Tt=1,...,T, the covariates and response (ğ’™,y)(\bm{x},y) follow a time-varying joint distribution PtP\_{t}. At the beginning of period tt, we have access to historical data {ğ’Ÿj}j=1tâˆ’1\{\mathcal{D}\_{j}\}\_{j=1}^{t-1}, where ğ’Ÿj={(ğ’™j,i,yj,i)}i=1Bj\mathcal{D}\_{j}=\{(\bm{x}\_{j,i},y\_{j,i})\}\_{i=1}^{B\_{j}} is a set of i.i.d.Â samples collected from PjP\_{j} at time jj.
Throughout our paper, we will assume that the samples are independent across time.

###### Assumption 2.1 (Independent data).

For each jâˆˆâ„¤+j\in\mathbb{Z}\_{+}, the dataset {(ğ±j,i,yj,i)}i=1Bj\{(\bm{x}\_{j,i},y\_{j,i})\}\_{i=1}^{B\_{j}} consists of i.i.d.Â samples. The datasets {ğ’Ÿj}j=1âˆ\{\mathcal{D}\_{j}\}\_{j=1}^{\infty} are independent.

While financial time series inherently exhibit temporal dependence, Assumption [2.1](https://arxiv.org/html/2512.23596v1#S2.Thmassumption1 "Assumption 2.1 (Independent data). â€£ 2 Problem Setup â€£ The nonstationarity-complexity tradeoff in return prediction") is a standard simplification in the theoretical analysis of machine learning for return prediction and asset pricing (KMZ24; DKK24). Adopting this independence assumption allows us to isolate the effect of non-stationarity, without introducing additional technicalities from temporal dependence.

Our goal is to use the historical data {ğ’Ÿj}j=1tâˆ’1\{\mathcal{D}\_{j}\}\_{j=1}^{t-1} to construct a prediction model ft:ğ’³â†’â„f\_{t}:\mathcal{X}\to\mathbb{R} that performs well on the current, unobserved distribution PtP\_{t}.
The performance of a model f:ğ’³â†’â„f:\mathcal{X}\to\mathbb{R} with respect to the data distribution PtP\_{t} is measured by the mean squared error (MSE):

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ltâ€‹(f)=ğ”¼(ğ’™,y)âˆ¼Ptâ€‹[(fâ€‹(ğ’™)âˆ’y)2].L\_{t}(f)=\mathbb{E}\_{(\bm{x},y)\sim P\_{t}}\left[\left(f(\bm{x})-y\right)^{2}\right]. |  | (2.1) |

In line with the empirical finance literature, we also use the R2R^{2} metric to evaluate the performance of a given modeling procedure or algorithm alg that produces a prediction model ftalgf\_{t}^{\texttt{alg}} at each time tt. The out-of-sample R2R^{2} for the algorithm alg over an evaluation period [t1,t2][t\_{1},t\_{2}] is computed as666We note that in the R2R^{2} metric ([2.2](https://arxiv.org/html/2512.23596v1#S2.E2 "Equation 2.2 â€£ 2 Problem Setup â€£ The nonstationarity-complexity tradeoff in return prediction")), the denominator is the sum of the squared responses yt,i2y\_{t,i}^{2} *without demeaning*. In other words, we are benchmarking against a forecast of zero rather than the historical mean as in the statistical R2R^{2} metric. As noted by GKX20, predicting future excess stock returns with historical averages can be problematic and is not assumption-free, because the historical mean is estimated with significant noise, often performing worse than a forecast of zero.

|  |  |  |  |
| --- | --- | --- | --- |
|  | R[t1,t2]2â€‹(alg)=1âˆ’âˆ‘t=t1t2âˆ‘i=1Bt(ftalgâ€‹(xt,i)âˆ’yt,i)2âˆ‘t=t1t2âˆ‘i=1Btyt,i2.R\_{[t\_{1},t\_{2}]}^{2}(\texttt{alg})=1-\frac{\sum\_{t=t\_{1}}^{t\_{2}}\sum\_{i=1}^{B\_{t}}\left(f\_{t}^{\texttt{alg}}(x\_{t,i})-y\_{t,i}\right)^{2}}{\sum\_{t=t\_{1}}^{t\_{2}}\sum\_{i=1}^{B\_{t}}y\_{t,i}^{2}}. |  | (2.2) |

For completeness, in the appendices we also present results using the statistical R2R^{2} metric

|  |  |  |  |
| --- | --- | --- | --- |
|  | R[t1,t2],s2â€‹(alg)=1âˆ’âˆ‘t=t1t2âˆ‘i=1Bt(ftalgâ€‹(xt,i)âˆ’yt,i)2âˆ‘t=t1t2âˆ‘i=1Bt(yt,iâˆ’yÂ¯)2,R\_{[t\_{1},t\_{2}],\texttt{s}}^{2}(\texttt{alg})=1-\frac{\sum\_{t=t\_{1}}^{t\_{2}}\sum\_{i=1}^{B\_{t}}\left(f\_{t}^{\texttt{alg}}(x\_{t,i})-y\_{t,i}\right)^{2}}{\sum\_{t=t\_{1}}^{t\_{2}}\sum\_{i=1}^{B\_{t}}(y\_{t,i}-\bar{y})^{2}}, |  | (2.3) |

where yÂ¯\bar{y} is the mean of the samples {yt,i:tâˆˆ[t1,t2],iâˆˆ[Bt]}\{y\_{t,i}:t\in[t\_{1},t\_{2}],\,i\in[B\_{t}]\}.

In a stationary environment where Pt=PP\_{t}=P for all tâˆˆ[T]t\in[T], the standard approach for learning a model ff consists in choosing a model class â„±\mathcal{F} (e.g., linear model, random forest) and then finding a model f^âˆˆâ„±\widehat{f}\in\mathcal{F} by minimizing the empirical loss over the training data. The choice of the model class â„±\mathcal{F} involves a classic bias-variance trade-off. A simple class may exhibit high bias due to model misspecification, while a complex class may suffer from high estimation variance.

When the environment is non-stationary (that is, Piâ‰ PjP\_{i}\neq P\_{j} for iâ‰ ji\neq j), the problem becomes significantly more complicated. One must now make two critical choices simultaneously: the model class â„±\mathcal{F} and the amount of historical data used for training. Data from the distant past may no longer be representative of the current environment, and can be misleading for model training. This creates the core tension of our paper: complex models require more data to reduce estimation variance, but using more data may introduce stronger non-stationarity that increases bias. Thus, it is possible for simple models with less training data to outperform complex models trained on more data. Our goal is to develop an approach to jointly choose the model class and training window size.

## 3 The Nonstationarity-Complexity Tradeoff

### 3.1 Empirical Evidence

We begin with an empirical illustration that highlights the challenges of jointly choosing a model class and a training window under non-stationarity. The task is to forecast the excess returns of 1717 industry portfolios from Kenneth Frenchâ€™s data library using a set of covariates, with training data starting from September 1987 and ending in October 2016.777More details about the dataset are provided in LABEL:sm:sec-experiments. We highlight that our data spans several recessions documented in [NBER Business Cycle Dating](https://www.nber.org/research/business-cycle-dating): the 1990 Gulf War recession, the 2001 dot-com bubble bust and the 9/11 attack, and the 2007-2009 Financial Crisis. To show that model performance is fundamentally linked to non-stationarity, we document a simple ranking of linear and nonlinear models in each period, across the different industries.

In each month tt, for each industry, we fit three prediction models: (1) a linear model trained by ridge regression using the most recent 6464 months of data, (2) a random forest trained on the most recent 6464 months of data, and (3) a random forest trained on all historical data. More details of the experiments are given in LABEL:sm:sec-tradeoff-empirics-details.

We compute each modelâ€™s annual out-of-sample R2R^{2} for every industry. To visualize the modelsâ€™ relative performance across industries, we count the number of industries in which each model achieves the highest out-of-sample R2R^{2} for a given year. LABEL:sm:fig-tradeoff-dominance summarizes the result. To provide a more granular understanding of the modelsâ€™ performance, LABEL:sm:fig-tradeoff-industry further plots the annual out-of-sample R2R^{2} of the models for the 1717 industries.

Figure 1: Number of Industries where Each Model Attains the Highest Annual Out-of-Sample R2R^{2}.

![Refer to caption](x1.png)


This figure reports the relative performance of three models in predicting the excess returns of the 1717 industry portfolios. The three models are: (1) a linear model trained by ridge regression on the most recent 6464 months of data (orange), (2) a random forest trained on the most recent 6464 months of data (blue), and (3) a random forest trained on all historical data (white). For each year from 1990 to 2016, we compute the annual out-of-sample R2R^{2} of the models for each of the 1717 industry portfolios, and then count, for each model, the number of industries in which it outperforms the other two models in terms of the annual out-of-sample R2R^{2}.




Figure 2: Annual Out-of-Sample R2R^{2} of Three Models for 1717 Industry Portfolios.

![Refer to caption](x2.png)

![Refer to caption](x3.png)

![Refer to caption](x4.png)

![Refer to caption](x5.png)

![Refer to caption](x6.png)

![Refer to caption](x7.png)

![Refer to caption](x8.png)

![Refer to caption](x9.png)

![Refer to caption](x10.png)

![Refer to caption](x11.png)

![Refer to caption](x12.png)

![Refer to caption](x13.png)

![Refer to caption](x14.png)

![Refer to caption](x15.png)

![Refer to caption](x16.png)

![Refer to caption](x17.png)

![Refer to caption](x18.png)

This figure reports, for each of the 1717 industry portfolios, the annual out-of-sample R2R^{2} from 1990 to 2016 for three models: (1) a linear model trained by ridge regression on the most recent 6464 months of data (red), (2) a random forest trained using the most recent 6464 months of data (gray), and (3) a random forest trained using all available historical data up to that year (black). In periods of strong non-stationarity, such as 1990-1991, 2001-2002 and 2008-2009, the linear model trained on a small window constantly outperforms the more complex random forest trained on a large window. The labels in each figure is the Kenneth French acronym for the industries. For full names of these industries, please refer to Table [4](https://arxiv.org/html/2512.23596v1#A1.T4 "Table 4 â€£ A.2 Figures and tables â€£ Appendix A Additional details for empirical asset pricing â€£ The nonstationarity-complexity tradeoff in return prediction").

We make two key observations. First, within the same model class, using less training data may lead to better performance. For example, in several years including 19941994, 19961996 and 20032003, the random forest trained on the most recent 6464 months of data outperforms the random forest trained on all historical data for at least half of the industries.

Second, and more strikingly, a simple model trained on a short window can outperform a complex model trained on a long window. In particular, during the three NBER-designated recessions, the simple linear model trained on 6464 months of data outperforms the more complex random forest model trained on all historical data for over half of the industries. This consistent pattern shows that the advantage of a more expressive model class can be completely negated by the non-stationarity in the training data. In LABEL:sm:sec-select, we propose data-driven approaches to select the best-performing model during such unusual economic regimes.

These empirical findings highlight that in a non-stationary environment, the model complexity and training data size are intricately linked with each other. We call this phenomenon the *nonstationarity-complexity tradeoff*. Crucially, the optimal choice of model class and training window size is not fixed; instead, it generally varies with the degree of the non-stationarity.

### 3.2 Theoretical Characterization

We provide theoretical support for the nonstationarity-complexity tradeoff, by deriving a finite-sample bound on a modelâ€™s prediction error under non-stationarity. The bound decomposes the prediction error into three key components: model misspecification error, statistical uncertainty, and non-stationarity, and shows how they interact with the choice of model class and training window length.

Consider a model f^\widehat{f} trained from a model class â„±\mathcal{F} by minimizing the empirical loss over training data from the last kk periods, denoted by {ğ’Ÿjtr}j=tâˆ’ktâˆ’1\big\{\mathcal{D}^{\operatorname{\mathrm{tr}}}\_{j}\big\}\_{j=t-k}^{t-1}, where ğ’Ÿjtr={(ğ’™j,itr,yj,itr)}i=1mj\mathcal{D}^{\operatorname{\mathrm{tr}}}\_{j}=\big\{\big(\bm{x}^{\operatorname{\mathrm{tr}}}\_{j,i},y^{\operatorname{\mathrm{tr}}}\_{j,i}\big)\big\}\_{i=1}^{m\_{j}} is the training data in period jj. That is,

|  |  |  |  |
| --- | --- | --- | --- |
|  | f^=argminfâˆˆâ„±1mt,kâ€‹âˆ‘j=tâˆ’ktâˆ’1âˆ‘i=1mj[fâ€‹(ğ’™j,itr)âˆ’yj,itr]2,\widehat{f}=\mathop{\mathrm{argmin}}\_{f\in\mathcal{F}}\frac{1}{m\_{t,k}}\sum\_{j=t-k}^{t-1}\sum\_{i=1}^{m\_{j}}\left[f(\bm{x}^{\operatorname{\mathrm{tr}}}\_{j,i})-y^{\operatorname{\mathrm{tr}}}\_{j,i}\right]^{2}, |  | (3.1) |

where mt,k=âˆ‘j=tâˆ’ktâˆ’1mjm\_{t,k}=\sum\_{j=t-k}^{t-1}m\_{j} is the number of training data points in {ğ’Ÿjtr}j=tâˆ’ktâˆ’1\big\{\mathcal{D}^{\operatorname{\mathrm{tr}}}\_{j}\big\}\_{j=t-k}^{t-1}.

Define the Bayes optimal least squares estimator ftâˆ—â€‹(â‹…)=ğ”¼(ğ’™,y)âˆ¼Ptâ€‹[yâˆ£ğ’™=â‹…]f\_{t}^{\*}(\cdot)=\mathbb{E}\_{(\bm{x},y)\sim P\_{t}}[y\mid\bm{x}=\cdot], which minimizes the MSE Ltâ€‹(f)L\_{t}(f) over all possible prediction models f:ğ’³â†’â„f:\mathcal{X}\to\mathbb{R}. Our bound will be stated in terms of the *excess risk*

|  |  |  |
| --- | --- | --- |
|  | â„°tâ€‹(f)=Ltâ€‹(f)âˆ’Ltâ€‹(ftâˆ—),\mathcal{E}\_{t}(f)=L\_{t}(f)-L\_{t}(f\_{t}^{\*}), |  |

which compares the prediction error of a model ff against that of ftâˆ—f\_{t}^{\*}. To facilitate analysis, we make the following boundedness assumption.

###### Assumption 3.1 (Boundedness).

There exists a constant M>0M>0 such that for all models ff in the class â„±\mathcal{F}, (ğ±,y)âˆ¼Pj(\bm{x},y)\sim P\_{j} and jâˆˆâ„¤+j\in\mathbb{Z}\_{+}, we have |fâ€‹(ğ±)|â‰¤M|f(\bm{x})|\leq M, and |y|â‰¤M|y|\leq M. Without loss of generality we assume Mâ‰¥1M\geq 1.

To quantify the effective complexity of the model class â„±\mathcal{F} relative to the training window size kk, we employ a measure rt,kâ€‹(â„±)r\_{t,k}(\mathcal{F}) derived from the theory of *local Rademacher complexity* (BBM05).
Given the technical nature of this measure, we defer its formal definition to LABEL:sm:sec-tradeoff-theory-appendix.
The local Rademacher complexity measures the ability of the near-optimal models in â„±\mathcal{F} to fit random noise using data within the training window kk. A higher complexity indicates a richer model class that is capable of approximating complex patterns, but also signals a higher estimation variance and thus a higher risk of overfitting.
As an illustration, we now present the complexity measure rt,kâ€‹(â„±)r\_{t,k}(\mathcal{F}) for several common model classes. The results are proved in LABEL:sm:sec-proof-eg-classes.

###### Example 3.1 (Finite class).

If |â„±|<âˆ|\mathcal{F}|<\infty, then rt,kâ€‹(â„±)â‰¤(4â€‹Mâ€‹logâ¡|â„±|)/mt,kr\_{t,k}(\mathcal{F})\leq(4M\log|\mathcal{F}|)/m\_{t,k}.

###### Example 3.2 (Linear class).

Recall ğ’³âŠ†â„d\mathcal{X}\subseteq\mathbb{R}^{d}. For every ğ›‰âˆˆâ„d\bm{\theta}\in\mathbb{R}^{d}, define fğ›‰:ğ’³â†’â„f\_{\bm{\theta}}:\mathcal{X}\to\mathbb{R} by fğ›‰â€‹(ğ±)=âŸ¨ğ›‰,ğ±âŸ©f\_{\bm{\theta}}(\bm{x})=\langle\bm{\theta},\bm{x}\rangle. Suppose that â„±âŠ†{fğ›‰:ğ›‰âˆˆâ„d}\mathcal{F}\subseteq\{f\_{\bm{\theta}}:\bm{\theta}\in\mathbb{R}^{d}\}. Then,
rt,kâ€‹(â„±)â‰¤câ€‹d/mt,kr\_{t,k}(\mathcal{F})\leq cd/m\_{t,k} holds with some constant cc.

###### Example 3.3 (Kernel class).

Let â„\mathbb{H} be a reproducing kernel Hilbert space (Wah90) with inner product âŸ¨â‹…,â‹…âŸ©\langle\cdot,\cdot\rangle and norm âˆ¥â‹…âˆ¥â„\|\cdot\|\_{\mathbb{H}}, and Ï•:ğ’³â†’â„\phi:\mathcal{X}\to\mathbb{H} be a feature mapping. For any ğ›‰âˆˆâ„\bm{\theta}\in\mathbb{H}, define fğ›‰:ğ’³â†’â„f\_{\bm{\theta}}:\mathcal{X}\to\mathbb{R} by fğ›‰â€‹(ğ±)=âŸ¨ğ›‰,Ï•â€‹(ğ±)âŸ©f\_{\bm{\theta}}(\bm{x})=\langle\bm{\theta},\phi(\bm{x})\rangle. Consider the function class â„±={fğ›‰:â€–ğ›‰â€–â„â‰¤R}\mathcal{F}=\{f\_{\bm{\theta}}:\|\bm{\theta}\|\_{\mathbb{H}}\leq R\} for some constant R>0R>0. Model fitting in this class can be efficiently implemented through kernel ridge regression, which is a finite-dimensional convex program even if â„\mathbb{H} and â„±\mathcal{F} are infinite-dimensional.

Suppose there exists a trace-class operator ğ’:â„â†’â„\bm{S}:\mathbb{H}\to\mathbb{H} such that for any jâˆˆâ„¤+j\in\mathbb{Z}\_{+} and ğ¯âˆˆâ„\bm{v}\in\mathbb{H}, we have ğ”¼(ğ±,y)âˆ¼Pjâ€‹|âŸ¨Ï•â€‹(ğ±),ğ¯âŸ©|2â‰¤âŸ¨ğ¯,ğ’â€‹ğ¯âŸ©\mathbb{E}\_{(\bm{x},y)\sim P\_{j}}|\langle\phi(\bm{x}),\bm{v}\rangle|^{2}\leq\langle\bm{v},\bm{S}\bm{v}\rangle. Let {Î¼k}k=1âˆ\{\mu\_{k}\}\_{k=1}^{\infty} be the eigenvalues of ğ’\bm{S} sorted in descending order. We have the following results:

* â€¢

  (Exponential decay) If there are constants c1,c2>0c\_{1},c\_{2}>0 such that Î¼kâ‰¤c1â€‹eâˆ’c2â€‹k\mu\_{k}\leq c\_{1}e^{-c\_{2}k} holds for all kk, then rt,kâ€‹(â„±)â‰¤(Câ€‹logâ¡mt,k)/mt,kr\_{t,k}(\mathcal{F})\leq(C\log m\_{t,k})/m\_{t,k} holds with some constant CC.
* â€¢

  (Polynomial decay) If there are constants c>0c>0 and Î±â‰¥1\alpha\geq 1 such that Î¼kâ‰¤câ€‹kâˆ’2â€‹Î±\mu\_{k}\leq ck^{-2\alpha} holds for all kk, then rt,kâ€‹(â„±)â‰¤Câ€‹mt,kâˆ’2â€‹Î±2â€‹Î±+1r\_{t,k}(\mathcal{F})\leq Cm\_{t,k}^{-\frac{2\alpha}{2\alpha+1}} holds with some constant CC.

Examples of the above two cases include function spaces induced by the Gaussian kernel and Sobolev spaces, respectively (Wai19).

In the classical setting where the training data {ğ’Ÿj}j=tâˆ’ktâˆ’1\{\mathcal{D}\_{j}\}\_{j=t-k}^{t-1} is i.i.d., the complexity measure rt,kâ€‹(â„±)r\_{t,k}(\mathcal{F}) is a key component in bounding the excess risk of f^\widehat{f}: with high probability,

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„°tâ€‹(f^)â‰²minfâˆˆâ„±â¡â„°tâ€‹(f)+(rt,kâ€‹(â„±)+1mt,k).\mathcal{E}\_{t}(\widehat{f})\lesssim\min\_{f\in\mathcal{F}}\mathcal{E}\_{t}(f)+\left(r\_{t,k}(\mathcal{F})+\frac{1}{m\_{t,k}}\right). |  | (3.2) |

In particular, the prediction error is decomposed into two terms:

1. 1.

   *Model misspecification error*

   |  |  |  |
   | --- | --- | --- |
   |  | minfâˆˆâ„±â¡â„°tâ€‹(f)=minfâˆˆâ„±â¡Ltâ€‹(f)âˆ’Ltâ€‹(ftâˆ—),\min\_{f\in\mathcal{F}}\mathcal{E}\_{t}(f)=\min\_{f\in\mathcal{F}}L\_{t}(f)-L\_{t}(f\_{t}^{\*}), |  |

   which describes how well â„±\mathcal{F} can approximate the Bayes optimal least squares estimator ftâˆ—f\_{t}^{\*} at time tt. A more complex model class tends to reduce the model misspecification error.
2. 2.

   *Statistical uncertainty*

   |  |  |  |
   | --- | --- | --- |
   |  | rt,kâ€‹(â„±)+1mt,k,r\_{t,k}(\mathcal{F})+\frac{1}{m\_{t,k}}, |  |

   which quantifies the estimation variance of the model f^\widehat{f}. As is discussed above, using a more complex model class increases the statistical uncertainty of the fitted model. Consequently, a more complex model typically requires a longer training window kk to mitigate its estimation variance.

The classical error bound ([3.2](https://arxiv.org/html/2512.23596v1#S3.E2 "Equation 3.2 â€£ 3.2 Theoretical Characterization â€£ 3 The Nonstationarity-Complexity Tradeoff â€£ The nonstationarity-complexity tradeoff in return prediction")) shows that in the i.i.d.Â case, increasing the training window size kk always reduces the statistical uncertainty, thereby lowering the total prediction error. However, we now present our theory to show that under non-stationarity, this logic is incomplete. As we increase the window size kk to reduce estimation variance, we inadvertently include older data distributions that differ from the target, introducing a third error component. We formalize this in the following theorem.

###### Theorem 3.1 (Prediction error bound).

Let Assumptions [2.1](https://arxiv.org/html/2512.23596v1#S2.Thmassumption1 "Assumption 2.1 (Independent data). â€£ 2 Problem Setup â€£ The nonstationarity-complexity tradeoff in return prediction") and [3.1](https://arxiv.org/html/2512.23596v1#S3.Thmassumption1 "Assumption 3.1 (Boundedness). â€£ 3.2 Theoretical Characterization â€£ 3 The Nonstationarity-Complexity Tradeoff â€£ The nonstationarity-complexity tradeoff in return prediction") hold, and fix Î´âˆˆ(0,1)\delta\in(0,1). With probability at least 1âˆ’Î´1-\delta, the model f^\widehat{f} defined by ([3.1](https://arxiv.org/html/2512.23596v1#S3.E1 "Equation 3.1 â€£ 3.2 Theoretical Characterization â€£ 3 The Nonstationarity-Complexity Tradeoff â€£ The nonstationarity-complexity tradeoff in return prediction")) satisfies

|  |  |  |
| --- | --- | --- |
|  | â„°tâ€‹(f^)â‰²minfâˆˆâ„±â¡â„°tâ€‹(f)+M2â€‹(rt,kâ€‹(â„±)+logâ¡(1/Î´)mt,k)+M2â€‹maxtâˆ’kâ‰¤jâ‰¤tâˆ’1â¡TVâ¡(Pj,Pt).\mathcal{E}\_{t}(\widehat{f})\lesssim\min\_{f\in\mathcal{F}}\mathcal{E}\_{t}(f)+M^{2}\left(r\_{t,k}(\mathcal{F})+\frac{\log(1/\delta)}{m\_{t,k}}\right)+M^{2}\max\_{t-k\leq j\leq t-1}\operatorname{TV}\left(P\_{j},P\_{t}\right). |  |

Here â‰²\lesssim hides a universal constant, and TVâ¡(Pj,Pt)=maxAâ¡|Pjâ€‹(A)âˆ’Ptâ€‹(A)|\operatorname{TV}(P\_{j},P\_{t})=\max\_{A}|P\_{j}(A)-P\_{t}(A)| is the total variation distance.

###### Proof of LABEL:sm:thm-tradeoff.

See LABEL:sm:sec-tradeoff-theory-appendix.
âˆ

LABEL:sm:thm-tradeoff reveals that the non-stationarity adds a third dimension to the classical prediction error bound ([3.2](https://arxiv.org/html/2512.23596v1#S3.E2 "Equation 3.2 â€£ 3.2 Theoretical Characterization â€£ 3 The Nonstationarity-Complexity Tradeoff â€£ The nonstationarity-complexity tradeoff in return prediction")), namely, a *non-stationarity* term

|  |  |  |
| --- | --- | --- |
|  | maxtâˆ’kâ‰¤jâ‰¤tâˆ’1â¡TVâ¡(Pj,Pt),\max\_{t-k\leq j\leq t-1}\operatorname{TV}\left(P\_{j},P\_{t}\right), |  |

which quantifies the distribution drift in the environment within the last kk periods. Unlike the statistical uncertainty, this error component increases with the window size kk.
In LABEL:sm:tab-generalization-components, we summarize how the model complexity and the training window size kk impact the three sources of error.

Table 1: Impacts of Model Complexity and Training Window Size on Prediction Error

|  |  |  |  |
| --- | --- | --- | --- |
|  | Misspecification Error | Statistical Uncertainty | Non-Stationarity |
| Model Complexity â†—\nearrow | â†˜\searrow | â†—\nearrow | - |
| Training Window kk â†—\nearrow | - | â†˜\searrow | â†—\nearrow |

The error decomposition in LABEL:sm:thm-tradeoff formalizes the empirical observations in LABEL:sm:sec-tradeoff-empirics: (i) Using a more expressive model class reduces misspecification error but increases the risk of overfitting, and (ii) using a longer training window reduces statistical uncertainty but increases non-stationarity. As a result, neither greater model complexity nor more training data is uniformly beneficial under non-stationarity.
Below we illustrate this phenomenon through a simple example.

###### Example 3.4 (Selection of model class and window under non-stationarity).

Let Î·,Î³âˆˆ[0,1]\eta,\gamma\in[0,1] be two small constants. Suppose that at each time tt, the covariate and response (x,y)âˆ¼Pt(x,y)\sim P\_{t} satisfy xâˆ¼Uniformâ€‹[0,1]x\sim\textrm{Uniform}[0,1], y|xâˆ¼Nâ€‹(ftâˆ—â€‹(x),1)y|x\sim N(f\_{t}^{\*}(x),1), and

|  |  |  |
| --- | --- | --- |
|  | ftâˆ—â€‹(x)=ctâ€‹x+Î³â€‹sinâ¡(2â€‹Ï€â€‹x),f\_{t}^{\*}(x)=c\_{t}x+\gamma\sin(2\pi x), |  |

where {ct}t=1âˆ\{c\_{t}\}\_{t=1}^{\infty} is a deterministic sequence in [0,1][0,1] satisfying |ct+1âˆ’ct|=Î·|c\_{t+1}-c\_{t}|=\eta. We observe a single sample per period. Consider two model classes: linear class and kernel class with a first-order Sobolev kernel (see, e.g., Example 12.16 in Wai19).

* â€¢

  If we train a linear model with a training window kk, then the three components of the prediction error bound in [TheoremËœ3.1](https://arxiv.org/html/2512.23596v1#S3.Thmtheorem1 "Theorem 3.1 (Prediction error bound). â€£ 3.2 Theoretical Characterization â€£ 3 The Nonstationarity-Complexity Tradeoff â€£ The nonstationarity-complexity tradeoff in return prediction") satisfy

  |  |  |  |
  | --- | --- | --- |
  |  | minfâˆˆâ„±â¡â„°tâ€‹(f)â‰Î³2,rt,kâ€‹(â„±)â‰kâˆ’1,maxtâˆ’kâ‰¤jâ‰¤tâˆ’1â¡TVâ¡(Pj,Pt)â‰kâ€‹Î·.\min\_{f\in\mathcal{F}}\mathcal{E}\_{t}(f)\asymp\gamma^{2},\qquad r\_{t,k}(\mathcal{F})\asymp k^{-1},\qquad\max\_{t-k\leq j\leq t-1}\operatorname{TV}\left(P\_{j},P\_{t}\right)\asymp k\eta. |  |

  Optimizing their sum over kk yields the optimal window size kâˆ—â‰Î·âˆ’1/2k^{\*}\asymp\eta^{-1/2}, which leads to an Oâ€‹(Î³2+Î·1/2)O(\gamma^{2}+\eta^{1/2}) bound on the prediction error.
* â€¢

  If we use the kernel class, then ftâˆ—f\_{t}^{\*} is well-specified. For a training window kk, we have

  |  |  |  |
  | --- | --- | --- |
  |  | minfâˆˆâ„±â¡â„°tâ€‹(f)=0,rt,kâ€‹(â„±)â‰kâˆ’2/3,maxtâˆ’kâ‰¤jâ‰¤tâˆ’1â¡TVâ¡(Pj,Pt)â‰kâ€‹Î·.\min\_{f\in\mathcal{F}}\mathcal{E}\_{t}(f)=0,\qquad r\_{t,k}(\mathcal{F})\asymp k^{-2/3},\qquad\max\_{t-k\leq j\leq t-1}\operatorname{TV}\left(P\_{j},P\_{t}\right)\asymp k\eta. |  |

  The optimal training window is kâˆ—â‰Î·âˆ’3/5k^{\*}\asymp\eta^{-3/5}, which results in a prediction error of Oâ€‹(Î·2/5)O(\eta^{2/5}).

We observe that for both classes, the optimal window size depends on the severity of the drift Î·\eta, and is in general not the full window size. If one naÃ¯vely uses the kernel class with a large window size, then the resulting error scales as Oâ€‹(kâ€‹Î·)O(k\eta), which is linear in kk and can be much worse than the above bounds.

As expected, the preferable model class depends on the interplay between misspecification Î³\gamma and drift Î·\eta. The kernel class is more expressive but more sensitive to drift. When Î·=Oâ€‹(Î³5)\eta=O(\gamma^{5}), drift is relatively mild and the kernel is optimal, consistent with the â€œvirtue of complexityâ€ (KellyMalamud2025Understanding; Kelly2022Virtue; KMZ24). However, when Î·â‰«Î³5\eta\gg\gamma^{5}, severe non-stationarity requires shorter training windows under which sample sizes are too limited for the kernel estimator to fully exploit its flexibility advantage. In this high-drift regime, the linear class achieves better performance with its shorter optimal window, explaining the â€œless can be moreâ€ phenomenon observed in our experiments.

## 4 Adaptive Model and Data Selection under Non-Stationarity

LABEL:sm:sec-tradeoff shows that the predictive performance of a model depends jointly on its complexity and the size of the training data, and that the optimal choice often varies over time with the non-stationarity. As the non-stationarity is generally unknown *a priori*, the selection of the model class and training window calls for a data-driven approach.

In this section, we develop a novel method that uses historical validation data to select the best model from a set of candidates. These candidate models can come from different model classes, be trained on different time windows, or use different hyperparameters. The main challenge is that the same non-stationarity that complicates model training also incapacitates standard model selection techniques such as holdout and cross validation.
Specifically, in a non-stationary environment, a model that performs well on a validation set from the distant past may not perform as well in the future. Our solution is to adaptively select the relevant validation data that best reflects the current environment, allowing for a more accurate comparison of the candidate modelsâ€™ future performance.

We now formally set up the framework, illustrated in [FigureËœ3](https://arxiv.org/html/2512.23596v1#S4.F3 "In 4 Adaptive Model and Data Selection under Non-Stationarity â€£ The nonstationarity-complexity tradeoff in return prediction"). In each period tt, we split the available data {ğ’Ÿj}j=1tâˆ’1\{\mathcal{D}\_{j}\}\_{j=1}^{t-1} into a training dataset {ğ’Ÿjtr}j=1tâˆ’1\{\mathcal{D}\_{j}^{\operatorname{\mathrm{tr}}}\}\_{j=1}^{t-1} and a validation dataset {ğ’Ÿjva}j=1tâˆ’1\{\mathcal{D}\_{j}^{\operatorname{\mathrm{va}}}\}\_{j=1}^{t-1}. We use the training data {ğ’Ÿjtr}j=1tâˆ’1\{\mathcal{D}\_{j}^{\operatorname{\mathrm{tr}}}\}\_{j=1}^{t-1} to produce a finite set of candidate models {fÎ»}Î»=1Î›\{f\_{\lambda}\}\_{\lambda=1}^{\Lambda}.
These candidates can come from different model classes, be trained on different data horizons, or use different hyperparameters. We will use the validation data {ğ’Ÿjva}j=1tâˆ’1\{\mathcal{D}\_{j}^{\operatorname{\mathrm{va}}}\}\_{j=1}^{t-1} to select a good model f^=fÎ»^\widehat{f}=f\_{\widehat{\lambda}} that performs best at time tt.

Figure 3: Our Framework for Model Training and Selection under Non-stationarity.

![Refer to caption](x19.png)

### 4.1 Adaptive Tournament Model Selection

In this section, we describe our model selection approach, which uses a sequential elimination tournament. The procedure relies on a pairwise comparison subroutine ğ’œ\mathcal{A} which is designed to compare two given models ff and fâ€²f^{\prime}, and output the better model, denoted by ğ’œâ€‹(f,fâ€²)\mathcal{A}(f,f^{\prime}). In each round, we choose one remaining model ff as a pivot model and compare it against each remaining model fâ€²f^{\prime} using ğ’œ\mathcal{A}. If the pivot ff wins all pairwise comparisons, it is declared the winner; otherwise, the models that defeated ff advance to the next round. The procedure is formally described in LABEL:sm:alg-tournament.

Input: Candidate models {fÎ»}Î»=1Î›\{f\_{\lambda}\}\_{\lambda=1}^{\Lambda}, validation data {ğ’Ÿjva}j=1t\{\mathcal{D}^{\operatorname{\mathrm{va}}}\_{j}\}\_{j=1}^{t}, pairwise model comparison subroutine ğ’œ\mathcal{A}.

Initialize S={fÎ»}Î»=1Î›S=\{f\_{\lambda}\}\_{\lambda=1}^{\Lambda}. // collection of remaining models

while |S|>1|S|>1

Choose a pivot model fâˆˆSf\in S uniformly at random.

Initialize Sâ€²â†âˆ…S^{\prime}\leftarrow\emptyset. // collection of models in SS that outperform ff

for fâ€²âˆˆS\{f}f^{\prime}\in S\backslash\{f\}

Run ğ’œ\mathcal{A} to compare {f,fâ€²}\{f,f^{\prime}\} to obtain ğ’œâ€‹(f,fâ€²)\mathcal{A}(f,f^{\prime}).

If ğ’œâ€‹(f,fâ€²)=fâ€²\mathcal{A}(f,f^{\prime})=f^{\prime}, set Sâ€²â†Sâ€²âˆª{fâ€²}S^{\prime}\leftarrow S^{\prime}\cup\{f^{\prime}\}.

if Sâ€²=âˆ…S^{\prime}=\emptyset

return f^=f\widehat{f}=f. // if no model outperforms ff, output ff

else

Set Sâ†Sâ€²S\leftarrow S^{\prime}.

return the only model f^âˆˆS\widehat{f}\in S.

Algorithm 1  Adaptive Tournament Model Selection (ATOMS)

LABEL:sm:alg-tournament has two attractive properties. First, in terms of computational efficiency, the expected number of pairwise comparisons scales linearly with the number of models Î›\Lambda.

###### Lemma 4.1 (Computational complexity).

LABEL:sm:alg-tournament calls the subroutine ğ’œ\mathcal{A} for Î˜â€‹(Î›)\Theta(\Lambda) times in expectation.

###### Proof of LABEL:sm:lem-complexity-tournament.

See LABEL:sm:sec-lem-complexity-tournament-proof.
âˆ

Second, regarding the statistical accuracy of model selection, we show later that LABEL:sm:alg-tournament preserves the performance guarantee of any pairwise comparison subroutine ğ’œ\mathcal{A}, incurring only a logarithmic factor overhead in the number of models Î›\Lambda.

##### Pairwise comparison subroutine.

We now detail the model comparison subroutine ğ’œ\mathcal{A}. As we mentioned before, directly comparing the models on the non-stationary validation data {ğ’Ÿjva}j=1tâˆ’1\{\mathcal{D}^{\operatorname{\mathrm{va}}}\_{j}\}\_{j=1}^{t-1} may lead to significantly biased estimates of the model performance. To address this problem, we take an approach based on the adaptive rolling window framework developed by HHW24.

The main idea is as follows. To choose between two models, it suffices to determine the sign of their *performance gap*

|  |  |  |
| --- | --- | --- |
|  | Î”t=Ltâ€‹(f1)âˆ’Ltâ€‹(f2).\Delta\_{t}=L\_{t}(f\_{1})-L\_{t}(f\_{2}). |  |

Indeed, f2f\_{2} is better than f1f\_{1} if and only if Î”t>0\Delta\_{t}>0. To estimate Î”t\Delta\_{t} from the non-stationary validation data, a natural idea is to take a *look-back window* â„“âˆˆ[tâˆ’1]\ell\in[t-1], and use the validation data from the last â„“\ell periods, ğ’Ÿjva={(ğ’™j,iva,yj,iva)}i=1nj\mathcal{D}^{\operatorname{\mathrm{va}}}\_{j}=\big\{\big(\bm{x}^{\operatorname{\mathrm{va}}}\_{j,i},y^{\operatorname{\mathrm{va}}}\_{j,i}\big)\big\}\_{i=1}^{n\_{j}}, j=tâˆ’â„“,â€¦,tâˆ’1j=t-\ell,...,t-1, to form a rolling window estimate

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î”^t,â„“=1nt,â„“â€‹âˆ‘j=tâˆ’â„“tâˆ’1âˆ‘i=1njuj,i,whereuj,i=[f1â€‹(ğ’™j,iva)âˆ’yj,iva]2âˆ’[f2â€‹(ğ’™j,iva)âˆ’yj,iva]2,\widehat{\Delta}\_{t,\ell}=\frac{1}{n\_{t,\ell}}\sum\_{j=t-\ell}^{t-1}\sum\_{i=1}^{n\_{j}}u\_{j,i},\quad\text{where}\quad u\_{j,i}=\left[f\_{1}(\bm{x}^{\operatorname{\mathrm{va}}}\_{j,i})-y^{\operatorname{\mathrm{va}}}\_{j,i}\right]^{2}-\left[f\_{2}(\bm{x}^{\operatorname{\mathrm{va}}}\_{j,i})-y^{\operatorname{\mathrm{va}}}\_{j,i}\right]^{2}, |  | (4.1) |

and nt,â„“=âˆ‘j=tâˆ’â„“tâˆ’1njn\_{t,\ell}=\sum\_{j=t-\ell}^{t-1}n\_{j}. The accuracy of model comparison depends on the estimation accuracy of Î”^t,â„“\widehat{\Delta}\_{t,\ell}. The critical challenge is choosing a validation window size â„“\ell such that the estimation error |Î”^t,â„“âˆ’Î”t||\widehat{\Delta}\_{t,\ell}-\Delta\_{t}| is small.

The choice of the validation window â„“\ell involves a bias-variance tradeoff: with probability at least 1âˆ’Î´1-\delta,

|  |  |  |  |
| --- | --- | --- | --- |
|  | |Î”^t,â„“âˆ’Î”t|â‰¤Ï•â€‹(t,â„“)+Ïˆâ€‹(t,â„“,Î´).\big|\widehat{\Delta}\_{t,\ell}-\Delta\_{t}\big|\leq\phi(t,\ell)+\psi(t,\ell,\delta). |  | (4.2) |

Here,

|  |  |  |
| --- | --- | --- |
|  | Ï•â€‹(t,â„“)=maxtâˆ’â„“â‰¤jâ‰¤tâˆ’1â¡|Î”jâˆ’Î”t|\phi(t,\ell)=\max\_{t-\ell\leq j\leq t-1}|\Delta\_{j}-\Delta\_{t}| |  |

is the bias term that measures the non-stationarity of Î”j\Delta\_{j} in the last â„“\ell periods, and

|  |  |  |
| --- | --- | --- |
|  | Ïˆâ€‹(t,â„“,Î´)={8â€‹M2,ifÂ â€‹nt,â„“=1Ïƒt,â„“â€‹2â€‹logâ¡(2/Î´)nt,â„“+16â€‹M2â€‹logâ¡(2/Î´)3â€‹nt,â„“,ifÂ â€‹nt,â„“â‰¥2,withÏƒt,â„“2=1nt,â„“â€‹âˆ‘j=tâˆ’â„“tâˆ’1njâ€‹varâ¡(uj,1),\psi(t,\ell,\delta)=\begin{cases}8M^{2},&\ \text{if }n\_{t,\ell}=1\\[6.0pt] \displaystyle\sigma\_{t,\ell}\sqrt{\frac{2\log(2/\delta)}{n\_{t,\ell}}}+\frac{16M^{2}\log(2/\delta)}{3n\_{t,\ell}},&\ \text{if }n\_{t,\ell}\geq 2\end{cases},\quad\text{with}\quad\sigma\_{t,\ell}^{2}=\frac{1}{n\_{t,\ell}}\sum\_{j=t-\ell}^{t-1}n\_{j}\operatorname{{\rm var}}(u\_{j,1}), |  |

is the variance term that quantifies the statistical uncertainty associated with the estimate Î”^t,â„“\widehat{\Delta}\_{t,\ell} via a Bernstein concentration inequality. In general, as the window â„“\ell increases, we expect the bias Ï•â€‹(t,â„“)\phi(t,\ell) to increase and the variance term Ïˆâ€‹(t,â„“)\psi(t,\ell) to decrease. The ideal validation window size â„“âˆ—\ell^{\*} should strike a balance between the bias and variance:

|  |  |  |
| --- | --- | --- |
|  | â„“âˆ—=argminâ„“âˆˆ[tâˆ’1]{Ï•â€‹(t,â„“)+Ïˆâ€‹(t,â„“,Î´)}.\ell^{\*}=\mathop{\mathrm{argmin}}\_{\ell\in[t-1]}\left\{\phi(t,\ell)+\psi(t,\ell,\delta)\right\}. |  |

However, as both Ï•â€‹(t,â„“)\phi(t,\ell) and Ïˆâ€‹(t,â„“)\psi(t,\ell) depend on the unknown non-stationarity, â„“âˆ—\ell^{\*} cannot be directly computed.

To tackle this problem, we construct proxies Ïˆ^\widehat{\psi} and Ï•^\widehat{\phi} for Ïˆ\psi and Ï•\phi, respectively. The proxy for Ïˆâ€‹(t,â„“,Î´)\psi(t,\ell,\delta) is constructed by replacing the unknown variance Ïƒt,â„“2\sigma\_{t,\ell}^{2} by the sample variance

|  |  |  |  |
| --- | --- | --- | --- |
|  | v^t,â„“2=1nt,â„“âˆ’1â€‹âˆ‘j=tâˆ’â„“tâˆ’1âˆ‘i=1nj(uj,iâˆ’Î”^t,â„“)2,\widehat{v}\_{t,\ell}^{2}=\frac{1}{n\_{t,\ell}-1}\sum\_{j=t-\ell}^{t-1}\sum\_{i=1}^{n\_{j}}\big(u\_{j,i}-\widehat{\Delta}\_{t,\ell}\big)^{2}, |  | (4.3) |

which gives

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ïˆ^â€‹(t,â„“,Î´)={8â€‹M2,ifÂ â€‹nt,â„“=1v^t,â„“â€‹2â€‹logâ¡(2/Î´)nt,â„“+64â€‹M2â€‹logâ¡(2/Î´)3â€‹(nt,â„“âˆ’1),ifÂ â€‹nt,â„“â‰¥2.\widehat{\psi}(t,\ell,\delta)=\begin{cases}8M^{2},&\ \text{if }n\_{t,\ell}=1\\[6.0pt] \displaystyle\widehat{v}\_{t,\ell}\sqrt{\frac{2\log(2/\delta)}{n\_{t,\ell}}}+\frac{64M^{2}\log(2/\delta)}{3(n\_{t,\ell}-1)},&\ \text{if }n\_{t,\ell}\geq 2\end{cases}. |  | (4.4) |

The proxy for the bias term is inspired by the Goldenshluger-Lepski method for adaptive non-parametric estimation (GLe08):

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ï•^(t,â„“,Î´)=maxiâˆˆ[â„“](|Î”^t,â„“âˆ’Î”^t,i|âˆ’[Ïˆ^(t,â„“,Î´)+Ïˆ^(t,i,Î´)])+.\widehat{\phi}(t,\ell,\delta)=\max\_{i\in[\ell]}\bigg(\big|\widehat{\Delta}\_{t,\ell}-\widehat{\Delta}\_{t,i}\big|-\big[\widehat{\psi}\left(t,\ell,\delta\right)+\widehat{\psi}\left(t,i,\delta\right)\big]\bigg)\_{+}. |  | (4.5) |

To interpret Ï•^\widehat{\phi}, in light of the bias-variance decomposition in ([4.2](https://arxiv.org/html/2512.23596v1#S4.E2 "Equation 4.2 â€£ Pairwise comparison subroutine. â€£ 4.1 Adaptive Tournament Model Selection â€£ 4 Adaptive Model and Data Selection under Non-Stationarity â€£ The nonstationarity-complexity tradeoff in return prediction")), the quantity

|  |  |  |  |
| --- | --- | --- | --- |
|  | (|Î”^t,â„“âˆ’Î”^t,i|âˆ’[Ïˆ^â€‹(t,â„“,Î´)+Ïˆ^â€‹(t,i,Î´)])+\left(|\widehat{\Delta}\_{t,\ell}-\widehat{\Delta}\_{t,i}|-\big[\widehat{\psi}\left(t,\ell,\delta\right)+\widehat{\psi}\left(t,i,\delta\right)\big]\right)\_{+} |  | (4.6) |

can be viewed as a measure of the bias between the window â„“\ell and a smaller window iâ‰¤â„“i\leq\ell, where subtracting Ïˆ^â€‹(t,â„“,Î´)\widehat{\psi}\left(t,\ell,\delta\right) and Ïˆ^â€‹(t,i,Î´)\widehat{\psi}\left(t,i,\delta\right) eliminates the stochastic error and teases out the bias. The term Ï•^â€‹(t,â„“,Î´)\widehat{\phi}(t,\ell,\delta) is then formed by taking the maximum of ([4.6](https://arxiv.org/html/2512.23596v1#S4.E6 "Equation 4.6 â€£ Pairwise comparison subroutine. â€£ 4.1 Adaptive Tournament Model Selection â€£ 4 Adaptive Model and Data Selection under Non-Stationarity â€£ The nonstationarity-complexity tradeoff in return prediction")) over all smaller windows iâˆˆ[â„“]i\in[\ell].

After constructing the bias and variance proxies, one chooses a window size â„“^\widehat{\ell} that minimizes their sum:

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„“^=argminâ„“âˆˆ[tâˆ’1]{Ï•^â€‹(t,â„“)+Ïˆ^â€‹(t,â„“,Î´)}.\widehat{\ell}=\mathop{\mathrm{argmin}}\_{\ell\in[t-1]}\left\{\widehat{\phi}(t,\ell)+\widehat{\psi}(t,\ell,\delta)\right\}. |  | (4.7) |

We then use Î”^t,â„“^\widehat{\Delta}\_{t,\widehat{\ell}} as our estimate of Î”t\Delta\_{t} for model comparison. In particular, the subroutine selects f1f\_{1} if and only if Î”^t,â„“^â‰¤0\widehat{\Delta}\_{t,\widehat{\ell}}\leq 0. The procedure is summarized in LABEL:sm:alg-compare.

Input: Candidate models {f1,f2}\{f\_{1},f\_{2}\}, validation data {ğ’Ÿjva}j=1tâˆ’1\{\mathcal{D}^{\operatorname{\mathrm{va}}}\_{j}\}\_{j=1}^{t-1}, hyperparameters Î´â€²\delta^{\prime} and MM.

for â„“=1,â‹¯,tâˆ’1\ell=1,\cdots,t-1

Compute Î”^t,â„“\widehat{\Delta}\_{t,\ell}, Ïˆ^â€‹(t,â„“,Î´â€²)\widehat{\psi}(t,\ell,\delta^{\prime}) and Ï•^â€‹(t,â„“,Î´â€²)\widehat{\phi}(t,\ell,\delta^{\prime}) according to ([4.1](https://arxiv.org/html/2512.23596v1#S4.E1 "Equation 4.1 â€£ Pairwise comparison subroutine. â€£ 4.1 Adaptive Tournament Model Selection â€£ 4 Adaptive Model and Data Selection under Non-Stationarity â€£ The nonstationarity-complexity tradeoff in return prediction")), ([4.4](https://arxiv.org/html/2512.23596v1#S4.E4 "Equation 4.4 â€£ Pairwise comparison subroutine. â€£ 4.1 Adaptive Tournament Model Selection â€£ 4 Adaptive Model and Data Selection under Non-Stationarity â€£ The nonstationarity-complexity tradeoff in return prediction")) and ([4.5](https://arxiv.org/html/2512.23596v1#S4.E5 "Equation 4.5 â€£ Pairwise comparison subroutine. â€£ 4.1 Adaptive Tournament Model Selection â€£ 4 Adaptive Model and Data Selection under Non-Stationarity â€£ The nonstationarity-complexity tradeoff in return prediction")).

Choose window size

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„“^âˆˆargminâ„“âˆˆ[tâˆ’1]{Ï•^â€‹(t,â„“,Î´â€²)+Ïˆ^â€‹(t,â„“,Î´â€²)}.\widehat{\ell}\in\mathop{\mathrm{argmin}}\_{\ell\in[t-1]}\Big\{\widehat{\phi}(t,\ell,\delta^{\prime})+\widehat{\psi}(t,\ell,\delta^{\prime})\Big\}. |  | (4.8) |

Select Î»^=1\widehat{\lambda}=1 if Î”^t,â„“^â‰¤0\widehat{\Delta}\_{t,\widehat{\ell}}\leq 0, and Î»^=2\widehat{\lambda}=2 otherwise.

return f^=fÎ»^\widehat{f}=f\_{\widehat{\lambda}}.

Algorithm 2  Adaptive Rolling Window for Model Comparison

By using LABEL:sm:alg-compare as the model comparison subroutine in LABEL:sm:alg-tournament, we obtain an algorithm that adaptively uses chooses non-stationary data to perform model selection. We call the algorithm Adaptive Tournament Model Selection, or ATOMS in short.

### 4.2 Theoretical Guarantees

We now present the theoretical guarantees for our model selection framework. LABEL:sm:thm-model-comparison below establishes a performance bound for the pairwise model comparison subroutine (LABEL:sm:alg-compare).

###### Theorem 4.1 (Near-optimal model comparison).

Let Assumptions [2.1](https://arxiv.org/html/2512.23596v1#S2.Thmassumption1 "Assumption 2.1 (Independent data). â€£ 2 Problem Setup â€£ The nonstationarity-complexity tradeoff in return prediction") and [3.1](https://arxiv.org/html/2512.23596v1#S3.Thmassumption1 "Assumption 3.1 (Boundedness). â€£ 3.2 Theoretical Characterization â€£ 3 The Nonstationarity-Complexity Tradeoff â€£ The nonstationarity-complexity tradeoff in return prediction") hold. Choose Î´âˆˆ(0,1)\delta\in(0,1) and take Î´â€²=Î´/(3â€‹t)\delta^{\prime}=\delta/(3t) in LABEL:sm:alg-compare. With probability at least 1âˆ’Î´1-\delta, the output f^\widehat{f} of LABEL:sm:alg-compare satisfies

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„°tâ€‹(f^)â‰²minâ¡{â„°tâ€‹(f1),â„°tâ€‹(f2)}+M2â€‹logâ¡(t/Î´)â‹…minâ„“âˆˆ[tâˆ’1]â¡{maxtâˆ’â„“â‰¤jâ‰¤tâˆ’1â¡TVâ¡(Pj,Pt)+1nt,â„“}.\mathcal{E}\_{t}(\widehat{f})\lesssim\min\{\mathcal{E}\_{t}(f\_{1}),\mathcal{E}\_{t}(f\_{2})\}+M^{2}\log(t/\delta)\cdot\min\_{\ell\in[t-1]}\left\{\max\_{t-\ell\leq j\leq t-1}\operatorname{TV}(P\_{j},P\_{t})+\frac{1}{n\_{t,\ell}}\right\}. |  | (4.9) |

Here â‰²\lesssim hides a universal constant.

###### Proof of LABEL:sm:thm-model-comparison.

See LABEL:sm:sec-thm-model-comparison-proof.
âˆ

LABEL:sm:thm-model-comparison gives a finite-sample oracle inequality ([4.9](https://arxiv.org/html/2512.23596v1#S4.E9 "Equation 4.9 â€£ Theorem 4.1 (Near-optimal model comparison). â€£ 4.2 Theoretical Guarantees â€£ 4 Adaptive Model and Data Selection under Non-Stationarity â€£ The nonstationarity-complexity tradeoff in return prediction")). It states that the excess risk of the f^\widehat{f} does not exceed that of the better model between f1f\_{1} and f2f\_{2}, plus an additional error term that reflects the difficulty of using the non-stationary data to make the comparison. Inside this additional term, the quantity

|  |  |  |
| --- | --- | --- |
|  | maxtâˆ’â„“â‰¤jâ‰¤tâˆ’1â¡TVâ¡(Pj,Pt)+1nt,â„“\max\_{t-\ell\leq j\leq t-1}\operatorname{TV}(P\_{j},P\_{t})+\frac{1}{n\_{t,\ell}} |  |

represents the two sources of errors that arise when using a validation window â„“\ell to compare models, namely, the non-stationarity maxtâˆ’â„“â‰¤jâ‰¤tâˆ’1â¡TVâ¡(Pj,Pt)\max\_{t-\ell\leq j\leq t-1}\operatorname{TV}(P\_{j},P\_{t}) and the statistical uncertainty 1/nt,â„“1/n\_{t,\ell} associated with the nt,â„“n\_{t,\ell} validation samples. The bound takes the minimum over all validation window sizes â„“\ell, meaning that LABEL:sm:alg-compare performs almost as well as an oracle that knows in hindsight which validation window size â„“\ell would lead to the most accurate comparison. This shows that LABEL:sm:alg-compare adaptively chooses a near-optimal validation window tailored to the local non-stationarity.

Building on this pairwise guarantee, LABEL:sm:thm-select-tournament below shows that our model selection algorithm ATOMS inherits the same oracle property when selecting from multiple candidate models {fÎ»}Î»=1Î›\{f\_{\lambda}\}\_{\lambda=1}^{\Lambda}.

###### Theorem 4.2 (Near-optimal model selection).

Let Assumptions [2.1](https://arxiv.org/html/2512.23596v1#S2.Thmassumption1 "Assumption 2.1 (Independent data). â€£ 2 Problem Setup â€£ The nonstationarity-complexity tradeoff in return prediction") and [3.1](https://arxiv.org/html/2512.23596v1#S3.Thmassumption1 "Assumption 3.1 (Boundedness). â€£ 3.2 Theoretical Characterization â€£ 3 The Nonstationarity-Complexity Tradeoff â€£ The nonstationarity-complexity tradeoff in return prediction") hold. Choose Î´âˆˆ(0,1)\delta\in(0,1) and take Î´â€²=Î´/(3â€‹Î›2â€‹t)\delta^{\prime}=\delta/(3\Lambda^{2}t) in ATOMS. With probability at least 1âˆ’Î´1-\delta, ATOMS outputs a model f^\widehat{f} satisfying

|  |  |  |
| --- | --- | --- |
|  | â„°tâ€‹(f^)â‰²minÎ»âˆˆ[Î›]â¡â„°tâ€‹(fÎ»)+M2â€‹logâ¡(Î›â€‹t/Î´)â‹…minâ„“âˆˆ[tâˆ’1]â¡{maxtâˆ’â„“â‰¤jâ‰¤tâˆ’1â¡TVâ¡(Pj,Pt)+1nt,â„“}.\mathcal{E}\_{t}(\widehat{f})\lesssim\min\_{\lambda\in[\Lambda]}\mathcal{E}\_{t}(f\_{\lambda})+M^{2}\log(\Lambda t/\delta)\cdot\min\_{\ell\in[t-1]}\left\{\max\_{t-\ell\leq j\leq t-1}\operatorname{TV}(P\_{j},P\_{t})+\frac{1}{n\_{t,\ell}}\right\}. |  |

Here â‰²\lesssim hides a universal constant.

###### Proof of LABEL:sm:thm-select-tournament.

See LABEL:sm:sec-thm-select-tournament-proof.
âˆ

LABEL:sm:thm-select-tournament states that the excess risk of the model f^\widehat{f} chosen by the tournament is at most the excess risk of the best model in {fÎ»}Î»=1Î›\{f\_{\lambda}\}\_{\lambda=1}^{\Lambda}, up to an additional term that has the same form as in the pairwise comparison bound, with an extra Oâ€‹(logâ¡Î›)O(\log\Lambda) multiplicative factor. In other words, ATOMS identifies a model whose performance is nearly as good as the best candidate one could have selected in hindsight using the non-stationary validation data.

We remark that our model selection framework (LABEL:sm:alg-tournament) is general and can be combined with any model comparison subroutine ğ’œ\mathcal{A}. In particular, in LABEL:sm:sec-thm-select-tournament-proof, we prove a general reduction lemma (LABEL:sm:lem-select-tournament-reduction) that converts any theoretical guarantee of the subroutine ğ’œ\mathcal{A} to a guarantee of LABEL:sm:alg-tournament. In LABEL:sm:sec-select-R2, we further develop a R2R^{2}-based pairwise comparison subroutine that targets the R2R^{2} metric. When equipped with this R2R^{2}-based subroutine, LABEL:sm:alg-tournament enjoys a guarantee with respect to the R2R^{2} metric.

###### Remark 1 (Comparison with prior work).

Our model selection framework builds upon the model comparison method of HHW24. Below we briefly discuss the main differences between our work and theirs. First, their analysis of the model comparison procedure (LABEL:sm:alg-compare) assumes that the distribution of the covariates ğ±\bm{x} remains fixed across time. Our theory removes this assumption entirely, and covers the general non-stationary setting where the joint data distribution (ğ±,y)(\bm{x},y) can change arbitrarily. Second, for model selection, they propose a single-elimination procedure which performs Î›âˆ’1\Lambda-1 model comparisons, but incurs additional factors (logâ¡Î›)2(\log\Lambda)^{2} in the performance bound. In contrast, our approach maintains a linear complexity in Î›\Lambda in expectation while achieving a sharper bound.

### 4.3 Application to Joint Model Class and Training Window Size Selection

Finally, we apply LABEL:sm:thm-select-tournament to the joint selection of model class and training sample size.
Let â„±\mathscr{F} be a finite collection of model classes, e.g., â„±={linear model,random forest of a certain size}\mathscr{F}=\{\text{linear model},\text{random forest of a certain size}\}. For each model class â„±âˆˆâ„±\mathcal{F}\in\mathscr{F}, we train models on different windows kâˆˆ[tâˆ’1]k\in[t-1] of the training data {ğ’Ÿjtr}j=1tâˆ’1\{\mathcal{D}\_{j}^{\operatorname{\mathrm{tr}}}\}\_{j=1}^{t-1}. Let h^â€‹(â„±,k)\widehat{h}(\mathcal{F},k) denote the model from â„±\mathcal{F} trained on {ğ’Ÿjtr}j=tâˆ’ktâˆ’1\{\mathcal{D}\_{j}^{\operatorname{\mathrm{tr}}}\}\_{j=t-k}^{t-1}. Then, the set of candidate models is given by

|  |  |  |  |
| --- | --- | --- | --- |
|  | {fÎ»}Î»=1Î›={h^â€‹(â„±,k):â„±âˆˆâ„±,kâˆˆ[tâˆ’1]}.\{f\_{\lambda}\}\_{\lambda=1}^{\Lambda}=\left\{\widehat{h}(\mathcal{F},k):\mathcal{F}\in\mathscr{F},\,k\in[t-1]\right\}. |  | (4.10) |

Applying LABEL:sm:thm-select-tournament to this set of candidate models yields the following guarantee. For simplicity, we assume that training-validation data splitting ratio is fixed across time.

###### Assumption 4.1 (Balanced training-validation split).

There exists c>0c>0 such that |ğ’Ÿjtr|/|ğ’Ÿjva|=c|\mathcal{D}\_{j}^{\operatorname{\mathrm{tr}}}|/|\mathcal{D}\_{j}^{\operatorname{\mathrm{va}}}|=c for all jâˆˆâ„¤+j\in\mathbb{Z}\_{+}.

###### Theorem 4.3 (Near-optimal model-and-data selection).

Let Assumptions [2.1](https://arxiv.org/html/2512.23596v1#S2.Thmassumption1 "Assumption 2.1 (Independent data). â€£ 2 Problem Setup â€£ The nonstationarity-complexity tradeoff in return prediction"), [3.1](https://arxiv.org/html/2512.23596v1#S3.Thmassumption1 "Assumption 3.1 (Boundedness). â€£ 3.2 Theoretical Characterization â€£ 3 The Nonstationarity-Complexity Tradeoff â€£ The nonstationarity-complexity tradeoff in return prediction") and [4.1](https://arxiv.org/html/2512.23596v1#S4.Thmassumption1 "Assumption 4.1 (Balanced training-validation split). â€£ 4.3 Application to Joint Model Class and Training Window Size Selection â€£ 4 Adaptive Model and Data Selection under Non-Stationarity â€£ The nonstationarity-complexity tradeoff in return prediction") hold. Suppose the set of candidate models is given by ([4.10](https://arxiv.org/html/2512.23596v1#S4.E10 "Equation 4.10 â€£ 4.3 Application to Joint Model Class and Training Window Size Selection â€£ 4 Adaptive Model and Data Selection under Non-Stationarity â€£ The nonstationarity-complexity tradeoff in return prediction")). Choose Î´âˆˆ(0,1)\delta\in(0,1) and take Î´â€²=Î´/(6â€‹|â„±|2â€‹t3)\delta^{\prime}=\delta/(6|\mathscr{F}|^{2}t^{3}) in ATOMS. Then, with probability at least 1âˆ’Î´1-\delta, the output f^\widehat{f} of ATOMS satisfies

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„°tâ€‹(f^)â‰²minâ„±âˆˆâ„±,kâˆˆ[tâˆ’1]â¡{minfâˆˆâ„±â¡â„°tâ€‹(f)+(rt,kâ€‹(â„±)+1Bt,k)+maxtâˆ’kâ‰¤jâ‰¤tâˆ’1â¡TVâ¡(Pj,Pt)},\mathcal{E}\_{t}(\widehat{f})\lesssim\min\_{\mathcal{F}\in\mathscr{F},\,k\in[t-1]}\left\{\min\_{f\in\mathcal{F}}\mathcal{E}\_{t}(f)+\left(r\_{t,k}(\mathcal{F})+\frac{1}{B\_{t,k}}\right)+\max\_{t-k\leq j\leq t-1}\operatorname{TV}(P\_{j},P\_{t})\right\}, |  | (4.11) |

where Bt,k=âˆ‘j=tâˆ’ktâˆ’1(|ğ’Ÿjtr|+|ğ’Ÿjva|)B\_{t,k}=\sum\_{j=t-k}^{t-1}\left(|\mathcal{D}\_{j}^{\operatorname{\mathrm{tr}}}|+|\mathcal{D}\_{j}^{\operatorname{\mathrm{va}}}|\right) is the total sample size, and â‰²\lesssim hides the constants MM and cc and logarithmic factors of tt, Î´âˆ’1\delta^{-1} and |â„±||\mathscr{F}|.

###### Proof of LABEL:sm:thm-joint-selection.

See LABEL:sm:sec-thm-joint-selection-proof.
âˆ

We note that the term

|  |  |  |
| --- | --- | --- |
|  | minfâˆˆâ„±â¡â„°tâ€‹(f)+(rt,kâ€‹(â„±)+1Bt,k)+maxtâˆ’kâ‰¤jâ‰¤tâˆ’1â¡TVâ¡(Pj,Pt)\min\_{f\in\mathcal{F}}\mathcal{E}\_{t}(f)+\left(r\_{t,k}(\mathcal{F})+\frac{1}{B\_{t,k}}\right)+\max\_{t-k\leq j\leq t-1}\operatorname{TV}(P\_{j},P\_{t}) |  |

on the right hand side of ([4.11](https://arxiv.org/html/2512.23596v1#S4.E11 "Equation 4.11 â€£ Theorem 4.3 (Near-optimal model-and-data selection). â€£ 4.3 Application to Joint Model Class and Training Window Size Selection â€£ 4 Adaptive Model and Data Selection under Non-Stationarity â€£ The nonstationarity-complexity tradeoff in return prediction")) is exactly the model performance bound of the model hâ€‹(â„±,k)h(\mathcal{F},k) in LABEL:sm:thm-tradeoff. Thus, our algorithm selects a near-optimal pair of model class and training window size, up to logarithmic factors.

## 5 Explaining the Cross-Section of Industry Portfolio Returns

In this section, we investigate whether our algorithm helps explain the cross-section of stock returns using industry portfolios as test assets. Rather than simply building predictive models, we approximate time-varying stochastic discount factors that capture the evolving relationship between risk and return. First, we describe our comprehensive dataset of firm-characteristic managed portfolios and industry portfolios. Then, we report our finding that our adaptive algorithm ATOMS achieves superior out-of-sample performance in explaining expected returns compared to fixed window and expanding window approaches across different economic regimes.

### 5.1 Data

We examine the pricing of 17 industry portfolio returns from Kenneth Frenchâ€™s data library, covering the period from September 1987 to November 2016. Our predictor set combines macroeconomic factors, risk premia from characteristic-sorted portfolios, and lagged returns to capture the complex dynamics driving industry returns, sourced for widely cited public datasets.888More specifically, our data combines daily and monthly sources to construct a comprehensive time series of covariates combining macroeconomic and cross-sectional signals. The final sample spans the time period from September 1987 to November 2016. We merge daily CRSP excess returns with monthly characteristics from GKX20, which provides 94 standardized characteristics for U.S. equities encompassing valuation ratios, profitability measures, investment activity, liquidity, and past return dynamics. These characteristics have become the canonical set of firm-level predictors in modern empirical asset pricing. We construct daily long-short portfolios by sorting firms into deciles based on each characteristic and taking the difference between the top and bottom decile returns, following the methodology of GKX20. We provide full details of the dataset construction in LABEL:sm:sec-dataapdx, including data preprocessing and long-short portfolio constructions.

##### Macroeconomic and Systematic Factors.

We incorporate 15 factors from CPZ24, who estimate a SDF using deep learning while imposing no-arbitrage restrictions. These factors include: (i) the estimated SDF representing the aggregate price of risk; (ii) ten beta-sorted decile portfolios based on firmsâ€™ SDF exposure; and (iii) four macroeconomic hidden states extracted from 178 macro time series via a generative adversarial network. These monthly observations are assigned to all trading days within each month. We also include the daily Fama-French three factors (market, size, and value) from FFr93 as benchmark risk factors.

##### Characteristic-Sorted Portfolios.

Following GKX20, we construct 94 long-short portfolios sorted on firm characteristics that capture price trends, liquidity, size, and risk measures. For each characteristic, we form decile portfolios using all CRSP-listed stocks and create a long-short strategy that buys the top decile and shorts the bottom decile. This approach transforms firm-level characteristics into interpretable factor returns that isolate the pricing implications of each characteristic.

##### Predictor Set.

Our final predictor set comprises: (i) 15 macroeconomic factors from CPZ24; (ii) 3 Fama-French factors; (iii) 94 characteristic-sorted long-short portfolio returns; and (iv) the 17 lagged industry returns. This comprehensive set combines traditional risk factors with modern high-dimensional representations, allowing us to test whether our adaptive asset pricing framework can effectively navigate the complex, time-varying relationships between these predictors and industry returns.

### 5.2 Return model

We evaluate our adaptive algorithm ATOMS using candidate models from different specifications with varying parameters and estimation windows. We take one month as a period, where the data ğ’Ÿt={(ğ’™t,i,yt,i)}i=1Bt\mathcal{D}\_{t}=\{(\bm{x}\_{t,i},y\_{t,i})\}\_{i=1}^{B\_{t}} in month tt consists of daily covariate-return pairs within that month.

##### Model Specifications.

We consider the following specifications that approximate stochastic discount factors. For a vector of covariates ğ’™âˆˆâ„d\bm{x}\in\mathbb{R}^{d}, we write ğ’™~=(ğ’™âŠ¤,1)âŠ¤âˆˆâ„d+1\widetilde{\bm{x}}=(\bm{x}^{\top},1)^{\top}\in\mathbb{R}^{d+1}.

1. 1.

   Non-linear specification using random forests (RF). Given training data {(ğ’™i,yi)}i=1n\{(\bm{x}\_{i},y\_{i})\}\_{i=1}^{n}, and two parameters, namely the number of trees ntreen\_{\texttt{tree}} and the maximum tree depth dmaxd\_{\max}, RF estimates a random forest model.
2. 2.

   Linear specification estimated with ridge regularization (Ridge). Given training data {(ğ’™i,yi)}i=1n\{(\bm{x}\_{i},y\_{i})\}\_{i=1}^{n} and regularization parameter Î±>0\alpha>0, Ridge estimates a linear model fâ€‹(ğ’™)=âŸ¨ğœ½^,ğ’™~âŸ©f(\bm{x})=\langle\widehat{\bm{\theta}},\,\widetilde{\bm{x}}\rangle by

   |  |  |  |
   | --- | --- | --- |
   |  | ğœ½^=argminğœ½âˆˆâ„d+1{1nâ€‹âˆ‘i=1n(âŸ¨ğœ½,ğ’™~iâŸ©âˆ’yi)2+Î±â€–ğœ½âˆ¥22}.\widehat{\bm{\theta}}=\mathop{\mathrm{argmin}}\_{\bm{\theta}\in\mathbb{R}^{d+1}}\left\{\frac{1}{n}\sum\_{i=1}^{n}\left(\langle\bm{\theta},\,\widetilde{\bm{x}}\_{i}\rangle-y\_{i}\right)^{2}+\alpha\|\bm{\theta}\|\_{2}^{2}\right\}. |  |
3. 3.

   Linear specification with LASSO regularization (LASSO). Given training data {(ğ’™i,yi)}i=1n\{(\bm{x}\_{i},y\_{i})\}\_{i=1}^{n} and regularization parameter Î±>0\alpha>0, LASSO estimates a linear model fâ€‹(ğ’™)=âŸ¨ğœ½^,ğ’™~âŸ©f(\bm{x})=\langle\widehat{\bm{\theta}},\,\widetilde{\bm{x}}\rangle by

   |  |  |  |
   | --- | --- | --- |
   |  | ğœ½^=argminğœ½âˆˆâ„d+1{12â‹…1nâ€‹âˆ‘i=1n(âŸ¨ğœ½,ğ’™~iâŸ©âˆ’yi)2+Î±â€–ğœ½âˆ¥1}.\widehat{\bm{\theta}}=\mathop{\mathrm{argmin}}\_{\bm{\theta}\in\mathbb{R}^{d+1}}\left\{\frac{1}{2}\cdot\frac{1}{n}\sum\_{i=1}^{n}\left(\langle\bm{\theta},\,\widetilde{\bm{x}}\_{i}\rangle-y\_{i}\right)^{2}+\alpha\|\bm{\theta}\|\_{1}\right\}. |  |
4. 4.

   Linear specification with elastic net regularization (E-Net). Given training data {(ğ’™i,yi)}i=1n\{(\bm{x}\_{i},y\_{i})\}\_{i=1}^{n} and regularization parameters Î±>0\alpha>0 and râˆˆ(0,1)r\in(0,1), E-Net estimates a linear model fâ€‹(ğ’™)=âŸ¨ğœ½^,ğ’™~âŸ©f(\bm{x})=\langle\widehat{\bm{\theta}},\,\widetilde{\bm{x}}\rangle by

   |  |  |  |
   | --- | --- | --- |
   |  | ğœ½^=argminğœ½âˆˆâ„d+1{12â‹…1nâ€‹âˆ‘i=1n(âŸ¨ğœ½,ğ’™~iâŸ©âˆ’yi)2+Î±â€‹râ€–ğœ½âˆ¥1+Î±2â€‹(1âˆ’r)â€‹â€–ğœ½â€–22}.\widehat{\bm{\theta}}=\mathop{\mathrm{argmin}}\_{\bm{\theta}\in\mathbb{R}^{d+1}}\left\{\frac{1}{2}\cdot\frac{1}{n}\sum\_{i=1}^{n}\left(\langle\bm{\theta},\,\widetilde{\bm{x}}\_{i}\rangle-y\_{i}\right)^{2}+\alpha r\|\bm{\theta}\|\_{1}+\frac{\alpha}{2}(1-r)\|\bm{\theta}\|\_{2}^{2}\right\}. |  |

In each month tt, we estimate models from these specifications on estimation windows of 4kâˆ§(tâˆ’1)4^{k}\wedge(t-1) months, where 0â‰¤kâ‰¤50\leq k\leq 5. We detail the parameter choices for the specifications in LABEL:sm:sec-hyperparameters.

##### Benchmark Approaches.

To verify the adaptivity of our framework, we compare it with two non-adaptive benchmarks that use a fixed window for estimation and/or validation.

1. 1.

   Fixed validation window for specification selection (Fixed-valâ€‹(â„“)\texttt{Fixed-val}(\ell)). This is the non-adaptive fixed-window counterpart of ATOMS. In each month tt, we estimate the same candidate specifications above, then use validation data from the last â„“\ell periods {ğ’Ÿjva}j=tâˆ’â„“tâˆ’1\{\mathcal{D}\_{j}^{\operatorname{\mathrm{va}}}\}\_{j=t-\ell}^{t-1} to perform specification selection. The detailed description of Fixed-valâ€‹(â„“)\texttt{Fixed-val}(\ell) is given in LABEL:sm:alg-fixed. We consider validation window sizes â„“=32,128,512\ell=32,128,512 months, where â„“=512\ell=512 corresponds to using all historical validation data at all times.
2. 2.

   Fixed-window cross-validation (Fixed-CV). In each month tt, we use data from the last 3636 months {ğ’Ÿj}j=tâˆ’36tâˆ’1\{\mathcal{D}\_{j}\}\_{j=t-36}^{t-1} to perform 55-fold cross-validation to estimate and select a specification out of the candidate specifications with the same sets of parameters.

We run each of these approaches over 2020 random splits of estimation and validation data. More details can be found in LABEL:sm:sec-hyperparameters.

Input: Candidate specifications {fÎ»}Î»=1Î›\{f\_{\lambda}\}\_{\lambda=1}^{\Lambda}, validation data {ğ’Ÿjva}j=1tâˆ’1\{\mathcal{D}^{\operatorname{\mathrm{va}}}\_{j}\}\_{j=1}^{t-1}, validation window size â„“\ell.

Select

|  |  |  |
| --- | --- | --- |
|  | Î»^=argminÎ»âˆˆ[Î›]âˆ‘j=(tâˆ’â„“)âˆ¨1tâˆ’1âˆ‘i=1nj[fÎ»â€‹(ğ’™j,iva)âˆ’yj,iva]2.\widehat{\lambda}=\mathop{\mathrm{argmin}}\_{\lambda\in[\Lambda]}\sum\_{j=(t-\ell)\vee 1}^{t-1}\sum\_{i=1}^{n\_{j}}\left[f\_{\lambda}(\bm{x}^{\operatorname{\mathrm{va}}}\_{j,i})-y^{\operatorname{\mathrm{va}}}\_{j,i}\right]^{2}. |  |

return f^=fÎ»^\widehat{f}=f\_{\widehat{\lambda}}.

Algorithm 3  Fixed Validation Window for Specification Selection (Fixed-valâ€‹(â„“)\texttt{Fixed-val}(\ell))

##### Performance Metrics.

We measure the performance of each approach using the out-of-sample R2R^{2} metric ([2.2](https://arxiv.org/html/2512.23596v1#S2.E2 "Equation 2.2 â€£ 2 Problem Setup â€£ The nonstationarity-complexity tradeoff in return prediction")) that benchmarks against a zero forecast. We compute both the overall out-of-sample R2R^{2} from January 1990 to November 2016, and the annual out-of-sample R2R^{2}. The latter provides a more granular understanding of the approachesâ€™ performance over time. In LABEL:sm:sec-experiments-standard-R2, we also report results for the standard R2R^{2} metric ([2.3](https://arxiv.org/html/2512.23596v1#S2.E3 "Equation 2.3 â€£ 2 Problem Setup â€£ The nonstationarity-complexity tradeoff in return prediction")).

### 5.3 Empirical Results: 17 Industry Portfolios

We next turn to the empirical analysis of pricing the 17 industry portfolios. The fundamental premise of our adaptive framework is that asset pricing relationships exhibit time-varying dynamics rather than remaining stationary across economic conditions. This non-stationarity is particularly pronounced during economic recessions, when structural breaks in risk premia, sudden shifts in investor risk aversion, and disruptions to market liquidity mechanisms create environments where long-term historical pricing relationships provide poor guidance for future returns. Recessions therefore serve as a natural laboratory for testing the adaptivity of our framework: if our approach can successfully navigate these turbulent periods when non-stationarity is most severe, it provides compelling evidence for the value of adaptive model selection in asset pricing more generally.

##### Recession Performance Analysis.

Our most striking empirical finding relates to the differential performance of ATOMS during economic downturns. LABEL:sm:tab:oos\_r2\_industry\_time presents out-of-sample R2R^{2} values across distinct economic regimes, revealing that ATOMS exhibits particular strength during recessionary periods when market dynamics are most volatile and traditional models typically fail. The adaptive framework achieves an out-of-sample R2R^{2} of 0.049 across the full sample period, representing a 14.0% improvement over the best fixed-window benchmark Fixed-valâ€‹(512)\texttt{Fixed-val}(512) which has R2=0.043R^{2}=0.043.

Table 2: Out-of-Sample R2R^{2} Averages Across Industries by Time Period

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
| Method | Full OOS Period | Recessions | | |
| Gulf War | 2001 Recession | Financial Crisis |
| ATOMS | 0.0490.049 | 0.0270.027 | 0.1250.125 | 0.0410.041 |
| Fixed-valâ€‹(32)\texttt{Fixed-val}(32) | 0.0220.022 | 0.0090.009 | 0.0960.096 | âˆ’0.001-0.001 |
| Fixed-valâ€‹(512)\texttt{Fixed-val}(512) | 0.0430.043 | âˆ’0.031-0.031 | 0.1170.117 | 0.0390.039 |
| Fixed-CV | 0.0350.035 | âˆ’0.007-0.007 | 0.0710.071 | 0.0140.014 |

This table reports out-of-sample (OOS) R2R^{2} averages for return prediction models across all 17 industry portfolios. Full OOS Period refers our largest available OOS period covering 01/1990âˆ¼\sim11/2016. Columns report OOS R2R^{2} averages across all industries and highlight this metric during three recessions, as documented in [NBER Business Cycle Dating](https://www.nber.org/research/business-cycle-dating):

* â€¢

  the 1990 Gulf War recession (06/1990âˆ¼\sim10/1990);
* â€¢

  the 2001 Recession of dot-com bubble burst and the 9/11 attack (05/2001âˆ¼\sim10/2001);
* â€¢

  the Financial Crisis led by defaults of subprime mortgages (11/2007âˆ¼\sim06/2009).

That is, the OOS performance in Gulf War column focuses on model performance comparisons exclusively in the out-of-sample period of 06/1990âˆ¼\sim10/1990. All values are calculated using monthly return data.

During the 2001 recession, characterized by the dot-com bubble collapse and the September 11 terrorist attacks, ATOMS achieves an impressive R2R^{2} of 0.125, outperforming Fixed-valâ€‹(512)\texttt{Fixed-val}(512) by 6.8% (0.117) and substantially exceeding Fixed-CVâ€™s 0.071. This superior performance suggests that our adaptive framework effectively captures the rapid regime shifts that occurred during this period, when technology-related stocks experienced dramatic revaluation and risk premia underwent fundamental restructuring.

The 1990 Gulf War recession provides particularly compelling evidence of our frameworkâ€™s adaptability. While ATOMS maintains a positive R2R^{2} of 0.0270.027 during this sharp but brief contraction, the fixed-window benchmark Fixed-valâ€‹(512)\texttt{Fixed-val}(512) produces a negative R2R^{2} of âˆ’0.031-0.031, indicating worse performance than a simple forecast of zero. This divergence highlights the critical importance of adaptivity during periods of sudden market stress, when historical relationships between risk factors and returns break down most severely. The adaptive frameworkâ€™s ability to rapidly adjust its validation window allows it to recognize and respond to the changing market dynamics that fixed-window models miss entirely.

During the Global Financial Crisis of 2007-2009, ATOMS achieves an R2R^{2} of 0.041, marginally outperforming Fixed-valâ€‹(512)\texttt{Fixed-val}(512) (0.039) and substantially exceeding Fixed-CV (0.014). The relatively smaller performance gap during this period is primarily driven by the fact that NBER defines the Financial Crisis-related recession with a much longer period that spans from 2007 to 2009, where eventually new data observed in the recession itself could be factored into training and validation for our benchmarked model that uses fixed look-back horizon. In other words, the long duration of this recession attenuates the advantages of our method. Nevertheless, ATOMS maintained its advantage throughout this long period, demonstrating robustness across different types of economic contractions.

Figure 4: Box Plot of Out-of-Sample R2R^{2} of ATOMS and Fixed-Window Baselines for 1717 Industry Portfolios.

![Refer to caption](x20.png)


This figure describes the distribution of each methodâ€™s OOS R2R^{2}. Each box corresponds to all industries and all years in our OOS horizon.




Figure 5: Annual Out-of-Sample R2R^{2} of ATOMS and Fixed-Window Baselines for 1717 Industry Portfolios.

![Refer to caption]()

![Refer to caption](x22.png)

![Refer to caption](x23.png)

![Refer to caption](x24.png)

![Refer to caption](x25.png)

![Refer to caption](x26.png)

![Refer to caption](x27.png)

![Refer to caption](x28.png)

![Refer to caption](x29.png)

![Refer to caption](x30.png)

![Refer to caption](x31.png)

![Refer to caption](x32.png)

![Refer to caption](x33.png)

![Refer to caption](x34.png)

![Refer to caption](x35.png)

![Refer to caption](x36.png)

![Refer to caption](x37.png)

This figure reports the annual out-of-sample R2R^{2} of our adaptive model selection algorithm ATOMS (black dashed line with Ã—\timesâ€™s), as well as the fixed-window baselines Fixed-valâ€‹(32)\texttt{Fixed-val}(32) (blue â–¼\blacktriangledownâ€™s), Fixed-valâ€‹(128)\texttt{Fixed-val}(128) (orange â– \blacksquareâ€™s), and Fixed-valâ€‹(512)\texttt{Fixed-val}(512) (red), which use the last 3232, last 128128 and all months of validation data. The title in each subfigure is Kenneth Frenchâ€™s acronym for each industry. For the full names of these industries, please refer to Table [4](https://arxiv.org/html/2512.23596v1#A1.T4 "Table 4 â€£ A.2 Figures and tables â€£ Appendix A Additional details for empirical asset pricing â€£ The nonstationarity-complexity tradeoff in return prediction").

##### Economic Interpretation.

The superior recession performance of ATOMS has important implications for asset pricing theory and practice. Traditional asset pricing models assume stationary risk-return relationships, an assumption that becomes particularly problematic during economic downturns when risk aversion typically increases and market liquidity conditions deteriorate. Our adaptive framework explicitly recognizes this non-stationarity by allowing the validation window to expand or contract based on recent predictive performance.

The empirical evidence suggests that during recessions, the optimal window for model selection shrinks significantly, reflecting the rapid evolution of risk premia. The 1990 Gulf War recession provides the clearest example: its sudden onset and brief duration created an environment where only models with very recent validation data could accurately capture the new pricing dynamics. Conversely, during more prolonged downturns like the Global Financial Crisis, the optimal window likely expanded gradually as new market conditions became established.

From a theoretical perspective, these findings support the view that stochastic discount factors exhibit time-varying dynamics that are particularly pronounced during economic stress. The adaptive frameworkâ€™s ability to track these dynamics more effectively than fixed-window approaches suggests that the non-stationarity of asset pricing relationships is not merely a statistical artifact but reflects fundamental economic mechanisms that vary with the business cycle.

##### Robustness Across Industries.

We conduct industry-by-industry robustness check to confirm that the recession outperformance of ATOMS is not driven by a subset of industries but represents a broad-based phenomenon. Firstly, we report in Figure [4](https://arxiv.org/html/2512.23596v1#S5.F4 "Figure 4 â€£ Recession Performance Analysis. â€£ 5.3 Empirical Results: 17 Industry Portfolios â€£ 5 Explaining the Cross-Section of Industry Portfolio Returns â€£ The nonstationarity-complexity tradeoff in return prediction") that ATOMS overall has better R2R^{2} across all years, as its median, and level position of the box is higher than those of the other methods.

In more details, LABEL:sm:fig-industry-yearly plots the annual out-of-sample R2R^{2} of ATOMS and the baselines in each industry. We observe that ATOMS maintains its advantage across diverse sectors including cyclical industries (Durbl, Cars, Trans) and defensive sectors (Food, Utils, Cnsum). This cross-sectional consistency strengthens our confidence that the observed performance reflects genuine adaptivity to changing market conditions rather than industry-specific anomalies.

Notably, the adaptive algorithm shows particular strength in industries most sensitive to business cycle fluctuations, such as durable goods (Durbl), consumer discretionary (Rtail), and financial services (Finan). This pattern aligns with economic intuition, as these sectors experience the most dramatic shifts in risk premia during economic transitions. Our methodâ€™s ability to capture these dynamics more effectively than benchmarks suggests it successfully identifies the changing risk-return tradeoffs that characterize different phases of the business cycle.

### 5.4 Trading Strategies

To assess the economic significance of our asset pricing framework, we implement trading strategies based on its return predictions and evaluate wealth accumulationâ€”this tests whether the documented predictive power translates into economic value for investors.

Our trading protocol is standard: we start with initial wealth W0=1W\_{0}=1. At the beginning of each month tt, the model yields a predicted return f^t\hat{f}\_{t}. We trade based on the sign of this predicted return. That is, on each trading day iâˆˆ[Bt]i\in[B\_{t}] of month tt, we trade according to the sign of the predicted return f^tâ€‹(ğ±t,i)\hat{f}\_{t}(\mathbf{x}\_{t,i}): we take a long position if f^tâ€‹(ğ±t,i)>0\hat{f}\_{t}(\mathbf{x}\_{t,i})>0, and a short position otherwise. Consequently, the portfolio wealth evolves according to the realized daily returns yt,iy\_{t,i}, adjusted by the position direction. The cumulative wealth WtW\_{t} at the end of month tt is given by:

|  |  |  |
| --- | --- | --- |
|  | Wt=Wtâˆ’1â€‹âˆi=1Bt(1+yt,iâ‹…signâ€‹(f^tâ€‹(ğ’™t,i))).W\_{t}=W\_{t-1}\prod\_{i=1}^{B\_{t}}\left(1+y\_{t,i}\cdot\textrm{sign}\big(\widehat{f}\_{t}(\bm{x}\_{t,i})\big)\right). |  |

Iterating this process, the final wealth WTW\_{T} at time TT is

|  |  |  |
| --- | --- | --- |
|  | WT=W0â€‹âˆt=1Tâˆi=1Bt(1+yt,iâ‹…signâ€‹(f^tâ€‹(ğ’™t,i))).W\_{T}=W\_{0}\prod\_{t=1}^{T}\prod\_{i=1}^{B\_{t}}\left(1+y\_{t,i}\cdot\textrm{sign}\big(\widehat{f}\_{t}(\bm{x}\_{t,i})\big)\right). |  |

Figure 6: Cumulative Wealth Curve of ATOMS and Baselines for 1717 Industry Portfolios.

![Refer to caption](x38.png)

![Refer to caption](x39.png)

![Refer to caption](x40.png)

![Refer to caption](x41.png)

![Refer to caption](x42.png)

![Refer to caption](x43.png)

![Refer to caption](x44.png)

![Refer to caption](x45.png)

![Refer to caption](x46.png)

![Refer to caption](x47.png)

![Refer to caption](x48.png)

![Refer to caption](x49.png)

![Refer to caption](x50.png)

![Refer to caption](x51.png)

![Refer to caption](x52.png)

![Refer to caption](x53.png)

![Refer to caption](x54.png)

This figure reports the cumulative wealth (in log scale) of trading strategies based on predictions from our adaptive algorithm ATOMS (black line), as well as the fixed-window baselines Fixed-valâ€‹(32)\texttt{Fixed-val}(32) (blue), Fixed-valâ€‹(128)\texttt{Fixed-val}(128) (orange), Fixed-valâ€‹(512)\texttt{Fixed-val}(512) (red), and Fixed-CV (gray dashed line). For most industries, our algorithm ATOMS consistently attains more cumulative wealth than the fixed-window baselines. The title in each subfigure is Kenneth Frenchâ€™s acronym for each industry. For the full names of these industries, please refer to Table [4](https://arxiv.org/html/2512.23596v1#A1.T4 "Table 4 â€£ A.2 Figures and tables â€£ Appendix A Additional details for empirical asset pricing â€£ The nonstationarity-complexity tradeoff in return prediction").

Our investment starts in January 1990 as the first month t=1t=1, and ends in November 2016. We generate wealth trajectories {Wtalg}t=1T\big\{W\_{t}^{\texttt{alg}}\big\}\_{t=1}^{T} for our algorithm and the fixed-window baselines algâˆˆ{ATOMS,Fixed-valâ€‹(â„“),Fixed-CV}\texttt{alg}\in\{\texttt{ATOMS},\,\texttt{Fixed-val}(\ell),\,\texttt{Fixed-CV}\}.

LABEL:sm:fig-industry-wealth-yearly depicts the evolution of the log cumulative wealth logâ¡Wtalg\log W\_{t}^{\texttt{alg}} for each method at the end of each year from 1990 to 2016. We observe that for most industries, the adaptive algorithm ATOMS consistently yields higher cumulative wealth than the fixed-window baselines, surpassing both the short and long validation windows. This superior performance highlights the algorithmâ€™s capacity to generate substantial excess returns by balancing the trade-off between non-stationarity and model complexity.

To further quantify the performance gain across the 1717 industries, we compute an Excess Ratio of WTATOMS/WTalgâˆ’1W\_{T}^{\texttt{ATOMS}}/W\_{T}^{\texttt{alg}}-1 against any baseline algâˆˆ{Fixed-valâ€‹(â„“),Fixed-CV}\texttt{alg}\in\{\texttt{Fixed-val}(\ell),\texttt{Fixed-CV}\}, and take its average across all 17 industries. This simple arithmetic average can be considered as an equal-weighted portfolio that invests 1/171/17th initial wealth in each of the industries. A value greater than zero indicates that our adaptive method ATOMS accumulates higher terminal wealth than the benchmark alg. In LABEL:sm:tab-cumwealth, we report the average Excess Ratio over the 1717 industries. The positive values of Excess Ratio indicate that ATOMS consistently generates superior wealth accumulation compared to the fixed-window benchmarks for the equal-weighted portfolio of industries. Compared with the best-performing benchmark of long horizon validation Fixed-valâ€‹(512)\texttt{Fixed-val}(512), our method yields 31% higher return for its investor by the end of our OOS period. With weaker benchmarked methods such as model picked by Fixed-CV that uses cross-validation, our method obtains 3.54 times more wealth over the investment horizon of 1990 to 2016.

Table 3: Average Cumulative Wealth Relative Excess of ATOMS over Baselines.

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
| Baseline Model | Fixed-valâ€‹(32)\texttt{Fixed-val}(32) | Fixed-valâ€‹(128)\texttt{Fixed-val}(128) | Fixed-valâ€‹(512)\texttt{Fixed-val}(512) | Fixed-CV |
| Excess Ratio | 3.383.38 | 0.480.48 | 0.310.31 | 3.543.54 |

This table reports the Excess Ratio. For each industry, we compute an Excess Ratio, then report average across the 17 computed industries. This metric can also be considered as the Excess Ratio if we were to invest in an equal-weighted portfolio that allocates evenly among all 17 industries in the initial period.

## 6 Conclusions

Our empirical results demonstrate the practical value of this framework across multiple dimensions. Most notably, during periods of heightened economic stress, our adaptive method ATOMS exhibits superior performance compared to fixed-window approaches. Our approach is well motivated by the documented facts in LABEL:sm:sec-tradeoff-empirics that, during recession periods including the 1990 Gulf War recession, the 2001 dot-com bubble burst and 9/11 attack, and the 2007-2009 Financial Crisis, simpler models trained on shorter windows consistently outperformed more complex models trained on longer windows. This empirical evidence validates our theory of the nonstationarity-complexity tradeoff.

The adaptive algorithmâ€™s performance during economic downturns is worth pointing out. As shown in LABEL:sm:tab:oos\_r2\_industry\_time, ATOMS achieves an out-of-sample R2R^{2} of 0.0270.027 during the brief but severe 1990 Gulf War recession, while the best fixed-window benchmark Fixed-valâ€‹(512)\texttt{Fixed-val}(512) produces a negative R2R^{2} of âˆ’0.031-0.031. During the 2001 recession, ATOMS attains an impressive R2R^{2} of 0.1250.125, outperforming Fixed-valâ€‹(512)\texttt{Fixed-val}(512) by 6.8% (0.117). Even during the prolonged Global Financial Crisis of 2007-2009, ATOMS maintains its advantage with an R2R^{2} of 0.0410.041 compared to Fixed-valâ€‹(512)\texttt{Fixed-val}(512)â€™s 0.0390.039. Beyond statistical performance metrics, the economic significance of our approach is demonstrated through trading strategy analysis. Averaged across the industries, our model yields 31% higher return than the best performing benchmark in the OOS period.

Several future directions are worth exploring. First, our adaptive model selection framework relies on the assumption that data is independent across time even though the distribution can change arbitrarily. While numerical experiments show that our method is robust against temporal dependence in real-world financial time series, it would be interesting and important to extend the framework in a principled way. Second, our framework of joint model and training window selection requires training a large number of candidate models, which can be computationally intensive. A valuable future direction is to reduce the these training costs. For example, a heuristic approach is to utilize the optimized parameters from previous periods as â€œwarm startsâ€ for subsequent training.

## Acknowledgement

Agostino Capponiâ€™s research was supported in part through grants of the Global Risk Institute and of Fi-Tek.
Chengpiao Huang and Kaizheng Wangâ€™s research is supported by National Science Foundation grants DMS-2210907 and DMS-2515679.

Appendix

## Appendix A Additional details for empirical asset pricing

In this section, we provide additional details for the numerical experiments in LABEL:sm:sec-experiments.

### A.1 Data

We aim to explain the cross-section of 1717 industry portfolio returns from Kenneth Frenchâ€™s website999<https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/Data_Library/det_17_ind_port.html>. We first describe our rich set of covariates, which combines daily and monthly sources to construct a comprehensive time series combining macroeconomic and cross-sectional signals. The final daily-frequency sample spans the time period from September 1987 to November 2016.

##### Common Factors.

To capture the underlying macroeconomic and systematic risk structure in our response variables, we include 15 common factors from the monthly dataset of CPZ24, who develop a deep learning framework for estimating the stochastic discount factor (SDF) consistent with the no-arbitrage condition.101010Their model integrates three neural network components: a feedforward network to approximate the nonlinear functional form of the SDF, a recurrent Long-Short Term Memory (LSTM) network to extract macroeconomic state variables, and a generative adversarial network (GAN) that constructs the most informative test assets for pricing. This architecture yields an empirically estimated SDF that represents the conditional mean-variance efficient portfolio of all U.S. equities.

The CPZ24 dataset provides three distinct elements relevant to our analysis:
(i) the estimated stochastic discount factor (SDF), capturing the aggregate price of risk implied by their no-arbitrage model;
(ii) the returns of ten equal-weighted decile portfolios sorted by firmsâ€™ exposure to the SDF (beta-sorted), which serve as cross-sectional test assets; and
(iii) four macroeconomic hidden state variables, derived from the GAN model, that summarize the joint dynamics of 178 macroeconomic time series into a small set of nonlinear factors reflecting business cycle conditions and systemic risk.111111These components jointly span the main drivers of systematic variation in expected returns. The SDF and beta-sorted deciles capture the cross-sectional risk-return trade-offs, while the macroeconomic hidden states track time variation in the pricing kernel associated with expansions, recessions, and crisis periods. The dataset covers the period 1967-2016; to align frequencies with our daily response data, each monthly observation is assigned to all trading days within the corresponding month.

In addition to these 15 factors, we incorporate the daily Fama-French three factors (FF3) from FFr93, which include the excess market return (MKT), the size factor (SMB), and the value factor (HML). These factors provide a benchmark set of linear risk exposures that have been shown to explain broad cross-sectional patterns in stock returns. Each factor is lagged appropriately to avoid look-ahead bias.

This unified set of common factors allows us to incorporate both the traditional linear risk structure captured by FF3 and the nonlinear, macroeconomically conditioned dynamics extracted from CPZ24.

##### Firm-Level Returns and Characteristics.

We augment the dataset with daily firm-level stock returns obtained from the CRSP database. Excess returns are computed relative to the daily risk-free rate from Kenneth Frenchâ€™s data library.121212To mitigate the impact of outliers that can distort cross-sectional averages, we Winsorize the daily cross-section of excess returns by removing the top and bottom 1% of firms within each day based on their excess return values. This procedure ensures that the resulting portfolio and factor constructions are not unduly influenced by extreme realizations. To characterize the cross-sectional heterogeneity in firm fundamentals and trading behavior, we incorporate the comprehensive panel of firm-level characteristics from GKX20. Their dataset provides 94 standardized monthly characteristics for U.S. equities, constructed to ensure comparability across firms and over time.

Specifically, GKX20 identify three dominant groups of predictive signals: (i) Price trend variables: measures of short- and long-term momentum, industry momentum, and short-term reversal that capture the persistence of returns and investor underreaction; (ii) Liquidity and size variables: market capitalization, trading volume, turnover, and bid-ask spread, which reflect trading frictions and the limits to arbitrage; (iii) Volatility and risk variables: total and idiosyncratic volatility, market beta, and higher-order terms such as beta squared, which proxy for systematic and residual risk exposure.131313Because the GKX20 characteristics are available only at a monthly frequency, we assign each monthly vector of characteristics to all trading days within that month for the corresponding firm. This alignment ensures consistency between the daily CRSP return data and the lower-frequency fundamental information.

#### A.1.1 Size Filter

We apply a size-based filter to focus the analysis on the subset of firms that are most representative of the small-cap universe. Specifically, within each day tt, we retain only the bottom 25% of firms by market equity.141414We measure market equity with the (monthly) â€œmvel1â€ size covariate from GKX20, this variable represents the firmâ€™s market capitalization at the end of month mm, standardized for comparability across firms and time.

Formally, let Si,mS\_{i,m} denote the size of firm ii in month mm. For all trading days tt belonging to month mm, we retain only those firms satisfying

|  |  |  |
| --- | --- | --- |
|  | Si,mâ‰¤Q0.25â€‹({Sj,m}jâˆˆâ„±m),S\_{i,m}\leq Q\_{0.25}\big(\{S\_{j,m}\}\_{j\in\mathcal{F}\_{m}}\big), |  |

where Q0.25â€‹(â‹…)Q\_{0.25}(\cdot) denotes the 25th percentile of the cross-sectional size distribution, and â„±m\mathcal{F}\_{m} is the set of all firms available in month mm. Restricting the sample in this way allows us to study the predictive role of firm characteristics and macroeconomic factors within a homogeneous segment of the equity market, mitigating heterogeneity arising from scale effects in firm size.

#### A.1.2 Long-Short Decile Portfolios from Characteristics

To transform the firm-level characteristics into tradable, interpretable covariates, we construct daily long-short portfolios sorted on each of the GKX20 characteristics. For each of the 94 characteristics, denoted by câˆˆ{1,â€¦,94}c\in\{1,\dots,94\}, and for each day tt, we sort firms into ten equal-weighted portfolios based on the value of characteristic cc.

Formally, let Xi,c,mX\_{i,c,m} denote the value of characteristic cc for firm ii in month mm, which is assigned to all trading days within that month, and let Ri,texR\_{i,t}^{\text{ex}} denote the daily excess return of firm ii on day tt. We compute monthly breakpoints for characteristic cc using cross-sectional quantiles:

|  |  |  |
| --- | --- | --- |
|  | bc,k,m=Qk/10â€‹({Xi,c,m}iâˆˆâ„±t),k=1,â€¦,10,b\_{c,k,m}=Q\_{k/10}\big(\{X\_{i,c,m}\}\_{i\in\mathcal{F}\_{t}}\big),\quad k=1,\dots,10, |  |

where Qpâ€‹(â‹…)Q\_{p}(\cdot) denotes the pp-th empirical quantile, and â„±t\mathcal{F}\_{t} is the set of all firms observed in day tt.

We then compute the daily return of the kk-th decile portfolio for characteristic cc as:

|  |  |  |
| --- | --- | --- |
|  | Dc,k,t=1Nc,k,tâ€‹âˆ‘i:bc,kâˆ’1,m<Xi,c,mâ‰¤bc,k,mRi,tex,D\_{c,k,t}=\frac{1}{N\_{c,k,t}}\sum\_{i:\,b\_{c,k-1,m}<X\_{i,c,m}\leq b\_{c,k,m}}R\_{i,t}^{\text{ex}}, |  |

where Nc,k,tN\_{c,k,t} is the number of firms assigned to the kk-th decile at time tt. Each decile is equal-weighted to ensure that portfolio performance reflects cross-sectional variation in firm characteristics rather than differences in market capitalization.

The corresponding long-short portfolio return for characteristic cc is defined as:

|  |  |  |
| --- | --- | --- |
|  | LSc,t=Dc,10,tâˆ’Dc,1,t,\text{LS}\_{c,t}=D\_{c,10,t}-D\_{c,1,t}, |  |

representing the daily return to a strategy that is long the highest-decile firms (those with the largest values of Xi,c,mX\_{i,c,m}) and short the lowest-decile firms (those with the smallest values of Xi,c,mX\_{i,c,m}).

This construction yields a balanced set of long-short factor returns that isolate the pricing implications of each firm characteristic. Repeating this procedure for all câˆˆ{1,â€¦,94}c\in\{1,\dots,94\} produces a time series matrix of 94 characteristic-sorted long-short portfolio returns at the daily frequency.

#### A.1.3 Final Covariate Set

Our final set of predictors integrates macroeconomic, factor-based, and cross-sectional sources of variation to form a unified time series of covariates for forecasting the Russell 2000 index and the French 17 industry portfolio returns. The resulting dataset combines information from three complementary dimensions of the asset pricing literature:

1. 1.

   Macroeconomic and systematic factors: the 10 equal-weighted beta-sorted decile portfolios, the 4 macroeconomic hidden states, and the estimated stochastic discount factor (SDF) from CPZ24.
2. 2.

   Benchmark risk factors: the daily Fama-French 3 factors (FF3) from FFr93, consisting of the market excess return (MKT), the size factor (SMB), and the value factor (HML).
3. 3.

   Cross-sectional characteristic factors: the 94 daily long-short characteristic-sorted portfolio returns, from the GKX20 characteristics.
4. 4.

   Lagging features: we augment the feature set with one-day lagged returns. For the prediction of the Russell 2000 index return, we use the indexâ€™s own lag. For the prediction of the 1717 industry portfolio returns, we use the full vector of the 1717 industry portfolio lagged returns.

By combining the first three components, our dataset links the macroeconomic and cross-sectional perspectives on asset pricing. The CPZ24 factors embed the nonlinear, time-varying structure of the stochastic discount factor, while the GKX20 characteristic-sorted long-short portfolios summarize the cross-sectional distribution of risk premia across firms. Incorporating the Fama-French 3 factors provides a benchmark for evaluating whether these modern, high-dimensional representations offer predictive power beyond the traditional linear framework.

### A.2 Figures and tables

Table 4: Name Mapping for the 17 Industries

|  |  |
| --- | --- |
| Industry Acronym | Full Industry Name |
| Food | Food |
| Mines | Mining and Minerals |
| Oil | Oil and Petroleum Products |
| Clths | Textiles, Apparel & Footwear |
| Durbl | Consumer Durables |
| Chems | Chemicals |
| Cnsum | Drugs, Soap, Perfumes, Tobacco |
| Cnstr | Construction and Construction Materials |
| Steel | Steel Works Etc |
| FabPr | Fabricated Products |
| Machn | Machinery and Business Equipment |
| Cars | Automobiles |
| Trans | Transportation |
| Utils | Utilities |
| Rtail | Retail Stores |
| Finan | Banks, Insurance Companies, and Other Financials |
| Other | Other |

### A.3 Machine learning model implementation details

In this appendix, we provide detailed specifications for the hyperparameter tuning procedures and training configurations used in our empirical analysis. These technical details complement the main text by offering comprehensive information about the model selection process and computational implementation.

#### A.3.1 Model Hyperparameter Grids

For each model class considered in our analysis, we systematically explore a comprehensive grid of hyperparameter values for model selection. The hyperparameter grids are designed to balance computational efficiency with thorough exploration of the model space.

##### Linear Models with Regularization.

For the ridge regression (Ridge), LASSO (LASSO), and elastic net (E-Net) models, we consider the following hyperparameter specifications:

1. 1.

   For ridge regression, we consider values of the regularization parameter Î±\alpha on a logarithmic scale:

   |  |  |  |
   | --- | --- | --- |
   |  | Î±âˆˆ{10âˆ’3,10âˆ’1.5,1,101.5,103}.\alpha\in\{10^{-3},10^{-1.5},1,10^{1.5},10^{3}\}. |  |

   This range allows for both strong regularization (small Î±\alpha) and weak regularization (large Î±\alpha), accommodating different levels of multicollinearity in our high-dimensional covariate space.
2. 2.

   For LASSO, we consider values of the regularization parameter Î±\alpha on a lagoarithmic scale:

   |  |  |  |
   | --- | --- | --- |
   |  | Î±âˆˆ{10âˆ’5,10âˆ’3.5,10âˆ’2,10âˆ’0.5,10}.\alpha\in\{10^{-5},10^{-3.5},10^{-2},10^{-0.5},10\}. |  |

   The â„“1\ell\_{1} penalty in LASSO facilitates feature selection, which is particularly valuable given our large set of covariates.
3. 3.

   For the elastic net, we consider the following combinations of the regularization parameter Î±\alpha and the mixing parameter rr:

   |  |  |  |
   | --- | --- | --- |
   |  | Î±âˆˆ{10âˆ’3,1,103},râˆˆ{0.01,0.05,0.1}.\alpha\in\{10^{-3},1,10^{3}\},\qquad r\in\{0.01,0.05,0.1\}. |  |

   This grid explores the balance between feature selection and coefficient shrinkage.

##### Random Forest.

For the random forest models, we consider the following combinations of the number of trees ntreen\_{\texttt{tree}} and the maximum tree depth dmaxd\_{\max}:

|  |  |  |
| --- | --- | --- |
|  | ntreeâˆˆ{10,100,200},dmaxâˆˆ{3,5,10}.n\_{\texttt{tree}}\in\{10,100,200\},\qquad d\_{\max}\in\{3,5,10\}. |  |

Increasing the number of trees generally improves model stability and reduces variance, though with diminishing returns beyond a certain point. Trees with shallower depths provide stronger regularization, while deeper trees can capture more complex nonlinear relationships.

#### A.3.2 Training Window Configurations

To assess the performance of our adaptive model selection algorithm across different data regimes, we train models on estimation windows of varying lengths. For each month tt, we consider training windows of 4kâˆ§(tâˆ’1)4^{k}\wedge(t-1) months with 0â‰¤kâ‰¤50\leq k\leq 5. In particular, since tâˆˆ{1,â€¦,350}t\in\{1,...,350\}, then k=5k=5 corresponds to a full training window of (tâˆ’1)(t-1) months. This yields the following window lengths:

Table 5: Training Window Lengths by Value of kk

|  |  |  |
| --- | --- | --- |
| kk | Window Length (months) | Approximate Years |
| 0 | 1 | 0.08 |
| 1 | 4 | 0.33 |
| 2 | 16 | 1.33 |
| 3 | 64 | 5.33 |
| 4 | 256 | 21.33 |
| 5 | tâˆ’1t-1 | (tâˆ’1)/12(t-1)/12 |

This exponential scaling allows us to examine how model performance varies with the amount of historical data available for training. Shorter windows capture recent market dynamics but may be susceptible to noise, while longer windows provide more stable parameter estimates but may miss structural changes in the data-generating process.

#### A.3.3 Hyperparameter for ATOMS

In our implementation of ATOMS, we set Î´â€²=0.1\delta^{\prime}=0.1 and M2=5Ã—10âˆ’4M^{2}=5\times 10^{-4}.

#### A.3.4 Computational Implementation

All models are implemented using Python 3.9 with the following libraries:

* â€¢

  Linear models: scikit-learn version 1.0.2, specifically Ridge, Lasso, and ElasticNet.
* â€¢

  Random forest: scikit-learnâ€™s RandomForestRegressor with random\_state=0.
* â€¢

  Data manipulation: pandas version 1.4.2 and numpy version 1.21.5.

## Appendix B Extension: Model Selection with the R2R^{2} Metric

In this section, we propose a variant of our model selection method that is tailored to the R2R^{2} metric. We consider the following population form:

|  |  |  |  |
| --- | --- | --- | --- |
|  | R~t2â€‹(f)=1âˆ’ğ”¼(ğ’™,y)âˆ¼Ptâ€‹[(fâ€‹(ğ’™)âˆ’y)2]ğ”¼(ğ’™,y)âˆ¼Ptâ€‹[y2],\widetilde{R}^{2}\_{t}(f)=1-\frac{\mathbb{E}\_{(\bm{x},y)\sim P\_{t}}\left[\left(f(\bm{x})-y\right)^{2}\right]}{\mathbb{E}\_{(\bm{x},y)\sim P\_{t}}[y^{2}]}, |  | (B.1) |

Define Vt=ğ”¼(ğ’™,y)âˆ¼Ptâ€‹[y2]V\_{t}=\mathbb{E}\_{(\bm{x},y)\sim P\_{t}}[y^{2}], then R~t2â€‹(f)=1âˆ’Ltâ€‹(f)/Vt\widetilde{R}^{2}\_{t}(f)=1-L\_{t}(f)/V\_{t}. For simplicity, we assume that at the beginning of each period tâˆˆâ„¤+t\in\mathbb{Z}\_{+}, we have access to {Vj}j=1tâˆ’1\{V\_{j}\}\_{j=1}^{t-1}. In our numerical experiments, we will approximate VjV\_{j} by its empirical counterpart computed from the validation data ğ’Ÿjva\mathcal{D}\_{j}^{\operatorname{\mathrm{va}}}. For the population R2R^{2} metric to be well defined, we assume for simplicity that {Vt}t=1âˆ\{V\_{t}\}\_{t=1}^{\infty} are bounded away from zero.

###### Assumption B.1 (Uniformly lower bounded second moments).

There exists v>0v>0 such that Vtâ‰¥vV\_{t}\geq v for all tâˆˆâ„¤+t\in\mathbb{Z}\_{+}.

We first define the model comparison subroutine, which aims to output the better of two given candidate models f1f\_{1} and f2f\_{2}. Define the R2R^{2} performance gap

|  |  |  |
| --- | --- | --- |
|  | Î”tR=R~t2â€‹(f2)âˆ’R~t2â€‹(f1)=Ltâ€‹(f1)âˆ’Ltâ€‹(f2)Vt.\Delta^{R}\_{t}=\widetilde{R}^{2}\_{t}(f\_{2})-\widetilde{R}^{2}\_{t}(f\_{1})=\frac{L\_{t}(f\_{1})-L\_{t}(f\_{2})}{V\_{t}}. |  |

Then f2f\_{2} outperforms f1f\_{1} if and only if Î”tR>0\Delta^{R}\_{t}>0. For each window size â„“âˆˆ[tâˆ’1]\ell\in[t-1], we can form a rolling-window estimator of Î”tR\Delta^{R}\_{t}, given by

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î”^t,â„“R=Î”^t,â„“Vt,â„“\widehat{\Delta}^{R}\_{t,\ell}=\frac{\widehat{\Delta}\_{t,\ell}}{V\_{t,\ell}} |  | (B.2) |

where Î”^t,â„“\widehat{\Delta}\_{t,\ell} is defined by ([4.1](https://arxiv.org/html/2512.23596v1#S4.E1 "Equation 4.1 â€£ Pairwise comparison subroutine. â€£ 4.1 Adaptive Tournament Model Selection â€£ 4 Adaptive Model and Data Selection under Non-Stationarity â€£ The nonstationarity-complexity tradeoff in return prediction")), and Vt,â„“=1nt,â„“â€‹âˆ‘j=tâˆ’ktâˆ’1njâ€‹VjV\_{t,\ell}=\frac{1}{n\_{t,\ell}}\sum\_{j=t-k}^{t-1}n\_{j}V\_{j}. We establish a bias-variance decomposition for the estimation error of Î”^t,â„“R\widehat{\Delta}^{R}\_{t,\ell}.

###### Lemma B.1 (Bias-variance decomposition).

Let Assumptions [2.1](https://arxiv.org/html/2512.23596v1#S2.Thmassumption1 "Assumption 2.1 (Independent data). â€£ 2 Problem Setup â€£ The nonstationarity-complexity tradeoff in return prediction"), [3.1](https://arxiv.org/html/2512.23596v1#S3.Thmassumption1 "Assumption 3.1 (Boundedness). â€£ 3.2 Theoretical Characterization â€£ 3 The Nonstationarity-Complexity Tradeoff â€£ The nonstationarity-complexity tradeoff in return prediction") and [B.1](https://arxiv.org/html/2512.23596v1#A2.Thmassumption1 "Assumption B.1 (Uniformly lower bounded second moments). â€£ Appendix B Extension: Model Selection with the ğ‘…Â² Metric â€£ The nonstationarity-complexity tradeoff in return prediction") hold. Let Ïƒt,â„“R=Ïƒt,â„“/Vt,â„“\sigma^{R}\_{t,\ell}=\sigma\_{t,\ell}/V\_{t,\ell}, where Ïƒt,â„“\sigma\_{t,\ell} is defined by ([4.4](https://arxiv.org/html/2512.23596v1#S4.E4 "Equation 4.4 â€£ Pairwise comparison subroutine. â€£ 4.1 Adaptive Tournament Model Selection â€£ 4 Adaptive Model and Data Selection under Non-Stationarity â€£ The nonstationarity-complexity tradeoff in return prediction")). For Î´âˆˆ(0,1)\delta\in(0,1), define

|  |  |  |
| --- | --- | --- |
|  | Ï•Râ€‹(t,â„“)=maxtâˆ’â„“â‰¤jâ‰¤tâˆ’1â¡|Î”jRâˆ’Î”tR|,\displaystyle\phi\_{R}(t,\ell)=\max\_{t-\ell\leq j\leq t-1}\big|\Delta^{R}\_{j}-\Delta^{R}\_{t}\big|, |  |
|  |  |  |
| --- | --- | --- |
|  | ÏˆRâ€‹(t,â„“,Î´)={8â€‹M2/v,ifÂ â€‹nt,â„“=1Ïƒt,â„“Râ€‹2â€‹logâ¡(2/Î´)nt,â„“+16â€‹(M2/v)â€‹logâ¡(2/Î´)3â€‹nt,â„“,ifÂ â€‹nt,â„“â‰¥2.\displaystyle\psi\_{R}(t,\ell,\delta)=\begin{cases}8M^{2}/v,&\ \text{if }n\_{t,\ell}=1\\[6.0pt] \displaystyle\sigma^{R}\_{t,\ell}\sqrt{\frac{2\log(2/\delta)}{n\_{t,\ell}}}+\frac{16(M^{2}/v)\log(2/\delta)}{3n\_{t,\ell}},&\ \text{if }n\_{t,\ell}\geq 2\end{cases}. |  |

With probability at least 1âˆ’Î´1-\delta,

|  |  |  |
| --- | --- | --- |
|  | |Î”^t,â„“Râˆ’Î”tR|â‰¤Ï•Râ€‹(t,â„“)+ÏˆRâ€‹(t,â„“,Î´).\big|\widehat{\Delta}^{R}\_{t,\ell}-\Delta^{R}\_{t}\big|\leq\phi\_{R}(t,\ell)+\psi\_{R}(t,\ell,\delta). |  |

###### Proof.

See LABEL:sm:sec-lem-bias-variance-decomp-R-proof.
âˆ

Based on LABEL:sm:lem-bias-variance-decomp-R and following the same idea as ([4.4](https://arxiv.org/html/2512.23596v1#S4.E4 "Equation 4.4 â€£ Pairwise comparison subroutine. â€£ 4.1 Adaptive Tournament Model Selection â€£ 4 Adaptive Model and Data Selection under Non-Stationarity â€£ The nonstationarity-complexity tradeoff in return prediction")), we form a data-driven proxy for ÏˆRâ€‹(t,â„“,Î´)\psi\_{R}(t,\ell,\delta), given by

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ïˆ^Râ€‹(t,â„“,Î´)={8â€‹M2/v,ifÂ â€‹nt,â„“=1v^t,â„“Râ€‹2â€‹logâ¡(2/Î´)nt,â„“+64â€‹(M2/v)â€‹logâ¡(2/Î´)3â€‹(nt,â„“âˆ’1),ifÂ â€‹nt,â„“â‰¥2,\widehat{\psi}\_{R}(t,\ell,\delta)=\begin{cases}8M^{2}/v,&\ \text{if }n\_{t,\ell}=1\\[6.0pt] \displaystyle\widehat{v}^{R}\_{t,\ell}\sqrt{\frac{2\log(2/\delta)}{n\_{t,\ell}}}+\frac{64(M^{2}/v)\log(2/\delta)}{3(n\_{t,\ell}-1)},&\ \text{if }n\_{t,\ell}\geq 2\end{cases}, |  | (B.3) |

where

|  |  |  |  |
| --- | --- | --- | --- |
|  | v^t,â„“R=v^t,â„“Vt,â„“,\widehat{v}^{R}\_{t,\ell}=\frac{\widehat{v}\_{t,\ell}}{V\_{t,\ell}}, |  | (B.4) |

and v^t,â„“\widehat{v}\_{t,\ell} is defined in ([4.3](https://arxiv.org/html/2512.23596v1#S4.E3 "Equation 4.3 â€£ Pairwise comparison subroutine. â€£ 4.1 Adaptive Tournament Model Selection â€£ 4 Adaptive Model and Data Selection under Non-Stationarity â€£ The nonstationarity-complexity tradeoff in return prediction")). Following the same idea as ([4.5](https://arxiv.org/html/2512.23596v1#S4.E5 "Equation 4.5 â€£ Pairwise comparison subroutine. â€£ 4.1 Adaptive Tournament Model Selection â€£ 4 Adaptive Model and Data Selection under Non-Stationarity â€£ The nonstationarity-complexity tradeoff in return prediction")), we also form a data-driven proxy for Ï•Râ€‹(t,â„“,Î´)\phi\_{R}(t,\ell,\delta):

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ï•^R(t,â„“,Î´)=maxiâˆˆ[â„“](|Î”^t,â„“Râˆ’Î”^t,iR|âˆ’[Ïˆ^R(t,â„“,Î´)+Ïˆ^R(t,i,Î´)])+.\widehat{\phi}\_{R}(t,\ell,\delta)=\max\_{i\in[\ell]}\bigg(\big|\widehat{\Delta}^{R}\_{t,\ell}-\widehat{\Delta}^{R}\_{t,i}\big|-\big[\widehat{\psi}\_{R}\left(t,\ell,\delta\right)+\widehat{\psi}\_{R}\left(t,i,\delta\right)\big]\bigg)\_{+}. |  | (B.5) |

This yields LABEL:sm:alg-compare-R as the model comparison routine.

Input: Candidate models {f1,f2}\{f\_{1},f\_{2}\}, validation data {ğ’Ÿjva}j=1tâˆ’1\{\mathcal{D}^{\operatorname{\mathrm{va}}}\_{j}\}\_{j=1}^{t-1}, variances {Vj}j=1tâˆ’1\{V\_{j}\}\_{j=1}^{t-1}, hyperparameters Î´â€²\delta^{\prime}, MM, vv.

for â„“=1,â‹¯,tâˆ’1\ell=1,\cdots,t-1

Compute Î”^t,â„“R\widehat{\Delta}^{R}\_{t,\ell}, v^t,â„“R\widehat{v}^{R}\_{t,\ell}, Ïˆ^Râ€‹(t,â„“,Î´â€²)\widehat{\psi}\_{R}(t,\ell,\delta^{\prime}) and Ï•^Râ€‹(t,â„“,Î´â€²)\widehat{\phi}\_{R}(t,\ell,\delta^{\prime}) according to ([B.2](https://arxiv.org/html/2512.23596v1#A2.E2 "Equation B.2 â€£ Appendix B Extension: Model Selection with the ğ‘…Â² Metric â€£ The nonstationarity-complexity tradeoff in return prediction")), ([B.4](https://arxiv.org/html/2512.23596v1#A2.E4 "Equation B.4 â€£ Appendix B Extension: Model Selection with the ğ‘…Â² Metric â€£ The nonstationarity-complexity tradeoff in return prediction")), ([B.3](https://arxiv.org/html/2512.23596v1#A2.E3 "Equation B.3 â€£ Appendix B Extension: Model Selection with the ğ‘…Â² Metric â€£ The nonstationarity-complexity tradeoff in return prediction")) and ([B.5](https://arxiv.org/html/2512.23596v1#A2.E5 "Equation B.5 â€£ Appendix B Extension: Model Selection with the ğ‘…Â² Metric â€£ The nonstationarity-complexity tradeoff in return prediction")).

Choose window size

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„“^âˆˆargminâ„“âˆˆ[tâˆ’1]{Ï•^Râ€‹(t,â„“,Î´â€²)+Ïˆ^Râ€‹(t,â„“,Î´â€²)}.\widehat{\ell}\in\mathop{\mathrm{argmin}}\_{\ell\in[t-1]}\Big\{\widehat{\phi}\_{R}(t,\ell,\delta^{\prime})+\widehat{\psi}\_{R}(t,\ell,\delta^{\prime})\Big\}. |  | (B.6) |

Select Î»^=1\widehat{\lambda}=1 if Î”^t,â„“^Râ‰¤0\widehat{\Delta}^{R}\_{t,\widehat{\ell}}\leq 0, and Î»^=2\widehat{\lambda}=2 otherwise.

return f^=fÎ»^\widehat{f}=f\_{\widehat{\lambda}}.

Algorithm 4  Adaptive Rolling Window for Model Comparison (R2R^{2} Metric)

By using LABEL:sm:alg-compare-R as the pairwise comparison subroutine ğ’œ\mathcal{A} in LABEL:sm:alg-tournament, we obtain an R2R^{2}-based model selection algorithm, which we call ATOMS-R2. We establish the following guarantee in terms of the R2R^{2} metric.

###### Theorem B.1 (Near-optimal model selection with R2R^{2}).

Let Assumptions [2.1](https://arxiv.org/html/2512.23596v1#S2.Thmassumption1 "Assumption 2.1 (Independent data). â€£ 2 Problem Setup â€£ The nonstationarity-complexity tradeoff in return prediction") and [3.1](https://arxiv.org/html/2512.23596v1#S3.Thmassumption1 "Assumption 3.1 (Boundedness). â€£ 3.2 Theoretical Characterization â€£ 3 The Nonstationarity-Complexity Tradeoff â€£ The nonstationarity-complexity tradeoff in return prediction") hold. Choose Î´âˆˆ(0,1)\delta\in(0,1) and set Î´â€²=1/(3â€‹Î›2â€‹t)\delta^{\prime}=1/(3\Lambda^{2}t) in LABEL:sm:alg-tournament. With probability at least 1âˆ’Î´1-\delta, the output f^\widehat{f} of ATOMS-R2 satisfies

|  |  |  |
| --- | --- | --- |
|  | maxÎ»âˆˆ[Î›]â¡R~t2â€‹(fÎ»)âˆ’R~t2â€‹(f^)â‰²logâ¡(Î›â€‹t/Î´)â‹…minâ„“âˆˆ[tâˆ’1]â¡{maxtâˆ’â„“â‰¤jâ‰¤tâˆ’1â¡maxÎ»âˆˆ[Î›]â¡|R~j2â€‹(fÎ»)âˆ’R~t2â€‹(fÎ»)|+M2/vnt,â„“}.\max\_{\lambda\in[\Lambda]}\widetilde{R}^{2}\_{t}(f\_{\lambda})-\widetilde{R}^{2}\_{t}(\widehat{f})\lesssim\log(\Lambda t/\delta)\cdot\min\_{\ell\in[t-1]}\left\{\max\_{t-\ell\leq j\leq t-1}\max\_{\lambda\in[\Lambda]}\big|\widetilde{R}^{2}\_{j}(f\_{\lambda})-\widetilde{R}^{2}\_{t}(f\_{\lambda})\big|+\frac{M^{2}/v}{\sqrt{n\_{t,\ell}}}\right\}. |  |

Here â‰²\lesssim hides a universal constant.

###### Proof of LABEL:sm:thm-select-tournament-R.

See LABEL:sm:sec-thm-select-tournament-R-proof.
âˆ

LABEL:sm:thm-select-tournament-R shares a similar interpretation as LABEL:sm:thm-select-tournament. The term

|  |  |  |
| --- | --- | --- |
|  | maxtâˆ’â„“â‰¤jâ‰¤tâˆ’1â¡maxÎ»âˆˆ[Î›]â¡|R~j2â€‹(fÎ»)âˆ’R~t2â€‹(fÎ»)|\max\_{t-\ell\leq j\leq t-1}\max\_{\lambda\in[\Lambda]}\big|\widetilde{R}^{2}\_{j}(f\_{\lambda})-\widetilde{R}^{2}\_{t}(f\_{\lambda})\big| |  |

quantifies the shift in the R2R^{2} metric of the models within the last â„“\ell periods, and (M2/v)/nt,â„“(M^{2}/v)/\sqrt{n\_{t,\ell}} represents the statistical uncertainty associated with the nt,â„“n\_{t,\ell} validation data points. Together, they represent the errors that arise when using a fixed validation window â„“\ell to select models. LABEL:sm:thm-select-tournament-R shows that our R2R^{2}-based model selection algorithm ATOMS-R2 is comparable to an oracle that uses the optimal validation window in hindsight to attain the highest R2R^{2}.

In LABEL:sm:sec-experiments-adaptive-R2, we present numerical experiment results for ATOMS-R2, which are similar to those for ATOMS.

## Appendix C Theoretical Analysis of Nonstationarity-Complexity Tradeoff

In this section, we provide more details on LABEL:sm:thm-tradeoff and its proof.

We first set up some mathematical notation. Recall that ftâˆ—â€‹(â‹…)=ğ”¼(ğ’™,y)âˆ¼Ptâ€‹[yâˆ£ğ’™=â‹…]f\_{t}^{\*}(\cdot)=\mathbb{E}\_{(\bm{x},y)\sim P\_{t}}[y\mid\bm{x}=\cdot] is the Bayes optimal least squares estimator, which minimizes Ltâ€‹(f)L\_{t}(f) over all measurable f:ğ’³â†’â„f:\mathcal{X}\to\mathbb{R}. Let

|  |  |  |
| --- | --- | --- |
|  | fÂ¯t=argminfâˆˆâ„±Ltâ€‹(f),\bar{f}\_{t}=\mathop{\mathrm{argmin}}\_{f\in\mathcal{F}}L\_{t}(f), |  |

which minimizes Ltâ€‹(f)L\_{t}(f) over all fâˆˆâ„±f\in\mathcal{F}. For each tt, let Pt,ğ’™P\_{t,\bm{x}} be the marginal distribution of PtP\_{t} with respect to the covariates ğ’™\bm{x}. The distribution Pt,ğ’™P\_{t,\bm{x}} induces a norm âˆ¥â‹…âˆ¥t\|\cdot\|\_{t}, given by

|  |  |  |
| --- | --- | --- |
|  | â€–fâ€–t=ğ”¼ğ’™âˆ¼Pt,ğ’™â€‹[fâ€‹(ğ’™)2].\|f\|\_{t}=\sqrt{\mathbb{E}\_{\bm{x}\sim P\_{t,\bm{x}}}\left[f(\bm{x})^{2}\right]}. |  |

It can be shown that â„°tâ€‹(f)=â€–fâˆ’ftâˆ—â€–t2\mathcal{E}\_{t}(f)=\|f-f\_{t}^{\*}\|\_{t}^{2} for all f:ğ’³â†’â„f:\mathcal{X}\to\mathbb{R}.

### C.1 A Non-Stationary Local Rademacher complexity

We now formally define rt,kâ€‹(â„±)r\_{t,k}(\mathcal{F}) through a non-stationary version of the *local Rademacher complexity* (BBM05). We first define the *Rademacher complexity*, which reflects the richness of a function class with respect to certain data samples.

###### Definition C.1 (Rademacher complexity).

Let {ğ³i}i=1n\{\bm{z}\_{i}\}\_{i=1}^{n} be independent random variables in ğ’³\mathcal{X}. Let ğ’¢\mathcal{G} be a class of functions from ğ’³\mathcal{X} to â„\mathbb{R}. The Rademacher complexity of ğ’¢\mathcal{G} associated with {ğ³i}i=1n\{\bm{z}\_{i}\}\_{i=1}^{n} is defined by

|  |  |  |
| --- | --- | --- |
|  | â„œâ€‹(ğ’¢;{ğ’›i}i=1n)=ğ”¼â€‹[supgâˆˆğ’¢1nâ€‹âˆ‘i=1nÎµiâ€‹gâ€‹(ğ’›i)],\mathfrak{R}(\mathcal{G};\{\bm{z}\_{i}\}\_{i=1}^{n})=\mathbb{E}\left[\sup\_{g\in\mathcal{G}}\frac{1}{n}\sum\_{i=1}^{n}\varepsilon\_{i}g(\bm{z}\_{i})\right], |  |

where Îµ1,â€¦,Îµn\varepsilon\_{1},...,\varepsilon\_{n} are i.i.d.Â random variables following the Rademacher distribution â„™â€‹(Îµ1=1)=â„™â€‹(Îµ1=âˆ’1)=1/2\mathbb{P}(\varepsilon\_{1}=1)=\mathbb{P}(\varepsilon\_{1}=-1)=1/2, and are independent of {zi}i=1n\{z\_{i}\}\_{i=1}^{n}.

The local Rademacher complexity is the Rademacher complexity of some local function class centered at fÂ¯t\bar{f}\_{t}.

###### Definition C.2 (Local function class).

For every râ‰¥0r\geq 0, define the local function class

|  |  |  |
| --- | --- | --- |
|  | â„±t,kâ€‹(r)={fâˆˆâ„±:1mt,kâ€‹âˆ‘j=tâˆ’ktâˆ’1mjâ€‹â€–fâˆ’fÂ¯tâ€–j2â‰¤r}.\mathcal{F}\_{t,k}(r)=\bigg\{f\in\mathcal{F}:\frac{1}{m\_{t,k}}\sum\_{j=t-k}^{t-1}m\_{j}\left\|f-\bar{f}\_{t}\right\|\_{j}^{2}\leq r\bigg\}. |  |

In words, â„±t,kâ€‹(r)\mathcal{F}\_{t,k}(r) consists of functions fâˆˆâ„±f\in\mathcal{F} that are close to fÂ¯t\bar{f}\_{t} with respect to the distributions {Pj,ğ’™}j=tâˆ’ktâˆ’1\{P\_{j,\bm{x}}\}\_{j=t-k}^{t-1} on average. We are now ready to define the quantity rt,kâ€‹(â„±)r\_{t,k}(\mathcal{F}), which is more formally known as the *critical radius* of the function

|  |  |  |
| --- | --- | --- |
|  | â„œt,kâ€‹(r;â„±)=â„œâ€‹(â„±t,kâ€‹(r);ğ’Ÿt,ktr).\mathfrak{R}\_{t,k}(r;\mathcal{F})=\mathfrak{R}\left(\mathcal{F}\_{t,k}(r);\mathcal{D}\_{t,k}^{\operatorname{\mathrm{tr}}}\right). |  |

###### Definition C.3 (Subroot function).

A function Ïˆ:â„+â†’â„+\psi:\mathbb{R}\_{+}\to\mathbb{R}\_{+} is subroot if it is increasing and râ†¦Ïˆâ€‹(r)/rr\mapsto\psi(r)/\sqrt{r} is decreasing on (0,âˆ)(0,\infty).

###### Definition C.4 (Critical radius).

The critical radius of â„œt,kâ€‹(r;â„±)\mathfrak{R}\_{t,k}(r;\mathcal{F}) is defined by

|  |  |  |
| --- | --- | --- |
|  | rt,kâ€‹(â„±)=inf{râ‰¥0:âˆƒsubrootÂ â€‹Ïˆâ€‹Â such thatÂ â€‹Ïˆâ€‹(r)=r,Â andÂ â€‹Ïˆâ€‹(s)â‰¥â„œt,kâ€‹(s;â„±)â€‹âˆ€sâ‰¥r}.r\_{t,k}(\mathcal{F})=\inf\left\{r\geq 0:\exists\,\text{subroot }\psi\text{ such that }\psi(r)=r,\text{ and }\psi(s)\geq\mathfrak{R}\_{t,k}(s;\mathcal{F})\ \forall s\geq r\right\}. |  |

### C.2 Proof of LABEL:sm:thm-tradeoff

We now prove LABEL:sm:thm-tradeoff. In the proof, we will write f^=f^t,k\widehat{f}=\widehat{f}\_{t,k} to emphasize its dependence on the time tt and the training window size kk. The key of the proof is the following lemma, which is a non-stationary version of Theorem 3.3 in BBM05.

###### Lemma C.1 (Localized uniform concentration).

Let ğ³1,â€¦,ğ³n\bm{z}\_{1},...,\bm{z}\_{n} be independent random variables taking values in a space ğ’µâŠ†â„d+1\mathcal{Z}\subseteq\mathbb{R}^{d+1}. Let ğ’¢\mathcal{G} be a collection of functions from ğ’µ\mathcal{Z} to [a,b][a,b]. Suppose that there exist T:ğ’¢â†’â„+T:\mathcal{G}\to\mathbb{R}\_{+} and C,Î·1,â€¦,Î·nâ‰¥0C,\eta\_{1},...,\eta\_{n}\geq 0 such that the following noise condition holds:

|  |  |  |  |
| --- | --- | --- | --- |
|  | 1nâ€‹âˆ‘i=1nvarâ¡[gâ€‹(ğ’›i)]â‰¤Tâ€‹(g)â‰¤Câ‹…1nâ€‹âˆ‘i=1n(ğ”¼â€‹[gâ€‹(ğ’›i)]+Î·i),âˆ€gâˆˆğ’¢.\frac{1}{n}\sum\_{i=1}^{n}\operatorname{{\rm var}}\left[g(\bm{z}\_{i})\right]\leq T(g)\leq C\cdot\frac{1}{n}\sum\_{i=1}^{n}\big(\mathbb{E}\left[g(\bm{z}\_{i})\right]+\eta\_{i}\big),\qquad\forall g\in\mathcal{G}. |  | (C.1) |

Let Ïˆ\psi be a sub-root function with a fixed point râˆ—r^{\*} satisfying

|  |  |  |
| --- | --- | --- |
|  | Ïˆâ€‹(r)â‰¥Câ‹…â„œâ€‹({gâˆˆğ’¢:Tâ€‹(g)â‰¤r};{ğ’›i}i=1n),âˆ€râ‰¥râˆ—.\psi(r)\geq C\cdot\mathfrak{R}\big(\{g\in\mathcal{G}:T(g)\leq r\};\{\bm{z}\_{i}\}\_{i=1}^{n}\big),\qquad\forall r\geq r^{\*}. |  |

Take Î´âˆˆ(0,1)\delta\in(0,1). With probability at least 1âˆ’Î´1-\delta, for all gâˆˆğ’¢g\in\mathcal{G} and K>1K>1,

|  |  |  |
| --- | --- | --- |
|  | 1nâ€‹âˆ‘i=1nğ”¼â€‹[gâ€‹(ğ’›i)]â‰¤KKâˆ’1â‹…1nâ€‹âˆ‘i=1ngâ€‹(ğ’›i)+câ€‹[KCâ€‹râˆ—+((bâˆ’a)+Câ€‹K)â€‹logâ¡(1/Î´)n]+1Kâˆ’1â‹…1nâ€‹âˆ‘i=1nÎ·i.\frac{1}{n}\sum\_{i=1}^{n}\mathbb{E}\left[g(\bm{z}\_{i})\right]\leq\frac{K}{K-1}\cdot\frac{1}{n}\sum\_{i=1}^{n}g(\bm{z}\_{i})+c\left[\frac{K}{C}r^{\*}+\left((b-a)+CK\right)\frac{\log(1/\delta)}{n}\right]+\frac{1}{K-1}\cdot\frac{1}{n}\sum\_{i=1}^{n}\eta\_{i}. |  |

Here c>0c>0 is a universal constant.

###### Proof of LABEL:sm:lem-local-uniform-concentration.

See LABEL:sm:sec-proof-lem-local-uniform-concentration.
âˆ

For every fâˆˆâ„±f\in\mathcal{F}, define â„“f:ğ’³Ã—â„â†’â„\ell\_{f}:\mathcal{X}\times\mathbb{R}\to\mathbb{R} by â„“fâ€‹((ğ’™,y))=[fâ€‹(ğ’™)âˆ’y]2\ell\_{f}((\bm{x},y))=[f(\bm{x})-y]^{2}. We also denote (ğ’™,y)(\bm{x},y) by ğ’›\bm{z}. In LABEL:sm:lem-local-uniform-concentration, take {ğ’›i}i=1n=â„¬t,ktr\{\bm{z}\_{i}\}\_{i=1}^{n}=\mathcal{B}\_{t,k}^{\operatorname{\mathrm{tr}}} and ğ’¢={â„“fâˆ’â„“fÂ¯t:fâˆˆâ„±}\mathcal{G}=\{\ell\_{f}-\ell\_{\bar{f}\_{t}}:f\in\mathcal{F}\}. The following lemma suggests a choice of gig\_{i} and TT for which ([C.1](https://arxiv.org/html/2512.23596v1#A3.E1 "Equation C.1 â€£ Lemma C.1 (Localized uniform concentration). â€£ C.2 Proof of â€£ Appendix C Theoretical Analysis of Nonstationarity-Complexity Tradeoff â€£ The nonstationarity-complexity tradeoff in return prediction")) holds.

###### Lemma C.2 (Noise condition).

Let Assumption LABEL:sm:assumption-bounded hold. For all f,fÂ¯âˆˆâ„±f,\bar{f}\in\mathcal{F},

|  |  |  |  |
| --- | --- | --- | --- |
|  | 1mt,kâ€‹âˆ‘j=tâˆ’ktâˆ’1mjâ€‹varğ’›âˆ¼Pjâ¡[â„“fâ€‹(ğ’›)âˆ’â„“fÂ¯â€‹(ğ’›)]\displaystyle\frac{1}{m\_{t,k}}\sum\_{j=t-k}^{t-1}m\_{j}\operatorname{{\rm var}}\_{\bm{z}\sim P\_{j}}\left[\ell\_{f}(\bm{z})-\ell\_{\bar{f}}(\bm{z})\right] | â‰¤16â€‹M2â‹…1mt,kâ€‹âˆ‘j=tâˆ’ktâˆ’1mjâ€‹â€–fâˆ’fÂ¯â€–j2\displaystyle\leq 16M^{2}\cdot\frac{1}{m\_{t,k}}\sum\_{j=t-k}^{t-1}m\_{j}\|f-\bar{f}\|\_{j}^{2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤32â€‹M2â‹…1mt,kâ€‹âˆ‘j=tâˆ’ktâˆ’1mjâ€‹{ğ”¼ğ’›âˆ¼Pjâ€‹[â„“fâ€‹(ğ’›)âˆ’â„“fÂ¯â€‹(ğ’›)]+2â€‹â„°jâ€‹(fÂ¯)}.\displaystyle\leq 32M^{2}\cdot\frac{1}{m\_{t,k}}\sum\_{j=t-k}^{t-1}m\_{j}\left\{\mathbb{E}\_{\bm{z}\sim P\_{j}}\left[\ell\_{f}(\bm{z})-\ell\_{\bar{f}}(\bm{z})\right]+2\mathcal{E}\_{j}(\bar{f})\right\}. |  |

###### Proof of LABEL:sm:lem-noise-condition-regression.

See LABEL:sm:sec-proof-lem-noise-condition-regression.
âˆ

Define

|  |  |  |
| --- | --- | --- |
|  | Tâ€‹(â„“fâˆ’â„“fÂ¯t)=16â€‹M2â‹…1mt,kâ€‹âˆ‘j=tâˆ’ktâˆ’1mjâ€‹â€–fâˆ’fÂ¯tâ€–j2,âˆ€fâˆˆâ„±.T(\ell\_{f}-\ell\_{\bar{f}\_{t}})=16M^{2}\cdot\frac{1}{m\_{t,k}}\sum\_{j=t-k}^{t-1}m\_{j}\left\|f-\bar{f}\_{t}\right\|\_{j}^{2},\qquad\forall f\in\mathcal{F}. |  |

By LABEL:sm:lem-noise-condition-regression, the noise condition ([C.1](https://arxiv.org/html/2512.23596v1#A3.E1 "Equation C.1 â€£ Lemma C.1 (Localized uniform concentration). â€£ C.2 Proof of â€£ Appendix C Theoretical Analysis of Nonstationarity-Complexity Tradeoff â€£ The nonstationarity-complexity tradeoff in return prediction")) holds with C=32â€‹M2C=32M^{2}, and LABEL:sm:lem-local-uniform-concentration is applicable. Moreover, for all râ‰¥0r\geq 0,

|  |  |  |
| --- | --- | --- |
|  | â„œâ€‹({â„“fâˆ’â„“fÂ¯t:fâˆˆâ„±,Tâ€‹(â„“fâˆ’â„“fÂ¯t)â‰¤r};ğ’Ÿt,ktr)\displaystyle\mathfrak{R}\left(\left\{\ell\_{f}-\ell\_{\bar{f}\_{t}}:f\in\mathcal{F},\,T(\ell\_{f}-\ell\_{\bar{f}\_{t}})\leq r\right\};\mathcal{D}^{\operatorname{\mathrm{tr}}}\_{t,k}\right) |  |
|  |  |  |
| --- | --- | --- |
|  | =ğ”¼â€‹sup{1mt,kâ€‹âˆ‘j=tâˆ’ktâˆ’1âˆ‘i=1mjÎµj,iâ€‹[fâ€‹(ğ’™j,i)âˆ’yj,i]2:fâˆˆâ„±,Tâ€‹(â„“fâˆ’â„“fÂ¯t)â‰¤r}\displaystyle\qquad=\mathbb{E}\sup\left\{\frac{1}{m\_{t,k}}\sum\_{j=t-k}^{t-1}\sum\_{i=1}^{m\_{j}}\varepsilon\_{j,i}\left[f(\bm{x}\_{j,i})-y\_{j,i}\right]^{2}:f\in\mathcal{F},\,T(\ell\_{f}-\ell\_{\bar{f}\_{t}})\leq r\right\} |  |
|  |  |  |
| --- | --- | --- |
|  | â‰¤4â€‹Mâ‹…ğ”¼â€‹sup{1mt,kâ€‹âˆ‘j=tâˆ’ktâˆ’1âˆ‘i=1mjÎµj,iâ€‹[fâ€‹(ğ’™j,i)âˆ’yj,i]:fâˆˆâ„±,Tâ€‹(â„“fâˆ’â„“fÂ¯t)â‰¤r}\displaystyle\qquad\leq 4M\cdot\mathbb{E}\sup\left\{\frac{1}{m\_{t,k}}\sum\_{j=t-k}^{t-1}\sum\_{i=1}^{m\_{j}}\varepsilon\_{j,i}\left[f(\bm{x}\_{j,i})-y\_{j,i}\right]:f\in\mathcal{F},\,T(\ell\_{f}-\ell\_{\bar{f}\_{t}})\leq r\right\} |  |
|  |  |  |
| --- | --- | --- |
|  | =4â€‹Mâ‹…ğ”¼â€‹sup{1mt,kâ€‹âˆ‘j=tâˆ’ktâˆ’1âˆ‘i=1mjÎµj,iâ€‹fâ€‹(ğ’™j,i):fâˆˆâ„±,Tâ€‹(â„“fâˆ’â„“fÂ¯t)â‰¤r}\displaystyle\qquad=4M\cdot\mathbb{E}\sup\left\{\frac{1}{m\_{t,k}}\sum\_{j=t-k}^{t-1}\sum\_{i=1}^{m\_{j}}\varepsilon\_{j,i}f(\bm{x}\_{j,i}):f\in\mathcal{F},\,T(\ell\_{f}-\ell\_{\bar{f}\_{t}})\leq r\right\} |  |
|  |  |  |
| --- | --- | --- |
|  | =4â€‹Mâ‹…â„œâ€‹({fâˆˆâ„±:Tâ€‹(â„“fâˆ’â„“fÂ¯t)â‰¤r};ğ’Ÿt,ktr),\displaystyle\qquad=4M\cdot\mathfrak{R}\left(\left\{f\in\mathcal{F}:T(\ell\_{f}-\ell\_{\bar{f}\_{t}})\leq r\right\};\mathcal{D}^{\operatorname{\mathrm{tr}}}\_{t,k}\right), |  |

where {Îµj,i}\{\varepsilon\_{j,i}\} are i.i.d.Â Rademacher random variables, and the inequality is due to Theorem A.6 in BBM05.

Define

|  |  |  |
| --- | --- | --- |
|  | Lt,ktrâ€‹(f)=1mt,kâ€‹âˆ‘j=tâˆ’ktâˆ’1mjâ€‹Ljâ€‹(f).L\_{t,k}^{\operatorname{\mathrm{tr}}}(f)=\frac{1}{m\_{t,k}}\sum\_{j=t-k}^{t-1}m\_{j}L\_{j}(f). |  |

Applying LABEL:sm:lem-local-uniform-concentration with g=f^t,kâˆ’fÂ¯tg=\widehat{f}\_{t,k}-\bar{f}\_{t}, we obtain that if Ïˆ~:â„+â†’â„+\widetilde{\psi}:\mathbb{R}\_{+}\to\mathbb{R}\_{+} is a sub-root function with fixed point r~\widetilde{r} satisfying

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ïˆ~â€‹(r)â‰¥128â€‹M3â‹…â„œâ€‹({fâˆˆâ„±:Tâ€‹(â„“fâˆ’â„“fÂ¯t)â‰¤r};ğ’Ÿt,ktr),âˆ€râ‰¥r~,\widetilde{\psi}(r)\geq 128M^{3}\cdot\mathfrak{R}\left(\left\{f\in\mathcal{F}:T(\ell\_{f}-\ell\_{\bar{f}\_{t}})\leq r\right\};\mathcal{D}^{\operatorname{\mathrm{tr}}}\_{t,k}\right),\qquad\forall r\geq\widetilde{r}, |  | (C.2) |

then with probability at least 1âˆ’Î´1-\delta,

|  |  |  |  |
| --- | --- | --- | --- |
|  | Lt,ktrâ€‹(f^t,k)âˆ’Lt,ktrâ€‹(fÂ¯t)\displaystyle L^{\operatorname{\mathrm{tr}}}\_{t,k}(\widehat{f}\_{t,k})-L^{\operatorname{\mathrm{tr}}}\_{t,k}(\bar{f}\_{t}) | â‰²(L^t,ktrâ€‹(f^t,k)âˆ’L^t,ktrâ€‹(fÂ¯t))+[r~M2+M2â€‹logâ¡(1/Î´)mt,k]+maxtâˆ’kâ‰¤jâ‰¤tâˆ’1â¡â„°jâ€‹(fÂ¯t)\displaystyle\lesssim\left(\widehat{L}\_{t,k}^{\operatorname{\mathrm{tr}}}(\widehat{f}\_{t,k})-\widehat{L}\_{t,k}^{\operatorname{\mathrm{tr}}}(\bar{f}\_{t})\right)+\left[\frac{\widetilde{r}}{M^{2}}+\frac{M^{2}\log(1/\delta)}{m\_{t,k}}\right]+\max\_{t-k\leq j\leq t-1}\mathcal{E}\_{j}(\bar{f}\_{t}) |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | â‰²[r~M2+M2â€‹logâ¡(1/Î´)mt,k]+maxtâˆ’kâ‰¤jâ‰¤tâˆ’1â¡â„°jâ€‹(fÂ¯t),\displaystyle\lesssim\left[\frac{\widetilde{r}}{M^{2}}+\frac{M^{2}\log(1/\delta)}{m\_{t,k}}\right]+\max\_{t-k\leq j\leq t-1}\mathcal{E}\_{j}(\bar{f}\_{t}), |  | (C.3) |

where â‰²\lesssim hides a universal constant, and the second inequality is due to L^t,ktrâ€‹(f^t,k)â‰¤L^t,ktrâ€‹(fÂ¯t)\widehat{L}^{\operatorname{\mathrm{tr}}}\_{t,k}(\widehat{f}\_{t,k})\leq\widehat{L}^{\operatorname{\mathrm{tr}}}\_{t,k}(\bar{f}\_{t}).

It remains to express r~\widetilde{r} in terms of rt,kr\_{t,k}, and convert ([C.3](https://arxiv.org/html/2512.23596v1#A3.E3 "Equation C.3 â€£ C.2 Proof of â€£ Appendix C Theoretical Analysis of Nonstationarity-Complexity Tradeoff â€£ The nonstationarity-complexity tradeoff in return prediction")) into a bound for Ltâ€‹(f^t,k)L\_{t}(\widehat{f}\_{t,k}). We work on r~\widetilde{r} first. Take a subroot function Ïˆ:â„+â†’â„+\psi:\mathbb{R}\_{+}\to\mathbb{R}\_{+} with fixed point rt,kr\_{t,k} such that

|  |  |  |
| --- | --- | --- |
|  | Ïˆâ€‹(r)â‰¥â„œâ€‹({fâˆˆâ„±:1nt,kâ€‹âˆ‘j=tâˆ’ktâˆ’1njâ€‹â€–fâˆ’fÂ¯tâ€–j2â‰¤r}),âˆ€râ‰¥rt,k.\psi(r)\geq\mathfrak{R}\bigg(\bigg\{f\in\mathcal{F}:\frac{1}{n\_{t,k}}\sum\_{j=t-k}^{t-1}n\_{j}\left\|f-\bar{f}\_{t}\right\|\_{j}^{2}\leq r\bigg\}\bigg),\quad\forall r\geq r\_{t,k}. |  |

We now show that Ïˆ~â€‹(r)=128â€‹M3â€‹Ïˆâ€‹(r16â€‹M2)\widetilde{\psi}(r)=128M^{3}\psi\left(\frac{r}{16M^{2}}\right) satisfies ([C.2](https://arxiv.org/html/2512.23596v1#A3.E2 "Equation C.2 â€£ C.2 Proof of â€£ Appendix C Theoretical Analysis of Nonstationarity-Complexity Tradeoff â€£ The nonstationarity-complexity tradeoff in return prediction")). By LABEL:sm:lem-property-subroot, the fixed point r~\widetilde{r} of Ïˆ~\widetilde{\psi} satisfies

|  |  |  |  |
| --- | --- | --- | --- |
|  | min{1,8M}216M2rt,kâ‰¤r~â‰¤max{1,8M}216M2rt,k.\min\{1,8M\}^{2}16M^{2}r\_{t,k}\leq\widetilde{r}\leq\max\{1,8M\}^{2}16M^{2}r\_{t,k}. |  | (C.4) |

For all râ‰¥r~r\geq\widetilde{r}, since râ‰¥rt,kr\geq r\_{t,k}, then

|  |  |  |
| --- | --- | --- |
|  | Ïˆ~â€‹(r)â‰¥128â€‹M3â‹…â„œâ€‹({fâˆˆâ„±:16â€‹M2â‹…1nt,kâ€‹âˆ‘j=tâˆ’ktâˆ’1njâ€‹â€–fâˆ’fÂ¯tâ€–j2â‰¤r}),\widetilde{\psi}(r)\geq 128M^{3}\cdot\mathfrak{R}\bigg(\bigg\{f\in\mathcal{F}:16M^{2}\cdot\frac{1}{n\_{t,k}}\sum\_{j=t-k}^{t-1}n\_{j}\left\|f-\bar{f}\_{t}\right\|\_{j}^{2}\leq r\bigg\}\bigg), |  |

so ([C.2](https://arxiv.org/html/2512.23596v1#A3.E2 "Equation C.2 â€£ C.2 Proof of â€£ Appendix C Theoretical Analysis of Nonstationarity-Complexity Tradeoff â€£ The nonstationarity-complexity tradeoff in return prediction")) holds for this choice of Ïˆ~\widetilde{\psi}, and ([C.3](https://arxiv.org/html/2512.23596v1#A3.E3 "Equation C.3 â€£ C.2 Proof of â€£ Appendix C Theoretical Analysis of Nonstationarity-Complexity Tradeoff â€£ The nonstationarity-complexity tradeoff in return prediction")) becomes

|  |  |  |  |
| --- | --- | --- | --- |
|  | Lt,ktrâ€‹(f^t,k)âˆ’Lt,ktrâ€‹(fÂ¯t)â‰²M2â€‹[rt,k+logâ¡(1+Î´âˆ’1)mt,k]+maxtâˆ’kâ‰¤jâ‰¤tâˆ’1â¡â„°jâ€‹(fÂ¯t).L^{\operatorname{\mathrm{tr}}}\_{t,k}(\widehat{f}\_{t,k})-L^{\operatorname{\mathrm{tr}}}\_{t,k}(\bar{f}\_{t})\lesssim M^{2}\left[r\_{t,k}+\frac{\log(1+\delta^{-1})}{m\_{t,k}}\right]+\max\_{t-k\leq j\leq t-1}\mathcal{E}\_{j}(\bar{f}\_{t}). |  | (C.5) |

Finally, we will convert ([C.5](https://arxiv.org/html/2512.23596v1#A3.E5 "Equation C.5 â€£ C.2 Proof of â€£ Appendix C Theoretical Analysis of Nonstationarity-Complexity Tradeoff â€£ The nonstationarity-complexity tradeoff in return prediction")) into a bound for Ltâ€‹(f^t,k)L\_{t}(\widehat{f}\_{t,k}). We invoke the following lemma.

###### Lemma C.3.

For all fâˆˆâ„±f\in\mathcal{F} and j,tâˆˆâ„¤+j,t\in\mathbb{Z}\_{+},

|  |  |  |
| --- | --- | --- |
|  | |Ljâ€‹(f)âˆ’Ltâ€‹(f)|â‰¤4â€‹M2â€‹TVâ¡(Pj,Pt),\displaystyle\left|L\_{j}(f)-L\_{t}(f)\right|\leq 4M^{2}\operatorname{TV}\left(P\_{j},P\_{t}\right), |  |
|  |  |  |
| --- | --- | --- |
|  | |â„°jâ€‹(f)âˆ’â„°tâ€‹(f)|â‰¤4â€‹M2â€‹TVâ¡(Pj,Pt).\displaystyle\left|\mathcal{E}\_{j}(f)-\mathcal{E}\_{t}(f)\right|\leq 4M^{2}\operatorname{TV}\left(P\_{j},P\_{t}\right). |  |

###### Proof of LABEL:sm:lem-risk-to-TV.

For every fâˆˆâ„±f\in\mathcal{F},

|  |  |  |
| --- | --- | --- |
|  | |Ljâ€‹(f)âˆ’Ltâ€‹(f)|=|ğ”¼(x,y)âˆ¼Pjâ€‹{[fâ€‹(x)âˆ’y]2}âˆ’ğ”¼(x,y)âˆ¼Ptâ€‹{[fâ€‹(x)âˆ’y]2}|â‰¤4â€‹M2â‹…TVâ¡(Pj,Pt).\left|L\_{j}(f)-L\_{t}(f)\right|=\left|\mathbb{E}\_{(x,y)\sim P\_{j}}\left\{\left[f(x)-y\right]^{2}\right\}-\mathbb{E}\_{(x,y)\sim P\_{t}}\left\{\left[f(x)-y\right]^{2}\right\}\right|\leq 4M^{2}\cdot\operatorname{TV}(P\_{j},P\_{t}). |  |

To prove the second inequality, since Ltâ€‹(ftâˆ—)â‰¤Ltâ€‹(fjâˆ—)L\_{t}(f\_{t}^{\*})\leq L\_{t}(f\_{j}^{\*}), then

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„°jâ€‹(f)âˆ’â„°tâ€‹(f)\displaystyle\mathcal{E}\_{j}(f)-\mathcal{E}\_{t}(f) | =[Ljâ€‹(f)âˆ’Ljâ€‹(fjâˆ—)]âˆ’[Ltâ€‹(f)âˆ’Ltâ€‹(ftâˆ—)]\displaystyle=\big[L\_{j}(f)-L\_{j}(f\_{j}^{\*})\big]-\big[L\_{t}(f)-L\_{t}(f\_{t}^{\*})\big] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤[Ljâ€‹(f)âˆ’Ljâ€‹(fjâˆ—)]âˆ’[Ltâ€‹(f)âˆ’Ltâ€‹(fjâˆ—)]\displaystyle\leq\big[L\_{j}(f)-L\_{j}(f\_{j}^{\*})\big]-\big[L\_{t}(f)-L\_{t}(f\_{j}^{\*})\big] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =ğ”¼(x,y)âˆ¼Pjâ€‹{[fâ€‹(x)âˆ’y]2âˆ’[fjâˆ—â€‹(x)âˆ’y]2}âˆ’ğ”¼(x,y)âˆ¼Ptâ€‹{[fâ€‹(x)âˆ’y]2âˆ’[fjâˆ—â€‹(x)âˆ’y]2}\displaystyle=\mathbb{E}\_{(x,y)\sim P\_{j}}\left\{\left[f(x)-y\right]^{2}-\left[f\_{j}^{\*}(x)-y\right]^{2}\right\}-\mathbb{E}\_{(x,y)\sim P\_{t}}\left\{\left[f(x)-y\right]^{2}-\left[f\_{j}^{\*}(x)-y\right]^{2}\right\} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤4â€‹M2â‹…TVâ¡(Pj,Pt).\displaystyle\leq 4M^{2}\cdot\operatorname{TV}\left(P\_{j},P\_{t}\right). |  |

By symmetry, â„°tâ€‹(f)âˆ’â„°jâ€‹(f)â‰¤M2â‹…TVâ¡(Pj,Pt)\mathcal{E}\_{t}(f)-\mathcal{E}\_{j}(f)\leq M^{2}\cdot\operatorname{TV}\left(P\_{j},P\_{t}\right), so

|  |  |  |
| --- | --- | --- |
|  | maxtâˆ’kâ‰¤jâ‰¤tâˆ’1â¡|â„°jâ€‹(f)âˆ’â„°tâ€‹(f)|â‰¤4â€‹M2â€‹maxtâˆ’kâ‰¤jâ‰¤tâˆ’1â¡TVâ¡(Pj,Pt).\max\_{t-k\leq j\leq t-1}\left|\mathcal{E}\_{j}(f)-\mathcal{E}\_{t}(f)\right|\leq 4M^{2}\max\_{t-k\leq j\leq t-1}\operatorname{TV}\left(P\_{j},P\_{t}\right). |  |

This finishes the proof.
âˆ

Since

|  |  |  |
| --- | --- | --- |
|  | |Lt,ktrâ€‹(f)âˆ’Ltâ€‹(f)|â‰¤maxtâˆ’kâ‰¤jâ‰¤tâˆ’1â¡|Ljâ€‹(f)âˆ’Ltâ€‹(f)|,\big|L^{\operatorname{\mathrm{tr}}}\_{t,k}(f)-L\_{t}(f)\big|\leq\max\_{t-k\leq j\leq t-1}|L\_{j}(f)-L\_{t}(f)|, |  |

then substituting LABEL:sm:lem-risk-to-TV into ([C.5](https://arxiv.org/html/2512.23596v1#A3.E5 "Equation C.5 â€£ C.2 Proof of â€£ Appendix C Theoretical Analysis of Nonstationarity-Complexity Tradeoff â€£ The nonstationarity-complexity tradeoff in return prediction")) yields

|  |  |  |
| --- | --- | --- |
|  | Ltâ€‹(f^t,k)âˆ’Ltâ€‹(fÂ¯t)â‰²M2â€‹[rt,k+logâ¡(1+Î´âˆ’1)mt,k]+â„°tâ€‹(fÂ¯t)+M2â€‹maxtâˆ’kâ‰¤jâ‰¤tâˆ’1â¡TVâ¡(Pj,Pt).L\_{t}(\widehat{f}\_{t,k})-L\_{t}(\bar{f}\_{t})\lesssim M^{2}\left[r\_{t,k}+\frac{\log(1+\delta^{-1})}{m\_{t,k}}\right]+\mathcal{E}\_{t}(\bar{f}\_{t})+M^{2}\max\_{t-k\leq j\leq t-1}\operatorname{TV}\left(P\_{j},P\_{t}\right). |  |

Since â„°tâ€‹(f)=Ltâ€‹(f)âˆ’Ltâ€‹(ftâˆ—)\mathcal{E}\_{t}(f)=L\_{t}(f)-L\_{t}(f\_{t}^{\*}), then

|  |  |  |
| --- | --- | --- |
|  | â„°â€‹(f^t,k)â‰²â„°tâ€‹(fÂ¯t)+M2â€‹[rt,k+logâ¡(1+Î´âˆ’1)mt,k]+M2â€‹maxtâˆ’kâ‰¤jâ‰¤tâˆ’1â¡TVâ¡(Pj,Pt).\mathcal{E}(\widehat{f}\_{t,k})\lesssim\mathcal{E}\_{t}(\bar{f}\_{t})+M^{2}\left[r\_{t,k}+\frac{\log(1+\delta^{-1})}{m\_{t,k}}\right]+M^{2}\max\_{t-k\leq j\leq t-1}\operatorname{TV}\left(P\_{j},P\_{t}\right). |  |

This completes the proof.

### C.3 Proof of LABEL:sm:lem-local-uniform-concentration

The core techniques are the same as those of Theorem 3.3 in BBM05, with small changes in the quantities to bound. For r,Î»>0r,\lambda>0, let

|  |  |  |
| --- | --- | --- |
|  | wâ€‹(g)=minâ¡{râ€‹Î»k:kâˆˆ{0}âˆªâ„¤+,râ€‹Î»kâ‰¥Tâ€‹(g)},ğ’¢r={rwâ€‹(g)â€‹g:gâˆˆğ’¢},w(g)=\min\{r\lambda^{k}:k\in\{0\}\cup\mathbb{Z}\_{+},\,r\lambda^{k}\geq T(g)\},\qquad\mathcal{G}\_{r}=\left\{\frac{r}{w(g)}g:g\in\mathcal{G}\right\}, |  |

and

|  |  |  |
| --- | --- | --- |
|  | Vr+=supgâˆˆğ’¢{rwâ€‹(g)â‹…1nâ€‹âˆ‘i=1n(ğ”¼â€‹[gâ€‹(ğ’›i)]âˆ’gâ€‹(ğ’›i))}.V\_{r}^{+}=\sup\_{g\in\mathcal{G}}\left\{\frac{r}{w(g)}\cdot\frac{1}{n}\sum\_{i=1}^{n}\big(\mathbb{E}[g(\bm{z}\_{i})]-g(\bm{z}\_{i})\big)\right\}. |  |

Similar to Lemma 3.8 of BBM05, for every K>1K>1 and gâˆˆğ’¢g\in\mathcal{G},

|  |  |  |
| --- | --- | --- |
|  | Vr+â‰¤rÎ»â€‹Câ€‹Kimplies1nâ€‹âˆ‘i=1nğ”¼â€‹[gâ€‹(ğ’›i)]â‰¤KKâˆ’1â‹…1nâ€‹âˆ‘i=1ngâ€‹(ğ’›i)+rÎ»â€‹Câ€‹K+1Kâˆ’1â‹…1nâ€‹âˆ‘i=1nÎ·i.V\_{r}^{+}\leq\frac{r}{\lambda CK}\quad\text{implies}\quad\frac{1}{n}\sum\_{i=1}^{n}\mathbb{E}[g(\bm{z}\_{i})]\leq\frac{K}{K-1}\cdot\frac{1}{n}\sum\_{i=1}^{n}g(\bm{z}\_{i})+\frac{r}{\lambda CK}+\frac{1}{K-1}\cdot\frac{1}{n}\sum\_{i=1}^{n}\eta\_{i}. |  |

We now invoke a uniform convergence result to give a bound for Vr+V\_{r}^{+}. It is a non-stationary version of Theorem 2.1 in BBM05.

###### Lemma C.4 (Uniform concentration).

Consider the setting of LABEL:sm:lem-local-uniform-concentration. Define

|  |  |  |
| --- | --- | --- |
|  | v=1nâ€‹supgâˆˆğ’¢âˆ‘i=1nvarâ¡[gâ€‹(ğ’›i)].v=\frac{1}{n}\sup\_{g\in\mathcal{G}}\sum\_{i=1}^{n}\operatorname{{\rm var}}[g(\bm{z}\_{i})]. |  |

Let Î´âˆˆ(0,1)\delta\in(0,1). With probability at least 1âˆ’Î´1-\delta,

|  |  |  |
| --- | --- | --- |
|  | supgâˆˆğ’¢1nâ€‹âˆ‘i=1n(ğ”¼â€‹[gâ€‹(ğ’›i)]âˆ’gâ€‹(ğ’›i))\displaystyle\sup\_{g\in\mathcal{G}}\frac{1}{n}\sum\_{i=1}^{n}\big(\mathbb{E}[g(\bm{z}\_{i})]-g(\bm{z}\_{i})\big) |  |
|  |  |  |
| --- | --- | --- |
|  | â‰²infÎ±>0{(1+Î±)â€‹â„œâ€‹(ğ’¢;{ğ’›i}i=1n)+vâ€‹logâ¡(1/Î´)n+(1+Î±âˆ’1)â€‹(bâˆ’a)â€‹logâ¡(1/Î´)n},\displaystyle\qquad\lesssim\inf\_{\alpha>0}\left\{(1+\alpha)\mathfrak{R}(\mathcal{G};\{\bm{z}\_{i}\}\_{i=1}^{n})+\sqrt{\frac{v\log(1/\delta)}{n}}+(1+\alpha^{-1})\frac{(b-a)\log(1/\delta)}{n}\right\}, |  |

where â‰²\lesssim hides a universal constant.

###### Proof of LABEL:sm:lem-uniform-concentration.

Let Z=supgâˆˆğ’¢1nâ€‹âˆ‘i=1n[gâ€‹(ğ’›i)âˆ’ğ”¼â€‹gâ€‹(ğ’›i)]Z=\sup\_{g\in\mathcal{G}}\frac{1}{n}\sum\_{i=1}^{n}\left[g(\bm{z}\_{i})-\mathbb{E}g(\bm{z}\_{i})\right]. Adapting the proof of Theorem 4 in Mas00, with probability at least 1âˆ’Î´1-\delta,

|  |  |  |
| --- | --- | --- |
|  | Zâˆ’ğ”¼â€‹Zâ‰²logâ¡(1/Î´)nâ€‹(v+(bâˆ’a)â€‹ğ”¼â€‹Z)+(bâˆ’a)â€‹logâ¡(1/Î´)n,Z-\mathbb{E}Z\lesssim\sqrt{\frac{\log(1/\delta)}{n}\left(v+(b-a)\mathbb{E}Z\right)}+\frac{(b-a)\log(1/\delta)}{n}, |  |

where â‰²\lesssim hides a universal constant. By Youngâ€™s inequality, for every Î±>0\alpha>0,

|  |  |  |
| --- | --- | --- |
|  | logâ¡(1/Î´)nâ€‹(bâˆ’a)â€‹ğ”¼â€‹Zâ‰¤Î±â€‹ğ”¼â€‹Z+Î±âˆ’1â€‹(bâˆ’a)â€‹logâ¡(1/Î´)n.\sqrt{\frac{\log(1/\delta)}{n}(b-a)\mathbb{E}Z}\leq\alpha\mathbb{E}Z+\alpha^{-1}\frac{(b-a)\log(1/\delta)}{n}. |  |

By Rademacher symmetrization (e.g., Lemma A.5 in BBM05), for i.i.d.Â Rademacher random variables Îµ1,â€¦,Îµn\varepsilon\_{1},...,\varepsilon\_{n} independent of ğ’›1,â€¦,ğ’›n\bm{z}\_{1},...,\bm{z}\_{n}, it holds that

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹Zâ‰¤2â€‹ğ”¼â€‹[supgâˆˆğ’¢1nâ€‹âˆ‘i=1nÎµiâ€‹gâ€‹(ğ’›i)]=2â€‹â„œâ€‹(ğ’¢;{ğ’›i}i=1n).\mathbb{E}Z\leq 2\mathbb{E}\left[\sup\_{g\in\mathcal{G}}\frac{1}{n}\sum\_{i=1}^{n}\varepsilon\_{i}g(\bm{z}\_{i})\right]=2\mathfrak{R}(\mathcal{G};\{\bm{z}\_{i}\}\_{i=1}^{n}). |  |

This completes the proof.
âˆ

By LABEL:sm:lem-uniform-concentration, each of the following inequalities holds with probability at least 1âˆ’Î´1-\delta:

|  |  |  |
| --- | --- | --- |
|  | Vr+â‰²infÎ±>0{(1+Î±)â€‹â„œâ€‹(ğ’¢r;{ğ’›i}i=1n)+vâ€‹logâ¡(1/Î´)n+(1+Î±âˆ’1)â€‹(bâˆ’a)â€‹logâ¡(1/Î´)n},V\_{r}^{+}\lesssim\inf\_{\alpha>0}\left\{(1+\alpha)\mathfrak{R}(\mathcal{G}\_{r};\{\bm{z}\_{i}\}\_{i=1}^{n})+\sqrt{\frac{v\log(1/\delta)}{n}}+(1+\alpha^{-1})\frac{(b-a)\log(1/\delta)}{n}\right\}, |  |

The rest of the proof follows that in Section 3.2 of BBM05.

### C.4 Proof of LABEL:sm:lem-noise-condition-regression

Recall that for ğ’›=(ğ’™,y)\bm{z}=(\bm{x},y), we define â„“fâ€‹(ğ’›)=[fâ€‹(ğ’™)âˆ’y]2\ell\_{f}(\bm{z})=[f(\bm{x})-y]^{2}. Then

|  |  |  |
| --- | --- | --- |
|  | 1mt,kâ€‹âˆ‘j=tâˆ’ktâˆ’1mjâ€‹varğ’›âˆ¼Pjâ¡[â„“fâ€‹(ğ’›)âˆ’â„“fÂ¯â€‹(ğ’›)]\displaystyle\frac{1}{m\_{t,k}}\sum\_{j=t-k}^{t-1}m\_{j}\operatorname{{\rm var}}\_{\bm{z}\sim P\_{j}}\left[\ell\_{f}(\bm{z})-\ell\_{\bar{f}}(\bm{z})\right] |  |
|  |  |  |
| --- | --- | --- |
|  | =1mt,kâ€‹âˆ‘j=tâˆ’ktâˆ’1mjâ€‹var(ğ’™,y)âˆ¼Pjâ¡[(fâ€‹(ğ’™)âˆ’y)2âˆ’(fÂ¯â€‹(ğ’™)âˆ’y)2]\displaystyle\qquad=\frac{1}{m\_{t,k}}\sum\_{j=t-k}^{t-1}m\_{j}\operatorname{{\rm var}}\_{(\bm{x},y)\sim P\_{j}}\left[\big(f(\bm{x})-y\big)^{2}-\big(\bar{f}(\bm{x})-y\big)^{2}\right] |  |
|  |  |  |
| --- | --- | --- |
|  | â‰¤1mt,kâ€‹âˆ‘j=tâˆ’ktâˆ’1mjâ€‹ğ”¼(ğ’™,y)âˆ¼Pjâ€‹{[(fâ€‹(ğ’™)âˆ’y)2âˆ’(fÂ¯â€‹(ğ’™)âˆ’y)2]2}\displaystyle\qquad\leq\frac{1}{m\_{t,k}}\sum\_{j=t-k}^{t-1}m\_{j}\mathbb{E}\_{(\bm{x},y)\sim P\_{j}}\left\{\left[\big(f(\bm{x})-y\big)^{2}-\big(\bar{f}(\bm{x})-y\big)^{2}\right]^{2}\right\} |  |
|  |  |  |
| --- | --- | --- |
|  | =1mt,kâ€‹âˆ‘j=tâˆ’ktâˆ’1mjâ€‹ğ”¼(ğ’™,y)âˆ¼Pjâ€‹[(fâ€‹(ğ’™)âˆ’fÂ¯â€‹(ğ’™))2â€‹(fâ€‹(ğ’™)+fÂ¯â€‹(ğ’™)âˆ’2â€‹y)2]\displaystyle\qquad=\frac{1}{m\_{t,k}}\sum\_{j=t-k}^{t-1}m\_{j}\mathbb{E}\_{(\bm{x},y)\sim P\_{j}}\left[\big(f(\bm{x})-\bar{f}(\bm{x})\big)^{2}\big(f(\bm{x})+\bar{f}(\bm{x})-2y\big)^{2}\right] |  |
|  |  |  |
| --- | --- | --- |
|  | â‰¤16â€‹M2â‹…1mt,kâ€‹âˆ‘j=tâˆ’ktâˆ’1mjâ€‹ğ”¼(ğ’™,y)âˆ¼Pjâ€‹[(fâ€‹(ğ’™)âˆ’fÂ¯â€‹(ğ’™))2]=16â€‹M2â‹…1mt,kâ€‹âˆ‘j=tâˆ’ktâˆ’1mjâ€‹â€–fâˆ’fÂ¯â€–j2\displaystyle\qquad\leq 16M^{2}\cdot\frac{1}{m\_{t,k}}\sum\_{j=t-k}^{t-1}m\_{j}\mathbb{E}\_{(\bm{x},y)\sim P\_{j}}\left[\left(f(\bm{x})-\bar{f}(\bm{x})\right)^{2}\right]=16M^{2}\cdot\frac{1}{m\_{t,k}}\sum\_{j=t-k}^{t-1}m\_{j}\|f-\bar{f}\|\_{j}^{2} |  |
|  |  |  |
| --- | --- | --- |
|  | â‰¤32â€‹M2â‹…1mt,kâ€‹âˆ‘j=tâˆ’ktâˆ’1mjâ€‹(â€–fâˆ’fjâˆ—â€–j2+â€–fÂ¯âˆ’fjâˆ—â€–j2)\displaystyle\qquad\leq 32M^{2}\cdot\frac{1}{m\_{t,k}}\sum\_{j=t-k}^{t-1}m\_{j}\left(\|f-f\_{j}^{\*}\|\_{j}^{2}+\|\bar{f}-f\_{j}^{\*}\|\_{j}^{2}\right) |  |
|  |  |  |
| --- | --- | --- |
|  | =32â€‹M2â‹…1mt,kâ€‹âˆ‘j=tâˆ’ktâˆ’1mjâ€‹{ğ”¼ğ’›âˆ¼Pjâ€‹[â„“fâ€‹(ğ’›)âˆ’â„“fÂ¯â€‹(ğ’›)]+2â€‹â„°jâ€‹(fÂ¯)}.\displaystyle\qquad=32M^{2}\cdot\frac{1}{m\_{t,k}}\sum\_{j=t-k}^{t-1}m\_{j}\left\{\mathbb{E}\_{\bm{z}\sim P\_{j}}\left[\ell\_{f}(\bm{z})-\ell\_{\bar{f}}(\bm{z})\right]+2\mathcal{E}\_{j}(\bar{f})\right\}. |  |

This finishes the proof.

### C.5 Proofs for LABEL:sm:eg-finite-class, LABEL:sm:eg-linear and LABEL:sm:eg-kernel

LABEL:sm:eg-finite-class follows from the result below. If is an immediate extension of Proposition 6.1 and Lemma D.1 in DJL21 to independent samples with non-identical distributions. The proof is omitted.

###### Lemma C.5.

Let ğ³1,â€¦,ğ³n\bm{z}\_{1},...,\bm{z}\_{n} be independent random variables taking values in ğ’³\mathcal{X}, and let Îµ1,â€¦,Îµn\varepsilon\_{1},...,\varepsilon\_{n} be i.i.d.Â Rademacher random variables independent of {ğ³i}i=1n\{\bm{z}\_{i}\}\_{i=1}^{n}. Let â„±\mathcal{F} be a finite class of functions from ğ’³\mathcal{X} to [âˆ’M,M][-M,M]. Take an arbitrary function fÂ¯:ğ’³â†’[âˆ’M,M]\bar{f}:\mathcal{X}\to[-M,M]. For râ‰¥0r\geq 0, define

|  |  |  |
| --- | --- | --- |
|  | â„›â€‹(r)=ğ”¼â€‹maxâ¡{1nâ€‹âˆ‘i=1nÎµiâ€‹fâ€‹(ğ’›i):fâˆˆâ„±,1nâ€‹âˆ‘i=1nğ”¼â€‹[(fâ€‹(ğ’›i)âˆ’fÂ¯â€‹(ğ’›i))2]â‰¤r}.\mathcal{R}(r)=\mathbb{E}\max\left\{\frac{1}{n}\sum\_{i=1}^{n}\varepsilon\_{i}f(\bm{z}\_{i}):f\in\mathcal{F},~\frac{1}{n}\sum\_{i=1}^{n}\mathbb{E}\left[\left(f(\bm{z}\_{i})-\bar{f}(\bm{z}\_{i})\right)^{2}\right]\leq r\right\}. |  |

Then for every râ‰¥0r\geq 0,

|  |  |  |
| --- | --- | --- |
|  | â„›â€‹(r)â‰¤2â€‹maxâ¡{râ€‹logâ¡|â„±|n,2â€‹Mâ€‹logâ¡|â„±|n}.\mathcal{R}(r)\leq 2\max\left\{\sqrt{\frac{r\log|\mathcal{F}|}{n}},\frac{2M\log|\mathcal{F}|}{n}\right\}. |  |

Moreover, the function on the right hand side is subroot, and has a unique fixed point

|  |  |  |
| --- | --- | --- |
|  | r=4â€‹maxâ¡{M,1}â€‹logâ¡|â„±|n.r=\frac{4\max\{M,1\}\log|\mathcal{F}|}{n}. |  |

To obtain the results in LABEL:sm:eg-linear and LABEL:sm:eg-kernel, we invoke a useful lemma. It is an extension of Theorem 41 in Men02 to the non-i.i.d.Â case. The proof is omitted.

###### Lemma C.6.

Take ğ›‰Â¯âˆˆBâ€‹(ğŸ,M)âŠ†â„d\bar{\bm{\theta}}\in B(\bm{0},\sqrt{M})\subseteq\mathbb{R}^{d}. Suppose ğ±iâˆ¼Qi\bm{x}\_{i}\sim Q\_{i}, iâˆˆ[n]i\in[n] are independent random vectors in â„d\mathbb{R}^{d}. Let Îµ1,â€¦,Îµn\varepsilon\_{1},...,\varepsilon\_{n} be i.i.d.Â Rademacher random variables independent of {ğ±i}i=1n\{\bm{x}\_{i}\}\_{i=1}^{n}. For râ‰¥0r\geq 0, define

|  |  |  |
| --- | --- | --- |
|  | â„›â€‹(r)=ğ”¼â€‹sup{1nâ€‹âˆ‘i=1nÎµiâ€‹ğ’™iâŠ¤â€‹ğœ½:1nâ€‹âˆ‘i=1nğ”¼â€‹[(ğ’™iâŠ¤â€‹(ğœ½âˆ’ğœ½Â¯))2]â‰¤r,â€–ğœ½â€–22â‰¤M}.\mathcal{R}(r)=\mathbb{E}\sup\left\{\frac{1}{n}\sum\_{i=1}^{n}\varepsilon\_{i}\bm{x}\_{i}^{\top}\bm{\theta}:\frac{1}{n}\sum\_{i=1}^{n}\mathbb{E}\left[\big(\bm{x}\_{i}^{\top}(\bm{\theta}-\bar{\bm{\theta}})\big)^{2}\right]\leq r,~\|\bm{\theta}\|\_{2}^{2}\leq M\right\}. |  |

Let ğšº=1nâ€‹âˆ‘i=1nğ”¼â€‹(ğ±iâ€‹ğ±iâŠ¤)\bm{\Sigma}=\frac{1}{n}\sum\_{i=1}^{n}\mathbb{E}(\bm{x}\_{i}\bm{x}\_{i}^{\top}), and denote by {Î»i}i=1d\{\lambda\_{i}\}\_{i=1}^{d} its eigenvalues sorted in descending order. Then

|  |  |  |
| --- | --- | --- |
|  | â„›â€‹(r)â‰¤cnâ€‹âˆ‘i=1dminâ¡{r,Mâ€‹Î»i}.\mathcal{R}(r)\leq\sqrt{\frac{c}{n}\sum\_{i=1}^{d}\min\{r,M\lambda\_{i}\}}. |  |

The above lemma leads to â„›â€‹(r)â‰¤câ€‹râ€‹d/n\mathcal{R}(r)\leq\sqrt{crd/n}.
The right-hand side is subroot and has a unique fixed point r=câ€‹d/nr=cd/n. This verifies LABEL:sm:eg-linear.

To get LABEL:sm:eg-kernel, we apply [LemmaËœC.6](https://arxiv.org/html/2512.23596v1#A3.Thmlemma6 "Lemma C.6. â€£ C.5 Proofs for , and â€£ Appendix C Theoretical Analysis of Nonstationarity-Complexity Tradeoff â€£ The nonstationarity-complexity tradeoff in return prediction") to the transformed features Ï•â€‹(ğ’™j,i)\phi(\bm{x}\_{j,i}) rather than the raw ones. Correspondingly, the feature space becomes â„\mathbb{H}, and the matrix ğšº\bm{\Sigma} in LABEL:sm:eg-kernel becomes
1mt,kâ€‹âˆ‘j=tâˆ’ktâˆ’1âˆ‘i=1mjğ”¼â€‹[Ï•â€‹(ğ’™j,i)âŠ—Ï•â€‹(ğ’™j,i)]\frac{1}{m\_{t,k}}\sum\_{j=t-k}^{t-1}\sum\_{i=1}^{m\_{j}}\mathbb{E}[\phi(\bm{x}\_{j,i})\otimes\phi(\bm{x}\_{j,i})]. Here âŠ—\otimes denotes the tensor product.

In LABEL:sm:eg-kernel, the assumption regarding the trace-class operator ğ‘º\bm{S} forces ğšºâª¯ğ‘º\bm{\Sigma}\preceq\bm{S}.
Then, [LemmaËœC.6](https://arxiv.org/html/2512.23596v1#A3.Thmlemma6 "Lemma C.6. â€£ C.5 Proofs for , and â€£ Appendix C Theoretical Analysis of Nonstationarity-Complexity Tradeoff â€£ The nonstationarity-complexity tradeoff in return prediction") yields

|  |  |  |
| --- | --- | --- |
|  | â„›â€‹(r)â‰²1mt,kâ€‹âˆ‘i=1âˆminâ¡{r,Î¼i}â‰¤1mt,kâ€‹minsâ‰¥1â¡sâ€‹r+âˆ‘i=s+1âˆÎ¼i,\mathcal{R}(r)\lesssim\sqrt{\frac{1}{m\_{t,k}}\sum\_{i=1}^{\infty}\min\{r,\mu\_{i}\}}\leq\frac{1}{\sqrt{m\_{t,k}}}\min\_{s\geq 1}\sqrt{sr+\sum\_{i=s+1}^{\infty}\mu\_{i}}, |  |

where â‰²\lesssim hides a constant factor.

* â€¢

  If there are constants c1,c2>0c\_{1},c\_{2}>0 such that Î¼kâ‰¤c1â€‹eâˆ’c2â€‹k\mu\_{k}\leq c\_{1}e^{-c\_{2}k} holds for all kk, then
  âˆ‘i=s+1âˆÎ¼iâ‰²eâˆ’c2â€‹s\sum\_{i=s+1}^{\infty}\mu\_{i}\lesssim e^{-c\_{2}s}. Taking s=âŒˆc2âˆ’1â€‹logâ¡(1/r)âŒ‰s=\lceil c\_{2}^{-1}\log(1/r)\rceil, we get

  |  |  |  |
  | --- | --- | --- |
  |  | â„›â€‹(r)â‰²1mt,kâ€‹râ€‹âŒˆc2âˆ’1â€‹logâ¡(1/r)+1âŒ‰.\mathcal{R}(r)\lesssim\frac{1}{\sqrt{m\_{t,k}}}\sqrt{r\lceil c\_{2}^{-1}\log(1/r)+1\rceil}. |  |

  The right-hand side is sub-root. Elementary calculation yields
  rt,kâ€‹(â„±)â‰²(logâ¡mt,k)/mt,kr\_{t,k}(\mathcal{F})\lesssim(\log m\_{t,k})/m\_{t,k}
* â€¢

  If there are constants c>0c>0 and Î±â‰¥1\alpha\geq 1 such that Î¼kâ‰¤câ€‹kâˆ’2â€‹Î±\mu\_{k}\leq ck^{-2\alpha} holds for all kk, then
  âˆ‘i=s+1âˆÎ¼iâ‰²s1âˆ’2â€‹Î±\sum\_{i=s+1}^{\infty}\mu\_{i}\lesssim s^{1-2\alpha}. Taking s=âŒˆrâˆ’1/(2â€‹Î±)âŒ‰s=\lceil r^{-1/(2\alpha)}\rceil, we get

  |  |  |  |
  | --- | --- | --- |
  |  | â„›â€‹(r)â‰²r1âˆ’1/(2â€‹Î±)mt,k.\mathcal{R}(r)\lesssim\sqrt{\frac{r^{1-1/(2\alpha)}}{m\_{t,k}}}. |  |

  The right-hand side is sub-root. Then, we can easily get
  rt,kâ€‹(â„±)â‰²mt,kâˆ’2â€‹Î±2â€‹Î±+1r\_{t,k}(\mathcal{F})\lesssim m\_{t,k}^{-\frac{2\alpha}{2\alpha+1}}.

## Appendix D Proofs for LABEL:sm:sec-select and LABEL:sm:sec-select-R2

### D.1 Proof of LABEL:sm:lem-complexity-tournament

Given two models f,fâ€²âˆˆâ„±f,f^{\prime}\in\mathcal{F}, denote the output of ğ’œ\mathcal{A} by ğ’œâ€‹(f,fâ€²)âˆˆ{f,fâ€²}\mathcal{A}(f,f^{\prime})\in\{f,f^{\prime}\}. Let Tâ€‹(Î›)T(\Lambda) be the maximum expected number of times LABEL:sm:alg-tournament can call ğ’œ\mathcal{A} after there are Î›\Lambda remaining models {fÎ»}Î»=1Î›\{f\_{\lambda}\}\_{\lambda=1}^{\Lambda} at the end of a while loop, where the maximum is taken over all possible choices of {fÎ»}Î»=1Î›\{f\_{\lambda}\}\_{\lambda=1}^{\Lambda}. Then Tâ€‹(Î›)T(\Lambda) is increasing in Î›\Lambda, and Tâ€‹(Î›)â‰¤Î›2/2T(\Lambda)\leq\Lambda^{2}/2. Let NN denote the number of remaining models at the end of the next while loop. Since that while loop calls ğ’œ\mathcal{A} at most Î›âˆ’1\Lambda-1 times, then

|  |  |  |  |
| --- | --- | --- | --- |
|  | Tâ€‹(Î›)â‰¤(Î›âˆ’1)+ğ”¼â€‹[Tâ€‹(N)].T(\Lambda)\leq(\Lambda-1)+\mathbb{E}\left[T(N)\right]. |  | (D.1) |

For each Î»âˆˆ[Î›]\lambda\in[\Lambda], let nÎ»=|{Î»â€²âˆˆ[Î›]\{Î»}:ğ’œâ€‹(fÎ»,fÎ»â€²)=fÎ»â€²}|n\_{\lambda}=|\{\lambda^{\prime}\in[\Lambda]\backslash\{\lambda\}:\mathcal{A}(f\_{\lambda},f\_{\lambda^{\prime}})=f\_{\lambda^{\prime}}\}| be the number of remaining models that would beat fÎ»f\_{\lambda} if they were paired. Since LABEL:sm:alg-tournament chooses each fÎ»f\_{\lambda} as the pivot model uniformly at random, then

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â€‹[Tâ€‹(N)]=ğ”¼â€‹[1Î›â€‹âˆ‘Î»=1Î›Tâ€‹(nÎ»)].\mathbb{E}\left[T(N)\right]=\mathbb{E}\left[\frac{1}{\Lambda}\sum\_{\lambda=1}^{\Lambda}T(n\_{\lambda})\right]. |  | (D.2) |

Since âˆ‘Î»=1Î›nÎ»\sum\_{\lambda=1}^{\Lambda}n\_{\lambda} counts exactly one of (fÎ»,fÎ»â€²)(f\_{\lambda},f\_{\lambda^{\prime}}) and (fÎ»â€²,fÎ»)(f\_{\lambda^{\prime}},f\_{\lambda}) for all Î»â‰ Î»â€²\lambda\neq\lambda^{\prime}, then âˆ‘Î»=1Î›nÎ»=Î›â€‹(Î›âˆ’1)/2\sum\_{\lambda=1}^{\Lambda}n\_{\lambda}=\Lambda(\Lambda-1)/2. Let n(1)â‰¤â‹¯â‰¤n(Î›)n\_{(1)}\leq\cdots\leq n\_{(\Lambda)} be the order statistics of n1,â€¦,nÎ›n\_{1},...,n\_{\Lambda}. Then for all i=1,â€¦,âŒˆÎ›/3âŒ‰i=1,...,\lceil\Lambda/3\rceil,

|  |  |  |
| --- | --- | --- |
|  | n(i)â‰¤n(âŒˆÎ›/3âŒ‰)â‰¤1Î›âˆ’âŒˆÎ›/3âŒ‰+1â€‹âˆ‘i=âŒˆÎ›/3âŒ‰Î›niâ‰¤32â€‹Î›â‹…Î›â€‹(Î›âˆ’1)2=3â€‹(Î›âˆ’1)4.n\_{(i)}\leq n\_{(\lceil\Lambda/3\rceil)}\leq\frac{1}{\Lambda-\lceil\Lambda/3\rceil+1}\sum\_{i=\lceil\Lambda/3\rceil}^{\Lambda}n\_{i}\leq\frac{3}{2\Lambda}\cdot\frac{\Lambda(\Lambda-1)}{2}=\frac{3(\Lambda-1)}{4}. |  |

By the monotonicity of TT,

|  |  |  |  |
| --- | --- | --- | --- |
|  | 1Î›â€‹âˆ‘Î»=1Î›Tâ€‹(nÎ»)=1Î›â€‹âˆ‘i=1Î›Tâ€‹(n(i))\displaystyle\frac{1}{\Lambda}\sum\_{\lambda=1}^{\Lambda}T(n\_{\lambda})=\frac{1}{\Lambda}\sum\_{i=1}^{\Lambda}T(n\_{(i)}) | â‰¤1Î›(âŒŠÎ›/3âŒ‹T(âŒˆ3Î›/4âŒ‰)+(Î›âˆ’âŒŠÎ›/3âŒ‹)T(Î›âˆ’1)]\displaystyle\leq\frac{1}{\Lambda}\bigg(\lfloor\Lambda/3\rfloor T(\lceil 3\Lambda/4\rceil)+\left(\Lambda-\lfloor\Lambda/3\rfloor\right)T(\Lambda-1)\bigg] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤13â€‹Tâ€‹(âŒˆ3â€‹Î›/4âŒ‰)+(23+1Î›)â€‹Tâ€‹(Î›)\displaystyle\leq\frac{1}{3}T(\lceil 3\Lambda/4\rceil)+\left(\frac{2}{3}+\frac{1}{\Lambda}\right)T(\Lambda) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤13â€‹Tâ€‹(âŒˆ3â€‹Î›/4âŒ‰)+23â€‹Tâ€‹(Î›)+Î›.\displaystyle\leq\frac{1}{3}T(\lceil 3\Lambda/4\rceil)+\frac{2}{3}T(\Lambda)+\Lambda. |  |

Substituting this into ([D.2](https://arxiv.org/html/2512.23596v1#A4.E2 "Equation D.2 â€£ D.1 Proof of â€£ Appendix D Proofs for and â€£ The nonstationarity-complexity tradeoff in return prediction")) and ([D.1](https://arxiv.org/html/2512.23596v1#A4.E1 "Equation D.1 â€£ D.1 Proof of â€£ Appendix D Proofs for and â€£ The nonstationarity-complexity tradeoff in return prediction")) gives

|  |  |  |
| --- | --- | --- |
|  | Tâ€‹(Î›)â‰¤6â€‹Î›+Tâ€‹(âŒˆ3â€‹Î›/4âŒ‰).T(\Lambda)\leq 6\Lambda+T(\lceil 3\Lambda/4\rceil). |  |

By the Master Theorem (Theorem 4.1) in CLRS22, we conclude that Tâ€‹(Î›)=Î˜â€‹(Î›)T(\Lambda)=\Theta(\Lambda).

### D.2 Proof of LABEL:sm:thm-model-comparison

We will prove the following stronger result: with probability at least 1âˆ’Î´1-\delta,

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„°tâ€‹(f^)âˆ’â„°tâ€‹(f2)â‰²Mâ€‹logâ¡(t/Î´)â‹…(maxtâˆ’â„“â‰¤jâ‰¤tâˆ’1â¡TVâ¡(Pj,Pt)+1nt,â„“)1/2.\sqrt{\mathcal{E}\_{t}(\widehat{f})}-\sqrt{\mathcal{E}\_{t}(f\_{2})}\lesssim M\sqrt{\log(t/\delta)}\cdot\left(\max\_{t-\ell\leq j\leq t-1}\operatorname{TV}(P\_{j},P\_{t})+\frac{1}{n\_{t,\ell}}\right)^{1/2}. |  | (D.3) |

The bound in LABEL:sm:thm-model-comparison is obtained by squaring both sides of ([D.3](https://arxiv.org/html/2512.23596v1#A4.E3 "Equation D.3 â€£ D.2 Proof of â€£ Appendix D Proofs for and â€£ The nonstationarity-complexity tradeoff in return prediction")).

Without loss of generality, assume Ltâ€‹(f1)â‰¥Ltâ€‹(f2)L\_{t}(f\_{1})\geq L\_{t}(f\_{2}), so minÎ»âˆˆ[2]â¡Ltâ€‹(fÎ»)=Ltâ€‹(f2)\min\_{\lambda\in[2]}L\_{t}(f\_{\lambda})=L\_{t}(f\_{2}). By Theorem 4.2 in HHW24, with probability at least 1âˆ’Î´1-\delta, LABEL:sm:alg-compare outputs a model f^\widehat{f} satisfying

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ltâ€‹(f^)âˆ’Ltâ€‹(f2)\displaystyle L\_{t}(\widehat{f})-L\_{t}(f\_{2}) | â‰¤|Î”^t,â„“^âˆ’Î”t|\displaystyle\leq|\widehat{\Delta}\_{t,\widehat{\ell}}-\Delta\_{t}| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰²minâ„“âˆˆ[tâˆ’1]â¡{logâ¡(t/Î´)â‹…maxtâˆ’â„“â‰¤jâ‰¤tâˆ’1â¡|Î”jâˆ’Î”t|+Ïƒt,â„“â€‹logâ¡(t/Î´)nt,â„“+M2â€‹logâ¡(t/Î´)nt,â„“},\displaystyle\lesssim\min\_{\ell\in[t-1]}\left\{\sqrt{\log(t/\delta)}\cdot\max\_{t-\ell\leq j\leq t-1}|\Delta\_{j}-\Delta\_{t}|+\sigma\_{t,\ell}\sqrt{\frac{\log(t/\delta)}{n\_{t,\ell}}}+\frac{M^{2}\log(t/\delta)}{n\_{t,\ell}}\right\}, |  |

where â‰²\lesssim hides a universal constant. When this event happens, it holds for all â„“âˆˆ[tâˆ’1]\ell\in[t-1] that

|  |  |  |
| --- | --- | --- |
|  | Ltâ€‹(f^)âˆ’Ltâ€‹(f2)â‰²logâ¡(t/Î´)â‹…maxtâˆ’â„“â‰¤jâ‰¤tâˆ’1â¡|Î”jâˆ’Î”t|+Ïƒt,â„“â€‹logâ¡(t/Î´)nt,â„“+M2â€‹logâ¡(t/Î´)nt,â„“.L\_{t}(\widehat{f})-L\_{t}(f\_{2})\lesssim\sqrt{\log(t/\delta)}\cdot\max\_{t-\ell\leq j\leq t-1}|\Delta\_{j}-\Delta\_{t}|+\sigma\_{t,\ell}\sqrt{\frac{\log(t/\delta)}{n\_{t,\ell}}}+\frac{M^{2}\log(t/\delta)}{n\_{t,\ell}}. |  |

When f^=f2\widehat{f}=f\_{2}, ([D.3](https://arxiv.org/html/2512.23596v1#A4.E3 "Equation D.3 â€£ D.2 Proof of â€£ Appendix D Proofs for and â€£ The nonstationarity-complexity tradeoff in return prediction")) automatically holds and there is nothing to prove.

Now consider the case when f^=f1\widehat{f}=f\_{1}. Fix â„“âˆˆ[tâˆ’1]\ell\in[t-1]. Define

|  |  |  |
| --- | --- | --- |
|  | â„°t,â„“vaâ€‹(f)=1nt,â„“â€‹âˆ‘j=tâˆ’â„“tâˆ’1njâ€‹[Ljâ€‹(f)âˆ’Ljâ€‹(fjâˆ—)].\mathcal{E}\_{t,\ell}^{\operatorname{\mathrm{va}}}(f)=\frac{1}{n\_{t,\ell}}\sum\_{j=t-\ell}^{t-1}n\_{j}\big[L\_{j}(f)-L\_{j}(f\_{j}^{\*})\big]. |  |

Since

|  |  |  |
| --- | --- | --- |
|  | |[â„°t,â„“vaâ€‹(f1)âˆ’â„°t,â„“vaâ€‹(f2)]âˆ’[Ltâ€‹(f1)âˆ’Ltâ€‹(f2)]|â‰¤maxtâˆ’â„“â‰¤jâ‰¤tâˆ’1â¡|Î”jâˆ’Î”t|,\left|\big[\mathcal{E}\_{t,\ell}^{\operatorname{\mathrm{va}}}(f\_{1})-\mathcal{E}\_{t,\ell}^{\operatorname{\mathrm{va}}}(f\_{2})\big]-\big[L\_{t}(f\_{1})-L\_{t}(f\_{2})\big]\right|\leq\max\_{t-\ell\leq j\leq t-1}|\Delta\_{j}-\Delta\_{t}|, |  |

then

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„°t,â„“vaâ€‹(f1)âˆ’â„°t,â„“vaâ€‹(f2)\displaystyle\mathcal{E}\_{t,\ell}^{\operatorname{\mathrm{va}}}(f\_{1})-\mathcal{E}\_{t,\ell}^{\operatorname{\mathrm{va}}}(f\_{2}) | â‰¤Ltâ€‹(f1)âˆ’Ltâ€‹(f2)+maxtâˆ’â„“â‰¤jâ‰¤tâˆ’1â¡|Î”jâˆ’Î”t|\displaystyle\leq L\_{t}(f\_{1})-L\_{t}(f\_{2})+\max\_{t-\ell\leq j\leq t-1}|\Delta\_{j}-\Delta\_{t}| |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | â‰¤Câ€‹[logâ¡(t/Î´)â‹…maxtâˆ’â„“â‰¤jâ‰¤tâˆ’1â¡|Î”jâˆ’Î”t|+Ïƒt,â„“â€‹logâ¡(t/Î´)nt,â„“+M2â€‹logâ¡(t/Î´)nt,â„“]\displaystyle\leq C\left[\sqrt{\log(t/\delta)}\cdot\max\_{t-\ell\leq j\leq t-1}|\Delta\_{j}-\Delta\_{t}|+\sigma\_{t,\ell}\sqrt{\frac{\log(t/\delta)}{n\_{t,\ell}}}+\frac{M^{2}\log(t/\delta)}{n\_{t,\ell}}\right] |  | (D.4) |

for some universal constant Câ‰¥1C\geq 1. The variance term Ïƒt,â„“2\sigma\_{t,\ell}^{2} can be bounded by

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ïƒt,â„“2\displaystyle\sigma\_{t,\ell}^{2} | =1nt,â„“â€‹âˆ‘j=tâˆ’â„“tâˆ’1njâ€‹var(ğ’™,y)âˆ¼Pjâ¡[(f1â€‹(ğ’™)âˆ’y)2âˆ’(f2â€‹(ğ’™)âˆ’y)2]\displaystyle=\frac{1}{n\_{t,\ell}}\sum\_{j=t-\ell}^{t-1}n\_{j}\operatorname{{\rm var}}\_{(\bm{x},y)\sim P\_{j}}\left[\big(f\_{1}(\bm{x})-y\big)^{2}-\big(f\_{2}(\bm{x})-y\big)^{2}\right] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤2â€‹âˆ‘Î»=12[1nt,â„“â€‹âˆ‘j=tâˆ’â„“tâˆ’1njâ€‹var(ğ’™,y)âˆ¼Pjâ¡[(fÎ»â€‹(ğ’™)âˆ’y)2âˆ’(fjâˆ—â€‹(ğ’™)âˆ’y)2]].\displaystyle\leq 2\sum\_{\lambda=1}^{2}\left[\frac{1}{n\_{t,\ell}}\sum\_{j=t-\ell}^{t-1}n\_{j}\operatorname{{\rm var}}\_{(\bm{x},y)\sim P\_{j}}\left[\big(f\_{\lambda}(\bm{x})-y\big)^{2}-\big(f\_{j}^{\*}(\bm{x})-y\big)^{2}\right]\right]. |  |

For each Î»âˆˆ[2]\lambda\in[2] and jâˆˆâ„¤+j\in\mathbb{Z}\_{+},

|  |  |  |  |
| --- | --- | --- | --- |
|  | var(ğ’™,y)âˆ¼Pjâ¡[(fÎ»â€‹(ğ’™)âˆ’y)2âˆ’(fjâˆ—â€‹(ğ’™)âˆ’y)2]\displaystyle\operatorname{{\rm var}}\_{(\bm{x},y)\sim P\_{j}}\left[\big(f\_{\lambda}(\bm{x})-y\big)^{2}-\big(f\_{j}^{\*}(\bm{x})-y\big)^{2}\right] | â‰¤ğ”¼(ğ’™,y)âˆ¼Pjâ€‹{[(fÎ»â€‹(ğ’™)âˆ’y)2âˆ’(fjâˆ—â€‹(ğ’™)âˆ’y)2]2}\displaystyle\leq\mathbb{E}\_{(\bm{x},y)\sim P\_{j}}\left\{\left[\big(f\_{\lambda}(\bm{x})-y\big)^{2}-\big(f\_{j}^{\*}(\bm{x})-y\big)^{2}\right]^{2}\right\} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =ğ”¼(ğ’™,y)âˆ¼Pjâ€‹[(fÎ»â€‹(ğ’™)âˆ’fjâˆ—â€‹(ğ’™))2â€‹(fÎ»+fjâˆ—â€‹(ğ’™)âˆ’2â€‹y)2]\displaystyle=\mathbb{E}\_{(\bm{x},y)\sim P\_{j}}\left[\big(f\_{\lambda}(\bm{x})-f\_{j}^{\*}(\bm{x})\big)^{2}\big(f\_{\lambda}+f\_{j}^{\*}(\bm{x})-2y\big)^{2}\right] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤16â€‹M2â€‹ğ”¼(ğ’™,y)âˆ¼Pjâ€‹[(fÎ»â€‹(ğ’™)âˆ’fjâˆ—â€‹(ğ’™))2]\displaystyle\leq 16M^{2}\mathbb{E}\_{(\bm{x},y)\sim P\_{j}}\left[\big(f\_{\lambda}(\bm{x})-f\_{j}^{\*}(\bm{x})\big)^{2}\right] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =16â€‹M2â€‹â„°jâ€‹(fÎ»).\displaystyle=16M^{2}\mathcal{E}\_{j}(f\_{\lambda}). |  |

Thus,

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ïƒt,â„“2â‰¤32â€‹M2â‹…âˆ‘Î»=12[1nt,â„“â€‹âˆ‘j=tâˆ’â„“tâˆ’1njâ€‹â„°jâ€‹(fÎ»)]=32â€‹M2â€‹[â„°t,â„“vaâ€‹(f1)+â„°t,â„“vaâ€‹(f2)].\sigma\_{t,\ell}^{2}\leq 32M^{2}\cdot\sum\_{\lambda=1}^{2}\left[\frac{1}{n\_{t,\ell}}\sum\_{j=t-\ell}^{t-1}n\_{j}\mathcal{E}\_{j}(f\_{\lambda})\right]=32M^{2}\big[\mathcal{E}\_{t,\ell}^{\operatorname{\mathrm{va}}}(f\_{1})+\mathcal{E}\_{t,\ell}^{\operatorname{\mathrm{va}}}(f\_{2})\big]. |  | (D.5) |

Substituting ([D.5](https://arxiv.org/html/2512.23596v1#A4.E5 "Equation D.5 â€£ D.2 Proof of â€£ Appendix D Proofs for and â€£ The nonstationarity-complexity tradeoff in return prediction")) into ([D.4](https://arxiv.org/html/2512.23596v1#A4.E4 "Equation D.4 â€£ D.2 Proof of â€£ Appendix D Proofs for and â€£ The nonstationarity-complexity tradeoff in return prediction")) yields

|  |  |  |
| --- | --- | --- |
|  | â„°t,â„“vaâ€‹(f1)âˆ’â„°t,â„“vaâ€‹(f2)â‰¤2â€‹Aâ€‹(â„°t,â„“vaâ€‹(f1)+â„°t,â„“vaâ€‹(f2))+D,\mathcal{E}\_{t,\ell}^{\operatorname{\mathrm{va}}}(f\_{1})-\mathcal{E}\_{t,\ell}^{\operatorname{\mathrm{va}}}(f\_{2})\leq 2A\left(\sqrt{\mathcal{E}\_{t,\ell}^{\operatorname{\mathrm{va}}}(f\_{1})}+\sqrt{\mathcal{E}\_{t,\ell}^{\operatorname{\mathrm{va}}}(f\_{2})}\,\right)+D, |  |

where

|  |  |  |
| --- | --- | --- |
|  | A=2â€‹2â€‹Câ€‹Mâ€‹logâ¡(t/Î´)nt,â„“andD=Câ€‹[logâ¡(t/Î´)â€‹maxtâˆ’â„“â‰¤jâ‰¤tâˆ’1â¡|Î”jâˆ’Î”t|+M2â€‹logâ¡(t/Î´)nt,â„“]A=2\sqrt{2}CM\sqrt{\frac{\log(t/\delta)}{n\_{t,\ell}}}\quad\text{and}\quad D=C\left[\sqrt{\log(t/\delta)}\max\_{t-\ell\leq j\leq t-1}|\Delta\_{j}-\Delta\_{t}|+\frac{M^{2}\log(t/\delta)}{n\_{t,\ell}}\right] |  |

Completing the squares gives

|  |  |  |
| --- | --- | --- |
|  | (â„°t,â„“vaâ€‹(f1)âˆ’A)2â‰¤(â„°t,â„“vaâ€‹(f2)+A)2+D,\left(\sqrt{\mathcal{E}\_{t,\ell}^{\operatorname{\mathrm{va}}}(f\_{1})}-A\right)^{2}\leq\left(\sqrt{\mathcal{E}\_{t,\ell}^{\operatorname{\mathrm{va}}}(f\_{2})}+A\right)^{2}+D, |  |

which implies

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„°t,â„“vaâ€‹(f1)âˆ’â„°t,â„“vaâ€‹(f2)\displaystyle\sqrt{\mathcal{E}\_{t,\ell}^{\operatorname{\mathrm{va}}}(f\_{1})}-\sqrt{\mathcal{E}\_{t,\ell}^{\operatorname{\mathrm{va}}}(f\_{2})} | â‰¤2â€‹A+D\displaystyle\leq 2A+\sqrt{D} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰²Mâ€‹logâ¡(t/Î´)nt,â„“+[logâ¡(t/Î´)â€‹maxtâˆ’â„“â‰¤jâ‰¤tâˆ’1â¡|Î”jâˆ’Î”t|+M2â€‹logâ¡(t/Î´)nt,â„“]1/2\displaystyle\lesssim M\sqrt{\frac{\log(t/\delta)}{n\_{t,\ell}}}+\left[\sqrt{\log(t/\delta)}\max\_{t-\ell\leq j\leq t-1}|\Delta\_{j}-\Delta\_{t}|+\frac{M^{2}\log(t/\delta)}{n\_{t,\ell}}\right]^{1/2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰²logâ¡(t/Î´)â‹…(maxtâˆ’â„“â‰¤jâ‰¤tâˆ’1â¡|Î”jâˆ’Î”t|+M2nt,â„“)1/2\displaystyle\lesssim\sqrt{\log(t/\delta)}\cdot\left(\max\_{t-\ell\leq j\leq t-1}|\Delta\_{j}-\Delta\_{t}|+\frac{M^{2}}{n\_{t,\ell}}\right)^{1/2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰²Mâ€‹logâ¡(t/Î´)â‹…(maxtâˆ’â„“â‰¤jâ‰¤tâˆ’1â¡TVâ¡(Pj,Pt)+1nt,â„“)1/2,\displaystyle\lesssim M\sqrt{\log(t/\delta)}\cdot\left(\max\_{t-\ell\leq j\leq t-1}\operatorname{TV}(P\_{j},P\_{t})+\frac{1}{n\_{t,\ell}}\right)^{1/2}, |  |

where the last inequality is due to

|  |  |  |
| --- | --- | --- |
|  | |Î”jâˆ’Î”t|â‰²M2â‹…TVâ¡(Pj,Pt).|\Delta\_{j}-\Delta\_{t}|\lesssim M^{2}\cdot\operatorname{TV}(P\_{j},P\_{t}). |  |

Finally, by LABEL:sm:lem-risk-to-TV, for every fâˆˆ{f1,f2}f\in\{f\_{1},f\_{2}\},

|  |  |  |
| --- | --- | --- |
|  | |â„°t,â„“vaâ€‹(f)âˆ’â„°tâ€‹(f)|â‰¤|â„°t,â„“vaâ€‹(f)âˆ’â„°tâ€‹(f)|â‰¤maxtâˆ’â„“â‰¤jâ‰¤tâˆ’1â¡|â„°jâ€‹(f)âˆ’â„°tâ€‹(f)|â‰¤2â€‹Mâ€‹maxtâˆ’â„“â‰¤jâ‰¤tâˆ’1â¡TVâ¡(Pj,Pt),\displaystyle\left|\sqrt{\mathcal{E}\_{t,\ell}^{\operatorname{\mathrm{va}}}(f)}-\sqrt{\mathcal{E}\_{t}(f)}\right|\leq\sqrt{\big|\mathcal{E}\_{t,\ell}^{\operatorname{\mathrm{va}}}(f)-\mathcal{E}\_{t}(f)\big|}\leq\max\_{t-\ell\leq j\leq t-1}\sqrt{\big|\mathcal{E}\_{j}(f)-\mathcal{E}\_{t}(f)\big|}\leq 2M\max\_{t-\ell\leq j\leq t-1}\sqrt{\operatorname{TV}(P\_{j},P\_{t})}, |  |

so

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„°tâ€‹(f1)âˆ’â„°tâ€‹(f2)\displaystyle\sqrt{\mathcal{E}\_{t}(f\_{1})}-\sqrt{\mathcal{E}\_{t}(f\_{2})} | â‰¤â„°t,â„“vaâ€‹(f1)âˆ’â„°t,â„“vaâ€‹(f2)+4â€‹Mâ€‹maxtâˆ’â„“â‰¤jâ‰¤tâˆ’1â¡TVâ¡(Pj,Pt)\displaystyle\leq\sqrt{\mathcal{E}\_{t,\ell}^{\operatorname{\mathrm{va}}}(f\_{1})}-\sqrt{\mathcal{E}\_{t,\ell}^{\operatorname{\mathrm{va}}}(f\_{2})}+4M\max\_{t-\ell\leq j\leq t-1}\sqrt{\operatorname{TV}(P\_{j},P\_{t})} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰²Mâ€‹logâ¡(t/Î´)â‹…(maxtâˆ’â„“â‰¤jâ‰¤tâˆ’1â¡TVâ¡(Pj,Pt)+1nt,â„“)1/2.\displaystyle\lesssim M\sqrt{\log(t/\delta)}\cdot\left(\max\_{t-\ell\leq j\leq t-1}\operatorname{TV}(P\_{j},P\_{t})+\frac{1}{n\_{t,\ell}}\right)^{1/2}. |  |

As f^=f1\widehat{f}=f\_{1}, this finishes the proof.

### D.3 Proof of LABEL:sm:thm-select-tournament

We first prove the following lemma, which converts any performance guarantee of the subroutine ğ’œ\mathcal{A} to that of LABEL:sm:alg-tournament.

###### Lemma D.1 (From comparison to selection).

Take a performance metric â„’:{fÎ»}Î»=1Î›â†’â„\mathcal{L}:\{f\_{\lambda}\}\_{\lambda=1}^{\Lambda}\to\mathbb{R}, and U:(0,1)â†’â„+U:(0,1)\to\mathbb{R}\_{+}. Fix Î´âˆˆ(0,1)\delta\in(0,1). Suppose that the model comparison subroutine ğ’œ\mathcal{A} in LABEL:sm:alg-tournament satisfies the following property: given two models h1,h2âˆˆ{fÎ»}Î»=1Î›h\_{1},h\_{2}\in\{f\_{\lambda}\}\_{\lambda=1}^{\Lambda}, it outputs a model h^âˆˆ{h1,h2}\widehat{h}\in\{h\_{1},h\_{2}\} satisfying

|  |  |  |
| --- | --- | --- |
|  | â„™â€‹(â„’â€‹(h^)âˆ’minâ¡{â„’â€‹(h1),â„’â€‹(h2)}â‰¤Uâ€‹(Î´))â‰¥1âˆ’Î´.\mathbb{P}\left(\mathcal{L}(\widehat{h})-\min\left\{\mathcal{L}(h\_{1}),\mathcal{L}(h\_{2})\right\}\leq U(\delta)\right)\geq 1-\delta. |  |

Then the output f^\widehat{f} of LABEL:sm:alg-tournament satisfies

|  |  |  |
| --- | --- | --- |
|  | â„™â€‹(â„’â€‹(f^)âˆ’minÎ»âˆˆ[Î›]â¡â„’â€‹(fÎ»)â‰¤2â€‹Uâ€‹(Î´))â‰¥1âˆ’Î›2â€‹Î´.\mathbb{P}\left(\mathcal{L}(\widehat{f})-\min\_{\lambda\in[\Lambda]}\mathcal{L}(f\_{\lambda})\leq 2U(\delta)\right)\geq 1-\Lambda^{2}\delta. |  |

###### Proof of LABEL:sm:lem-select-tournament-reduction.

Given two models f,fâ€²âˆˆâ„±f,f^{\prime}\in\mathcal{F}, denote the output of ğ’œ\mathcal{A} by ğ’œâ€‹(f,fâ€²)âˆˆ{f,fâ€²}\mathcal{A}(f,f^{\prime})\in\{f,f^{\prime}\}. For notational convenience we also set ğ’œâ€‹(f,f)=f\mathcal{A}(f,f)=f for every fâˆˆâ„±f\in\mathcal{F}. By a union bound, with probability at least 1âˆ’Î›2â€‹Î´1-\Lambda^{2}\delta,

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„’â€‹(ğ’œâ€‹(fÎ»â€²,fÎ»â€²â€²))âˆ’minÎ»âˆˆ{Î»â€²,Î»â€²â€²}â¡â„’â€‹(fÎ»)â‰¤Uâ€‹(Î´),âˆ€Î»â€²,Î»â€²â€²âˆˆ[Î›].\mathcal{L}\left(\mathcal{A}\big(f\_{\lambda^{\prime}},f\_{\lambda^{\prime\prime}}\big)\right)-\min\_{\lambda\in\{\lambda^{\prime},\lambda^{\prime\prime}\}}\mathcal{L}(f\_{\lambda})\leq U(\delta),\qquad\forall\lambda^{\prime},\lambda^{\prime\prime}\in[\Lambda]. |  | (D.6) |

From now on suppose that ([D.6](https://arxiv.org/html/2512.23596v1#A4.E6 "Equation D.6 â€£ Proof of . â€£ D.3 Proof of â€£ Appendix D Proofs for and â€£ The nonstationarity-complexity tradeoff in return prediction")) holds. Take fÂ¯âˆˆ{fÎ»}Î»=1Î›\bar{f}\in\{f\_{\lambda}\}\_{\lambda=1}^{\Lambda} such that â„’â€‹(fÂ¯)=minÎ»âˆˆ[Î›]â¡â„’â€‹(fÎ»)\mathcal{L}(\bar{f})=\min\_{\lambda\in[\Lambda]}\mathcal{L}(f\_{\lambda}).

If LABEL:sm:alg-tournament outputs f^=fÂ¯\widehat{f}=\bar{f}, then there is nothing to prove. Now assume that f^â‰ fÂ¯\widehat{f}\neq\bar{f}. Then, there exists â„“âˆˆâ„¤+\ell\in\mathbb{Z}\_{+} such that at the end of the KK-th while loop, fÂ¯\bar{f} is not in Sâ€²S^{\prime}. Take the smallest such KK. Let gKg\_{K} denote the pivot model ff during the KK-th while loop, and let SKâ€²S^{\prime}\_{K} denote the set Sâ€²S^{\prime} at the end of the KK-th while loop. There are two cases.

1. 1.

   If at the end of the KK-th while loop, SKâ€²=âˆ…S^{\prime}\_{K}=\emptyset and LABEL:sm:alg-tournament outputs f^\widehat{f}, then during this while loop, a call of LABEL:sm:alg-compare has yielded ğ’œâ€‹({f^,fÂ¯})=f^\mathcal{A}(\{\widehat{f},\bar{f}\})=\widehat{f}, so by ([D.6](https://arxiv.org/html/2512.23596v1#A4.E6 "Equation D.6 â€£ Proof of . â€£ D.3 Proof of â€£ Appendix D Proofs for and â€£ The nonstationarity-complexity tradeoff in return prediction")), â„’â€‹(f^)âˆ’â„’â€‹(fÂ¯)â‰¤Uâ€‹(Î´)\mathcal{L}(\widehat{f})-\mathcal{L}(\bar{f})\leq U(\delta).
2. 2.

   Otherwise, at the end of the KK-th while loop, SKâ€²â‰ âˆ…S^{\prime}\_{K}\neq\emptyset. There are two cases.

   1. (a)

      If gK=fÂ¯g\_{K}=\bar{f}, then every fâˆˆSKâ€²f\in S^{\prime}\_{K}, a call of ğ’œ\mathcal{A} has yielded ğ’œâ€‹({fÂ¯,f})=f\mathcal{A}(\{\bar{f},f\})=f, so by ([D.6](https://arxiv.org/html/2512.23596v1#A4.E6 "Equation D.6 â€£ Proof of . â€£ D.3 Proof of â€£ Appendix D Proofs for and â€£ The nonstationarity-complexity tradeoff in return prediction")), â„’â€‹(f)âˆ’â„’â€‹(fÂ¯)â‰¤Uâ€‹(Î´)\mathcal{L}(f)-\mathcal{L}(\bar{f})\leq U(\delta).
      Since the output f^\widehat{f} must come from SKâ€²S^{\prime}\_{K}, then automatically â„’â€‹(f^)âˆ’â„’â€‹(fÂ¯)â‰¤Uâ€‹(Î´)\mathcal{L}(\widehat{f})-\mathcal{L}(\bar{f})\leq U(\delta).
   2. (b)

      If gKâ‰ fÂ¯g\_{K}\neq\bar{f}, then for every fâˆˆSKâ€²f\in S^{\prime}\_{K}, a call of ğ’œ\mathcal{A} has yielded ğ’œâ€‹({gK,f})=f\mathcal{A}(\{g\_{K},f\})=f, so by ([D.6](https://arxiv.org/html/2512.23596v1#A4.E6 "Equation D.6 â€£ Proof of . â€£ D.3 Proof of â€£ Appendix D Proofs for and â€£ The nonstationarity-complexity tradeoff in return prediction")),

      |  |  |  |  |
      | --- | --- | --- | --- |
      |  | â„’â€‹(f)âˆ’â„’â€‹(gK)â‰¤Uâ€‹(Î´).\mathcal{L}(f)-\mathcal{L}(g\_{K})\leq U(\delta). |  | (D.7) |

      Since fÂ¯âˆ‰SKâ€²\bar{f}\not\in S^{\prime}\_{K}, then a call of LABEL:sm:alg-compare has yielded ğ’œâ€‹({gK,fÂ¯})=gK\mathcal{A}(\{g\_{K},\bar{f}\})=g\_{K}, so by ([D.6](https://arxiv.org/html/2512.23596v1#A4.E6 "Equation D.6 â€£ Proof of . â€£ D.3 Proof of â€£ Appendix D Proofs for and â€£ The nonstationarity-complexity tradeoff in return prediction")),

      |  |  |  |  |
      | --- | --- | --- | --- |
      |  | â„’â€‹(gK)âˆ’â„’â€‹(fÂ¯)â‰¤Uâ€‹(Î´).\mathcal{L}(g\_{K})-\mathcal{L}(\bar{f})\leq U(\delta). |  | (D.8) |

      Putting together ([D.7](https://arxiv.org/html/2512.23596v1#A4.E7 "Equation D.7 â€£ Item 2b â€£ Item 2 â€£ Proof of . â€£ D.3 Proof of â€£ Appendix D Proofs for and â€£ The nonstationarity-complexity tradeoff in return prediction")) and ([D.8](https://arxiv.org/html/2512.23596v1#A4.E8 "Equation D.8 â€£ Item 2b â€£ Item 2 â€£ Proof of . â€£ D.3 Proof of â€£ Appendix D Proofs for and â€£ The nonstationarity-complexity tradeoff in return prediction")) yields that for all fâˆˆSKâ€²f\in S^{\prime}\_{K},

      |  |  |  |
      | --- | --- | --- |
      |  | â„’â€‹(f)â‰¤â„’â€‹(gK)+Uâ€‹(Î´)â‰¤â„’â€‹(fÂ¯)+2â€‹Uâ€‹(Î´).\mathcal{L}(f)\leq\mathcal{L}(g\_{K})+U(\delta)\leq\mathcal{L}(\bar{f})+2U(\delta). |  |

      Since the output f^\widehat{f} must come from SKâ€²S^{\prime}\_{K}, then automatically â„’â€‹(f^)âˆ’â„’â€‹(fÂ¯)â‰¤2â€‹Uâ€‹(Î´)\mathcal{L}(\widehat{f})-\mathcal{L}(\bar{f})\leq 2U(\delta).

In all the cases above, we have â„’â€‹(f^)âˆ’â„’â€‹(fÂ¯)â‰¤2â€‹Uâ€‹(Î´)\mathcal{L}(\widehat{f})-\mathcal{L}(\bar{f})\leq 2U(\delta).
âˆ

We now prove LABEL:sm:thm-select-tournament. By the stronger bound ([D.3](https://arxiv.org/html/2512.23596v1#A4.E3 "Equation D.3 â€£ D.2 Proof of â€£ Appendix D Proofs for and â€£ The nonstationarity-complexity tradeoff in return prediction")) in the proof of LABEL:sm:thm-model-comparison, we can set in LABEL:sm:lem-select-tournament-reduction â„’â€‹(f)=â„°tâ€‹(f)\mathcal{L}(f)=\sqrt{\mathcal{E}\_{t}(f)} and

|  |  |  |
| --- | --- | --- |
|  | U(Î´)=CMlogâ¡(t/Î´)â‹…minâ„“âˆˆ[tâˆ’1](maxtâˆ’â„“â‰¤jâ‰¤tâˆ’1TV(Pj,Pt)+1nt,â„“)1/2,U(\delta)=CM\sqrt{\log(t/\delta)}\cdot\min\_{\ell\in[t-1]}\left(\max\_{t-\ell\leq j\leq t-1}\operatorname{TV}(P\_{j},P\_{t})+\frac{1}{n\_{t,\ell}}\right)^{1/2}, |  |

for some universal constant C>0C>0. Then, by LABEL:sm:lem-select-tournament-reduction and the choice of Î´â€²\delta^{\prime}, with probability 1âˆ’Î´1-\delta, the output f^\widehat{f} of LABEL:sm:alg-tournament satisfies

|  |  |  |
| --- | --- | --- |
|  | â„°tâ€‹(f^)âˆ’minÎ»âˆˆ[Î›]â„°tâ€‹(fÎ»)â‰²U(Î´/Î›2)â‰²Mlogâ¡(Î›â€‹t/Î´)â‹…minâ„“âˆˆ[tâˆ’1](maxtâˆ’â„“â‰¤jâ‰¤tâˆ’1TV(Pj,Pt)+1nt,â„“)1/2.\sqrt{\mathcal{E}\_{t}(\widehat{f})}-\min\_{\lambda\in[\Lambda]}\sqrt{\mathcal{E}\_{t}(f\_{\lambda})}\lesssim U(\delta/\Lambda^{2})\lesssim M\sqrt{\log(\Lambda t/\delta)}\cdot\min\_{\ell\in[t-1]}\left(\max\_{t-\ell\leq j\leq t-1}\operatorname{TV}(P\_{j},P\_{t})+\frac{1}{n\_{t,\ell}}\right)^{1/2}. |  |

Here â‰²\lesssim only hides universal constants. This finishes the proof.

### D.4 Proof of LABEL:sm:thm-joint-selection

Recall that |ğ’Ÿjtr|=mj|\mathcal{D}\_{j}^{\operatorname{\mathrm{tr}}}|=m\_{j}, |ğ’Ÿjva|=nj|\mathcal{D}\_{j}^{\operatorname{\mathrm{va}}}|=n\_{j}, |ğ’Ÿj|=mj+nj|\mathcal{D}\_{j}|=m\_{j}+n\_{j}, mt,k=âˆ‘j=tâˆ’ktâˆ’1mjm\_{t,k}=\sum\_{j=t-k}^{t-1}m\_{j}, nt,â„“=âˆ‘j=tâˆ’â„“tâˆ’1njn\_{t,\ell}=\sum\_{j=t-\ell}^{t-1}n\_{j}, and Bt,k=mt,k+nt,kB\_{t,k}=m\_{t,k}+n\_{t,k}. Since there are (tâˆ’1)â€‹|â„±|(t-1)|\mathscr{F}| candidate models, then by LABEL:sm:thm-select-tournament, with probability at least 1âˆ’Î´/21-\delta/2, the output f^\widehat{f} of LABEL:sm:alg-tournament satisfies

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„°tâ€‹(f^)â‰²minâ„±âˆˆâ„±,kâˆˆ[tâˆ’1]â¡â„°tâ€‹(h^â€‹(â„±,k))+M2â€‹logâ¡(t2â€‹|â„±|/Î´)â‹…minâ„“âˆˆ[tâˆ’1]â¡{maxtâˆ’â„“â‰¤jâ‰¤tâˆ’1â¡TVâ¡(Pj,Pt)+1nt,â„“}.\mathcal{E}\_{t}(\widehat{f})\lesssim\min\_{\mathcal{F}\in\mathscr{F},\,k\in[t-1]}\mathcal{E}\_{t}\big(\widehat{h}(\mathcal{F},k)\big)+M^{2}\log\big(t^{2}|\mathscr{F}|/\delta\big)\cdot\min\_{\ell\in[t-1]}\left\{\max\_{t-\ell\leq j\leq t-1}\operatorname{TV}(P\_{j},P\_{t})+\frac{1}{n\_{t,\ell}}\right\}. |  | (D.9) |

By LABEL:sm:thm-tradeoff and a union bound, with probability at least 1âˆ’Î´/21-\delta/2,

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„°tâ€‹(h^â€‹(â„±,k))â‰²minfâˆˆâ„±â¡â„°tâ€‹(f)+M2â€‹(rt,kâ€‹(â„±)+logâ¡(tâ€‹|â„±|/Î´)mt,k)+M2â€‹maxtâˆ’kâ‰¤jâ‰¤tâˆ’1â¡TVâ¡(Pj,Pt).\mathcal{E}\_{t}\big(\widehat{h}(\mathcal{F},k)\big)\lesssim\min\_{f\in\mathcal{F}}\mathcal{E}\_{t}(f)+M^{2}\left(r\_{t,k}(\mathcal{F})+\frac{\log(t|\mathscr{F}|/\delta)}{m\_{t,k}}\right)+M^{2}\max\_{t-k\leq j\leq t-1}\operatorname{TV}\left(P\_{j},P\_{t}\right). |  | (D.10) |

Combining ([D.9](https://arxiv.org/html/2512.23596v1#A4.E9 "Equation D.9 â€£ D.4 Proof of â€£ Appendix D Proofs for and â€£ The nonstationarity-complexity tradeoff in return prediction")) and ([D.10](https://arxiv.org/html/2512.23596v1#A4.E10 "Equation D.10 â€£ D.4 Proof of â€£ Appendix D Proofs for and â€£ The nonstationarity-complexity tradeoff in return prediction")) yields that, with probability at least 1âˆ’Î´1-\delta,

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„°tâ€‹(f^)\displaystyle\mathcal{E}\_{t}(\widehat{f}) | â‰²minâ„±âˆˆâ„±,kâˆˆ[tâˆ’1]â¡{minfâˆˆâ„±â¡â„°tâ€‹(f)+M2â€‹(rt,kâ€‹(â„±)+logâ¡(tâ€‹|â„±|/Î´)mt,k)+M2â€‹maxtâˆ’kâ‰¤jâ‰¤tâˆ’1â¡TVâ¡(Pj,Pt)}\displaystyle\lesssim\min\_{\mathcal{F}\in\mathscr{F},\,k\in[t-1]}\left\{\min\_{f\in\mathcal{F}}\mathcal{E}\_{t}(f)+M^{2}\left(r\_{t,k}(\mathcal{F})+\frac{\log(t|\mathscr{F}|/\delta)}{m\_{t,k}}\right)+M^{2}\max\_{t-k\leq j\leq t-1}\operatorname{TV}\left(P\_{j},P\_{t}\right)\right\} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +M2â€‹logâ¡(t2â€‹|â„±|/Î´)â‹…minâ„“âˆˆ[tâˆ’1]â¡{maxtâˆ’â„“â‰¤jâ‰¤tâˆ’1â¡TVâ¡(Pj,Pt)+1nt,â„“}\displaystyle\qquad+M^{2}\log\big(t^{2}|\mathscr{F}|/\delta\big)\cdot\min\_{\ell\in[t-1]}\left\{\max\_{t-\ell\leq j\leq t-1}\operatorname{TV}(P\_{j},P\_{t})+\frac{1}{n\_{t,\ell}}\right\} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰²minâ„±âˆˆâ„±,kâˆˆ[tâˆ’1]â¡{minfâˆˆâ„±â¡â„°tâ€‹(f)+M2â€‹(rt,kâ€‹(â„±)+logâ¡(tâ€‹|â„±|/Î´)Bt,k)+M2â€‹maxtâˆ’kâ‰¤jâ‰¤tâˆ’1â¡TVâ¡(Pj,Pt)}\displaystyle\lesssim\min\_{\mathcal{F}\in\mathscr{F},\,k\in[t-1]}\left\{\min\_{f\in\mathcal{F}}\mathcal{E}\_{t}(f)+M^{2}\left(r\_{t,k}(\mathcal{F})+\frac{\log(t|\mathscr{F}|/\delta)}{B\_{t,k}}\right)+M^{2}\max\_{t-k\leq j\leq t-1}\operatorname{TV}\left(P\_{j},P\_{t}\right)\right\} |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | +M2â€‹logâ¡(t2â€‹|â„±|/Î´)â‹…minkâˆˆ[tâˆ’1]â¡{maxtâˆ’kâ‰¤jâ‰¤tâˆ’1â¡TVâ¡(Pj,Pt)+1Bt,k}\displaystyle\qquad+M^{2}\log\big(t^{2}|\mathscr{F}|/\delta\big)\cdot\min\_{k\in[t-1]}\left\{\max\_{t-k\leq j\leq t-1}\operatorname{TV}(P\_{j},P\_{t})+\frac{1}{B\_{t,k}}\right\} |  | (by Assumption LABEL:sm:assumption-data-split) |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰²logâ¡(tâ€‹|â„±|/Î´)â‹…minâ„±âˆˆâ„±,kâˆˆ[tâˆ’1]â¡{minfâˆˆâ„±â¡â„°tâ€‹(f)+M2â€‹(rt,kâ€‹(â„±)+1Bt,k)+M2â€‹maxtâˆ’kâ‰¤jâ‰¤tâˆ’1â¡TVâ¡(Pj,Pt)}.\displaystyle\lesssim\log\big(t|\mathscr{F}|/\delta\big)\cdot\min\_{\mathcal{F}\in\mathscr{F},\,k\in[t-1]}\left\{\min\_{f\in\mathcal{F}}\mathcal{E}\_{t}(f)+M^{2}\left(r\_{t,k}(\mathcal{F})+\frac{1}{B\_{t,k}}\right)+M^{2}\max\_{t-k\leq j\leq t-1}\operatorname{TV}\left(P\_{j},P\_{t}\right)\right\}. |  |

Here the second inequality hides the constant cc in Assumption LABEL:sm:assumption-data-split. This completes the proof.

### D.5 Proof of LABEL:sm:lem-bias-variance-decomp-R

By the triangle inequality,

|  |  |  |  |
| --- | --- | --- | --- |
|  | |Î”^t,â„“Râˆ’Î”tR|â‰¤|Î”^t,â„“Râˆ’Î”t,â„“R|+|Î”t,â„“Râˆ’Î”tR|,\big|\widehat{\Delta}^{R}\_{t,\ell}-\Delta^{R}\_{t}\big|\leq\big|\widehat{\Delta}^{R}\_{t,\ell}-\Delta^{R}\_{t,\ell}\big|+\big|\Delta^{R}\_{t,\ell}-\Delta^{R}\_{t}\big|, |  | (D.11) |

where

|  |  |  |
| --- | --- | --- |
|  | Î”t,â„“R=ğ”¼â€‹[Î”^t,â„“R]=Î”t,â„“Vt,â„“.\Delta^{R}\_{t,\ell}=\mathbb{E}\big[\widehat{\Delta}^{R}\_{t,\ell}\big]=\frac{\Delta\_{t,\ell}}{V\_{t,\ell}}. |  |

By LABEL:sm:lem-max-diff-ratio,

|  |  |  |  |
| --- | --- | --- | --- |
|  | |Î”t,â„“Râˆ’Î”tR|=|Î”t,â„“Vt,â„“âˆ’Î”tVt|â‰¤maxtâˆ’â„“â‰¤jâ‰¤tâˆ’1â¡|Î”jRâˆ’Î”tR|.\big|\Delta^{R}\_{t,\ell}-\Delta^{R}\_{t}\big|=\left|\frac{\Delta\_{t,\ell}}{V\_{t,\ell}}-\frac{\Delta\_{t}}{V\_{t}}\right|\leq\max\_{t-\ell\leq j\leq t-1}|\Delta^{R}\_{j}-\Delta^{R}\_{t}|. |  | (D.12) |

We have

|  |  |  |
| --- | --- | --- |
|  | Î”^t,â„“R=1nt,â„“â€‹âˆ‘j=tâˆ’â„“tâˆ’1âˆ‘i=1njuj,iVt,â„“,\widehat{\Delta}^{R}\_{t,\ell}=\frac{1}{n\_{t,\ell}}\sum\_{j=t-\ell}^{t-1}\sum\_{i=1}^{n\_{j}}\frac{u\_{j,i}}{V\_{t,\ell}}, |  |

where uj,i=[f1â€‹(ğ’™j,iva)âˆ’yj,iva]2âˆ’[f2â€‹(ğ’™j,iva)âˆ’yj,iva]2u\_{j,i}=\big[f\_{1}(\bm{x}\_{j,i}^{\operatorname{\mathrm{va}}})-y\_{j,i}^{\operatorname{\mathrm{va}}}\big]^{2}-\big[f\_{2}(\bm{x}\_{j,i}^{\operatorname{\mathrm{va}}})-y\_{j,i}^{\operatorname{\mathrm{va}}}\big]^{2}. By Assumptions LABEL:sm:assumption-bounded and LABEL:sm:assumption-positive-variance, |uj,i/Vt,â„“|â‰¤8â€‹M2/v|u\_{j,i}/V\_{t,\ell}|\leq 8M^{2}/v for all jj and ii. By Bernsteinâ€™s concentration inequality (LABEL:sm:lem-Bernstein), with probability at least 1âˆ’Î´1-\delta,

|  |  |  |  |
| --- | --- | --- | --- |
|  | |Î”^t,â„“Râˆ’Î”t,â„“R|â‰¤Ïƒt,â„“Râ€‹2â€‹logâ¡(2/Î´)nt,â„“+16â€‹(M2/v)â€‹logâ¡(2/Î´)3â€‹nt,â„“.\big|\widehat{\Delta}^{R}\_{t,\ell}-\Delta^{R}\_{t,\ell}\big|\leq\sigma^{R}\_{t,\ell}\sqrt{\frac{2\log(2/\delta)}{n\_{t,\ell}}}+\frac{16(M^{2}/v)\log(2/\delta)}{3n\_{t,\ell}}. |  | (D.13) |

Substituting ([D.12](https://arxiv.org/html/2512.23596v1#A4.E12 "Equation D.12 â€£ D.5 Proof of â€£ Appendix D Proofs for and â€£ The nonstationarity-complexity tradeoff in return prediction")) and ([D.13](https://arxiv.org/html/2512.23596v1#A4.E13 "Equation D.13 â€£ D.5 Proof of â€£ Appendix D Proofs for and â€£ The nonstationarity-complexity tradeoff in return prediction")) into ([D.11](https://arxiv.org/html/2512.23596v1#A4.E11 "Equation D.11 â€£ D.5 Proof of â€£ Appendix D Proofs for and â€£ The nonstationarity-complexity tradeoff in return prediction")) completes the proof.

### D.6 Proof of LABEL:sm:thm-select-tournament-R

We first prove the following theoretical guarantee for the R2R^{2}-based comparison subroutine LABEL:sm:alg-compare-R.

###### Theorem D.1 (Near-optimal model comparison with R2R^{2}).

Let Assumptions LABEL:sm:assumption-bounded and LABEL:sm:assumption-positive-variance hold. Choose Î´âˆˆ(0,1)\delta\in(0,1) and set Î´â€²=1/(3â€‹t)\delta^{\prime}=1/(3t) in LABEL:sm:alg-tournament. With probability at least 1âˆ’Î´1-\delta, the output f^\widehat{f} of LABEL:sm:alg-compare-R satisfies

|  |  |  |  |
| --- | --- | --- | --- |
|  | maxÎ»âˆˆ[2]â¡R~t2â€‹(fÎ»)âˆ’R~t2â€‹(f^)â‰²logâ¡(t/Î´)â‹…minâ„“âˆˆ[tâˆ’1]â¡{maxtâˆ’â„“â‰¤jâ‰¤tâˆ’1â¡maxÎ»âˆˆ[2]â¡|R~j2â€‹(fÎ»)âˆ’R~t2â€‹(fÎ»)|+M2/vnt,â„“}.\max\_{\lambda\in[2]}\widetilde{R}^{2}\_{t}(f\_{\lambda})-\widetilde{R}^{2}\_{t}(\widehat{f})\lesssim\log(t/\delta)\cdot\min\_{\ell\in[t-1]}\left\{\max\_{t-\ell\leq j\leq t-1}\max\_{\lambda\in[2]}\big|\widetilde{R}^{2}\_{j}(f\_{\lambda})-\widetilde{R}^{2}\_{t}(f\_{\lambda})\big|+\frac{M^{2}/v}{\sqrt{n\_{t,\ell}}}\right\}. |  | (D.14) |

Here â‰²\lesssim hides a universal constant.

###### Proof of LABEL:sm:thm-model-comparison-R.

Following the same argument as Theorem 4.2 in HHW24, we can show that with probability at least 1âˆ’Î´1-\delta,

|  |  |  |  |
| --- | --- | --- | --- |
|  | maxÎ»âˆˆ[2]â¡R~t2â€‹(fÎ»)âˆ’R~t2â€‹(f^)\displaystyle\max\_{\lambda\in[2]}\widetilde{R}^{2}\_{t}(f\_{\lambda})-\widetilde{R}^{2}\_{t}(\widehat{f}) | â‰¤|Î”^t,â„“^Râˆ’Î”tR|\displaystyle\leq\big|\widehat{\Delta}^{R}\_{t,\widehat{\ell}}-\Delta^{R}\_{t}\big| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰²logâ¡(t/Î´)â‹…minâ„“âˆˆ[tâˆ’1]â¡{maxtâˆ’â„“â‰¤jâ‰¤tâˆ’1â¡|Î”jRâˆ’Î”tR|+v^t,â„“Rnt,â„“+M2/vnt,â„“}.\displaystyle\lesssim\log(t/\delta)\cdot\min\_{\ell\in[t-1]}\left\{\max\_{t-\ell\leq j\leq t-1}\big|\Delta^{R}\_{j}-\Delta^{R}\_{t}\big|+\frac{\widehat{v}^{R}\_{t,\ell}}{\sqrt{n\_{t,\ell}}}+\frac{M^{2}/v}{n\_{t,\ell}}\right\}. |  |

We finish the proof by noting that Ïƒt,â„“Râ‰²M2/v\sigma^{R}\_{t,\ell}\lesssim M^{2}/v and

|  |  |  |
| --- | --- | --- |
|  | |Î”jRâˆ’Î”tR|=|[R~j2â€‹(f1)âˆ’R~j2â€‹(f2)]âˆ’[R~t2â€‹(f1)âˆ’R~t2â€‹(f2)]|â‰¤2â€‹maxÎ»âˆˆ[2]â¡|R~j2â€‹(fÎ»)âˆ’R~j2â€‹(fÎ»)|.\big|\Delta^{R}\_{j}-\Delta^{R}\_{t}\big|=\left|\Big[\widetilde{R}^{2}\_{j}(f\_{1})-\widetilde{R}^{2}\_{j}(f\_{2})\Big]-\Big[\widetilde{R}^{2}\_{t}(f\_{1})-\widetilde{R}^{2}\_{t}(f\_{2})\Big]\right|\leq 2\max\_{\lambda\in[2]}\left|\widetilde{R}^{2}\_{j}(f\_{\lambda})-\widetilde{R}^{2}\_{j}(f\_{\lambda})\right|. |  |

âˆ

We can now use LABEL:sm:lem-select-tournament-reduction to translate LABEL:sm:thm-model-comparison to a theoretical guarantee for general model selection. Set â„’â€‹(f)=1âˆ’R~t2â€‹(f)\mathcal{L}(f)=1-\widetilde{R}^{2}\_{t}(f) and

|  |  |  |
| --- | --- | --- |
|  | Uâ€‹(Î´)=Câ€‹logâ¡(t/Î´)â‹…minâ„“âˆˆ[tâˆ’1]â¡{maxtâˆ’â„“â‰¤jâ‰¤tâˆ’1â¡maxÎ»âˆˆ[Î›]â¡|R~j2â€‹(fÎ»)âˆ’R~t2â€‹(fÎ»)|+M2/vnt,â„“},U(\delta)=C\log(t/\delta)\cdot\min\_{\ell\in[t-1]}\left\{\max\_{t-\ell\leq j\leq t-1}\max\_{\lambda\in[\Lambda]}\big|\widetilde{R}^{2}\_{j}(f\_{\lambda})-\widetilde{R}^{2}\_{t}(f\_{\lambda})\big|+\frac{M^{2}/v}{\sqrt{n\_{t,\ell}}}\right\}, |  |

for a sufficiently large universal constant C>0C>0. Then for any ff,

|  |  |  |
| --- | --- | --- |
|  | â„’â€‹(f)âˆ’minÎ»âˆˆ[Î›]â¡â„’â€‹(fÎ»)=maxÎ»âˆˆ[Î›]â¡R~t2â€‹(fÎ»)âˆ’R~t2â€‹(f).\mathcal{L}(f)-\min\_{\lambda\in[\Lambda]}\mathcal{L}(f\_{\lambda})=\max\_{\lambda\in[\Lambda]}\widetilde{R}^{2}\_{t}(f\_{\lambda})-\widetilde{R}^{2}\_{t}(f). |  |

By LABEL:sm:lem-select-tournament-reduction and the choice of Î´â€²\delta^{\prime}, the output f^\widehat{f} of LABEL:sm:alg-tournament satisfies that with probability at least 1âˆ’Î´1-\delta,

|  |  |  |  |
| --- | --- | --- | --- |
|  | maxÎ»âˆˆ[Î›]â¡R~t2â€‹(fÎ»)âˆ’R~t2â€‹(f^)\displaystyle\max\_{\lambda\in[\Lambda]}\widetilde{R}^{2}\_{t}(f\_{\lambda})-\widetilde{R}^{2}\_{t}(\widehat{f}) | â‰²Uâ€‹(Î´/Î›2)\displaystyle\lesssim U(\delta/\Lambda^{2}) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰²logâ¡(Î›â€‹t/Î´)â‹…minâ„“âˆˆ[tâˆ’1]â¡{maxtâˆ’â„“â‰¤jâ‰¤tâˆ’1â¡maxÎ»âˆˆ[Î›]â¡|R~j2â€‹(fÎ»)âˆ’R~t2â€‹(fÎ»)|+M2/vnt,â„“}.\displaystyle\lesssim\log(\Lambda t/\delta)\cdot\min\_{\ell\in[t-1]}\left\{\max\_{t-\ell\leq j\leq t-1}\max\_{\lambda\in[\Lambda]}\big|\widetilde{R}^{2}\_{j}(f\_{\lambda})-\widetilde{R}^{2}\_{t}(f\_{\lambda})\big|+\frac{M^{2}/v}{\sqrt{n\_{t,\ell}}}\right\}. |  |

Here â‰²\lesssim only hides universal constants.

## Appendix E Technical Lemmas

###### Lemma E.1.

Let Ïˆ:â„+â†’â„+\psi:\mathbb{R}\_{+}\to\mathbb{R}\_{+} be a sub-root function with fixed point râˆ—>0r^{\*}>0. For all A,a>0A,a>0, the function Ïˆ~â€‹(r)=Aâ€‹Ïˆâ€‹(aâ€‹r)\widetilde{\psi}(r)=A\psi(ar) is sub-root and its fixed point r~\widetilde{r} satisfies

|  |  |  |
| --- | --- | --- |
|  | min{1,Aa}2aâ€‹râˆ—â‰¤r~â‰¤max{1,Aa}2aâ€‹râˆ—.\frac{\min\{1,Aa\}^{2}}{a}r^{\*}\leq\widetilde{r}\leq\frac{\max\{1,Aa\}^{2}}{a}r^{\*}. |  |

###### Proof of [LemmaËœE.1](https://arxiv.org/html/2512.23596v1#A5.Thmlemma1 "Lemma E.1. â€£ Appendix E Technical Lemmas â€£ The nonstationarity-complexity tradeoff in return prediction").

It is easy to verify that Ïˆ~\widetilde{\psi} is subroot. We now study r~\widetilde{r}. First consider the case a=1a=1. Since Aâ€‹Ïˆâ€‹(r~)=r~A\psi(\widetilde{r})=\widetilde{r}, then Ïˆâ€‹(r~)/r~=r~/A\psi(\widetilde{r})/\sqrt{\widetilde{r}}=\sqrt{\widetilde{r}}/A. There are two cases.

* â€¢

  If r~â‰¥râˆ—\widetilde{r}\geq r^{\*}, then Ïˆâ€‹(r~)/r~â‰¤Ïˆâ€‹(râˆ—)/râˆ—=râˆ—\psi(\widetilde{r})/\sqrt{\widetilde{r}}\leq\psi(r^{\*})/\sqrt{r^{\*}}=\sqrt{r^{\*}}, so r~â‰¤A2â€‹râˆ—\widetilde{r}\leq A^{2}r^{\*}.
* â€¢

  If r~â‰¤râˆ—\widetilde{r}\leq r^{\*}, then Ïˆâ€‹(r~)/r~â‰¥Ïˆâ€‹(râˆ—)/râˆ—=râˆ—\psi(\widetilde{r})/\sqrt{\widetilde{r}}\geq\psi(r^{\*})/\sqrt{r^{\*}}=\sqrt{r^{\*}}, so r~â‰¥A2â€‹râˆ—\widetilde{r}\geq A^{2}r^{\*}.

Therefore, if A<1A<1, then A2â€‹râˆ—â‰¤r~<râˆ—A^{2}r^{\*}\leq\widetilde{r}<r^{\*}. If A>1A>1, then râˆ—<r~â‰¤A2â€‹râˆ—r^{\*}<\widetilde{r}\leq A^{2}r^{\*}. This shows that

|  |  |  |
| --- | --- | --- |
|  | min{1,A}2râˆ—â‰¤r~â‰¤max{1,A}2râˆ—.\min\{1,A\}^{2}r^{\*}\leq\widetilde{r}\leq\max\{1,A\}^{2}r^{\*}. |  |

In the general case of a>0a>0, the function râ†¦aâˆ’1â€‹Ïˆâ€‹(aâ€‹r)r\mapsto a^{-1}\psi(ar) is sub-root and has fixed point aâˆ’1â€‹râˆ—a^{-1}r^{\*}. The proof is finished by noting Ïˆ~â€‹(r)=(Aâ€‹a)â‹…aâˆ’1â€‹Ïˆâ€‹(aâ€‹r)\widetilde{\psi}(r)=(Aa)\cdot a^{-1}\psi(ar).
âˆ

###### Lemma E.2 (Bernsteinâ€™s concentration inequality).

Let {xi}i=1n\{x\_{i}\}\_{i=1}^{n} be independent random variables taking values in [a,b][a,b] almost surely. Define the average variance Ïƒ2=1nâ€‹âˆ‘i=1nvarâ¡(xi)\sigma^{2}=\frac{1}{n}\sum\_{i=1}^{n}\operatorname{{\rm var}}(x\_{i}). For any Î´âˆˆ(0,1)\delta\in(0,1), with probability at least 1âˆ’Î´1-\delta,

|  |  |  |
| --- | --- | --- |
|  | |1nâ€‹âˆ‘i=1n(xiâˆ’ğ”¼â€‹xi)|â‰¤Ïƒâ€‹2â€‹logâ¡(2/Î´)n+2â€‹(bâˆ’a)â€‹logâ¡(2/Î´)3â€‹n.\bigg|\frac{1}{n}\sum\_{i=1}^{n}(x\_{i}-\mathbb{E}x\_{i})\bigg|\leq\sigma\sqrt{\frac{2\log(2/\delta)}{n}}+\frac{2(b-a)\log(2/\delta)}{3n}. |  |

###### Proof of [LemmaËœE.2](https://arxiv.org/html/2512.23596v1#A5.Thmlemma2 "Lemma E.2 (Bernsteinâ€™s concentration inequality). â€£ Appendix E Technical Lemmas â€£ The nonstationarity-complexity tradeoff in return prediction").

Inequality (2.10) in BLM13 implies that for any tâ‰¥0t\geq 0,

|  |  |  |
| --- | --- | --- |
|  | â„™â€‹(1nâ€‹âˆ‘i=1n(xiâˆ’ğ”¼â€‹xi)>t)â‰¤expâ¡(âˆ’nâ€‹t2/2Ïƒ2+(bâˆ’a)â€‹t/3).\mathbb{P}\bigg(\frac{1}{n}\sum\_{i=1}^{n}(x\_{i}-\mathbb{E}x\_{i})>t\bigg)\leq\exp\left(-\frac{nt^{2}/2}{\sigma^{2}+(b-a)t/3}\right). |  |

Fix Î´âˆˆ(0,1)\delta\in(0,1). Then,

|  |  |  |  |
| --- | --- | --- | --- |
|  |  | expâ¡(âˆ’nâ€‹t2/2Ïƒ2+(bâˆ’a)â€‹t/3)â‰¤Î´\displaystyle\quad\exp\left(-\frac{nt^{2}/2}{\sigma^{2}+(b-a)t/3}\right)\leq\delta |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‡”\displaystyle\Leftrightarrow | nâ€‹t22â‰¥Ïƒ2â€‹logâ¡(1/Î´)+tâ€‹(bâˆ’a)â€‹logâ¡(1/Î´)3\displaystyle\quad\frac{nt^{2}}{2}\geq\sigma^{2}\log(1/\delta)+\frac{t(b-a)\log(1/\delta)}{3} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‡”\displaystyle\Leftrightarrow | n2â€‹(tâˆ’(bâˆ’a)â€‹logâ¡(1/Î´)3â€‹n)2â‰¥Ïƒ2â€‹logâ¡(1/Î´)+n2â€‹((bâˆ’a)â€‹logâ¡(1/Î´)3â€‹n)2\displaystyle\quad\frac{n}{2}\bigg(t-\frac{(b-a)\log(1/\delta)}{3n}\bigg)^{2}\geq\sigma^{2}\log(1/\delta)+\frac{n}{2}\bigg(\frac{(b-a)\log(1/\delta)}{3n}\bigg)^{2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‡\displaystyle\Leftarrow | (tâˆ’(bâˆ’a)â€‹logâ¡(1/Î´)3â€‹n)2â‰¥(Ïƒâ€‹2â€‹logâ¡(1/Î´)n+(bâˆ’a)â€‹logâ¡(1/Î´)3â€‹n)2\displaystyle\quad\bigg(t-\frac{(b-a)\log(1/\delta)}{3n}\bigg)^{2}\geq\bigg(\sigma\sqrt{\frac{2\log(1/\delta)}{n}}+\frac{(b-a)\log(1/\delta)}{3n}\bigg)^{2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‡\displaystyle\Leftarrow | tâ‰¥Ïƒâ€‹2â€‹logâ¡(1/Î´)n+2â€‹(bâˆ’a)â€‹logâ¡(1/Î´)3â€‹n.\displaystyle\quad t\geq\sigma\sqrt{\frac{2\log(1/\delta)}{n}}+\frac{2(b-a)\log(1/\delta)}{3n}. |  |

Hence,

|  |  |  |
| --- | --- | --- |
|  | â„™â€‹(1nâ€‹âˆ‘i=1n(xiâˆ’ğ”¼â€‹xi)>Ïƒâ€‹2â€‹logâ¡(1/Î´)n+2â€‹(bâˆ’a)â€‹logâ¡(1/Î´)3â€‹n)â‰¤Î´.\mathbb{P}\left(\frac{1}{n}\sum\_{i=1}^{n}(x\_{i}-\mathbb{E}x\_{i})>\sigma\sqrt{\frac{2\log(1/\delta)}{n}}+\frac{2(b-a)\log(1/\delta)}{3n}\right)\leq\delta. |  |

Replacing each xix\_{i} by âˆ’xi-x\_{i} gives bounds on the lower tail and the absolute deviation.
âˆ

###### Lemma E.3.

For all a,a1,â€¦,anâ‰¥0a,a\_{1},...,a\_{n}\geq 0 and b,b1,â€¦,bn>0b,b\_{1},...,b\_{n}>0, it holds that

|  |  |  |
| --- | --- | --- |
|  | |abâˆ’âˆ‘i=1naiâˆ‘i=1nbi|â‰¤maxiâˆˆ[n]â¡|abâˆ’aibi|.\left|\frac{a}{b}-\frac{\sum\_{i=1}^{n}a\_{i}}{\sum\_{i=1}^{n}b\_{i}}\right|\leq\max\_{i\in[n]}\left|\frac{a}{b}-\frac{a\_{i}}{b\_{i}}\right|. |  |

###### Proof of [LemmaËœE.3](https://arxiv.org/html/2512.23596v1#A5.Thmlemma3 "Lemma E.3. â€£ Appendix E Technical Lemmas â€£ The nonstationarity-complexity tradeoff in return prediction").

This is due to

|  |  |  |  |
| --- | --- | --- | --- |
|  | |abâˆ’âˆ‘i=1naiâˆ‘i=1nbi|=|abâˆ’âˆ‘i=1nbiâˆ‘j=1nbjâ‹…aibi|\displaystyle\left|\frac{a}{b}-\frac{\sum\_{i=1}^{n}a\_{i}}{\sum\_{i=1}^{n}b\_{i}}\right|=\left|\frac{a}{b}-\sum\_{i=1}^{n}\frac{b\_{i}}{\sum\_{j=1}^{n}b\_{j}}\cdot\frac{a\_{i}}{b\_{i}}\right| | =|âˆ‘i=1nbiâˆ‘j=1nbjâ‹…(abâˆ’aibi)|\displaystyle=\left|\sum\_{i=1}^{n}\frac{b\_{i}}{\sum\_{j=1}^{n}b\_{j}}\cdot\left(\frac{a}{b}-\frac{a\_{i}}{b\_{i}}\right)\right| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤âˆ‘i=1nbiâˆ‘j=1nbjâ‹…|abâˆ’aibi|â‰¤maxiâˆˆ[n]â¡|abâˆ’aibi|.\displaystyle\leq\sum\_{i=1}^{n}\frac{b\_{i}}{\sum\_{j=1}^{n}b\_{j}}\cdot\left|\frac{a}{b}-\frac{a\_{i}}{b\_{i}}\right|\leq\max\_{i\in[n]}\left|\frac{a}{b}-\frac{a\_{i}}{b\_{i}}\right|. |  |

This finishes the proof.
âˆ

## Appendix F Additional Experiment Details

### F.1 Summary Statistics of the Dataset

We now provide an overview of the longâ€“short firm characteristic covariates used in the analysis, their time-series behavior, and cross-sectional dependence as well as a brief summary of the stochastic discount factor (SDF) and decile portfolios from CPZ24. Recall that all of the long-short characteristic portfolios are computed at the daily frequency, for the subsequent summary plots, we have aggregated them into the monthly frequency using within-month averages, in line with the standard practice of aligning signals with monthly returns. The monthly aggregation smooths out day-to-day noise and highlights the economically relevant medium-horizon variations.

##### Monthly Evolution of Covariates.

FigureÂ [7](https://arxiv.org/html/2512.23596v1#A6.F7 "Figure 7 â€£ Monthly Evolution of Covariates. â€£ F.1 Summary Statistics of the Dataset â€£ Appendix F Additional Experiment Details â€£ The nonstationarity-complexity tradeoff in return prediction") displays the time series of monthly mean values for the twelve most volatile covariates, ranked by their total-sample standard deviation. The figure highlights that variables such as retvol, mom12m, and baspread exhibit pronounced month-to-month fluctuations, while others such as turn and operprof remain relatively stable. These series reveal persistent heteroskedasticity and regime shifts over time, particularly during market dislocations such as the early 2000s and 2008 crises.

Figure 7: Monthly Means of the 12 Most Volatile Covariates.

![Refer to caption](x55.png)


Each panel shows the monthly mean of a longâ€“short firm characteristic.
Covariates are ranked by total-sample volatility.
The time series reveal which characteristics exhibit the greatest month-to-month variation and long-run persistence.

##### Distributional and Correlation Structure.

FigureÂ [8](https://arxiv.org/html/2512.23596v1#A6.F8 "Figure 8 â€£ Distributional and Correlation Structure. â€£ F.1 Summary Statistics of the Dataset â€£ Appendix F Additional Experiment Details â€£ The nonstationarity-complexity tradeoff in return prediction") summarizes the time-series distributions of the same twelve covariates using standardized (z-scored) monthly values. The median, interquartile range, and whiskers capture the magnitude and symmetry of fluctuations across time. Most variables display near-zero median values but differ in dispersion and tail behavior, consistent with heterogeneous economic mechanisms underlying each characteristic.

Figure 8: Distributions (Boxplots) of Standardized Monthly Covariates.

![Refer to caption](x56.png)


Z-scored monthly series for the twelve most volatile covariates. The figure compares dispersion and tail behavior across characteristics, highlighting differences in amplitude and symmetry.

For example, variables such as retvol, baspread, and mom12m exhibit wide interquartile ranges and thick tails, suggesting that these signals experience substantial time variation and occasional extreme realizations. In contrast, variables such as turn and operprof have narrower boxes, implying greater stability through time.

Each distribution is constructed by pooling monthly observations over the entire sample period for that specific covariate. This provides a concise view of the temporal heterogeneity and persistence of each characteristic after accounting for scale differences. The figure thus complements the time-series plots in FigureÂ [7](https://arxiv.org/html/2512.23596v1#A6.F7 "Figure 7 â€£ Monthly Evolution of Covariates. â€£ F.1 Summary Statistics of the Dataset â€£ Appendix F Additional Experiment Details â€£ The nonstationarity-complexity tradeoff in return prediction") by providing a scale-free summary of long-run variability and skewness in the underlying longâ€“short characteristics.

The pairwise dependence structure among the top thirty covariates (in terms of volatility) is visualized in FigureÂ [9](https://arxiv.org/html/2512.23596v1#A6.F9 "Figure 9 â€£ Distributional and Correlation Structure. â€£ F.1 Summary Statistics of the Dataset â€£ Appendix F Additional Experiment Details â€£ The nonstationarity-complexity tradeoff in return prediction"). The heatmap reveals clusters of strongly correlated signals, such as volatility-related measures (retvol, idiovol, roavol) and liquidity-related variables (baspread, zerotrade, turn). The presence of such correlation blocks indicates there could be shared economic channels.

Figure 9: Correlation Heatmap of the 30 Most Volatile Covariates.

![Refer to caption](x57.png)


The matrix shows pairwise Pearson correlations between the thirty most volatile monthly covariates.
Red indicates positive correlation and blue indicates negative correlation.
Distinct blocks suggest clusters of related characteristics.

##### Time-Varying Volatility of Covariates.

FigureÂ [10](https://arxiv.org/html/2512.23596v1#A6.F10 "Figure 10 â€£ Time-Varying Volatility of Covariates. â€£ F.1 Summary Statistics of the Dataset â€£ Appendix F Additional Experiment Details â€£ The nonstationarity-complexity tradeoff in return prediction") plots the 12-month rolling standard deviation of the twelve most volatile covariates. Unlike FigureÂ [7](https://arxiv.org/html/2512.23596v1#A6.F7 "Figure 7 â€£ Monthly Evolution of Covariates. â€£ F.1 Summary Statistics of the Dataset â€£ Appendix F Additional Experiment Details â€£ The nonstationarity-complexity tradeoff in return prediction"), which ranks variables by overall volatility, the rolling volatility tracks how the variability of each covariate evolves through time. Periods such as the dot-com bubble and the global financial crisis correspond to distinct spikes in volatility across multiple signals, indicating that the informational strength and instability of certain factors are regime-dependent.

Figure 10: Twelve-Month Rolling Volatility of the Most Volatile Covariates.

![Refer to caption](x58.png)


The panels show rolling standard deviations computed using a 12-month moving window for each of the twelve most volatile covariates.
This highlights temporal variation in the stability and amplitude of the long-short signals.

##### SDF and Decile Portfolios.

Finally, FigureÂ [11](https://arxiv.org/html/2512.23596v1#A6.F11 "Figure 11 â€£ SDF and Decile Portfolios. â€£ F.1 Summary Statistics of the Dataset â€£ Appendix F Additional Experiment Details â€£ The nonstationarity-complexity tradeoff in return prediction") plots the monthly time series of the stochastic discount factor (SDF) alongside the ten equal-weighted decile portfolios sorted by the underlying characteristic. The decile portfolios exhibit substantial comovement, with the SDF (shown in black) fluctuating more smoothly. This figure provides a benchmark for comparing the magnitude and temporal alignment of the SDF with characteristic-sorted portfolio returns, and serves as a diagnostic for whether the constructed SDF captures systematic components of asset pricing variation.

Figure 11: Monthly SDF and Decile Portfolios.

![Refer to caption](x59.png)


The black line represents the stochastic discount factor (SDF), while the colored lines correspond to ten equal-weighted decile portfolios sorted on firm characteristics.
The comovement between the SDF and the characteristic-sorted portfolios provides an initial indication of factor relevance.

### F.2 Experiment Details for LABEL:sm:sec-tradeoff-empirics

In this section, we given more details on our empirical investigations of the nonstationarity-complexity tradeoff in LABEL:sm:sec-tradeoff-empirics.

The three prediction models, along with their hyperparameters, are: (1) a linear model trained by Ridge regression using the most recent 6464 months of data, with Î±=1\alpha=1, (2) a random forest trained on the most recent 6464 months of data, with ntree=200n\_{\texttt{tree}}=200 and dmax=5d\_{\max}=5, and (3) a random forest trained on all historical data, with ntree=200n\_{\texttt{tree}}=200 and dmax=5d\_{\max}=5.

In each month tt, we construct training data by randomly subsampling 4/54/5 of the observations ğ’Ÿj\mathcal{D}\_{j} in each previous month jâˆˆ[tâˆ’1]j\in[t-1]. The process is repeated 2020 times with independent random seeds. We then average the out-of-sample R2R^{2} over the 2020 random seeds, which are then used to produce the figures in LABEL:sm:sec-tradeoff-empirics.

## Appendix G Additional Experiment Results

### G.1 Experiment Results with the Standard R2R^{2} Metric

In LABEL:sm:sec-experiments, we evaluated predictive performance using the zero-benchmark R2R^{2} to avoid the noise inherent in historical mean estimation. For completeness, this section reports the corresponding results using the standard out-of-sample R2R^{2} metric, which benchmarks model performance against the historical sample mean. Qualitatively, the relative performance among models remains consistent with the the observations in LABEL:sm:sec-experiments: our adaptive algorithm ATOMS continues to outperform fixed-window benchmarks. Quantitatively, we observe that the standard R2R^{2} values are generally lower than their zero-benchmark counterparts.

LABEL:sm:tab:oos\_r2\_industry\_time-standardR2 presents out-of-sample standard R2R^{2} values of ATOMS and baselines across distinct economic regimes, serving as the counterpart to LABEL:sm:tab:oos\_r2\_industry\_time in the main text.

Table 6: OOS Standard R2R^{2} Averages Across Industries by Time Period.

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
| Method | Full OOS Period | Recessions | | |
| Gulf War | 2001 Recession | Financial Crisis |
| ATOMS | 0.0410.041 | âˆ’0.019-0.019 | 0.1150.115 | 0.0390.039 |
| Fixed-valâ€‹(32)\texttt{Fixed-val}(32) | 0.0130.013 | âˆ’0.038-0.038 | 0.0850.085 | âˆ’0.003-0.003 |
| Fixed-valâ€‹(512)\texttt{Fixed-val}(512) | 0.0340.034 | âˆ’0.080-0.080 | 0.1070.107 | 0.0370.037 |
| Fixed-CV | 0.0260.026 | âˆ’0.056-0.056 | 0.0600.060 | 0.0120.012 |

This table reports OOS standard R2R^{2} averages for return prediction models across all 17 industry portfolios. Full OOS Period refers to OOS period covering 01/1990âˆ¼\sim11/2016. Columns report OOS R2R^{2} averages across all industries and highlight this metric during three recessions, as documented in [NBER Business Cycle Dating](https://www.nber.org/research/business-cycle-dating):

* â€¢

  the 1990 Gulf War recession (06/1990âˆ¼\sim10/1990);
* â€¢

  the 2001 Recession of dot-com bubble burst and the 9/11 attack (05/2001âˆ¼\sim10/2001);
* â€¢

  the Financial Crisis led by defaults of subprime mortgages (11/2007âˆ¼\sim06/2009).

That is, the OOS performance in Gulf War column focuses on model performance comparisons exclusively in the out-of-sample period of 06/1990âˆ¼\sim10/1990. All values are calculated using monthly return data.

LABEL:sm:fig-boxplot-standardR2 gives a box plot of the OOS standard R2R^{2} of ATOMS and the fixed-window baselines over the 17 industry portfolios, mirroring LABEL:sm:fig-boxplot.

Figure 12: Box Plot of OOS Standard R2R^{2} of ATOMS and Baselines for 1717 Industry Portfolios.

![Refer to caption](x60.png)


This figure describes the distribution of each methodâ€™s OOS R2R^{2}. Each box corresponds to all industries and all years in our OOS horizon.

Finally, LABEL:sm:fig-industry-yearly-standardR2 plots the annual out-of-sample R2R^{2} for the 1717 industry portfolios, paralleling LABEL:sm:fig-industry-yearly.

Figure 13: Annual OOS Standard R2R^{2} of Different Approaches for 1717 Industry Portfolios.

![Refer to caption](x61.png)

![Refer to caption](x62.png)

![Refer to caption](x63.png)

![Refer to caption](x64.png)

![Refer to caption](x65.png)

![Refer to caption](x66.png)

![Refer to caption](x67.png)

![Refer to caption](x68.png)

![Refer to caption](x69.png)

![Refer to caption](x70.png)

![Refer to caption](x71.png)

![Refer to caption](x72.png)

![Refer to caption](x73.png)

![Refer to caption](x74.png)

![Refer to caption](x75.png)

![Refer to caption](x76.png)

![Refer to caption](x77.png)

This figure reports the annual OOS standard R2R^{2} of our adaptive model selection algorithm ATOMS (black dashed line with Ã—\timesâ€™s), as well as the fixed-window baselines Fixed-valâ€‹(32)\texttt{Fixed-val}(32) (blue â–¼\blacktriangledownâ€™s), Fixed-valâ€‹(128)\texttt{Fixed-val}(128) (orange â– \blacksquareâ€™s), and Fixed-valâ€‹(512)\texttt{Fixed-val}(512) (red), which use the last 3232, last 128128 and all months of validation data. The title in each subfigure is Kenneth Frenchâ€™s acronym for each industry. For the full names of these industries, please refer to Table LABEL:sm:tab-industry-name-mapping.

### G.2 Experiment Results for ATOMS-R2

In this section, we present experiment results for the R2R^{2}-based model selection method ATOMS-R2 developed in LABEL:sm:sec-select-R2. We set its hyperparameters Î´â€²=0.1\delta^{\prime}=0.1 and M2=5M^{2}=5. Its performance is similar to the MSE-based approach ATOMS. We will report results for the R2R^{2} metric that benchmarks against a zero forecast.

In LABEL:sm:fig-boxplot-supp, we give a box plot of the overall out-of-sample R2R^{2} of ATOMS-R2 along with ATOMS and the fixed-window benchmarks across the 1717 industries. In LABEL:sm:fig-industry-yearly-supp, we compare the annual out-of-sample R2R^{2} of ATOMS-R2 and ATOMS for the 1717 industries. Again, ATOMS-R2 and ATOMS have similar performance.

Figure 14: Box Plot of Out-of-Sample R2R^{2} of ATOMS-R2, ATOMS and Baselines for 1717 Industry Portfolios.

![Refer to caption](x78.png)


This figure describes the distribution of each methodâ€™s OOS R2R^{2}. Each box corresponds to all industries and all years in our OOS horizon.




Figure 15: Annual Out-of-Sample R2R^{2} of ATOMS-R2 and ATOMS for 1717 Industry Portfolios.

![Refer to caption](x79.png)

![Refer to caption](x80.png)

![Refer to caption](x81.png)

![Refer to caption](x82.png)

![Refer to caption](x83.png)

![Refer to caption](x84.png)

![Refer to caption](x85.png)

![Refer to caption](x86.png)

![Refer to caption](x87.png)

![Refer to caption](x88.png)

![Refer to caption](x89.png)

![Refer to caption](x90.png)

![Refer to caption](x91.png)

![Refer to caption](x92.png)

![Refer to caption](x93.png)

![Refer to caption](x94.png)

![Refer to caption](x95.png)

This figure reports the annual OOS R2R^{2} of our adaptive model selection algorithms ATOMS-R2 (orange line with â– \blacksquareâ€™s) and ATOMS (black dashed line with Ã—\timesâ€™s). The title in each subfigure is Kenneth Frenchâ€™s acronym for each industry. For the full names of these industries, please refer to Table LABEL:sm:tab-industry-name-mapping.