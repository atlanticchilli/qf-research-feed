---
authors:
- Zhenyu Gao
- Wenxi Jiang
- Yutong Yan
doc_id: arxiv:2512.23847v1
family_id: arxiv:2512.23847
is_current: true
taxonomy:
  alpha_families: []
  asset_classes: []
  horizons: []
  themes: []
title: A Test of Lookahead Bias in LLM Forecasts
url_abs: http://arxiv.org/abs/2512.23847v1
url_html: https://arxiv.org/html/2512.23847v1
venue: arXiv q-fin
version: 1
year: 2025
---


Zhenyu Gao, Wenxi Jiang, Yutong Yan
Gao, Jiang, and Yan are at the Department of Finance, CUHK Business School, The Chinese University of Hong Kong. For helpful comments, we thank Chengwang Liao, Ron Kaniel, and seminar participants at CUHK. Our correspondences are gaozhenyu@baf.cuhk.edu.hk, wenxijiang@baf.cuhk.edu.hk, and yutong.yan@link.cuhk.edu.hk, respectively. First draft: December 2025.

(December 29, 2025)

###### Abstract

We develop a statistical test to detect lookahead bias in economic forecasts generated by large language models (LLMs). Using state‚Äëof‚Äëthe‚Äëart pre‚Äëtraining data detection techniques, we estimate the likelihood that a given prompt appeared in an LLM‚Äôs training corpus, a statistic we term Lookahead Propensity (LAP). We formally show that a positive correlation between LAP and forecast accuracy indicates the presence and magnitude of lookahead bias, and apply the test to two forecasting tasks: news headlines predicting stock returns and earnings call transcripts predicting capital expenditures. Our test provides a cost-efficient, diagnostic tool for assessing the validity and reliability of LLM-generated forecasts.

## 1 Introduction

Large language models (LLMs) are increasingly used by researchers and practitioners to generate economic and financial forecasts. In a typical setting, an LLM such as ChatGPT is prompted with textual inputs‚Äîsuch as news headlines or earnings call transcripts‚Äîto predict future firm‚Äëlevel outcomes, including stock returns or capital expenditures (e.g., lopez2023can; jha2024chatgpt). A growing literature finds that LLM‚Äëbased forecasts often outperform both conventional econometric models and standard machine‚Äëlearning techniques.

Despite these promising results, their interpretation requires caution. When pre‚Äëtrained, off‚Äëthe‚Äëshelf LLMs are used, the evaluation effectively occurs in‚Äësample: the model may have been exposed to overlapping or closely related data during training, making it difficult to distinguish genuine reasoning from simple recall. This chance of lookahead bias is especially high when forecasting variables such as stock returns that are widely reported in public sources likely included in the model‚Äôs training corpus.

Several prompting strategies attempt to mitigate this issue, such as masking firm identifiers, anonymizing contextual information, or limiting access to temporal cues (sarkar2024storieslm; engelberg2025entity; wu2025anonymization). Yet their effectiveness remains mixed. Ideally, one would evaluate forecasts strictly on out‚Äëof‚Äësample data unseen by the LLM. However, for widely used large‚Äëscale models, the available out‚Äëof‚Äësample horizon is short, limiting statistical power.
Retraining such models on year-by-year corpora, a clean alternative, is computationally prohibitive at current scales.

An additional complication, often overlooked, is that lookahead bias is task‚Äëspecific rather than an inherent characteristic across LLMs. As shown by carlini2022quantifying, memorization depends systematically on factors such as the nature and public visibility of the input text and target variable, model size and architecture, and prompt design. These variations underscore the need for a systematic and generalizable test to detect lookahead bias in LLM forecasts on a case‚Äëby‚Äëcase basis.

To address this challenge, we develop a statistical test that determines whether an LLM‚Äôs forecasts arise from genuine reasoning or from training data leakage. Building on state‚Äëof‚Äëthe‚Äëart membership inference attack (MIA) techniques, the proposed method is cost‚Äëefficient and requires neither model retraining nor access to proprietary training data. It offers researchers and practitioners a practical diagnostic tool for assessing the reliability and real‚Äëworld validity of LLM‚Äëgenerated forecasts.

We first introduce the Lookahead Propensity (LAP), a measure of how likely the text in a prompt appeared in an LLM‚Äôs training data. For each prompt, we compute the model‚Äëassigned probability of every token conditional on its preceding tokens. We define LAP as the mean token probabilities of the bottom KK% of tokens, i.e., those tokens with the lowest predicted probabilities.
This focus on uncommon tokens is key: frequent words such as ‚Äúthe‚Äù or ‚Äúand‚Äù are assigned high probabilities regardless of prior exposure, whereas rare tokens carry more information about whether the text was previously seen.
The intuition is that unseen prompts tend to contain more low-probability (outlier) tokens under the model, whereas seen prompts are less likely to have such extreme outliers, and those tokens instead receive higher probability.
Accordingly, a higher LAP indicates greater model familiarity and a higher likelihood of training‚Äëdata overlap.111Following shi2023detecting, we set K%K\% as 20%.

The LAP corresponds directly to the MIN‚ÄëK% PROB statistic developed in the membership inference attack (MIA) literature (e.g., shokri2017membership; carlini2021extracting; carlini2022membership; shi2023detecting; cheng2024dated). The MIA literature proposes various measures to detect whether a given text was included in an LLM‚Äôs training data; among them, MIN‚ÄëK% PROB statistic stands out as a robust and benchmark‚Äëlevel baseline, demonstrating consistently superior performance across reported evaluation settings (shi2023detecting). It has been successfully applied in several LLM engineering contexts, including detecting copyrighted material and auditing the effectiveness of machine‚Äëunlearning procedures. See Section [2.1](https://arxiv.org/html/2512.23847v1#S2.SS1 "2.1 Lookahead Propensity (LAP) ‚Ä£ 2 Lookahead Bias Detection ‚Ä£ A Test of Lookahead Bias in LLM Forecasts") for a summary.
Our contribution lies in applying this measure to prompt inputs used for economic forecasting. We develop a formal statistical test to detect lookahead bias in LLM-based forecasts, which is a central concern for empirical economists.

In Section [2.2](https://arxiv.org/html/2512.23847v1#S2.SS2 "2.2 Econometric Framework ‚Ä£ 2 Lookahead Bias Detection ‚Ä£ A Test of Lookahead Bias in LLM Forecasts"), we formally develop our test in an econometric framework. The main proposition implies that if forecast accuracy is correlated with LAP, such a relationship indicates lookahead bias rather than genuine predictive reasoning. Let us first illustrate the intuition using the following example. We prompt the model with:

> *‚ÄúHere is a piece of news on July 28, 2020: Kodak Triples on Loan to Make Covid-19 Drug Ingredients. Do you think this is good or bad for the stock price in the short term?‚Äù*

Without access to future information, the LLM would need to reason about how government pharmaceutical contracts might influence firm value. However, if its training corpus contains subsequent coverage of the same event, the task effectively becomes one of recall rather than reasoning. Indeed, on the following day, July 29, a major news outlet published the headline: ‚ÄúKodak‚Äôs stock rose so fast it tripped 20 circuit breakers in a single day.‚Äù The article body reported:

> *‚ÄúOn Tuesday, President Donald Trump announced the company would receive a $765 million loan to launch Kodak Pharmaceuticals ‚Ä¶ Following a more than 200% jump in Tuesday trading, the rally continued on Wednesday and the shares ended up 318%.‚Äù*

When presented with the July 28 headline, the LLM may recall textual patterns from the July 29 report, such as ‚Äúshares skyrocketed,‚Äù ‚Äútripped 20 circuit breakers,‚Äù or ‚Äú318 percent increase‚Äù, and consequently infer that the news is positive for the stock price. In this instance, what appears to be predictive ability actually reflects information recall from the training data rather than reasoning over unseen information. Conceptually, a model that achieves high predictive accuracy only when the prompt contains previously seen text is analogous to a student excelling only on exam questions encountered during practice. This pattern indicates memorization rather than genuine comprehension.
A high LAP of the input text, therefore, indicates that the LLM has likely been exposed to the realized outcome.

In our econometric framework, the LLM‚Äôs prediction, denoted by ^\hat{\mu}, consists of two indistinguishable components: genuine reasoning and potential memorization. A standard forecast‚Äëaccuracy regression of the realized outcome Y{Y} on ^\hat{\mu} cannot disentangle these sources of predictability. Our proposed test augments the regression with an interaction term between ^\hat{\mu} and LAP. Given that LAP serves as a validated proxy for memorization in the MIA literature, we prove that a positive coefficient on the interaction term is equivalent to the presence of lookahead bias. In other words, if forecast accuracy systematically increases as the input text becomes more familiar to the model, the predictive signal is driven at least partially by memory rather than reasoning.

In Section [3](https://arxiv.org/html/2512.23847v1#S3 "3 Result ‚Ä£ A Test of Lookahead Bias in LLM Forecasts"), we implement the LAP test on two forecasting exercises using Llama‚Äë3.3, an open‚Äësource LLM released by Meta in December 2024. The first exercise adopts the setup in lopez2023can, which uses firm‚Äëspecific news headlines to predict next‚Äëday stock returns. Our results indicate that memorization substantially amplifies the apparent predictive power of LLM‚Äëgenerated forecasts. Specifically, the baseline regression shows that a one‚Äìstandard-deviation increase in LLM prediction predicts a 0.197% higher next-day return. By comparison, a one‚Äìstandard-deviation increase in LAP increases the marginal effect of LLM prediction on next-day stock return by 0.077%, which is about 37% of the standalone LLM effect in the baseline test. Moreover, the stronger predictability observed among small‚Äëcap stocks is also largely due to LAP amplification, reflecting stronger lookahead bias.

The amplification effect remains robust when controlling for two confidence measures: the first-token conditional probability of the LLM‚Äôs response (chen2024out) and the model‚Äôs self-reported confidence level. This robustness indicates that memorization operates through a mechanism distinct from the model‚Äôs internal measure of predictive certainty.

Finally, we conduct the LAP test in a genuinely out-of-sample period using Llama-2, from September 2023 to December 2024. In the period after the release date, no lookahead bias should arise. We first show that the coefficient on the interaction between LAP and the model‚Äôs prediction indeed becomes statistically insignificant. Next, we conduct a bootstrap analysis. The bootstrap distribution based on out‚Äëof‚Äësample data is clearly separated from the in‚Äësample estimate, resulting in a one‚Äësided bootstrap pp-value of 0.033 for the in‚Äësample interaction term. These findings further suggest that the in‚Äësample predictability is largely driven by look‚Äëahead bias.

Our second forecasting exercise replicates the analysis in jha2024chatgpt, which uses firms‚Äô earnings conference call transcripts to predict subsequent capital expenditures. Consistent with our previous findings, LAP significantly amplifies the relationship between LLM predictions and future investment. The model exhibits stronger predictive performance when analyzing transcripts that contain linguistic patterns familiar from its training data, suggesting that apparent foresight arises at least in part from memorization rather than genuine inference. A one‚Äìstandard-deviation increase in LLM prediction is associated with a 0.324% higher future two-quarter capital expenditure ratio (scaled by total assets). By comparison, a one‚Äìstandard-deviation increase in LAP increases the marginal effect of LLM prediction on future CapEx ratio by 0.149%, which is about 19% of the standalone LLM effect in the baseline.

The two exercises demonstrate that at least part of the LLM‚Äôs apparent predictive power stems from memorization rather than genuine reasoning, underscoring the need for caution when applying LLMs to economic forecasting. However, this finding does not imply that forecasts by LLMs should be avoided altogether. The extent of lookahead bias is task‚Äëspecific, not a universal property of LLMs. It depends on factors such as the nature of the input text and target variable, model architecture, and prompt design. The LAP measure and the statistical test we propose provide a simple, cost‚Äëeffective diagnostic for identifying cases in which model outputs are influenced by memorization rather than analytical reasoning. This distinction is crucial for ensuring the validity of inferences drawn from LLM‚Äëbased forecasts, especially in backtesting exercises that rely on historical data potentially present in the model‚Äôs training corpus. As LLMs become increasingly integrated into empirical finance and economic research, systematically distinguishing between memory and reasoning will be essential for enhancing the credibility of their predictions.

### Related Literature

A growing body of work applies LLMs to extract economically meaningful signals from financial text. Some studies use corporate disclosures and earnings calls as model inputs to predict firm actions (e.g., cao2023talk; jha2024chatgpt). Others apply LLMs to financial news to forecast stock returns (e.g., chen2022expected; lopez2023can) and macroeconomic variables (e.g., hansen2024can; bybee2023ghost). chen2024out analyze how LLMs‚Äô conditional probabilities affect model predictability.

Meanwhile, recent studies have highlighted the potential for lookahead bias in LLM‚Äëbased predictions (e.g., sarkar2024lookahead). lopez2025memorization detect LLM memorization by comparing the model‚Äôs recalled economic variables with their realized values. They find that LLMs can sometimes perfectly reproduce exact historical values from their training period, revealing that strong forecasting performance on pre-cutoff data may reflect memorization.

Several studies also explore strategies to mitigate such bias. engelberg2025entity introduce an entity‚Äëneutering prompting approach, where identifying details such as firm names and dates are removed by LLMs to reduce recognition while retaining informational content. Another line of work develops self-developed models trained under controlled information sets (e.g., sarkar2024storieslm; he2025chronologically). For example, he2025chronologically train leak‚Äëfree language models‚ÄîChronoBERT (149M) and ChronoGPT (1.5B)‚Äîusing data available only up to each year‚Äëend, and confirm that the findings of chen2022expected involve only modest look‚Äëahead bias.

Our study complements the existing literature by taking a different approach. Rather than mitigating lookahead bias, we develop a statistical test to assess its likelihood and magnitude. Since lookahead bias in LLM predictions is inherently task‚Äëspecific, our test provides a more generalizable and adaptable diagnostic tool. Moreover, the proposed LAP test is cost‚Äëefficient, requiring neither model retraining nor access to proprietary training data.

This paper also builds on the engineering literature on memorization in language models. carlini2021extracting demonstrate that LLMs can memorize portions of their training data, complicating applications that require strict train‚Äìtest separation or date‚Äëspecific cutoffs. Subsequent studies, including carlini2022quantifying and cheng2024dated, show that token‚Äëlevel likelihoods and perplexity are effective indicators of memorization.
The MIA literature‚Äîreviewed comprehensively in Section [2.1](https://arxiv.org/html/2512.23847v1#S2.SS1 "2.1 Lookahead Propensity (LAP) ‚Ä£ 2 Lookahead Bias Detection ‚Ä£ A Test of Lookahead Bias in LLM Forecasts")‚Äîhas further developed a suite of quantitative measures and validation procedures to determine whether a given text is likely part of an LLM‚Äôs training corpus.
Our contribution lies in extending these insights by applying MIA techniques to develop a statistical test for detecting lookahead bias in LLM‚Äëbased economic forecasts.

## 2 Lookahead Bias Detection

### 2.1 Lookahead Propensity (LAP)

We introduce the statistical properties of language models, the MIA literature, and LAP construction in this subsection.

#### Language Model

A large language model (LLM) is a probabilistic framework that predicts the next token in a text sequence.222In natural language processing (NLP), a token is the smallest unit of text that carries meaning, such as a word, subword, or character, depending on the tokenization scheme.
Given a sequence of observed tokens w‚â§n‚àí1‚âî(w1,w2,‚Ä¶,wn‚àí1)w\_{\leq n-1}\coloneqq(w\_{1},w\_{2},\ldots,w\_{n-1}), an LLM parameterized by estimates the likelihood of the next token wnw\_{n} through the conditional probability

|  |  |  |
| --- | --- | --- |
|  | P‚Äã(wn‚à£w‚â§n‚àí1)=P‚Äã(wn‚à£w1,w2,‚Ä¶,wn‚àí1).P\!\left(w\_{n}\mid w\_{\leq n-1}\right)=P\!\left(w\_{n}\mid w\_{1},w\_{2},\ldots,w\_{n-1}\right). |  |

LLMs learn these conditional probabilities by training on massive text corpora, adjusting internal parameters to maximize the likelihood of observed sequences.

Through repeated exposure to billions of examples, the model internalizes statistical patterns‚Äîstrengthening associations between words, phrases, and events that frequently co-occur. This process does not involve ‚Äúunderstanding‚Äù in the human sense; rather, LLMs build a compressed representation of co-occurrence patterns across language. When presented with a prompt, the model retrieves relevant patterns from this learned distribution to generate contextually coherent continuations.

Critically for our analysis, this learning process absorbs not only contemporaneous associations but also retrospective narratives written after the occurrence of the event of interest, creating the potential for lookahead bias in applications that treat model outputs as real-time information.

#### LLM Forecast and Lookahead Bias

Training data often contain both original news and subsequent articles describing market reactions, allowing models to learn events and outcomes together. In the Kodak example from Section [1](https://arxiv.org/html/2512.23847v1#S1 "1 Introduction ‚Ä£ A Test of Lookahead Bias in LLM Forecasts"), training data likely include both the loan announcement and next-day coverage of the stock surge. The model thus learns not only that ‚Äúa loan announcement happened‚Äù but also that ‚Äúit was followed by a sharp stock increase.‚Äù When the model encounters similar news later, it may effectively already ‚Äúknow‚Äù what happens next. This creates a kind of lookahead bias, where predictions mirror memorized outcomes rather than genuine reasoning.

When prompted with a headline consisting of NN tokens (w1,‚Ä¶,wN)(w\_{1},\dots,w\_{N}), the model computes

|  |  |  |
| --- | --- | --- |
|  | P‚Äã(prediction‚à£headline)=P‚Äã(wN+1‚à£w‚â§N)P(\text{prediction}\mid\text{headline})=P(w\_{N+1}\mid w\_{\leq N}) |  |

where wN+1w\_{N+1} corresponds to the highest probability category (‚Äúpositive,‚Äù ‚Äúnegative,‚Äù or ‚Äúneutral‚Äù).333Since we select the maximum probability token at each step, output is deterministic given the same input. These probabilities reflect statistical associations from training rather than causal reasoning. For the Kodak headline ‚ÄúKodak Triples on Loan to Make Covid-19 Drug Ingredients‚Äù (Figure¬†[I](https://arxiv.org/html/2512.23847v1#S4.F1 "Figure I ‚Ä£ 4 Conclusion ‚Ä£ A Test of Lookahead Bias in LLM Forecasts")), the model‚Äôs prediction likely reflects memorized event-outcome pairs from its July 2020 training data.

Because media coverage of an event and its subsequent outcome typically occur in close succession, the inclusion of event‚Äërelated articles in an LLM‚Äôs training corpus likely coincides with the inclusion of outcome‚Äërelated coverage.
This time-period overlap introduces a form of lookahead bias, whereby the model‚Äôs apparent ability to anticipate future outcomes may partly arise from exposure to post‚Äëevent information during training.
Building on this intuition, we can infer lookahead bias from the model‚Äôs memorization of event‚Äëspecific news headlines, quantified using our LAP measure introduced below.

#### MIA Techniques

Membership inference attacks (MIAs), introduced by shokri2017membership, formalize the task of determining whether a sample was included in a machine learning (ML) model‚Äôs training set through a shadow‚Äëmodel framework. Subsequent studies such as yeom2018privacy refined attack methods and evaluation metrics. Later, carlini2022membership framed MIA as a hypothesis‚Äëtesting problem using likelihood‚Äëratio principles, offering a more rigorous theoretical foundation.

The rise of LLMs has spurred growing interest in testing whether specific texts were included in a model‚Äôs pre‚Äëtraining corpus, a setting distinct from classical MIAs developed for conventional ML models. This interest is driven both by the opaque nature of LLM training data and by concerns over the potential extraction of copyrighted or sensitive content (e.g., carlini2021extracting).

shi2023detecting advance this line of work by formally defining reference-free pretraining-data detection problem and introducing the time-split WIKIMIA benchmark for reliable validation. A key insight motivating their approach is that not all tokens‚Äô conditional probabilities provide equal information about memorization. Common words such as ‚Äúthe,‚Äù ‚Äúis,‚Äù or ‚Äúto‚Äù appear frequently across all texts and thus have high probabilities regardless of whether the model has seen the specific content before. These high-frequency tokens add noise rather than signal to detection efforts.

Building on this insight, shi2023detecting propose MIN‚ÄëK% PROB, a simple, training‚Äëfree detection score for assessing whether a given text has been memorized by an LLM. The method focuses on the most informative tokens‚Äîthose assigned unusually low probabilities by the model. Formally, for a passage with nn tokens, let the LLM assign each token a conditional probability pip\_{i}. MIN‚ÄëK% PROB sorts tokens by pip\_{i} and computes the average log‚Äëprobability of the bottom K%‚Äîthe least likely tokens under the model. The hyperparameter KK determines the sensitivity of the measure to low‚Äëprobability ‚Äúoutlier‚Äù tokens.

Empirically, K=20K=20 yields the most robust performance. Evaluated on WIKIMIA across multiple large language models, shi2023detecting show that MIN-K% PROB attains an average AUC of 0.72, significantly higher than alternative MIA measures.444AUC (area under the ROC curve) measures how well a detector distinguishes members from non-members. It equals the probability that a random member gets a higher score than a random non-member. An AUC of 0.5 means no better than guessing; 1.0 means perfect detection. On an additional copyrighted-book validation test, it achieves an AUC of 0.88. Also, its detection performance generally improves for larger models and longer passages. Therefore, we adopt MIN-K% PROB with K=20K=20 as our primary detection method given its strong empirical performance.

#### Construction of LAP

We now formally define LAP (i.e., MIN-K% PROB). Consider a language model with parameters , when presented with a prompt w=(w1,‚Ä¶,wN)w=(w\_{1},\ldots,w\_{N}), LAP is computed as,

|  |  |  |  |
| --- | --- | --- | --- |
|  | Lookahead Propensity‚Äã(w,K)‚âîexp‚Å°(1|SK|‚Äã‚àët‚ààSKlog‚Å°P‚Äã(wn‚à£w‚â§n‚àí1)),\text{Lookahead Propensity}(w,K)\;\coloneqq\;\exp\!\left(\frac{1}{|S\_{K}|}\sum\_{t\in S\_{K}}\log P(w\_{n}\mid w\_{\leq n-1})\right), |  | (1) |

where P‚Äã(wn‚à£w‚â§n‚àí1)P(w\_{n}\mid w\_{\leq n-1}) is the conditional probability assigned by model to token wnw\_{n} given its preceding context w<n‚âî(w1,‚Ä¶,wn‚àí1)w\_{<n}\coloneqq(w\_{1},\ldots,w\_{n-1}),
and SK‚äÇ{1,‚Ä¶,N}S\_{K}\subset\{1,\ldots,N\} denotes the subset containing the lowest K%K\% of tokens ranked by P‚Äã(wn‚à£w‚â§n‚àí1)P(w\_{n}\mid w\_{\leq n-1}), and K=20K=20. The input ww encompasses any text provided to the model, including prompts, headlines, or full news articles, as exampled in Figures [I](https://arxiv.org/html/2512.23847v1#S4.F1 "Figure I ‚Ä£ 4 Conclusion ‚Ä£ A Test of Lookahead Bias in LLM Forecasts") and [II](https://arxiv.org/html/2512.23847v1#S4.F2 "Figure II ‚Ä£ 4 Conclusion ‚Ä£ A Test of Lookahead Bias in LLM Forecasts").

### 2.2 Econometric Framework

We formalize lookahead bias through a contamination model where predictions incorporate future information via memorization mechanisms. Formally, following sarkar2024lookahead, lookahead bias manifests when a model‚Äôs prediction ^t=^‚Äã(Xt)\hat{\mu}\_{t}=\hat{\mu}(X\_{t}) for time t+1t+1 violates the orthogonality condition:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Cov(^t,)t+1‚â†0.\text{Cov}(\hat{\mu}\_{t},{}\_{t+1})\neq 0. |  | (2) |

Consider a standard forecasting environment with the following data-generating process:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Yt+1=(Xt)+,t+1Y\_{t+1}=\mu(X\_{t})+{}\_{t+1}, |  | (3) |

where YtY\_{t} denotes the realized outcome and XtX\_{t} observable information at time tt (e.g., news headlines, earnings call transcripts), (Xt)=E‚Äã[Yt+1‚à£Xt]\mu(X\_{t})=\mdmathbb{E}[Y\_{t+1}\mid X\_{t}] is the true conditional expectation, and ‚àºt+1ùí©(0,2){}\_{t+1}\sim\mathcal{N}(0,^{2}) represents future innovations unpredictable given the information set ‚Ñ≥t\mathcal{M}\_{t} available at time tt, with (Xt)‚üÇt+1\mu(X\_{t})\perp{}\_{t+1}.

###### Definition 1 (Lookahead Bias Contamination).

When a LLM suffers from lookahead bias, its predictions take the form:

|  |  |  |  |
| --- | --- | --- | --- |
|  | ^t=(Xt)+Ltt+1\hat{\mu}\_{t}=\mu(X\_{t})+L\_{t}{}\_{t+1} |  | (4) |

where LtL\_{t} measures memorization strength.

The key implications of our framework are as follows. First, the contamination term Ltt+1L\_{t}{}\_{t+1} violates the orthogonality condition in Equation¬†([2](https://arxiv.org/html/2512.23847v1#S2.E2 "In 2.2 Econometric Framework ‚Ä£ 2 Lookahead Bias Detection ‚Ä£ A Test of Lookahead Bias in LLM Forecasts")), thereby introducing lookahead bias whenever Lt>0L\_{t}>0. Specifically, Cov(Lt,t+1)t+1=LtVar()t+1>0\text{Cov}(L\_{t}{}\_{t+1},{}\_{t+1})=L\_{t}\operatorname{Var}({}\_{t+1})>0 implies that any positive LtL\_{t} induces mechanical correlation between the model prediction and the irreducible error.

Second, the magnitude of this bias scales with the degree of LLM memorization LtL\_{t}. LLMs trained with greater exposure to future data implicitly encode information about later-period content, leading to stronger deviations from the true conditional process even when no explicit future reference is present.

Third, the resulting predictor exhibits distorted accuracy, since its expected squared prediction error

|  |  |  |
| --- | --- | --- |
|  | E[(^t‚àíYt+1)2]=Var()t+1[1‚àíLt]2,\mdmathbb{E}[(\hat{\mu}\_{t}-Y\_{t+1})^{2}]=\operatorname{Var}({}\_{t+1})[1-L\_{t}]^{2}, |  |

falls mechanically as LtL\_{t} increases.

Finally, two boundary cases clarify the mechanism: when Lt=0L\_{t}=0 for all LtL\_{t}, pretraining restricted to past data eliminates bias; whereas as Lt‚Üí1L\_{t}\to 1, complete memorization of future information produces full contamination.

Based on our earlier discussion, we proxy LL with LAP defined in Equation ([1](https://arxiv.org/html/2512.23847v1#S2.E1 "In Construction of LAP ‚Ä£ 2.1 Lookahead Propensity (LAP) ‚Ä£ 2 Lookahead Bias Detection ‚Ä£ A Test of Lookahead Bias in LLM Forecasts")).
To detect lookahead bias, we propose testing the significance of the interaction term in the regression:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Yt+1=^t1+Lt2+(Lt√ó^t)3+t+1Y\_{t+1}={}\_{1}\hat{\mu}\_{t}+{}\_{2}L\_{t}+{}\_{3}(L\_{t}\times\hat{\mu}\_{t})+{}\_{t+1} |  | (5) |

###### Proposition 1 (Detection Statistic).

By the Frisch‚ÄìWaugh‚ÄìLovell (FWL) theorem,

|  |  |  |
| --- | --- | --- |
|  | =3Cov‚Äã(y~,L‚Äã^~)Var‚Å°(L‚Äã^~),{}\_{3}\;=\;\frac{\text{Cov}(\tilde{y},\,\widetilde{L\hat{\mu}})}{\operatorname{Var}(\widetilde{L\hat{\mu}})}, |  |

where y~\tilde{y} and L‚Äã^~\widetilde{L\hat{\mu}} are the residuals from projecting Yt+1Y\_{t+1} and L‚Äã^L\hat{\mu} onto (^,L)(\hat{\mu},L), respectively. Hence,

|  |  |  |
| --- | --- | --- |
|  | >30‚ü∫Cov(y~,L‚Äã^~)>0.{}\_{3}>0\;\Longleftrightarrow\;\text{Cov}(\tilde{y},\,\widetilde{L\hat{\mu}})>0. |  |

The partial covariance admits the structural representation

|  |  |  |  |
| --- | --- | --- | --- |
|  | Cov(y~,L‚Äã^~)=E[L2Var(‚à£^,L)].\text{Cov}(\tilde{y},\,\widetilde{L\hat{\mu}})=\mdmathbb{E}\!\left[L^{2}\operatorname{Var}(\varepsilon\mid\hat{\mu},L)\right]. |  | (6) |

Therefore,

|  |  |  |
| --- | --- | --- |
|  | >30iffE[L2Var(‚à£^,L)]>0,{}\_{3}>0\quad\text{iff}\quad\mdmathbb{E}\!\left[L^{2}\operatorname{Var}(\varepsilon\mid\hat{\mu},L)\right]>0, |  |

which holds whenever LL is nonzero on a set of positive probability and
Var(‚à£^,L)>0\operatorname{Var}(\varepsilon\mid\hat{\mu},L)>0 on that set.
In particular, if L‚â•0L\geq 0 a.s. and Pr‚Å°(L>0)>0\Pr(L>0)>0, then >30{}\_{3}>0.
If L=0L=0 a.s., then =30{}\_{3}=0.

Proof: See Appendix¬†[B.1](https://arxiv.org/html/2512.23847v1#A2.SS1 "B.1 Proof of Proposition 1 ‚Ä£ Appendix B Proofs of Theoretical Results ‚Ä£ A Test of Lookahead Bias in LLM Forecasts").

#### Interpretation

Equation¬†([6](https://arxiv.org/html/2512.23847v1#S2.E6 "In Proposition 1 (Detection Statistic). ‚Ä£ 2.2 Econometric Framework ‚Ä£ 2 Lookahead Bias Detection ‚Ä£ A Test of Lookahead Bias in LLM Forecasts")) implies that the covariance term is strictly positive whenever L>0L>0, indicating that lookahead bias arises whenever >30{}\_{3}>0. The magnitude of this covariance scales jointly with the intensity of memorization LL, and it vanishes when L=0L=0, corresponding to the absence of lookahead bias. This formulation provides a theoretically grounded test statistic for empirically detecting such bias. The result also clarifies why common mitigation strategies such as prompt engineering or identifier masking are ineffective: they do not eliminate the structural contamination term Ltt+1L\_{t}{}\_{t+1} that embeds future information in LLM outputs.

## 3 Result

### 3.1 Data

#### Stock Market Information

Daily stock returns, open prices, and close prices are obtained from the Center for Research in Security Prices (CRSP), covering all common stocks listed on the New York Stock Exchange (NYSE), the National Association of Securities Dealers Automated Quotations (NASDAQ), and the American Stock Exchange (AMEX). We restrict the sample to common stocks with a share code of 10 or 11, consistent with prior studies. We obtain financial statement variables from Compustat.

#### Stock News Headlines

Data on news headlines are collected via web scraping from Bloomberg News. We select Bloomberg News because it is part of Bloomberg L.P., a leading global financial information provider whose real-time news service is updated more frequently than traditional outlets.
We collect the news headlines for all CRSP-listed companies in the sample period and match them based on company names and ticker symbols.
The final dataset consists of 91,361 news headlines covering 1,587 unique companies from January 2012 to December 2023.

#### Earnings Call Transcripts

We obtain data on firms‚Äô earnings call transcripts from Thomson Reuter‚Äôs StreetEvents database. Then, we merge them with CRSP and Compustat data using firm tickers and corresponding call dates, resulting in 135,541 matched observations. After excluding records with missing values in the key variables, our final sample consists of 74,338 firm-quarter observations from 3,897 unique U.S. publicly listed firms between 2006 and 2020.

### 3.2 LLM Setup and Prompt Design

Our analysis adopts Llama-3.3, an open-source model released by Meta in December 2024. We choose Llama-3.3 over proprietary alternatives such as OpenAI‚Äôs API for two key methodological reasons.

Most importantly, Llama-3.3 provides access to prompt token probabilities, which are essential for computing our LAP measure. While OpenAI exposes token probabilities for generated output tokens, it does not provide access to prompt token probabilities, making it unsuitable for our research design.

Additionally, the open-source nature of Llama-3.3 ensures long-term replicability.
Model checkpoints, which are saved snapshots of trained model parameters, can be freely downloaded from platforms such as [HuggingFace](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct). This allows any researcher to reproduce our analysis using the identical December 2024 version.
In contrast, proprietary API providers may update or deprecate models over time (as OpenAI has done with ChatGPT 3.5), preventing future replication using the exact model version.

For the stock news exercise, we follow the prompt design in lopez2023can. To ensure robustness in our analysis, we also instruct the LLM to provide a confidence score alongside its prediction. The prompt explicitly instructs the model to predict whether the news is ‚Äúgood‚Äù, ‚Äúbad‚Äù, or ‚Äúneutral‚Äù for the stock price of the mentioned company,555We also conduct robustness tests by replacing ‚Äústock price‚Äù with ‚Äústock return‚Äù, as returns more directly reflect changes in market expectations. This aligns with baker2006investor who showed that sentiment-driven mispricing in certain stock categories creates predictable return patterns‚Äîunderpricing during low sentiment periods leads to higher subsequent returns, while overpricing during high sentiment leads to lower returns. The results remain qualitatively similar. accompanied by a confidence score and a brief explanation, as shown in Figure [I](https://arxiv.org/html/2512.23847v1#S4.F1 "Figure I ‚Ä£ 4 Conclusion ‚Ä£ A Test of Lookahead Bias in LLM Forecasts"). This structured format is intended to enhance interpretability, allowing for clear numerical indicators for subsequent analysis.

For each headline, the model‚Äôs response is parsed and mapped to a numerical score, where good is mapped to +1+1, neutral is mapped to 0, and bad is mapped to ‚àí1-1. The confidence score is directly recorded from the LLM‚Äôs output, while the explanation is stored for interpretive analysis.

For the earnings call exercise, we adopt the approach of jha2024chatgpt, similarly incorporating confidence scores and limiting the input to the first 500 words of each transcript, as shown in Figure [II](https://arxiv.org/html/2512.23847v1#S4.F2 "Figure II ‚Ä£ 4 Conclusion ‚Ä£ A Test of Lookahead Bias in LLM Forecasts"). The prediction signal generated by the LLM for firm ii in quarter qq is based on its earnings call transcript. From the LLM prediction on earnings call data, the variable takes values of ‚àí1-1, ‚àí0.5-0.5, 0, 0.50.5, or 11, indicating whether future capital expenditures are expected to significantly decrease, slightly decrease, not change, slightly increase, or significantly increase before they are realized.

We use [vLLM](https://docs.vllm.ai/en/latest/) to run the Llama-3.3 checkpoint locally, enabling efficient batched inference on GPU. For each headline prompt, we compute the LAP measure by requesting token-level log probabilities via prompt\_logprobs = 1. We configure inference with do\_sample = False to ensure deterministic token selection and full reproducibility across runs.

### 3.3 Prompt News Headlines to Predict Stock Returns

In this exercise, we generally follow the approach of lopez2023can except the following minor differences. First, we use news headlines from Bloomberg, whereas they collect headlines from multiple media sources. Second, their analysis uses ChatGPT-4 to classify each headline as good (+1+1), bad (‚àí1-1), or neutral (0); we adopt the same classification scheme but use Llama-3.3 for the reasons we discussed above. As shown later, these differences do not affect the results, since we are able to replicate their main findings within our setting.

For simplicity, we use stock market close time 4 p.m. as the cutoff to classify headlines on day tt. Specifically, if a headline is published before 4 p.m., it is considered news on day tt and will link it to the next-day stock return; otherwise, it is treated as news on day t+1t+1.

We begin with the in‚Äësample analysis, as the LAP measure is designed to detect potential memorization within the model‚Äôs own training period, spanning January 2012 to December 2023. We then repeat the analysis on the out-of-sample data as a placebo test. In contrast, due to a different research focus, lopez2023can examine an out‚Äëof‚Äësample period from October 2021 to May 2024, consistent with ChatGPT‚Äë4‚Äôs reported knowledge cutoff of September 2021.

Following the discussion in Section [2.2](https://arxiv.org/html/2512.23847v1#S2.SS2 "2.2 Econometric Framework ‚Ä£ 2 Lookahead Bias Detection ‚Ä£ A Test of Lookahead Bias in LLM Forecasts"), the baseline regression is specified as follows:

|  |  |  |  |
| --- | --- | --- | --- |
|  | ri,t+1=+i+t‚ãÖLLMi,t+‚ãÖ(LLMi,t√óLAPi,t)+‚ãÖLAPi,t+ui,t+1,r\_{i,t+1}={}\_{i}+{}\_{t}+\gamma\cdot\text{LLM}\_{i,t}+\delta\cdot(\text{LLM}\_{i,t}\times\text{LAP}\_{i,t})+\eta\cdot\text{LAP}\_{i,t}+u\_{i,t+1}, |  | (7) |

where ri,t+1r\_{i,t+1} is the stock return of firm ii on day t+1t+1 (in %), LLMi,t‚àà{‚àí1,0,+1}\text{LLM}\_{i,t}\in\{-1,0,+1\} derived from the LLM‚Äôs evaluation of news headlines,
and LAPi,t\text{LAP}\_{i,t} is the lookahead propensity for the news headline as specified in equation [1](https://arxiv.org/html/2512.23847v1#S2.E1 "In Construction of LAP ‚Ä£ 2.1 Lookahead Propensity (LAP) ‚Ä£ 2 Lookahead Bias Detection ‚Ä£ A Test of Lookahead Bias in LLM Forecasts"). Following lopez2023can, we includes firm and date fixed effects.
Panel A of Table [I](https://arxiv.org/html/2512.23847v1#S4.T1 "Table I ‚Ä£ 4 Conclusion ‚Ä£ A Test of Lookahead Bias in LLM Forecasts") shows summary statistics of main variables.

Table¬†[II](https://arxiv.org/html/2512.23847v1#S4.T2 "Table II ‚Ä£ 4 Conclusion ‚Ä£ A Test of Lookahead Bias in LLM Forecasts") presents our results. The specification in Column (1) does not include the interaction term and thus replicates the main finding of lopez2023can. It shows that LLMs can predict returns: a one‚Äìstandard-deviation increase in the LLM prediction measure predicts a 0.197% higher next-day return. Column (2) introduces our main test. The coefficient before the interaction term LLMi,t√óLAPi,t\text{LLM}\_{i,t}\times\text{LAP}\_{i,t} is positive and highly significant. This means that the LLM‚Äôs predictive power increases dramatically for high-LAP headlines. Economically, a one‚Äìstandard-deviation increase in LAP increases the marginal effect of LLM on next-day stock return by 0.077%, which is about 37% of the standalone LLM effect shown in Column (1). The coefficient of LLM itself become insignificant.

lopez2023can also find that LLM‚Äëbased predictions exhibit stronger forecasting power for small firms. They attribute this pattern to the relatively less efficient price discovery in small‚Äëcap stocks, which tend to adjust more slowly to new information. We revisit this result in Table [III](https://arxiv.org/html/2512.23847v1#S4.T3 "Table III ‚Ä£ 4 Conclusion ‚Ä£ A Test of Lookahead Bias in LLM Forecasts"), Column (1), by adding an interaction term between LLM and Small, a dummy variable equal to one for firms in the bottom quintile of market capitalization, into the our main regression. The coefficient on the interaction term is positive, consistent with their findings. In Column (2), we further introduce a triple interaction term, LLM√óSmall√óLAP\text{LLM}\times\text{Small}\times\text{LAP}, to examine whether the stronger predictability for small‚Äëcap stocks is also driven by lookahead bias, as specified below:

|  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- |
|  | ri,t+1\displaystyle r\_{i,t+1} | =\displaystyle= | +i+tLLMi,t1+Smalli,t2+(LLMi,t√óSmalli,t)3\displaystyle{}\_{i}+{}\_{t}+{}\_{1}\text{LLM}\_{i,t}+{}\_{2}\text{Small}\_{i,t}+{}\_{3}(\text{LLM}\_{i,t}\times\text{Small}\_{i,t}) |  | (8) |
|  |  |  | +LAPi,t1+(LLMi,t√óLAPi,t)2+(LAPi,t√óSmalli,t)3\displaystyle+{}\_{1}\text{LAP}\_{i,t}+{}\_{2}(\text{LLM}\_{i,t}\times\text{LAP}\_{i,t})+{}\_{3}(\text{LAP}\_{i,t}\times\text{Small}\_{i,t}) |  |
|  |  |  | +(LLMi,t√óLAPi,t√óSmalli,t)4+ui,t+1.\displaystyle+{}\_{4}(\text{LLM}\_{i,t}\times\text{LAP}\_{i,t}\times\text{Small}\_{i,t})+u\_{i,t+1}. |  |

The results confirm this suspect: the coefficient on the triple interaction term is positive and statistically significant. Moreover, once the triple interaction is included, the coefficient on LLM√óLAP\text{LLM}\times\text{LAP} becomes negative.

These patterns are consistent with lookahead bias. The apparent predictability of the LLM increases with higher LAP values, indicating that its forecasts are more accurate when the input text is familiar to the model.

#### Compare to LLM Prediction Confidence

Recent studies have examined how an LLM‚Äôs self‚Äëreported or inferred confidence interacts with its forecast accuracy. For example, using a similar financial‚Äënews and stock‚Äëreturn setting, chen2024out show that the conditional probability of an LLM‚Äôs response contains economically meaningful information and improves predictive performance. Although these confidence measures are conceptually distinct from our LAP metric, it is worthwhile to test empirically whether they capture related mechanisms.

Specifically, we incorporate two confidence measures into our analysis: (1) the first-token conditional probability in the LLM response p‚Äã(wi,N+1‚à£wi,‚â§N)p(w\_{i,N+1}\mid w\_{i,\leq N}), which reflects the model‚Äôs inferred belief in its prediction given the wording of the prompt (chen2024out), and (2) the model‚Äôs self-reported confidence level, Confidencei,t\text{Confidence}\_{i,t}. We then conduct horse‚Äërace regressions between these confidence measures and LAP to assess whether model confidence and memorization propensity explain overlapping or distinct components of LLM forecasting performance.

Table [IV](https://arxiv.org/html/2512.23847v1#S4.T4 "Table IV ‚Ä£ 4 Conclusion ‚Ä£ A Test of Lookahead Bias in LLM Forecasts") shows the results. In Column (1), we include LLM prediction, p‚Äã(wi,N+1‚à£wi,‚â§N)p(w\_{i,N+1}\mid w\_{i,\leq N}), and their interaction term. We find that the coefficient before the interaction term is significantly positive, consistent with findings of chen2024out. In Column (2), we add LAP and LLM √ó\times LAP. The main finding remains robust: the coefficient on LLM √ó\times LAP is still significantly posistive with a similar magnitude to that reported in Table [II](https://arxiv.org/html/2512.23847v1#S4.T2 "Table II ‚Ä£ 4 Conclusion ‚Ä£ A Test of Lookahead Bias in LLM Forecasts"). The effect of p‚Äã(wi,N+1‚à£wi,‚â§N)p(w\_{i,N+1}\mid w\_{i,\leq N}) on LLM do not change materially either. In Columns (3) and (4), we repeat the analysis by using model‚Äôs self-reported confidence, and the results are similar.

Taken together, the result suggests that LAP and model confidence capture distinct components of LLM forecasts.

#### Out-of-Sample and Bootstrap Analysis

Finally, we conduct a placebo test to validate our proposition. Specifically, we repeat the analysis using news headlines released after the model‚Äôs release date, during which forecasts should not be subject to lookahead bias. In this out‚Äëof‚Äësample period, the LAP measure should capture only random noise rather than any memorization of the input text. Accordingly, in the placebo regression, the interaction term between LAP and LLM is expected to yield an insignificant coefficient, consistent with the absence of lookahead bias once the model no longer has access to training‚Äëperiod information.

To implement this idea, we use Llama‚Äë2 instead of Llama‚Äë3.3, which was released in December 2024 and therefore offers too short a horizon for our test. Llama‚Äë2 was released in July 2023 and updated with some of the files in Aug 2023, providing a longer evaluation window. To be conservative, we use the model‚Äôs release date‚Äîrather than its claimed knowledge cutoff‚Äîas the starting point for our out‚Äëof‚Äësample period. Thus, the out‚Äëof‚Äësample period is from September 2023 to December 2024. Appendix Table [A.1](https://arxiv.org/html/2512.23847v1#A1.T1 "Table A.1 ‚Ä£ A.1 Llama Model Family ‚Ä£ Appendix A Online Appendix ‚Ä£ A Test of Lookahead Bias in LLM Forecasts") summarizes the corresponding in‚Äësample and out‚Äëof‚Äësample periods for each model.

To ensure comparability across in-sample and out-of-sample periods, we standardize all variables separately within each period and re-estimate the same set of regressions. Table [V](https://arxiv.org/html/2512.23847v1#S4.T5 "Table V ‚Ä£ 4 Conclusion ‚Ä£ A Test of Lookahead Bias in LLM Forecasts") reports the results. Columns (1) and (2) replicate our main in-sample findings, which are qualitatively similar to those obtained using Llama-3.3. LLM predictions are positively correlated with next-day stock returns, and this relationship is stronger when LAP is high.

In Columns (3) and (4), we re-estimate the regressions using the out-of-sample data. As shown in Column (4), the coefficient on the interaction term LLMstd\text{LLM}^{\text{std}} √ó\times LAPstd\text{LAP}^{\text{std}} is negative and statistically insignificant, consistent with the absence of lookahead bias during the out-of-sample period.

Next, we conduct a bootstrap analysis. We use the pairs bootstrap approach (e.g., efron1994introduction). Specifically, we repeatedly sample observations from the out-of-sample data with replacement for 10,000 repetitions and re-estimate the regression in each bootstrap sample. The bootstrap sample size is set equal to that of the out-of-sample period. For each iteration, we record the estimated coefficient on the interaction term between LLMstd\text{LLM}^{\text{std}} and LAPstd\text{LAP}^{\text{std}}.

Figure [III](https://arxiv.org/html/2512.23847v1#S4.F3 "Figure III ‚Ä£ 4 Conclusion ‚Ä£ A Test of Lookahead Bias in LLM Forecasts") shows the empirical distribution of the bootstrap coefficients. The blue, dotted vertical line indicates the 95th percentile of the empirical bootstrap distribution, while the red, solid vertical line represents the in-sample estimate of the interaction coefficient shown in Column (2). Consistent with our baseline results, the distribution in the out-of-sample placebo period is centered at a
negative value. Moreover, the in-sample interaction coefficient lies well outside the 95th percentile of the empirical distribution of the out-of-sample interaction coefficients (with one-sided pp-value of 0.033), highlighting a clear difference between the in-sample and out-of-sample distributions.

### 3.4 Prompt Earnings Call Transcripts to Predict Capex

In our second forecast exercise, we follow the approach of jha2024chatgpt, who use earnings call transcripts to predict firms‚Äô capital expenditures two quarters ahead. We generally follow their procedure of data cleaning and variable construction for consistency. In their analysis, each transcript is segmented into 2,000-word chunks and classified by ChatGPT-3.5 into five categories: significantly increase (+1.0+1.0), slightly increase (+0.5+0.5), no change (0.00.0), slightly decrease (‚àí0.5-0.5), and significantly decrease (‚àí1.0-1.0). We use the same classification scheme but replace ChatGPT-3.5 with Llama-3.3. Due to the context limitation of the self-hosted Llama-3.3 model, we restrict each input to the first 512 words of the transcript, yet we are still able to replicate their baseline results. Our sample period is 2006‚Äì2020, identical to theirs. Summary statistics of main variables are reported in Panel B of Table [I](https://arxiv.org/html/2512.23847v1#S4.T1 "Table I ‚Ä£ 4 Conclusion ‚Ä£ A Test of Lookahead Bias in LLM Forecasts").666The distribution of LAP can vary substantially with the length of the input text, which explains why the LAP distributions differ markedly across the two exercises.

jha2024chatgpt find that, when prompted with earnings call transcripts, LLMs can effectively predict firms‚Äô subsequent CapEx ratios. We examine whether this apparent predictability is driven by memorization, as proxied by LAP. Column (1) of Table [VI](https://arxiv.org/html/2512.23847v1#S4.T6 "Table VI ‚Ä£ 4 Conclusion ‚Ä£ A Test of Lookahead Bias in LLM Forecasts") reproduces their main finding: the LLM-generated prediction score LLMi,q\text{LLM}\_{i,q} strongly forecasts future capital expenditure.

Column (2) introduces the interaction term between LLM and LAP. The estimation indicates that the observed predictability is amplified by memorization: the coefficient on the interaction term is positive and highly significant. In terms of economic magnitude, a one‚Äëstandard‚Äëdeviation increase in the LLM prediction corresponds to a 0.324% higher CapEx ratio, while a one‚Äëstandard‚Äëdeviation increase in LAP raises the marginal effect of LLM by 0.149%‚Äîapproximately 19% of the standalone LLM effect in Column (1).777We were unable to replicate the baseline results using Llama-2, potentially due to limitations in its ability to handle long-context inputs. As a result, we do not include out-of-sample test results for the earnings call exercise.

The pattern suggests that the model‚Äôs apparent ability to extract forward‚Äëlooking information from corporate communications may partly reflect its memory of firms‚Äô investment activities. This is not surprising, as articles discussing a company‚Äôs investments and earnings‚Äëcall communications are likely included in the model‚Äôs training corpus.

## 4 Conclusion

Large language models (LLMs) are increasingly used to generate economic and financial forecasts, often appearing to outperform conventional econometric and machine‚Äëlearning methods. However, recent studies suggest that this apparent predictive power may partly reflect lookahead bias, as evaluation is effectively conducted in‚Äësample, and the model may have been exposed to overlapping information during training.

We develop a statistical test to distinguish genuine reasoning from training‚Äëdata leakage in LLM‚Äëbased forecasts. Building on MIA techniques, we estimate the likelihood that a given prompt appeared in an LLM‚Äôs pre‚Äëtraining corpus‚Äîa measure we term Lookahead Propensity (LAP). We show that a positive correlation between LAP and forecast accuracy indicates the presence and magnitude of lookahead bias.

Applying the LAP test to two forecasting tasks‚Äînews headlines predicting stock returns and earnings‚Äëcall transcripts predicting capital expenditures‚Äîwe find that the predictive power of LLMs partly reflects memorization rather than genuine inference.

Lookahead bias is task‚Äëspecific, varying with factors such as input visibility, target variable, model scale, and prompt design. Our LAP test provides a cost‚Äëefficient, diagnostic tool that requires no model retraining or proprietary data access, offering a practical framework for assessing the validity and reliability of LLM‚Äëgenerated forecasts.

Figure I:  Example Prompt and Response for Stock News Analysis

Example Prompt:

Here is a piece of news: ‚Äú(2020-07-28) Kodak Triples on Loan to Make Covid-19 Drug Ingredients.‚Äù
Do you think this news is good, bad, or neutral for the stock price of  Eastman Kodak Company (KODK) in the short term?
Write your answer as:
{good/ bad / neutral}
{confidence (0-1):}
{explanation (less than 25 words)}

Example Response:

{good}
{1.0}
{Loan to produce Covid-19 drug ingredients boosts prospects.}




Figure II: Example Prompt and Response for Earnings Call Analysis

Example Prompt:

Here is an excerpt from the earnings call transcript of  Oceaneering International Inc. (OII):
Q1 2020 Oceaneering International Inc. Earnings Call
‚Äú‚Ä¶We began 2020 with the expectation of marginal growth and improving business fundamentals across all of our segments. And then the COVID-19 pandemic erupted and fueled the further deterioration of the crude oil market fundamentals‚Ä¶‚Äù
Do you think the company Oceaneering International Inc. (OII) plans to increase or decrease its capital expenditures over the next year?
Write your answer as:
{significantly\_increase / slightly\_increase / no\_change / slightly\_decrease / significantly\_decrease}
{confidence (0‚Äì1):}
{explanation (less than 25 words):}

Example Response:

{significantly\_decrease}
{0.8}
{Due to COVID-19 pandemic and crude oil market deterioration.}




Figure III: Results from Pairs Bootstrap Inference

This figure illustrates the bootstrap distribution of the interaction coefficient
between standardized LLM and LAP, from the specification

|  |  |  |
| --- | --- | --- |
|  | ri,t+1=LLMi,tstd1+LAPi,tstd2+(LLMi,tstd√óLAPi,tstd)++i+tui,t+1.r\_{i,t+1}={}\_{1}\,\text{LLM}^{\text{std}}\_{i,t}+{}\_{2}\,\text{LAP}^{\text{std}}\_{i,t}+\beta\!\left(\text{LLM}^{\text{std}}\_{i,t}\times\text{LAP}^{\text{std}}\_{i,t}\right)+{}\_{i}+{}\_{t}+u\_{i,t+1}. |  |

In each bootstrap replication b=1,‚Ä¶,10,000b=1,\ldots,10{,}000,
out-of-sample observations are resampled with replacement,
the regression above is re-estimated,
and an interaction coefficient (b) is obtained.
The histogram plots the empirical distribution of these
{}(b)b=110,000\{{}^{(b)}\}\_{b=1}^{10{,}000}. The red solid vertical line denotes the interaction coefficient estimated from in-sample, while the blue dotted vertical line marks the 95th percentile of the bootstrap distribution.

![Refer to caption](x1.png)




Table I: Descriptive Statistics




Panel A: Stock News Prediction

This table presents descriptive statistics for key variables used in the stock news analysis. Statistics reported include mean, standard deviation (SD), and percentiles (P10, P25, Median, P75, P90).

| Variable | Mean | SD | P10 | P25 | Median | P75 | P90 | N |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| LAP (√ó104)(\times 10^{4}) | 0.073 | 0.027 | 0.041 | 0.053 | 0.069 | 0.089 | 0.110 | 91,361 |
| rt+1r\_{t+1} (%)(\%) | 0.061 | 4.301 | -2.599 | -1.036 | 0.035 | 1.136 | 2.626 | 91,361 |
| LLM | 0.084 | 0.937 | -1.000 | -1.000 | 0.000 | 1.000 | 1.000 | 91,361 |
| p‚Äã(wN+1‚à£w‚â§N)p(w\_{N+1}\mid w\_{\leq N}) | 0.987 | 0.054 | 0.988 | 0.999 | 1.000 | 1.000 | 1.000 | 91,361 |
| Confidence | 0.801 | 0.073 | 0.700 | 0.800 | 0.800 | 0.800 | 0.900 | 91,361 |




Panel B: Earnings Call Prediction

This panel presents descriptive statistics for key variables used in the corporate investment prediction analysis. Statistics reported include mean, standard deviation (SD), and percentiles (P10, P25, Median, P75, P90).

| Variable | Mean | SD | P10 | P25 | Median | P75 | P90 | N |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| LAP (%)(\%) | 2.049 | 1.007 | 0.947 | 1.330 | 1.872 | 2.573 | 3.373 | 74,338 |
| CapEx (%)(\%) | 2.823 | 3.967 | 0.108 | 0.479 | 1.427 | 3.471 | 7.104 | 74,338 |
| LLM | 0.076 | 0.407 | -0.500 | 0.000 | 0.000 | 0.500 | 0.500 | 74,338 |




Table II: Regression of Next Day Returns on LLM Prediction and LAP

This table reports regression results from the specification

|  |  |  |
| --- | --- | --- |
|  | ri,t+1=+i+t‚ãÖLLMi,t+‚ãÖ(LLMi,t√óLAPi,t)+‚ãÖLAPi,t+ui,t+1,r\_{i,t+1}={}\_{i}+{}\_{t}+\gamma\cdot\text{LLM}\_{i,t}+\delta\cdot(\text{LLM}\_{i,t}\times\text{LAP}\_{i,t})+\eta\cdot\text{LAP}\_{i,t}+u\_{i,t+1}, |  |

where ri,t+1r\_{i,t+1} is the next-day stock return for firm ii at time tt, measured in percentage points. The variable LLMi,t\text{LLM}\_{i,t} represents the LLM-generated investment signal, taking values of ‚àí1-1, 0, or +1+1, indicating negative, neutral, or positive predictions, respectively. The measure LAPi,t\text{LAP}\_{i,t} is computed by averaging the model‚Äôs token probabilities for the hardest-to-predict K=20%K=20\% of prompt tokens. Column (1) reports results from a specification that excludes the interaction term between LLMi,t\text{LLM}\_{i,t} and LAPi,t\text{LAP}\_{i,t}. Column (2) adds this interaction term to evaluate whether the predictive power of the LLM signal varies with LAP. All regressions include firm and time fixed effects, and standard errors are clustered by date. The sample includes all U.S. common stocks with at least one news headline covering the firm from January 2012 to December 2023.

|  | (1) | (2) |
| --- | --- | --- |
|  | ri,t+1r\_{i,t+1} | ri,t+1r\_{i,t+1} |
| LLMi,t\text{LLM}\_{i,t} | 0.210\*\*\* | 0.00117 |
|  | (12.24) | (0.03) |
| LAPi,t\text{LAP}\_{i,t} |  | -1.297\*\*\* |
|  |  | (-2.61) |
| LLMi,t√óLAPi,t\text{LLM}\_{i,t}\times\text{LAP}\_{i,t} |  | 2.866\*\*\* |
|  |  | (4.86) |
| Firm FE | Yes | Yes |
| Date FE | Yes | Yes |
| R2R^{2} | 0.179 | 0.180 |
| NN | 91,361 | 91,361 |




Table III: Regression of Next-Period Returns on LLM Prediction, LAP, and Firm Size

This table reports regression results examining how the predictive power of LLM-based prediction varies with LAP and firm size. The variable LLMi,t\text{LLM}\_{i,t} denotes the prediction score generated by the model for firm ii at time tt, taking values of ‚àí1-1, 0, or +1+1 to indicate negative, neutral, or positive prediction, respectively. The measure LAPi,t\text{LAP}\_{i,t} is computed by averaging the model‚Äôs token probabilities for the hardest-to-predict K=20%K=20\% of prompt tokens. The indicator Smalli,t\text{Small}\_{i,t} equals one if the firm is in the bottom quintile of market capitalization based on the previous day‚Äôs market cap, and zero otherwise. Column (1) reports a specification that includes the LLM score, the small-firm indicator, and their interaction to assess whether the return predictability of the LLM signal differs between small and large firms. Column (2) adds the interaction between LLMi,t\text{LLM}\_{i,t} and LAPi,t\text{LAP}\_{i,t}, as well as the corresponding triple interaction with Smalli,t\text{Small}\_{i,t}, to evaluate whether the predictive power of the LLM signal varies jointly with LAP and firm size. All regressions include firm and time fixed effects, and standard errors are clustered by date. The sample includes all U.S. common stocks with at least one news headline covering the firm from January 2012 to December 2023.

|  | (1) | (2) |
| --- | --- | --- |
|  | ri,t+1r\_{i,t+1} | ri,t+1r\_{i,t+1} |
| LLMi,t\text{LLM}\_{i,t} | 0.154\*\*\* | 0.0733\*\* |
|  | (11.28) | (2.37) |
| Smalli,t\text{Small}\_{i,t} | -0.128 | 0.00502 |
|  | (-0.89) | (0.03) |
| LLMi,t√óSmalli,t\text{LLM}\_{i,t}\times\text{Small}\_{i,t} | 0.263\*\*\* | -0.316\*\* |
|  | (4.23) | (-2.03) |
| LAPi,t\text{LAP}\_{i,t} |  | -0.8743\*\* |
|  |  | (-2.21) |
| LLMi,t√óLAPi,t\text{LLM}\_{i,t}\times\text{LAP}\_{i,t} |  | 1.116\*\*\* |
|  |  | (2.67) |
| LAPi,t√óSmalli,t\text{LAP}\_{i,t}\times\text{Small}\_{i,t} |  | -1.906 |
|  |  | (-1.00) |
| LLMi,t√óLAPi,t√óSmalli,t\text{LLM}\_{i,t}\times\text{LAP}\_{i,t}\times\text{Small}\_{i,t} |  | 7.910\*\*\* |
|  |  | (3.53) |
| Firm FE | Yes | Yes |
| Date FE | Yes | Yes |
| R2R^{2} | 0.180 | 0.181 |
| NN | 91,361 | 91,361 |




Table IV: Regression of Next Day Returns on LLM Prediction, LAP, Conditional Probability in Response, and Confidence

This table reports regression results that assess the incremental predictive power of LAP and alternative confidence measures for next-day stock returns.
The main predictors include the LLM prediction LLMi,t\text{LLM}\_{i,t}, the LAP measure LAPi,t\text{LAP}\_{i,t}, the probability of the first generated token p‚Äã(wi,N+1‚à£wi,‚â§N)p(w\_{i,N+1}\mid w\_{i,\leq N}), and the model‚Äôs stated confidence Confidencei,t\text{Confidence}\_{i,t}.
The variable LLMi,t\text{LLM}\_{i,t} takes values of ‚àí1-1, 0, or +1+1, indicating negative, neutral, or positive prediction, respectively.
The measure LAPi,t\text{LAP}\_{i,t} is computed by averaging the model‚Äôs token probabilities for the hardest-to-predict K=20%K=20\% of prompt tokens.
The term p‚Äã(wi,N+1‚à£wi,‚â§N)p(w\_{i,N+1}\mid w\_{i,\leq N}) captures the probability of the model‚Äôs first generated token, conditional on the headline (chen2024out).
The variable Confidencei,t\text{Confidence}\_{i,t} reflects the model‚Äôs reported confidence in its prediction.
Columns (1)‚Äì(4) report specifications that sequentially add these measures and their interactions to assess whether the predictive content of the LLM signal depends on LAP, the probability of the first generated token, or model-reported confidence.
All regressions include firm and time fixed effects, and standard errors are clustered by date.
The sample includes all U.S. common stocks with at least one news headline covering the firm from January 2012 to December 2023.

|  | (1) | (2) | (3) | (4) |
| --- | --- | --- | --- | --- |
|  | ri,t+1r\_{i,t+1} | ri,t+1r\_{i,t+1} | ri,t+1r\_{i,t+1} | ri,t+1r\_{i,t+1} |
| LLMi,t\text{LLM}\_{i,t} | -0.499\*\* | -0.702\*\*\* | -3.425\*\*\* | -3.491\*\*\* |
|  | (-2.21) | (-3.05) | (-11.11) | (-11.15) |
| LAPi,t\text{LAP}\_{i,t} |  | -1.305\*\*\* |  | -1.179\*\* |
|  |  | (-2.63) |  | (-2.41) |
| LLMi,t√óLAPi,t\text{LLM}\_{i,t}\times\text{LAP}\_{i,t} |  | 2.860\*\*\* |  | 1.772\*\*\* |
|  |  | (4.85) |  | (3.12) |
| p‚Äã(wi,N+1‚à£wi,‚â§N)p(w\_{i,N+1}\mid w\_{i,\leq N}) | 0.143 | 0.143 |  |  |
|  | (0.93) | (0.93) |  |  |
| LLMi,t√óp‚Äã(wi,N+1‚à£wi,‚â§N)\text{LLM}\_{i,t}\times p(w\_{i,N+1}\mid w\_{i,\leq N}) | 0.714\*\*\* | 0.710\*\*\* |  |  |
|  | (3.13) | (3.12) |  |  |
| Confidencei,t\text{Confidence}\_{i,t} |  |  | 0.519\* | 0.575\*\* |
|  |  |  | (1.80) | (2.00) |
| LLMi,t√óConfidencei,t\text{LLM}\_{i,t}\times\text{Confidence}\_{i,t} |  |  | 4.520\*\*\* | 4.442\*\*\* |
|  |  |  | (11.46) | (11.38) |
| Firm FE | Yes | Yes | Yes | Yes |
| Date FE | Yes | Yes | Yes | Yes |
| R2R^{2} | 0.179 | 0.180 | 0.183 | 0.184 |
| NN | 91,361 | 91,361 | 91,361 | 91,361 |




Table V: Regression of Next-Period Returns on LLM Prediction Scores: In-Sample and Out-of-Sample Analysis Using Llama-2

This table reports regression results evaluating the predictive performance of LLM predictions in both in-sample (IS) and out-of-sample (OOS) periods using the Llama-2-70B model. All variables are standardized separately within the in-sample and out-of-sample periods. The in-sample period spans January 2012 to September 2022, and the out-of-sample period covers September 2023 to December 2024; definitions of these periods are provided in Table [A.1](https://arxiv.org/html/2512.23847v1#A1.T1 "Table A.1 ‚Ä£ A.1 Llama Model Family ‚Ä£ Appendix A Online Appendix ‚Ä£ A Test of Lookahead Bias in LLM Forecasts"). Columns (1) and (3) report in-sample estimates, while Columns (2) and (4) report the corresponding out-of-sample estimates using an evaluation window that does not overlap with the training period. Columns (3)‚Äì(4) assess whether the role of LAP in enhancing predictive performance carries over from the in-sample period to the out-of-sample period. All regressions include firm and time fixed effects, and standard errors are clustered by date.

|  | (1) | (2) | (3) | (4) |
| --- | --- | --- | --- | --- |
|  | ri,t+1stdr\_{i,t+1}^{\text{std}} | ri,t+1stdr\_{i,t+1}^{\text{std}} | ri,t+1stdr\_{i,t+1}^{\text{std}} | ri,t+1stdr\_{i,t+1}^{\text{std}} |
| LLMi,tstd\text{LLM}\_{i,t}^{\text{std}} | 0.0487\*\*\* | 0.0489\*\*\* | 0.0839\*\*\* | 0.0838\*\*\* |
|  | (12.09) | (12.10) | (7.45) | (7.47) |
| LAPi,tstd\text{LAP}\_{i,t}^{\text{std}} |  | -0.00163 |  | 0.000945 |
|  |  | (-0.50) |  | (0.09) |
| LLMi,tstd√óLAPi,tstd\text{LLM}\_{i,t}^{\text{std}}\times\text{LAP}\_{i,t}^{\text{std}} |  | 0.0120\*\*\* |  | -0.00736 |
|  |  | (3.44) |  | (-0.74) |
| OOS Bootstrap pp-value |  | [0.033] |  |  |
| Sample Period | In-sample | In-sample | Out-of-sample | Out-of-sample |
| Firm FE | Yes | Yes | Yes | Yes |
| Date FE | Yes | Yes | Yes | Yes |
| Observations | 82,234 | 82,234 | 9,942 | 9,942 |




Table VI: Regression of Future Capital Expenditures on LLM Predictions and LAP

This table reports regression results from the specification

|  |  |  |
| --- | --- | --- |
|  | CapExi,q+2=+i+q‚ãÖLLMi,q+‚ãÖ(LLMi,q√óLAPi,q)+‚ãÖLAPi,q+ui,q+2,\text{CapEx}\_{i,q+2}={}\_{i}+{}\_{q}+\gamma\cdot\text{LLM}\_{i,q}+\delta\cdot(\text{LLM}\_{i,q}\times\text{LAP}\_{i,q})+\eta\cdot\text{LAP}\_{i,q}+u\_{i,q+2}, |  |

where CapExi,q+2\text{CapEx}\_{i,q+2} denotes firm ii‚Äôs capital expenditures two quarters ahead, scaled by total assets. The terms i and q represent firm and quarter fixed effects, respectively. The variable LLMi,q\text{LLM}\_{i,q} is the model-generated prediction score for firm ii in quarter qq, taking values of ‚àí1-1, ‚àí0.5-0.5, 0, 0.50.5, or 11, corresponding to significantly decrease, slightly decrease, no change, slightly increase, and significantly increase. The measure LAPi,q\text{LAP}\_{i,q} is computed by averaging the model‚Äôs token probabilities for the hardest-to-predict K=20%K=20\% of prompt tokens. Column (1) reports results from a specification that excludes the interaction between LLMi,q\text{LLM}\_{i,q} and LAPi,q\text{LAP}\_{i,q}. Column (2) adds this interaction term to assess whether the predictive content of the LLM signal varies with LAP. All regressions include firm and quarter fixed effects, and standard errors are clustered by firm.

|  | (1) | (2) |
| --- | --- | --- |
|  | CapExi,q+2\text{CapEx}\_{i,q+2} | CapExi,q+2\text{CapEx}\_{i,q+2} |
| LLMi,q\text{LLM}\_{i,q} | 0.798\*\*\* | 0.514\*\*\* |
|  | (15.89) | (5.88) |
| LAPi,q\text{LAP}\_{i,q} |  | -0.0160 |
|  |  | (-0.57) |
| LLMi,q√óLAPi,q\text{LLM}\_{i,q}\times\text{LAP}\_{i,q} |  | 0.148\*\*\* |
|  |  | (3.59) |
| Firm FE | Yes | Yes |
| Quarter FE | Yes | Yes |
| R2R^{2} | 0.642 | 0.643 |
| NN | 74,338 | 74,338 |

## Appendix A Online Appendix

### A.1 Llama Model Family

Table A.1: Timeline of the Llama model family.

The data cutoff marks the latest date of information included in the model‚Äôs training data, while the release date shows when the model became publicly available. Chat versions may contain limited knowledge beyond the cutoff due to additional tuning and reinforcement learning from human feedback (RLHF).

| Model | Knowledge Cutoff | Latest Update Date | In-Sample Period | Out-of-Sample Period |
| --- | --- | --- | --- | --- |
| [Llama 2 (70B)](https://huggingface.co/meta-llama/Llama-2-70b-hf) | Sept 2022 | Aug 2023 | Jan 2012 to Sept 2022 | Sep 2023 to Dec 2024 |
| [Llama 3.3 (70B)](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | Dec 2023 | Dec 2024 | Jan 2012 to Dec 2023 | ‚Äì |

### A.2 Variable definitions

Table A.2: Variable Definitions

This table summarizes the construction and definitions of all variables used in the analysis, including LLM prediction, the LAP measure, the model‚Äôs predicted capital spending, and realized future capital expenditure.



| Variable | Definition |
| --- | --- |
| News Headlines Predicting Stock Returns | |
| ri,t+1r\_{i,t+1} | Next-period stock return for firm ii, measured in percentage points. If news is released before 4:00 p.m. on day tt, the return is computed between the closing prices of day tt and day t+1t\!+\!1. If news is released after 4:00 p.m. on day tt, the return is computed between the closing prices of day t+1t\!+\!1 and day t+2t\!+\!2. |
| LLMi,t\text{LLM}\_{i,t} | Prediction score generated by the LLM for firm ii at time tt. Takes discrete values of ‚àí1-1, 0, or +1+1, indicating negative, neutral, or positive prediction, respectively. |
| LAPi,t\text{LAP}\_{i,t} | Lookahead Propensity measure based on the model‚Äôs token probabilities for the hardest-to-predict K=20%K=20\% of prompt tokens. |
| p‚Äã(wi,N+1‚à£wi,‚â§N)p(w\_{i,N+1}\mid w\_{i,\leq N}) | Probability that the model generates the first output token wi,N+1w\_{i,N+1} (e.g., ‚Äúgood‚Äù, ‚Äúbad‚Äù, or ‚Äúneutral‚Äù) given the input prompt wi,‚â§Nw\_{i,\leq N}. It reflects the model‚Äôs inner confidence (chen2024out). |
| Confidencei,t\text{Confidence}\_{i,t} | LLM‚Äôs self-reported confidence in its prediction for firm ii at time tt, expressed as a percentage. |
| Smalli,t\text{Small}\_{i,t} | Binary indicator equal to 1 if firm ii falls into the bottom quintile of market capitalization based on trading-day t‚àí1t-1 market cap, and 0 otherwise. |
| Earnings Call Predicting Capital Expenditure | |
| CapExi,q+2\text{CapEx}\_{i,q+2} | Capital expenditures reported at the end of the quarter q+2q+2, scaled by total book assets. |
| LLMi,q\text{LLM}\_{i,q} | Prediction generated by the LLM for firm ii in quarter qq based on its earnings call. The variable takes values of ‚àí1-1, ‚àí0.5-0.5, 0, 0.50.5, or 11, indicating whether future capital expenditures are expected to significantly decrease, slightly decrease, not change, slightly increase, or significantly increase before they are realized. |
| LAPi,q\text{LAP}\_{i,q} | Lookahead Propensity measure based on the model‚Äôs token probabilities for the hardest-to-predict K=20%K=20\% of prompt tokens. |

## Appendix B Proofs of Theoretical Results

### B.1 Proof of Proposition¬†[1](https://arxiv.org/html/2512.23847v1#Thmproposition1 "Proposition 1 (Detection Statistic). ‚Ä£ 2.2 Econometric Framework ‚Ä£ 2 Lookahead Bias Detection ‚Ä£ A Test of Lookahead Bias in LLM Forecasts")

###### Proof.

By definition,

|  |  |  |
| --- | --- | --- |
|  | Cov‚Äã(y~,L‚Äã^~)=E‚Äã[y~‚ãÖL‚Äã^~]‚àíE‚Äã[y~]‚ãÖE‚Äã[L‚Äã^~].\text{Cov}(\tilde{y},\widetilde{L\hat{\mu}})=\mdmathbb{E}[\tilde{y}\cdot\widetilde{L\hat{\mu}}]-\mdmathbb{E}[\tilde{y}]\cdot\mdmathbb{E}[\widetilde{L\hat{\mu}}]. |  |

Since E‚Äã[y~]=0\mdmathbb{E}[\tilde{y}]=0, this reduces to

|  |  |  |
| --- | --- | --- |
|  | Cov‚Äã(y~,L‚Äã^~)=E‚Äã[y~‚ãÖL‚Äã^~].\text{Cov}(\tilde{y},\widetilde{L\hat{\mu}})=\mdmathbb{E}[\tilde{y}\cdot\widetilde{L\hat{\mu}}]. |  |

Using the residualized forms,

|  |  |  |
| --- | --- | --- |
|  | y~=‚àíE[‚à£^,L],L‚Äã^~=L2‚àíE[L2‚à£^,L].\tilde{y}=\varepsilon-\mdmathbb{E}[\varepsilon\mid\hat{\mu},L],\quad\widetilde{L\hat{\mu}}=L^{2}\varepsilon-\mdmathbb{E}[L^{2}\varepsilon\mid\hat{\mu},L]. |  |

Taking conditional expectation with respect to (^,L)(\hat{\mu},L),

|  |  |  |  |
| --- | --- | --- | --- |
|  | E‚Äã[y~‚ãÖL‚Äã^~]\displaystyle\mdmathbb{E}[\tilde{y}\cdot\widetilde{L\hat{\mu}}] | =E‚Äã[E‚Äã[y~‚ãÖL‚Äã^~‚à£^,L]]\displaystyle=\mdmathbb{E}\!\left[\mdmathbb{E}[\tilde{y}\cdot\widetilde{L\hat{\mu}}\mid\hat{\mu},L]\right] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =E[(‚àíE[‚à£^,L])(L2‚àíE[L2‚à£^,L])‚à£^,L]\displaystyle=\mdmathbb{E}\!\left[\big(\varepsilon-\mdmathbb{E}[\varepsilon\mid\hat{\mu},L]\big)\Big(L^{2}\varepsilon-\mdmathbb{E}[L^{2}\varepsilon\mid\hat{\mu},L]\Big)\mid\hat{\mu},L\right] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =L2E[(‚àíE[‚à£^,L])2‚à£^,L]\displaystyle=L^{2}\,\mdmathbb{E}\!\left[\big(\varepsilon-\mdmathbb{E}[\varepsilon\mid\hat{\mu},L]\big)^{2}\mid\hat{\mu},L\right] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =L2Var(‚à£^,L).\displaystyle=L^{2}\,\operatorname{Var}(\varepsilon\mid\hat{\mu},L). |  |

Taking the outer expectation yields

|  |  |  |
| --- | --- | --- |
|  | Cov(y~,L‚Äã^~)=E[L2‚ãÖVar(‚à£^,L)],\text{Cov}(\tilde{y},\widetilde{L\hat{\mu}})=\mdmathbb{E}\!\left[L^{2}\cdot\operatorname{Var}(\varepsilon\mid\hat{\mu},L)\right], |  |

which is strictly positive whenever L>0L>0.
‚àé