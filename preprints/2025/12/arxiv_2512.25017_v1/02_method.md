---
authors:
- Chenguang Liu
- Antonis Papapantoleon
- Jasper Rou
doc_id: arxiv:2512.25017v1
family_id: arxiv:2512.25017
is_current: true
taxonomy:
  alpha_families: []
  asset_classes: []
  horizons: []
  themes: []
title: Convergence of the generalization error for Deep Gradient Flow Methods for
  PDEs
url_abs: http://arxiv.org/abs/2512.25017v1
url_html: https://arxiv.org/html/2512.25017v1
venue: arXiv q-fin
version: 1
year: 2025
---


Chenguang Liu
, 
Antonis Papapantoleon
 and 
Jasper Rou
Delft Institute of Applied Mathematics, EEMCS, TU Delft, 2628 Delft, The Netherlands.
[C.Liu-13@tudelft.nl](mailto:C.Liu-13@tudelft.nl)
Delft Institute of Applied Mathematics, EEMCS, TU Delft, 2628 Delft, The Netherlands & Institute of Applied and Computational Mathematics, FORTH, 70013 Heraklion, Greece
[a.papapantoleon@tudelft.nl](mailto:a.papapantoleon@tudelft.nl)
Delft Institute of Applied Mathematics, EEMCS, TU Delft, 2628 Delft, The Netherlands
[J.G.Rou@tudelft.nl](mailto:J.G.Rou@tudelft.nl)

###### Abstract.

The aim of this article is to provide a firm mathematical foundation for the application of deep gradient flow methods (DGFMs) for the solution of (high-dimensional) partial differential equations (PDEs).
We decompose the generalization error of DGFMs into an approximation and a training error.
We first show that the solution of PDEs that satisfy reasonable and verifiable assumptions can be approximated by neural networks, thus the approximation error tends to zero as the number of neurons tends to infinity.
Then, we derive the gradient flow that the training process follows in the ‚Äúwide network limit‚Äù and analyze the limit of this flow as the training time tends to infinity. These results combined show that the generalization error of DGFMs tends to zero as the number of neurons and the training time tend to infinity.

###### Key words and phrases:

Partial differential equations, deep learning, neural networks, gradient flows, generalization error, training error, approximation error, convergence

###### 2020 Mathematics Subject Classification:

68T07, 65M12

## 1. Introduction

Deep learning methods for the solution of high-dimensional partial differential equations (PDEs) have gained tremendous popularity in the last few years, since they can tackle equations in dimensions that were not attainable by classical methods, such as finite difference and finite element schemes.
This ability allows the modeling of more realistic phenomena across various fields of science and technology, including engineering, biology, economics, and finance.
The seminal articles of Sirignano and Spiliopoulos [[29](https://arxiv.org/html/2512.25017v1#bib.bib29)] on the Deep Galerkin Method (DGM) and of Raissi and Karniadakis [[26](https://arxiv.org/html/2512.25017v1#bib.bib26)] and Raissi et¬†al. [[27](https://arxiv.org/html/2512.25017v1#bib.bib27)] on physics-informed neural networks (PINNs), building on the earlier work of Lagaris et¬†al. [[18](https://arxiv.org/html/2512.25017v1#bib.bib18)] and Lagaris et¬†al. [[19](https://arxiv.org/html/2512.25017v1#bib.bib19)], incorporate the PDE residual and the initial and boundary conditions into the loss function of a neural network, which is then minimized by stochastic gradient descent (SGD).
These methods have laid the foundations for a variety of extensions and applications, including among many others, fractional differential equations¬†(Pang et¬†al. [[23](https://arxiv.org/html/2512.25017v1#bib.bib23)]), variational PINNs¬†(Kharazmi et¬†al. [[17](https://arxiv.org/html/2512.25017v1#bib.bib17)]), Bayesian variants¬†(Yang et¬†al. [[31](https://arxiv.org/html/2512.25017v1#bib.bib31)]) and mean-field games¬†(Carmona and Zeng [[4](https://arxiv.org/html/2512.25017v1#bib.bib4)]).

On the other hand, deep gradient flow methods (DGFMs), also known as deep Ritz methods, formulate the PDE as an energy minimization problem, where the energy is derived from the differential operator, which typically leads to a loss function that is easier to compute.
Moreover, they usually discretize the equation in time and train one network for each time step, instead of using a monolithic space-time discretization; see e.g., E and Yu [[6](https://arxiv.org/html/2512.25017v1#bib.bib6)], Liao and Ming [[20](https://arxiv.org/html/2512.25017v1#bib.bib20)], Georgoulis et¬†al. [[8](https://arxiv.org/html/2512.25017v1#bib.bib8)], Park et¬†al. [[25](https://arxiv.org/html/2512.25017v1#bib.bib25)], Papapantoleon and Rou [[24](https://arxiv.org/html/2512.25017v1#bib.bib24)], Bruna et¬†al. [[3](https://arxiv.org/html/2512.25017v1#bib.bib3)] for differential operators, and Georgoulis et¬†al. [[9](https://arxiv.org/html/2512.25017v1#bib.bib9)] for an integro-differential operator.
A comprehensive review of the available methods appears in the forthcoming book of Jentzen et¬†al. [[14](https://arxiv.org/html/2512.25017v1#bib.bib14)].

In the present article, we are interested in analyzing the error of deep gradient flow methods for the solution of PDEs.
Let us consider the PDE

|  |  |  |  |
| --- | --- | --- | --- |
|  | ut+ùíú‚Äãu=0,(t,x)‚àà[0,T]√óD,u‚Äã(0,x)=Œ¶‚Äã(x),x‚àà‚àÇD,\begin{split}u\_{t}+\mathcal{A}u&=0,\quad(t,x)\in[0,T]\times D,\\ u(0,x)&=\Phi(x),\quad x\in\partial D,\end{split} |  | (1.1) |

where ùíú\mathcal{A} is a differential operator, Œ¶\Phi determines the initial condition, TT is a (finite) time horizon, and D‚äÜ‚ÑùdD\subseteq\mathbb{R}^{d} is the domain of the PDE.
The DGFMs translate the PDE into an energy minimization problem, which is then computed using stochastic gradient descent or one of its variants (e.g. ADAM), and can be described in the following manner:

|  |  |  |  |
| --- | --- | --- | --- |
|  | uŒ∏,n‚ãÜ=arg‚Äãminv‚ààùíûŒ∏n‚Äã‚à´‚Ñì‚Äã(v‚Äã(x))‚Äãdx,u^{\star}\_{\theta,n}=\operatorname\*{arg\,min}\_{v\in\mathcal{C}^{n}\_{\theta}}\int\ell(v(x))\mathrm{d}x, |  | (1.2) |

where ùíûŒ∏n\mathcal{C}^{n}\_{\theta} denotes the space of neural networks with nn neurons where Œ∏\theta is the set of trainable parameters, while ‚Ñì\ell denotes the energy functional associated to the operator ùíú\mathcal{A}.

Let u‚ãÜu^{\star} denote the unique solution of¬†([1.1](https://arxiv.org/html/2512.25017v1#S1.E1 "Equation 1.1 ‚Ä£ 1. Introduction ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")).
We would like to analyze and study the difference between the true solution of¬†([1.1](https://arxiv.org/html/2512.25017v1#S1.E1 "Equation 1.1 ‚Ä£ 1. Introduction ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")) and the solution computed by the deep gradient flow methods, *i.e.* by the outcome of the minimization problem¬†([1.2](https://arxiv.org/html/2512.25017v1#S1.E2 "Equation 1.2 ‚Ä£ 1. Introduction ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")).
This difference is known as the generalization error in the machine learning literature, *i.e.*

|  |  |  |
| --- | --- | --- |
|  | ‚Ñ∞gen=‚Äñu‚ãÜ‚àíuŒ∏,n‚ãÜ‚Äñ.\mathcal{E}\_{\text{gen}}=\|u^{\star}-u^{\star}\_{\theta,n}\|. |  |

The generalization error ‚Ñ∞gen\mathcal{E}\_{\text{gen}} can be decomposed in three separate components:

* ‚Ä¢

  the quadrature error ‚Ñ∞quad\mathcal{E}\_{\text{quad}}, which refers to how well the integral in¬†([1.2](https://arxiv.org/html/2512.25017v1#S1.E2 "Equation 1.2 ‚Ä£ 1. Introduction ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")) is approximated by Monte Carlo simulations or another quadrature method;
* ‚Ä¢

  the approximation error ‚Ñ∞approx\mathcal{E}\_{\text{approx}}, which refers to how well the neural network vv can approximate the continuous function uu that solves the PDE¬†([1.1](https://arxiv.org/html/2512.25017v1#S1.E1 "Equation 1.1 ‚Ä£ 1. Introduction ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"));
* ‚Ä¢

  the training error ‚Ñ∞train\mathcal{E}\_{\text{train}}, which refers to how well GD or SGD approximate the true solution of the minimization problem¬†([1.2](https://arxiv.org/html/2512.25017v1#S1.E2 "Equation 1.2 ‚Ä£ 1. Introduction ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")).

Then, we have the error decomposition

|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚Ñ∞gen=‚Ñ∞train+‚Ñ∞quad+‚Ñ∞approx,\displaystyle\mathcal{E}\_{\text{gen}}=\mathcal{E}\_{\text{train}}+\mathcal{E}\_{\text{quad}}+\mathcal{E}\_{\text{approx}}, |  | (1.3) |

and the aim of the present paper is to study these errors and show that, as the numbers of neurons tends to infinity and the training time also tends to infinity, then the generalization error tends to zero, and the outcome of the deep gradient flow method indeed approximates the solution of the PDE.

There are several articles available that study the generalization error of deep learning methods for PDEs, typically focusing on the popular DGM and PINN methods.
These methods rely on approximability properties of neural networks and properties of quadrature methods in order to control the generalization error, while they typically consider only a posteriori estimates for the training error.
We refer the interested reader to Mishra and Molinaro [[22](https://arxiv.org/html/2512.25017v1#bib.bib22)] and Gazoulis et¬†al. [[7](https://arxiv.org/html/2512.25017v1#bib.bib7)] for results on PINNs, and the related while more general article of Loulakis and
Makridakis [[21](https://arxiv.org/html/2512.25017v1#bib.bib21)].
Moreover, several articles consider the approximation error of DMG and PINNs; see, for example, Sirignano and Spiliopoulos [[29](https://arxiv.org/html/2512.25017v1#bib.bib29)] and Shin et¬†al. [[28](https://arxiv.org/html/2512.25017v1#bib.bib28)].
The recent article of Jiang et¬†al. [[15](https://arxiv.org/html/2512.25017v1#bib.bib15)] considers the ‚Äúglobal‚Äù convergence of DGM and PINNs, which amounts to the convergence of the training error in our notation.
Combined with other available results, this article allows to deduce the convergence of the generalization error of these methods.
Compared to the extended literature on DGM and PINNs, there are significantly fewer papers on DGFMs; let us mention here the articles of Dondl et¬†al. [[5](https://arxiv.org/html/2512.25017v1#bib.bib5)] which focuses on the approximation error, and Jiao et¬†al. [[16](https://arxiv.org/html/2512.25017v1#bib.bib16)] which provides a convergence rate using the Rademacher complexities.

The aim of the present article is to provide convergence results on the generalization error of DGFMs under reasonable and verifiable hypothesis on the underlying PDEs.
The first part of this work focuses on the analysis of the approximation error, i.e. we show that there exists a neural network that approximates the solution of the PDE.
This result uses ideas from PDE theory, optimization and the calculus of variations, and is inspired by the seminal paper of Sirignano and Spiliopoulos [[29](https://arxiv.org/html/2512.25017v1#bib.bib29)].
The second part of this work focuses on the analysis of the training error, and we show that as the number of neurons tend to infinity and the training time also tends to infinity, then the outcome of the deep gradient flow method tends to the true solution of the PDE.
This result is inspired by the work of Jiang et¬†al. [[15](https://arxiv.org/html/2512.25017v1#bib.bib15)].
The quadrature error is the most well-understood error of the three, thus this work focuses on the other two errors.
Moreover, let us mention for the sake of completeness, that our method also induces a discretization error, from the time-stepping scheme.
However, as this error is also well-studied and understood, we have chosen to omit it from the discussion here.
The combination of these results, yields that the generalization error also tends to zero.

This article is organized as follows:
[2](https://arxiv.org/html/2512.25017v1#S2 "2. Deep gradient flow methods for PDEs ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs") provides an overview of deep gradient flow methods for the solution of PDEs.
[3](https://arxiv.org/html/2512.25017v1#S3 "3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs") studies the approximation error of DGFMs, using the variational formulation of PDEs and a tailored version of the universal approximation theorem.
[4](https://arxiv.org/html/2512.25017v1#S4 "4. Convergence of the training error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs") studies the training error of DGFMs; we first derive a gradient flow that the training process satisfies in the ‚Äúwide network limit‚Äù and then analyze the behavior of this flow as the training time tends to infinity.
Finally, the appendices contain auxilliary estimates and examples.

### 1.1. Notation

Let ‚Ñã\mathcal{H} denote an arbitrary space, then ‚à•‚ãÖ‚à•‚Ñã\|\cdot\|\_{\mathcal{H}} denotes the norm, ‚ü®‚ãÖ,‚ãÖ‚ü©‚Ñã\langle\cdot,\cdot\rangle\_{\mathcal{H}} denotes the inner product, and wm‚áÄ‚Ñãww\_{m}\xrightharpoonup{\,\,\,\mathcal{H}\,\,\,}w denotes the weak convergence on this space.
We abbreviate spaces and norms as ‚Ñã=‚Ñã‚Äã(‚Ñùd)\mathcal{H}=\mathcal{H}(\mathbb{R}^{d}) and ‚Äñf‚Äñ‚Ñã=‚Äñf‚Äñ‚Ñã‚Äã(‚Ñùd)\left\|f\right\|\_{\mathcal{H}}=\left\|f\right\|\_{\mathcal{H}(\mathbb{R}^{d})}.

Let 1‚â§p<‚àû1\leq p<\infty and denote by Lp‚Äã(‚Ñùd)L^{p}(\mathbb{R}^{d}) the space of functions with finite pp-norm, where

|  |  |  |
| --- | --- | --- |
|  | ‚Äñf‚ÄñLp=(‚à´‚Ñùd|f‚Äã(x)|p‚Äãdx)1p,\left\|f\right\|\_{L^{p}}=\left(\int\_{\mathbb{R}^{d}}\left|f(x)\right|^{p}\mathrm{d}x\right)^{\frac{1}{p}}, |  |

while LlocpL^{p}\_{\text{loc}} denotes the space of functions in LpL^{p} that are locally integrable.
Let Cck‚Äã(‚Ñùd)C\_{c}^{k}\left(\mathbb{R}^{d}\right) denote the space of functions with compact support and continuous partial derivatives up to order kk.
Moreover, let W0k,p‚Äã(‚Ñùd)W\_{0}^{k,p}(\mathbb{R}^{d}) denote the Sobolev space with norm

|  |  |  |
| --- | --- | --- |
|  | ‚Äñf‚ÄñW0k,p=(‚àë|Œ±|‚â§k‚à´‚Ñùd|DŒ±‚Äãf‚Äã(x)|p‚Äãdx)1p<‚àû,\left\|f\right\|\_{W\_{0}^{k,p}}=\left(\sum\_{\left|\alpha\right|\leq k}\int\_{\mathbb{R}^{d}}\left|D^{\alpha}f(x)\right|^{p}\mathrm{d}x\right)^{\frac{1}{p}}<\infty, |  |

with DŒ±‚ÄãfD^{\alpha}f the weak derivative of ff and Œ±\alpha a multi-index.
Let us introduce the shorthand notation ‚Ñã0k‚Äã(‚Ñùd):=W0k,2‚Äã(‚Ñùd)\mathcal{H}\_{0}^{k}(\mathbb{R}^{d}):=W\_{0}^{k,2}(\mathbb{R}^{d}) for Sobolev spaces, and let ‚Ñã‚àí1‚Äã(‚Ñùd)\mathcal{H}^{-1}(\mathbb{R}^{d}) denote the dual space of ‚Ñã01‚Äã(‚Ñùd)\mathcal{H}\_{0}^{1}(\mathbb{R}^{d}).

Finally, let ùí±‚äÇ‚Ñã‚äÇùí±‚àó\mathcal{V}\subset\mathcal{H}\subset\mathcal{V}^{\*} denote a Gelfand triple, in which ‚Ñã\mathcal{H} is a separable Hilbert space, ùí±\mathcal{V} is a Banach space and ùí±‚àó\mathcal{V}^{\*} is the topological dual of ùí±\mathcal{V}.

###### Definition 1.1 (Self-adjoint operator).

An operator ‚Ñí:ùí±‚Üíùí±‚àó\mathcal{L}:\mathcal{V}\to\mathcal{V}^{\*} is self-adjoint if

|  |  |  |
| --- | --- | --- |
|  | ‚ü®‚Ñí‚Äãu,v‚ü©ùí±‚àó,ùí±=‚ü®‚Ñí‚Äãv,u‚ü©ùí±‚àó,ùí±for all¬†‚Äãu,v‚ààùí±.\left\langle\mathcal{L}u,v\right\rangle\_{\mathcal{V}^{\*},\mathcal{V}}=\left\langle\mathcal{L}v,u\right\rangle\_{\mathcal{V}^{\*},\mathcal{V}}\quad\text{for all \ }u,v\in\mathcal{V}. |  |

###### Remark 1.2.

The inner product ‚ü®‚Ñí‚Äãu,v‚ü©ùí±‚àó,ùí±\left\langle\mathcal{L}u,v\right\rangle\_{\mathcal{V}^{\*},\mathcal{V}} means that ‚Ñí‚Äãu\mathcal{L}u acts on vv as a functional.
An important example is the following: ùí±=‚Ñã01‚Äã(‚Ñùd)\mathcal{V}=\mathcal{H}\_{0}^{1}(\mathbb{R}^{d}), ùí±‚àó=‚Ñã‚àí1‚Äã(‚Ñùd)\mathcal{V}^{\*}=\mathcal{H}^{-1}(\mathbb{R}^{d}), ‚Ñã=L2‚Äã(‚Ñùd)\mathcal{H}=L^{2}(\mathbb{R}^{d}), and ‚Ñí=‚àíŒî\mathcal{L}=-\Delta, where Œî\Delta denotes the Laplace operator.
Then, we define the functional ‚Ñí‚Äãu\mathcal{L}u as follows

|  |  |  |
| --- | --- | --- |
|  | ‚ü®‚Ñí‚Äãu,v‚ü©‚Ñã‚àí1,‚Ñã01=‚ü®‚àíŒî‚Äãu,v‚ü©‚Ñã‚àí1,‚Ñã01:=‚ü®‚àáu,‚àáv‚ü©L2.\displaystyle\left\langle\mathcal{L}u,v\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}}=\left\langle-\Delta u,v\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}}:=\left\langle\nabla u,\nabla v\right\rangle\_{L^{2}}. |  |

## 2. Deep gradient flow methods for PDEs

Let us start by providing an overview of deep gradient flow methods (DGFMs) for the solution of PDEs.
These methods have gained increased popularity in the literature because they can efficiently handle high-dimensional PDEs stemming from physics, engineering, and finance; see e.g. E and Yu [[6](https://arxiv.org/html/2512.25017v1#bib.bib6)], Liao and Ming [[20](https://arxiv.org/html/2512.25017v1#bib.bib20)], Georgoulis et¬†al. [[8](https://arxiv.org/html/2512.25017v1#bib.bib8)], Park et¬†al. [[25](https://arxiv.org/html/2512.25017v1#bib.bib25)] and Papapantoleon and Rou [[24](https://arxiv.org/html/2512.25017v1#bib.bib24)] for differential operators, and Georgoulis et¬†al. [[9](https://arxiv.org/html/2512.25017v1#bib.bib9)] for an integro-differential operator.
Deep gradient flow methods reformulate the PDE as an energy minimization problem, which is then approximated in a time-stepping fashion by deep artificial neural networks.
This method results in a loss function that is tailor-made to the PDE at hand, avoids the use of a second derivative, which is computationally costly, and reduces the training time compared to, for instance, the DGM of Sirignano and Spiliopoulos [[29](https://arxiv.org/html/2512.25017v1#bib.bib29)]; see e.g. Georgoulis et¬†al. [[8](https://arxiv.org/html/2512.25017v1#bib.bib8), Sec.¬†5].

Let u‚Äã(t,x):[0,T]√ó‚Ñùd‚Üí‚Ñùu\left(t,x\right):\left[0,T\right]\times\mathbb{R}^{d}\to\mathbb{R} be the solution of the following partial (integro-)differential equation:

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | ut+ùíú‚Äãu\displaystyle u\_{t}+\mathcal{A}u | =0,u‚Äã(0)=u0,\displaystyle=0,\quad u(0)=u\_{0}, |  | (2.1) |

where ùíú\mathcal{A} is an operator from ùí±\mathcal{V} to ùí±‚àó\mathcal{V}^{\*} and u0‚àà‚Ñãu\_{0}\in\mathcal{H} is the initial condition.
In order to write the PDE as an energy minimization problem, we need to split the operator in a symmetric and an (asymmetric) remainder part, *i.e.*

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | ùíú‚Äãu\displaystyle\mathcal{A}u | =‚Ñí‚Äãu+F‚Äã(u),\displaystyle=\mathcal{L}u+F(u), |  | (2.2) |

where ‚Ñí\mathcal{L} is a self-adjoint, linear operator and FF is a (possibly non-linear) operator from ùí±\mathcal{V} to ùí±‚àó\mathcal{V}^{\*}.
This PDE is then discretized using, for example, the backward Euler differentiation scheme, which yields

|  |  |  |
| --- | --- | --- |
|  | Uk‚àíUk‚àí1h+‚Ñí‚ÄãUk+F‚Äã(Uk‚àí1)=0,U0=u0,\frac{U^{k}-U^{k-1}}{h}+\mathcal{L}U^{k}+F\Big(U^{k-1}\Big)=0,\quad U^{0}=u\_{0}, |  |

where UkU^{k} denotes the approximation to the solution of the PDE u‚Äã(tk)u(t\_{k}) at time step tkt\_{k}, on an appropriate grid.
The variational formulation of this equation yields an energy functional Ik‚Äã(v)I^{k}(v) such that UkU^{k} is a critical point of IkI^{k}, where

|  |  |  |
| --- | --- | --- |
|  | Ik‚Äã(v)=12‚Äã‚Äñv‚àíUk‚àí1‚Äñ‚Ñã2+h2‚Äã‚ü®‚Ñí‚Äãv,v‚ü©ùí±‚àó,ùí±+h‚Äã‚ü®F‚Äã(Uk‚àí1),v‚ü©‚Ñã.I^{k}(v)=\frac{1}{2}\left\|v-U^{k-1}\right\|\_{\mathcal{H}}^{2}+\frac{h}{2}\left\langle\mathcal{L}v,v\right\rangle\_{\mathcal{V}^{\*},\mathcal{V}}+h\left\langle F\left(U^{k-1}\right),v\right\rangle\_{\mathcal{H}}. |  |

The function vv is approximated by artificial neural networks which are trained using the stochastic gradient descent (SGD) algorithm, or one of its variants, while the functional IkI^{k} provides a loss function for the SGD iterations which is tailor-made for this problem.
The aim of this paper is to show that this procedure converges to the true solution u‚ãÜu^{\star} of the PDE ([2.1](https://arxiv.org/html/2512.25017v1#S2.E1 "Equation 2.1 ‚Ä£ 2. Deep gradient flow methods for PDEs ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")).

Next, we present examples of PDEs that have been treated by DGFMs, and their applications.

###### Example 2.1 (Heat equation).

The simplest example that fits this framework is the celebrated heat equation, which reads

|  |  |  |
| --- | --- | --- |
|  | ut=Œ∫‚ÄãŒî‚Äãu,Œ∫>0,u\_{t}=\kappa\Delta u,\quad\kappa>0, |  |

subject to an initial condition.
Then ùíú=‚Ñí=‚àíŒ∫‚ÄãŒî\mathcal{A}=\mathcal{L}=-\kappa\Delta and F‚Äã(u)=0F(u)=0.

###### Example 2.2.

Georgoulis et¬†al. [[8](https://arxiv.org/html/2512.25017v1#bib.bib8)] consider dissipative evolution PDEs of the following form

|  |  |  |
| --- | --- | --- |
|  | ut‚àí‚àá‚ãÖ(A‚Äã‚àáu)=F,u\_{t}-\nabla\cdot(A\nabla u)=F, |  |

subject to appropriate initial and terminal conditions, where AA is a symmetric, uniformly positive definite and bounded diffusion tensor and FF is a suitable function.
Then, we have that ùíú=‚Ñí=‚àí‚àá‚ãÖ(A‚Äã‚àáu)\mathcal{A}=\mathcal{L}=-\nabla\cdot(A\nabla u).

###### Example 2.3 (Option pricing PDEs).

PDEs arising in the valuation of financial derivatives fit naturally in this setting.
In the Black and Scholes [[2](https://arxiv.org/html/2512.25017v1#bib.bib2)] model, for example, we have directly that

|  |  |  |
| --- | --- | --- |
|  | ‚Ñí‚Äãu=‚àíœÉ22‚ÄãŒî‚Äãu+r‚Äãu¬†and¬†F‚Äã(u)=(œÉ22‚àír)‚Äã‚àáu.\mathcal{L}u=-\frac{\sigma^{2}}{2}\Delta u+ru\quad\text{ and }\quad F(u)=\Big(\frac{\sigma^{2}}{2}-r\Big)\nabla u. |  |

Here rr and œÉ\sigma are positive parameters that denote the risk-free interest rate and the asset volatility respectively.

More general and more realistic diffusion models also fit in this framework.
Let us consider the Heston [[11](https://arxiv.org/html/2512.25017v1#bib.bib11)] model as an example, where SS denotes the asset price process and VV the variance process.
The option pricing PDE in this model takes the form ([2.2](https://arxiv.org/html/2512.25017v1#S2.E2 "Equation 2.2 ‚Ä£ 2. Deep gradient flow methods for PDEs ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")) with

|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚Ñí‚Äãu=‚àí‚àá‚ãÖ(A‚Äã‚àáu)+r‚Äãu¬†and¬†F‚Äã(u)=ùêõ‚ãÖ‚àáu,\mathcal{L}u=-\nabla\cdot(A\nabla u)+ru\quad\text{ and }\quad F(u)=\mathbf{b}\cdot\nabla u, |  | (2.3) |

where

|  |  |  |
| --- | --- | --- |
|  | A=V2‚Äã[S2Œ∑‚ÄãœÅ‚ÄãSŒ∑‚ÄãœÅ‚ÄãSŒ∑2]¬†and¬†ùêõ=[(V‚àír+12‚ÄãœÅ‚ÄãŒ∑)‚ÄãSŒ∫‚Äã(V‚àíŒ∏)+12‚ÄãŒ∑‚ÄãœÅ‚ÄãV+Œ∑22].A=\frac{V}{2}\begin{bmatrix}S^{2}&\eta\rho S\\ \eta\rho S&\eta^{2}\end{bmatrix}\quad\text{ and }\quad\mathbf{b}=\begin{bmatrix}(V-r+\frac{1}{2}\rho\eta)S\\ \kappa(V-\theta)+\frac{1}{2}\eta\rho V+\frac{\eta^{2}}{2}\end{bmatrix}. |  |

Here, Œ∑\eta denotes the volatility of the volatility, œÅ\rho the correlation between the Brownian motions driving the asset price and the variance process, Œ∏\theta the long term variance and Œ∫\kappa the reversion rate of the variance to Œ∏\theta.

###### Example 2.4 (Option pricing PIDEs).

Certain classes of partial integro-differential equations (PIDEs) arising in the pricing of financial derivatives can also be casted in this framework, in particular when the integro-differential operator is not ‚Äúsymmetrized‚Äù.
Let us consider, for example, the multi-dimensional Merton model as described in Georgoulis et¬†al. [[9](https://arxiv.org/html/2512.25017v1#bib.bib9)].
Then, the PIDE arising for the pricing of basket options can be described using ([2.3](https://arxiv.org/html/2512.25017v1#S2.E3 "Equation 2.3 ‚Ä£ Example 2.3 (Option pricing PDEs). ‚Ä£ 2. Deep gradient flow methods for PDEs ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")), where the operator ‚Ñí\mathcal{L} retains the same structure, while the function FF takes now the form

|  |  |  |
| --- | --- | --- |
|  | F‚Äã(u)=ùêõ‚ãÖ‚àáu‚àíŒª‚Äã‚à´‚Ñùd(u‚Äã(x‚Äãez)‚àíu‚Äã(x))‚ÄãŒΩ‚Äã(d‚Äãz),F(u)=\mathbf{b}\cdot\nabla u-\lambda\int\_{\mathbb{R}^{d}}\big(u\left(x\mathrm{e}^{z}\right)-u(x)\big)\nu(\mathrm{d}z), |  |

where ŒΩ\nu denotes the multivariate normal density function.

###### Example 2.5 (Allen‚ÄìCahn equation).

Park et¬†al. [[25](https://arxiv.org/html/2512.25017v1#bib.bib25)] consider the example of the two-dimensional Allen‚ÄìCahn equation:

|  |  |  |
| --- | --- | --- |
|  | ut=Œî‚Äãu‚àíœµ‚àí2‚ÄãW‚Ä≤‚Äã(u),\displaystyle u\_{t}=\Delta u-\epsilon^{-2}W^{\prime}(u), |  |

with appropriate initial and boundary conditions, where WW is a double well potential; for instance, W‚Äã(u)=(u2‚àí1)24W(u)=\frac{(u^{2}-1)^{2}}{4}.
Then ‚Ñí‚Äãu=‚àíŒî‚Äãu+œµ‚àí2‚ÄãW‚Ä≤‚Äã(u)\mathcal{L}u=-\Delta u+\epsilon^{-2}W^{\prime}(u) and F‚Äã(u)=0F(u)=0.

## 3. Convergence of the approximation error

In this section, we show that the approximation error of the deep gradient flow method converges to zero, *i.e.* we consider a neural network with a single layer and prove that as the number of nodes in the network tends to infinity, there exists a neural network that converges to the solution of the PDE. This proof consists of several steps.
First, we show that the problem is well-posed in [Section¬†3.1](https://arxiv.org/html/2512.25017v1#S3.SS1 "3.1. Well-posedness ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs").
Second, we prove convergence of the time-stepping scheme in [Section¬†3.2](https://arxiv.org/html/2512.25017v1#S3.SS2 "3.2. Time stepping ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs").
Third, we prove the equivalence between the discretized PDE and the minimization of the variational formulation in [Section¬†3.3](https://arxiv.org/html/2512.25017v1#S3.SS3 "3.3. Weak formulation and uniqueness of minimizer ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs").
Fourth, we prove a version of the universal approximation theorem (UAT) in [Section¬†3.4](https://arxiv.org/html/2512.25017v1#S3.SS4 "3.4. Neural network approximation and a version of the Universal Approximation Theorem ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs").
Finally, in [Section¬†3.5](https://arxiv.org/html/2512.25017v1#S3.SS5 "3.5. Convergence of the minimizer ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"), we deduce the convergence of the neural network approximation to the solution of the minimization problem by utilizing the UAT.

In the sequel, we consider the following Gelfand triple: ùí±=‚Ñã01‚Äã(‚Ñùd)\mathcal{V}=\mathcal{H}\_{0}^{1}(\mathbb{R}^{d}), ùí±‚àó=‚Ñã‚àí1‚Äã(‚Ñùd)\mathcal{V}^{\*}=\mathcal{H}^{-1}(\mathbb{R}^{d}) and ‚Ñã=L2‚Äã(‚Ñùd)\mathcal{H}=L^{2}(\mathbb{R}^{d}).
Let us consider the PDE ([2.1](https://arxiv.org/html/2512.25017v1#S2.E1 "Equation 2.1 ‚Ä£ 2. Deep gradient flow methods for PDEs ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"))‚Äì([2.2](https://arxiv.org/html/2512.25017v1#S2.E2 "Equation 2.2 ‚Ä£ 2. Deep gradient flow methods for PDEs ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")) and assume that the operators ‚Ñí\mathcal{L} and FF satisfy the following conditions.

###### Assumption (CON).

Assume that the operators ‚Ñí\mathcal{L} and FF satisfy the following inequalities, for any u,v‚àà‚Ñã01‚Äã(‚Ñùd)u,v\in\mathcal{H}\_{0}^{1}(\mathbb{R}^{d}),

|  |  |  |
| --- | --- | --- |
|  | |‚ü®‚Ñí‚Äãu,v‚ü©‚Ñã‚àí1,‚Ñã01|‚â§M‚Äã‚Äñu‚Äñ‚Ñã01‚Äã‚Äñv‚Äñ‚Ñã01and‚ÄñF‚Äã(u)‚ÄñL2‚â§M‚Äã‚Äñu‚Äñ‚Ñã01,\displaystyle\left|\left\langle\mathcal{L}u,v\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}}\right|\leq M\left\|u\right\|\_{\mathcal{H}\_{0}^{1}}\left\|v\right\|\_{\mathcal{H}\_{0}^{1}}\quad\text{and}\quad\left\|F(u)\right\|\_{L^{2}}\leq M\left\|u\right\|\_{\mathcal{H}\_{0}^{1}}, |  |

where M>0M>0 is a constant.

###### Assumption (G√Ö).

The operator ‚Ñí\mathcal{L} satisfies the G√•rding inequality, *i.e.* there exist constants Œª1>0,Œª2‚â•0\lambda\_{1}>0,\lambda\_{2}\geq 0 such that, for any u‚àà‚Ñã01‚Äã(‚Ñùd),u\in\mathcal{H}\_{0}^{1}(\mathbb{R}^{d}), holds

|  |  |  |
| --- | --- | --- |
|  | ‚ü®‚Ñí‚Äãu,u‚ü©‚Ñã‚àí1,‚Ñã01‚â•Œª1‚Äã‚Äñu‚Äñ‚Ñã012‚àíŒª2‚Äã‚Äñu‚ÄñL22.\displaystyle\left\langle\mathcal{L}u,u\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}}\geq\lambda\_{1}\left\|u\right\|\_{\mathcal{H}\_{0}^{1}}^{2}-\lambda\_{2}\left\|u\right\|^{2}\_{L^{2}}. |  |

###### Assumption (SA).

The operator ‚Ñí\mathcal{L} is self-adjoint and positive definite.

###### Assumption (LIP).

The operator FF satisfies an estimate of the form

|  |  |  |
| --- | --- | --- |
|  | ‚ÄñF‚Äã(v)‚àíF‚Äã(w)‚Äñ‚Ñã‚àí1‚â§Œª‚Äã‚Äñv‚àíw‚Äñ‚Ñã01+Œº‚Äã‚Äñv‚àíw‚ÄñL2,\left\|F(v)-F(w)\right\|\_{\mathcal{H}^{-1}}\leq\lambda\left\|v-w\right\|\_{\mathcal{H}^{1}\_{0}}+\mu\left\|v-w\right\|\_{L^{2}}, |  |

for all v,w‚àà{v‚àà‚Ñã01:minx‚Å°‚Äñu‚Äã(x)‚àív‚Äñ‚Ñã01‚â§1}v,w\in\left\{v\in\mathcal{H}\_{0}^{1}:\min\_{x}\left\|u(x)-v\right\|\_{\mathcal{H}^{1}\_{0}}\leq 1\right\}, where Œª<1\lambda<1 and Œº‚àà‚Ñù\mu\in\mathbb{R}.

###### Remark 3.1.

The examples of PDEs considered in the previous section typically satisfy these assumptions.
More details, focusing on the option pricing PDEs of [Examples¬†2.3](https://arxiv.org/html/2512.25017v1#S2.Thmtheorem3 "Example 2.3 (Option pricing PDEs). ‚Ä£ 2. Deep gradient flow methods for PDEs ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs") and¬†[2.4](https://arxiv.org/html/2512.25017v1#S2.Thmtheorem4 "Example 2.4 (Option pricing PIDEs). ‚Ä£ 2. Deep gradient flow methods for PDEs ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"), are deferred to [Section¬†A.3](https://arxiv.org/html/2512.25017v1#A1.SS3 "A.3. Examples ‚Ä£ Appendix A Auxiliary results ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs").

### 3.1. Well-posedness

Let us first discuss the existence and uniqueness of solutions for equation ([2.1](https://arxiv.org/html/2512.25017v1#S2.E1 "Equation 2.1 ‚Ä£ 2. Deep gradient flow methods for PDEs ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")).

###### Theorem 3.2 (Well-posedness).

Assume that the operators ‚Ñí\mathcal{L} and FF satisfy [Assumptions¬†(CON)](https://arxiv.org/html/2512.25017v1#Thmassumption1 "Assumption (CON). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs") and¬†[(G√Ö)](https://arxiv.org/html/2512.25017v1#Thmassumption2 "Assumption (G√Ö). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"), then equation ([2.1](https://arxiv.org/html/2512.25017v1#S2.E1 "Equation 2.1 ‚Ä£ 2. Deep gradient flow methods for PDEs ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")) admits a unique weak solution u‚ààL2‚Äã((0,T);‚Ñã01‚Äã(‚Ñùd))‚à©‚Ñã1‚Äã((0,T);‚Ñã‚àí1‚Äã(‚Ñùd))u\in L^{2}\left(\left(0,T\right);\mathcal{H}\_{0}^{1}(\mathbb{R}^{d})\right)\cap\mathcal{H}^{1}\left(\left(0,T\right);\mathcal{H}^{-1}(\mathbb{R}^{d})\right), that satisfies

|  |  |  |
| --- | --- | --- |
|  | dd‚Äãt‚Äã‚ü®u,v‚ü©L2+‚ü®‚Ñí‚Äãu,v‚ü©‚Ñã‚àí1,‚Ñã01+‚ü®F‚Äã(u),v‚ü©L2=0\displaystyle\frac{\mathrm{d}}{\mathrm{d}t}\left\langle u,v\right\rangle\_{L^{2}}+\left\langle\mathcal{L}u,v\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}}+\left\langle F(u),v\right\rangle\_{L^{2}}=0 |  |

for any v‚àà‚Ñã01‚Äã(‚Ñùd)v\in\mathcal{H}\_{0}^{1}(\mathbb{R}^{d}) and u‚Äã(0)=u0u\left(0\right)=u\_{0}.

###### Proof.

According to Hilber et¬†al. [[12](https://arxiv.org/html/2512.25017v1#bib.bib12), Theorem 3.2.2], we only need to verify that the bilinear form ‚ü®ùíú‚Äãu,v‚ü©‚Ñã‚àí1,‚Ñã01\left\langle\mathcal{A}u,v\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}} is continuous and satisfies the ‚ÄúG√•rding inequality‚Äù, where

|  |  |  |
| --- | --- | --- |
|  | ‚ü®ùíú‚Äãu,v‚ü©‚Ñã‚àí1,‚Ñã01=‚ü®‚Ñí‚Äãu,v‚ü©‚Ñã‚àí1,‚Ñã01+‚ü®F‚Äã(u),v‚ü©L2.\left\langle\mathcal{A}u,v\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}}=\left\langle\mathcal{L}u,v\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}}+\left\langle F(u),v\right\rangle\_{L^{2}}. |  |

The continuity follows directly from [Assumption¬†(CON)](https://arxiv.org/html/2512.25017v1#Thmassumption1 "Assumption (CON). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs") and the Cauchy‚ÄìSchwarz inequality, since

|  |  |  |  |
| --- | --- | --- | --- |
|  | |‚ü®ùíú‚Äãu,v‚ü©‚Ñã‚àí1,‚Ñã01|\displaystyle\left|\left\langle\mathcal{A}u,v\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}}\right| | ‚â§|‚ü®‚Ñí‚Äãu,v‚ü©‚Ñã‚àí1,‚Ñã01|+|‚ü®F‚Äã(u),v‚ü©L2|\displaystyle\leq\left|\left\langle\mathcal{L}u,v\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}}\right|+\left|\left\langle F(u),v\right\rangle\_{L^{2}}\right| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§M‚Äã[‚Äñu‚Äñ‚Ñã01‚Äã‚Äñv‚Äñ‚Ñã01+‚Äñu‚Äñ‚Ñã01‚Äã‚Äñv‚ÄñL2]‚â§2‚ÄãM‚Äã‚Äñu‚Äñ‚Ñã01‚Äã‚Äñv‚Äñ‚Ñã01.\displaystyle\leq M\left[\left\|u\right\|\_{\mathcal{H}\_{0}^{1}}\left\|v\right\|\_{\mathcal{H}\_{0}^{1}}+\left\|u\right\|\_{\mathcal{H}\_{0}^{1}}\left\|v\right\|\_{L^{2}}\right]\leq 2M\left\|u\right\|\_{\mathcal{H}\_{0}^{1}}\left\|v\right\|\_{\mathcal{H}\_{0}^{1}}. |  |

Let us also verify that the bilinear form satisfies the G√•rding inequality, *i.e.* that there exist C1,C2>0C\_{1},C\_{2}>0, such that

|  |  |  |
| --- | --- | --- |
|  | |‚ü®ùíú‚Äãu,u‚ü©‚Ñã‚àí1,‚Ñã01|‚â•C1‚Äã‚Äñu‚Äñ‚Ñã012‚àíC2‚Äã‚Äñu‚ÄñL22.\left|\left\langle\mathcal{A}u,u\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}}\right|\geq C\_{1}\left\|u\right\|\_{\mathcal{H}\_{0}^{1}}^{2}-C\_{2}\left\|u\right\|\_{L^{2}}^{2}. |  |

We have that

|  |  |  |  |
| --- | --- | --- | --- |
|  | |‚ü®ùíú‚Äãu,u‚ü©‚Ñã‚àí1,‚Ñã01|\displaystyle\left|\left\langle\mathcal{A}u,u\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}}\right| | ‚â•|‚ü®‚Ñí‚Äãu,u‚ü©‚Ñã‚àí1,‚Ñã01|‚àí|‚ü®F‚Äã(u),u‚ü©L2|\displaystyle\geq\left|\left\langle\mathcal{L}u,u\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}}\right|-\left|\left\langle F(u),u\right\rangle\_{L^{2}}\right| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â•Œª1‚Äã‚Äñu‚Äñ‚Ñã012‚àíŒª2‚Äã‚Äñu‚ÄñL22‚àíM‚Äã‚Äñu‚Äñ‚Ñã01‚Äã‚Äñu‚ÄñL2\displaystyle\geq\lambda\_{1}\left\|u\right\|\_{\mathcal{H}\_{0}^{1}}^{2}-\lambda\_{2}\left\|u\right\|^{2}\_{L^{2}}-M\left\|u\right\|\_{\mathcal{H}\_{0}^{1}}\left\|u\right\|\_{L^{2}} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â•Œª1‚Äã‚Äñu‚Äñ‚Ñã012‚àíŒª2‚Äã‚Äñu‚ÄñL22‚àíM‚Äã(Œª12‚ÄãM‚Äã‚Äñu‚Äñ‚Ñã012+M2‚ÄãŒª1‚Äã‚Äñu‚ÄñL22)\displaystyle\geq\lambda\_{1}\left\|u\right\|\_{\mathcal{H}\_{0}^{1}}^{2}-\lambda\_{2}\left\|u\right\|^{2}\_{L^{2}}-M\left(\frac{\lambda\_{1}}{2M}\left\|u\right\|\_{\mathcal{H}\_{0}^{1}}^{2}+\frac{M}{2\lambda\_{1}}\left\|u\right\|\_{L^{2}}^{2}\right) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =Œª12‚Äã‚Äñu‚Äñ‚Ñã012‚àí(Œª2+M22‚ÄãŒª1)‚Äã‚Äñu‚ÄñL22,\displaystyle=\frac{\lambda\_{1}}{2}\left\|u\right\|\_{\mathcal{H}\_{0}^{1}}^{2}-\left(\lambda\_{2}+\frac{M^{2}}{2\lambda\_{1}}\right)\left\|u\right\|^{2}\_{L^{2}}, |  |

where have used [Assumptions¬†(CON)](https://arxiv.org/html/2512.25017v1#Thmassumption1 "Assumption (CON). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs") and¬†[(G√Ö)](https://arxiv.org/html/2512.25017v1#Thmassumption2 "Assumption (G√Ö). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs") and the Cauchy‚ÄìSchwarz inequality for the second step, and the Young inequality with Œµ=Œª1M\varepsilon=\frac{\lambda\_{1}}{M} for the third step.
‚àé

### 3.2. Time stepping

The second step is to discretize the PDE in time and prove that this discretization converges to the true solution as the time step tends to zero.
Consider the PDE in formulation ([2.1](https://arxiv.org/html/2512.25017v1#S2.E1 "Equation 2.1 ‚Ä£ 2. Deep gradient flow methods for PDEs ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"))‚Äì([2.2](https://arxiv.org/html/2512.25017v1#S2.E2 "Equation 2.2 ‚Ä£ 2. Deep gradient flow methods for PDEs ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")), *i.e.*

|  |  |  |
| --- | --- | --- |
|  | ut+‚Ñí‚Äãu+F‚Äã(u)=0,u‚Äã(0)=u0.\displaystyle u\_{t}+\mathcal{L}u+F(u)=0,\quad u(0)=u\_{0}. |  |

Let us divide [0,T][0,T] in KK intervals (tk‚àí1,tk](t\_{k-1},t\_{k}] with step size h=tk‚àítk‚àí1=1Kh=t\_{k}-t\_{k-1}=\frac{1}{K}.
Let UkU^{k} denote the approximation of u‚Äã(tk)u(t\_{k}) using the backward Euler discretization scheme, *i.e.*

|  |  |  |  |
| --- | --- | --- | --- |
|  | Uk‚àíUk‚àí1h+‚Ñí‚ÄãUk+F‚Äã(Uk‚àí1)=0,U0=u0.\frac{U^{k}-U^{k-1}}{h}+\mathcal{L}U^{k}+F\left(U^{k-1}\right)=0,\quad U^{0}=u\_{0}. |  | (3.1) |

###### Theorem 3.3.

Assume that the operators ‚Ñí\mathcal{L} and FF satisfy [Assumptions¬†(CON)](https://arxiv.org/html/2512.25017v1#Thmassumption1 "Assumption (CON). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"), [(G√Ö)](https://arxiv.org/html/2512.25017v1#Thmassumption2 "Assumption (G√Ö). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"), [(SA)](https://arxiv.org/html/2512.25017v1#Thmassumption3 "Assumption (SA). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs") and¬†[(LIP)](https://arxiv.org/html/2512.25017v1#Thmassumption4 "Assumption (LIP). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs").
Then, there exists a constant CC independent of hh and kk such that, for hh sufficiently small, holds

|  |  |  |
| --- | --- | --- |
|  | max0‚â§k‚â§K‚Å°‚Äñu‚Äã(tk)‚àíUk‚ÄñL2‚â§C‚Äãh.\max\_{0\leq k\leq K}\left\|u(t\_{k})-U^{k}\right\|\_{L^{2}}\leq Ch. |  |

###### Proof.

The proof follows directly from Theorem 2.1 in Akrivis et¬†al. [[1](https://arxiv.org/html/2512.25017v1#bib.bib1)].
Indeed, using that U0=u‚Äã(0)U^{0}=u\left(0\right), we can show by direct, but tedious, calculations that the assumptions of [[1](https://arxiv.org/html/2512.25017v1#bib.bib1), p.¬†523] are satisfied for Œª<1\lambda<1 and q=1q=1.
‚àé

### 3.3. Weak formulation and uniqueness of minimizer

The third step is to reformulate equation ([3.1](https://arxiv.org/html/2512.25017v1#S3.E1 "Equation 3.1 ‚Ä£ 3.2. Time stepping ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")) as a variational problem and prove that its solution is equivalent to the minimization of an energy functional.
Let us first rewrite ([3.1](https://arxiv.org/html/2512.25017v1#S3.E1 "Equation 3.1 ‚Ä£ 3.2. Time stepping ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")) as follows

|  |  |  |  |
| --- | --- | --- | --- |
|  | (Uk‚àíUk‚àí1)+h‚Äã(‚Ñí‚ÄãUk+F‚Äã(Uk‚àí1))=0,U0=u0.\left(U^{k}-U^{k-1}\right)+h\left(\mathcal{L}U^{k}+F\left(U^{k-1}\right)\right)=0,\quad U^{0}=u\_{0}. |  | (3.2) |

We want to find an energy functional Ik‚Äã(u)I^{k}(u) such that UkU^{k} is a critical point of IkI^{k}.
Consider the following functional IkI^{k} on ‚Ñã01‚Äã(‚Ñùd)\mathcal{H}\_{0}^{1}(\mathbb{R}^{d})

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | Ik‚Äã(u)\displaystyle I^{k}(u) | =12‚Äã‚Äñu‚àíUk‚àí1‚ÄñL22+h2‚Äã‚ü®‚Ñí‚Äãu,u‚ü©‚Ñã‚àí1,‚Ñã01+h‚Äã‚ü®F‚Äã(Uk‚àí1),u‚ü©L2\displaystyle=\frac{1}{2}\left\|u-U^{k-1}\right\|^{2}\_{L^{2}}+\frac{h}{2}\left\langle\mathcal{L}u,u\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}}+h\left\langle F\left(U^{k-1}\right),u\right\rangle\_{L^{2}} |  | (3.3) |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =:‚Ñ≥k(u)+ùí¢k(u),\displaystyle=:\mathcal{M}^{k}(u)+\mathcal{G}^{k}(u), |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
| where | | | | |
|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚Ñ≥k‚Äã(u)\displaystyle\mathcal{M}^{k}(u) | =12‚Äã‚Äñu‚ÄñL22+h2‚Äã‚ü®‚Ñí‚Äãu,u‚ü©‚Ñã‚àí1,‚Ñã01\displaystyle=\frac{1}{2}\left\|u\right\|^{2}\_{L^{2}}+\frac{h}{2}\left\langle\mathcal{L}u,u\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}} |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
| and | | | | |
|  |  |  |  |
| --- | --- | --- | --- |
|  | ùí¢k‚Äã(u)\displaystyle\mathcal{G}^{k}(u) | =‚àí‚ü®u,Uk‚àí1‚ü©L2+12‚Äã‚ÄñUk‚àí1‚ÄñL22+h‚Äã‚ü®F‚Äã(Uk‚àí1),u‚ü©L2.\displaystyle=-\left\langle u,U^{k-1}\right\rangle\_{L^{2}}+\frac{1}{2}\left\|U^{k-1}\right\|^{2}\_{L^{2}}+h\left\langle F\left(U^{k-1}\right),u\right\rangle\_{L^{2}}. |  |

Here, ùí¢k\mathcal{G}^{k} is a linear functional and ‚Ñ≥k\mathcal{M}^{k} is a nonlinear (quadratic) term.

###### Theorem 3.4.

Assume that the operators ‚Ñí\mathcal{L} and FF satisfy [Assumptions¬†(CON)](https://arxiv.org/html/2512.25017v1#Thmassumption1 "Assumption (CON). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"), [(G√Ö)](https://arxiv.org/html/2512.25017v1#Thmassumption2 "Assumption (G√Ö). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"), [(SA)](https://arxiv.org/html/2512.25017v1#Thmassumption3 "Assumption (SA). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs") and¬†[(LIP)](https://arxiv.org/html/2512.25017v1#Thmassumption4 "Assumption (LIP). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs") and that 0<h<12‚ÄãŒª2,0<h<\frac{1}{2\lambda\_{2}}, where Œª2\lambda\_{2} is the constant from [Assumption¬†(G√Ö)](https://arxiv.org/html/2512.25017v1#Thmassumption2 "Assumption (G√Ö). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs").
Then, the minimizer of ([3.3](https://arxiv.org/html/2512.25017v1#S3.E3 "Equation 3.3 ‚Ä£ 3.3. Weak formulation and uniqueness of minimizer ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")) is the unique solution of ([3.2](https://arxiv.org/html/2512.25017v1#S3.E2 "Equation 3.2 ‚Ä£ 3.3. Weak formulation and uniqueness of minimizer ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")) in ‚Ñã01‚Äã(‚Ñùd)\mathcal{H}\_{0}^{1}(\mathbb{R}^{d}).

The proof of this theorem is based on the following two preparatory results.

###### Lemma 3.5.

Consider the setting of [Theorem¬†3.4](https://arxiv.org/html/2512.25017v1#S3.Thmtheorem4 "Theorem 3.4. ‚Ä£ 3.3. Weak formulation and uniqueness of minimizer ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs").
Then, the functional IkI^{k} is bounded from below and, for any w‚àó‚àà‚Ñã01‚Äã(‚Ñùd)w\_{\*}\in\mathcal{H}\_{0}^{1}(\mathbb{R}^{d}) and sequence wm‚áÄ‚Ñã01w‚àów\_{m}\xrightharpoonup{\mathcal{H}\_{0}^{1}}w\_{\*}, we have

|  |  |  |
| --- | --- | --- |
|  | lim infm‚Üí‚àûIk‚Äã(wm)‚â•Ik‚Äã(w‚àó).\displaystyle\liminf\_{m\to\infty}I^{k}\left(w\_{m}\right)\geq I^{k}\left(w\_{\*}\right). |  |

###### Proof.

Let us first prove that IkI^{k} is bounded from below.
Using the Cauchy‚ÄìSchwarz inequality and the inequality Œ±‚ÄãŒ≤‚â§Œ±2/4+Œ≤2\alpha\beta\leq\alpha^{2}/4+\beta^{2}, we get that

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùí¢k‚Äã(u)\displaystyle\mathcal{G}^{k}(u) | ‚â•‚àí‚ÄñUk‚àí1‚ÄñL2‚Äã‚Äñu‚ÄñL2+12‚Äã‚ÄñUk‚àí1‚ÄñL22‚àíh‚Äã‚ÄñF‚Äã(Uk‚àí1)‚ÄñL2‚Äã‚Äñu‚ÄñL2\displaystyle\geq-\left\|U^{k-1}\right\|\_{L^{2}}\left\|u\right\|\_{L^{2}}+\frac{1}{2}\left\|U^{k-1}\right\|^{2}\_{L^{2}}-h\left\|F\left(U^{k-1}\right)\right\|\_{L^{2}}\left\|u\right\|\_{L^{2}} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =‚àí‚Äñu‚ÄñL2‚Äã(‚ÄñUk‚àí1‚ÄñL2+h‚Äã‚ÄñF‚Äã(Uk‚àí1)‚ÄñL2)+12‚Äã‚ÄñUk‚àí1‚ÄñL22\displaystyle=-\left\|u\right\|\_{L^{2}}\Big(\left\|U^{k-1}\right\|\_{L^{2}}+h\left\|F\left(U^{k-1}\right)\right\|\_{L^{2}}\Big)+\frac{1}{2}\left\|U^{k-1}\right\|^{2}\_{L^{2}} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â•‚àí14‚Äã‚Äñu‚ÄñL22‚àí{‚ÄñUk‚àí1‚ÄñL2+h‚Äã‚ÄñF‚Äã(Uk‚àí1)‚ÄñL2}2+12‚Äã‚ÄñUk‚àí1‚ÄñL22.\displaystyle\geq-\frac{1}{4}\left\|u\right\|^{2}\_{L^{2}}-\left\{\left\|U^{k-1}\right\|\_{L^{2}}+h\left\|F\left(U^{k-1}\right)\right\|\_{L^{2}}\right\}^{2}+\frac{1}{2}\left\|U^{k-1}\right\|^{2}\_{L^{2}}. |  |

Hence, the functional IkI^{k} satisfies

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ik‚Äã(u)\displaystyle I^{k}(u) | =‚Ñ≥k‚Äã(u)+ùí¢k‚Äã(u)\displaystyle=\mathcal{M}^{k}(u)+\mathcal{G}^{k}(u) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â•14‚Äã‚Äñu‚ÄñL22+h2‚Äã‚ü®‚Ñí‚Äãu,u‚ü©‚Ñã‚àí1,‚Ñã01‚àí{‚ÄñUk‚àí1‚ÄñL2+h‚Äã‚ÄñF‚Äã(Uk‚àí1)‚ÄñL2}2+12‚Äã‚ÄñUk‚àí1‚ÄñL22.\displaystyle\geq\frac{1}{4}\left\|u\right\|^{2}\_{L^{2}}+\frac{h}{2}\left\langle\mathcal{L}u,u\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}}-\left\{\left\|U^{k-1}\right\|\_{L^{2}}+h\left\|F\left(U^{k-1}\right)\right\|\_{L^{2}}\right\}^{2}+\frac{1}{2}\left\|U^{k-1}\right\|^{2}\_{L^{2}}. |  |

Using [Assumption¬†(G√Ö)](https://arxiv.org/html/2512.25017v1#Thmassumption2 "Assumption (G√Ö). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs") and the condition h<12‚ÄãŒª2h<\frac{1}{2\lambda\_{2}}, we have that Ik‚Äã(u)I^{k}(u) is bounded below by

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ik‚Äã(u)\displaystyle I^{k}(u) | ‚â•14‚Äã‚Äñu‚ÄñL22+h2‚Äã‚ü®‚Ñí‚Äãu,u‚ü©‚Ñã‚àí1,‚Ñã01‚àíRk(1)\displaystyle\geq\frac{1}{4}\left\|u\right\|^{2}\_{L^{2}}+\frac{h}{2}\left\langle\mathcal{L}u,u\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}}-R^{\left(1\right)}\_{k} |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | ‚â•(14‚àíh‚ÄãŒª22)‚Äã‚Äñu‚ÄñL22+h‚ÄãŒª12‚Äã‚Äñu‚Äñ‚Ñã012‚àíRk(1),\displaystyle\geq\Big(\frac{1}{4}-\frac{h\lambda\_{2}}{2}\Big)\left\|u\right\|^{2}\_{L^{2}}+\frac{h\lambda\_{1}}{2}\left\|u\right\|^{2}\_{\mathcal{H}\_{0}^{1}}-R^{\left(1\right)}\_{k}, |  | (3.4) |

where the term Rk(1)R\_{k}^{(1)}, defined below, is independent of uu and finite

|  |  |  |  |
| --- | --- | --- | --- |
|  | Rk(1):={‚ÄñUk‚àí1‚ÄñL2+h‚Äã‚ÄñF‚Äã(Uk‚àí1)‚ÄñL2}2‚àí12‚Äã‚ÄñUk‚àí1‚ÄñL22.\displaystyle R^{\left(1\right)}\_{k}:=\left\{\left\|U^{k-1}\right\|\_{L^{2}}+h\left\|F\left(U^{k-1}\right)\right\|\_{L^{2}}\right\}^{2}-\frac{1}{2}\left\|U^{k-1}\right\|^{2}\_{L^{2}}. |  | (3.5) |

As for the second part, consider w‚àó‚àà‚Ñã01‚Äã(‚Ñùd)w\_{\*}\in\mathcal{H}\_{0}^{1}(\mathbb{R}^{d}) and a sequence (wm)m(w\_{m})\_{m} such that wm‚áÄ‚Ñã01w‚àów\_{m}\xrightharpoonup{\mathcal{H}\_{0}^{1}}w\_{\*} as m‚Üí‚àûm\to\infty.
Then, by the definition of weak convergence

|  |  |  |  |
| --- | --- | --- | --- |
|  | 12‚Äã‚ü®wm,w‚àó‚ü©L2+h2‚Äã‚ü®‚Ñí‚Äãw‚àó,wm‚ü©‚Ñã‚àí1,‚Ñã01\displaystyle\frac{1}{2}\left\langle w\_{m},w\_{\*}\right\rangle\_{L^{2}}+\frac{h}{2}\left\langle\mathcal{L}w\_{\*},w\_{m}\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}} | ‚Üím‚Üí‚àû12‚Äã‚Äñw‚àó‚ÄñL22+h2‚Äã‚ü®‚Ñí‚Äãw‚àó,w‚àó‚ü©‚Ñã‚àí1,‚Ñã01,\displaystyle\xrightarrow[\ m\to\infty\ ]{}\frac{1}{2}\left\|w\_{\*}\right\|^{2}\_{L^{2}}+\frac{h}{2}\left\langle\mathcal{L}w\_{\*},w\_{\*}\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}}, |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
| while for the linear part we also have that | | | | |
|  |  |  |  |
| --- | --- | --- | --- |
|  | ùí¢k‚Äã(wm)\displaystyle\mathcal{G}^{k}\left(w\_{m}\right) | ‚Üím‚Üí‚àûùí¢k‚Äã(w‚àó).\displaystyle\xrightarrow[\ m\to\infty\ ]{}\mathcal{G}^{k}\left(w\_{\*}\right). |  |

Consider now the functional

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ik‚Äã(wm)+Ik‚Äã(w‚àó)‚àíIk‚Äã(w‚àó‚àíwm)=‚ü®wm,w‚àó‚ü©L2+h‚Äã‚ü®‚Ñí‚Äãw‚àó,wm‚ü©‚Ñã‚àí1,‚Ñã01+2‚Äãùí¢k‚Äã(wm)‚èü‚ü∂ 2‚ÄãIk‚Äã(w‚àó),\displaystyle I^{k}\left(w\_{m}\right)+I^{k}\left(w\_{\*}\right)-I^{k}\left(w\_{\*}-w\_{m}\right)=\underbrace{\left\langle w\_{m},w\_{\*}\right\rangle\_{L^{2}}+h\left\langle\mathcal{L}w\_{\*},w\_{m}\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}}+2\mathcal{G}^{k}\left(w\_{m}\right)}\_{\longrightarrow\ 2I^{k}\left(w\_{\*}\right)}, |  | (3.6) |

and notice that

|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚Ñ≥k‚Äã(u)=12‚Äã‚Äñu‚ÄñL22+h2‚Äã‚ü®‚Ñí‚Äãu,u‚ü©‚Ñã‚àí1,‚Ñã01‚â•(12‚àíh‚ÄãŒª22)‚Äã‚Äñu‚ÄñL22+h‚ÄãŒª12‚Äã‚Äñu‚Äñ‚Ñã012‚â•0,\displaystyle\mathcal{M}^{k}(u)=\frac{1}{2}\left\|u\right\|^{2}\_{L^{2}}+\frac{h}{2}\left\langle\mathcal{L}u,u\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}}\geq\Big(\frac{1}{2}-\frac{h\lambda\_{2}}{2}\Big)\left\|u\right\|^{2}\_{L^{2}}+\frac{h\lambda\_{1}}{2}\left\|u\right\|^{2}\_{\mathcal{H}\_{0}^{1}}\geq 0, |  | (3.7) |

from [Assumption¬†(G√Ö)](https://arxiv.org/html/2512.25017v1#Thmassumption2 "Assumption (G√Ö). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs") and h<12‚ÄãŒª2h<\frac{1}{2\lambda\_{2}}.
Then, taking the limit as m‚Üí‚àûm\to\infty on both sides of ([3.6](https://arxiv.org/html/2512.25017v1#S3.E6 "Equation 3.6 ‚Ä£ Proof. ‚Ä£ 3.3. Weak formulation and uniqueness of minimizer ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")) and using ([3.7](https://arxiv.org/html/2512.25017v1#S3.E7 "Equation 3.7 ‚Ä£ Proof. ‚Ä£ 3.3. Weak formulation and uniqueness of minimizer ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")), we get that

|  |  |  |
| --- | --- | --- |
|  | lim infm‚Üí‚àûIk‚Äã(wm)+Ik‚Äã(w‚àó)‚àílim infm‚Üí‚àûIk‚Äã(w‚àó‚àíwm)‚èü‚â•0‚â•2‚ÄãIk‚Äã(w‚àó),\displaystyle\liminf\_{m\to\infty}I^{k}\left(w\_{m}\right)+I^{k}\left(w\_{\*}\right)-\underbrace{\liminf\_{m\to\infty}I^{k}\left(w\_{\*}-w\_{m}\right)}\_{\geq 0}\geq 2I^{k}\left(w\_{\*}\right), |  |

which implies lim infm‚Üí‚àûIk‚Äã(wm)‚â•Ik‚Äã(w‚àó)\liminf\_{m\to\infty}I^{k}\left(w\_{m}\right)\geq I^{k}\left(w\_{\*}\right).
‚àé

###### Proposition 3.6.

Consider the setting of [Theorem¬†3.4](https://arxiv.org/html/2512.25017v1#S3.Thmtheorem4 "Theorem 3.4. ‚Ä£ 3.3. Weak formulation and uniqueness of minimizer ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs").
Let Uk‚àí1‚àà‚Ñã01‚Äã(‚Ñùd)U^{k-1}\in\mathcal{H}\_{0}^{1}(\mathbb{R}^{d}), then there exists a unique minimizer in ‚Ñã01‚Äã(‚Ñùd)\mathcal{H}\_{0}^{1}(\mathbb{R}^{d}) of the functional IkI^{k}.

###### Proof.

Let us first show the uniqueness of the minimizer of the functional IkI^{k}.
Let w1,w2‚àà‚Ñã01‚Äã(‚Ñùd)w\_{1},w\_{2}\in\mathcal{H}\_{0}^{1}(\mathbb{R}^{d}) be two minimizers of IkI^{k} then, using [Assumptions¬†(SA)](https://arxiv.org/html/2512.25017v1#Thmassumption3 "Assumption (SA). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs") and¬†[(G√Ö)](https://arxiv.org/html/2512.25017v1#Thmassumption2 "Assumption (G√Ö). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"), we get

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ik‚Äã(w1)+Ik‚Äã(w2)‚àí2‚ÄãIk‚Äã(w1+w22)\displaystyle I^{k}\left(w\_{1}\right)+I^{k}\left(w\_{2}\right)-2I^{k}\left(\frac{w\_{1}+w\_{2}}{2}\right) | =14‚Äã‚Äñw1‚àíw2‚ÄñL22+h4‚Äã‚ü®‚Ñí‚Äã(w1‚àíw2),w1‚àíw2‚ü©‚Ñã‚àí1,‚Ñã01\displaystyle=\frac{1}{4}\left\|w\_{1}-w\_{2}\right\|^{2}\_{L^{2}}+\frac{h}{4}\left\langle\mathcal{L}(w\_{1}-w\_{2}),w\_{1}-w\_{2}\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â•(14‚àíh‚ÄãŒª24)‚Äã‚Äñw1‚àíw2‚ÄñL22+h‚ÄãŒª14‚Äã‚Äñw1‚àíw2‚Äñ‚Ñã012‚Äã‚â•([3.7](https://arxiv.org/html/2512.25017v1#S3.E7 "Equation 3.7 ‚Ä£ Proof. ‚Ä£ 3.3. Weak formulation and uniqueness of minimizer ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"))‚Äã0,\displaystyle\geq\Big(\frac{1}{4}-\frac{h\lambda\_{2}}{4}\Big)\left\|w\_{1}-w\_{2}\right\|^{2}\_{L^{2}}+\frac{h\lambda\_{1}}{4}\left\|w\_{1}-w\_{2}\right\|^{2}\_{\mathcal{H}\_{0}^{1}}\overset{\eqref{eq:Mk-positive}}{\geq}0, |  |

which is 0 if and only if w1=w2w\_{1}=w\_{2} almost everywhere. Otherwise, Ik‚Äã(w1+w22)I^{k}\left(\frac{w\_{1}+w\_{2}}{2}\right) is smaller than Ik‚Äã(w1)I^{k}\left(w\_{1}\right), which is a contradiction.

Next, we show the existence of a minimizer for IkI^{k}.
Define the bounded set ‚Ñ¨k‚äÇ‚Ñã01‚Äã(‚Ñùd)\mathcal{B}^{k}\subset\mathcal{H}\_{0}^{1}(\mathbb{R}^{d}) via

|  |  |  |
| --- | --- | --- |
|  | ‚Ñ¨k:={f‚àà‚Ñã01‚Äã(‚Ñùd)|(14‚àíh‚ÄãŒª22)‚Äã‚Äñf‚ÄñL22+h‚ÄãŒª12‚Äã‚Äñf‚Äñ‚Ñã012‚â§Rk(1)+12‚Äã‚ÄñUk‚àí1‚ÄñL22},\displaystyle\mathcal{B}^{k}:=\left\{f\in\mathcal{H}\_{0}^{1}(\mathbb{R}^{d})\Big|\ \Big(\frac{1}{4}-\frac{h\lambda\_{2}}{2}\Big)\left\|f\right\|^{2}\_{L^{2}}+\frac{h\lambda\_{1}}{2}\left\|f\right\|^{2}\_{\mathcal{H}\_{0}^{1}}\leq R^{\left(1\right)}\_{k}+\frac{1}{2}\left\|U^{k-1}\right\|^{2}\_{L^{2}}\right\}, |  |

where the constant Rk(1)R^{\left(1\right)}\_{k} is defined in ([3.5](https://arxiv.org/html/2512.25017v1#S3.E5 "Equation 3.5 ‚Ä£ Proof. ‚Ä£ 3.3. Weak formulation and uniqueness of minimizer ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")).
Consider an f‚àâ‚Ñ¨kf\notin\mathcal{B}^{k} then, using inequality ([3.3](https://arxiv.org/html/2512.25017v1#S3.Ex24 "Proof. ‚Ä£ 3.3. Weak formulation and uniqueness of minimizer ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")), we have that Ik‚Äã(f)‚â•12‚Äã‚ÄñUk‚àí1‚ÄñL22I^{k}\left(f\right)\geq\frac{1}{2}\left\|U^{k-1}\right\|^{2}\_{L^{2}}.
Using that 0‚àà‚Ñ¨k0\in\mathcal{B}^{k}, Ik‚Äã(0)=12‚Äã‚ÄñUk‚àí1‚ÄñL22I^{k}(0)=\frac{1}{2}\left\|U^{k-1}\right\|^{2}\_{L^{2}}, and that IkI^{k} is bounded from below, we conclude that

|  |  |  |
| --- | --- | --- |
|  | infw‚àà‚Ñ¨kIk‚Äã(w)=infw‚àà‚Ñã01Ik‚Äã(w)>‚àí‚àû.\displaystyle\inf\_{w\in\mathcal{B}^{k}}I^{k}\left(w\right)=\inf\_{w\in\mathcal{H}\_{0}^{1}}I^{k}\left(w\right)>-\infty. |  |

Let us now choose wm‚àà‚Ñ¨kw\_{m}\in\mathcal{B}^{k} such that Ik‚Äã(wm)‚Üíinfw‚àà‚Ñ¨kIk‚Äã(w)I^{k}\left(w\_{m}\right)\to\inf\_{w\in\mathcal{B}^{k}}I^{k}\left(w\right).
Let us also define w‚àów\_{\*} as the weak limit of wmw\_{m} in ‚Ñã01\mathcal{H}\_{0}^{1}.
Then, by [Lemma¬†3.5](https://arxiv.org/html/2512.25017v1#S3.Thmtheorem5 "Lemma 3.5. ‚Ä£ 3.3. Weak formulation and uniqueness of minimizer ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"),

|  |  |  |
| --- | --- | --- |
|  | infw‚àà‚Ñã01Ik‚Äã(w)=infw‚àà‚Ñ¨kIk‚Äã(w)=lim infm‚Üí‚àûIk‚Äã(wm)‚â•Ik‚Äã(w‚àó).\displaystyle\inf\_{w\in\mathcal{H}\_{0}^{1}}I^{k}\left(w\right)=\inf\_{w\in\mathcal{B}^{k}}I^{k}\left(w\right)=\liminf\_{m\to\infty}I^{k}\left(w\_{m}\right)\geq I^{k}\left(w\_{\*}\right). |  |

The last inequality readily implies Ik‚Äã(w‚àó)=infw‚àà‚Ñã01Ik‚Äã(w)I^{k}\left(w\_{\*}\right)=\inf\_{w\in\mathcal{H}\_{0}^{1}}I^{k}\left(w\right).
‚àé

###### Proof of [Theorem¬†3.4](https://arxiv.org/html/2512.25017v1#S3.Thmtheorem4 "Theorem 3.4. ‚Ä£ 3.3. Weak formulation and uniqueness of minimizer ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs").

Consider the homogeneous equation

|  |  |  |
| --- | --- | --- |
|  | wh+‚Ñí‚Äãw=0.\frac{w}{h}+\mathcal{L}w=0. |  |

Multiplying with ww on each side and integrating, implies 1h‚Äã‚Äñw‚ÄñL22+‚ü®‚Ñí‚Äãw,w‚ü©‚Ñã‚àí1,‚Ñã01=0\frac{1}{h}\left\|w\right\|^{2}\_{L^{2}}+\left\langle\mathcal{L}w,w\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}}=0.
Using [Assumption¬†(G√Ö)](https://arxiv.org/html/2512.25017v1#Thmassumption2 "Assumption (G√Ö). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs") and h<12‚ÄãŒª2h<\frac{1}{2\lambda\_{2}}, yields that w=0.w=0.
Therefore the homogeneous equation only has the solution w=0w=0 in ‚Ñã01‚Äã(‚Ñùd)\mathcal{H}\_{0}^{1}(\mathbb{R}^{d}).
Thus, the solution of ([3.2](https://arxiv.org/html/2512.25017v1#S3.E2 "Equation 3.2 ‚Ä£ 3.3. Weak formulation and uniqueness of minimizer ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")) is unique.

Assume that UkU^{k} minimizes IkI^{k}, and let vv be a smooth function.
Consider the function

|  |  |  |  |
| --- | --- | --- | --- |
|  | ik‚Äã(œÑ)=Ik‚Äã(Uk+œÑ‚Äãv)\displaystyle i^{k}\left(\tau\right)=I^{k}\left(U^{k}+\tau v\right) | =12‚Äã‚ÄñUk+œÑ‚Äãv‚àíUk‚àí1‚ÄñL22\displaystyle=\frac{1}{2}\left\|U^{k}+\tau v-U^{k-1}\right\|^{2}\_{L^{2}} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +h2‚Äã‚ü®‚Ñí‚Äã(Uk+œÑ‚Äãv),Uk+œÑ‚Äãv‚ü©‚Ñã‚àí1,‚Ñã01+h‚Äã‚ü®F‚Äã(Uk‚àí1),Uk+œÑ‚Äãv‚ü©L2,\displaystyle\quad+\frac{h}{2}\left\langle\mathcal{L}(U^{k}+\tau v),U^{k}+\tau v\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}}+h\left\langle F\left(U^{k-1}\right),U^{k}+\tau v\right\rangle\_{L^{2}}, |  |

for œÑ‚àà‚Ñù\tau\in\mathbb{R}.
Since UkU^{k} minimizes IkI^{k}, œÑ=0\tau=0 should minimize iki^{k}.
Hence, we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | 0=(ik)‚Ä≤‚Äã(0)=\displaystyle 0=\left(i^{k}\right)^{\prime}\left(0\right)= | ‚ü®Uk‚àíUk‚àí1,v‚ü©L2+h2‚Äã(‚ü®‚Ñí‚ÄãUk,v‚ü©‚Ñã‚àí1,‚Ñã01+‚ü®‚Ñí‚Äãv,Uk‚ü©‚Ñã‚àí1,‚Ñã01)+h‚Äã‚ü®F‚Äã(Uk‚àí1),v‚ü©L2\displaystyle\left\langle U^{k}-U^{k-1},v\right\rangle\_{L^{2}}+\frac{h}{2}\Big(\left\langle\mathcal{L}U^{k},v\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}}+\left\langle\mathcal{L}v,U^{k}\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}}\Big)+h\left\langle F\left(U^{k-1}\right),v\right\rangle\_{L^{2}} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | =\displaystyle= | ‚ü®Uk‚àíUk‚àí1,v‚ü©L2+h‚Äã‚ü®‚Ñí‚ÄãUk,v‚ü©‚Ñã‚àí1,‚Ñã01+h‚Äã‚ü®F‚Äã(Uk‚àí1),v‚ü©L2,\displaystyle\left\langle U^{k}-U^{k-1},v\right\rangle\_{L^{2}}+h\left\langle\mathcal{L}U^{k},v\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}}+h\left\langle F\left(U^{k-1}\right),v\right\rangle\_{L^{2}}, |  |

where in the last equality we used that ‚Ñí\mathcal{L} is self-adjoint.
This equality must hold for all vv, thus ([3.2](https://arxiv.org/html/2512.25017v1#S3.E2 "Equation 3.2 ‚Ä£ 3.3. Weak formulation and uniqueness of minimizer ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")) holds.
Finally, note that the second derivative of iki^{k} equals

|  |  |  |
| --- | --- | --- |
|  | (ik)‚Ä≤‚Ä≤‚Äã(œÑ)=‚Äñv‚ÄñL22+h‚Äã‚ü®‚Ñí‚Äãv,v‚ü©‚Ñã‚àí1,‚Ñã01‚â•(1‚àíh‚ÄãŒª2)‚Äã‚Äñv‚ÄñL22+h‚ÄãŒª1‚Äã‚Äñv‚Äñ‚Ñã012>0,\left(i^{k}\right)^{\prime\prime}(\tau)=\left\|v\right\|\_{L^{2}}^{2}+h\left\langle\mathcal{L}v,v\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}}\geq(1-h\lambda\_{2})\left\|v\right\|\_{L^{2}}^{2}+h\lambda\_{1}\left\|v\right\|\_{\mathcal{H}\_{0}^{1}}^{2}>0, |  |

where we used [Assumption¬†(G√Ö)](https://arxiv.org/html/2512.25017v1#Thmassumption2 "Assumption (G√Ö). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs").
Therefore, œÑ=0\tau=0 is indeed the minimizer.
‚àé

### 3.4. Neural network approximation and a version of the Universal Approximation Theorem

We use a neural network to approximate the solution of the PDE ([3.1](https://arxiv.org/html/2512.25017v1#S3.E1 "Equation 3.1 ‚Ä£ 3.2. Time stepping ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")) or, more specifically, the solution of the optimization problem minu‚àà‚Ñã01‚Å°Ik‚Äã(u)\min\_{u\in\mathcal{H}^{1}\_{0}}I^{k}(u) in ([3.3](https://arxiv.org/html/2512.25017v1#S3.E3 "Equation 3.3 ‚Ä£ 3.3. Weak formulation and uniqueness of minimizer ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")).
The fourth step is to consider a more general problem: show that any function v‚àà‚Ñã01‚Äã(‚Ñùd)v\in\mathcal{H}\_{0}^{1}(\mathbb{R}^{d}) can be approximated by a neural network.
Hornik [[13](https://arxiv.org/html/2512.25017v1#bib.bib13)] proved that a different class of neural networks, see [Remark¬†3.10](https://arxiv.org/html/2512.25017v1#S3.Thmtheorem10 "Remark 3.10. ‚Ä£ 3.4. Neural network approximation and a version of the Universal Approximation Theorem ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"), is dense in ‚Ñã01‚Äã(D)\mathcal{H}\_{0}^{1}\left(D\right), for some bounded domain D‚äÜ‚ÑùdD\subseteq\mathbb{R}^{d}.
However, in our case, the domain equals ‚Ñùd\mathbb{R}^{d}, therefore we need a tailor-made version of the Universal Approximation Theorem.

###### Definition 3.7 (Activation function).

An activation function is a function œà:‚Ñùd‚Üí‚Ñù\psi:\mathbb{R}^{d}\to\mathbb{R} such that œà‚ààCc‚àû‚Äã(‚Ñùd)\psi\in C\_{c}^{\infty}(\mathbb{R}^{d}) and ‚à´‚Ñùdœà‚Äã(x)‚Äãdx‚â†0\int\_{\mathbb{R}^{d}}\psi(x)\mathrm{d}x\neq 0.

###### Definition 3.8 (Neural network).

Let œà\psi be an activation function, then we define

|  |  |  |
| --- | --- | --- |
|  | ùíûn‚Äã(œà)={Œ∂:‚Ñùd‚Üí‚Ñù|Œ∂‚Äã(x)=‚àëi=1nŒ≤i‚Äãœà‚Äã(Œ±i‚Äãx+ci)},\mathcal{C}^{n}\left(\psi\right)=\left\{\zeta:\mathbb{R}^{d}\to\mathbb{R}\ \big|\ \zeta(x)=\sum\_{i=1}^{n}\beta\_{i}\psi(\alpha\_{i}x+c\_{i})\right\}, |  |

as the class of neural networks with a single hidden layer and nn hidden units.
The vector of weights and biases equals

|  |  |  |
| --- | --- | --- |
|  | Œ∏=(Œ≤1,‚Ä¶,Œ≤n,Œ±1,‚Ä¶,Œ±n,c1,‚Ä¶,cn)‚àà‚Ñùn√ó‚Ñùn√ó‚Ñùd√ón,\theta=\left(\beta\_{1},\dots,\beta\_{n},\alpha\_{1},\dots,\alpha\_{n},c\_{1},\dots,c\_{n}\right)\in\mathbb{R}^{n}\times\mathbb{R}^{n}\times\mathbb{R}^{d\times n}, |  |

with Œ±i‚â†0\alpha\_{i}\neq 0 for all i‚àà{1,‚Ä¶,n}i\in\{1,\dots,n\}, thus the dimension of the parameter space equals (2+d)‚Äãn\left(2+d\right)n.
Moreover, we set ùíû‚Äã(œà)=‚à™n‚â•1ùíûn‚Äã(œà)\mathcal{C}\left(\psi\right)=\cup\_{n\geq 1}\mathcal{C}^{n}\left(\psi\right).

###### Remark 3.9.

In the sequel, we consider PDEs that take values in ‚Ñùd\mathbb{R}^{d}, thus choosing an activation function œà\psi in Cc‚àû‚Äã(‚Ñùd)C\_{c}^{\infty}(\mathbb{R}^{d}) is convenient.
Then, we require that Œ±i‚â†0\alpha\_{i}\neq 0, otherwise œà‚Äã(Œ±i‚Äãx+ci)\psi\left(\alpha\_{i}x+c\_{i}\right) is a constant, which is not integrable on ‚Ñùd\mathbb{R}^{d}.

###### Remark 3.10.

Hornik [[13](https://arxiv.org/html/2512.25017v1#bib.bib13)] introduced a class of neural networks of the form Œæ‚Äã(x)=‚àëi=1nŒ≤i‚Äãœï‚Äã(ai‚ãÖx+ci)\xi\left(x\right)=\sum\_{i=1}^{n}\beta\_{i}\phi\left(a\_{i}\cdot x+c\_{i}\right) where œï:‚Ñù‚Üí‚Ñù\phi:\mathbb{R}\to\mathbb{R} and ai‚àà‚Ñùd.a\_{i}\in\mathbb{R}^{d}.
The dimension of the parameter space in this case equals again (2+d)‚Äãn\left(2+d\right)n.
However, this kind of neural network does not belong to L2‚Äã(‚Ñùd)L^{2}(\mathbb{R}^{d}).
In fact, it is not possible to prove that this class of neural networks is dense in ‚Ñã01‚Äã(‚Ñùd),\mathcal{H}\_{0}^{1}(\mathbb{R}^{d}), even if œï\phi has compact support.
Consider, for example, the case d=2,d=2, set n=1n=1, œï=ùüè[‚àí1,1],\phi=\boldsymbol{1}\_{\left[-1,1\right]}, a1=(1,‚àí1)a\_{1}=\left(1,-1\right).
Then ‚à•œï(Œ±1‚ãÖ)‚à•L2=‚à´‚Ñù2ùüè|x‚àíy|‚â§1dxdy,\left\|\phi\left(\alpha\_{1}\cdot\right)\right\|\_{L^{2}}=\int\_{\mathbb{R}^{2}}\boldsymbol{1}\_{\left|x-y\right|\leq 1}\mathrm{d}x\mathrm{d}y, which is the area of an unbounded belt, and therefore equal to +‚àû.+\infty.

###### Theorem 3.11.

Let œà\psi be an activation function, then the space of neural networks ùíû‚Äã(œà)\mathcal{C}\left(\psi\right) is dense in ‚Ñã01‚Äã(‚Ñùd)\mathcal{H}\_{0}^{1}(\mathbb{R}^{d}).

The proof of this theorem builds on the proof of the next two lemmata.

###### Lemma 3.12.

Let œà\psi be an activation function.
Let gg be a continuous function, *i.e.* g‚ààC‚Äã(‚Ñùd)g\in C(\mathbb{R}^{d}).
Suppose that, for any Œ∂‚ààùíû‚Äã(œà)\zeta\in\mathcal{C}(\psi), holds ‚à´‚ÑùdŒ∂‚Äã(x)‚Äãg‚Äã(x)‚Äãdx=0\int\_{\mathbb{R}^{d}}\zeta(x)g(x)\mathrm{d}x=0.
Then g=0g=0.

###### Remark 3.13.

Since œà‚ààCc‚àû‚Äã(‚Ñùd)\psi\in C^{\infty}\_{c}(\mathbb{R}^{d}), any function Œ∂\zeta in ùíû‚Äã(œà)\mathcal{C}\left(\psi\right) has compact support. Hence ‚à´‚ÑùdŒ∂‚Äã(x)‚Äãg‚Äã(x)‚Äãdx\int\_{\mathbb{R}^{d}}\zeta(x)g(x)\mathrm{d}x is well-defined.

###### Proof.

Let g‚ààC‚Äã(‚Ñùd)g\in C(\mathbb{R}^{d}), x‚àà‚Ñùdx\in\mathbb{R}^{d}, 0<Œµ‚â§10<{\varepsilon}\leq 1, and define

|  |  |  |
| --- | --- | --- |
|  | Œ¶Œµ‚Äã(g)‚Äã(x):=‚à´‚ÑùdŒµ‚àíd‚Äãœà‚Äã(x‚àíyŒµ)‚Äãg‚Äã(y)‚Äãdy.\Phi^{\varepsilon}\left(g\right)(x):=\int\_{\mathbb{R}^{d}}{\varepsilon}^{-d}\psi\left(\frac{x-y}{{\varepsilon}}\right)g\left(y\right)\mathrm{d}y. |  |

We would like to show that

|  |  |  |
| --- | --- | --- |
|  | limŒµ‚Üí0Œ¶Œµ‚Äã(g)‚Äã(x)=c‚Äãg‚Äã(x),\lim\_{{\varepsilon}\to 0}\Phi^{\varepsilon}\left(g\right)(x)=cg(x), |  |

where c=‚àí‚à´‚Ñùdœà‚Äã(x)‚Äãdxc=-\int\_{\mathbb{R}^{d}}\psi(x)\mathrm{d}x.
Using a change of variables twice, we have that

|  |  |  |  |
| --- | --- | --- | --- |
|  | Œ¶Œµ‚Äã(g)‚Äã(x)\displaystyle\Phi^{\varepsilon}\left(g\right)(x) | =‚à´‚ÑùdŒµ‚àíd‚Äãœà‚Äã(x‚àíyŒµ)‚Äãg‚Äã(y)‚Äãdy=z=x‚àíy‚àí‚à´‚ÑùdŒµ‚àíd‚Äãœà‚Äã(zŒµ)‚Äãg‚Äã(x‚àíz)‚Äãdz\displaystyle=\int\_{\mathbb{R}^{d}}{\varepsilon}^{-d}\psi\left(\frac{x-y}{{\varepsilon}}\right)g\left(y\right)\mathrm{d}y\stackrel{{\scriptstyle z=x-y}}{{=}}-\int\_{\mathbb{R}^{d}}{\varepsilon}^{-d}\psi\left(\frac{z}{{\varepsilon}}\right)g\left(x-z\right)\mathrm{d}z |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =m=Œµ‚àí1‚Äãz‚àí‚à´‚Ñùdœà‚Äã(m)‚Äãg‚Äã(x‚àíŒµ‚Äãm)‚Äãdm=‚àí‚à´Kœà‚Äã(m)‚Äãg‚Äã(x‚àíŒµ‚Äãm)‚Äãdm,\displaystyle\stackrel{{\scriptstyle m={\varepsilon}^{-1}z}}{{=}}-\int\_{\mathbb{R}^{d}}\psi\left(m\right)g\left(x-{\varepsilon}m\right)\mathrm{d}m=-\int\_{K}\psi\left(m\right)g\left(x-{\varepsilon}m\right)\mathrm{d}m, |  |

where KK denotes the (compact) support of œà\psi.
Notice that, since zz is a vector, we have that m=zŒµm=\frac{z}{{\varepsilon}} yields d‚Äãm=Œµ‚àíd‚Äãd‚Äãz\mathrm{d}m={\varepsilon}^{-d}\mathrm{d}z.
Then, using the dominated convergence theorem, we get that

|  |  |  |
| --- | --- | --- |
|  | limŒµ‚Üí0Œ¶Œµ‚Äã(g)‚Äã(x)=limŒµ‚Üí0{‚àí‚à´Kœà‚Äã(m)‚Äãg‚Äã(x‚àíŒµ‚Äãm)‚Äãdm}=c‚Äãg‚Äã(x).\displaystyle\lim\_{{\varepsilon}\to 0}\Phi^{\varepsilon}\left(g\right)(x)=\lim\_{{\varepsilon}\to 0}\Big\{-\int\_{K}\psi\left(m\right)g\left(x-{\varepsilon}m\right)\mathrm{d}m\Big\}=cg(x). |  |

Now, consider any Œ∂‚ààùíû‚Äã(œà)\zeta\in\mathcal{C}\left(\psi\right) such that ‚à´‚ÑùdŒ∂‚Äã(y)‚Äãg‚Äã(y)‚Äãdy=0\int\_{\mathbb{R}^{d}}\zeta(y)g(y)\mathrm{d}y=0; then, for any x‚àà‚Ñùdx\in\mathbb{R}^{d}, setting n=1n=1, Œ≤=Œµ‚àíd\beta={\varepsilon}^{-d}, Œ±=Œµ‚àí1\alpha={\varepsilon}^{-1} and c=xŒµc=\frac{x}{{\varepsilon}} in the definition of ùíû‚Äã(œà)\mathcal{C}\left(\psi\right), we get that

|  |  |  |
| --- | --- | --- |
|  | ‚à´‚ÑùdŒµ‚àíd‚Äãœà‚Äã(x‚àíyŒµ)‚Äãg‚Äã(y)‚Äãdy=0.\int\_{\mathbb{R}^{d}}{\varepsilon}^{-d}\psi\left(\frac{x-y}{{\varepsilon}}\right)g\left(y\right)\mathrm{d}y=0. |  |

We conclude the proof by sending Œµ‚Üí0{\varepsilon}\to 0 and using that c‚â†0c\neq 0, by definition of an activation function.
‚àé

###### Lemma 3.14.

Let ww be a function on C‚àû‚Äã(‚Ñùd)C^{\infty}(\mathbb{R}^{d}) with support on the unit sphere, where

|  |  |  |
| --- | --- | --- |
|  | w(x)={c‚Äãexp‚Å°(‚àí11‚àí|x|2),if¬†|x|<1,0,if¬†|x|‚â•1,w(x)=\left\{\begin{aligned} c\exp\left(\frac{-1}{1-\left|x\right|^{2}}\right),\quad&\text{if $\left|x\right|<1$},\\ 0,\quad&\text{if $\left|x\right|\geq 1$,}\end{aligned}\right. |  |

where cc is a constant such that the integral of ww equals 11.
Let f‚ààL*loc*1‚Äã(‚Ñùd)f\in L^{1}\_{\emph{loc}}(\mathbb{R}^{d}), and introduce

|  |  |  |
| --- | --- | --- |
|  | JŒµ‚Äãf‚Äã(x)=wŒµ‚àóf‚Äã(x)=‚à´‚ÑùdwŒµ‚Äã(y)‚Äãf‚Äã(x‚àíy)‚Äãdy,J^{\varepsilon}f(x)=w\_{\varepsilon}\*f(x)=\int\_{\mathbb{R}^{d}}w\_{\varepsilon}\left(y\right)f\left(x-y\right)\mathrm{d}y, |  |

with wŒµ=Œµ‚àíd‚Äãw‚Äã(xŒµ)w\_{\varepsilon}={\varepsilon}^{-d}w\left(\frac{x}{{\varepsilon}}\right).
Then, for any œÜ‚ààCc‚àû\varphi\in C^{\infty}\_{c} and f‚ààL*loc*1‚Äã(‚Ñùd)f\in L^{1}\_{\emph{loc}}(\mathbb{R}^{d}), we have that

|  |  |  |
| --- | --- | --- |
|  | limŒµ‚Üí0‚ü®œÜ,JŒµ‚Äãf‚ü©L2=‚ü®œÜ,f‚ü©L2.\displaystyle\lim\_{{\varepsilon}\to 0}\left\langle\varphi,J^{\varepsilon}f\right\rangle\_{L^{2}}=\left\langle\varphi,f\right\rangle\_{L^{2}}. |  |

###### Remark 3.15.

The convolution of wŒµw\_{\varepsilon} with ff is convenient, because then JŒµ‚ÄãfJ^{\varepsilon}f is infinitely differentiable.

###### Proof.

Let us first rewrite ‚ü®œÜ,JŒµ‚Äãf‚ü©L2\left\langle\varphi,J^{\varepsilon}f\right\rangle\_{L^{2}} as follows

|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚ü®œÜ,JŒµ‚Äãf‚ü©L2\displaystyle\left\langle\varphi,J^{\varepsilon}f\right\rangle\_{L^{2}} | =‚à´‚Ñùd‚à´‚ÑùdœÜ‚Äã(x)‚ÄãwŒµ‚Äã(y)‚Äãf‚Äã(x‚àíy)‚Äãdy‚Äãdx=Œµ‚àíd‚Äã‚à´‚Ñùd‚à´‚ÑùdœÜ‚Äã(x)‚Äãw‚Äã(yŒµ)‚Äãf‚Äã(x‚àíy)‚Äãdy‚Äãdx\displaystyle=\int\_{\mathbb{R}^{d}}\int\_{\mathbb{R}^{d}}\varphi(x)w\_{\varepsilon}\left(y\right)f\left(x-y\right)\mathrm{d}y\mathrm{d}x={\varepsilon}^{-d}\int\_{\mathbb{R}^{d}}\int\_{\mathbb{R}^{d}}\varphi(x)w\left(\frac{y}{{\varepsilon}}\right)f\left(x-y\right)\mathrm{d}y\mathrm{d}x |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =z=y/Œµ‚à´‚Ñùd‚à´‚ÑùdœÜ‚Äã(x)‚Äãw‚Äã(z)‚Äãf‚Äã(x‚àíŒµ‚Äãz)‚Äãdz‚Äãdx=‚à´K‚à´KœÜ‚Äã(x)‚Äãw‚Äã(z)‚Äãf‚Äã(x‚àíŒµ‚Äãz)‚Äãdz‚Äãdx\displaystyle\stackrel{{\scriptstyle z=y/{\varepsilon}}}{{=}}\int\_{\mathbb{R}^{d}}\int\_{\mathbb{R}^{d}}\varphi(x)w\left(z\right)f\left(x-{\varepsilon}z\right)\mathrm{d}z\mathrm{d}x=\int\_{K}\int\_{K}\varphi(x)w\left(z\right)f\left(x-{\varepsilon}z\right)\mathrm{d}z\mathrm{d}x |  |

where KK is a compact set that contains the support of ww and œÜ\varphi.
Hence, by the dominated convergence theorem and Lusin‚Äôs theorem, letting Œµ‚Üí0{\varepsilon}\to 0 and using that the integral of ww equals 1, we have

|  |  |  |
| --- | --- | --- |
|  | limŒµ‚Üí0‚ü®œÜ,JŒµ‚Äãf‚ü©L2=limŒµ‚Üí0‚à´K‚à´KœÜ‚Äã(x)‚Äãw‚Äã(z)‚Äãf‚Äã(x‚àíŒµ‚Äãz)‚Äãdz‚Äãdx=‚ü®œÜ,f‚ü©L2.‚àé\lim\_{{\varepsilon}\to 0}\left\langle\varphi,J^{\varepsilon}f\right\rangle\_{L^{2}}=\lim\_{{\varepsilon}\to 0}\int\_{K}\int\_{K}\varphi(x)w\left(z\right)f\left(x-{\varepsilon}z\right)\mathrm{d}z\mathrm{d}x=\left\langle\varphi,f\right\rangle\_{L^{2}}.\qed |  |

###### Proof of [Theorem¬†3.11](https://arxiv.org/html/2512.25017v1#S3.Thmtheorem11 "Theorem 3.11. ‚Ä£ 3.4. Neural network approximation and a version of the Universal Approximation Theorem ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs").

Observe that ùíû‚Äã(œà)‚äÇ‚Ñã01‚Äã(‚Ñùd)\mathcal{C}\left(\psi\right)\subset\mathcal{H}\_{0}^{1}(\mathbb{R}^{d}), since œà‚ààCc‚àû‚Äã(‚Ñùd)\psi\in C^{\infty}\_{c}(\mathbb{R}^{d}). Assume that ùíû‚Äã(œà)\mathcal{C}\left(\psi\right) is not dense in ‚Ñã01‚Äã(‚Ñùd)\mathcal{H}\_{0}^{1}(\mathbb{R}^{d}) then, as a corollary of the Hahn‚ÄìBanach extension theorem, see e.g. van Neerven [[30](https://arxiv.org/html/2512.25017v1#bib.bib30), Corollary 4.12], there exists a non-zero continuous linear functional GG on ‚Ñã01‚Äã(‚Ñùd)\mathcal{H}\_{0}^{1}(\mathbb{R}^{d}) such that for any Œ∂‚ààùíû‚Äã(œà)\zeta\in\mathcal{C}\left(\psi\right),

|  |  |  |
| --- | --- | --- |
|  | G‚Äã(Œ∂)=0.\displaystyle G\left(\zeta\right)=0. |  |

Using the Riesz representation theorem, there exists a g‚â†0g\neq 0 in ‚Ñã01‚Äã(‚Ñùd)\mathcal{H}\_{0}^{1}(\mathbb{R}^{d}), such that for any f‚àà‚Ñã01‚Äã(‚Ñùd)f\in\mathcal{H}\_{0}^{1}(\mathbb{R}^{d}),

|  |  |  |
| --- | --- | --- |
|  | ‚ü®f,g‚ü©‚Ñã01‚Äã(‚Ñùd)=G‚Äã(f).\left\langle f,g\right\rangle\_{\mathcal{H}\_{0}^{1}(\mathbb{R}^{d})}=G\left(f\right). |  |

Therefore ‚ü®Œ∂,g‚ü©L2+‚ü®‚àáŒ∂,‚àág‚ü©L2=0\left\langle\zeta,g\right\rangle\_{L^{2}}+\left\langle\nabla\zeta,\nabla g\right\rangle\_{L^{2}}=0.
Let us denote g1Œµ=JŒµ‚Äãgg^{\varepsilon}\_{1}=J^{\varepsilon}g and g2Œµ=JŒµ‚Äã‚àágg^{\varepsilon}\_{2}=J^{\varepsilon}\nabla g, for convenience.
Consider the inner product of these functions with Œ∂\zeta, then we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚ü®Œ∂,g1Œµ‚ü©L2\displaystyle\left\langle\zeta,g^{\varepsilon}\_{1}\right\rangle\_{L^{2}} | +‚ü®‚àáŒ∂,g2Œµ‚ü©L2\displaystyle+\left\langle\nabla\zeta,g^{\varepsilon}\_{2}\right\rangle\_{L^{2}} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =‚à´‚Ñùd‚à´‚ÑùdŒ∂‚Äã(x)‚ÄãwŒµ‚Äã(y)‚Äãg‚Äã(x‚àíy)‚Äãdy‚Äãdx+‚à´‚Ñùd‚à´‚Ñùd‚àáŒ∂‚Äã(x)‚ãÖ[wŒµ‚Äã(y)‚Äã‚àág‚Äã(x‚àíy)]‚Äãdy‚Äãdx\displaystyle=\int\_{\mathbb{R}^{d}}\int\_{\mathbb{R}^{d}}\zeta(x)w\_{\varepsilon}\left(y\right)g\left(x-y\right)\mathrm{d}y\mathrm{d}x+\int\_{\mathbb{R}^{d}}\int\_{\mathbb{R}^{d}}\nabla\zeta(x)\cdot\left[w\_{\varepsilon}\left(y\right)\nabla g\left(x-y\right)\right]\mathrm{d}y\mathrm{d}x |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =‚à´‚Ñùd(‚à´‚ÑùdŒ∂‚Äã(x)‚Äãg‚Äã(x‚àíy)‚Äãdx)‚ÄãwŒµ‚Äã(y)‚Äãdy+‚à´‚Ñùd(‚à´‚Ñùd‚àáŒ∂‚Äã(x)‚ãÖ‚àág‚Äã(x‚àíy)‚Äãdx)‚ÄãwŒµ‚Äã(y)‚Äãdy\displaystyle=\int\_{\mathbb{R}^{d}}\left(\int\_{\mathbb{R}^{d}}\zeta(x)g\left(x-y\right)\mathrm{d}x\right)w\_{\varepsilon}\left(y\right)\mathrm{d}y+\int\_{\mathbb{R}^{d}}\left(\int\_{\mathbb{R}^{d}}\nabla\zeta(x)\cdot\nabla g\left(x-y\right)\mathrm{d}x\right)w\_{\varepsilon}\left(y\right)\mathrm{d}y |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =x=z+y‚à´‚Ñùd(‚à´‚ÑùdŒ∂‚Äã(z+y)‚Äãg‚Äã(z)‚Äãùëëz)‚ÄãwŒµ‚Äã(y)‚Äãdy+‚à´‚Ñùd(‚à´‚Ñùd‚àáŒ∂‚Äã(z+y)‚ãÖ‚àág‚Äã(z)‚Äãùëëz)‚ÄãwŒµ‚Äã(y)‚Äãdy\displaystyle\stackrel{{\scriptstyle x=z+y}}{{=}}\int\_{\mathbb{R}^{d}}\left(\int\_{\mathbb{R}^{d}}\zeta\left(z+y\right)g\left(z\right)dz\right)w\_{\varepsilon}\left(y\right)\mathrm{d}y+\int\_{\mathbb{R}^{d}}\left(\int\_{\mathbb{R}^{d}}\nabla\zeta\left(z+y\right)\cdot\nabla g\left(z\right)dz\right)w\_{\varepsilon}\left(y\right)\mathrm{d}y |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =‚à´‚Ñùd{‚ü®Œ∂(‚ãÖ+y),g‚ü©L2+‚ü®‚àáŒ∂(‚ãÖ+y),‚àág‚ü©L2}wŒµ(y)dy=0.\displaystyle=\int\_{\mathbb{R}^{d}}\Big\{\left\langle\zeta\left(\cdot+y\right),g\right\rangle\_{L^{2}}+\left\langle\nabla\zeta\left(\cdot+y\right),\nabla g\right\rangle\_{L^{2}}\Big\}\,w\_{\varepsilon}\left(y\right)\mathrm{d}y=0. |  |

Since Œ∂‚ààùíû‚Äã(œà)\zeta\in\mathcal{C}\left(\psi\right), it has compact support and we can apply Fubini‚Äôs theorem in the second step, while we can also use that Œ∂(‚ãÖ+y)‚ààùíû(œà)\zeta\left(\cdot+y\right)\in\mathcal{C}\left(\psi\right) in the last step.
Hence ‚ü®Œ∂,g1Œµ‚ü©L2+‚ü®‚àáŒ∂,g2Œµ‚ü©L2=0\left\langle\zeta,g^{\varepsilon}\_{1}\right\rangle\_{L^{2}}+\left\langle\nabla\zeta,g^{\varepsilon}\_{2}\right\rangle\_{L^{2}}=0.
Then, using integration by parts,

|  |  |  |
| --- | --- | --- |
|  | ‚ü®Œ∂,g1Œµ‚àí‚àá‚ãÖ(g2Œµ)‚ü©L2=0.\displaystyle\left\langle\zeta,g^{\varepsilon}\_{1}-\nabla\cdot\left(g^{\varepsilon}\_{2}\right)\right\rangle\_{L^{2}}=0. |  |

Since g1Œµ‚àí‚àá‚ãÖ(g2Œµ)g^{\varepsilon}\_{1}-\nabla\cdot\left(g^{\varepsilon}\_{2}\right) is continuous, see e.g. van Neerven [[30](https://arxiv.org/html/2512.25017v1#bib.bib30), Proposition 11.1], by [Lemma¬†3.12](https://arxiv.org/html/2512.25017v1#S3.Thmtheorem12 "Lemma 3.12. ‚Ä£ 3.4. Neural network approximation and a version of the Universal Approximation Theorem ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"), we conclude that g1Œµ‚àí‚àá‚ãÖ(g2Œµ)=0g^{\varepsilon}\_{1}-\nabla\cdot\left(g^{\varepsilon}\_{2}\right)=0.
Then, for any f‚ààCc‚àû‚Äã(‚Ñùd)f\in C^{\infty}\_{c}(\mathbb{R}^{d}),

|  |  |  |
| --- | --- | --- |
|  | ‚ü®f,g1Œµ‚ü©L2+‚ü®‚àáf,g2Œµ‚ü©L2=‚ü®f,g1Œµ‚àí‚àá‚ãÖ(g2Œµ)‚ü©L2=0.\left\langle f,g^{\varepsilon}\_{1}\right\rangle\_{L^{2}}+\left\langle\nabla f,g^{\varepsilon}\_{2}\right\rangle\_{L^{2}}=\left\langle f,g^{\varepsilon}\_{1}-\nabla\cdot\left(g^{\varepsilon}\_{2}\right)\right\rangle\_{L^{2}}=0. |  |

Using [Lemma¬†3.14](https://arxiv.org/html/2512.25017v1#S3.Thmtheorem14 "Lemma 3.14. ‚Ä£ 3.4. Neural network approximation and a version of the Universal Approximation Theorem ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"), for any f‚ààCc‚àû‚Äã(‚Ñùd)f\in C^{\infty}\_{c}(\mathbb{R}^{d}), we get that

|  |  |  |
| --- | --- | --- |
|  | G‚Äã(f)=‚ü®f,g‚ü©L2+‚ü®‚àáf,‚àág‚ü©L2=limŒµ‚Üí0‚ü®f,g1Œµ‚ü©L2+‚ü®‚àáf,g2Œµ‚ü©L2=0.\displaystyle G\left(f\right)=\left\langle f,g\right\rangle\_{L^{2}}+\left\langle\nabla f,\nabla g\right\rangle\_{L^{2}}=\lim\_{{\varepsilon}\to 0}\left\langle f,g^{\varepsilon}\_{1}\right\rangle\_{L^{2}}+\left\langle\nabla f,g^{\varepsilon}\_{2}\right\rangle\_{L^{2}}=0. |  |

Since Cc‚àû‚Äã(‚Ñùd)C^{\infty}\_{c}(\mathbb{R}^{d}) is dense in ‚Ñã01\mathcal{H}\_{0}^{1} with norm ‚à•‚ãÖ‚à•‚Ñã01\left\|\cdot\right\|\_{\mathcal{H}\_{0}^{1}}, GG is a zero functional on ‚Ñã01\mathcal{H}\_{0}^{1}, which is a contradiction.
‚àé

### 3.5. Convergence of the minimizer

The final step, is to show that the minimizer approximated by neural networks converges to the solution of the PDE, which yields that the approximation error of the method converges to zero.
[Theorem¬†3.4](https://arxiv.org/html/2512.25017v1#S3.Thmtheorem4 "Theorem 3.4. ‚Ä£ 3.3. Weak formulation and uniqueness of minimizer ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs") yields that the minimizer of the functional IkI^{k} in ([3.3](https://arxiv.org/html/2512.25017v1#S3.E3 "Equation 3.3 ‚Ä£ 3.3. Weak formulation and uniqueness of minimizer ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")) equals the unique solution of discretized equation ([3.2](https://arxiv.org/html/2512.25017v1#S3.E2 "Equation 3.2 ‚Ä£ 3.3. Weak formulation and uniqueness of minimizer ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")).
Here, we show that this minimizer can be approximated by a neural network as defined in [Definition¬†3.8](https://arxiv.org/html/2512.25017v1#S3.Thmtheorem8 "Definition 3.8 (Neural network). ‚Ä£ 3.4. Neural network approximation and a version of the Universal Approximation Theorem ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs").
The final conclusion, *i.e.* the convergence to the true solution of PDE ([2.1](https://arxiv.org/html/2512.25017v1#S2.E1 "Equation 2.1 ‚Ä£ 2. Deep gradient flow methods for PDEs ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"))‚Äì([2.2](https://arxiv.org/html/2512.25017v1#S2.E2 "Equation 2.2 ‚Ä£ 2. Deep gradient flow methods for PDEs ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")), follows by an application of [Theorem¬†3.3](https://arxiv.org/html/2512.25017v1#S3.Thmtheorem3 "Theorem 3.3. ‚Ä£ 3.2. Time stepping ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"), which shows that the time-stepping scheme converges to the PDE.

###### Theorem 3.16.

Let (wm)m‚àà‚Ñï(w\_{m})\_{m\in\mathbb{N}} be a sequence in ‚Ñã01‚Äã(‚Ñùd)\mathcal{H}\_{0}^{1}(\mathbb{R}^{d}) and w‚àów\_{\*} be the minimizer of IkI^{k}. Then

|  |  |  |
| --- | --- | --- |
|  | limm‚Üí‚àû‚Äñwm‚àíw‚àó‚Äñ‚Ñã01=0¬†if and only if¬†limm‚Üí‚àûIk‚Äã(wm)=Ik‚Äã(w‚àó).\lim\_{m\to\infty}\left\|w\_{m}-w\_{\*}\right\|\_{\mathcal{H}^{1}\_{0}}=0\quad\text{ if and only if }\quad\lim\_{m\to\infty}I^{k}\left(w\_{m}\right)=I^{k}\left(w\_{\*}\right). |  |

###### Remark 3.17.

Therefore, we can select the approximation sequence (wm)(w\_{m}) from the space of neural networks ùíû‚Äã(œà)\mathcal{C}\left(\psi\right).
Using that ùíû‚Äã(œà)\mathcal{C}\left(\psi\right) is dense in ‚Ñã01‚Äã(‚Ñùd)\mathcal{H}\_{0}^{1}(\mathbb{R}^{d}), an approximation sequence always exists.

###### Remark 3.18.

Let us point out that for an arbitrary u‚àà‚Ñã01‚Äã(‚Ñùd)u\in\mathcal{H}\_{0}^{1}(\mathbb{R}^{d}), |Ik‚Äã(um)‚àíIk‚Äã(u)|‚Üí0\left|I^{k}\left(u\_{m}\right)-I^{k}(u)\right|\to 0, does not imply ‚Äñum‚àíu‚ÄñL2‚Üí0\left\|u\_{m}-u\right\|\_{L^{2}}\to 0.
Consider, for example, F=0F=0, then IkI^{k} is quadratic and we can always choose um=‚àíuu\_{m}=-u since Ik‚Äã(u)=Ik‚Äã(‚àíu)I^{k}(u)=I^{k}\left(-u\right).

###### Proposition 3.19 (Continuity).

Assume that the operators ‚Ñí\mathcal{L} and FF satisfy [Assumptions¬†(CON)](https://arxiv.org/html/2512.25017v1#Thmassumption1 "Assumption (CON). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs") and¬†[(SA)](https://arxiv.org/html/2512.25017v1#Thmassumption3 "Assumption (SA). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"), then the functional IkI^{k} is continuous, *i.e.* for any f,u‚àà‚Ñã01‚Äã(‚Ñùd)f,u\in\mathcal{H}\_{0}^{1}(\mathbb{R}^{d}), holds

|  |  |  |
| --- | --- | --- |
|  | |Ik‚Äã(f)‚àíIk‚Äã(u)|‚â§(1+h‚ÄãM)‚Äã‚Äñf‚àíu‚Äñ‚Ñã01‚Äã(‚Äñf+u‚Äñ‚Ñã01+‚ÄñUk‚àí1‚Äñ‚Ñã01).\displaystyle\big|I^{k}\left(f\right)-I^{k}(u)\big|\leq\left(1+hM\right)\left\|f-u\right\|\_{\mathcal{H}\_{0}^{1}}\left(\left\|f+u\right\|\_{\mathcal{H}\_{0}^{1}}+\left\|U^{k-1}\right\|\_{\mathcal{H}\_{0}^{1}}\right). |  |

###### Proof.

Using the definition of the energy functional in ([3.3](https://arxiv.org/html/2512.25017v1#S3.E3 "Equation 3.3 ‚Ä£ 3.3. Weak formulation and uniqueness of minimizer ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")) and that ‚Ñí\mathcal{L} is linear and self-adjoint, we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | |Ik‚Äã(f)‚àíIk‚Äã(u)|\displaystyle\big|I^{k}\left(f\right)-I^{k}(u)\big| | =|12‚Äñ‚Äãf‚àíUk‚àí1‚à•L22+h2‚Äã‚ü®‚Ñí‚Äãf,f‚ü©‚Ñã‚àí1,‚Ñã01+h‚Äã‚ü®F‚Äã(Uk‚àí1),f‚ü©L2\displaystyle=\left|\frac{1}{2}\left\|f-U^{k-1}\right\|^{2}\_{L^{2}}+\frac{h}{2}\left\langle\mathcal{L}f,f\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}}+h\left\langle F\left(U^{k-1}\right),f\right\rangle\_{L^{2}}\right. |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚àí12‚à•u‚àíUk‚àí1‚à•L22‚àíh2‚ü®‚Ñíu,u‚ü©‚Ñã‚àí1,‚Ñã01‚àíh‚ü®F(Uk‚àí1),u‚ü©L2|\displaystyle\qquad\left.-\frac{1}{2}\left\|u-U^{k-1}\right\|^{2}\_{L^{2}}-\frac{h}{2}\left\langle\mathcal{L}u,u\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}}-h\left\langle F\left(U^{k-1}\right),u\right\rangle\_{L^{2}}\right| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§12‚Äã|‚Äñf‚ÄñL22‚àí‚Äñu‚ÄñL22‚àí2‚Äã‚ü®f‚àíu,Uk‚àí1‚ü©L2‚èü=‚ü®f‚àíu,f+u‚àí2‚ÄãUk+1‚ü©|\displaystyle\leq\frac{1}{2}\Big|\underbrace{\left\|f\right\|^{2}\_{L^{2}}-\left\|u\right\|^{2}\_{L^{2}}-2\left\langle f-u,U^{k-1}\right\rangle\_{L^{2}}}\_{=\left\langle f-u,f+u-2U^{k+1}\right\rangle}\Big| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +h2‚Äã|‚ü®‚Ñí‚Äã(f‚àíu),f+u‚ü©‚Ñã‚àí1,‚Ñã01|+h‚Äã|‚ü®F‚Äã(Uk‚àí1),f‚àíu‚ü©L2|\displaystyle\quad+\frac{h}{2}\left|\left\langle\mathcal{L}(f-u),f+u\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}}\right|+h\left|\left\langle F\left(U^{k-1}\right),f-u\right\rangle\_{L^{2}}\right| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§CS12‚Äã‚Äñf‚àíu‚ÄñL2‚Äã‚Äñf+u‚àí2‚ÄãUk‚àí1‚ÄñL2+h2‚Äã|‚ü®‚Ñí‚Äã(f‚àíu),f+u‚ü©‚Ñã‚àí1,‚Ñã01|\displaystyle\stackrel{{\scriptstyle\text{CS}}}{{\leq}}\frac{1}{2}\left\|f-u\right\|\_{L^{2}}\left\|f+u-2U^{k-1}\right\|\_{L^{2}}+\frac{h}{2}\left|\left\langle\mathcal{L}(f-u),f+u\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}}\right| |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | +h‚Äã|‚ü®F‚Äã(Uk‚àí1),f‚àíu‚ü©L2|.\displaystyle\quad+h\left|\left\langle F\left(U^{k-1}\right),f-u\right\rangle\_{L^{2}}\right|. |  | (3.8) |

Moreover, using [Assumption¬†(CON)](https://arxiv.org/html/2512.25017v1#Thmassumption1 "Assumption (CON). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs") and the Cauchy‚ÄìSchwarz inequality again, we get

|  |  |  |  |
| --- | --- | --- | --- |
|  | h2‚Äã|‚ü®‚Ñí‚Äã(f‚àíu),f+u‚ü©‚Ñã‚àí1,‚Ñã01|\displaystyle\frac{h}{2}\left|\left\langle\mathcal{L}(f-u),f+u\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}}\right| | +h‚Äã|‚ü®F‚Äã(Uk‚àí1),f‚àíu‚ü©L2|\displaystyle+h\left|\left\langle F\left(U^{k-1}\right),f-u\right\rangle\_{L^{2}}\right| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§h‚ÄãM2‚Äã‚Äñf‚àíu‚Äñ‚Ñã01‚Äã‚Äñf+u‚Äñ‚Ñã01+h‚ÄãM‚Äã‚ÄñUk‚àí1‚Äñ‚Ñã01‚Äã‚Äñf‚àíu‚ÄñL2\displaystyle\leq\frac{hM}{2}\left\|f-u\right\|\_{\mathcal{H}\_{0}^{1}}\left\|f+u\right\|\_{\mathcal{H}\_{0}^{1}}+hM\left\|U^{k-1}\right\|\_{\mathcal{H}\_{0}^{1}}\left\|f-u\right\|\_{L^{2}} |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | ‚â§h‚ÄãM‚Äã(‚Äñf+u‚Äñ‚Ñã01+‚ÄñUk‚àí1‚Äñ‚Ñã01)‚Äã‚Äñf‚àíu‚Äñ‚Ñã01,\displaystyle\leq hM\left(\left\|f+u\right\|\_{\mathcal{H}\_{0}^{1}}+\left\|U^{k-1}\right\|\_{\mathcal{H}\_{0}^{1}}\right)\left\|f-u\right\|\_{\mathcal{H}\_{0}^{1}}, |  | (3.9) |

while from the triangle inequality, we have that

|  |  |  |  |
| --- | --- | --- | --- |
|  | 12‚Äã‚Äñf‚àíu‚ÄñL2‚Äã‚Äñf+u‚àí2‚ÄãUk‚àí1‚ÄñL2‚â§‚Äñf‚àíu‚Äñ‚Ñã01‚Äã(‚Äñf+u‚Äñ‚Ñã01+‚ÄñUk‚àí1‚Äñ‚Ñã01).\displaystyle\frac{1}{2}\left\|f-u\right\|\_{L^{2}}\left\|f+u-2U^{k-1}\right\|\_{L^{2}}\leq\left\|f-u\right\|\_{\mathcal{H}\_{0}^{1}}\left(\left\|f+u\right\|\_{\mathcal{H}\_{0}^{1}}+\left\|U^{k-1}\right\|\_{\mathcal{H}\_{0}^{1}}\right). |  | (3.10) |

Replacing ([3.5](https://arxiv.org/html/2512.25017v1#S3.Ex70 "Proof. ‚Ä£ 3.5. Convergence of the minimizer ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")) and ([3.10](https://arxiv.org/html/2512.25017v1#S3.E10 "Equation 3.10 ‚Ä£ Proof. ‚Ä£ 3.5. Convergence of the minimizer ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")) into ([3.5](https://arxiv.org/html/2512.25017v1#S3.Ex65 "Proof. ‚Ä£ 3.5. Convergence of the minimizer ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")) completes the proof.
‚àé

###### Proof of [Theorem¬†3.16](https://arxiv.org/html/2512.25017v1#S3.Thmtheorem16 "Theorem 3.16. ‚Ä£ 3.5. Convergence of the minimizer ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs").

Assume that ‚Äñwm‚àíw‚àó‚Äñ‚Ñã01‚Üí0\left\|w\_{m}-w\_{\*}\right\|\_{\mathcal{H}\_{0}^{1}}\to 0.
Then, the sequence ‚Äñwm‚àíw‚àó‚Äñ‚Ñã01\left\|w\_{m}-w\_{\*}\right\|\_{\mathcal{H}\_{0}^{1}} is bounded by some constant C>0C>0.
Using [Proposition¬†3.19](https://arxiv.org/html/2512.25017v1#S3.Thmtheorem19 "Proposition 3.19 (Continuity). ‚Ä£ 3.5. Convergence of the minimizer ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"), we get that

|  |  |  |
| --- | --- | --- |
|  | |Ik‚Äã(wm)‚àíIk‚Äã(w‚àó)|‚â§‚Äñwm‚àíw‚àó‚Äñ‚Ñã01‚ÄãC‚Äã(1+‚ÄñUk‚àí1‚Äñ‚Ñã01)‚Üí0.\displaystyle\left|I^{k}\left(w\_{m}\right)-I^{k}\left(w\_{\*}\right)\right|\leq\left\|w\_{m}-w\_{\*}\right\|\_{\mathcal{H}\_{0}^{1}}C\left(1+\left\|U^{k-1}\right\|\_{\mathcal{H}\_{0}^{1}}\right)\to 0. |  |

Thus Ik‚Äã(wm)‚ÜíIk‚Äã(w‚àó)I^{k}\left(w\_{m}\right)\to I^{k}\left(w\_{\*}\right).

Next, we prove that Ik‚Äã(wm)‚ÜíIk‚Äã(w‚àó)I^{k}\left(w\_{m}\right)\to I^{k}\left(w\_{\*}\right) implies that wm‚Üíw‚àów\_{m}\to w\_{\*} in ‚Ñã01\mathcal{H}\_{0}^{1}.
Let us first notice that wm‚áÄw‚àów\_{m}\rightharpoonup w\_{\*}.
Otherwise, there exists a subsequence (wmi)(w\_{m\_{i}}), an Œµ>0{\varepsilon}>0 and a nonzero functional ff such that |f‚Äã[wmi]‚àíf‚Äã[w‚àó]|‚â•Œµ\left|f\left[w\_{m\_{i}}\right]-f\left[w\_{\*}\right]\right|\geq{\varepsilon}.
Since (wmi)(w\_{m\_{i}}) is bounded in ‚Ñã01\mathcal{H}\_{0}^{1} (otherwise Ik‚Äã(wmi)I^{k}\left(w\_{m\_{i}}\right) is unbounded), it is pre-weakly compact (which means it has a weakly convergent subsequence, see e.g. van Neerven [[30](https://arxiv.org/html/2512.25017v1#bib.bib30), Corollary 4.56]).
Let us denote one of its weak limits by w‚àó‚àów\_{\*\*}.
Using [Lemma¬†3.5](https://arxiv.org/html/2512.25017v1#S3.Thmtheorem5 "Lemma 3.5. ‚Ä£ 3.3. Weak formulation and uniqueness of minimizer ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"), Ik‚Äã(w‚àó)=limi‚Üí‚àûIk‚Äã(wmi)‚â•Ik‚Äã(w‚àó‚àó)I^{k}\left(w\_{\*}\right)=\lim\_{i\to\infty}I^{k}\left(w\_{m\_{i}}\right)\geq I^{k}\left(w\_{\*\*}\right).
This inequality implies w‚àó=w‚àó‚àów\_{\*}=w\_{\*\*} by the uniqueness of the minimizer.
Hence wm‚áÄw‚àów\_{m}\rightharpoonup w\_{\*}, a contradiction with |f‚Äã[wmi]‚àíf‚Äã[w‚àó]|‚â•Œµ\left|f\left[w\_{m\_{i}}\right]-f\left[w\_{\*}\right]\right|\geq{\varepsilon}.
Therefore, wm‚áÄw‚àów\_{m}\rightharpoonup w\_{\*}.

Now, since Ik‚Äã(wm)‚ÜíIk‚Äã(w‚àó)I^{k}\left(w\_{m}\right)\to I^{k}\left(w\_{\*}\right), ùí¢n‚Äã(wm)‚Üíùí¢n‚Äã(w‚àó)\mathcal{G}^{n}(w\_{m})\to\mathcal{G}^{n}(w\_{\*}), and ‚Ñ≥k‚Äã(wm)‚Üí‚Ñ≥k‚Äã(w‚àó)\mathcal{M}^{k}\left(w\_{m}\right)\to\mathcal{M}^{k}\left(w\_{\*}\right), we have that

|  |  |  |
| --- | --- | --- |
|  | 12‚Äã‚Äñwm‚ÄñL22+h2‚Äã‚ü®‚Ñí‚Äãwm,wm‚ü©‚Ñã‚àí1,‚Ñã01‚Üí12‚Äã‚Äñw‚àó‚ÄñL22+h2‚Äã‚ü®‚Ñí‚Äãw‚àó,w‚àó‚ü©‚Ñã‚àí1,‚Ñã01.\displaystyle\frac{1}{2}\left\|w\_{m}\right\|^{2}\_{L^{2}}+\frac{h}{2}\left\langle\mathcal{L}w\_{m},w\_{m}\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}}\to\frac{1}{2}\left\|w\_{\*}\right\|^{2}\_{L^{2}}+\frac{h}{2}\left\langle\mathcal{L}w\_{\*},w\_{\*}\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}}. |  |

This convergence implies

|  |  |  |  |
| --- | --- | --- | --- |
|  | (12‚àíh‚ÄãŒª22)‚Äã‚Äñwm‚àíw‚àó‚ÄñL22\displaystyle\left(\frac{1}{2}-\frac{h\lambda\_{2}}{2}\right)\left\|w\_{m}-w\_{\*}\right\|^{2}\_{L^{2}} | +h‚ÄãŒª22‚Äã‚Äñwm‚àíw‚àó‚Äñ‚Ñã012\displaystyle+\frac{h\lambda\_{2}}{2}\left\|w\_{m}-w\_{\*}\right\|^{2}\_{\mathcal{H}\_{0}^{1}} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§12‚Äã‚Äñwm‚àíw‚àó‚ÄñL22+h2‚Äã‚ü®‚Ñí‚Äã(wm‚àíw‚àó),wm‚àíw‚àó‚ü©‚Ñã‚àí1,‚Ñã01‚Üí0,\displaystyle\leq\frac{1}{2}\left\|w\_{m}-w\_{\*}\right\|^{2}\_{L^{2}}+\frac{h}{2}\left\langle\mathcal{L}(w\_{m}-w\_{\*}),w\_{m}-w\_{\*}\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}}\to 0, |  |

since wm‚áÄw‚àów\_{m}\rightharpoonup w\_{\*}.
Therefore, by [Assumption¬†(G√Ö)](https://arxiv.org/html/2512.25017v1#Thmassumption2 "Assumption (G√Ö). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"), we conclude ‚Äñwm‚àíw‚àó‚Äñ‚Ñã01‚Üí0\left\|w\_{m}-w\_{\*}\right\|\_{\mathcal{H}\_{0}^{1}}\to 0.
‚àé

## 4. Convergence of the training error

In this section, we show that for each fixed time step kk, the trained neural network converges to the true solution of the discretized PDE ([3.1](https://arxiv.org/html/2512.25017v1#S3.E1 "Equation 3.1 ‚Ä£ 3.2. Time stepping ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")) as the number of neurons and the training time tend to infinity.
Therefore, using the convergence of the time-stepping scheme, we can conclude the convergence of the training error.

### 4.1. Convergence of the trained neural network

In this subsection, we analyze the training of the neural network for the deep gradient flow method as a function of the number of neurons nn.
In particular, we would like to study the training process of the parameters Œ∏n\theta^{n} as n‚Üí‚àûn\to\infty, such that the neural network introduced in [Definition¬†3.8](https://arxiv.org/html/2512.25017v1#S3.Thmtheorem8 "Definition 3.8 (Neural network). ‚Ä£ 3.4. Neural network approximation and a version of the Universal Approximation Theorem ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs") approximates the solution of the discretized PDE ([3.1](https://arxiv.org/html/2512.25017v1#S3.E1 "Equation 3.1 ‚Ä£ 3.2. Time stepping ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")).
We show that this process satisfies a gradient flow equation as the number of neurons tends to infinity, *i.e.* in the so-called ‚Äúwide network limit‚Äù.

Let us denote the parameters of the neural network by Œ∏n=(Œ≤i,Œ±i,ci)i=1n‚àà‚Ñùn√ó‚Ñùn√ó‚Ñùd√ón\theta^{n}=\left(\beta^{i},\alpha^{i},c^{i}\right)^{n}\_{i=1}\in\mathbb{R}^{n}\times\mathbb{R}^{n}\times\mathbb{R}^{d\times n}.
Moreover, for 12<Œ¥<1\frac{1}{2}<\delta<1, let us introduce a neural network

|  |  |  |  |
| --- | --- | --- | --- |
|  | Vn‚Äã(Œ∏n;x)\displaystyle V^{n}\left(\theta^{n};x\right) | =1nŒ¥‚Äã‚àëi=1nŒ≤^i,n‚Äãœà‚Äã(Œ±^i,n‚Äãx+c^i,n),\displaystyle=\frac{1}{n^{\delta}}\sum\_{i=1}^{n}\hat{\beta}^{i,n}\psi\left(\hat{\alpha}^{i,n}x+\hat{c}^{i,n}\right), |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
| in accordance with [Definition¬†3.8](https://arxiv.org/html/2512.25017v1#S3.Thmtheorem8 "Definition 3.8 (Neural network). ‚Ä£ 3.4. Neural network approximation and a version of the Universal Approximation Theorem ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"), where the ‚Äúclipped‚Äù parameters are defined as follows: | | | | |
|  |  |  |  |
| --- | --- | --- | --- |
|  | Œ±^i,n\displaystyle\hat{\alpha}^{i,n} | ={(rn‚àßŒ±i)‚à®1rn,for¬†‚ÄãŒ±i>0,(‚àí1rn‚àßŒ±i)‚à®(‚àírn),for¬†‚ÄãŒ±i<0,\displaystyle=\begin{cases}\begin{aligned} \left(r\_{n}\land\alpha^{i}\right)\lor\frac{1}{r\_{n}},\quad&\text{for }\alpha^{i}>0,\\ \left(\frac{-1}{r\_{n}}\land\alpha^{i}\right)\lor(-r\_{n}),\quad&\text{for }\alpha^{i}<0,\end{aligned}\end{cases} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | Œ≤^i,n\displaystyle\hat{\beta}^{i,n} | =(rn‚àßŒ≤i)‚à®(‚àírn),\displaystyle=\left(r\_{n}\land\beta^{i}\right)\lor(-r\_{n}), |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | c^i,n\displaystyle\hat{c}^{i,n} | =(rn‚àßci)‚à®(‚àírn),\displaystyle=\left(r\_{n}\land c^{i}\right)\lor(-r\_{n}), |  |

for some rnr\_{n} increasing with nn. We restrict the domain of the parameters (Œ≤i,Œ±i,ci)(\beta^{i},\alpha^{i},c^{i}) to [‚àírn,rn][-r\_{n},r\_{n}] which converges to ‚Ñù\mathbb{R} as n‚Üí‚àûn\to\infty, and for Œ±i\alpha^{i} we also need to subtract the ball (‚àí1rn,1rn)\left(-\frac{1}{r\_{n}},\frac{1}{r\_{n}}\right).
Gradient clipping is in accordance with deep learning literature, see, for example, Zhang et¬†al. [[32](https://arxiv.org/html/2512.25017v1#bib.bib32)] and Goodfellow et¬†al. [[10](https://arxiv.org/html/2512.25017v1#bib.bib10), Ch. 10 and 11].

Next, let us introduce the gradient descent dynamics for the training process of the parameters Œ∏n\theta^{n}, where tt denotes the training time.
The neural network Vn‚Äã(Œ∏n;‚ãÖ)V^{n}(\theta^{n};\cdot) should minimize the loss functional IkI^{k} in ([3.3](https://arxiv.org/html/2512.25017v1#S3.E3 "Equation 3.3 ‚Ä£ 3.3. Weak formulation and uniqueness of minimizer ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")) of the deep gradient flow method.
Hence, the dynamic of Œ∏tn\theta\_{t}^{n} should match the gradient of Ik‚Äã(Vn;‚ãÖ)I^{k}(V^{n};\cdot), *i.e.*

|  |  |  |  |
| --- | --- | --- | --- |
|  | d‚ÄãŒ∏tnd‚Äãt=\displaystyle\frac{\mathrm{d}\theta\_{t}^{n}}{\mathrm{d}t}= | ‚àíŒ∑n‚Äã‚àáŒ∏Ik‚Äã(Vn‚Äã(Œ∏tn;x))\displaystyle-\eta\_{n}\nabla\_{\theta}I^{k}\left(V^{n}\left(\theta\_{t}^{n};x\right)\right) |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | =\displaystyle= | ‚àíŒ∑n‚Äã‚ü®ùíü‚ÄãIk‚Äã(Vtn),‚àáŒ∏Vtn‚ü©‚Ñã01,\displaystyle-\eta\_{n}\left\langle\mathcal{D}I^{k}\left(V\_{t}^{n}\right),\nabla\_{\theta}V\_{t}^{n}\right\rangle\_{\mathcal{H}\_{0}^{1}}, |  | (4.1) |

with learning rate Œ∑n=n2‚ÄãŒ¥‚àí1\eta\_{n}=n^{2\delta-1}, where ùíü\mathcal{D} denotes the Fr√©chet derivative; *i.e.* for any u,v‚àà‚Ñã01‚Äã(‚Ñùd)u,v\in\mathcal{H}\_{0}^{1}(\mathbb{R}^{d}),

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | ‚ü®ùíü‚ÄãIk‚Äã(v),u‚ü©‚Ñã01=\displaystyle\left\langle\mathcal{D}I^{k}\left(v\right),u\right\rangle\_{\mathcal{H}\_{0}^{1}}= | ‚ü®v‚àíUk‚àí1,u‚ü©L2+h‚Äã‚ü®‚Ñí‚Äãv,u‚ü©‚Ñã‚àí1,‚Ñã01+h‚Äã‚ü®F‚Äã(Uk‚àí1),u‚ü©L2.\displaystyle\left\langle v-U^{k-1},u\right\rangle\_{L^{2}}+h\left\langle\mathcal{L}v,u\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}}+h\left\langle F\left(U^{k-1}\right),u\right\rangle\_{L^{2}}. |  | (4.2) |

We obtain a coordinate dynamic (Œ∏tn)t‚â•0=(Œ∏ti,n)t‚â•0=(Œ≤ti,n,Œ±ti,n,cti,n)t‚â•0{(\theta^{n}\_{t})\_{t\geq 0}}=(\theta\_{t}^{i,n})\_{t\geq 0}=\big(\beta\_{t}^{i,n},\alpha\_{t}^{i,n},c\_{t}^{i,n}\big)\_{t\geq 0}.
This dynamic depends on the number of hidden layers nn of the neural network, since the parameters that optimally approximate a function depend on the number of parameters we use.
We use a random initialization for this process, independent of nn, denoted by:

|  |  |  |
| --- | --- | --- |
|  | (Œ≤0i,n,Œ±0i,n,c0i,n)=(Œ≤0i,Œ±0i,c0i)=Œ∏0i.\left(\beta\_{0}^{i,n},\alpha\_{0}^{i,n},c\_{0}^{i,n}\right)=\left(\beta\_{0}^{i},\alpha\_{0}^{i},c\_{0}^{i}\right)=\theta^{i}\_{0}. |  |

###### Assumption (NNI).

The parameters Œ≤0i,Œ±0i,c0i\beta\_{0}^{i},\alpha\_{0}^{i},c\_{0}^{i} that initialize the neural network are i.i.d. random variables that satisfy:

* (i)

  Œ≤0i\beta\_{0}^{i} is a symmetric random variable with finite second moment: ùîº‚Äã[|Œ≤0i|2]<+‚àû\mathbb{E}\left[\left|\beta\_{0}^{i}\right|^{2}\right]<+\infty;
* (ii)

  Œ±0i‚â†0\alpha\_{0}^{i}\neq 0 ‚Ñô\mathbb{P}-almost surely and ùîº‚Äã[|Œ±0i|d+7+|Œ±0i|‚àíd‚àí2]<+‚àû\mathbb{E}\left[\left|\alpha\_{0}^{i}\right|^{d+7}+\left|\alpha\_{0}^{i}\right|^{-d-2}\right]<+\infty;
* (iii)

  c0ic\_{0}^{i} is an ‚Ñùd\mathbb{R}^{d}-valued random variable and ùîº‚Äã[|c0i|d+7]<+‚àû\mathbb{E}\left[\left|c\_{0}^{i}\right|^{d+7}\right]<+\infty;
* (iv)

  Œ±0i,c0i\alpha\_{0}^{i},\ c\_{0}^{i}, have full support, *i.e.* for any Borel set A‚äÇ‚ÑùA\subset\mathbb{R} and B‚äÇ‚ÑùdB\subset\mathbb{R}^{d} with positive Lebesgue measure, ‚Ñô‚Äã(Œ±0i‚ààA)\mathbb{P}\left(\alpha\_{0}^{i}\in A\right) and ‚Ñô‚Äã(c0i‚ààB)\mathbb{P}\left(c\_{0}^{i}\in B\right) are positive.

Using the chain rule, the dynamic Vtn‚Äã(x)=Vn‚Äã(Œ∏tn;x)V\_{t}^{n}(x)=V^{n}\left(\theta\_{t}^{n};x\right) satisfies the following equation

|  |  |  |  |
| --- | --- | --- | --- |
|  | d‚ÄãVtn‚Äã(x)d‚Äãt\displaystyle\frac{\mathrm{d}V\_{t}^{n}(x)}{\mathrm{d}t} | =‚àáŒ∏Vn‚Äã(Œ∏tn;x)‚ãÖd‚ÄãŒ∏tnd‚Äãt=‚àíŒ∑n‚Äã‚àáŒ∏Vn‚Äã(Œ∏tn;x)‚ãÖ‚àáŒ∏Ik‚Äã(Vn‚Äã(Œ∏tn;x))\displaystyle=\nabla\_{\theta}V^{n}\left(\theta\_{t}^{n};x\right)\cdot\frac{\mathrm{d}\theta^{n}\_{t}}{\mathrm{d}t}=-\eta\_{n}\nabla\_{\theta}V^{n}\left(\theta\_{t}^{n};x\right)\cdot\nabla\_{\theta}I^{k}\left(V^{n}\left(\theta\_{t}^{n};x\right)\right) |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | =‚àí‚ü®ùíü‚ÄãIk‚Äã(Vtn),Ztn‚Äã(x,‚ãÖ)‚ü©‚Ñã01,\displaystyle=-\left\langle\mathcal{D}I^{k}\left(V^{n}\_{t}\right),Z\_{t}^{n}(x,\cdot)\right\rangle\_{\mathcal{H}\_{0}^{1}}, |  | (4.3) |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
| with V0n‚Äã(x)=Vn‚Äã(Œ∏0n;x)V\_{0}^{n}(x)=V^{n}\left(\theta\_{0}^{n};x\right) and | | | | |
|  |  |  |  |
| --- | --- | --- | --- |
|  | Ztn‚Äã(x,y)\displaystyle Z\_{t}^{n}\left(x,y\right) | =Œ∑n‚Äã‚àáŒ∏Vtn‚Äã(x)‚ãÖ‚àáŒ∏Vtn‚Äã(y)\displaystyle=\eta\_{n}\nabla\_{\theta}V\_{t}^{n}(x)\cdot\nabla\_{\theta}V\_{t}^{n}\left(y\right) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =1n‚Äã‚àëi=1n‚àáŒ∏Œ≤^ti,n‚Äãœà‚Äã(Œ±^ti,n‚Äãx+c^ti,n)‚ãÖ‚àáŒ∏Œ≤^ti,n‚Äãœà‚Äã(Œ±^ti,n‚Äãy+c^ti,n).\displaystyle=\frac{1}{n}\sum\_{i=1}^{n}\nabla\_{\theta}\hat{\beta}^{i,n}\_{t}\psi\left(\hat{\alpha}^{i,n}\_{t}x+\hat{c}^{i,n}\_{t}\right)\cdot\nabla\_{\theta}\hat{\beta}^{i,n}\_{t}\psi\left(\hat{\alpha}^{i,n}\_{t}y+\hat{c}^{i,n}\_{t}\right). |  |

We expect ([4.1](https://arxiv.org/html/2512.25017v1#S4.Ex7 "4.1. Convergence of the trained neural network ‚Ä£ 4. Convergence of the training error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")) to converge to the following gradient flow

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | d‚ÄãVt‚Äã(x)d‚Äãt\displaystyle\frac{\mathrm{d}V\_{t}(x)}{\mathrm{d}t} | =‚àí‚ü®ùíü‚ÄãIk‚Äã(Vt),Z‚Äã(x,‚ãÖ)‚ü©‚Ñã01,\displaystyle=-\left\langle\mathcal{D}I^{k}\left(V\_{t}\right),Z(x,\cdot)\right\rangle\_{\mathcal{H}\_{0}^{1}}, |  | (4.4) |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
| with V0=0V\_{0}=0, where | | | | |
|  |  |  |  |
| --- | --- | --- | --- |
|  | Z‚Äã(x,y)\displaystyle Z(x,y) | =ùîº‚Äã[‚àáŒ∏Œ≤01‚Äãœà‚Äã(Œ±01‚Äãx+c01)‚ãÖ‚àáŒ∏Œ≤01‚Äãœà‚Äã(Œ±01‚Äãy+c01)],\displaystyle=\mathbb{E}\big[\nabla\_{\theta}\beta^{1}\_{0}\psi\left(\alpha^{1}\_{0}x+c^{1}\_{0}\right)\cdot\nabla\_{\theta}\beta^{1}\_{0}\psi\left(\alpha^{1}\_{0}y+c^{1}\_{0}\right)\big], |  |

while the inner product of the Fr√©chet derivative of the loss functional with another functional is defined in ([4.2](https://arxiv.org/html/2512.25017v1#S4.E2 "Equation 4.2 ‚Ä£ 4.1. Convergence of the trained neural network ‚Ä£ 4. Convergence of the training error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")).
The gradient flow ([4.4](https://arxiv.org/html/2512.25017v1#S4.E4 "Equation 4.4 ‚Ä£ 4.1. Convergence of the trained neural network ‚Ä£ 4. Convergence of the training error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")) is an infinite-dimensional ODE that governs the dynamics of the wide network limit of the neural network during the training process and, the right-hand side depends on the loss function of the gradient flow method for the solution of PDEs in ([3.3](https://arxiv.org/html/2512.25017v1#S3.E3 "Equation 3.3 ‚Ä£ 3.3. Weak formulation and uniqueness of minimizer ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")).
Let us also point out that the kernel Z‚Äã(‚ãÖ,‚ãÖ)Z(\cdot,\cdot) is not the standard neural tangent kernel, as it also depends on the loss functional, which further complicates the analysis.
The right-hand side of ([4.4](https://arxiv.org/html/2512.25017v1#S4.E4 "Equation 4.4 ‚Ä£ 4.1. Convergence of the trained neural network ‚Ä£ 4. Convergence of the training error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")), using ([4.2](https://arxiv.org/html/2512.25017v1#S4.E2 "Equation 4.2 ‚Ä£ 4.1. Convergence of the trained neural network ‚Ä£ 4. Convergence of the training error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")), takes the following form:

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùíØ‚Äã(v)‚Äã(x)\displaystyle\mathcal{T}\left(v\right)\left(x\right) | :=‚ü®ùíü‚ÄãIk‚Äã(v),Z‚Äã(x,‚ãÖ)‚ü©‚Ñã01\displaystyle=\left\langle\mathcal{D}I^{k}\left(v\right),Z(x,\cdot)\right\rangle\_{\mathcal{H}\_{0}^{1}} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =‚ü®v‚àíUk‚àí1,Z‚Äã(x,‚ãÖ)‚ü©L2+h‚Äã‚ü®‚Ñí‚Äãv,Z‚Äã(x,‚ãÖ)‚ü©‚Ñã‚àí1,‚Ñã01+h‚Äã‚ü®F‚Äã(Uk‚àí1),Z‚Äã(x,‚ãÖ)‚ü©L2.\displaystyle=\left\langle v-U^{k-1},Z(x,\cdot)\right\rangle\_{L^{2}}+h\left\langle\mathcal{L}v,Z(x,\cdot)\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}}+h\left\langle F\left(U^{k-1}\right),Z(x,\cdot)\right\rangle\_{L^{2}}. |  |

The analysis of this operator plays a crucial role in the next subsection, where we study the long term behavior of this gradient flow.

Let us introduce the following shorthand notation:

|  |  |  |
| --- | --- | --- |
|  | ùí≥‚Äã(Œ∏;x):=‚àáŒ∏Œ≤‚Äãœà‚Äã(Œ±‚Äãx+c)andùí≥n‚Äã(Œ∏;x):=‚àáŒ∏Œ≤^‚Äãœà‚Äã(Œ±^‚Äãx+c^)\mathcal{X}(\theta;x):=\nabla\_{\theta}\beta\psi\left(\alpha x+c\right)\quad\text{and}\quad\mathcal{X}^{n}(\theta;x):=\nabla\_{\theta}\hat{\beta}\psi\left(\hat{\alpha}x+\hat{c}\right) |  |

for some generic parameters Œ∏=(Œ≤,Œ±,c)‚àà‚Ñù√ó‚Ñù√ó‚Ñùd\theta=\left(\beta,\alpha,c\right)\in\mathbb{R}\times\mathbb{R}\times\mathbb{R}^{d}, where (Œ≤^,Œ±^,c^)(\hat{\beta},\hat{\alpha},\hat{c}) denotes the clipped version of these parameters.
Moreover, in order to simplify the notation, let us set

|  |  |  |
| --- | --- | --- |
|  | X‚Äã(x):=ùí≥‚Äã(Œ∏01;x),Xn‚Äã(x):=ùí≥n‚Äã(Œ∏01;x)andXti,n‚Äã(x):=ùí≥n‚Äã(Œ∏ti,n;x).X(x):=\mathcal{X}\left(\theta^{1}\_{0};x\right),\quad X^{n}(x):=\mathcal{X}^{n}\left(\theta^{1}\_{0};x\right)\quad\text{and}\quad X\_{t}^{i,n}(x):=\mathcal{X}^{n}\left(\theta^{i,n}\_{t};x\right). |  |

Then, using this notation we have that

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ztn‚Äã(x,y)\displaystyle Z\_{t}^{n}(x,y) | =1n‚Äã‚àëi=1nXti,n‚Äã(x)‚ãÖXti,n‚Äã(y),\displaystyle=\frac{1}{n}\sum\_{i=1}^{n}X\_{t}^{i,n}\left(x\right)\cdot X\_{t}^{i,n}\left(y\right), |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
| and | | | | |
|  |  |  |  |
| --- | --- | --- | --- |
|  | Z‚Äã(x,y)\displaystyle Z(x,y) | =ùîº‚Äã[X‚Äã(x)‚ãÖX‚Äã(y)].\displaystyle=\mathbb{E}\left[X\left(x\right)\cdot X\left(y\right)\right]. |  |

This representation invites us to use the law of large numbers to conclude that Ztn‚Äã(x,y)‚ÜíZ‚Äã(x,y)Z^{n}\_{t}(x,y)\to Z(x,y) as n‚Üí‚àûn\to\infty.
Intuitively, this is the connection between the gradient flow in ([4.1](https://arxiv.org/html/2512.25017v1#S4.Ex7 "4.1. Convergence of the trained neural network ‚Ä£ 4. Convergence of the training error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")) that the neural network follows during the training process, and the corresponding ‚Äúwide network limit‚Äù in ([4.4](https://arxiv.org/html/2512.25017v1#S4.E4 "Equation 4.4 ‚Ä£ 4.1. Convergence of the trained neural network ‚Ä£ 4. Convergence of the training error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")).

The main result of this subsection follows, which states that as the number of neurons tends to infinity during the training process, then the neural network VnV^{n} converges to the wide network limit VV, which satisfies the gradient flow ([4.4](https://arxiv.org/html/2512.25017v1#S4.E4 "Equation 4.4 ‚Ä£ 4.1. Convergence of the trained neural network ‚Ä£ 4. Convergence of the training error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")).

###### Theorem 4.1.

Assume that the neural network satisfies [Assumption¬†(NNI)](https://arxiv.org/html/2512.25017v1#Thmassumption5 "Assumption (NNI). ‚Ä£ 4.1. Convergence of the trained neural network ‚Ä£ 4. Convergence of the training error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"), and let rnr\_{n} increase with nn, while rn‚â§log‚Å°nr\_{n}\leq\log n.
Moreover, assume that the operators of the PDE ([2.1](https://arxiv.org/html/2512.25017v1#S2.E1 "Equation 2.1 ‚Ä£ 2. Deep gradient flow methods for PDEs ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"))‚Äì([2.2](https://arxiv.org/html/2512.25017v1#S2.E2 "Equation 2.2 ‚Ä£ 2. Deep gradient flow methods for PDEs ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")) satisfy [Assumptions¬†(CON)](https://arxiv.org/html/2512.25017v1#Thmassumption1 "Assumption (CON). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs") and¬†[(G√Ö)](https://arxiv.org/html/2512.25017v1#Thmassumption2 "Assumption (G√Ö). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs").
Then, the dynamic ([4.1](https://arxiv.org/html/2512.25017v1#S4.Ex7 "4.1. Convergence of the trained neural network ‚Ä£ 4. Convergence of the training error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")) converges to the gradient flow ([4.4](https://arxiv.org/html/2512.25017v1#S4.E4 "Equation 4.4 ‚Ä£ 4.1. Convergence of the trained neural network ‚Ä£ 4. Convergence of the training error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")) as n‚Üí‚àûn\to\infty, *i.e.* for any T>0,T>0,

|  |  |  |
| --- | --- | --- |
|  | sup0‚â§t‚â§Tùîº‚Äã[‚ÄñVtn‚àíVt‚Äñ‚Ñã01]‚Üín‚Üí‚àû0.\displaystyle\sup\_{0\leq t\leq T}\mathbb{E}\left[\big\|V^{n}\_{t}-V\_{t}\big\|\_{\mathcal{H}\_{0}^{1}}\right]\xrightarrow[n\to\infty]{}0. |  |

###### Lemma 4.2.

Assume that the neural network satisfies [Assumption¬†(NNI)](https://arxiv.org/html/2512.25017v1#Thmassumption5 "Assumption (NNI). ‚Ä£ 4.1. Convergence of the trained neural network ‚Ä£ 4. Convergence of the training error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"), then

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Äã[‚ÄñV0n‚Äñ‚Ñã01]‚â§C(1)‚Äãn12‚àíŒ¥,\displaystyle\mathbb{E}\left[\left\|V^{n}\_{0}\right\|\_{\mathcal{H}\_{0}^{1}}\right]\leq C^{\left(1\right)}n^{\frac{1}{2}-\delta}, |  |

where C(1)=ùîº‚Äã[|Œ≤0i|2]12‚Äã(ùîº‚Äã[|Œ±0i|‚àíd]+ùîº‚Äã[|Œ±0i|2‚àíd]+2)12‚Äã‚Äñœà‚Äñ‚Ñã01C^{\left(1\right)}=\mathbb{E}\left[\left|\beta\_{0}^{i}\right|^{2}\right]^{\frac{1}{2}}\left(\mathbb{E}\left[\left|\alpha\_{0}^{i}\right|^{-d}\right]+\mathbb{E}\left[\left|\alpha\_{0}^{i}\right|^{2-d}\right]+2\right)^{\frac{1}{2}}\left\|\psi\right\|\_{\mathcal{H}\_{0}^{1}}.

###### Proof.

Let us denote a neuron by Yi‚Äã(x):=Œ≤^0i‚Äãœà‚Äã(Œ±^0i‚Äãx+c^0i)Y^{i}(x):=\hat{\beta}\_{0}^{i}\psi\left(\hat{\alpha}\_{0}^{i}x+\hat{c}\_{0}^{i}\right), then V0n=1nŒ¥‚Äã‚àëi=1nYi‚Äã(x)V\_{0}^{n}=\frac{1}{n^{\delta}}\sum\_{i=1}^{n}Y^{i}(x) and

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùîº‚Äã[‚ÄñV0n‚Äñ‚Ñã012]=ùîº‚Äã[‚à´‚Ñùd|V0n|2+|‚àáxV0n|2‚Äãd‚Äãx].\mathbb{E}\left[\left\|V^{n}\_{0}\right\|\_{\mathcal{H}\_{0}^{1}}^{2}\right]=\mathbb{E}\left[\int\_{\mathbb{R}^{d}}\left|V^{n}\_{0}\right|^{2}+\left|\nabla\_{x}V^{n}\_{0}\right|^{2}\mathrm{d}x\right]. |  | (4.5) |

Let us first compute the value of the cross terms, for i‚â†ji\neq j, which equals

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùîº‚Äã[‚à´‚ÑùdYi‚Äã(x)‚ÄãYj‚Äã(x)‚Äãdx]\displaystyle\mathbb{E}\left[\int\_{\mathbb{R}^{d}}Y^{i}(x)Y^{j}(x)\mathrm{d}x\right] | =ùîº‚Äã[‚à´‚ÑùdŒ≤^0i‚Äãœà‚Äã(Œ±^0i‚Äãx+c^0i)‚ÄãŒ≤^0j‚Äãœà‚Äã(Œ±^0j‚Äãx+c^0j)‚Äãdx]\displaystyle=\mathbb{E}\left[\int\_{\mathbb{R}^{d}}\hat{\beta}\_{0}^{i}\psi\left(\hat{\alpha}\_{0}^{i}x+\hat{c}\_{0}^{i}\right)\hat{\beta}\_{0}^{j}\psi\left(\hat{\alpha}\_{0}^{j}x+\hat{c}\_{0}^{j}\right)\mathrm{d}x\right] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =‚à´‚Ñùdùîº‚Äã[Œ≤^0i]‚Äãùîº‚Äã[Œ≤^0j]‚Äãùîº‚Äã[œà‚Äã(Œ±^0i‚Äãx+c^0i)‚Äãœà‚Äã(Œ±^0j‚Äãx+c^0j)]‚Äãdx=0;\displaystyle=\int\_{\mathbb{R}^{d}}\mathbb{E}\left[\hat{\beta}\_{0}^{i}\right]\mathbb{E}\left[\hat{\beta}\_{0}^{j}\right]\mathbb{E}\left[\psi\left(\hat{\alpha}\_{0}^{i}x+\hat{c}\_{0}^{i}\right)\psi\left(\hat{\alpha}\_{0}^{j}x+\hat{c}\_{0}^{j}\right)\right]\mathrm{d}x=0; |  |

here, we can apply Fubini‚Äôs theorem since œà\psi has compact support and the parameters Œ∏\theta are bounded, while the random variables Œ≤0i\beta^{i}\_{0} are symmetric, hence their expectation is zero.
Therefore, we can bound the L2L^{2}-norm of V0nV\_{0}^{n} by

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùîº‚Äã[‚à´‚Ñùd|V0n|2‚Äãdx]\displaystyle\mathbb{E}\left[\int\_{\mathbb{R}^{d}}\left|V^{n}\_{0}\right|^{2}\mathrm{d}x\right] | =1n2‚ÄãŒ¥‚Äãùîº‚Äã[‚à´‚Ñùd‚àëi=1n|Œ≤^0i|2‚Äã|œà‚Äã(Œ±^0i‚Äãx+c^0i)|2‚Äãd‚Äãx]\displaystyle=\frac{1}{n^{2\delta}}\mathbb{E}\left[\int\_{\mathbb{R}^{d}}\sum\_{i=1}^{n}\left|\hat{\beta}\_{0}^{i}\right|^{2}\left|\psi\left(\hat{\alpha}\_{0}^{i}x+\hat{c}\_{0}^{i}\right)\right|^{2}\mathrm{d}x\right] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =1n2‚ÄãŒ¥‚àí1‚Äãùîº‚Äã[|Œ≤^0i|2‚Äã‚à´‚Ñùd|Œ±^0i|‚àíd‚Äã|œà‚Äã(y)|2‚Äãdy]\displaystyle=\frac{1}{n^{2\delta-1}}\mathbb{E}\left[\left|\hat{\beta}\_{0}^{i}\right|^{2}\int\_{\mathbb{R}^{d}}\left|\hat{\alpha}\_{0}^{i}\right|^{-d}\left|\psi(y)\right|^{2}\mathrm{d}y\right] |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | ‚â§1n2‚ÄãŒ¥‚àí1‚Äãùîº‚Äã[|Œ≤^0i|2]‚Äãùîº‚Äã[|Œ±^0i|‚àíd]‚Äã‚à´‚Ñùd|œà‚Äã(y)|2‚Äãdy.\displaystyle\leq\frac{1}{n^{2\delta-1}}\mathbb{E}\left[\left|\hat{\beta}\_{0}^{i}\right|^{2}\right]\mathbb{E}\left[\left|\hat{\alpha}\_{0}^{i}\right|^{-d}\right]\int\_{\mathbb{R}^{d}}\left|\psi(y)\right|^{2}\mathrm{d}y. |  | (4.6) |

The derivative of V0nV^{n}\_{0} can be expressed as

|  |  |  |
| --- | --- | --- |
|  | ‚àáxV0n=1nŒ¥‚Äã‚àëi=1n‚àáxYi‚Äã(x)=1nŒ¥‚Äã‚àëi=1nŒ≤^0i‚ÄãŒ±^0i‚Äã(‚àáœà)‚Äã(Œ±^0i‚Äãx+c^0i).\nabla\_{x}V^{n}\_{0}=\frac{1}{n^{\delta}}\sum\_{i=1}^{n}\nabla\_{x}Y^{i}(x)=\frac{1}{n^{\delta}}\sum\_{i=1}^{n}\hat{\beta}\_{0}^{i}\hat{\alpha}\_{0}^{i}\left(\nabla\psi\right)\left(\hat{\alpha}\_{0}^{i}x+\hat{c}\_{0}^{i}\right). |  |

Hence, we can analogously bound the L2L^{2}-norm of the derivative of V0nV^{n}\_{0} by

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùîº‚Äã[‚à´‚Ñùd|‚àáxV0n|2‚Äãdx]\displaystyle\mathbb{E}\left[\int\_{\mathbb{R}^{d}}\left|\nabla\_{x}V^{n}\_{0}\right|^{2}\mathrm{d}x\right] | =1n2‚ÄãŒ¥‚àí1‚Äãùîº‚Äã[‚à´‚Ñùd|Œ≤^0i|2‚Äã|Œ±^0i|2‚Äã|(‚àáœà)‚Äã(Œ±^0i‚Äãx+c^0i)|2‚Äãdx]\displaystyle=\frac{1}{n^{2\delta-1}}\mathbb{E}\left[\int\_{\mathbb{R}^{d}}\left|\hat{\beta}\_{0}^{i}\right|^{2}\left|\hat{\alpha}\_{0}^{i}\right|^{2}\left|\left(\nabla\psi\right)\left(\hat{\alpha}\_{0}^{i}x+\hat{c}\_{0}^{i}\right)\right|^{2}\mathrm{d}x\right] |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | ‚â§1n2‚ÄãŒ¥‚àí1‚Äãùîº‚Äã[|Œ≤^0i|2]‚Äãùîº‚Äã[|Œ±^0i|2‚àíd]‚Äã‚à´‚Ñùd|‚àáœà‚Äã(x)|2‚Äãdx.\displaystyle\leq\frac{1}{n^{2\delta-1}}\mathbb{E}\left[\left|\hat{\beta}\_{0}^{i}\right|^{2}\right]\mathbb{E}\left[\left|\hat{\alpha}\_{0}^{i}\right|^{2-d}\right]\int\_{\mathbb{R}^{d}}\left|\nabla\psi(x)\right|^{2}\mathrm{d}x. |  | (4.7) |

Applying bounds ([4.1](https://arxiv.org/html/2512.25017v1#S4.Ex20 "Proof. ‚Ä£ 4.1. Convergence of the trained neural network ‚Ä£ 4. Convergence of the training error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")) and ([4.1](https://arxiv.org/html/2512.25017v1#S4.Ex23 "Proof. ‚Ä£ 4.1. Convergence of the trained neural network ‚Ä£ 4. Convergence of the training error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")) to ([4.5](https://arxiv.org/html/2512.25017v1#S4.E5 "Equation 4.5 ‚Ä£ Proof. ‚Ä£ 4.1. Convergence of the trained neural network ‚Ä£ 4. Convergence of the training error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")), and using Jensen‚Äôs inequality and that |Œ≤^0i|‚â§|Œ≤0i|\left|\hat{\beta}\_{0}^{i}\right|\leq\left|\beta\_{0}^{i}\right|, yields

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùîº‚Äã[‚ÄñV0n‚Äñ‚Ñã01]\displaystyle\mathbb{E}\left[\left\|V^{n}\_{0}\right\|\_{\mathcal{H}\_{0}^{1}}\right] | ‚â§ùîº‚Äã[‚ÄñV0n‚Äñ‚Ñã012]\displaystyle\leq\sqrt{\mathbb{E}\left[\left\|V^{n}\_{0}\right\|\_{\mathcal{H}\_{0}^{1}}^{2}\right]} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§1n2‚ÄãŒ¥‚àí1‚Äãùîº‚Äã[|Œ≤0i|2]‚Äãùîº‚Äã[|Œ±^0i|‚àíd]‚Äã‚à´‚Ñùd|œà|2‚Äãdx+1n2‚ÄãŒ¥‚àí1‚Äãùîº‚Äã[|Œ≤0i|2]‚Äãùîº‚Äã[|Œ±^0i|2‚àíd]‚Äã‚à´‚Ñùd|‚àáœà|2‚Äãdx\displaystyle\leq\sqrt{\frac{1}{n^{2\delta-1}}\mathbb{E}\left[\left|\beta\_{0}^{i}\right|^{2}\right]\mathbb{E}\left[\left|\hat{\alpha}\_{0}^{i}\right|^{-d}\right]\int\_{\mathbb{R}^{d}}\left|\psi\right|^{2}\mathrm{d}x+\frac{1}{n^{2\delta-1}}\mathbb{E}\left[\left|\beta\_{0}^{i}\right|^{2}\right]\mathbb{E}\left[\left|\hat{\alpha}\_{0}^{i}\right|^{2-d}\right]\int\_{\mathbb{R}^{d}}\left|\nabla\psi\right|^{2}\mathrm{d}x} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§n12‚àíŒ¥‚Äãùîº‚Äã[|Œ≤0i|2]12‚Äã(ùîº‚Äã[|Œ±^0i|‚àíd]+ùîº‚Äã[|Œ±^0i|2‚àíd])12‚Äã‚à´‚Ñùd|œà|2‚Äãdx+‚à´‚Ñùd|‚àáœà|2‚Äãdx.\displaystyle\leq n^{\frac{1}{2}-\delta}\mathbb{E}\left[\left|\beta\_{0}^{i}\right|^{2}\right]^{\frac{1}{2}}\left(\mathbb{E}\left[\left|\hat{\alpha}\_{0}^{i}\right|^{-d}\right]+\mathbb{E}\left[\left|\hat{\alpha}\_{0}^{i}\right|^{2-d}\right]\right)^{\frac{1}{2}}\sqrt{\int\_{\mathbb{R}^{d}}\left|\psi\right|^{2}\mathrm{d}x+\int\_{\mathbb{R}^{d}}\left|\nabla\psi\right|^{2}\mathrm{d}x}. |  |

If |Œ±|‚â§rn\left|\alpha\right|\leq r\_{n}, then |Œ±^|‚àí1‚â§|Œ±|‚àí1\left|\hat{\alpha}\right|^{-1}\leq\left|\alpha\right|^{-1} and if |Œ±|>rn\left|\alpha\right|>r\_{n}, then |Œ±^|‚àí1=rn‚àí1\left|\hat{\alpha}\right|^{-1}=r\_{n}^{-1}.
Therefore, for d=1,2d=1,2, we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùîº‚Äã[|Œ±^0i|‚àíd]+ùîº‚Äã[|Œ±^0i|2‚àíd]\displaystyle\mathbb{E}\left[\left|\hat{\alpha}\_{0}^{i}\right|^{-d}\right]+\mathbb{E}\left[\left|\hat{\alpha}\_{0}^{i}\right|^{2-d}\right] | ‚â§ùîº‚Äã[|Œ±01|‚àíd+(rn)‚àíd+|Œ±01|2‚àíd],\displaystyle\leq\mathbb{E}\left[\left|\alpha\_{0}^{1}\right|^{-d}+\left(r\_{n}\right)^{-d}+\left|\alpha\_{0}^{1}\right|^{2-d}\right], |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
| while for d‚â•3d\geq 3, we get | | | | |
|  |  |  |  |
| --- | --- | --- | --- |
|  | ùîº‚Äã[|Œ±^0i|‚àíd]+ùîº‚Äã[|Œ±^0i|2‚àíd]\displaystyle\mathbb{E}\left[\left|\hat{\alpha}\_{0}^{i}\right|^{-d}\right]+\mathbb{E}\left[\left|\hat{\alpha}\_{0}^{i}\right|^{2-d}\right] | ‚â§ùîº‚Äã[|Œ±01|‚àíd+(rn)‚àíd+|Œ±01|2‚àíd+(rn)2‚àíd].‚àé\displaystyle\leq\mathbb{E}\left[\left|\alpha\_{0}^{1}\right|^{-d}+\left(r\_{n}\right)^{-d}+\left|\alpha\_{0}^{1}\right|^{2-d}+\left(r\_{n}\right)^{2-d}\right].\qed |  |

###### Proof of [Theorem¬†4.1](https://arxiv.org/html/2512.25017v1#S4.Thmtheorem1 "Theorem 4.1. ‚Ä£ 4.1. Convergence of the trained neural network ‚Ä£ 4. Convergence of the training error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs").

Using ([4.1](https://arxiv.org/html/2512.25017v1#S4.Ex7 "4.1. Convergence of the trained neural network ‚Ä£ 4. Convergence of the training error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")) and ([4.4](https://arxiv.org/html/2512.25017v1#S4.E4 "Equation 4.4 ‚Ä£ 4.1. Convergence of the trained neural network ‚Ä£ 4. Convergence of the training error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")), we need to estimate the following difference:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Vtn‚Äã(x)‚àíVt‚Äã(x)\displaystyle V^{n}\_{t}(x)-V\_{t}(x) | =V0n‚Äã(x)‚àíV0‚Äã(x)+‚à´0t{‚ü®ùíü‚ÄãIk‚Äã(Vs),Z‚Äã(x,‚ãÖ)‚ü©‚Ñã01‚àí‚ü®ùíü‚ÄãIk‚Äã(Vsn),Zsn‚Äã(x,‚ãÖ)‚ü©‚Ñã01}‚Äãds\displaystyle=V^{n}\_{0}(x)-V\_{0}(x)+\int\_{0}^{t}\Big\{\left\langle\mathcal{D}I^{k}\left(V\_{s}\right),Z(x,\cdot)\right\rangle\_{\mathcal{H}\_{0}^{1}}-\left\langle\mathcal{D}I^{k}\left(V^{n}\_{s}\right),Z\_{s}^{n}\left(x,\cdot\right)\right\rangle\_{\mathcal{H}\_{0}^{1}}\Big\}\,\mathrm{d}s |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =V0n‚Äã(x)‚àíV0‚Äã(x)‚èü(a‚Äã1)+‚à´0t{‚ü®ùíü‚ÄãIk‚Äã(Vs)‚àíùíü‚ÄãIk‚Äã(Vsn),Z‚Äã(x,‚ãÖ)‚ü©‚Ñã01‚èü(a‚Äã2)\displaystyle=\underbrace{V^{n}\_{0}(x)-V\_{0}(x)}\_{\left(a1\right)}+\int\_{0}^{t}\Big\{\underbrace{\left\langle\mathcal{D}I^{k}\left(V\_{s}\right)-\mathcal{D}I^{k}\left(V^{n}\_{s}\right),Z(x,\cdot)\right\rangle\_{\mathcal{H}\_{0}^{1}}}\_{\left(a2\right)} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +‚ü®ùíü‚ÄãIk‚Äã(Vsn),Z‚Äã(x,‚ãÖ)‚àíZ0n‚Äã(x,‚ãÖ)‚ü©‚Ñã01‚èü(a‚Äã3)+‚ü®ùíü‚ÄãIk‚Äã(Vsn),Z0n‚Äã(x,‚ãÖ)‚àíZsn‚Äã(x,‚ãÖ)‚ü©‚Ñã01‚èü(a‚Äã4)}ds.\displaystyle\qquad+\underbrace{\left\langle\mathcal{D}I^{k}\left(V^{n}\_{s}\right),Z(x,\cdot)-Z\_{0}^{n}\left(x,\cdot\right)\right\rangle\_{\mathcal{H}\_{0}^{1}}}\_{\left(a3\right)}+\underbrace{\left\langle\mathcal{D}I^{k}\left(V^{n}\_{s}\right),Z^{n}\_{0}\left(x,\cdot\right)-Z\_{s}^{n}\left(x,\cdot\right)\right\rangle\_{\mathcal{H}\_{0}^{1}}}\_{\left(a4\right)}\Big\}\,\mathrm{d}s. |  |

The proof is now separated in several steps, corresponding to the estimation of each of the terms above.

Step (a1).
We know that V0‚Äã(x)=0V\_{0}(x)=0 by definition, hence, using [Lemma¬†4.2](https://arxiv.org/html/2512.25017v1#S4.Thmtheorem2 "Lemma 4.2. ‚Ä£ 4.1. Convergence of the trained neural network ‚Ä£ 4. Convergence of the training error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"), we get

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Äã[‚ÄñV0n‚Äã(x)‚àíV0‚Äã(x)‚Äñ‚Ñã01]‚â§C(1)‚Äãn12‚àíŒ¥.\mathbb{E}\left[\left\|V^{n}\_{0}(x)-V\_{0}(x)\right\|\_{\mathcal{H}\_{0}^{1}}\right]\leq C^{\left(1\right)}n^{\frac{1}{2}-\delta}. |  |

Step (a2).
In order to separate the random terms VV and XX, we define another probability space Œ©‚Ä≤\Omega^{\prime} and probability measure ‚Ñô‚Ä≤\mathbb{P}^{\prime} such that X‚Äã(œâ‚Ä≤)X\left(\omega^{\prime}\right) under ‚Ñô‚Ä≤\mathbb{P}^{\prime} has the same distribution as XX under ‚Ñô.\mathbb{P}.
Then, we can rewrite (a‚Äã2)(a2) as follows

|  |  |  |  |
| --- | --- | --- | --- |
|  | (a‚Äã2)\displaystyle(a2) | =‚ü®ùíü‚ÄãIk‚Äã(Vs)‚àíùíü‚ÄãIk‚Äã(Vsn),ùîº‚Äã[X‚ãÖX‚Äã(x)]‚ü©‚Ñã01\displaystyle=\left\langle\mathcal{D}I^{k}\left(V\_{s}\right)-\mathcal{D}I^{k}\left(V^{n}\_{s}\right),\mathbb{E}\left[X\cdot X\left(x\right)\right]\right\rangle\_{\mathcal{H}\_{0}^{1}} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =‚à´Œ©‚Ä≤‚ü®ùíü‚ÄãIk‚Äã(Vs)‚àíùíü‚ÄãIk‚Äã(Vsn),X‚Äã(œâ‚Ä≤)‚ü©‚Ñã01‚ÄãX‚Äã(x)‚Äã(œâ‚Ä≤)‚Äã‚Ñô‚Ä≤‚Äã(d‚Äãœâ‚Ä≤).\displaystyle=\int\_{\Omega^{\prime}}\left\langle\mathcal{D}I^{k}\left(V\_{s}\right)-\mathcal{D}I^{k}\left(V^{n}\_{s}\right),X\left(\omega^{\prime}\right)\right\rangle\_{\mathcal{H}\_{0}^{1}}X\left(x\right)\left(\omega^{\prime}\right)\mathbb{P}^{\prime}\left(\mathrm{d}\omega^{\prime}\right). |  |

Hence, we can bound the ‚Ñã01\mathcal{H}\_{0}^{1}-norm of (a2) by

|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚Äñ(a‚Äã2)‚Äñ‚Ñã01\displaystyle\left\|(a2)\right\|\_{\mathcal{H}\_{0}^{1}} | ‚â§‚à´Œ©‚Ä≤‚Äñ‚ü®ùíü‚ÄãIk‚Äã(Vs)‚àíùíü‚ÄãIk‚Äã(Vsn),X‚Äã(œâ‚Ä≤)‚ü©‚Ñã01‚ãÖX‚Äã(x)‚Äã(œâ‚Ä≤)‚Äñ‚Ñã01‚Äã‚Ñô‚Äã(d‚Äãœâ‚Ä≤)\displaystyle\leq\int\_{\Omega^{\prime}}\left\|\left\langle\mathcal{D}I^{k}\left(V\_{s}\right)-\mathcal{D}I^{k}\left(V^{n}\_{s}\right),X\left(\omega^{\prime}\right)\right\rangle\_{\mathcal{H}\_{0}^{1}}\cdot X\left(x\right)\left(\omega^{\prime}\right)\right\|\_{\mathcal{H}\_{0}^{1}}\mathbb{P}\left(\mathrm{d}\omega^{\prime}\right) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =‚à´Œ©‚Ä≤|‚ü®ùíü‚ÄãIk‚Äã(Vs)‚àíùíü‚ÄãIk‚Äã(Vsn),X‚Äã(œâ‚Ä≤)‚ü©‚Ñã01|‚Äã‚ÄñX‚Äã(œâ‚Ä≤)‚Äñ‚Ñã01‚Äã‚Ñô‚Äã(d‚Äãœâ‚Ä≤)\displaystyle=\int\_{\Omega^{\prime}}\left|\left\langle\mathcal{D}I^{k}\left(V\_{s}\right)-\mathcal{D}I^{k}\left(V^{n}\_{s}\right),X\left(\omega^{\prime}\right)\right\rangle\_{\mathcal{H}\_{0}^{1}}\right|\left\|X\left(\omega^{\prime}\right)\right\|\_{\mathcal{H}\_{0}^{1}}\mathbb{P}\left(\mathrm{d}\omega^{\prime}\right) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§[Lemma¬†A.1](https://arxiv.org/html/2512.25017v1#A1.Thmtheorem1 "Lemma A.1 (Continuity of the Fr√©chet derivative). ‚Ä£ A.1. Functional inequalities and norm estimates ‚Ä£ Appendix A Auxiliary results ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")‚ÄãK‚Äã‚ÄñVs‚àíVsn‚Äñ‚Ñã01‚Äã‚à´Œ©‚Ä≤‚ÄñX‚Äã(œâ‚Ä≤)‚Äñ‚Ñã012‚Äã‚Ñô‚Äã(d‚Äãœâ‚Ä≤)\displaystyle\hskip-13.50008pt\overset{\text{\lx@cref{creftype~refnum}{lem:con\_frechet}}}{\leq}K\left\|V\_{s}-V\_{s}^{n}\right\|\_{\mathcal{H}\_{0}^{1}}\int\_{\Omega^{\prime}}\left\|X\left(\omega^{\prime}\right)\right\|\_{\mathcal{H}\_{0}^{1}}^{2}\mathbb{P}\left(\mathrm{d}\omega^{\prime}\right) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =[Lemma¬†A.4](https://arxiv.org/html/2512.25017v1#A1.Thmtheorem4 "Lemma A.4 (‚Ñã‚ÇÄ¬π-boundedness of ùëã). ‚Ä£ A.2. Gradient ùúÉ estimates ‚Ä£ Appendix A Auxiliary results ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")‚ÄãK‚Äã‚ÄñVs‚àíVsn‚Äñ‚Ñã01‚Äãùîº‚Äã[‚ÄñX‚Äñ‚Ñã012]\displaystyle\hskip-13.50008pt\overset{\text{\lx@cref{creftype~refnum}{lem:bound\_XH}}}{=}K\left\|V\_{s}-V\_{s}^{n}\right\|\_{\mathcal{H}\_{0}^{1}}\mathbb{E}\left[\left\|X\right\|\_{\mathcal{H}\_{0}^{1}}^{2}\right] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§Cœà‚Äã‚ÄñVs‚àíVsn‚Äñ‚Ñã01.\displaystyle\leq C\_{\psi}\left\|V\_{s}-V\_{s}^{n}\right\|\_{\mathcal{H}\_{0}^{1}}. |  |

Here, KK is the constant from [Lemma¬†A.1](https://arxiv.org/html/2512.25017v1#A1.Thmtheorem1 "Lemma A.1 (Continuity of the Fr√©chet derivative). ‚Ä£ A.1. Functional inequalities and norm estimates ‚Ä£ Appendix A Auxiliary results ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs") and CœàC\_{\psi} is another constant that depends on the activation function œà\psi and may change from line to line.

Step (a3).
Let us rewrite the term (a‚Äã3)\left(a3\right) as follows:

|  |  |  |  |
| --- | --- | --- | --- |
|  | (a‚Äã3)\displaystyle\left(a3\right) | =1n‚Äã‚àëi=1n‚ü®ùíü‚ÄãIk‚Äã(Vsn),X0i,n‚ãÖX0i,n‚Äã(x)‚àíùîº‚Äã[X0i,n‚ãÖX0i,n‚Äã(x)]‚ü©‚Ñã01‚èü(a‚Äã3.1)\displaystyle=\underbrace{\frac{1}{n}\sum\_{i=1}^{n}\left\langle\mathcal{D}I^{k}\left(V^{n}\_{s}\right),X^{i,n}\_{0}\cdot X^{i,n}\_{0}\left(x\right)-\mathbb{E}\left[X^{i,n}\_{0}\cdot X^{i,n}\_{0}\left(x\right)\right]\right\rangle\_{\mathcal{H}\_{0}^{1}}}\_{\left(a3.1\right)} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +‚ü®ùíü‚ÄãIk‚Äã(Vsn),ùîº‚Äã[Xn‚ãÖXn‚Äã(x)‚àíX‚ãÖX‚Äã(x)]‚ü©‚Ñã01‚èü(a‚Äã3.2),\displaystyle\quad+\underbrace{\left\langle\mathcal{D}I^{k}\left(V^{n}\_{s}\right),\mathbb{E}\left[X^{n}\cdot X^{n}\left(x\right)-X\cdot X\left(x\right)\right]\right\rangle\_{\mathcal{H}\_{0}^{1}}}\_{\left(a3.2\right)}, |  |

where we can use XnX^{n} instead of X0i,nX^{i,n}\_{0} in the second term since, by [Assumption¬†(NNI)](https://arxiv.org/html/2512.25017v1#Thmassumption5 "Assumption (NNI). ‚Ä£ 4.1. Convergence of the trained neural network ‚Ä£ 4. Convergence of the training error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"), these two terms have the same expectation.
Then, we can further separate the term (a‚Äã3.2)\left(a3.2\right) as follows:

|  |  |  |
| --- | --- | --- |
|  | (a‚Äã3.2)=‚ü®ùíü‚ÄãIk‚Äã(Vsn),ùîº‚Äã[(Xn‚àíX)‚ãÖXn‚Äã(x)]‚ü©‚Ñã01‚èü(a‚Äã3.21)+‚ü®ùíü‚ÄãIk‚Äã(Vsn),ùîº‚Äã[X‚ãÖ(Xn‚Äã(x)‚àíX‚Äã(x))]‚ü©‚Ñã01‚èü(a‚Äã3.22).\displaystyle\left(a3.2\right)=\underbrace{\left\langle\mathcal{D}I^{k}\left(V^{n}\_{s}\right),\mathbb{E}\left[\left(X^{n}-X\right)\cdot X^{n}\left(x\right)\right]\right\rangle\_{\mathcal{H}\_{0}^{1}}}\_{\left(a3.21\right)}+\underbrace{\left\langle\mathcal{D}I^{k}\left(V^{n}\_{s}\right),\mathbb{E}\left[X\cdot\left(X^{n}\left(x\right)-X\left(x\right)\right)\right]\right\rangle\_{\mathcal{H}\_{0}^{1}}}\_{\left(a3.22\right)}. |  |

A similar computation as for the term (a‚Äã2)(a2), yields

|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚Äñ(a‚Äã3.21)‚Äñ‚Ñã01\displaystyle\left\|\left(a3.21\right)\right\|\_{\mathcal{H}\_{0}^{1}} | ‚â§‚à´Œ©‚Ä≤|‚ü®ùíü‚ÄãIk‚Äã(Vsn),Xn‚Äã(œâ‚Ä≤)‚àíX‚Äã(œâ‚Ä≤)‚ü©‚Ñã01|‚Äã‚ÄñXn‚Äã(œâ‚Ä≤)‚Äñ‚Ñã01‚Äã‚Ñô‚Äã(d‚Äãœâ‚Ä≤)\displaystyle\leq\int\_{\Omega^{\prime}}\left|\left\langle\mathcal{D}I^{k}\left(V^{n}\_{s}\right),X^{n}\left(\omega^{\prime}\right)-X\left(\omega^{\prime}\right)\right\rangle\_{\mathcal{H}\_{0}^{1}}\right|\left\|X^{n}\left(\omega^{\prime}\right)\right\|\_{\mathcal{H}\_{0}^{1}}\mathbb{P}\left(\mathrm{d}\omega^{\prime}\right) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§[Lemma¬†A.1](https://arxiv.org/html/2512.25017v1#A1.Thmtheorem1 "Lemma A.1 (Continuity of the Fr√©chet derivative). ‚Ä£ A.1. Functional inequalities and norm estimates ‚Ä£ Appendix A Auxiliary results ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")‚ÄãK‚Äã(1+‚ÄñVsn‚Äñ‚Ñã01)‚Äã‚à´Œ©‚Ä≤‚ÄñXn‚Äã(œâ‚Ä≤)‚àíX‚Äã(œâ‚Ä≤)‚Äñ‚Ñã01‚Äã‚ÄñXn‚Äã(œâ‚Ä≤)‚Äñ‚Ñã01‚Äã‚Ñô‚Äã(d‚Äãœâ‚Ä≤)\displaystyle\hskip-13.50008pt\overset{\text{\lx@cref{creftype~refnum}{lem:con\_frechet}}}{\leq}K\left(1+\left\|V\_{s}^{n}\right\|\_{\mathcal{H}\_{0}^{1}}\right)\int\_{\Omega^{\prime}}\left\|X^{n}\left(\omega^{\prime}\right)-X\left(\omega^{\prime}\right)\right\|\_{\mathcal{H}\_{0}^{1}}\left\|X^{n}\left(\omega^{\prime}\right)\right\|\_{\mathcal{H}\_{0}^{1}}\mathbb{P}\left(\mathrm{d}\omega^{\prime}\right) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =K‚Äã(1+‚ÄñVsn‚Äñ‚Ñã01)‚Äãùîº‚Äã[‚ÄñXn‚àíX‚Äñ‚Ñã01‚Äã‚ÄñXn‚Äñ‚Ñã01].\displaystyle=K\left(1+\left\|V\_{s}^{n}\right\|\_{\mathcal{H}\_{0}^{1}}\right)\mathbb{E}\left[\left\|X^{n}-X\right\|\_{\mathcal{H}\_{0}^{1}}\left\|X^{n}\right\|\_{\mathcal{H}\_{0}^{1}}\right]. |  |

Analogously, we have that

|  |  |  |
| --- | --- | --- |
|  | ‚Äñ(a‚Äã3.22)‚Äñ‚Ñã01‚â§K‚Äã(1+‚ÄñVsn‚Äñ‚Ñã01)‚Äãùîº‚Äã[‚ÄñXn‚àíX‚Äñ‚Ñã01‚Äã‚ÄñX‚Äñ‚Ñã01].\left\|\left(a3.22\right)\right\|\_{\mathcal{H}\_{0}^{1}}\leq K\left(1+\left\|V\_{s}^{n}\right\|\_{\mathcal{H}\_{0}^{1}}\right)\mathbb{E}\left[\left\|X^{n}-X\right\|\_{\mathcal{H}\_{0}^{1}}\left\|X\right\|\_{\mathcal{H}\_{0}^{1}}\right]. |  |

Overall, combining the two bounds and then using [Lemma¬†A.2](https://arxiv.org/html/2512.25017v1#A1.Thmtheorem2 "Lemma A.2. ‚Ä£ A.1. Functional inequalities and norm estimates ‚Ä£ Appendix A Auxiliary results ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"), then the Cauchy‚ÄìSchwarz inequality, and finally [Lemmas¬†A.4](https://arxiv.org/html/2512.25017v1#A1.Thmtheorem4 "Lemma A.4 (‚Ñã‚ÇÄ¬π-boundedness of ùëã). ‚Ä£ A.2. Gradient ùúÉ estimates ‚Ä£ Appendix A Auxiliary results ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs") and¬†[A.5](https://arxiv.org/html/2512.25017v1#A1.Thmtheorem5 "Lemma A.5. ‚Ä£ A.2. Gradient ùúÉ estimates ‚Ä£ Appendix A Auxiliary results ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"), we arrive at

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùîº‚Äã[‚Äñ(a‚Äã3.2)‚Äñ‚Ñã01]\displaystyle\mathbb{E}\left[\left\|\left(a3.2\right)\right\|\_{\mathcal{H}\_{0}^{1}}\right] | ‚â§2‚ÄãK‚Äãùîº‚Äã[1+‚ÄñVsn‚Äñ‚Ñã01]‚Äãùîº‚Äã[‚ÄñXn‚àíX‚Äñ‚Ñã01‚Äã(‚ÄñXn‚Äñ‚Ñã01+‚ÄñX‚Äñ‚Ñã01)]\displaystyle\leq 2K\mathbb{E}\left[1+\left\|V\_{s}^{n}\right\|\_{\mathcal{H}\_{0}^{1}}\right]\mathbb{E}\left[\left\|X^{n}-X\right\|\_{\mathcal{H}\_{0}^{1}}\left(\left\|X^{n}\right\|\_{\mathcal{H}\_{0}^{1}}+\left\|X\right\|\_{\mathcal{H}\_{0}^{1}}\right)\right] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§Cœà‚Äãùîº‚Äã[‚ÄñXn‚àíX‚Äñ‚Ñã01‚Äã(‚ÄñXn‚Äñ‚Ñã01+‚ÄñX‚Äñ‚Ñã01)]\displaystyle\leq C\_{\psi}\mathbb{E}\left[\left\|X^{n}-X\right\|\_{\mathcal{H}\_{0}^{1}}\left(\left\|X^{n}\right\|\_{\mathcal{H}\_{0}^{1}}+\left\|X\right\|\_{\mathcal{H}\_{0}^{1}}\right)\right] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§Cœà‚Äãùîº‚Äã[‚ÄñXn‚àíX‚Äñ‚Ñã012]12‚Äãùîº‚Äã[‚ÄñXn‚Äñ‚Ñã012+‚ÄñX‚Äñ‚Ñã012]12‚â§Cœà‚ÄãŒµn12,\displaystyle\leq C\_{\psi}\mathbb{E}\left[\left\|X^{n}-X\right\|\_{\mathcal{H}\_{0}^{1}}^{2}\right]^{\frac{1}{2}}\mathbb{E}\left[\left\|X^{n}\right\|^{2}\_{\mathcal{H}\_{0}^{1}}+\left\|X\right\|^{2}\_{\mathcal{H}\_{0}^{1}}\right]^{\frac{1}{2}}\leq C\_{\psi}\,{\varepsilon}\_{n}^{\frac{1}{2}}, |  |

where Œµn{\varepsilon}\_{n} is defined in [Lemma¬†A.5](https://arxiv.org/html/2512.25017v1#A1.Thmtheorem5 "Lemma A.5. ‚Ä£ A.2. Gradient ùúÉ estimates ‚Ä£ Appendix A Auxiliary results ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs") and equals

|  |  |  |
| --- | --- | --- |
|  | Œµn=ùîº‚Äã[‚ÄñXn‚àíX‚Äñ‚Ñã012].{\varepsilon}\_{n}=\mathbb{E}\left[\left\|X^{n}-X\right\|\_{\mathcal{H}\_{0}^{1}}^{2}\right]. |  |

On the other hand, using [Lemma¬†A.1](https://arxiv.org/html/2512.25017v1#A1.Thmtheorem1 "Lemma A.1 (Continuity of the Fr√©chet derivative). ‚Ä£ A.1. Functional inequalities and norm estimates ‚Ä£ Appendix A Auxiliary results ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs") the norm of (a‚Äã3.1)\left(a3.1\right) can be bounded by

|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚Äñ(a‚Äã3.1)‚ÄñL2\displaystyle\left\|\left(a3.1\right)\right\|\_{L^{2}} | =1n‚Äã(‚à´‚Ñùd‚ü®ùíü‚ÄãIk‚Äã(Vsn),‚àëi=1n(X0i,n‚ãÖX0i,n‚Äã(x)‚àíùîº‚Äã[X0i,n‚ãÖX0i,n‚Äã(x)])‚ü©‚Ñã012‚Äãdx)12\displaystyle=\frac{1}{n}\left(\int\_{\mathbb{R}^{d}}\left\langle\mathcal{D}I^{k}\left(V^{n}\_{s}\right),\sum\_{i=1}^{n}\left(X^{i,n}\_{0}\cdot X^{i,n}\_{0}\left(x\right)-\mathbb{E}\left[X^{i,n}\_{0}\cdot X^{i,n}\_{0}\left(x\right)\right]\right)\right\rangle^{2}\_{\mathcal{H}\_{0}^{1}}\mathrm{d}x\right)^{\frac{1}{2}} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§Kn‚Äã(1+‚ÄñVsn‚Äñ‚Ñã01)‚Äã(‚à´‚Ñùd‚Äñ‚àëi=1n(X0i,n‚ãÖX0i,n‚Äã(x)‚àíùîº‚Äã[X0i,n‚ãÖX0i,n‚Äã(x)])‚Äñ‚Ñã012‚Äãdx)12.\displaystyle\leq\frac{K}{n}\left(1+\left\|V^{n}\_{s}\right\|\_{\mathcal{H}\_{0}^{1}}\right)\left(\int\_{\mathbb{R}^{d}}\left\|\sum\_{i=1}^{n}\left(X^{i,n}\_{0}\cdot X^{i,n}\_{0}\left(x\right)-\mathbb{E}\left[X^{i,n}\_{0}\cdot X^{i,n}\_{0}\left(x\right)\right]\right)\right\|^{2}\_{\mathcal{H}\_{0}^{1}}\mathrm{d}x\right)^{\frac{1}{2}}. |  |

Then, using the Cauchy‚ÄìSchwarz inequality and [Lemma¬†A.2](https://arxiv.org/html/2512.25017v1#A1.Thmtheorem2 "Lemma A.2. ‚Ä£ A.1. Functional inequalities and norm estimates ‚Ä£ Appendix A Auxiliary results ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"), we get that

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùîº‚Äã[‚Äñ(a‚Äã3.1)‚ÄñL2]\displaystyle\mathbb{E}\left[\left\|\left(a3.1\right)\right\|\_{L^{2}}\right] | ‚â§Kn‚Äãùîº‚Äã[(1+‚ÄñVsn‚Äñ‚Ñã01)2]12‚Äãùîº‚Äã[‚à´‚Ñùd‚Äñ‚àëi=1n(X0i,n‚ãÖX0i,n‚Äã(x)‚àíùîº‚Äã[X0i,n‚ãÖX0i,n‚Äã(x)])‚Äñ‚Ñã012‚Äãdx]12\displaystyle\leq\frac{K}{n}\mathbb{E}\left[\left(1+\left\|V^{n}\_{s}\right\|\_{\mathcal{H}\_{0}^{1}}\right)^{2}\right]^{\frac{1}{2}}\mathbb{E}\left[\int\_{\mathbb{R}^{d}}\left\|\sum\_{i=1}^{n}\left(X^{i,n}\_{0}\cdot X^{i,n}\_{0}\left(x\right)-\mathbb{E}\left[X^{i,n}\_{0}\cdot X^{i,n}\_{0}\left(x\right)\right]\right)\right\|^{2}\_{\mathcal{H}\_{0}^{1}}\mathrm{d}x\right]^{\frac{1}{2}} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§Cœàn‚Äãùîº‚Äã[‚à´‚Ñùd‚Äñ‚àëi=1n(X0i,n‚ãÖX0i,n‚Äã(x)‚àíùîº‚Äã[X0i,n‚ãÖX0i,n‚Äã(x)])‚Äñ‚Ñã012‚Äãdx]12\displaystyle\leq\frac{C\_{\psi}}{n}\mathbb{E}\left[\int\_{\mathbb{R}^{d}}\left\|\sum\_{i=1}^{n}\left(X^{i,n}\_{0}\cdot X^{i,n}\_{0}\left(x\right)-\mathbb{E}\left[X^{i,n}\_{0}\cdot X^{i,n}\_{0}\left(x\right)\right]\right)\right\|^{2}\_{\mathcal{H}\_{0}^{1}}\mathrm{d}x\right]^{\frac{1}{2}} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =Cœàn‚Äãùîº‚Äã[‚à´‚Ñùd‚ÄñXn‚ãÖXn‚Äã(x)‚àíùîº‚Äã[Xn‚ãÖXn‚Äã(x)]‚Äñ‚Ñã012‚Äãdx]12,\displaystyle=\frac{C\_{\psi}}{\sqrt{n}}\mathbb{E}\left[\int\_{\mathbb{R}^{d}}\left\|X^{n}\cdot X^{n}\left(x\right)-\mathbb{E}\left[X^{n}\cdot X^{n}\left(x\right)\right]\right\|^{2}\_{\mathcal{H}\_{0}^{1}}\mathrm{d}x\right]^{\frac{1}{2}}, |  |

where the last equality follows because XnX^{n} and X0i,nX^{i,n}\_{0} are equally distributed by [Assumption¬†(NNI)](https://arxiv.org/html/2512.25017v1#Thmassumption5 "Assumption (NNI). ‚Ä£ 4.1. Convergence of the trained neural network ‚Ä£ 4. Convergence of the training error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"), while Xi,n‚ãÖXi,n‚Äã(x)X^{i,n}\cdot X^{i,n}\left(x\right) are i.i.d. variables that satisfy

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Äã[‚ü®Xi,n‚ãÖXi,n‚Äã(x),Xj,n‚ãÖXj,n‚Äã(x)‚ü©‚Ñã01]=0,for¬†‚Äãi‚â†j.\mathbb{E}\left[\left\langle X^{i,n}\cdot X^{i,n}\left(x\right),X^{j,n}\cdot X^{j,n}\left(x\right)\right\rangle\_{\mathcal{H}\_{0}^{1}}\right]=0,\quad\text{for }i\neq j. |  |

Using the triangle inequality, we have

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Äã[‚à´‚Ñùd‚ÄñXn‚ãÖXn‚Äã(x)‚àíùîº‚Äã[Xn‚ãÖXn‚Äã(x)]‚Äñ‚Ñã012‚Äãdx]\displaystyle\mathbb{E}\left[\int\_{\mathbb{R}^{d}}\left\|X^{n}\cdot X^{n}\left(x\right)-\mathbb{E}\left[X^{n}\cdot X^{n}\left(x\right)\right]\right\|^{2}\_{\mathcal{H}\_{0}^{1}}\mathrm{d}x\right] |  |
|  |  |  |
| --- | --- | --- |
|  | ‚â§2‚Äãùîº‚Äã[‚à´‚Ñùd‚ÄñXn‚ãÖXn‚Äã(x)‚Äñ‚Ñã012‚Äãdx]+2‚Äã‚à´‚Ñùd‚Äñùîº‚Äã[Xn‚ãÖXn‚Äã(x)]‚Äñ‚Ñã012‚Äãdx\displaystyle\qquad\qquad\leq 2\mathbb{E}\left[\int\_{\mathbb{R}^{d}}\left\|X^{n}\cdot X^{n}\left(x\right)\right\|^{2}\_{\mathcal{H}\_{0}^{1}}\mathrm{d}x\right]+2\int\_{\mathbb{R}^{d}}\left\|\mathbb{E}\left[X^{n}\cdot X^{n}\left(x\right)\right]\right\|^{2}\_{\mathcal{H}\_{0}^{1}}\mathrm{d}x |  |
|  |  |  |
| --- | --- | --- |
|  | ‚â§2‚Äãùîº‚Äã[‚ÄñXn‚Äñ‚Ñã012‚Äã‚ÄñXn‚ÄñL22]+2‚Äãùîº‚Äã[‚ÄñXn‚Äñ‚Ñã012]‚Äãùîº‚Äã[‚ÄñXn‚ÄñL22].\displaystyle\qquad\qquad\leq 2\mathbb{E}\left[\left\|X^{n}\right\|^{2}\_{\mathcal{H}\_{0}^{1}}\left\|X^{n}\right\|^{2}\_{L^{2}}\right]+2\mathbb{E}\left[\left\|X^{n}\right\|^{2}\_{\mathcal{H}\_{0}^{1}}\right]\mathbb{E}\left[\left\|X^{n}\right\|\_{L^{2}}^{2}\right]. |  |

Then, combining the last two inequalities, we arrive at

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Äã[‚Äñ(a‚Äã3.1)‚ÄñL2]‚â§Cœàn‚Äã(ùîº‚Äã[‚ÄñXn‚Äñ‚Ñã012‚Äã‚ÄñXn‚ÄñL22]12+ùîº‚Äã[‚ÄñXn‚Äñ‚Ñã012]12‚Äãùîº‚Äã[‚ÄñXn‚ÄñL22]12).\displaystyle\mathbb{E}\left[\left\|\left(a3.1\right)\right\|\_{L^{2}}\right]\leq\frac{C\_{\psi}}{\sqrt{n}}\left(\mathbb{E}\left[\left\|X^{n}\right\|^{2}\_{\mathcal{H}\_{0}^{1}}\left\|X^{n}\right\|^{2}\_{L^{2}}\right]^{\frac{1}{2}}+\mathbb{E}\left[\left\|X^{n}\right\|^{2}\_{\mathcal{H}\_{0}^{1}}\right]^{\frac{1}{2}}\mathbb{E}\left[\left\|X^{n}\right\|^{2}\_{L^{2}}\right]^{\frac{1}{2}}\right). |  |

We can analogously estimate the term ùîº‚Äã[‚Äñ‚àáx(a‚Äã3.1)‚ÄñL2]\mathbb{E}\left[\left\|\nabla\_{x}\left(a3.1\right)\right\|\_{L^{2}}\right] and deduce that

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùîº‚Äã[‚Äñ(a‚Äã3.1)‚Äñ‚Ñã01]\displaystyle\mathbb{E}\left[\left\|\left(a3.1\right)\right\|\_{\mathcal{H}\_{0}^{1}}\right] | ‚â§Cœàn‚Äã(ùîº‚Äã[‚ÄñXn‚Äñ‚Ñã014]12+ùîº‚Äã[‚ÄñXn‚Äñ‚Ñã012])‚â§Cœàn‚Äãùîº‚Äã[‚ÄñXn‚Äñ‚Ñã014]12\displaystyle\leq\frac{C\_{\psi}}{\sqrt{n}}\left(\mathbb{E}\left[\left\|X^{n}\right\|^{4}\_{\mathcal{H}\_{0}^{1}}\right]^{\frac{1}{2}}+\mathbb{E}\left[\left\|X^{n}\right\|^{2}\_{\mathcal{H}\_{0}^{1}}\right]\right)\leq\frac{C\_{\psi}}{\sqrt{n}}\mathbb{E}\left[\left\|X^{n}\right\|^{4}\_{\mathcal{H}\_{0}^{1}}\right]^{\frac{1}{2}} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§[Lemma¬†A.3](https://arxiv.org/html/2512.25017v1#A1.Thmtheorem3 "Lemma A.3 (‚Ñã‚ÇÄ¬π-boundedness of ùëã^ùëõ). ‚Ä£ A.2. Gradient ùúÉ estimates ‚Ä£ Appendix A Auxiliary results ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")‚ÄãCœà‚Äã(rn)8+dn.\displaystyle\hskip-13.50008pt\overset{\text{\lx@cref{creftype~refnum}{lem:boundXN}}}{\leq}\frac{C\_{\psi}\left(r\_{n}\right)^{8+d}}{\sqrt{n}}. |  |

Overall, we finish this step by concluding that

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Äã[‚Äñ(a‚Äã3)‚Äñ‚Ñã01]‚â§Cœà‚Äã((rn)8+dn+Œµn12).\mathbb{E}\left[\left\|\left(a3\right)\right\|\_{\mathcal{H}\_{0}^{1}}\right]\leq C\_{\psi}\left(\frac{\left(r\_{n}\right)^{8+d}}{\sqrt{n}}+{\varepsilon}\_{n}^{\frac{1}{2}}\right). |  |

Step (a4).
Recalling the definition of Ztn,Z\_{t}^{n}, we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | (a‚Äã4)\displaystyle\left(a4\right) | =1n‚Äã‚àëi=1n‚ü®ùíü‚ÄãIk‚Äã(Vsn),X0i,n‚ü©‚Ñã01‚ãÖX0i,n‚Äã(x)‚àí1n‚Äã‚àëi=1n‚ü®ùíü‚ÄãIk‚Äã(Vsn),Xsi,n‚ü©‚Ñã01‚ãÖXsi,n‚Äã(x)\displaystyle=\frac{1}{n}\sum\_{i=1}^{n}\left\langle\mathcal{D}I^{k}\left(V^{n}\_{s}\right),X^{i,n}\_{0}\right\rangle\_{\mathcal{H}\_{0}^{1}}\cdot X^{i,n}\_{0}(x)-\frac{1}{n}\sum\_{i=1}^{n}\left\langle\mathcal{D}I^{k}\left(V^{n}\_{s}\right),X\_{s}^{i,n}\right\rangle\_{\mathcal{H}\_{0}^{1}}\cdot X\_{s}^{i,n}(x) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =1n‚Äã‚àëi=1n‚ü®ùíü‚ÄãIk‚Äã(Vsn),X0i,n‚àíXsi,n‚ü©‚Ñã01‚ãÖX0i,n‚Äã(x)\displaystyle=\frac{1}{n}\sum\_{i=1}^{n}\left\langle\mathcal{D}I^{k}\left(V^{n}\_{s}\right),X^{i,n}\_{0}-X\_{s}^{i,n}\right\rangle\_{\mathcal{H}\_{0}^{1}}\cdot X^{i,n}\_{0}\left(x\right) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚àí1n‚Äã‚àëi=1n‚ü®ùíü‚ÄãIk‚Äã(Vsn),Xsi,n‚ü©‚Ñã01‚ãÖ(Xsi,n‚Äã(x)‚àíX0i,n‚Äã(x)).\displaystyle\qquad-\frac{1}{n}\sum\_{i=1}^{n}\left\langle\mathcal{D}I^{k}\left(V^{n}\_{s}\right),X\_{s}^{i,n}\right\rangle\_{\mathcal{H}\_{0}^{1}}\cdot\left(X\_{s}^{i,n}\left(x\right)-X^{i,n}\_{0}\left(x\right)\right). |  |

Using first [Lemma¬†A.1](https://arxiv.org/html/2512.25017v1#A1.Thmtheorem1 "Lemma A.1 (Continuity of the Fr√©chet derivative). ‚Ä£ A.1. Functional inequalities and norm estimates ‚Ä£ Appendix A Auxiliary results ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs") and then [Lemmas¬†A.3](https://arxiv.org/html/2512.25017v1#A1.Thmtheorem3 "Lemma A.3 (‚Ñã‚ÇÄ¬π-boundedness of ùëã^ùëõ). ‚Ä£ A.2. Gradient ùúÉ estimates ‚Ä£ Appendix A Auxiliary results ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs") and¬†[A.6](https://arxiv.org/html/2512.25017v1#A1.Thmtheorem6 "Lemma A.6 (ùúÉ-Lipschitz continuity). ‚Ä£ A.2. Gradient ùúÉ estimates ‚Ä£ Appendix A Auxiliary results ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"), we get

|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚Äñ(a‚Äã4)‚Äñ‚Ñã01\displaystyle\left\|\left(a4\right)\right\|\_{\mathcal{H}\_{0}^{1}} | ‚â§Kn‚Äã(1+‚ÄñVsn‚Äñ‚Ñã01)‚Äã‚àëi=1n(‚ÄñXsi,n‚Äñ‚Ñã01+‚ÄñXi,n‚Äñ‚Ñã01)‚Äã‚ÄñXsi,n‚àíXi,n‚Äñ‚Ñã01\displaystyle\leq\frac{K}{n}\left(1+\left\|V\_{s}^{n}\right\|\_{\mathcal{H}\_{0}^{1}}\right)\sum\_{i=1}^{n}\left(\left\|X\_{s}^{i,n}\right\|\_{\mathcal{H}\_{0}^{1}}+\left\|X^{i,n}\right\|\_{\mathcal{H}\_{0}^{1}}\right)\left\|X\_{s}^{i,n}-X^{i,n}\right\|\_{\mathcal{H}\_{0}^{1}} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§Cœàn‚Äã(rn)8+d‚Äã(1+‚ÄñVsn‚Äñ‚Ñã01)‚Äã‚àëi=1n|Œ∏si,n‚àíŒ∏0i|12.\displaystyle\leq\frac{C\_{\psi}}{n}\left(r\_{n}\right)^{8+d}\left(1+\left\|V\_{s}^{n}\right\|\_{\mathcal{H}\_{0}^{1}}\right)\sum\_{i=1}^{n}\left|\theta\_{s}^{i,n}-\theta\_{0}^{i}\right|^{\frac{1}{2}}. |  |

Hence we can bound its expectation by

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùîº‚Äã[‚Äñ(a‚Äã4)‚Äñ‚Ñã01]\displaystyle\mathbb{E}\left[\left\|\left(a4\right)\right\|\_{\mathcal{H}\_{0}^{1}}\right] | ‚â§Cœàn‚Äã(rn)8+d‚Äãùîº‚Äã[(‚ÄñVsn‚Äñ‚Ñã01+1)2]12‚Äãùîº‚Äã[(‚àëi=1n|Œ∏si,n‚àíŒ∏0i|12)2]12\displaystyle\leq\frac{C\_{\psi}}{n}\left(r\_{n}\right)^{8+d}\mathbb{E}\left[\left(\left\|V\_{s}^{n}\right\|\_{\mathcal{H}\_{0}^{1}}+1\right)^{2}\right]^{\frac{1}{2}}\mathbb{E}\left[\left(\sum\_{i=1}^{n}\left|\theta\_{s}^{i,n}-\theta\_{0}^{i}\right|^{\frac{1}{2}}\right)^{2}\right]^{\frac{1}{2}} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§Cœà‚Äãn‚àí12‚Äã(rn)8+d‚Äãùîº‚Äã[(‚ÄñVsn‚Äñ‚Ñã01+1)2]12‚Äãùîº‚Äã[|Œ∏s1,n‚àíŒ∏01|]12\displaystyle\leq C\_{\psi}n^{-\frac{1}{2}}\left(r\_{n}\right)^{8+d}\mathbb{E}\left[\left(\left\|V\_{s}^{n}\right\|\_{\mathcal{H}\_{0}^{1}}+1\right)^{2}\right]^{\frac{1}{2}}\mathbb{E}\left[\left|\theta\_{s}^{1,n}-\theta\_{0}^{1}\right|\right]^{\frac{1}{2}} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§Lem.¬†[A.2](https://arxiv.org/html/2512.25017v1#A1.Thmtheorem2 "Lemma A.2. ‚Ä£ A.1. Functional inequalities and norm estimates ‚Ä£ Appendix A Auxiliary results ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")¬†&¬†[A.7](https://arxiv.org/html/2512.25017v1#A1.Thmtheorem7 "Lemma A.7. ‚Ä£ A.2. Gradient ùúÉ estimates ‚Ä£ Appendix A Auxiliary results ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")‚ÄãCœà‚Äãt‚ÄãnŒ¥2‚àí1‚Äã(rn)10+2‚Äãd.\displaystyle\hskip-19.49998pt\overset{\text{Lem. \ref{lem:EVtN} \& \ref{lem:thetat0}}}{\leq}C\_{\psi}\sqrt{t}n^{\frac{\delta}{2}-1}\left(r\_{n}\right)^{10+2d}. |  |

Final step.
Combining the previous steps, we arrive at

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùîº‚Äã[‚ÄñVtn‚àíVt‚Äñ‚Ñã01]\displaystyle\mathbb{E}\left[\left\|V^{n}\_{t}-V\_{t}\right\|\_{\mathcal{H}\_{0}^{1}}\right] | ‚â§ùîº‚Äã[‚Äñ(a‚Äã1)‚Äñ‚Ñã01]+ùîº‚Äã[‚Äñ‚à´0t(a‚Äã2)+(a‚Äã3)+(a‚Äã4)‚Äãd‚Äãs‚Äñ‚Ñã01]\displaystyle\leq\mathbb{E}\left[\left\|\left(a1\right)\right\|\_{\mathcal{H}\_{0}^{1}}\right]+\mathbb{E}\left[\left\|\int\_{0}^{t}(a2)+\left(a3\right)+\left(a4\right)\mathrm{d}s\right\|\_{\mathcal{H}\_{0}^{1}}\right] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§ùîº‚Äã[‚Äñ(a‚Äã1)‚Äñ‚Ñã01]+‚à´0tùîº‚Äã[‚Äñ(a‚Äã2)‚Äñ‚Ñã01]+ùîº‚Äã[‚Äñ(a‚Äã3)‚Äñ‚Ñã01]+ùîº‚Äã[‚Äñ(a‚Äã4)‚Äñ‚Ñã01]‚Äãd‚Äãs\displaystyle\leq\mathbb{E}\left[\left\|\left(a1\right)\right\|\_{\mathcal{H}\_{0}^{1}}\right]+\int\_{0}^{t}\mathbb{E}\left[\left\|(a2)\right\|\_{\mathcal{H}\_{0}^{1}}\right]+\mathbb{E}\left[\left\|\left(a3\right)\right\|\_{\mathcal{H}\_{0}^{1}}\right]+\mathbb{E}\left[\left\|\left(a4\right)\right\|\_{\mathcal{H}\_{0}^{1}}\right]\mathrm{d}s |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§M‚Äã‚à´0tùîº‚Äã[‚ÄñVsn‚àíVs‚Äñ‚Ñã01]‚Äãds\displaystyle\leq M\int\_{0}^{t}\mathbb{E}\left[\left\|V^{n}\_{s}-V\_{s}\right\|\_{\mathcal{H}\_{0}^{1}}\right]\mathrm{d}s |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +C(1)‚Äãn12‚àíŒ¥+T32‚ÄãCœà‚Äã((rn)8+dn+Œµn12+nŒ¥2‚àí1‚Äã(rn)10+2‚Äãd).\displaystyle\qquad+C^{\left(1\right)}n^{\frac{1}{2}-\delta}+T^{\frac{3}{2}}C\_{\psi}\left(\frac{\left(r\_{n}\right)^{8+d}}{\sqrt{n}}+{\varepsilon}\_{n}^{\frac{1}{2}}+n^{\frac{\delta}{2}-1}\left(r\_{n}\right)^{10+2d}\right). |  |

Hence, using Gr√∂nwall‚Äôs inequality, we conclude

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùîº‚Äã[‚ÄñVtn‚àíVt‚Äñ‚Ñã01]\displaystyle\mathbb{E}\left[\left\|V^{n}\_{t}-V\_{t}\right\|\_{\mathcal{H}\_{0}^{1}}\right] | ‚â§eM‚ÄãT‚Äã(C(1)‚Äãn12‚àíŒ¥+T32‚ÄãCœà‚Äã((rn)8+dn+Œµn12+nŒ¥2‚àí1‚Äã(rn)10+2‚Äãd))\displaystyle\leq\mathrm{e}^{MT}\left(C^{\left(1\right)}n^{\frac{1}{2}-\delta}+T^{\frac{3}{2}}C\_{\psi}\left(\frac{\left(r\_{n}\right)^{8+d}}{\sqrt{n}}+{\varepsilon}\_{n}^{\frac{1}{2}}+n^{\frac{\delta}{2}-1}\left(r\_{n}\right)^{10+2d}\right)\right) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚Üín‚Üí‚àû0.‚àé\displaystyle\xrightarrow[n\to\infty]{}0.\qed |  |

### 4.2. Long time behavior of the gradient flow

In this subsection, we prove that the wide network limit of the trained neural network, i.e. the process VtV\_{t} defined in ([4.4](https://arxiv.org/html/2512.25017v1#S4.E4 "Equation 4.4 ‚Ä£ 4.1. Convergence of the trained neural network ‚Ä£ 4. Convergence of the training error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")), converges to the global minimizer w‚àów\_{\*} of the loss function IkI^{k} of the DGFMs, as the training time t‚Üí‚àût\to\infty.
This result, combined with the convergence of the time-stepping scheme, then proves the convergence of the training error.

###### Theorem 4.3.

Assume that the neural network satisfies [Assumption¬†(NNI)](https://arxiv.org/html/2512.25017v1#Thmassumption5 "Assumption (NNI). ‚Ä£ 4.1. Convergence of the trained neural network ‚Ä£ 4. Convergence of the training error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs") and the coefficients of the PDE ([2.1](https://arxiv.org/html/2512.25017v1#S2.E1 "Equation 2.1 ‚Ä£ 2. Deep gradient flow methods for PDEs ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"))‚Äì([2.2](https://arxiv.org/html/2512.25017v1#S2.E2 "Equation 2.2 ‚Ä£ 2. Deep gradient flow methods for PDEs ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")) satisfy [Assumptions¬†(CON)](https://arxiv.org/html/2512.25017v1#Thmassumption1 "Assumption (CON). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs") and¬†[(G√Ö)](https://arxiv.org/html/2512.25017v1#Thmassumption2 "Assumption (G√Ö). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs").
Then, we have

|  |  |  |
| --- | --- | --- |
|  | limt‚Üí‚àû‚ÄñVt‚àíw‚àó‚Äñ‚Ñã01=0.\displaystyle\lim\_{t\to\infty}\left\|V\_{t}-w\_{\*}\right\|\_{\mathcal{H}\_{0}^{1}}=0. |  |

Let us start by rewriting the dynamics of the gradient flow VV in ([4.4](https://arxiv.org/html/2512.25017v1#S4.E4 "Equation 4.4 ‚Ä£ 4.1. Convergence of the trained neural network ‚Ä£ 4. Convergence of the training error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")) as follows:

|  |  |  |  |
| --- | --- | --- | --- |
|  | d‚Äã(Vt‚àíw‚àó)‚Äã(x)d‚Äãt=d‚Äã(Vt)‚Äã(x)d‚Äãt=‚àí‚ü®ùíü‚ÄãIk‚Äã(Vt‚àíw‚àó+w‚àó),Z‚Äã(x,‚ãÖ)‚ü©‚Ñã01=‚àíùíØ~‚Äã(Vt‚àíw‚àó)‚Äã(x),\displaystyle\frac{\mathrm{d}\left(V\_{t}-w\_{\*}\right)(x)}{\mathrm{d}t}=\frac{\mathrm{d}\left(V\_{t}\right)(x)}{\mathrm{d}t}=-\left\langle\mathcal{D}I^{k}\left(V\_{t}-w\_{\*}+w\_{\*}\right),Z(x,\cdot)\right\rangle\_{\mathcal{H}\_{0}^{1}}=-\widetilde{\mathcal{T}}\left(V\_{t}-w\_{\*}\right)\left(x\right), |  | (4.8) |

where

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùíØ~‚Äã(v)\displaystyle\widetilde{\mathcal{T}}\left(v\right) | :=ùíØ‚Äã(v+w‚àó)\displaystyle:=\mathcal{T}\left(v+w\_{\*}\right) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =‚ü®v+w‚àó‚àíUk‚àí1,Z‚Äã(x,‚ãÖ)‚ü©L2+h‚Äã‚ü®‚Ñí‚Äã(v+w‚àó),Z‚Äã(x,‚ãÖ)‚ü©‚Ñã‚àí1,‚Ñã01+h‚Äã‚ü®F‚Äã(Uk‚àí1),Z‚Äã(x,‚ãÖ)‚ü©L2\displaystyle=\left\langle v+w\_{\*}-U^{k-1},Z(x,\cdot)\right\rangle\_{L^{2}}+h\left\langle\mathcal{L}(v+w\_{\*}),Z(x,\cdot)\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}}+h\left\langle F\left(U^{k-1}\right),Z(x,\cdot)\right\rangle\_{L^{2}} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =‚ü®v,Z‚Äã(x,‚ãÖ)‚ü©L2+h‚Äã‚ü®‚Ñí‚Äãv,Z‚Äã(x,‚ãÖ)‚ü©‚Ñã‚àí1,‚Ñã01+‚ü®w‚àó‚àíUk‚àí1+h‚Äã(‚Ñí‚Äãw‚àó+F‚Äã(Uk‚àí1)),Z‚Äã(x,‚ãÖ)‚ü©L2\displaystyle=\left\langle v,Z(x,\cdot)\right\rangle\_{L^{2}}+h\left\langle\mathcal{L}v,Z(x,\cdot)\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}}+\left\langle w\_{\*}-U^{k-1}+h\left(\mathcal{L}w\_{\*}+F\left(U^{k-1}\right)\right),Z(x,\cdot)\right\rangle\_{L^{2}} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =‚ü®v,Z‚Äã(x,‚ãÖ)‚ü©L2+h‚Äã‚ü®‚Ñí‚Äãv,Z‚Äã(x,‚ãÖ)‚ü©‚Ñã‚àí1,‚Ñã01.\displaystyle=\left\langle v,Z(x,\cdot)\right\rangle\_{L^{2}}+h\left\langle\mathcal{L}v,Z(x,\cdot)\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}}. |  |

We work with ùíØ~\widetilde{\mathcal{T}} in the sequel, because ùíØ\mathcal{T} is not linear (ùíØ‚Äã(0)‚â†0)\left(\mathcal{T}\left(0\right)\neq 0\right).
Next, let us define another inner product, such that ùíØ~\widetilde{\mathcal{T}} becomes positive semi-definite.
Indeed, for any u,v‚àà‚Ñã01‚Äã(‚Ñùd),u,v\in\mathcal{H}\_{0}^{1}\left(\mathbb{R}^{d}\right), set

|  |  |  |
| --- | --- | --- |
|  | ‚ü®v,u‚ü©‚Ñã~01:=‚ü®v,u‚ü©L2+h‚Äã‚ü®‚Ñí‚Äãv,u‚ü©‚Ñã‚àí1,‚Ñã01,\left\langle v,u\right\rangle\_{\widetilde{\mathcal{H}}\_{0}^{1}}:=\left\langle v,u\right\rangle\_{L^{2}}+h\left\langle\mathcal{L}v,u\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}}, |  |

then, using [Assumptions¬†(CON)](https://arxiv.org/html/2512.25017v1#Thmassumption1 "Assumption (CON). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs") and¬†[(G√Ö)](https://arxiv.org/html/2512.25017v1#Thmassumption2 "Assumption (G√Ö). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"), we have

|  |  |  |
| --- | --- | --- |
|  | ‚Äñu‚Äñ‚Ñã~012=‚ü®u,u‚ü©‚Ñã~01=‚ü®u,u‚ü©L2+h‚Äã‚ü®‚Ñí‚Äãu,u‚ü©‚Ñã‚àí1,‚Ñã01‚Äã{‚â§(1+h‚ÄãM)‚Äã‚Äñu‚Äñ‚Ñã012‚â•h‚ÄãŒª1‚Äã‚Äñu‚Äñ‚Ñã012+(1‚àíh‚ÄãŒª2)‚Äã‚Äñu‚ÄñL22‚â•h‚ÄãŒª1‚Äã‚Äñu‚Äñ‚Ñã012.\left\|u\right\|\_{\widetilde{\mathcal{H}}\_{0}^{1}}^{2}=\left\langle u,u\right\rangle\_{\widetilde{\mathcal{H}}\_{0}^{1}}=\left\langle u,u\right\rangle\_{L^{2}}+h\left\langle\mathcal{L}u,u\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}}\begin{cases}\leq(1+hM)\left\|u\right\|\_{\mathcal{H}\_{0}^{1}}^{2}\\ \geq h\lambda\_{1}\left\|u\right\|\_{\mathcal{H}\_{0}^{1}}^{2}+(1-h\lambda\_{2})\left\|u\right\|\_{L^{2}}^{2}\geq h\lambda\_{1}\left\|u\right\|\_{\mathcal{H}\_{0}^{1}}^{2}.\end{cases} |  |

Hence, this inner product induces a norm on ‚Ñã01‚Äã(‚Ñùd)\mathcal{H}\_{0}^{1}\left(\mathbb{R}^{d}\right), denoted by ‚à•‚ãÖ‚à•‚Ñã~01\left\|\cdot\right\|\_{\widetilde{\mathcal{H}}\_{0}^{1}}, which is equivalent to ‚à•‚ãÖ‚à•‚Ñã01\left\|\cdot\right\|\_{\mathcal{H}\_{0}^{1}}.
In this case, we can rewrite ùíØ~‚Äã(v)‚Äã(x)=‚ü®v,Z‚Äã(x,‚ãÖ)‚ü©‚Ñã~01\widetilde{\mathcal{T}}\left(v\right)\left(x\right)=\left\langle v,Z(x,\cdot)\right\rangle\_{\widetilde{\mathcal{H}}\_{0}^{1}}.

###### Proposition 4.4.

Assume that the neural network satisfies [Assumption¬†(NNI)](https://arxiv.org/html/2512.25017v1#Thmassumption5 "Assumption (NNI). ‚Ä£ 4.1. Convergence of the trained neural network ‚Ä£ 4. Convergence of the training error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs") and the coefficients of the PDE ([2.1](https://arxiv.org/html/2512.25017v1#S2.E1 "Equation 2.1 ‚Ä£ 2. Deep gradient flow methods for PDEs ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"))‚Äì([2.2](https://arxiv.org/html/2512.25017v1#S2.E2 "Equation 2.2 ‚Ä£ 2. Deep gradient flow methods for PDEs ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")) satisfy [Assumption¬†(CON)](https://arxiv.org/html/2512.25017v1#Thmassumption1 "Assumption (CON). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs").
Then, ùíØ~\widetilde{\mathcal{T}} is a self-adjoint, positive definite, and trace class operator on ‚Ñã01‚Äã(‚Ñùd)\mathcal{H}^{1}\_{0}\left(\mathbb{R}^{d}\right) with inner product ‚ü®‚ãÖ,‚ãÖ‚ü©‚Ñã~01\left\langle\cdot,\cdot\right\rangle\_{\widetilde{\mathcal{H}}\_{0}^{1}} *i.e.*, for any u,v‚àà‚Ñã01‚Äã(‚Ñùd)u,v\in\mathcal{H}\_{0}^{1}\left(\mathbb{R}^{d}\right) holds

|  |  |  |
| --- | --- | --- |
|  | ‚ü®ùíØ~‚Äã(v),u‚ü©‚Ñã~01=‚ü®v,ùíØ~‚Äã(u)‚ü©‚Ñã~01,‚ü®ùíØ~‚Äã(v),v‚ü©‚Ñã~01>0‚Äã¬†for¬†‚Äãv‚â†0,¬†and¬†‚àëi=1‚àû‚ü®ùíØ~‚Äã(ei),ei‚ü©‚Ñã~01<+‚àû,\displaystyle\left\langle\widetilde{\mathcal{T}}\left(v\right),u\right\rangle\_{\widetilde{\mathcal{H}}^{1}\_{0}}=\left\langle v,\widetilde{\mathcal{T}}(u)\right\rangle\_{\widetilde{\mathcal{H}}^{1}\_{0}},\quad\left\langle\widetilde{\mathcal{T}}\left(v\right),v\right\rangle\_{\widetilde{\mathcal{H}}^{1}\_{0}}>0\ \text{ for }v\neq 0,\quad\text{ and }\quad\sum\_{i=1}^{\infty}\left\langle\widetilde{\mathcal{T}}\left(\mathrm{e}\_{i}\right),\mathrm{e}\_{i}\right\rangle\_{\widetilde{\mathcal{H}}^{1}\_{0}}<+\infty, |  |

where {ei}i=1‚àû\{\mathrm{e}\_{i}\}^{\infty}\_{i=1} is an orthogonal basis on ‚Ñã01‚Äã(‚Ñùd)\mathcal{H}\_{0}^{1}(\mathbb{R}^{d}) under the norm ‚ü®‚ãÖ,‚ãÖ‚ü©‚Ñã~01\left\langle\cdot,\cdot\right\rangle\_{\widetilde{\mathcal{H}}\_{0}^{1}}.

###### Proof.

Let us first verify that ùíØ~\widetilde{\mathcal{T}} is self-adjoint and positive definite.
Using that

|  |  |  |
| --- | --- | --- |
|  | ùíØ~‚Äã(v)‚Äã(x)=‚ü®v,Z‚Äã(x,‚ãÖ)‚ü©‚Ñã~01=ùîº‚Äã[‚ü®v,X‚ü©‚Ñã~01‚ãÖX],\widetilde{\mathcal{T}}\left(v\right)\left(x\right)=\left\langle v,Z(x,\cdot)\right\rangle\_{\widetilde{\mathcal{H}}\_{0}^{1}}=\mathbb{E}\left[\left\langle v,X\right\rangle\_{\widetilde{\mathcal{H}}\_{0}^{1}}\cdot X\right], |  |

taking the inner product yields

|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚ü®ùíØ~‚Äã(v),u‚ü©‚Ñã~01\displaystyle\left\langle\widetilde{\mathcal{T}}\left(v\right),u\right\rangle\_{\tilde{\mathcal{H}}^{1}\_{0}} | =ùîº‚Äã[‚ü®v,X‚ü©‚Ñã~01‚ãÖ‚ü®u,X‚ü©‚Ñã~01]=‚ü®v,ùíØ~‚Äã(u)‚ü©‚Ñã~01,\displaystyle=\mathbb{E}\left[\left\langle v,X\right\rangle\_{\widetilde{\mathcal{H}}\_{0}^{1}}\cdot\left\langle u,X\right\rangle\_{\widetilde{\mathcal{H}}\_{0}^{1}}\right]=\left\langle v,\widetilde{\mathcal{T}}(u)\right\rangle\_{\widetilde{\mathcal{H}}^{1}\_{0}}, |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
| and | | | | |
|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚ü®ùíØ~‚Äã(v),v‚ü©‚Ñã~01\displaystyle\left\langle\widetilde{\mathcal{T}}\left(v\right),v\right\rangle\_{\tilde{\mathcal{H}}^{1}\_{0}} | =ùîº‚Äã[|‚ü®v,X‚ü©‚Ñã~01|2]‚â•0.\displaystyle=\mathbb{E}\left[\left|\left\langle v,X\right\rangle\_{\widetilde{\mathcal{H}}\_{0}^{1}}\right|^{2}\right]\geq 0. |  |

Next, let us verify that ùîº‚Äã[|‚ü®v,X‚ü©‚Ñã~01|2]=0\mathbb{E}\left[\left|\left\langle v,X\right\rangle\_{\widetilde{\mathcal{H}}\_{0}^{1}}\right|^{2}\right]=0 only if v=0v=0.
The first marginal of XX is œà‚Äã(Œ±01‚Äãx+c01)\psi\left(\alpha\_{0}^{1}x+c\_{0}^{1}\right) and we know from [Assumption¬†(NNI)](https://arxiv.org/html/2512.25017v1#Thmassumption5 "Assumption (NNI). ‚Ä£ 4.1. Convergence of the trained neural network ‚Ä£ 4. Convergence of the training error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs") that the random variables Œ±01\alpha\_{0}^{1} and c01c\_{0}^{1} have full support.
Hence, ùîº‚Äã[|‚ü®v,X‚ü©‚Ñã~01|2]=0\mathbb{E}\left[\left|\left\langle v,X\right\rangle\_{\widetilde{\mathcal{H}}\_{0}^{1}}\right|^{2}\right]=0 implies ‚ü®v,w‚ü©‚Ñã~01=0\left\langle v,w\right\rangle\_{\widetilde{\mathcal{H}}\_{0}^{1}}=0 for any w‚ààùíû‚Äã(œà)w\in\mathcal{C}\left(\psi\right).
Using that ùíû‚Äã(œà)\mathcal{C}\left(\psi\right) is dense in ‚Ñã01‚Äã(‚Ñùd)\mathcal{H}\_{0}^{1}\left(\mathbb{R}^{d}\right) with the norm ‚à•‚ãÖ‚à•‚Ñã01\left\|\cdot\right\|\_{\mathcal{H}\_{0}^{1}}, see [Theorem¬†3.11](https://arxiv.org/html/2512.25017v1#S3.Thmtheorem11 "Theorem 3.11. ‚Ä£ 3.4. Neural network approximation and a version of the Universal Approximation Theorem ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"), then it is dense with the norm ‚à•‚ãÖ‚à•‚Ñã~01\left\|\cdot\right\|\_{\widetilde{\mathcal{H}}\_{0}^{1}} as well.
Therefore, v=0.v=0.

Finally, let us show that ùíØ~\widetilde{\mathcal{T}} is a trace class operator.
Using Parseval‚Äôs identity and [Lemma¬†A.4](https://arxiv.org/html/2512.25017v1#A1.Thmtheorem4 "Lemma A.4 (‚Ñã‚ÇÄ¬π-boundedness of ùëã). ‚Ä£ A.2. Gradient ùúÉ estimates ‚Ä£ Appendix A Auxiliary results ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"), we have

|  |  |  |
| --- | --- | --- |
|  | ‚àëi=1‚àû‚ü®ùíØ~‚Äã(ei),ei‚ü©‚Ñã~01=‚àëi=1‚àûùîº‚Äã[|‚ü®ei,X‚ü©‚Ñã~01|2]=ùîº‚Äã[‚ÄñX‚Äñ‚Ñã~012]‚â§C‚Äãùîº‚Äã[‚ÄñX‚Äñ‚Ñã012]<+‚àû.‚àé\sum\_{i=1}^{\infty}\left\langle\widetilde{\mathcal{T}}\left(\mathrm{e}\_{i}\right),\mathrm{e}\_{i}\right\rangle\_{\widetilde{\mathcal{H}}^{1}\_{0}}=\sum\_{i=1}^{\infty}\mathbb{E}\left[\left|\left\langle\mathrm{e}\_{i},X\right\rangle\_{\widetilde{\mathcal{H}}\_{0}^{1}}\right|^{2}\right]=\mathbb{E}\left[\left\|X\right\|\_{\widetilde{\mathcal{H}}\_{0}^{1}}^{2}\right]\leq C\mathbb{E}\left[\left\|X\right\|\_{\mathcal{H}\_{0}^{1}}^{2}\right]<+\infty.\qed |  |

###### Proof of [Theorem¬†4.3](https://arxiv.org/html/2512.25017v1#S4.Thmtheorem3 "Theorem 4.3. ‚Ä£ 4.2. Long time behavior of the gradient flow ‚Ä£ 4. Convergence of the training error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs").

Using van Neerven [[30](https://arxiv.org/html/2512.25017v1#bib.bib30), Proposition 14.13], every trace class operator is compact and positive definite.
Therefore, we can do a spectral decomposition for the operator ùíØ~\widetilde{\mathcal{T}}.
There exists an orthogonal basis {e~i}i=1‚àû\{\tilde{\mathrm{e}}\_{i}\}^{\infty}\_{i=1}, such that

|  |  |  |
| --- | --- | --- |
|  | ùíØ~‚Äã(e~i)=Œ≥i‚Äãe~i,\displaystyle\widetilde{\mathcal{T}}\left(\tilde{\mathrm{e}}\_{i}\right)=\gamma\_{i}\tilde{\mathrm{e}}\_{i}, |  |

with Œ≥1‚â•Œ≥2‚â•‚ãØ>0.\gamma\_{1}\geq\gamma\_{2}\geq\dots>0.
Set hti:=‚ü®Vt‚àíw‚àó,e~i‚ü©‚Ñã~01h\_{t}^{i}:=\left\langle V\_{t}-w\_{\*},\tilde{\mathrm{e}}\_{i}\right\rangle\_{\widetilde{\mathcal{H}}\_{0}^{1}}.
Then, using ([4.8](https://arxiv.org/html/2512.25017v1#S4.E8 "Equation 4.8 ‚Ä£ 4.2. Long time behavior of the gradient flow ‚Ä£ 4. Convergence of the training error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")), we have

|  |  |  |
| --- | --- | --- |
|  | d‚Äãhtid‚Äãt=‚ü®d‚Äã(Vt‚àíw‚àó),e~i‚ü©‚Ñã~01d‚Äãt=‚àí‚ü®ùíØ~‚Äã(Vt‚àíw‚àó),e~i‚ü©‚Ñã~01=‚àí‚ü®Vt‚àíw‚àó,ùíØ~‚Äãe~i‚ü©‚Ñã~01=‚àíŒ≥i‚Äãhti.\displaystyle\frac{\mathrm{d}h\_{t}^{i}}{\mathrm{d}t}=\frac{\left\langle\mathrm{d}\left(V\_{t}-w\_{\*}\right),\tilde{\mathrm{e}}\_{i}\right\rangle\_{\widetilde{\mathcal{H}}\_{0}^{1}}}{\mathrm{d}t}=-\left\langle\widetilde{\mathcal{T}}\left(V\_{t}-w\_{\*}\right),\tilde{\mathrm{e}}\_{i}\right\rangle\_{\widetilde{\mathcal{H}}\_{0}^{1}}=-\left\langle V\_{t}-w\_{\*},\widetilde{\mathcal{T}}\tilde{\mathrm{e}}\_{i}\right\rangle\_{\widetilde{\mathcal{H}}\_{0}^{1}}=-\gamma\_{i}h\_{t}^{i}. |  |

Therefore, hti=e‚àíŒ≥i‚Äãt‚Äãh0ih\_{t}^{i}=\mathrm{e}^{-\gamma\_{i}t}h\_{0}^{i}.
Hence, using Parseval‚Äôs identity again, we get

|  |  |  |
| --- | --- | --- |
|  | ‚ÄñVt‚àíw‚àó‚Äñ‚Ñã~012=‚àëi=1‚àû(hti)2=‚àëi=1‚àûe‚àí2‚ÄãŒ≥i‚Äãt‚Äã(h0i)2,\displaystyle\left\|V\_{t}-w\_{\*}\right\|^{2}\_{\widetilde{\mathcal{H}}\_{0}^{1}}=\sum\_{i=1}^{\infty}\left(h^{i}\_{t}\right)^{2}=\sum\_{i=1}^{\infty}\mathrm{e}^{-2\gamma\_{i}t}\left(h\_{0}^{i}\right)^{2}, |  |

which converges to 0 because Œ≥i>0\gamma\_{i}>0 and ‚àëi=1‚àû(h0i)2=‚Äñw‚àó‚Äñ‚Ñã~012<+‚àû.\sum\_{i=1}^{\infty}\left(h\_{0}^{i}\right)^{2}=\left\|w\_{\*}\right\|^{2}\_{\widetilde{\mathcal{H}}\_{0}^{1}}<+\infty.
Finally, since the norm ‚à•‚ãÖ‚à•‚Ñã~01\left\|\cdot\right\|\_{\widetilde{\mathcal{H}}\_{0}^{1}} is equivalent to ‚à•‚ãÖ‚à•‚Ñã01,\left\|\cdot\right\|\_{\mathcal{H}\_{0}^{1}}, we conclude

|  |  |  |
| --- | --- | --- |
|  | limt‚Üí‚àû‚ÄñVt‚àíw‚àó‚Äñ‚Ñã01=0.‚àé\lim\_{t\to\infty}\left\|V\_{t}-w\_{\*}\right\|\_{\mathcal{H}\_{0}^{1}}=0.\qed |  |

## Appendix A Auxiliary results

### A.1. Functional inequalities and norm estimates

In the first part of the appendix, we show that the Fr√©chet derivative of the loss function is continuous, and we also prove that the neural network and its wide network limit are bounded in the ‚Ñã01\mathcal{H}\_{0}^{1}-norm.

###### Lemma A.1 (Continuity of the Fr√©chet derivative).

Assume that the operators of the PDE ([2.1](https://arxiv.org/html/2512.25017v1#S2.E1 "Equation 2.1 ‚Ä£ 2. Deep gradient flow methods for PDEs ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"))‚Äì([2.2](https://arxiv.org/html/2512.25017v1#S2.E2 "Equation 2.2 ‚Ä£ 2. Deep gradient flow methods for PDEs ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")) satisfy [Assumption¬†(CON)](https://arxiv.org/html/2512.25017v1#Thmassumption1 "Assumption (CON). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs").
Then, the Fr√©chet derivative of the loss function is continuous, *i.e.* there exists a constant K>0K>0, such that for any u,v,wu,v,w in ‚Ñã01‚Äã(‚Ñùd)\mathcal{H}\_{0}^{1}(\mathbb{R}^{d}) holds

|  |  |  |
| --- | --- | --- |
|  | |‚ü®ùíü‚ÄãIk‚Äã(v),u‚ü©‚Ñã01‚àí‚ü®ùíü‚ÄãIk‚Äã(w),u‚ü©‚Ñã01|‚â§K‚Äã‚Äñv‚àíw‚Äñ‚Ñã01‚Äã‚Äñu‚Äñ‚Ñã01.\displaystyle\left|\left\langle\mathcal{D}I^{k}\left(v\right),u\right\rangle\_{\mathcal{H}\_{0}^{1}}-\left\langle\mathcal{D}I^{k}\left(w\right),u\right\rangle\_{\mathcal{H}\_{0}^{1}}\right|\leq K\left\|v-w\right\|\_{\mathcal{H}\_{0}^{1}}\left\|u\right\|\_{\mathcal{H}\_{0}^{1}}. |  |

In particular, by choosing w=0w=0, we have

|  |  |  |
| --- | --- | --- |
|  | |‚ü®ùíü‚ÄãIk‚Äã(v),u‚ü©‚Ñã01|‚â§K‚Äã(1+‚Äñv‚Äñ‚Ñã01)‚Äã‚Äñu‚Äñ‚Ñã01.\displaystyle\left|\left\langle\mathcal{D}I^{k}\left(v\right),u\right\rangle\_{\mathcal{H}\_{0}^{1}}\right|\leq K\left(1+\left\|v\right\|\_{\mathcal{H}\_{0}^{1}}\right)\left\|u\right\|\_{\mathcal{H}\_{0}^{1}}. |  |

###### Proof.

Using the definition of the Fr√©chet derivative in ([4.2](https://arxiv.org/html/2512.25017v1#S4.E2 "Equation 4.2 ‚Ä£ 4.1. Convergence of the trained neural network ‚Ä£ 4. Convergence of the training error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")), the triangle and Cauchy‚ÄìSchwarz inequalities and [Assumption¬†(CON)](https://arxiv.org/html/2512.25017v1#Thmassumption1 "Assumption (CON). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"), we get that

|  |  |  |  |
| --- | --- | --- | --- |
|  | |‚ü®ùíü‚ÄãIk‚Äã(v),u‚ü©‚Ñã01‚àí‚ü®ùíü‚ÄãIk‚Äã(w),u‚ü©‚Ñã01|\displaystyle\left|\left\langle\mathcal{D}I^{k}\left(v\right),u\right\rangle\_{\mathcal{H}\_{0}^{1}}-\left\langle\mathcal{D}I^{k}\left(w\right),u\right\rangle\_{\mathcal{H}\_{0}^{1}}\right| | =|‚ü®v‚àíw,u‚ü©L2+h2‚Äã‚ü®‚Ñí‚Äã(v‚àíw),u‚ü©‚Ñã‚àí1,‚Ñã01|\displaystyle=\left|\left\langle v-w,u\right\rangle\_{L^{2}}+\frac{h}{2}\left\langle\mathcal{L}(v-w),u\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}}\right| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§‚Äñv‚àíw‚ÄñL2‚Äã‚Äñu‚ÄñL2+h‚ÄãM2‚Äã‚Äñv‚àíw‚Äñ‚Ñã01‚Äã‚Äñu‚Äñ‚Ñã01\displaystyle\leq\left\|v-w\right\|\_{L^{2}}\left\|u\right\|\_{L^{2}}+\frac{hM}{2}\left\|v-w\right\|\_{\mathcal{H}\_{0}^{1}}\left\|u\right\|\_{\mathcal{H}\_{0}^{1}} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§K‚Äã‚Äñv‚àíw‚Äñ‚Ñã01‚Äã‚Äñu‚Äñ‚Ñã01.\displaystyle\leq K\left\|v-w\right\|\_{\mathcal{H}\_{0}^{1}}\left\|u\right\|\_{\mathcal{H}\_{0}^{1}}. |  |

Setting w=0w=0 and using again the Cauchy‚ÄìSchwarz inequality and [Assumption¬†(CON)](https://arxiv.org/html/2512.25017v1#Thmassumption1 "Assumption (CON). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"), yields

|  |  |  |  |
| --- | --- | --- | --- |
|  | |‚ü®ùíü‚ÄãIk‚Äã(0),u‚ü©‚Ñã01|\displaystyle\left|\left\langle\mathcal{D}I^{k}\left(0\right),u\right\rangle\_{\mathcal{H}\_{0}^{1}}\right| | =|‚àí‚ü®Uk‚àí1,u‚ü©L2+h‚Äã‚ü®F‚Äã(Uk‚àí1),u‚ü©L2|\displaystyle=\left|-\left\langle U^{k-1},u\right\rangle\_{L^{2}}+h\left\langle F(U^{k-1}),u\right\rangle\_{L^{2}}\right| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§‚ÄñUk‚àí1‚ÄñL2‚Äã‚Äñu‚ÄñL2+h‚Äã‚ÄñF‚Äã(Uk‚àí1)‚ÄñL2‚Äã‚Äñu‚ÄñL2‚â§K‚Äã‚Äñu‚Äñ‚Ñã01,\displaystyle\leq\left\|U^{k-1}\right\|\_{L^{2}}\left\|u\right\|\_{L^{2}}+h\left\|F(U^{k-1})\right\|\_{L^{2}}\left\|u\right\|\_{L^{2}}\leq K\left\|u\right\|\_{\mathcal{H}\_{0}^{1}}, |  |

since Uk‚àí1‚àà‚Ñã01U^{k-1}\in\mathcal{H}\_{0}^{1}.
Then, combining the two results and using the triangle inequality, we arrive at

|  |  |  |  |
| --- | --- | --- | --- |
|  | |‚ü®ùíü‚ÄãIk‚Äã(v),u‚ü©‚Ñã01|\displaystyle\left|\left\langle\mathcal{D}I^{k}\left(v\right),u\right\rangle\_{\mathcal{H}\_{0}^{1}}\right| | =|‚ü®ùíü‚ÄãIk‚Äã(v),u‚ü©‚Ñã01‚àí‚ü®ùíü‚ÄãIk‚Äã(0),u‚ü©‚Ñã01+‚ü®ùíü‚ÄãIk‚Äã(0),u‚ü©‚Ñã01|\displaystyle=\left|\left\langle\mathcal{D}I^{k}\left(v\right),u\right\rangle\_{\mathcal{H}\_{0}^{1}}-\left\langle\mathcal{D}I^{k}\left(0\right),u\right\rangle\_{\mathcal{H}\_{0}^{1}}+\left\langle\mathcal{D}I^{k}\left(0\right),u\right\rangle\_{\mathcal{H}\_{0}^{1}}\right| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§|‚ü®ùíü‚ÄãIk‚Äã(v),u‚ü©‚Ñã01‚àí‚ü®ùíü‚ÄãIk‚Äã(0),u‚ü©‚Ñã01|+|‚ü®ùíü‚ÄãIk‚Äã(0),u‚ü©‚Ñã01|\displaystyle\leq\left|\left\langle\mathcal{D}I^{k}\left(v\right),u\right\rangle\_{\mathcal{H}\_{0}^{1}}-\left\langle\mathcal{D}I^{k}\left(0\right),u\right\rangle\_{\mathcal{H}\_{0}^{1}}\right|+\left|\left\langle\mathcal{D}I^{k}\left(0\right),u\right\rangle\_{\mathcal{H}\_{0}^{1}}\right| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§K‚Äã(1+‚Äñv‚Äñ‚Ñã01)‚Äã‚Äñu‚Äñ‚Ñã01.‚àé\displaystyle\leq K\left(1+\left\|v\right\|\_{\mathcal{H}\_{0}^{1}}\right)\left\|u\right\|\_{\mathcal{H}\_{0}^{1}}.\qed |  |

###### Lemma A.2.

Assume that the neural network satisfies [Assumption¬†(NNI)](https://arxiv.org/html/2512.25017v1#Thmassumption5 "Assumption (NNI). ‚Ä£ 4.1. Convergence of the trained neural network ‚Ä£ 4. Convergence of the training error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs") and the operators of the PDE ([2.1](https://arxiv.org/html/2512.25017v1#S2.E1 "Equation 2.1 ‚Ä£ 2. Deep gradient flow methods for PDEs ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"))‚Äì([2.2](https://arxiv.org/html/2512.25017v1#S2.E2 "Equation 2.2 ‚Ä£ 2. Deep gradient flow methods for PDEs ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")) satisfy [Assumptions¬†(CON)](https://arxiv.org/html/2512.25017v1#Thmassumption1 "Assumption (CON). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs") and¬†[(G√Ö)](https://arxiv.org/html/2512.25017v1#Thmassumption2 "Assumption (G√Ö). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs").
Then, we have the following inequalities, for all t‚â•0,t\geq 0,

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Äã[‚ÄñVtn‚Äñ‚Ñã012]‚â§Cœà¬†and¬†‚ÄñVt‚Äñ‚Ñã012‚â§Cœà,\displaystyle\mathbb{E}\left[\left\|V^{n}\_{t}\right\|\_{\mathcal{H}\_{0}^{1}}^{2}\right]\leq C\_{\psi}\quad\text{ and }\quad\left\|V\_{t}\right\|^{2}\_{\mathcal{H}\_{0}^{1}}\leq C\_{\psi}, |  |

where CœàC\_{\psi} is a positive constant that only depends on the activation function œà\psi.

###### Proof.

Let us first show that Ik‚Äã(Vtn)I^{k}\left(V\_{t}^{n}\right) is not increasing in tt.
According to ([4.1](https://arxiv.org/html/2512.25017v1#S4.Ex5 "4.1. Convergence of the trained neural network ‚Ä£ 4. Convergence of the training error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")), we have

|  |  |  |
| --- | --- | --- |
|  | d‚ÄãIk‚Äã(Vtn)d‚Äãt=‚àáŒ∏Ik‚Äã(Vtn)‚ãÖd‚ÄãŒ∏tnd‚Äãt=‚àíŒ∑n‚Äã|‚àáŒ∏Ik‚Äã(Vn‚Äã(Œ∏tn;x))|2‚â§0.\frac{\mathrm{d}I^{k}\left(V\_{t}^{n}\right)}{\mathrm{d}t}=\nabla\_{\theta}I^{k}\left(V\_{t}^{n}\right)\cdot\frac{\mathrm{d}\theta\_{t}^{n}}{\mathrm{d}t}=-\eta\_{n}\left|\nabla\_{\theta}I^{k}\left(V^{n}\left(\theta\_{t}^{n};x\right)\right)\right|^{2}\leq 0. |  |

This inequality readily implies Ik‚Äã(Vtn)‚â§Ik‚Äã(V0n)I^{k}\left(V\_{t}^{n}\right)\leq I^{k}\left(V\_{0}^{n}\right).
Using [Assumption¬†(CON)](https://arxiv.org/html/2512.25017v1#Thmassumption1 "Assumption (CON). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"), the Cauchy‚ÄìSchwarz inequality and a‚Äãb‚â§a2+b22ab\leq\frac{a^{2}+b^{2}}{2}, we get that

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ik‚Äã(u)\displaystyle I^{k}(u) | =12‚Äã‚Äñu‚àíUk‚àí1‚ÄñL22+h2‚Äã‚ü®‚Ñí‚Äãu,u‚ü©‚Ñã‚àí1,‚Ñã01+h‚Äã‚ü®F‚Äã(Uk‚àí1),u‚ü©L2\displaystyle=\frac{1}{2}\left\|u-U^{k-1}\right\|\_{L^{2}}^{2}+\frac{h}{2}\left\langle\mathcal{L}u,u\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}}+h\left\langle F\left(U^{k-1}\right),u\right\rangle\_{L^{2}} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =12‚Äã‚Äñu‚ÄñL22+h2‚Äã‚ü®‚Ñí‚Äãu,u‚ü©‚Ñã‚àí1,‚Ñã01+‚ü®h‚ÄãF‚Äã(Uk‚àí1)‚àíUk‚àí1,u‚ü©L2+12‚Äã‚ÄñUk‚àí1‚ÄñL22\displaystyle=\frac{1}{2}\left\|u\right\|^{2}\_{L^{2}}+\frac{h}{2}\left\langle\mathcal{L}u,u\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}}+\left\langle hF\left(U^{k-1}\right)-U^{k-1},u\right\rangle\_{L^{2}}+\frac{1}{2}\left\|U^{k-1}\right\|\_{L^{2}}^{2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§12‚Äã‚Äñu‚ÄñL22+h‚ÄãM2‚Äã‚Äñu‚Äñ‚Ñã012+‚Äñh‚ÄãF‚Äã(Uk‚àí1)‚àíUk‚àí1‚ÄñL2‚Äã‚Äñu‚ÄñL2+12‚Äã‚ÄñUk‚àí1‚ÄñL22\displaystyle\leq\frac{1}{2}\left\|u\right\|^{2}\_{L^{2}}+\frac{hM}{2}\left\|u\right\|^{2}\_{\mathcal{H}\_{0}^{1}}+\left\|hF(U^{k-1})-U^{k-1}\right\|\_{L^{2}}\left\|u\right\|\_{L^{2}}+\frac{1}{2}\left\|U^{k-1}\right\|\_{L^{2}}^{2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§‚Äñu‚ÄñL22+h‚ÄãM2‚Äã‚Äñu‚Äñ‚Ñã012+12‚Äã‚Äñh‚ÄãF‚Äã(Uk‚àí1)‚àíUk‚àí1‚ÄñL22+12‚Äã‚ÄñUk‚àí1‚ÄñL22\displaystyle\leq\left\|u\right\|^{2}\_{L^{2}}+\frac{hM}{2}\left\|u\right\|^{2}\_{\mathcal{H}\_{0}^{1}}+\frac{1}{2}\left\|hF(U^{k-1})-U^{k-1}\right\|\_{L^{2}}^{2}+\frac{1}{2}\left\|U^{k-1}\right\|\_{L^{2}}^{2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =C1‚Äã‚Äñu‚Äñ‚Ñã012+C2.\displaystyle=C\_{1}\left\|u\right\|\_{\mathcal{H}\_{0}^{1}}^{2}+C\_{2}. |  |

Moreover, using [Assumption¬†(G√Ö)](https://arxiv.org/html/2512.25017v1#Thmassumption2 "Assumption (G√Ö). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"), the Cauchy‚ÄìSchwarz inequality again and the inequality

|  |  |  |
| --- | --- | --- |
|  | ‚Äñm‚ÄñL2‚Äã‚Äñn‚ÄñL2‚â§Œª2‚Äã‚Äñm‚ÄñL22+12‚ÄãŒª‚Äã‚Äñv‚ÄñL22‚Äã¬†with¬†‚ÄãŒª=h‚ÄãŒª12,\left\|m\right\|\_{L^{2}}\left\|n\right\|\_{L^{2}}\leq\frac{\lambda}{2}\left\|m\right\|\_{L^{2}}^{2}+\frac{1}{2\lambda}\left\|v\right\|\_{L^{2}}^{2}\text{ with }\lambda=\frac{h\lambda\_{1}}{2}, |  |

we arrive at

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ik‚Äã(u)\displaystyle I^{k}(u) | =12‚Äã‚Äñu‚ÄñL22+h2‚Äã‚ü®‚Ñí‚Äãu,u‚ü©‚Ñã‚àí1,‚Ñã01+‚ü®h‚ÄãF‚Äã(Uk‚àí1)‚àíUk‚àí1,u‚ü©L2+12‚Äã‚ÄñUk‚àí1‚ÄñL22\displaystyle=\frac{1}{2}\left\|u\right\|^{2}\_{L^{2}}+\frac{h}{2}\left\langle\mathcal{L}u,u\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}}+\left\langle hF\left(U^{k-1}\right)-U^{k-1},u\right\rangle\_{L^{2}}+\frac{1}{2}\left\|U^{k-1}\right\|\_{L^{2}}^{2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â•(12‚àíh‚ÄãŒª22)‚Äã‚Äñu‚ÄñL22+h‚ÄãŒª12‚Äã‚Äñu‚Äñ‚Ñã012‚àí‚Äñh‚ÄãF‚Äã(Uk‚àí1)‚àíUk‚àí1‚ÄñL2‚Äã‚Äñu‚ÄñL2‚àí12‚Äã‚ÄñUk‚àí1‚ÄñL22\displaystyle\geq\left(\frac{1}{2}-\frac{h\lambda\_{2}}{2}\right)\left\|u\right\|^{2}\_{L^{2}}+\frac{h\lambda\_{1}}{2}\left\|u\right\|\_{\mathcal{H}\_{0}^{1}}^{2}-\left\|hF\left(U^{k-1}\right)-U^{k-1}\right\|\_{L^{2}}\left\|u\right\|\_{L^{2}}-\frac{1}{2}\left\|U^{k-1}\right\|\_{L\_{2}}^{2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â•(12‚àíh‚ÄãŒª22)‚Äã‚Äñu‚ÄñL22+h‚ÄãŒª14‚Äã‚Äñu‚Äñ‚Ñã012‚àí1h‚ÄãŒª1‚Äã‚Äñh‚ÄãF‚Äã(Uk‚àí1)‚àíUk‚àí1‚ÄñL22‚àí12‚Äã‚ÄñUk‚àí1‚ÄñL22\displaystyle\geq\left(\frac{1}{2}-\frac{h\lambda\_{2}}{2}\right)\left\|u\right\|^{2}\_{L^{2}}+\frac{h\lambda\_{1}}{4}\left\|u\right\|\_{\mathcal{H}\_{0}^{1}}^{2}-\frac{1}{h\lambda\_{1}}\left\|hF\left(U^{k-1}\right)-U^{k-1}\right\|\_{L^{2}}^{2}-\frac{1}{2}\left\|U^{k-1}\right\|\_{L^{2}}^{2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =C3‚Äã‚Äñu‚Äñ‚Ñã012‚àíC4.\displaystyle=C\_{3}\left\|u\right\|\_{\mathcal{H}\_{0}^{1}}^{2}-C\_{4}. |  |

Therefore, since Ik‚Äã(Vtn)I^{k}\left(V\_{t}^{n}\right) is not increasing with tt and using [Lemma¬†4.2](https://arxiv.org/html/2512.25017v1#S4.Thmtheorem2 "Lemma 4.2. ‚Ä£ 4.1. Convergence of the trained neural network ‚Ä£ 4. Convergence of the training error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"), we have that

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùîº‚Äã[‚ÄñVtn‚Äñ‚Ñã012]\displaystyle\mathbb{E}\left[\left\|V^{n}\_{t}\right\|\_{\mathcal{H}\_{0}^{1}}^{2}\right] | ‚â§ùîº‚Äã[1C3‚ÄãIk‚Äã(Vtn)+C4C3]‚â§ùîº‚Äã[1C3‚ÄãIk‚Äã(V0n)+C4C3]\displaystyle\leq\mathbb{E}\left[\frac{1}{C\_{3}}I^{k}\left(V^{n}\_{t}\right)+\frac{C\_{4}}{C\_{3}}\right]\leq\mathbb{E}\left[\frac{1}{C\_{3}}I^{k}\left(V\_{0}^{n}\right)+\frac{C\_{4}}{C\_{3}}\right] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§ùîº‚Äã[C1C3‚Äã‚ÄñV0n‚Äñ‚Ñã012+C2+C4C3]‚â§Cœà.\displaystyle\leq\mathbb{E}\left[\frac{C\_{1}}{C\_{3}}\left\|V^{n}\_{0}\right\|\_{\mathcal{H}\_{0}^{1}}^{2}+\frac{C\_{2}+C\_{4}}{C\_{3}}\right]\leq C\_{\psi}. |  |

Using similar arguments, we get

|  |  |  |
| --- | --- | --- |
|  | d‚ÄãIk‚Äã(Vt)d‚Äãt=‚ü®ùíü‚ÄãIk‚Äã(Vt),d‚ÄãVtd‚Äãt‚ü©‚Ñã01=‚àí|‚ü®ùíü‚ÄãIk‚Äã(Vt),ùîº‚Äã[‚àáŒ∏Œ≤0‚Äãœà‚Äã(Œ±0‚Äãx+c0)]‚ü©‚Ñã01|2‚â§0.\displaystyle\frac{\mathrm{d}I^{k}\left(V\_{t}\right)}{\mathrm{d}t}=\left\langle\mathcal{D}I^{k}\left(V\_{t}\right),\frac{\mathrm{d}V\_{t}}{\mathrm{d}t}\right\rangle\_{\mathcal{H}\_{0}^{1}}=-\left|\left\langle\mathcal{D}I^{k}\left(V\_{t}\right),\mathbb{E}\left[\nabla\_{\theta}\beta\_{0}\psi\left(\alpha\_{0}x+c\_{0}\right)\right]\right\rangle\_{\mathcal{H}\_{0}^{1}}\right|^{2}\leq 0. |  |

This inequality yields that Ik‚Äã(Vt)‚â§Ik‚Äã(V0)=Ik‚Äã(0)I^{k}\left(V\_{t}\right)\leq I^{k}\left(V\_{0}\right)=I^{k}\left(0\right), hence ‚ÄñVt‚Äñ‚Ñã012‚â§Cœà.\left\|V\_{t}\right\|^{2}\_{\mathcal{H}\_{0}^{1}}\leq C\_{\psi}.
‚àé

### A.2. Gradient Œ∏\theta estimates

This subsection contains several useful results concerning gradient estimates of the neurons of the neural network with respect to its parameters.
Moreover, we show that gradients of the neurons are Lipschitz continuous with respect to the parameters Œ∏\theta of the network, that they converge to their ‚Äúunclipped‚Äù analogs for large nn, and we estimate the distance between the parameters as the training progresses.

###### Lemma A.3 (‚Ñã01\mathcal{H}\_{0}^{1}-boundedness of XnX^{n}).

Let Œ∏‚àà‚Ñù√ó‚Ñù√ó‚Ñùd\theta\in\mathbb{R}\times\mathbb{R}\times\mathbb{R}^{d}, then XnX^{n} is bounded in ‚Ñã01\mathcal{H}\_{0}^{1}, *i.e.* there exists a constant Cœà>0C\_{\psi}>0 such that

|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚ÄñXn‚Äã(Œ∏)‚Äñ‚Ñã012\displaystyle\left\|X^{n}(\theta)\right\|^{2}\_{\mathcal{H}\_{0}^{1}} | ‚â§Cœà‚Äã(rn)d+8.\displaystyle\leq C\_{\psi}\left(r\_{n}\right)^{d+8}. |  |

###### Proof.

The derivatives of a neuron with respect to its parameters equal

|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚àÇ‚àÇŒ≤‚ÄãŒ≤^‚Äãœà‚Äã(Œ±^‚Äãx+c^)\displaystyle\frac{\partial}{\partial\beta}\hat{\beta}\psi\left(\hat{\alpha}x+\hat{c}\right) | =œà‚Äã(Œ±^‚Äãx+c^)‚Äãùüè{|Œ≤|‚â§rn},\displaystyle=\psi\left(\hat{\alpha}x+\hat{c}\right)\mathbf{1}\_{\{\left|\beta\right|\leq r\_{n}\}}, |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚àÇ‚àÇŒ±‚ÄãŒ≤^‚Äãœà‚Äã(Œ±^‚Äãx+c^)\displaystyle\frac{\partial}{\partial\alpha}\hat{\beta}\psi\left(\hat{\alpha}x+\hat{c}\right) | =Œ≤^‚Äãxùñ≥‚Äã(‚àáœà)‚Äã(Œ±^‚Äãx+c^)‚Äãùüè{1rn‚â§|Œ±|‚â§rn},\displaystyle=\hat{\beta}x^{\mathsf{T}}\left(\nabla\psi\right)\left(\hat{\alpha}x+\hat{c}\right)\mathbf{1}\_{\{\frac{1}{r\_{n}}\leq\left|\alpha\right|\leq r\_{n}\}}, |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚àÇ‚àÇc‚ÄãŒ≤^‚Äãœà‚Äã(Œ±^‚Äãx+c^)\displaystyle\frac{\partial}{\partial c}\hat{\beta}\psi\left(\hat{\alpha}x+\hat{c}\right) | =Œ≤^‚Äã(‚àáœà)‚Äã(Œ±^‚Äãx+c^)‚Äãùüè{|c|‚â§rn}.\displaystyle=\hat{\beta}\left(\nabla\psi\right)\left(\hat{\alpha}x+\hat{c}\right)\mathbf{1}\_{\{\left|c\right|\leq r\_{n}\}}. |  |

Therefore, we obtain the bound

|  |  |  |  |
| --- | --- | --- | --- |
|  | |Xn‚Äã(Œ∏)|\displaystyle\left|X^{n}(\theta)\right| | =|‚àÇ‚àÇŒ≤‚ÄãŒ≤^‚Äãœà‚Äã(Œ±^‚Äãx+c^)+‚àÇ‚àÇŒ±‚ÄãŒ≤^‚Äãœà‚Äã(Œ±^‚Äãx+c^)+‚àÇ‚àÇc‚ÄãŒ≤^‚Äãœà‚Äã(Œ±^‚Äãx+c^)|\displaystyle=\left|\frac{\partial}{\partial\beta}\hat{\beta}\psi\left(\hat{\alpha}x+\hat{c}\right)+\frac{\partial}{\partial\alpha}\hat{\beta}\psi\left(\hat{\alpha}x+\hat{c}\right)+\frac{\partial}{\partial c}\hat{\beta}\psi\left(\hat{\alpha}x+\hat{c}\right)\right| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§|œà‚Äã(Œ±^‚Äãx+c^)‚Äãùüè{|Œ≤|‚â§rn}|+|Œ≤^‚Äãxùñ≥‚Äã(‚àáœà)‚Äã(Œ±^‚Äãx+c^)‚Äãùüè{1rn‚â§|Œ±|‚â§rn}|+|Œ≤^‚Äã(‚àáœà)‚Äã(Œ±^‚Äãx+c^)‚Äãùüè{|c|‚â§rn}|\displaystyle\leq\left|\psi\left(\hat{\alpha}x+\hat{c}\right)\mathbf{1}\_{\{\left|\beta\right|\leq r\_{n}\}}\right|+\left|\hat{\beta}x^{\mathsf{T}}\left(\nabla\psi\right)\left(\hat{\alpha}x+\hat{c}\right)\mathbf{1}\_{\{\frac{1}{r\_{n}}\leq\left|\alpha\right|\leq r\_{n}\}}\right|+\left|\hat{\beta}\left(\nabla\psi\right)\left(\hat{\alpha}x+\hat{c}\right)\mathbf{1}\_{\{\left|c\right|\leq r\_{n}\}}\right| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§|œà‚Äã(Œ±^‚Äãx+c^)|+rn‚Äã|x‚ãÖ‚àáœà‚Äã(Œ±^‚Äãx+c^)|+rn‚Äã|(‚àáœà)‚Äã(Œ±^‚Äãx+c^)|.\displaystyle\leq\left|\psi\left(\hat{\alpha}x+\hat{c}\right)\right|+r\_{n}\left|x\cdot\nabla\psi\left(\hat{\alpha}x+\hat{c}\right)\right|+r\_{n}\left|\left(\nabla\psi\right)\left(\hat{\alpha}x+\hat{c}\right)\right|. |  |

The second term above can be bounded by

|  |  |  |
| --- | --- | --- |
|  | rn‚Äã|x‚ãÖ‚àáœà‚Äã(Œ±^‚Äãx+c^)|‚â§rn‚Äã(Œ±^n)‚àí1‚Äã(|(Œ±^n‚Äãx+c^)‚ãÖ‚àáœà‚Äã(Œ±^‚Äãx+c^)|+|c^‚ãÖ‚àáœà‚Äã(Œ±^‚Äãx+c^)|)‚â§Cœà‚Äã(rn)3.\displaystyle r\_{n}\left|x\cdot\nabla\psi\left(\hat{\alpha}x+\hat{c}\right)\right|\leq r\_{n}(\hat{\alpha}\_{n})^{-1}\Big(\left|(\hat{\alpha}\_{n}x+\hat{c})\cdot\nabla\psi\left(\hat{\alpha}x+\hat{c}\right)\right|+\left|\hat{c}\cdot\nabla\psi\left(\hat{\alpha}x+\hat{c}\right)\right|\Big)\leq C\_{\psi}(r\_{n})^{3}. |  |

Therefore, using that |Œ±^|‚â•(rn)‚àí1\left|\hat{\alpha}\right|\geq(r\_{n})^{-1}, we have

|  |  |  |
| --- | --- | --- |
|  | ‚à´‚Ñùd|Xn‚Äã(Œ∏)|2‚Äãdx‚â§‚à´‚Ñùd|œà‚Äã(Œ±^‚Äãx+c^)|2‚Äãdx+(rn)2‚Äã‚à´‚Ñùd|x|2‚Äã|(‚àáœà)‚Äã(Œ±^‚Äãx+c^)|2‚Äãdx+(rn)2‚Äã‚à´‚Ñùd|(‚àáœà)‚Äã(Œ±^‚Äãx+c^)|2‚Äãdx=(y=Œ±^‚Äãx)|Œ±^|‚àíd‚Äã‚à´‚Ñùd|œà‚Äã(y+c^)|2‚Äãdy+(rn)2‚Äã|Œ±^|‚àíd‚àí2‚Äã‚à´‚Ñùd|y|2‚Äã|(‚àáœà)‚Äã(y+c^)|2‚Äãdy+(rn)2‚Äã|Œ±^|‚àíd‚Äã‚à´‚Ñùd|(‚àáœà)‚Äã(y+c^)|2‚Äãdy‚â§(z=y+c^)(rn)d‚Äã‚à´‚Ñùd|œà‚Äã(z)|2‚Äãdz+(rn)d+4‚Äã‚à´‚Ñùd|z‚àíc^|2‚Äã|(‚àáœà)‚Äã(z)|2‚Äãdz+(rn)d+2‚Äã‚à´‚Ñùd|(‚àáœà)‚Äã(z)|2‚Äãdz‚â§Cœà‚Äã(rn)d+6\int\_{\mathbb{R}^{d}}\left|X^{n}(\theta)\right|^{2}\mathrm{d}x\\ \leq\int\_{\mathbb{R}^{d}}\left|\psi\left(\hat{\alpha}x+\hat{c}\right)\right|^{2}\mathrm{d}x+(r\_{n})^{2}\int\_{\mathbb{R}^{d}}\left|x\right|^{2}\left|(\nabla\psi)\left(\hat{\alpha}x+\hat{c}\right)\right|^{2}\mathrm{d}x+(r\_{n})^{2}\int\_{\mathbb{R}^{d}}\left|\left(\nabla\psi\right)\left(\hat{\alpha}x+\hat{c}\right)\right|^{2}\mathrm{d}x\\ \stackrel{{\scriptstyle\left(y=\hat{\alpha}x\right)}}{{=}}\left|\hat{\alpha}\right|^{-d}\int\_{\mathbb{R}^{d}}\left|\psi\left(y+\hat{c}\right)\right|^{2}\mathrm{d}y+(r\_{n})^{2}\left|\hat{\alpha}\right|^{-d-2}\int\_{\mathbb{R}^{d}}\left|y\right|^{2}\left|\left(\nabla\psi\right)\left(y+\hat{c}\right)\right|^{2}\mathrm{d}y\\ \quad+(r\_{n})^{2}\left|\hat{\alpha}\right|^{-d}\int\_{\mathbb{R}^{d}}\left|\left(\nabla\psi\right)\left(y+\hat{c}\right)\right|^{2}\mathrm{d}y\\ \stackrel{{\scriptstyle\left(z=y+\hat{c}\right)}}{{\leq}}(r\_{n})^{d}\int\_{\mathbb{R}^{d}}\left|\psi\left(z\right)\right|^{2}\mathrm{d}z+(r\_{n})^{d+4}\int\_{\mathbb{R}^{d}}\left|z-\hat{c}\right|^{2}\left|\left(\nabla\psi\right)\left(z\right)\right|^{2}\mathrm{d}z+(r\_{n})^{d+2}\int\_{\mathbb{R}^{d}}\left|\left(\nabla\psi\right)\left(z\right)\right|^{2}\mathrm{d}z\\ \leq C\_{\psi}(r\_{n})^{d+6} |  |

Analogously we can estimate the gradient, and we get that,

|  |  |  |
| --- | --- | --- |
|  | |‚àáxXn‚Äã(Œ∏)|‚â§2‚Äãrn‚Äã|‚àáœà‚Äã(Œ±^‚Äãx+c^)|+(rn)2‚Äã|x|‚Äã|(D2‚Äãœà)‚Äã(Œ±^‚Äãx+c^)|+(rn)2‚Äã|(D2‚Äãœà)‚Äã(Œ±^‚Äãx+c^)|,\displaystyle\left|\nabla\_{x}X^{n}(\theta)\right|\leq 2r\_{n}\left|\nabla\psi\left(\hat{\alpha}x+\hat{c}\right)\right|+\left(r\_{n}\right)^{2}\left|x\right|\left|\left(D^{2}\psi\right)\left(\hat{\alpha}x+\hat{c}\right)\right|+(r\_{n})^{2}\left|\left(D^{2}\psi\right)\left(\hat{\alpha}x+\hat{c}\right)\right|, |  |

hence

|  |  |  |
| --- | --- | --- |
|  | ‚à´‚Ñùd|‚àáxXn‚Äã(Œ∏)|2‚Äãdx‚â§Cœà‚Äã(rn)d+8.\int\_{\mathbb{R}^{d}}\left|\nabla\_{x}X^{n}(\theta)\right|^{2}\mathrm{d}x\leq C\_{\psi}(r\_{n})^{d+8}. |  |

Finally, we arrive at

|  |  |  |
| --- | --- | --- |
|  | ‚ÄñXn‚Äã(Œ∏)‚Äñ‚Ñã012=‚à´‚Ñùd|Xn‚Äã(Œ∏)|2+|‚àáxXn‚Äã(Œ∏)|2‚Äãd‚Äãx‚â§Cœà‚Äã(rn)d+8.‚àé\left\|X^{n}(\theta)\right\|^{2}\_{\mathcal{H}\_{0}^{1}}=\int\_{\mathbb{R}^{d}}\left|X^{n}(\theta)\right|^{2}+\left|\nabla\_{x}X^{n}(\theta)\right|^{2}\mathrm{d}x\leq C\_{\psi}\left(r\_{n}\right)^{d+8}.\qed |  |

###### Lemma A.4 (‚Ñã01\mathcal{H}\_{0}^{1}-boundedness of XX).

Let Œ∏‚àà‚Ñù√ó‚Ñù√ó‚Ñùd\theta\in\mathbb{R}\times\mathbb{R}\times\mathbb{R}^{d}, and assume that the neural network satisfies [Assumption¬†(NNI)](https://arxiv.org/html/2512.25017v1#Thmassumption5 "Assumption (NNI). ‚Ä£ 4.1. Convergence of the trained neural network ‚Ä£ 4. Convergence of the training error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs").
Then, XX is bounded in ‚Ñã01\mathcal{H}\_{0}^{1}, *i.e.* there exists a constant Cœà>0C\_{\psi}>0 such that

|  |  |  |
| --- | --- | --- |
|  | ‚ÄñX‚Äã(Œ∏)‚Äñ‚Ñã012‚â§Cœà‚Äã(1+Œ≤2)‚Äã(1+c2)‚Äã(|Œ±|‚àíd+|Œ±|‚àíd‚àí2+|Œ±|2‚àíd).\displaystyle\left\|X(\theta)\right\|^{2}\_{\mathcal{H}\_{0}^{1}}\leq C\_{\psi}\left(1+\beta^{2}\right)\left(1+c^{2}\right)\left(\left|\alpha\right|^{-d}+\left|\alpha\right|^{-d-2}+\left|\alpha\right|^{2-d}\right). |  |

Moreover,

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Äã[‚ÄñX‚Äñ‚Ñã012]<+‚àû¬†and¬†supn‚â•1ùîº‚Äã[‚ÄñXn‚Äñ‚Ñã012]<+‚àû.\displaystyle\mathbb{E}\left[\left\|X\right\|^{2}\_{\mathcal{H}\_{0}^{1}}\right]<+\infty\quad\text{ and }\quad\sup\_{n\geq 1}\mathbb{E}\left[\left\|X^{n}\right\|^{2}\_{\mathcal{H}\_{0}^{1}}\right]<+\infty. |  |

###### Proof.

Let us first consider the L2L^{2}-norm of X‚Äã(Œ∏)X(\theta).
As in the proof of [Lemma¬†A.3](https://arxiv.org/html/2512.25017v1#A1.Thmtheorem3 "Lemma A.3 (‚Ñã‚ÇÄ¬π-boundedness of ùëã^ùëõ). ‚Ä£ A.2. Gradient ùúÉ estimates ‚Ä£ Appendix A Auxiliary results ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"), we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚ÄñX‚Äã(Œ∏)‚ÄñL22\displaystyle\left\|X(\theta)\right\|\_{L^{2}}^{2} | ‚â§‚à´‚Ñùd|œà‚Äã(Œ±‚Äãx+c)|2‚Äãdx+Œ≤2‚Äã‚à´‚Ñùd|x|2‚Äã|(‚àáœà)‚Äã(Œ±‚Äãx+c)|2‚Äãdx+Œ≤2‚Äã‚à´‚Ñùd|(‚àáœà)‚Äã(Œ±‚Äãx+c)|2‚Äãdx\displaystyle\leq\int\_{\mathbb{R}^{d}}\left|\psi\left(\alpha x+c\right)\right|^{2}\mathrm{d}x+\beta^{2}\int\_{\mathbb{R}^{d}}\left|x\right|^{2}\left|\left(\nabla\psi\right)\left(\alpha x+c\right)\right|^{2}\mathrm{d}x+\beta^{2}\int\_{\mathbb{R}^{d}}\left|\left(\nabla\psi\right)\left(\alpha x+c\right)\right|^{2}\mathrm{d}x |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =|Œ±|‚àíd‚Äã‚à´‚Ñùd|œà‚Äã(z)|2‚Äãdz+Œ≤2‚Äã|Œ±|‚àíd‚àí2‚Äã‚à´‚Ñùd|z‚àíc|2‚Äã|(‚àáœà)‚Äã(z)|2‚Äãdz+Œ≤2‚Äã|Œ±|‚àíd‚Äã‚à´‚Ñùd|(‚àáœà)‚Äã(z)|2‚Äãdz\displaystyle=\left|\alpha\right|^{-d}\int\_{\mathbb{R}^{d}}\left|\psi\left(z\right)\right|^{2}\mathrm{d}z+\beta^{2}\left|\alpha\right|^{-d-2}\int\_{\mathbb{R}^{d}}\left|z-c\right|^{2}\left|\left(\nabla\psi\right)\left(z\right)\right|^{2}\mathrm{d}z+\beta^{2}\left|\alpha\right|^{-d}\int\_{\mathbb{R}^{d}}\left|\left(\nabla\psi\right)\left(z\right)\right|^{2}\mathrm{d}z |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§Cœà‚Äã(1+Œ≤2)‚Äã(1+|c|2)‚Äã(|Œ±|‚àíd+|Œ±|‚àíd‚àí2),\displaystyle\leq C\_{\psi}\left(1+\beta^{2}\right)\left(1+\left|c\right|^{2}\right)\left(\left|\alpha\right|^{-d}+\left|\alpha\right|^{-d-2}\right), |  |

where we have used the change of variables z=Œ±‚Äãx+cz=\alpha x+c for the equality in the second step.
Analogously, we have that

|  |  |  |
| --- | --- | --- |
|  | ‚Äñ‚àáX‚Äã(Œ∏)‚ÄñL22‚â§Cœà‚Äã(1+Œ≤2)‚Äã(1+|c|2)‚Äã(|Œ±|‚àíd+|Œ±|2‚àíd).\left\|\nabla X(\theta)\right\|\_{L^{2}}^{2}\leq C\_{\psi}\left(1+\beta^{2}\right)\left(1+\left|c\right|^{2}\right)\left(\left|\alpha\right|^{-d}+\left|\alpha\right|^{2-d}\right). |  |

Combining these two results, we recover the ‚Ñã01\mathcal{H}^{1}\_{0}-estimate of X‚Äã(Œ∏)X(\theta).
Taking expectations on the ‚Ñã01\mathcal{H}^{1}\_{0}-estimate of X‚Äã(Œ∏)X(\theta) and using [Assumption¬†(NNI)](https://arxiv.org/html/2512.25017v1#Thmassumption5 "Assumption (NNI). ‚Ä£ 4.1. Convergence of the trained neural network ‚Ä£ 4. Convergence of the training error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"), we have that

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùîº‚Äã[‚ÄñX‚Äñ‚Ñã012]\displaystyle\mathbb{E}\left[\left\|X\right\|^{2}\_{\mathcal{H}\_{0}^{1}}\right] | ‚â§Cœà‚Äãùîº‚Äã[1+|Œ≤01|2]‚Äãùîº‚Äã[1+|c01|2]‚Äãùîº‚Äã[|Œ±01|‚àíd+|Œ±01|‚àíd‚àí2+|Œ±01|2‚àíd]<+‚àû,\displaystyle\leq C\_{\psi}\mathbb{E}\left[1+\left|\beta^{1}\_{0}\right|^{2}\right]\mathbb{E}\left[1+\left|c^{1}\_{0}\right|^{2}\right]\mathbb{E}\left[\left|\alpha\_{0}^{1}\right|^{-d}+\left|\alpha\_{0}^{1}\right|^{-d-2}+\left|\alpha\_{0}^{1}\right|^{2-d}\right]<+\infty, |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
| while using the ‚Ñã01\mathcal{H}^{1}\_{0}-estimate of Xn‚Äã(Œ∏)X^{n}(\theta) from the previous lemma, we arrive at | | | | |
|  |  |  |  |
| --- | --- | --- | --- |
|  | ùîº‚Äã[‚ÄñXn‚Äñ‚Ñã012]\displaystyle\mathbb{E}\left[\left\|X^{n}\right\|^{2}\_{\mathcal{H}\_{0}^{1}}\right] | ‚â§Cœà‚Äãùîº‚Äã[1+|Œ≤^01|2]‚Äãùîº‚Äã[1+|c^01|2]‚Äãùîº‚Äã[|Œ±^01|‚àíd+|Œ±^01|‚àíd‚àí2+|Œ±^01|2‚àíd]\displaystyle\leq C\_{\psi}\mathbb{E}\left[1+\left|\hat{\beta}^{1}\_{0}\right|^{2}\right]\mathbb{E}\left[1+\left|\hat{c}^{1}\_{0}\right|^{2}\right]\mathbb{E}\left[\left|\hat{\alpha}\_{0}^{1}\right|^{-d}+\left|\hat{\alpha}\_{0}^{1}\right|^{-d-2}+\left|\hat{\alpha}\_{0}^{1}\right|^{2-d}\right] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§Cœà‚Äãùîº‚Äã[1+|Œ≤01|2]‚Äãùîº‚Äã[1+|c01|2]‚èü<+‚àû‚Äãùîº‚Äã[|Œ±^01|‚àíd+|Œ±^01|‚àíd‚àí2+|Œ±^01|2‚àíd].\displaystyle\leq C\_{\psi}\underbrace{\mathbb{E}\left[1+\left|\beta^{1}\_{0}\right|^{2}\right]\mathbb{E}\left[1+\left|c^{1}\_{0}\right|^{2}\right]}\_{<+\infty}\mathbb{E}\left[\left|\hat{\alpha}\_{0}^{1}\right|^{-d}+\left|\hat{\alpha}\_{0}^{1}\right|^{-d-2}+\left|\hat{\alpha}\_{0}^{1}\right|^{2-d}\right]. |  |

Using a similar reasoning as in the proof of [Lemma¬†4.2](https://arxiv.org/html/2512.25017v1#S4.Thmtheorem2 "Lemma 4.2. ‚Ä£ 4.1. Convergence of the trained neural network ‚Ä£ 4. Convergence of the training error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"), if d=1,2d=1,2,

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Äã[|Œ±^01|‚àíd+|Œ±^01|‚àíd‚àí2+|Œ±^01|2‚àíd]‚â§ùîº‚Äã[|Œ±01|‚àíd+|Œ±01|‚àíd‚àí2+(rn)‚àíd+(rn)‚àíd‚àí2+|Œ±01|+1]<‚àû,\mathbb{E}\left[\left|\hat{\alpha}\_{0}^{1}\right|^{-d}+\left|\hat{\alpha}\_{0}^{1}\right|^{-d-2}+\left|\hat{\alpha}\_{0}^{1}\right|^{2-d}\right]\leq\mathbb{E}\left[\left|\alpha\_{0}^{1}\right|^{-d}+\left|\alpha\_{0}^{1}\right|^{-d-2}+\left(r\_{n}\right)^{-d}+\left(r\_{n}\right)^{-d-2}+\left|\alpha\_{0}^{1}\right|+1\right]<\infty, |  |

and if d‚â•3d\geq 3,

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùîº‚Äã[|Œ±^01|‚àíd+|Œ±^01|‚àíd‚àí2+|Œ±^01|2‚àíd]\displaystyle\mathbb{E}\left[\left|\hat{\alpha}\_{0}^{1}\right|^{-d}+\left|\hat{\alpha}\_{0}^{1}\right|^{-d-2}+\left|\hat{\alpha}\_{0}^{1}\right|^{2-d}\right] | ‚â§ùîº‚Äã[|Œ±01|‚àíd+|Œ±01|‚àíd‚àí2+(rn)‚àíd+(rn)‚àíd‚àí2+|Œ±01|2‚àíd+(rn)2‚àíd]\displaystyle\leq\mathbb{E}\left[\left|\alpha\_{0}^{1}\right|^{-d}+\left|\alpha\_{0}^{1}\right|^{-d-2}+\left(r\_{n}\right)^{-d}+\left(r\_{n}\right)^{-d-2}+\left|\alpha\_{0}^{1}\right|^{2-d}+\left(r\_{n}\right)^{2-d}\right] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | <‚àû.\displaystyle<\infty. |  |

Therefore, supn‚â•1ùîº‚Äã[‚ÄñXn‚Äñ‚Ñã012]<+‚àû.\sup\_{n\geq 1}\mathbb{E}\left[\left\|X^{n}\right\|^{2}\_{\mathcal{H}\_{0}^{1}}\right]<+\infty.
‚àé

###### Lemma A.5.

Assume that the neural network satisfies [Assumption¬†(NNI)](https://arxiv.org/html/2512.25017v1#Thmassumption5 "Assumption (NNI). ‚Ä£ 4.1. Convergence of the trained neural network ‚Ä£ 4. Convergence of the training error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs").
Then

|  |  |  |
| --- | --- | --- |
|  | Œµn:=ùîº‚Äã[‚ÄñXn‚àíX‚Äñ‚Ñã012]‚Üín‚Üí‚àû0.\displaystyle{\varepsilon}\_{n}:=\mathbb{E}\left[\left\|X^{n}-X\right\|\_{\mathcal{H}\_{0}^{1}}^{2}\right]\xrightarrow[n\to\infty]{}0. |  |

###### Proof.

Let us decompose Œµn{\varepsilon}\_{n} as follows,

|  |  |  |
| --- | --- | --- |
|  | Œµn=ùîº‚Äã[‚ÄñXn‚àíX‚Äñ‚Ñã012‚Äã{ùüè|Œ≤01|‚â§rn,|Œ±01|‚â§rn,|c01|‚â§rn+ùüè|Œ≤01|>rn,|Œ±01|‚â§rn,|c01|‚â§rn+ùüè|Œ±01|>rn,|c01|‚â§rn+ùüè|c01|>rn}],\displaystyle{\varepsilon}\_{n}=\mathbb{E}\left[\left\|X^{n}-X\right\|\_{\mathcal{H}\_{0}^{1}}^{2}\left\{\boldsymbol{1}\_{\left|\beta\_{0}^{1}\right|\leq r\_{n},\left|\alpha\_{0}^{1}\right|\leq r\_{n},\left|c\_{0}^{1}\right|\leq r\_{n}}+\boldsymbol{1}\_{\left|\beta\_{0}^{1}\right|>r\_{n},\left|\alpha\_{0}^{1}\right|\leq r\_{n},\left|c\_{0}^{1}\right|\leq r\_{n}}+\boldsymbol{1}\_{\left|\alpha\_{0}^{1}\right|>r\_{n},\left|c\_{0}^{1}\right|\leq r\_{n}}+\boldsymbol{1}\_{\left|c\_{0}^{1}\right|>r\_{n}}\right\}\right], |  |

and then we treat each summand separately.

Term 1.
By definition ‚ÄñXn‚àíX‚Äñ‚Ñã012‚Äãùüè{|Œ≤01|‚â§rn,|Œ±01|‚â§rn,|c01|‚â§rn}=0.\left\|X^{n}-X\right\|\_{\mathcal{H}\_{0}^{1}}^{2}\boldsymbol{1}\_{\{\left|\beta\_{0}^{1}\right|\leq r\_{n},\left|\alpha\_{0}^{1}\right|\leq r\_{n},\left|c\_{0}^{1}\right|\leq r\_{n}\}}=0.

Term 2.
Let |Œ≤01|>rn\left|\beta\_{0}^{1}\right|>r\_{n} and |Œ±01|‚â§rn\left|\alpha\_{0}^{1}\right|\leq r\_{n}, then ‚ÄñXn‚Äñ‚Ñã01‚â§‚ÄñX‚Äñ‚Ñã01\left\|X^{n}\right\|\_{\mathcal{H}\_{0}^{1}}\leq\left\|X\right\|\_{\mathcal{H}\_{0}^{1}}.
Hence,

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Äã[‚ÄñXn‚àíX‚Äñ‚Ñã012‚Äãùüè{|Œ≤01|>rn,|Œ±01|‚â§rn,|c01|‚â§rn}]‚â§2‚Äãùîº‚Äã[‚ÄñX‚Äñ‚Ñã012‚Äãùüè{|Œ≤01|>rn,|Œ±01|‚â§rn,|c01|‚â§rn}],\mathbb{E}\left[\left\|X^{n}-X\right\|\_{\mathcal{H}\_{0}^{1}}^{2}\boldsymbol{1}\_{\{\left|\beta\_{0}^{1}\right|>r\_{n},\left|\alpha\_{0}^{1}\right|\leq r\_{n},\left|c\_{0}^{1}\right|\leq r\_{n}\}}\right]\leq 2\mathbb{E}\left[\left\|X\right\|\_{\mathcal{H}\_{0}^{1}}^{2}\boldsymbol{1}\_{\{\left|\beta\_{0}^{1}\right|>r\_{n},\left|\alpha\_{0}^{1}\right|\leq r\_{n},\left|c\_{0}^{1}\right|\leq r\_{n}\}}\right], |  |

which converges to 0 by the dominated convergence theorem.

Terms 3 & 4.
The following inequality holds in this case

|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚ÄñXn‚àíX‚Äñ‚Ñã012\displaystyle\left\|X^{n}-X\right\|\_{\mathcal{H}\_{0}^{1}}^{2} | (ùüè{|Œ±01|>rn,|c01|‚â§rn}+ùüè|c01|>rn)\displaystyle\Big(\boldsymbol{1}\_{\{\left|\alpha\_{0}^{1}\right|>r\_{n},\left|c\_{0}^{1}\right|\leq r\_{n}\}}+\boldsymbol{1}\_{\left|c\_{0}^{1}\right|>r\_{n}}\Big) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§2‚Äã(‚ÄñXn‚Äñ‚Ñã012+‚ÄñX‚Äñ‚Ñã012)‚Äã(ùüè{|Œ±01|>rn,|c01|‚â§rn}+ùüè|c01|>rn).\displaystyle\leq 2\left(\left\|X^{n}\right\|\_{\mathcal{H}\_{0}^{1}}^{2}+\left\|X\right\|\_{\mathcal{H}\_{0}^{1}}^{2}\right)\Big(\boldsymbol{1}\_{\{\left|\alpha\_{0}^{1}\right|>r\_{n},\left|c\_{0}^{1}\right|\leq r\_{n}\}}+\boldsymbol{1}\_{\left|c\_{0}^{1}\right|>r\_{n}}\Big). |  |

Using the dominated convergence theorem, we have

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Äã[‚ÄñX‚Äñ‚Ñã012‚Äã(ùüè{|Œ±01|>rn,|c01|‚â§rn}+ùüè|c01|>rn})]‚Üín‚Üí‚àû0.\displaystyle\mathbb{E}\left[\left\|X\right\|\_{\mathcal{H}\_{0}^{1}}^{2}\Big(\boldsymbol{1}\_{\{\left|\alpha\_{0}^{1}\right|>r\_{n},\left|c\_{0}^{1}\right|\leq r\_{n}\}}+\boldsymbol{1}\_{\left|c\_{0}^{1}\right|>r\_{n}\}}\Big)\right]\xrightarrow[n\to\infty]{}0. |  |

Moreover, applying [Lemma¬†A.3](https://arxiv.org/html/2512.25017v1#A1.Thmtheorem3 "Lemma A.3 (‚Ñã‚ÇÄ¬π-boundedness of ùëã^ùëõ). ‚Ä£ A.2. Gradient ùúÉ estimates ‚Ä£ Appendix A Auxiliary results ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"), we arrive at

|  |  |  |
| --- | --- | --- |
|  | ‚ÄñXn‚Äñ‚Ñã012‚Äã(ùüè{|Œ±01|>rn}+ùüè{|c01|>rn})‚â§Cœà‚Äã(rn)d+8‚Äã(ùüè{|Œ±01|>rn}+ùüè{|c01|>rn}),\displaystyle\left\|X^{n}\right\|\_{\mathcal{H}\_{0}^{1}}^{2}\Big(\boldsymbol{1}\_{\{\left|\alpha\_{0}^{1}\right|>r\_{n}\}}+\boldsymbol{1}\_{\{\left|c\_{0}^{1}\right|>r\_{n}\}}\Big)\leq C\_{\psi}(r\_{n})^{d+8}\Big(\boldsymbol{1}\_{\{\left|\alpha\_{0}^{1}\right|>r\_{n}\}}+\boldsymbol{1}\_{\{\left|c\_{0}^{1}\right|>r\_{n}\}}\Big), |  |

which converges to 0 almost surely.
Therefore, using the dominated convergence theorem once again, we get

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Äã[‚ÄñXn‚Äñ‚Ñã012‚Äã(ùüè{|Œ±01|>rn}+ùüè{|c01|>rn})]‚â§ùîº‚Äã[|Œ±01|d+8‚Äãùüè{|Œ±01|>rn}+|c01|d+8‚Äãùüè{|c01|>rn}]‚Üín‚Üí‚àû0.‚àé\mathbb{E}\left[\left\|X^{n}\right\|\_{\mathcal{H}\_{0}^{1}}^{2}\Big(\boldsymbol{1}\_{\{\left|\alpha\_{0}^{1}\right|>r\_{n}\}}+\boldsymbol{1}\_{\{\left|c\_{0}^{1}\right|>r\_{n}\}}\Big)\right]\leq\mathbb{E}\Big[\left|\alpha\_{0}^{1}\right|^{d+8}\boldsymbol{1}\_{\{\left|\alpha\_{0}^{1}\right|>r\_{n}\}}+\left|c\_{0}^{1}\right|^{d+8}\boldsymbol{1}\_{\{\left|c\_{0}^{1}\right|>r\_{n}\}}\Big]\xrightarrow[n\to\infty]{}0.\qed |  |

###### Lemma A.6 (Œ∏\theta-Lipschitz continuity).

Let Œ∏,Œ∏‚Ä≤‚àà‚Ñù√ó‚Ñù√ó‚Ñùd\theta,\theta^{\prime}\in\mathbb{R}\times\mathbb{R}\times\mathbb{R}^{d}, then XnX^{n} is Lipschitz continuous in ‚Ñã01\mathcal{H}\_{0}^{1}, *i.e.* there exists a constant Cœà>0C\_{\psi}>0 such that

|  |  |  |
| --- | --- | --- |
|  | ‚ÄñXn‚Äã(Œ∏)‚àíXn‚Äã(Œ∏‚Ä≤)‚Äñ‚Ñã01‚â§Cœà‚Äã(rn)4+d2‚Äã|Œ∏‚àíŒ∏‚Ä≤|12.\displaystyle\left\|X^{n}(\theta)-X^{n}(\theta^{\prime})\right\|\_{\mathcal{H}\_{0}^{1}}\leq C\_{\psi}\left(r\_{n}\right)^{4+\frac{d}{2}}\left|\theta-\theta^{\prime}\right|^{\frac{1}{2}}. |  |

###### Proof.

As in the proof of [Lemma¬†A.3](https://arxiv.org/html/2512.25017v1#A1.Thmtheorem3 "Lemma A.3 (‚Ñã‚ÇÄ¬π-boundedness of ùëã^ùëõ). ‚Ä£ A.2. Gradient ùúÉ estimates ‚Ä£ Appendix A Auxiliary results ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"), we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚ÄñXn‚Äã(Œ∏)‚àíXn‚Äã(Œ∏‚Ä≤)‚Äñ‚Ñã01\displaystyle\left\|X^{n}(\theta)-X^{n}\left(\theta^{\prime}\right)\right\|\_{\mathcal{H}\_{0}^{1}} | ‚â§‚à•œà(Œ±^‚ãÖ+c^)‚àíœà(Œ±^‚Ä≤‚ãÖ+c^‚Ä≤)‚à•‚Ñã01\displaystyle\leq\left\|\psi\left(\hat{\alpha}\cdot+\hat{c}\right)-\psi\left(\hat{\alpha}^{\prime}\cdot+\hat{c}^{\prime}\right)\right\|\_{\mathcal{H}\_{0}^{1}} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +‚à•Œ≤^(‚ãÖ‚àáœà)(Œ±^‚ãÖ+c^)‚àíŒ≤^‚Ä≤‚ãÖ(‚àáœà)(Œ±^‚Ä≤‚ãÖ+c^‚Ä≤)‚à•‚Ñã01\displaystyle\qquad+\left\|\hat{\beta}\left(\cdot\nabla\psi\right)\left(\hat{\alpha}\cdot+\hat{c}\right)-\hat{\beta}^{\prime}\cdot\left(\nabla\psi\right)\left(\hat{\alpha}^{\prime}\cdot+\hat{c}^{\prime}\right)\right\|\_{\mathcal{H}\_{0}^{1}} |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | +‚à•Œ≤^(‚àáœà)(Œ±^‚ãÖ+c^)‚àíŒ≤^‚Ä≤(‚àáœà)(Œ±^‚Ä≤‚ãÖ+c^‚Ä≤)‚à•‚Ñã01.\displaystyle\qquad+\left\|\hat{\beta}\left(\nabla\psi\right)\left(\hat{\alpha}\cdot+\hat{c}\right)-\hat{\beta}^{\prime}\left(\nabla\psi\right)\left(\hat{\alpha}^{\prime}\cdot+\hat{c}^{\prime}\right)\right\|\_{\mathcal{H}\_{0}^{1}}. |  | (A.1) |

Let us recall that |Œ±^|,|Œ≤^|,|c^|‚â§rn\left|\hat{\alpha}\right|,|\hat{\beta}|,\left|\hat{c}\right|\leq r\_{n} and |Œ±^|‚â•rn‚àí1\left|\hat{\alpha}\right|\geq r\_{n}^{-1}.
Using that œà‚ààCc‚àû‚Äã(‚Ñùd)\psi\in C\_{c}^{\infty}\left(\mathbb{R}^{d}\right), and therefore Lipschitz, we get

|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚à´‚Ñùd|œà‚Äã(Œ±^‚Äãx+c^)‚àíœà‚Äã(Œ±^‚Ä≤‚Äãx+c^‚Ä≤)|2‚Äãdx\displaystyle\int\_{\mathbb{R}^{d}}\left|\psi\left(\hat{\alpha}x+\hat{c}\right)-\psi\left(\hat{\alpha}^{\prime}x+\hat{c}^{\prime}\right)\right|^{2}\mathrm{d}x | ‚â§‚à´‚ÑùdCœà‚Äã|(Œ±^‚àíŒ±^‚Ä≤)‚Äãx+c^‚àíc^‚Ä≤|‚Äã|œà‚Äã(Œ±^‚Äãx+c^)‚àíœà‚Äã(Œ±^‚Ä≤‚Äãx+c^‚Ä≤)|‚Äãdx\displaystyle\leq\int\_{\mathbb{R}^{d}}C\_{\psi}\left|\left(\hat{\alpha}-\hat{\alpha}^{\prime}\right)x+\hat{c}-\hat{c}^{\prime}\right|\left|\psi\left(\hat{\alpha}x+\hat{c}\right)-\psi\left(\hat{\alpha}^{\prime}x+\hat{c}^{\prime}\right)\right|\mathrm{d}x |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§Cœà‚Äã|Œ∏‚àíŒ∏‚Ä≤|‚Äã‚à´‚Ñùd(1+|x|)‚Äã(|œà‚Äã(Œ±^‚Äãx+c^)|+|œà‚Äã(Œ±^‚Ä≤‚Äãx+c^‚Ä≤)|)‚Äãdx.\displaystyle\leq C\_{\psi}\left|\theta-\theta^{\prime}\right|\int\_{\mathbb{R}^{d}}\left(1+\left|x\right|\right)\left(\left|\psi\left(\hat{\alpha}x+\hat{c}\right)\right|+\left|\psi\left(\hat{\alpha}^{\prime}x+\hat{c}^{\prime}\right)\right|\right)\mathrm{d}x. |  |

Using the change of variables y=Œ±^‚Äãx+c^y=\hat{\alpha}x+\hat{c}, we arrive at

|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚à´‚Ñùd(1+|x|)‚Äã|œà‚Äã(Œ±^‚Äãx+c^)|‚Äãdx\displaystyle\int\_{\mathbb{R}^{d}}\left(1+\left|x\right|\right)\left|\psi\left(\hat{\alpha}x+\hat{c}\right)\right|\mathrm{d}x | =|Œ±^|‚àíd‚Äã‚à´‚Ñùd|œà‚Äã(y)|‚Äãdy+|Œ±^|‚àíd‚àí1‚Äã‚à´‚Ñùd|y‚àíc^|‚Äã|œà‚Äã(y)|‚Äãùëëy\displaystyle=\left|\hat{\alpha}\right|^{-d}\int\_{\mathbb{R}^{d}}\left|\psi(y)\right|\mathrm{d}y+\left|\hat{\alpha}\right|^{-d-1}\int\_{\mathbb{R}^{d}}\left|y-\hat{c}\right|\left|\psi\left(y\right)\right|dy |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§Cœà‚Äã(rnd+rnd+2),\displaystyle\leq C\_{\psi}\Big(r\_{n}^{d}+r\_{n}^{d+2}\Big), |  |

and we obtain the bound

|  |  |  |
| --- | --- | --- |
|  | ‚à´‚Ñùd|œà‚Äã(Œ±^‚Äãx+c^)‚àíœà‚Äã(Œ±^‚Ä≤‚Äãx+c^‚Ä≤)|2‚Äãdx‚â§Cœà‚Äã|Œ∏‚àíŒ∏‚Ä≤|‚Äãrnd+2.\int\_{\mathbb{R}^{d}}\left|\psi\left(\hat{\alpha}x+\hat{c}\right)-\psi\left(\hat{\alpha}^{\prime}x+\hat{c}^{\prime}\right)\right|^{2}\mathrm{d}x\leq C\_{\psi}\left|\theta-\theta^{\prime}\right|r\_{n}^{d+2}. |  |

Analogously, using that ‚àáœà\nabla\psi is Lipschitz as well, we have that

|  |  |  |
| --- | --- | --- |
|  | ‚à´‚Ñùd|Œ±^‚Äã‚àáœà‚Äã(Œ±^‚Äãx+c^)‚àíŒ±^‚Ä≤‚Äã‚àáœà‚Äã(Œ±^‚Ä≤‚Äãx+c^‚Ä≤)|2‚Äãdx‚â§2‚Äã‚à´‚Ñùd|Œ±^‚àíŒ±^‚Ä≤|2‚Äã|‚àáœà‚Äã(Œ±^‚Äãx+c^)|2‚Äãdx+2‚Äã‚à´‚Ñùd|Œ±^‚Ä≤|2‚Äã|‚àáœà‚Äã(Œ±^‚Äãx+c^)‚àí‚àáœà‚Äã(Œ±^‚Ä≤‚Äãx+c^‚Ä≤)|2‚Äãdx‚â§4‚Äãrn‚Äã|Œ∏‚àíŒ∏‚Ä≤|‚Äã‚à´‚Ñùd|‚àáœà‚Äã(Œ±^‚Äãx+c^)|2‚Äãdx+2‚Äãrn2‚Äã‚à´‚Ñùd|‚àáœà‚Äã(Œ±^‚Äãx+c^)‚àí‚àáœà‚Äã(Œ±^‚Ä≤‚Äãx+c^‚Ä≤)|2‚Äãdx‚â§Cœà‚Äã|Œ∏‚àíŒ∏‚Ä≤|‚Äã(rn)d+4.\int\_{\mathbb{R}^{d}}\left|\hat{\alpha}\nabla\psi\left(\hat{\alpha}x+\hat{c}\right)-\hat{\alpha}^{\prime}\nabla\psi\left(\hat{\alpha}^{\prime}x+\hat{c}^{\prime}\right)\right|^{2}\mathrm{d}x\\ \leq 2\int\_{\mathbb{R}^{d}}\left|\hat{\alpha}-\hat{\alpha}^{\prime}\right|^{2}\left|\nabla\psi\left(\hat{\alpha}x+\hat{c}\right)\right|^{2}\mathrm{d}x+2\int\_{\mathbb{R}^{d}}\left|\hat{\alpha}^{\prime}\right|^{2}\left|\nabla\psi\left(\hat{\alpha}x+\hat{c}\right)-\nabla\psi\left(\hat{\alpha}^{\prime}x+\hat{c}^{\prime}\right)\right|^{2}\mathrm{d}x\\ \leq 4r\_{n}\left|\theta-\theta^{\prime}\right|\int\_{\mathbb{R}^{d}}\left|\nabla\psi\left(\hat{\alpha}x+\hat{c}\right)\right|^{2}\mathrm{d}x+2r\_{n}^{2}\int\_{\mathbb{R}^{d}}\left|\nabla\psi\left(\hat{\alpha}x+\hat{c}\right)-\nabla\psi\left(\hat{\alpha}^{\prime}x+\hat{c}^{\prime}\right)\right|^{2}\mathrm{d}x\\ \leq C\_{\psi}\left|\theta-\theta^{\prime}\right|(r\_{n})^{d+4}. |  |

Therefore, we arrive at the following bound for the ‚Ñã01\mathcal{H}\_{0}^{1}-norm of this term

|  |  |  |
| --- | --- | --- |
|  | ‚à•œà(Œ±^‚ãÖ+c^)‚àíœà(Œ±^‚Ä≤‚ãÖ+c^‚Ä≤)‚à•‚Ñã01‚â§Cœà|Œ∏‚àíŒ∏‚Ä≤|1/2(rn)d/2+2.\left\|\psi\left(\hat{\alpha}\cdot+\hat{c}\right)-\psi\left(\hat{\alpha}^{\prime}\cdot+\hat{c}^{\prime}\right)\right\|\_{\mathcal{H}\_{0}^{1}}\leq C\_{\psi}\left|\theta-\theta^{\prime}\right|^{1/2}(r\_{n})^{d/2+2}. |  |

We can similarly estimate the other two terms in ([A.2](https://arxiv.org/html/2512.25017v1#A1.Ex58 "Proof. ‚Ä£ A.2. Gradient ùúÉ estimates ‚Ä£ Appendix A Auxiliary results ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")), and we get that

|  |  |  |
| --- | --- | --- |
|  | ‚à•Œ≤^(‚ãÖ‚àáœà)(Œ±^‚ãÖ+c^)‚àíŒ≤^‚Ä≤‚ãÖ(‚àáœà)(Œ±^‚Ä≤‚ãÖ+c^‚Ä≤)‚à•‚Ñã01\displaystyle\left\|\hat{\beta}\left(\cdot\nabla\psi\right)\left(\hat{\alpha}\cdot+\hat{c}\right)-\hat{\beta}^{\prime}\cdot\left(\nabla\psi\right)\left(\hat{\alpha}^{\prime}\cdot+\hat{c}^{\prime}\right)\right\|\_{\mathcal{H}\_{0}^{1}} |  |
|  |  |  |
| --- | --- | --- |
|  | +‚à•Œ≤^(‚àáœà)(Œ±^‚ãÖ+c^)‚àíŒ≤^‚Ä≤(‚àáœà)(Œ±^‚Ä≤‚ãÖ+c^‚Ä≤)‚à•‚Ñã01‚â§Cœà|Œ∏‚àíŒ∏‚Ä≤|1/2(rn)d/2+4\displaystyle\qquad+\left\|\hat{\beta}\left(\nabla\psi\right)\left(\hat{\alpha}\cdot+\hat{c}\right)-\hat{\beta}^{\prime}\left(\nabla\psi\right)\left(\hat{\alpha}^{\prime}\cdot+\hat{c}^{\prime}\right)\right\|\_{\mathcal{H}\_{0}^{1}}\leq C\_{\psi}\left|\theta-\theta^{\prime}\right|^{1/2}(r\_{n})^{d/2+4} |  |

Concluding, we have

|  |  |  |
| --- | --- | --- |
|  | ‚ÄñXn‚Äã(Œ∏)‚àíXn‚Äã(Œ∏‚Ä≤)‚Äñ‚Ñã01‚â§Cœà‚Äã(rn)4+d2‚Äã|Œ∏‚àíŒ∏‚Ä≤|12.‚àé\left\|X^{n}(\theta)-X^{n}\left(\theta^{\prime}\right)\right\|\_{\mathcal{H}\_{0}^{1}}\leq C\_{\psi}\left(r\_{n}\right)^{4+\frac{d}{2}}\left|\theta-\theta^{\prime}\right|^{\frac{1}{2}}.\qed |  |

###### Lemma A.7.

Assume that the neural network satisfies [Assumption¬†(NNI)](https://arxiv.org/html/2512.25017v1#Thmassumption5 "Assumption (NNI). ‚Ä£ 4.1. Convergence of the trained neural network ‚Ä£ 4. Convergence of the training error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs").
Then, for t‚â•0t\geq 0, we have

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Äã[|Œ∏ti,n‚àíŒ∏0i,n|]‚â§Cœà‚Äãt‚ÄãnŒ¥‚àí1‚Äã(rn)d2+4.\displaystyle\mathbb{E}\left[\left|\theta\_{t}^{i,n}-\theta\_{0}^{i,n}\right|\right]\leq C\_{\psi}tn^{\delta-1}\left(r\_{n}\right)^{\frac{d}{2}+4}. |  |

###### Remark A.8.

This lemma yields that, when nn is large, then the value Œ∏ti,n\theta\_{t}^{i,n} of the evolution of the parameters of the neural network does not differ significantly from its initial value Œ∏0i,n\theta\_{0}^{i,n}.

###### Proof.

Recalling ([4.1](https://arxiv.org/html/2512.25017v1#S4.Ex5 "4.1. Convergence of the trained neural network ‚Ä£ 4. Convergence of the training error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs")), we have

|  |  |  |
| --- | --- | --- |
|  | Œ∏ti,n‚àíŒ∏0i,n=‚àíŒ∑n‚Äã‚à´0t‚ü®ùíü‚ÄãIk‚Äã(Vsn),‚àáŒ∏iVsn‚ü©‚Ñã01‚Äãds,\displaystyle\theta\_{t}^{i,n}-\theta\_{0}^{i,n}=-\eta\_{n}\int\_{0}^{t}\left\langle\mathcal{D}I^{k}\left(V\_{s}^{n}\right),\nabla\_{\theta^{i}}V\_{s}^{n}\right\rangle\_{\mathcal{H}\_{0}^{1}}\mathrm{d}s, |  |

thus, their squared difference equals

|  |  |  |  |
| --- | --- | --- | --- |
|  | |Œ∏ti,n‚àíŒ∏0i,n|2=\displaystyle\left|\theta\_{t}^{i,n}-\theta\_{0}^{i,n}\right|^{2}= | |Œ∑n|2‚Äã|‚à´0t‚ü®ùíü‚ÄãIk‚Äã(Vsn),‚àáŒ∏iVsn‚ü©‚Ñã01‚Äãds|2‚â§n4‚ÄãŒ¥‚àí2‚Äãt‚Äã‚à´0t‚ü®ùíü‚ÄãIk‚Äã(Vsn),‚àáŒ∏iVsn‚ü©‚Ñã012‚Äãds.\displaystyle\left|\eta\_{n}\right|^{2}\left|\int\_{0}^{t}\left\langle\mathcal{D}I^{k}\left(V\_{s}^{n}\right),\nabla\_{\theta^{i}}V\_{s}^{n}\right\rangle\_{\mathcal{H}\_{0}^{1}}\mathrm{d}s\right|^{2}\leq n^{4\delta-2}t\int\_{0}^{t}\left\langle\mathcal{D}I^{k}\left(V\_{s}^{n}\right),\nabla\_{\theta^{i}}V\_{s}^{n}\right\rangle^{2}\_{\mathcal{H}\_{0}^{1}}\mathrm{d}s. |  |

Using LABEL:{lem:con\_frechet}, we get

|  |  |  |
| --- | --- | --- |
|  | |‚ü®ùíü‚ÄãIk‚Äã(Vsn),‚àáŒ∏iVsn‚ü©‚Ñã01|2‚â§K‚Äã(1+‚ÄñVsn‚Äñ‚Ñã012)‚Äã‚Äñ‚àáŒ∏iVsn‚Äñ‚Ñã012.\displaystyle\left|\left\langle\mathcal{D}I^{k}\left(V\_{s}^{n}\right),\nabla\_{\theta^{i}}V\_{s}^{n}\right\rangle\_{\mathcal{H}\_{0}^{1}}\right|^{2}\leq K\left(1+\left\|V^{n}\_{s}\right\|^{2}\_{\mathcal{H}\_{0}^{1}}\right)\left\|\nabla\_{\theta^{i}}V\_{s}^{n}\right\|^{2}\_{\mathcal{H}\_{0}^{1}}. |  |

Using that

|  |  |  |
| --- | --- | --- |
|  | |‚àáŒ∏iVn‚Äã(Œ∏tn;x)|=1nŒ¥‚Äã|ùí≥n‚Äã(Œ∏ti,n;x)|,\left|\nabla\_{\theta^{i}}V^{n}\left(\theta^{n}\_{t};x\right)\right|=\frac{1}{n^{\delta}}\left|\mathcal{X}^{n}\left(\theta^{i,n}\_{t};x\right)\right|, |  |

and applying [Lemma¬†A.3](https://arxiv.org/html/2512.25017v1#A1.Thmtheorem3 "Lemma A.3 (‚Ñã‚ÇÄ¬π-boundedness of ùëã^ùëõ). ‚Ä£ A.2. Gradient ùúÉ estimates ‚Ä£ Appendix A Auxiliary results ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"), we get

|  |  |  |
| --- | --- | --- |
|  | ‚Äñ‚àáŒ∏iVtn‚Äñ‚Ñã012‚â§Cœàn2‚ÄãŒ¥‚Äã(rn)d+8.\left\|\nabla\_{\theta^{i}}V\_{t}^{n}\right\|^{2}\_{\mathcal{H}\_{0}^{1}}\leq\frac{C\_{\psi}}{n^{2\delta}}\left(r\_{n}\right)^{d+8}. |  |

Moreover, from [Lemma¬†A.2](https://arxiv.org/html/2512.25017v1#A1.Thmtheorem2 "Lemma A.2. ‚Ä£ A.1. Functional inequalities and norm estimates ‚Ä£ Appendix A Auxiliary results ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"), we have

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Äã[‚ÄñVtn‚Äñ‚Ñã012]‚â§Cœà.\mathbb{E}\left[\left\|V^{n}\_{t}\right\|^{2}\_{\mathcal{H}\_{0}^{1}}\right]\leq C\_{\psi}. |  |

Hence, we can bound the norm of the square difference by

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Äã[|Œ∏ti,n‚àíŒ∏0i,n|2]‚â§Cœà‚Äãt‚Äãn2‚Äã(Œ¥‚àí1)‚Äã(rn)d+8‚Äã‚à´0tùîº‚Äã[‚ÄñVsn‚Äñ‚Ñã012+1]‚Äãds‚â§Cœà‚Äãt2‚Äãn2‚Äã(Œ¥‚àí1)‚Äã(rn)d+8.\mathbb{E}\left[\left|\theta\_{t}^{i,n}-\theta\_{0}^{i,n}\right|^{2}\right]\leq C\_{\psi}tn^{2\left(\delta-1\right)}\left(r\_{n}\right)^{d+8}\int\_{0}^{t}\mathbb{E}\left[\left\|V^{n}\_{s}\right\|^{2}\_{\mathcal{H}\_{0}^{1}}+1\right]\mathrm{d}s\leq C\_{\psi}t^{2}n^{2\left(\delta-1\right)}\left(r\_{n}\right)^{d+8}. |  |

The result follows now using Jensen‚Äôs inequality.
‚àé

### A.3. Examples

This final appendix contains certain details in order to verify that the option pricing PDEs of [Examples¬†2.3](https://arxiv.org/html/2512.25017v1#S2.Thmtheorem3 "Example 2.3 (Option pricing PDEs). ‚Ä£ 2. Deep gradient flow methods for PDEs ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs") and¬†[2.4](https://arxiv.org/html/2512.25017v1#S2.Thmtheorem4 "Example 2.4 (Option pricing PIDEs). ‚Ä£ 2. Deep gradient flow methods for PDEs ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs") indeed satisfy [Assumptions¬†(CON)](https://arxiv.org/html/2512.25017v1#Thmassumption1 "Assumption (CON). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"), [(G√Ö)](https://arxiv.org/html/2512.25017v1#Thmassumption2 "Assumption (G√Ö). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"), [(LIP)](https://arxiv.org/html/2512.25017v1#Thmassumption4 "Assumption (LIP). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs") and¬†[(SA)](https://arxiv.org/html/2512.25017v1#Thmassumption3 "Assumption (SA). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs").

Starting from the Black‚ÄìScholes PDE of [Example¬†2.3](https://arxiv.org/html/2512.25017v1#S2.Thmtheorem3 "Example 2.3 (Option pricing PDEs). ‚Ä£ 2. Deep gradient flow methods for PDEs ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"), we have that

|  |  |  |
| --- | --- | --- |
|  | ‚Ñí‚Äãu=‚àíœÉ22‚Äã‚àÇ2u‚àÇx2+r‚Äãu¬†and¬†F‚Äã(u)=(œÉ22‚àír)‚Äã‚àÇu‚àÇx,\displaystyle\mathcal{L}u=-\frac{\sigma^{2}}{2}\frac{\partial^{2}u}{\partial x^{2}}+ru\quad\text{ and }\quad F(u)=\left(\frac{\sigma^{2}}{2}-r\right)\frac{\partial u}{\partial x}, |  |

and the energy functional takes the form

|  |  |  |
| --- | --- | --- |
|  | Ik‚Äã(u)=12‚Äã‚Äñu‚àíUk‚àí1‚ÄñL22+h2‚Äã‚à´‚Ñù{œÉ22‚Äã(‚àÇu‚àÇx)2+r‚Äãu2}‚Äãdx+h‚Äã‚à´‚ÑùF‚Äã(Uk‚àí1)‚Äãu‚Äãdx.I^{k}(u)=\frac{1}{2}\left\|u-U^{k-1}\right\|^{2}\_{L^{2}}+\frac{h}{2}\int\_{\mathbb{R}}\Big\{\frac{\sigma^{2}}{2}\left(\frac{\partial u}{\partial x}\right)^{2}+ru^{2}\Big\}\mathrm{d}x+h\int\_{\mathbb{R}}F\left(U^{k-1}\right)u\mathrm{d}x. |  |

[Assumption¬†(SA)](https://arxiv.org/html/2512.25017v1#Thmassumption3 "Assumption (SA). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs") is obviously satisfied.
Using the triangle and the Cauchy‚ÄìSchwarz inequalities, following some straightforward calculations, we arrive at

|  |  |  |
| --- | --- | --- |
|  | |‚ü®‚Ñí‚Äãu,v‚ü©‚Ñã‚àí1,‚Ñã01|‚â§(|œÉ22|+|r|)‚Äã‚Äñu‚Äñ‚Ñã01‚Äã‚Äñv‚Äñ‚Ñã01¬†and¬†‚ÄñF‚Äã(u)‚ÄñL2‚â§|œÉ22‚àír|‚Äã‚Äñu‚Äñ‚Ñã01.\displaystyle\left|\left\langle\mathcal{L}u,v\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}}\right|\leq\left(\left|\frac{\sigma^{2}}{2}\right|+\left|r\right|\right)\left\|u\right\|\_{\mathcal{H}\_{0}^{1}}\left\|v\right\|\_{\mathcal{H}\_{0}^{1}}\quad\text{ and }\quad\left\|F(u)\right\|\_{L^{2}}\leq\left|\frac{\sigma^{2}}{2}-r\right|\left\|u\right\|\_{\mathcal{H}\_{0}^{1}}. |  |

Hence, [Assumption¬†(CON)](https://arxiv.org/html/2512.25017v1#Thmassumption1 "Assumption (CON). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs") is satisfied with M=|œÉ22|+|r|M=\left|\frac{\sigma^{2}}{2}\right|+\left|r\right|.
Moreover, we have that

|  |  |  |
| --- | --- | --- |
|  | ‚ü®‚Ñí‚Äãu,u‚ü©‚Ñã‚àí1,‚Ñã01‚â•(œÉ22+r)‚Äã‚Äñu‚Äñ‚Ñã012,\left\langle\mathcal{L}u,u\right\rangle\_{\mathcal{H}^{-1},\mathcal{H}\_{0}^{1}}\geq\left(\frac{\sigma^{2}}{2}+r\right)\left\|u\right\|\_{\mathcal{H}\_{0}^{1}}^{2}, |  |

therefore [Assumption¬†(G√Ö)](https://arxiv.org/html/2512.25017v1#Thmassumption2 "Assumption (G√Ö). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs") is satisfied with Œª1=œÉ22+r>0\lambda\_{1}=\frac{\sigma^{2}}{2}+r>0 and Œª2=0\lambda\_{2}=0.
[Assumption¬†(LIP)](https://arxiv.org/html/2512.25017v1#Thmassumption4 "Assumption (LIP). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs") follows by similar computations.

Turning our attention to the multi-dimensional Merton model of [Example¬†2.4](https://arxiv.org/html/2512.25017v1#S2.Thmtheorem4 "Example 2.4 (Option pricing PIDEs). ‚Ä£ 2. Deep gradient flow methods for PDEs ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs"), we only need to show that the function FF, which now contains the integro-differential operator stemming from the jumps of the dynamics, still satisfies [Assumption¬†(CON)](https://arxiv.org/html/2512.25017v1#Thmassumption1 "Assumption (CON). ‚Ä£ 3. Convergence of the approximation error ‚Ä£ Convergence of the generalization error for Deep Gradient Flow Methods for PDEs").
Let us start with the integral operator, denoted by FŒΩF\_{\nu}, then we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚ÄñFŒΩ‚Äã(u)‚ÄñL22\displaystyle\left\|F\_{\nu}(u)\right\|\_{L^{2}}^{2} | ‚â§‚à´‚Ñùd|Œª‚Äã‚à´‚Ñùd(u‚Äã(x‚Äãez)‚àíu‚Äã(x))‚ÄãŒΩ‚Äã(d‚Äãz)|2‚Äãdx\displaystyle\leq\int\_{\mathbb{R}^{d}}\left|\lambda\int\_{\mathbb{R}^{d}}\left(u\left(x\mathrm{e}^{z}\right)-u\left(x\right)\right)\nu\left(\mathrm{d}z\right)\right|^{2}\mathrm{d}x |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§2‚ÄãŒª‚Äã‚à´‚Ñùd‚à´‚Ñùd|u‚Äã(x‚Äãez)|2‚ÄãŒΩ‚Äã(d‚Äãz)‚Äãdx+2‚ÄãŒª‚Äã‚à´‚Ñùd|u‚Äã(x)|2‚Äãdx\displaystyle\leq 2\lambda\int\_{\mathbb{R}^{d}}\int\_{\mathbb{R}^{d}}\left|u\left(x\mathrm{e}^{z}\right)\right|^{2}\nu\left(\mathrm{d}z\right)\mathrm{d}x+2\lambda\int\_{\mathbb{R}^{d}}\left|u\left(x\right)\right|^{2}\mathrm{d}x |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =2‚ÄãŒª(2‚ÄãœÄ)d‚Äã‚à´‚Ñùd‚à´‚Ñùd|u‚Äã(x‚Äãez)|2‚Äãe‚àíz22‚Äãdz‚Äãdx+2‚ÄãŒª‚Äã‚Äñu‚ÄñL22\displaystyle=\frac{2\lambda}{(\sqrt{2\pi})^{d}}\int\_{\mathbb{R}^{d}}\int\_{\mathbb{R}^{d}}\left|u\left(x\mathrm{e}^{z}\right)\right|^{2}\mathrm{e}^{-\frac{z^{2}}{2}}\mathrm{d}z\mathrm{d}x+2\lambda\left\|u\right\|\_{L^{2}}^{2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =2‚ÄãŒª(2‚ÄãœÄ)d‚Äã‚à´‚Ñùd‚à´‚Ñùd|u‚Äã(x‚Äãez)|2‚Äãe‚àíz22‚Äãdx‚Äãdz+2‚ÄãŒª‚Äã‚Äñu‚ÄñL22\displaystyle=\frac{2\lambda}{(\sqrt{2\pi})^{d}}\int\_{\mathbb{R}^{d}}\int\_{\mathbb{R}^{d}}\left|u\left(x\mathrm{e}^{z}\right)\right|^{2}\mathrm{e}^{-\frac{z^{2}}{2}}\mathrm{d}x\mathrm{d}z+2\lambda\left\|u\right\|\_{L^{2}}^{2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =2‚ÄãŒª(2‚ÄãœÄ)d‚Äã‚à´‚Ñùd‚à´‚Ñùd|u‚Äã(y)|2‚Äãe‚àí2‚Äãz‚àíz22‚Äãdy‚Äãdz+2‚ÄãŒª‚Äã‚Äñu‚ÄñL22\displaystyle=\frac{2\lambda}{(\sqrt{2\pi})^{d}}\int\_{\mathbb{R}^{d}}\int\_{\mathbb{R}^{d}}\left|u\left(y\right)\right|^{2}\mathrm{e}^{\frac{-2z-z^{2}}{2}}\mathrm{d}y\mathrm{d}z+2\lambda\left\|u\right\|\_{L^{2}}^{2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =2‚ÄãŒª‚Äã(e+1)‚Äã‚Äñu‚ÄñL22‚â§2‚ÄãŒª‚Äã(e+1)‚Äã‚Äñu‚Äñ‚Ñã012,\displaystyle=2\lambda\left(\mathrm{e}+1\right)\left\|u\right\|\_{L^{2}}^{2}\leq 2\lambda\left(\mathrm{e}+1\right)\left\|u\right\|\_{\mathcal{H}\_{0}^{1}}^{2}, |  |

where we have used the properties of the normal distribution, Fubini‚Äôs theorem for the fourth step, and the change of variables y=x‚Äãezy=x\mathrm{e}^{z} for the fifth step.
Therefore,

|  |  |  |
| --- | --- | --- |
|  | ‚ÄñF‚Äã(u)‚ÄñL2‚â§|ùíÉ|‚Äã‚Äñ‚àáu‚ÄñL2+Œª‚Äã‚ÄñFŒΩ‚Äã(u)‚ÄñL2‚â§2‚ÄãŒª‚Äã(C+e+1)‚Äã‚Äñu‚Äñ‚Ñã01,\displaystyle\left\|F(u)\right\|\_{L^{2}}\leq\left|\boldsymbol{b}\right|\left\|\nabla u\right\|\_{L^{2}}+\lambda\left\|F\_{\nu}(u)\right\|\_{L^{2}}\leq 2\lambda\left(C+\mathrm{e}+1\right)\left\|u\right\|\_{\mathcal{H}\_{0}^{1}}, |  |

which implies the result.

## References

* Akrivis et¬†al. [1999]

  G.¬†Akrivis, M.¬†Crouzeix, and C.¬†Makridakis.
  Implicit-explicit multistep methods for quasilinear parabolic
  equations.
  *Numerische Mathematik*, 82:521‚Äì541, 1999.
* Black and Scholes [1973]

  F.¬†Black and M.¬†Scholes.
  The pricing of options and corporate liabilities.
  *Journal of Political Economy*, 81(3):637‚Äì654, 1973.
* Bruna et¬†al. [2024]

  J.¬†Bruna, B.¬†Peherstorfer, and E.¬†Vanden-Eijnden.
  Neural carmoalerkin schemes with active learning for
  high-dimensional evolution equations.
  *Journal of Computational Physics*, 496:112588, 2024.
* Carmona and Zeng [2024]

  R.¬†A. Carmona and C.¬†Zeng.
  Leveraging the turnpike effect for mean field games numerics.
  *IEEE Open Journal of Control Systems*, 3:389‚Äì404,
  2024.
* Dondl et¬†al. [2022]

  P.¬†Dondl, J.¬†M√ºller, and M.¬†Zeinhofer.
  Uniform convergence guarantees for the deep Ritz method for
  nonlinear problems.
  *Advances in Continuous and Discrete Models*, 2022(1):49, 2022.
* E and Yu [2018]

  W.¬†E and B.¬†Yu.
  The deep Ritz method: A deep learning-based numerical algorithm
  for solving variational problems.
  *Communications in Mathematics and Statistics*, 6:1‚Äì12, 2018.
* Gazoulis et¬†al. [2025]

  D.¬†Gazoulis, I.¬†Gkanis, and C.¬†G. Makridakis.
  On the stability and convergence of physics informed neural networks.
  *IMA Journal of Numerical Analysis*, 2025.
  (advance article).
* Georgoulis et¬†al. [2023]

  E.¬†Georgoulis, M.¬†Loulakis, and A.¬†Tsiourvas.
  Discrete gradient flow approximations of high dimensional evolution
  partial differential equations via deep neural networks.
  *Communications in Nonlinear Science and Numerical Simulation*,
  117:106893, 2023.
* Georgoulis et¬†al. [2024]

  E.¬†Georgoulis, A.¬†Papapantoleon, and C.¬†Smaragdakis.
  A deep implicit-explicit minimizing movement method for option
  pricing in jump-diffusion models.
  Preprint, arXiv:2401.06740, 2024.
* Goodfellow et¬†al. [2016]

  I.¬†Goodfellow, Y.¬†Bengio, and A.¬†Courville.
  *Deep Learning*.
  MIT Press, 2016.
* Heston [1993]

  S.¬†L. Heston.
  A closed-form solution for options with stochastic volatility with
  applications to bond and currency options.
  *The Review of Financial Studies*, 6(2):327‚Äì343, 1993.
* Hilber et¬†al. [2013]

  N.¬†Hilber, O.¬†Reichmann, C.¬†Schwab, and C.¬†Winter.
  *Computational Methods for Quantitative Finance: Finite Element
  Methods for Derivative Pricing*.
  Springer Science & Business Media, 2013.
* Hornik [1991]

  K.¬†Hornik.
  Approximation capabilities of multilayer feedforward networks.
  *Neural Networks*, 4(2):251‚Äì257, 1991.
* Jentzen et¬†al. [2025]

  A.¬†Jentzen, B.¬†Kuckuck, and P.¬†von Wurstemberger.
  Mathematical Introduction to Deep Learning: Methods,
  Implementations, and Theory, 2025.
  Available at arXiv:2310.20360.
* Jiang et¬†al. [2023]

  D.¬†Jiang, J.¬†Sirignano, and S.¬†Cohen.
  Global convergence of deep Galerkin and PINNs methods for solving
  partial differential equations.
  Preprint, arXiv:2305.06000, 2023.
* Jiao et¬†al. [2024]

  Y.¬†Jiao, Y.¬†Lai, Y.¬†Lo, Y.¬†Wang, and Y.¬†Yang.
  Error analysis of deep Ritz methods for elliptic equations.
  *Analysis and Applications*, 22(1):57‚Äì87,
  2024.
* Kharazmi et¬†al. [2021]

  E.¬†Kharazmi, Z.¬†Zhang, and G.¬†E. Karniadakis.
  Variational physics-informed neural networks for solving partial
  differential equations.
  *Journal of Computational Physics*, 418:109629, 2021.
* Lagaris et¬†al. [1998]

  I.¬†E. Lagaris, A.¬†Likas, and D.¬†I. Fotiadis.
  Artificial neural networks for solving ordinary and partial
  differential equations.
  *IEEE Transactions on Neural Networks*, 9(5):987‚Äì1000, 1998.
* Lagaris et¬†al. [2000]

  I.¬†E. Lagaris, A.¬†C. Likas, and D.¬†G. Papageorgiou.
  Neural-network methods for boundary value problems with irregular
  boundaries.
  *IEEE Transactions on Neural Networks*, 11(5):1041‚Äì1049, 2000.
* Liao and Ming [2021]

  Y.¬†Liao and P.¬†Ming.
  Deep Nitsche method: deep Ritz method with essential boundary
  conditions.
  *Communications in Computational Physics*, 29(5):1365‚Äì1384, 2021.
* Loulakis and
  Makridakis [2023]

  M.¬†Loulakis and C.¬†G. Makridakis.
  A new approach to generalisation error of machine learning
  algorithms: Estimates and convergence, 2023.
  Preprint, arXiv:2306.13784.
* Mishra and Molinaro [2022]

  S.¬†Mishra and R.¬†Molinaro.
  Estimates on the generalization error of physics-informed neural
  networks for approximating PDEs.
  *IMA Journal of Numerical Analysis*, 42(2):981‚Äì1022, 2022.
* Pang et¬†al. [2019]

  G.¬†Pang, L.¬†Lu, and G.¬†E. Karniadakis.
  fPINNs: Fractional physics-informed neural networks.
  *SIAM Journal on Scientific Computing*, 41(4):A2603‚ÄìA2626, 2019.
* Papapantoleon and Rou [2025]

  A.¬†Papapantoleon and J.¬†Rou.
  A time-stepping deep gradient flow method for option pricing in
  (rough) diffusion models.
  *Quantitative Finance*, 25:2009‚Äì2020, 2025.
* Park et¬†al. [2023]

  M.¬†S. Park, C.¬†Kim, H.¬†Son, and H.¬†J. Hwang.
  The deep minimizing movement scheme.
  *Journal of Computational Physics*, 494:112518, 2023.
* Raissi and Karniadakis [2018]

  M.¬†Raissi and G.¬†E. Karniadakis.
  Hidden physics models: Machine learning of nonlinear partial
  differential equations.
  *Journal of Computational Physics*, 357:125‚Äì141,
  2018.
* Raissi et¬†al. [2019]

  M.¬†Raissi, P.¬†Perdikaris, and G.¬†E. Karniadakis.
  Physics-informed neural networks: a deep learning framework for
  solving forward and inverse problems involving nonlinear partial differential
  equations.
  *Journal of Computational Physics*, 378:686‚Äì707,
  2019.
* Shin et¬†al. [2020]

  Y.¬†Shin, J.¬†Darbon, and G.¬†E. Karniadakis.
  On the convergence of physics informed neural networks for linear
  second-order elliptic and parabolic type pdes.
  *Communications in Computational Physics*, 28(5):2042‚Äì2074, 2020.
* Sirignano and Spiliopoulos [2018]

  J.¬†Sirignano and K.¬†Spiliopoulos.
  DGM: A deep learning algorithm for solving partial differential
  equations.
  *Journal of Computational Physics*, 375:1339‚Äì1364,
  2018.
* van Neerven [2022]

  J.¬†van Neerven.
  *Functional Analysis*.
  Cambridge University Press, 2022.
* Yang et¬†al. [2021]

  L.¬†Yang, X.¬†Meng, and G.¬†E. Karniadakis.
  B-pinns: Bayesian physics-informed neural networks for forward and
  inverse pde problems under uncertainty.
  *Journal of Computational Physics*, 425:109913, 2021.
* Zhang et¬†al. [2020]

  J.¬†Zhang, T.¬†He, S.¬†Sra, and A.¬†Jadbabaie.
  Why gradient clipping accelerates training: A theoretical
  justification for adaptivity.
  In *International Conference on Learning Representations*, 2020.