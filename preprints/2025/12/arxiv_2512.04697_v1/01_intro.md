---
authors:
- Yijie Huang
- Mengge Li
- Xiang Yu
- Zhou Zhou
doc_id: arxiv:2512.04697v1
family_id: arxiv:2512.04697
is_current: true
taxonomy:
  alpha_families: []
  asset_classes: []
  horizons: []
  themes: []
title: Continuous-time reinforcement learning for optimal switching over multiple
  regimes
url_abs: http://arxiv.org/abs/2512.04697v1
url_html: https://arxiv.org/html/2512.04697v1
venue: arXiv q-fin
version: 1
year: 2025
---


Yijie Huang
Department of Applied Mathematics, The Hong Kong Polytechnic University, Kowloon, Hong Kong. Email:<yijie.huang@polyu.edu.hk>
â€ƒâ€ƒ
Mengge Li
Department of Applied Mathematics, The Hong Kong Polytechnic University, Kowloon, Hong Kong. Email:<meng-ge.li@polyu.edu.hk>
â€ƒâ€ƒ
Xiang Yu
Department of Applied Mathematics, The Hong Kong Polytechnic University, Kowloon, Hong Kong. Email:<xiang.yu@polyu.edu.hk>
â€ƒâ€ƒ
Zhou Zhou
School of Mathematics and Statistics, University of Sydney, Sydney, Australia. Email:<zhou.zhou@sydney.edu.au>

###### Abstract

This paper studies the continuous-time reinforcement learning (RL) for optimal switching problems across multiple regimes. We consider a type of exploratory formulation under entropy regularization where the agent randomizes both the timing of switches and the selection of regimes through the generator matrix of an associated continuous-time finite-state Markov chain. We establish the well-posedness of the associated system of Hamilton-Jacobi-Bellman (HJB) equations and provide a characterization of the optimal policy. The policy improvement and the convergence of the policy iterations are rigorously established by analyzing the system of equations. We also show the convergence of the value function in the exploratory formulation towards the value function in the classical formulation as the temperature parameter vanishes. Finally, a reinforcement learning algorithm is devised and implemented by invoking the policy evaluation based on the martingale characterization. Our numerical examples with the aid of neural networks illustrate the effectiveness of the proposed RL algorithm.

Keywords: Optimal regime switching, multiple regimes, continuous-time reinforcement learning, system of HJB equations, policy improvement, policy iteration convergence

## 1 Introduction

The optimal switching problem across multiple regimes entails solving a stochastic optimization problem in which the admissible strategies are formalized by sequences of discrete interventions. A decision-maker in this context faces two basic questions: (i) when to switch from the current regime to another, and (ii) which regime to select when the decision of switching is made. These problems are characterized by their hybrid nature, combining continuous state dynamics with discrete control actions, where each switch between regimes typically incurs a cost while different regimes yield different reward outcomes. Over recent decades, the optimal switching problem has found extensive applications across different fields. Seminal work includes carmona2008pricing on pricing asset scheduling, carmona2010valuation on energy storage valuation, porchet2009valuation on power plant valuation, and olofsson2022management on hydropower production planning, among others.

The classical stochastic control problem typically assumes a fully known and accurate underlying model. However, this assumption of complete model knowledge often turns out to be unrealistic in practical applications. RL offers a powerful framework for learning optimal strategies in the unknown environment through trial-and-error interactions. While most conventional RL algorithms are designed in discrete-time settings, many real-world applications evolve continuously in time, motivating a systemic study in theories and algorithms for the continuous-time RL approach. Within the continuous-time framework, decision-makers face the fundamental exploration-exploitation trade-off in a continuous-time manner: whether to exploit current knowledge by executing the best-known policy or to explore alternative actions to gather information for potential long-term improvement. wang2020reinforcement addressed this problem by introducing an entropy-regularization on the randomized policy to encourage the exploration. This fundamental study spurred further pioneer studies of theories and algorithms in the continuous-time exploratory framework including jia2022policy; jia2022gradient; jia2023q, laying the foundations for the policy evaluation, the policy gradient, and the continuous-time q-learning, respectively. Later, the well-posedness of the exploratory HJB equation, the convergence of policy iterations and the regret analysis have also been examined in tang2022exploratory; huang2025convergence; tran2025policy; t-z-regret.

In addition, vast extensions and applications of continuous-time RL algorithms in various context have been considered in the recent literature. To name a few, wu2024reinforcement addressed the continuous-time mean-variance portfolio selection problem in regime-switching markets with unobservable states using reinforcement learning approach; bo2025optimal extended the q-learning theory in the model of reflected diffusion processes and applied it to learn the optimal tracking portfolio in incomplete markets; wei2025continuous generalized the continuous-time q-learning to mean-field control problems within McKean-Vlasov diffusion models; wyy2024 further developed the continuous-time q-learning for both mean-field control and mean-field game problems from the perspective of the representative agent; gao2024reinforcement studied the extension of q-learning in jump-diffusion models; bo2024continuous examined the same jump-diffusion model by invoking the Tsallis entropy; dong2024randomized investigated the optimal stopping in an exploratory framework by considering the randomization of stopping time via the intensity control; dianetti2024exploratory utilized the randomization of stopping times as singular control and studied its exploratory formulation under residual entropy regularization; dai2024learning exploited the penalization method to transform the optimal stopping problem to an optimal control problem for which the entropy regularization is formalized; liang2025reinforcement proposed a continuous-time RL framework for singular stochastic control problems without entropy regularization, characterizing the optimal control through singular control laws; liang2025reinforcement-2 further proposed a type of randomization of the singular control laws in liang2025reinforcement by considering an auxiliary singular control and entropy regularization, which lead to a time-inconsistent two-stage optimal control problem such that the task is to learn the time-consistent equilibrium.

Despite these advancements of continuous-time RL in different model setups, its application to optimal regime switching problems remains relatively underexplored. This paper studies the exploratory formulation of the optimal regime switching with multiple regimes and bridges its connection to the classical optimal switching problem as the entropy regularization vanishes. To this end, we propose a type of exploratory formulation where the decision-maker randomizes both switching time and the selection of the targeted regime state by invoking a generator matrix of an associated continuous-time Markov chain (CTMC) defined on finite state space. The entropy regularization on the generator is imposed to encourage the exploration. Specifically, we utilize the inherent property of the CTMCâ€”particularly its jump times and state transitionsâ€”to determine the switching decision. This formulation, governed by the control of the CTMCâ€™s generator matrix, transformed the randomized switching problem into an optimal control problem.

We summarize the main contributions of the present paper as follows:

* (i)

  We derive the system of exploratory HJB equations and establish the existence of a bounded classical solution to this system (see Lemma [3.2](https://arxiv.org/html/2512.04697v1#S3.Thmtheorem2 "Lemma 3.2. â€£ 3 Exploratory Formulation under Entropy Regularization â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")) by resorting to some established partial differential equation (PDE) theories together with a tailor-made truncation argument. Furthermore, we prove its uniqueness and demonstrate through a verification theorem (see Proposition [3.3](https://arxiv.org/html/2512.04697v1#S3.Thmtheorem3 "Proposition 3.3 (Verification Theorem). â€£ 3 Exploratory Formulation under Entropy Regularization â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")) that this solution coincides with the value function.
* (ii)

  We employ the policy iteration (PI) method to learn the optimal strategy through iterative updates and prove the policy improvement result in Proposition [4.1](https://arxiv.org/html/2512.04697v1#S4.Thmtheorem1 "Proposition 4.1. â€£ 4 Policy Iteration and Convergence â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes"). As the main result of this paper, in the context of PDE system, we establish the convergence result of the policy iteration in Theorem [4.2](https://arxiv.org/html/2512.04697v1#S4.Thmtheorem2 "Theorem 4.2. â€£ 4 Policy Iteration and Convergence â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes") with an explicit convergence rate, which is new to the literature.
* (iii)

  We also draw the connection to the classical optimal switching problem by establishing the convergence of the value function in the exploratory formulation towards the value function of the classical optimal switching problem as the temperature parameter approaches zero. To this end, we resort to some delicate stability analysis of viscosity solutions of the PDE system, see Lemma [4.3](https://arxiv.org/html/2512.04697v1#S4.Thmtheorem3 "Lemma 4.3. â€£ 4 Policy Iteration and Convergence â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes") and Theorem [4.4](https://arxiv.org/html/2512.04697v1#S4.Thmtheorem4 "Theorem 4.4. â€£ 4 Policy Iteration and Convergence â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes"). In particular, it is shown that the solution of the system of PDEs will converge to the solution of the system of variational inequalities as the temperature parameter tends to zero.
* (iv)

  We develop a reinforcement learning algorithm by implementing a policy evaluation method based on martingale characterization, which calls for the stochastic approximation when using the martingale orthogonality condition. We obtain an explicit error analysis for the convergence of this stochastic approximation method in Theorem [5.4](https://arxiv.org/html/2512.04697v1#S5.Thmtheorem4 "Theorem 5.4. â€£ 5 Reinforcement Learning Algorithm â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes"). To illustrate the effectiveness of our proposed RL algorithm, we conducted numerical experiments in two examples with satisfactory iteration convergence, both necessitate the application of neural networks to parameterize the targeted functions.

Let us also briefly compare the present work with three recent related studies. denkert2025control introduced a control randomization method without entropy regularization in continuous-time RL with the application to optimal switching problems. They developed an Actor-Critic policy gradient algorithm that alternately learns the value function and the optimal intensity policy. In contrast, our paper propose a different randomization approach for the optimal switching problem, utilizing the generator matrix of a CTMC and incorporating entropy regularization to encourage the exploration. A key advantage of our formulation is that the optimal policy depends explicitly on the value function itself, without requiring any of its derivatives. This allows us to parameterize both the policy and the value function using the same set of parameters. More recently, dai2025reinforcement developed a RL approach to identify arbitrage strategies in stock index futures. Following the randomization method in dong2024randomized, they randomized the switching times in dai2025reinforcement using the Cox processes and formulated the problem as an optimal switching problem with three regimes where the state process is independent of the regimes. In comparison, we consider an exploratory framework for a more general multi-regime optimal switching problem, where the state process dynamics can also depend on the regime states. Furthermore, we rigorously establish the convergence of the policy iterations with an explicit convergence rate and also show the convergence as the entropy regularization vanishes. Finally, our work differs from cao2025two, which studied a randomization scheme for impulse control problems characterized by fixed points of compound operators combining regularized nonlocal and stopping operators. In contrast, our distinct exploratory formulation leads to the study of PDE system instead of a single PDE problem, for which we need to develop some delicate analysis for the system of equations to deduce some desired convergence results.

The remainder of this paper is organized as follows. Section [2](https://arxiv.org/html/2512.04697v1#S2 "2 Classical Optimal Switching Problem â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes") reviews the classical optimal switching problem and presents preliminary results on viscosity solutions to the associated system of HJB variational inequalities. Section [3](https://arxiv.org/html/2512.04697v1#S3 "3 Exploratory Formulation under Entropy Regularization â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes") introduces the exploratory formulation of the optimal switching problem, providing a regularity analysis of the value function and the characterization of the optimal policy. Section [4](https://arxiv.org/html/2512.04697v1#S4 "4 Policy Iteration and Convergence â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes") establishes both policy improvement and the convergence result of the policy iteration. Moreover, the convergence behavior of the exploratory solution as the temperature parameter vanishes is also discusses therein. Section [5](https://arxiv.org/html/2512.04697v1#S5 "5 Reinforcement Learning Algorithm â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes") develops a reinforcement learning algorithm that implements the martingale-based policy evaluation and the previous policy iteration, accompanied by an error analysis for the proposed algorithm. Finally, Section [6](https://arxiv.org/html/2512.04697v1#S6 "6 Numerical Examples â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes") presents some numerical examples demonstrating the satisfactory performance of our proposed RL algorithm.

Notations.â€ƒWe specify the following list of notations for the rest of this paper.

* â€¢

  â„n\mathds{R}^{n} denotes the nn-dimensional Euclidean space. For all x=(x1,â‹¯,xn),y=(y1,â‹¯,yn)âˆˆâ„nx=(x\_{1},\cdots,x\_{n}),y=(y\_{1},\cdots,y\_{n})\in\mathds{R}^{n}, we denote by â‹…\cdot the scalar product and by |â‹…||\cdot| the Euclidean norm:

  |  |  |  |
  | --- | --- | --- |
  |  | xâ‹…y=âˆ‘i=1nxiâ€‹yi,|x|=xâ‹…x=âˆ‘i=1nxi2.\displaystyle x\cdot y=\sum\_{i=1}^{n}x\_{i}y\_{i},\quad|x|=\sqrt{x\cdot x}=\sqrt{\sum\_{i=1}^{n}x\_{i}^{2}}. |  |
* â€¢

  â„nÃ—d\mathds{R}^{n\times d} is the set of real-valued nÃ—dn\times d matrices. For Ïƒâˆˆâ„nÃ—d\sigma\in\mathds{R}^{n\times d}, we denote by ÏƒâŠ¤\sigma^{\top} the transpose matrix of Ïƒ\sigma. For A=(aiâ€‹j)1â‰¤i,jâ‰¤nâˆˆâ„nÃ—nA=(a\_{ij})\_{1\leq i,j\leq n}\in\mathds{R}^{n\times n}, trâ€‹(A)=âˆ‘i=1naiâ€‹i\text{tr}(A)=\sum\_{i=1}^{n}a\_{ii} is the trace of AA. We define the matrix norm on â„nÃ—d\mathds{R}^{n\times d} as |Ïƒ|=(trâ€‹(Ïƒâ€‹ÏƒâŠ¤))12|\sigma|=(\text{tr}(\sigma\sigma^{\top}))^{\frac{1}{2}}.
* â€¢

  For ğ’ªâŠ‚â„n{\cal O}\subset\mathds{R}^{n}, Ckâ€‹(ğ’ª)C^{k}({\cal O}) is the space of all real-valued continuous functions on ğ’ª{\cal O} with continuous derivatives up to order kk. For Tâ‰¥0T\geq 0, C1,2â€‹([0,T]Ã—ğ’ª)C^{1,2}([0,T]\times{\cal O}) is the space of real-valued functions uu on [0,T]Ã—ğ’ª[0,T]\times{\cal O} whose partial derivatives âˆ‚uâˆ‚t,âˆ‚uâˆ‚xi,âˆ‚2uâˆ‚xiâ€‹xj\frac{\partial u}{\partial t},\frac{\partial u}{\partial x\_{i}},\frac{\partial^{2}u}{\partial x\_{i}x\_{j}}, 1â‰¤i,jâ‰¤n1\leq i,j\leq n, exist and are continuous on [0,T]Ã—ğ’ª[0,T]\times{\cal O}. For uâˆˆC2â€‹(ğ’ª)u\in C^{2}({\cal O}), we denote by Dxâ€‹uD\_{x}u the gradient vector of uu and Dx2â€‹uD\_{x}^{2}u the Hessian matrix of uu.
* â€¢

  For points P=(t,x),Pâ€²=(t,x)âˆˆ[0,T]Ã—â„nP=(t,x),P^{\prime}=(t,x)\in[0,T]\times\mathds{R}^{n}, we define the parabolic distance between PP and Pâ€²P^{\prime} by

  |  |  |  |
  | --- | --- | --- |
  |  | dâ€‹(P,Pâ€²)=(|tâˆ’tâ€²|+|xâˆ’xâ€²|2)12.\displaystyle d(P,P^{\prime})=(|t-t^{\prime}|+|x-x^{\prime}|^{2})^{\frac{1}{2}}. |  |
* â€¢

  For ğ’ŸâŠ‚[0,T]Ã—â„n{\cal D}\subset[0,T]\times\mathds{R}^{n} and Î±âˆˆ(0,1)\alpha\in(0,1) we introduce the following norms for functions defined on ğ’Ÿ{\cal D}:

  |  |  |  |
  | --- | --- | --- |
  |  | â€–uâ€–C0â€‹(ğ’Ÿ)=supPâˆˆğ’Ÿ|fâ€‹(P)|,â€–uâ€–CÎ±â€‹(ğ’Ÿ)=â€–uâ€–C0â€‹(ğ’Ÿ)+supP,Pâ€²âˆˆğ’Ÿ,Pâ‰ Pâ€²|uâ€‹(P)âˆ’uâ€‹(Pâ€²)|dâ€‹(P,Pâ€²)Î±,\displaystyle||u||\_{C^{0}({\cal D})}=\sup\_{P\in{\cal D}}|f(P)|,\quad||u||\_{C^{\alpha}({\cal D})}=||u||\_{C^{0}({\cal D})}+\sup\_{P,P^{\prime}\in{\cal D},P\neq P^{\prime}}\frac{|u(P)-u(P^{\prime})|}{d(P,P^{\prime})^{\alpha}}, |  |
  |  |  |  |
  | --- | --- | --- |
  |  | â€–uâ€–C1â€‹(ğ’Ÿ)=â€–uâ€–C0â€‹(ğ’Ÿ)+âˆ‘i=1nâ€–âˆ‚uâˆ‚xiâ€–C0â€‹(ğ’Ÿ),â€–uâ€–C1+Î±â€‹(ğ’Ÿ)=â€–uâ€–CÎ±â€‹(ğ’Ÿ)+âˆ‘i=1nâ€–âˆ‚uâˆ‚xiâ€–CÎ±â€‹(ğ’Ÿ),\displaystyle||u||\_{C^{1}({\cal D})}=||u||\_{C^{0}({\cal D})}+\sum\_{i=1}^{n}\left|\left|\frac{\partial u}{\partial x\_{i}}\right|\right|\_{C^{0}({\cal D})},\quad||u||\_{C^{1+\alpha}({\cal D})}=||u||\_{C^{\alpha}({\cal D})}+\sum\_{i=1}^{n}\left|\left|\frac{\partial u}{\partial x\_{i}}\right|\right|\_{C^{\alpha}({\cal D})}, |  |
  |  |  |  |
  | --- | --- | --- |
  |  | â€–uâ€–C2â€‹(ğ’Ÿ)=â€–uâ€–C1â€‹(ğ’Ÿ)+âˆ‘i=1nâ€–âˆ‚uâˆ‚xiâ€–C1â€‹(ğ’Ÿ)+â€–âˆ‚uâˆ‚tâ€–C0â€‹(ğ’Ÿ),\displaystyle||u||\_{C^{2}({\cal D})}=||u||\_{C^{1}({\cal D})}+\sum\_{i=1}^{n}\left|\left|\frac{\partial u}{\partial x\_{i}}\right|\right|\_{C^{1}({\cal D})}+\left|\left|\frac{\partial u}{\partial t}\right|\right|\_{C^{0}({\cal D})}, |  |
  |  |  |  |
  | --- | --- | --- |
  |  | â€–uâ€–C2+Î±â€‹(ğ’Ÿ)=â€–uâ€–C1+Î±â€‹(ğ’Ÿ)+âˆ‘i=1nâ€–âˆ‚uâˆ‚xiâ€–C1+Î±â€‹(ğ’Ÿ)+â€–âˆ‚uâˆ‚tâ€–CÎ±â€‹(ğ’Ÿ).\displaystyle||u||\_{C^{2+\alpha}({\cal D})}=||u||\_{C^{1+\alpha}({\cal D})}+\sum\_{i=1}^{n}\left|\left|\frac{\partial u}{\partial x\_{i}}\right|\right|\_{C^{1+\alpha}({\cal D})}+\left|\left|\frac{\partial u}{\partial t}\right|\right|\_{C^{\alpha}({\cal D})}. |  |

  We shall say that function uâ€‹(t,x)u(t,x) is in Cqâ€‹(ğ’Ÿ)C^{q}({\cal D}) if â€–uâ€–Cqâ€‹(ğ’Ÿ)||u||\_{C^{q}({\cal D})} is finite (q=0,Î±,1+Î±,2+Î±q=0,\alpha,1+\alpha,2+\alpha).

## 2 Classical Optimal Switching Problem

This section first reviews the classical optimal switching problem and introduce some preliminary results on viscosity solutions to the associated system of HJB variational inequalities.

We fix a complete probability space (Î©,â„±,â„™)(\Omega,\mathcal{F},\mathbb{P}), supporting a dd-dimensional standard Brownian motion W=(Wt)tâ‰¥0W=(W\_{t})\_{t\geq 0}. We denote by ğ”½\mathbb{F} the complete and right continuous filtration generated by WW. The terminal time is denoted by T>0T>0. Let us introduce the domain ğ’Ÿ:=[0,T)Ã—â„n{\cal D}:=[0,T)\times\mathds{R}^{n}, then the closure of ğ’Ÿ{\cal D} is given by ğ’ŸÂ¯=[0,T]Ã—â„n\overline{{\cal D}}=[0,T]\times\mathds{R}^{n}.

We then define the set ğ’œt\mathcal{A}\_{t} of admissible switching controls at time tâˆˆ[0,T]t\in[0,T] as the set of double sequences Î±=(Ï„k,Îºk)kâ‰¥0\alpha=(\tau\_{k},\kappa\_{k})\_{k\geq 0}, where (Ï„k)kâ‰¥0\left(\tau\_{k}\right)\_{k\geq 0} is a non-decreasing sequence of ğ”½\mathbb{F}-stopping times with Ï„0=t\tau\_{0}=t and limkâ†’âˆÏ„k>T\lim\_{k\rightarrow\infty}\tau\_{k}>T; Îºk\kappa\_{k} is an â„±Ï„k\mathcal{F}\_{\tau\_{k}}-measurable random variable valued in the set ğ•€m={1,2,â‹¯,m}\mathbb{I}\_{m}=\{1,2,\cdots,m\}. With a strategy Î±=(Ï„k,Îºk)kâ‰¥0âˆˆğ’œt\alpha=\left(\tau\_{k},\kappa\_{k}\right)\_{k\geq 0}\in\mathcal{A}\_{t} and an initial regime value iâˆˆğ•€mi\in\mathbb{I}\_{m}, we associate the process (Ist,i)sâ‰¥t(I\_{s}^{t,i})\_{s\geq t} defined by

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ist,i=âˆ‘kâ‰¥0Îºkâ€‹ğŸsâˆˆ[Ï„k,Ï„k+1),sâ‰¥t,Itâˆ’t,i=Îº0=i.\displaystyle I\_{s}^{t,i}=\sum\_{k\geq 0}\kappa\_{k}{\bf 1}\_{s\in[\tau\_{k},\tau\_{k+1})},\penalty 10000\ s\geq t,\quad I^{t,i}\_{t-}=\kappa\_{0}=i. |  | (2.1) |

Given (t,x,i)âˆˆ[0,T]Ã—â„nÃ—ğ•€m(t,x,i)\in[0,T]\times\mathds{R}^{n}\times\mathbb{I}\_{m}, and a switching control Î±âˆˆğ’œt\alpha\in\mathcal{A}\_{t}, we consider the controlled diffusion Xt,x,i,Î±=(Xst,x,i,Î±)sâˆˆ[t,T]X^{t,x,i,\alpha}=(X\_{s}^{t,x,i,\alpha})\_{s\in[t,T]} governed by the SDE:

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | dâ€‹Xst,x,i,Î±=Î¼â€‹(s,Xst,x,i,Î±,Ist,i)â€‹dâ€‹s+Ïƒâ€‹(s,Xst,x,i,Î±,Ist,i)â€‹dâ€‹Ws,sâˆˆ(t,T].\displaystyle dX^{t,x,i,\alpha}\_{s}=\mu(s,X^{t,x,i,\alpha}\_{s},I\_{s}^{t,i})ds+\sigma(s,X^{t,x,i,\alpha}\_{s},I\_{s}^{t,i})dW\_{s},\quad s\in(t,T]. |  | (2.2) |

with Xtt,x,i,Î±=xX\_{t}^{t,x,i,\alpha}=x. We have the following assumptions for the model coefficients.

###### Assumption 2.1.

* (i)

  The drift Î¼â€‹(â‹…,â‹…,â‹…):[0,T]Ã—â„nÃ—ğ•€mâ†’â„n\mu(\cdot,\cdot,\cdot):[0,T]\times\mathds{R}^{n}\times\mathbb{I}\_{m}\to\mathds{R}^{n} and volatility Ïƒâ€‹(â‹…,â‹…,â‹…):[0,T]Ã—â„nÃ—ğ•€mâ†’â„nÃ—d\sigma(\cdot,\cdot,\cdot):[0,T]\times\mathds{R}^{n}\times\mathbb{I}\_{m}\to\mathds{R}^{n\times d} are
  uniformly Lipschitz continuous with respect to xx, that is, there exists a constant L>0L>0 such that

  |  |  |  |  |
  | --- | --- | --- | --- |
  |  | |Î¼â€‹(s,x1,i)âˆ’Î¼â€‹(s,x2,i)|+|Ïƒâ€‹(s,x1,i)âˆ’Ïƒâ€‹(s,x2,i)|â‰¤Lâ€‹|x1âˆ’x2|\displaystyle|\mu(s,x\_{1},i)-\mu(s,x\_{2},i)|+|\sigma(s,x\_{1},i)-\sigma(s,x\_{2},i)|\leq L|x\_{1}-x\_{2}| |  | (2.3) |

  for all (s,x1,x2,i)âˆˆ[0,T]Ã—â„2â€‹nÃ—ğ•€m(s,x\_{1},x\_{2},i)\in[0,T]\times\mathds{R}^{2n}\times\mathbb{I}\_{m}.
* (ii)

  There exist some constant Ïƒ0>0\sigma\_{0}>0 such that, for all (t,x,i)âˆˆğ’ŸÂ¯Ã—ğ•€m(t,x,i)\in\overline{{\cal D}}\times\mathbb{I}\_{m} and Î¾âˆˆâ„n\xi\in\mathds{R}^{n},

  |  |  |  |
  | --- | --- | --- |
  |  | Î¾â€‹Ïƒâ€‹(t,x,i)â€‹ÏƒâŠ¤â€‹(t,x,i)â€‹Î¾âŠ¤â‰¥Ïƒ0â€‹Î¾â€‹Î¾âŠ¤.\displaystyle\xi\sigma(t,x,i)\sigma^{\top}(t,x,i)\xi^{\top}\geq\sigma\_{0}\xi\xi^{\top}. |  |

The expected total profit with the initial state (t,x,i)(t,x,i) and the impulse control Î±=(Ï„k,Îºk)kâ‰¥0âˆˆğ’œt\alpha=\left(\tau\_{k},\kappa\_{k}\right)\_{k\geq 0}\in\mathcal{A}\_{t} is given by

|  |  |  |  |
| --- | --- | --- | --- |
|  | Jiâ€‹(t,x;Î±)=ğ”¼â€‹[âˆ«tTfâ€‹(s,Xst,x,i,Î±,Ist,i)â€‹ğ‘‘sâˆ’âˆ‘k=1âˆgÎºkâˆ’1â€‹Îºkâ€‹ğŸ{Ï„kâ‰¤T}+hâ€‹(XTt,x,i,Î±)],\displaystyle J\_{i}(t,x;\alpha)=\mathbb{E}\bigg[\int\_{t}^{T}f(s,X\_{s}^{t,x,i,\alpha},I\_{s}^{t,i})ds-\sum\limits\_{k=1}^{\infty}g\_{\kappa\_{k-1}\kappa\_{k}}{\bf 1}\_{\{\tau\_{k}\leq T\}}+h(X\_{T}^{t,x,i,\alpha})\bigg], |  | (2.4) |

where fâ€‹(â‹…,â‹…,â‹…):[0,T]Ã—â„nÃ—ğ•€mâ†’â„f(\cdot,\cdot,\cdot):[0,T]\times\mathds{R}^{n}\times\mathbb{I}\_{m}\to\mathds{R} is the running profit function, hâ€‹(â‹…):â„nâ†’â„h(\cdot):\mathds{R}^{n}\to\mathds{R} is the terminal reward function, and the constant giâ€‹jg\_{ij} denotes the cost for switching from regime ii to jj for all iâ‰ ji\neq j. We also impose the following assumptions.

###### Assumption 2.2.

* (i)

  For iâˆˆğ•€mi\in\mathbb{I}\_{m}, the running profit fâ€‹(â‹…,â‹…,i)f(\cdot,\cdot,i) and terminal reward hâ€‹(â‹…)h(\cdot) are assumed to be continuous. Furthermore, there exists a constant Kf,h>0K\_{f,h}>0 such that

  |  |  |  |  |
  | --- | --- | --- | --- |
  |  | |fâ€‹(t,x,i)|+|hâ€‹(x)|â‰¤Kf,h,âˆ€(t,x,i)âˆˆ[0,T]Ã—â„nÃ—ğ•€m.\displaystyle|f(t,x,i)|+|h(x)|\leq K\_{f,h},\quad\forall(t,x,i)\in[0,T]\times\mathds{R}^{n}\times\mathbb{I}\_{m}. |  | (2.5) |
* (ii)

  For i,jâˆˆğ•€mi,j\in\mathbb{I}\_{m} with jâ‰ ij\neq i, the cost for switching from regime ii to jj is positive, that is, giâ€‹j>0g\_{ij}>0, with the convention giâ€‹i=0g\_{ii}=0. For i,j,kâˆˆğ•€mi,j,k\in\mathbb{I}\_{m} with jâ‰ i,kj\neq i,k, it is less expensive to switch directly in one step from regime ii to kk than in two steps via an intermediate regime jj, that is, giâ€‹k<giâ€‹j+gjâ€‹kg\_{ik}<g\_{ij}+g\_{jk}.

The objective is to maximize the expected total profit over all strategies Î±\alpha.
Accordingly, the classical value functions is defined by

|  |  |  |  |
| --- | --- | --- | --- |
|  | Viâ€‹(t,x)=supÎ±âˆˆğ’œtJiâ€‹(t,x;Î±),(t,x,i)âˆˆ[0,T]Ã—â„nÃ—ğ•€m.\displaystyle V\_{i}(t,x)=\sup\limits\_{\alpha\in\mathcal{A}\_{t}}J\_{i}(t,x;\alpha),\quad(t,x,i)\in[0,T]\times\mathds{R}^{n}\times\mathbb{I}\_{m}. |  | (2.6) |

We now consider the following system of HJB variational inequalities, for iâˆˆğ•€mi\in\mathbb{I}\_{m},

|  |  |  |  |
| --- | --- | --- | --- |
|  | {minâ¡{âˆ’âˆ‚Viâ€‹(t,x)âˆ‚tâˆ’â„’xiâ€‹Viâ€‹(t,x)âˆ’fâ€‹(t,x,i),Viâ€‹(t,x)âˆ’maxjâ‰ iâ¡(Vjâ€‹(t,x)âˆ’giâ€‹j)}=0,(t,x)âˆˆğ’Ÿ,Viâ€‹(T,x)=hâ€‹(x),xâˆˆâ„n,\displaystyle\begin{cases}\displaystyle\min\left\{-\frac{\partial V\_{i}(t,x)}{\partial t}-\mathcal{L}^{i}\_{x}V\_{i}(t,x)-f(t,x,i),V\_{i}(t,x)-\max\_{j\neq i}(V\_{j}(t,x)-g\_{ij})\right\}=0,\quad(t,x)\in{\cal D},\\ \displaystyle V\_{i}(T,x)=h(x),\quad x\in\mathds{R}^{n},\end{cases} |  | (2.7) |

where the operator â„’xi\mathcal{L}\_{x}^{i} with iâˆˆğ•€mi\in\mathbb{I}\_{m} is defined by

|  |  |  |
| --- | --- | --- |
|  | â„’xiâ€‹lâ€‹(t,x):=Î¼â€‹(t,x,i)â€‹Dxâ€‹lâ€‹(t,x)+12â€‹trâ€‹(Ïƒâ€‹ÏƒâŠ¤â€‹(t,x,i)â€‹Dx2â€‹lâ€‹(t,x)),forâ€‹lâ€‹(t,â‹…)âˆˆC2â€‹(â„n).\displaystyle\mathcal{L}\_{x}^{i}l(t,x):=\mu(t,x,i)D\_{x}l(t,x)+\frac{1}{2}\text{tr}(\sigma\sigma^{\top}(t,x,i)D\_{x}^{2}l(t,x)),\quad\text{for}\penalty 10000\ l(t,\cdot)\in C^{2}(\mathds{R}^{n}). |  |

The value function (V1,â‹¯,Vm)(V\_{1},\cdots,V\_{m}) can be characterized as the viscosity solution of system ([2.7](https://arxiv.org/html/2512.04697v1#S2.E7 "In 2 Classical Optimal Switching Problem â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")), which is defined as below.

###### Definition 2.1.

Let (u1,â‹¯,um)(u\_{1},\cdots,u\_{m}) be a mm-uplet of functions defined on ğ’ŸÂ¯\overline{{\cal D}}, â„\mathds{R}-valued and such that uiâ€‹(T,x)=hâ€‹(x)u\_{i}(T,x)=h(x) for any (i,x)âˆˆğ•€mÃ—â„n(i,x)\in\mathbb{I}\_{m}\times\mathds{R}^{n}. The mm-uplet (u1,â‹¯,um)(u\_{1},\cdots,u\_{m}) is called:

* (i)

  a viscosity supersolution (respectively, subsolution) of system ([2.7](https://arxiv.org/html/2512.04697v1#S2.E7 "In 2 Classical Optimal Switching Problem â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")) if, for each iâˆˆğ•€mi\in\mathbb{I}\_{m}, uiu\_{i} is lower-semicontinuous (respectively, upper-semicontinuous) on ğ’Ÿ{\cal D} and for any (t0,x0)âˆˆğ’Ÿ(t\_{0},x\_{0})\in{\cal D} and any test function Ï†iâˆˆC1,2â€‹(ğ’Ÿ)\varphi\_{i}\in C^{1,2}({\cal D}) such that (t0,x0)(t\_{0},x\_{0}) is a local minimum point of uiâˆ’Ï†iu\_{i}-\varphi\_{i} (respectively, maximum), we have

  |  |  |  |  |
  | --- | --- | --- | --- |
  |  | min{\displaystyle\min\Bigg\{ | âˆ’âˆ‚Ï†iâ€‹(t0,x0)âˆ‚tâˆ’â„’xiâ€‹Ï†iâ€‹(t0,x0)âˆ’fâ€‹(t0,x0,i),\displaystyle-\frac{\partial\varphi\_{i}(t\_{0},x\_{0})}{\partial t}-\mathcal{L}^{i}\_{x}\varphi\_{i}(t\_{0},x\_{0})-f(t\_{0},x\_{0},i), |  |
  |  |  |  |  |
  | --- | --- | --- | --- |
  |  |  | ui(t0,x0)âˆ’maxjâ‰ i(uj(t0,x0)âˆ’giâ€‹j)}â‰¥0(respectively,Â â‰¤0);\displaystyle\qquad\qquad\quad u\_{i}(t\_{0},x\_{0})-\max\_{j\neq i}(u\_{j}(t\_{0},x\_{0})-g\_{ij})\Bigg\}\geq 0\penalty 10000\ \text{(respectively, $\leq 0$)}; |  |
* (ii)

  a viscosity solution of system ([2.7](https://arxiv.org/html/2512.04697v1#S2.E7 "In 2 Classical Optimal Switching Problem â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")) if it both a viscosity supersolution and subsolution.

By using a similar proof of Theorem 5.1 in el2013stochastic, we have the comparison principle for the system ([2.7](https://arxiv.org/html/2512.04697v1#S2.E7 "In 2 Classical Optimal Switching Problem â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")).

###### Lemma 2.3 (Comparison Principle).

Suppose Assumptions [2.1](https://arxiv.org/html/2512.04697v1#S2.Thmtheorem1 "Assumption 2.1. â€£ 2 Classical Optimal Switching Problem â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes") and [2.2](https://arxiv.org/html/2512.04697v1#S2.Thmtheorem2 "Assumption 2.2. â€£ 2 Classical Optimal Switching Problem â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes") hold. Let (u1,â‹¯,um)(u\_{1},\cdots,u\_{m}) be a bounded viscosity supersolution of system ([2.7](https://arxiv.org/html/2512.04697v1#S2.E7 "In 2 Classical Optimal Switching Problem â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")) and (v1,â‹¯,vm)(v\_{1},\cdots,v\_{m}) be a bounded viscosity subsolution of system ([2.7](https://arxiv.org/html/2512.04697v1#S2.E7 "In 2 Classical Optimal Switching Problem â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")). Then viâ€‹(t,x)â‰¤uiâ€‹(t,x)v\_{i}(t,x)\leq u\_{i}(t,x) for all (t,x,i)âˆˆğ’ŸÂ¯Ã—ğ•€m(t,x,i)\in\overline{{\cal D}}\times\mathbb{I}\_{m}.

Lemma [2.3](https://arxiv.org/html/2512.04697v1#S2.Thmtheorem3 "Lemma 2.3 (Comparison Principle). â€£ 2 Classical Optimal Switching Problem â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes") will help the proof of uniqueness of viscosity solution. The next result relates the value function (V1,â‹¯,Vm)(V\_{1},\cdots,V\_{m}) to the system of variational inequalities.

###### Theorem 2.4.

Under Assumptions [2.1](https://arxiv.org/html/2512.04697v1#S2.Thmtheorem1 "Assumption 2.1. â€£ 2 Classical Optimal Switching Problem â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes") and [2.2](https://arxiv.org/html/2512.04697v1#S2.Thmtheorem2 "Assumption 2.2. â€£ 2 Classical Optimal Switching Problem â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes"), the value function (V1,â‹¯,Vm)(V\_{1},\cdots,V\_{m}) given by ([2.6](https://arxiv.org/html/2512.04697v1#S2.E6 "In 2 Classical Optimal Switching Problem â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")) is the unique bounded viscosity solution of system ([2.7](https://arxiv.org/html/2512.04697v1#S2.E7 "In 2 Classical Optimal Switching Problem â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")).

###### Proof.

We begin by proving that the value function (V1,â‹¯,Vm)(V\_{1},\cdots,V\_{m}) defined by ([2.6](https://arxiv.org/html/2512.04697v1#S2.E6 "In 2 Classical Optimal Switching Problem â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")) is bounded. By Assumption [2.2](https://arxiv.org/html/2512.04697v1#S2.Thmtheorem2 "Assumption 2.2. â€£ 2 Classical Optimal Switching Problem â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes"), for any (i,t,x)âˆˆğ•€mÃ—ğ’ŸÂ¯(i,t,x)\in\mathbb{I}\_{m}\times\overline{{\cal D}} and Î±âˆˆğ’œt\alpha\in{\cal A}\_{t},

|  |  |  |  |
| --- | --- | --- | --- |
|  | Jiâ€‹(t,x,Î±)\displaystyle J\_{i}(t,x,\alpha) | â‰¤ğ”¼â€‹[âˆ«tTfâ€‹(s,Xst,x,i,Î±,Ist,i)â€‹ğ‘‘s+hâ€‹(XTt,x,i,Î±)]\displaystyle\leq\mathbb{E}\bigg[\int\_{t}^{T}f(s,X\_{s}^{t,x,i,\alpha},I\_{s}^{t,i})ds+h(X\_{T}^{t,x,i,\alpha})\bigg] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤(Tâˆ’t)â€‹Kf,h+Kf,h,\displaystyle\leq(T-t)K\_{f,h}+K\_{f,h}, |  |

which implies Viâ€‹(t,x)â‰¤(Tâˆ’t)â€‹Kf,h+Kf,hV\_{i}(t,x)\leq(T-t)K\_{f,h}+K\_{f,h}. For the lower bound, consider the no-switching control Ï„n=âˆ\tau\_{n}=\infty, nâ‰¥1n\geq 1, i.e., Ist,i=iI\_{s}^{t,i}=i, sâ‰¥ts\geq t. Applying Assumption [2.2](https://arxiv.org/html/2512.04697v1#S2.Thmtheorem2 "Assumption 2.2. â€£ 2 Classical Optimal Switching Problem â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes") again yields

|  |  |  |  |
| --- | --- | --- | --- |
|  | Viâ€‹(t,x)\displaystyle V\_{i}(t,x) | â‰¥ğ”¼â€‹[âˆ«tTfâ€‹(s,Xst,x,i,Î±,Ist,i)â€‹ğ‘‘s+hâ€‹(XTt,x,i,Î±)]\displaystyle\geq\mathbb{E}\bigg[\int\_{t}^{T}f(s,X\_{s}^{t,x,i,\alpha},I\_{s}^{t,i})ds+h(X\_{T}^{t,x,i,\alpha})\bigg] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¥âˆ’(Tâˆ’t)â€‹Kf,hâˆ’Kf,h.\displaystyle\geq-(T-t)K\_{f,h}-K\_{f,h}. |  |

Therefore, the value function is bounded. As it is bounded, it follows from Proposition 4.2 in bouchard2009stochastic and Lemma [2.3](https://arxiv.org/html/2512.04697v1#S2.Thmtheorem3 "Lemma 2.3 (Comparison Principle). â€£ 2 Classical Optimal Switching Problem â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes") that the value function (V1,â‹¯,Vm)(V\_{1},\cdots,V\_{m}) is the unique bounded viscosity solution of system ([2.7](https://arxiv.org/html/2512.04697v1#S2.E7 "In 2 Classical Optimal Switching Problem â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")).
âˆ

## 3 Exploratory Formulation under Entropy Regularization

In this section, we introduce our exploratory formulation of the optimal switching problem, and study the well-posedness of the associated exploratory HJB system as well as the verification theorem.

To explore the system and reward, we let the agent randomize the choice of the stopping times and the regimes that she would like to switch to. Let I:=(It)tâ‰¥0I:=(I\_{t})\_{t\geq 0} denote a continuous-time finite-state Markov chain with state space ğ•€m\mathbb{I}\_{m}, which is independent of the Brownian motion WW. The randomization is achieved by considering the choice of the generator, ğ…=(Ï€tiâ€‹j)i,jâˆˆğ•€m,tâˆˆ[0,T]{\bm{\pi}}=(\pi^{ij}\_{t})\_{i,j\in\mathbb{I}\_{m},t\in[0,T]}, of the Markov chain II. For iâ‰ ji\neq j, Ï€tiâ€‹j\pi\_{t}^{ij} is the instantaneous intensity of the transition of II from state ii to state jj at time tt. Here, for each tâˆˆ[0,T]t\in[0,T], Ï€tiâ€‹jâ‰¥0\pi^{ij}\_{t}\geq 0, for iâ‰ ji\neq j and âˆ‘j=1mÏ€tiâ€‹j=0\sum\_{j=1}^{m}\pi\_{t}^{ij}=0.

Given (t,x,i)âˆˆ[0,T]Ã—â„nÃ—ğ•€m(t,x,i)\in[0,T]\times\mathds{R}^{n}\times\mathbb{I}\_{m}, we consider the controlled diffusion X=(Xs)sâˆˆ[t,T]X=(X\_{s})\_{s\in[t,T]} defined by the following SDE:

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | dâ€‹Xs=Î¼â€‹(s,Xs,Is)â€‹dâ€‹s+Ïƒâ€‹(s,Xs,Is)â€‹dâ€‹Ws,sâˆˆ(t,T].\displaystyle dX\_{s}=\mu(s,X\_{s},I\_{s})ds+\sigma(s,X\_{s},I\_{s})dW\_{s},\quad s\in(t,T]. |  | (3.1) |

with Xt=xX\_{t}=x and It=iI\_{t}=i. For kâ‰¥1k\geq 1, denote by Ï„k\tau\_{k} the kk-th jump time of process II with Ï„0=0\tau\_{0}=0 and Îºk:=IÏ„k\kappa\_{k}:=I\_{\tau\_{k}}. For tâ‰¥0t\geq 0, let ğ•Œt\mathbb{U}\_{t} be the set of all admissable policies (Ï€iâ€‹j)i,jâˆˆğ•€m(\pi^{ij})\_{i,j\in\mathbb{I}\_{m}} such that for every i,jâˆˆğ•€mi,j\in\mathbb{I}\_{m}, the process Ï€iâ€‹j=(Ï€siâ€‹j)sâˆˆ[t,T]\pi^{ij}=(\pi^{ij}\_{s})\_{s\in[t,T]} is ğ”½\mathbb{F}-adapted and satisfies (i) for iâ‰ ji\neq j, Ï€siâ€‹jâ‰¥0\pi^{ij}\_{s}\geq 0 for all sâˆˆ[t,T]s\in[t,T]; (ii) for every iâˆˆğ•€mi\in\mathbb{I}\_{m}, âˆ‘j=1mÏ€siâ€‹j=0\sum\_{j=1}^{m}\pi\_{s}^{ij}=0, for all sâˆˆ[t,T]s\in[t,T].

For ğ…âˆˆğ•Œt{\bm{\pi}}\in\mathbb{U}\_{t}, denote by ğ…i=(Ï€iâ€‹j)jâˆˆğ•€m{\bm{\pi}}^{i}=(\pi^{ij})\_{j\in\mathbb{I}\_{m}} for iâˆˆğ•€mi\in\mathbb{I}\_{m}. To encourage the exploration, we adopt the normalized entropy similar to dong2024randomized that Râ€‹(ğ…,i):=âˆ‘jâ‰ iÏ€iâ€‹jâˆ’Ï€iâ€‹jâ€‹logâ¡Ï€iâ€‹jR({\bm{\pi}},i):=\sum\_{j\neq i}\pi^{ij}-\pi^{ij}\log\pi^{ij} for iâˆˆğ•€mi\in\mathbb{I}\_{m}. The exploratory formulation of objective functional under entropy regularizer is given by, for (t,x,i)âˆˆ[0,T]Ã—â„nÃ—ğ•€m(t,x,i)\in[0,T]\times\mathds{R}^{n}\times\mathbb{I}\_{m} and ğ…=(Ï€siâ€‹j)i,jâˆˆğ•€m,sâˆˆ[t,T]âˆˆğ•Œt{\bm{\pi}}=(\pi^{ij}\_{s})\_{i,j\in\mathbb{I}\_{m},s\in[t,T]}\in\mathbb{U}\_{t},

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | JiÎ»â€‹(t,x;ğ…):=ğ”¼t,x,iâ€‹[âˆ«tTfâ€‹(s,Xs,Is)â€‹ğ‘‘sâˆ’âˆ‘k=1âˆgÎºkâˆ’1â€‹Îºkâ€‹ğŸ{Ï„kâ‰¤T}+Î»â€‹âˆ«tTRâ€‹(ğ…s,Is)â€‹ğ‘‘s+hâ€‹(XT)],\displaystyle J\_{i}^{\lambda}(t,x;{\bm{\pi}}):=\mathbb{E}\_{t,x,i}\bigg[\int^{T}\_{t}f(s,X\_{s},I\_{s})ds-\sum\_{k=1}^{\infty}g\_{\kappa\_{k-1}\kappa\_{k}}{\bf 1}\_{\{\tau\_{k}\leq T\}}+\lambda\int^{T}\_{t}R({\bm{\pi}}\_{s},I\_{s})ds+h(X\_{T})\bigg], |  | (3.2) |

where ğ”¼t,x,i[â‹…]:=ğ”¼[â‹…|Xt=x,It=i]\mathbb{E}\_{t,x,i}[\cdot]:=\mathbb{E}[\cdot|X\_{t}=x,I\_{t}=i], and Î»>0\lambda>0 is the temperature parameter. Furthermore, the optimal value function is denoted by

|  |  |  |  |
| --- | --- | --- | --- |
|  | ViÎ»â€‹(t,x)=supğ…âˆˆğ•ŒtJiÎ»â€‹(t,x;ğ…).\displaystyle V\_{i}^{\lambda}(t,x)=\sup\_{{\bm{\pi}}\in\mathbb{U}\_{t}}J\_{i}^{\lambda}(t,x;{\bm{\pi}}). |  | (3.3) |

Applying the dynamic programming arguments (c.f. Section 5.3.2 in pham2009continuous), we derive the system of coupled HJB equations as follows: for iâˆˆğ•€mi\in\mathbb{I}\_{m},

|  |  |  |  |
| --- | --- | --- | --- |
|  | {âˆ‚ViÎ»â€‹(t,x)âˆ‚t+â„’xiâ€‹ViÎ»â€‹(t,x)+fâ€‹(t,x,i)+supğ…i{âˆ‘jâ‰ iÏ€iâ€‹jâ€‹(VjÎ»â€‹(t,x)âˆ’giâ€‹jâˆ’ViÎ»â€‹(t,x))+Î»â€‹âˆ‘jâ‰ i(Ï€iâ€‹jâˆ’Ï€iâ€‹jâ€‹logâ¡Ï€iâ€‹j)}=0,(t,x)âˆˆğ’Ÿ,ViÎ»â€‹(T,x)=hâ€‹(x),xâˆˆâ„n.\displaystyle\begin{cases}\displaystyle\frac{\partial V^{\lambda}\_{i}(t,x)}{\partial t}+\mathcal{L}^{i}\_{x}V^{\lambda}\_{i}(t,x)+f(t,x,i)\\ \displaystyle\qquad\quad+\sup\_{{\bm{\pi}}\_{i}}\left\{\sum\_{j\neq i}\pi\_{ij}(V^{\lambda}\_{j}(t,x)-g\_{ij}-V^{\lambda}\_{i}(t,x))+\lambda\sum\_{j\neq i}(\pi\_{ij}-\pi\_{ij}\log\pi\_{ij})\right\}=0,\penalty 10000\ (t,x)\in{\cal D},\\ \displaystyle V^{\lambda}\_{i}(T,x)=h(x),\penalty 10000\ x\in\mathds{R}^{n}.\end{cases} |  | (3.4) |

Using the first-order condition, we arrive at the characterization of the optimal feedback policy by

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ï€iâ€‹jâˆ—â€‹(t,x)=expâ¡(VjÎ»â€‹(t,x)âˆ’giâ€‹jâˆ’ViÎ»â€‹(t,x)Î»),jâˆˆğ•€mâˆ–{i},(t,x)âˆˆğ’ŸÂ¯.\displaystyle\pi\_{ij}^{\*}(t,x)=\exp\left(\frac{V^{\lambda}\_{j}(t,x)-g\_{ij}-V^{\lambda}\_{i}(t,x)}{\lambda}\right),\quad j\in\mathbb{I}\_{m}\setminus\{i\},\penalty 10000\ (t,x)\in\overline{{\cal D}}. |  | (3.5) |

Plugging ([3.5](https://arxiv.org/html/2512.04697v1#S3.E5 "In 3 Exploratory Formulation under Entropy Regularization â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")) into ([3.4](https://arxiv.org/html/2512.04697v1#S3.E4 "In 3 Exploratory Formulation under Entropy Regularization â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")), we get

|  |  |  |  |
| --- | --- | --- | --- |
|  | âˆ‚ViÎ»â€‹(t,x)âˆ‚t+â„’xiâ€‹ViÎ»â€‹(t,x)+fâ€‹(t,x,i)+Î»â€‹âˆ‘jâ‰ iexpâ¡(VjÎ»â€‹(t,x)âˆ’giâ€‹jâˆ’ViÎ»â€‹(t,x)Î»)=0,(t,x)âˆˆğ’Ÿ,\displaystyle\frac{\partial V^{\lambda}\_{i}(t,x)}{\partial t}+\mathcal{L}^{i}\_{x}V^{\lambda}\_{i}(t,x)+f(t,x,i)+\lambda\sum\_{j\neq i}\exp\left(\frac{V^{\lambda}\_{j}(t,x)-g\_{ij}-V^{\lambda}\_{i}(t,x)}{\lambda}\right)=0,\penalty 10000\ (t,x)\in{\cal D}, |  | (3.6) |

with the terminal condition ViÎ»â€‹(T,x)=hâ€‹(x)V^{\lambda}\_{i}(T,x)=h(x) for xâˆˆâ„nx\in\mathds{R}^{n}.

To establish the well-posedness of the HJB system ([3.4](https://arxiv.org/html/2512.04697v1#S3.E4 "In 3 Exploratory Formulation under Entropy Regularization â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")), we impose the following assumption.

###### Assumption 3.1.

The running reward function fâ€‹(â‹…,â‹…,i)âˆˆCÎ±â€‹(ğ’Ÿ)f(\cdot,\cdot,i)\in C^{\alpha}({\cal D}) for iâˆˆğ•€mi\in\mathbb{I}\_{m} and terminal reward function hâ€‹(â‹…)âˆˆC2+Î±â€‹(ğ’Ÿ)h(\cdot)\in C^{2+\alpha}({\cal D}).

###### Lemma 3.2.

Let Assumptions [2.1](https://arxiv.org/html/2512.04697v1#S2.Thmtheorem1 "Assumption 2.1. â€£ 2 Classical Optimal Switching Problem â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes"), [2.2](https://arxiv.org/html/2512.04697v1#S2.Thmtheorem2 "Assumption 2.2. â€£ 2 Classical Optimal Switching Problem â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes") and [3.1](https://arxiv.org/html/2512.04697v1#S3.Thmtheorem1 "Assumption 3.1. â€£ 3 Exploratory Formulation under Entropy Regularization â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes") hold. Then for any Î»>0\lambda>0, the system of HJB equations ([3.4](https://arxiv.org/html/2512.04697v1#S3.E4 "In 3 Exploratory Formulation under Entropy Regularization â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")) has a classical solution (V1Î»,V2Î»,â‹¯,VmÎ»)(V^{\lambda}\_{1},V^{\lambda}\_{2},\cdots,V^{\lambda}\_{m}) with ViÎ»âˆˆC1,2â€‹(ğ’Ÿ)âˆ©Câ€‹(ğ’ŸÂ¯)V^{\lambda}\_{i}\in C^{1,2}({\cal D})\cap C(\overline{{\cal D}}) for iâˆˆğ•€mi\in\mathbb{I}\_{m}.

###### Proof.

Given M>0M>0, consider a smooth and non-decreasing cut-off function Ï•M\phi\_{M} such that Ï•Mâ€‹(x)=ex\phi\_{M}(x)=e^{x} for xâ‰¤Mx\leq M, Ï•Mâ€‹(x)â‰¤ex\phi\_{M}(x)\leq e^{x} for xâˆˆ(M,M+1)x\in(M,M+1) and Ï•Mâ€‹(x)=eM+1\phi\_{M}(x)=e^{M+1} for xâ‰¥M+1x\geq M+1. Hence, Ï•M\phi\_{M} is bounded and Lipschitz continuous. Denote ğ’ŸN:={(t,x):(t,x)âˆˆğ’Ÿ,|x|<N}{\cal D}\_{N}:=\{(t,x):(t,x)\in{\cal D},|x|<N\}. First, we will solve the following partial differential equation (PDE) systems: for iâˆˆğ•€mi\in\mathbb{I}\_{m},

|  |  |  |  |
| --- | --- | --- | --- |
|  | {âˆ‚ViM,Nâ€‹(t,x)âˆ‚t+â„’xiâ€‹ViM,Nâ€‹(t,x)+fâ€‹(t,x,i)+Î»â€‹âˆ‘jâ‰ iÏ•Mâ€‹(VjM,Nâ€‹(t,x)âˆ’ViM,Nâ€‹(t,x)âˆ’giâ€‹jÎ»)=0,(t,x)âˆˆğ’ŸN,ViM,Nâ€‹(t,x)=Kâ€‹(Tâˆ’t)+hâ€‹(x),(t,x)âˆˆâˆ‚ğ’ŸN,\displaystyle\begin{cases}\displaystyle\frac{\partial V\_{i}^{M,N}(t,x)}{\partial t}+\mathcal{L}^{i}\_{x}V\_{i}^{M,N}(t,x)+f(t,x,i)+\lambda\sum\_{j\neq i}\phi\_{M}\left(\frac{V^{M,N}\_{j}(t,x)-V^{M,N}\_{i}(t,x)-g\_{ij}}{\lambda}\right)=0,\\[10.00002pt] \displaystyle\hfill(t,x)\in{\cal D}\_{N},\\[10.00002pt] \displaystyle V\_{i}^{M,N}(t,x)=K(T-t)+h(x),\quad(t,x)\in\partial{\cal D}\_{N},\end{cases} |  | (3.7) |

where the constant K>0K>0 is given by

|  |  |  |  |
| --- | --- | --- | --- |
|  | K:=Kf,h+Î»â€‹supiâˆˆğ•€m(âˆ‘jâ‰ iexpâ¡(âˆ’giâ€‹jÎ»)).\displaystyle K:=K\_{f,h}+\lambda\sup\_{i\in\mathbb{I}\_{m}}\left(\sum\_{j\neq i}\exp\left(-\frac{g\_{ij}}{\lambda}\right)\right). |  | (3.8) |

For iâˆˆğ•€mi\in\mathbb{I}\_{m}, let us introduce the function

|  |  |  |  |
| --- | --- | --- | --- |
|  | uiâ€‹(t,x)\displaystyle u\_{i}(t,x) | =Kâ€‹(Tâˆ’t)+Kf,h,(t,x)âˆˆğ’ŸÂ¯N.\displaystyle=K(T-t)+K\_{f,h},\quad(t,x)\in\overline{{\cal D}}\_{N}. |  |

It follows from assumption [2.2](https://arxiv.org/html/2512.04697v1#S2.Thmtheorem2 "Assumption 2.2. â€£ 2 Classical Optimal Switching Problem â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes") that

|  |  |  |
| --- | --- | --- |
|  | âˆ‚uiâ€‹(t,x)âˆ‚t+â„’xiâ€‹uiâ€‹(t,x)+fâ€‹(t,x,i)+Î»â€‹âˆ‘jâ‰ iÏ•Mâ€‹(ujâ€‹(t,x)âˆ’uiâ€‹(t,x)âˆ’giâ€‹jÎ»)\displaystyle\frac{\partial u\_{i}(t,x)}{\partial t}+\mathcal{L}^{i}\_{x}u\_{i}(t,x)+f(t,x,i)+\lambda\sum\_{j\neq i}\phi\_{M}\left(\frac{u\_{j}(t,x)-u\_{i}(t,x)-g\_{ij}}{\lambda}\right) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | =K+fâ€‹(t,x,i)+Î»â€‹âˆ‘jâ‰ iÏ•Mâ€‹(âˆ’giâ€‹jÎ»)â‰¥0,âˆ€(t,x)âˆˆğ’ŸN,\displaystyle=K+f(t,x,i)+\lambda\sum\_{j\neq i}\phi\_{M}\left(-\frac{g\_{ij}}{\lambda}\right)\geq 0,\quad\forall(t,x)\in{\cal D}\_{N}, |  | (3.9) |

and uiâ€‹(t,x)â‰¥ViÎ»â€‹(t,x)u\_{i}(t,x)\geq V^{\lambda}\_{i}(t,x) for all (t,x)âˆˆâˆ‚ğ’ŸN(t,x)\in\partial{\cal D}\_{N}. Similarly, we have

|  |  |  |
| --- | --- | --- |
|  | âˆ‚(âˆ’uiâ€‹(t,x))âˆ‚t+â„’xiâ€‹(âˆ’uiâ€‹(t,x))+fâ€‹(t,x,i)+Î»â€‹âˆ‘jâ‰ iÏ•Mâ€‹((âˆ’ujâ€‹(t,x))âˆ’(âˆ’uiâ€‹(t,x))âˆ’giâ€‹jÎ»)\displaystyle\frac{\partial(-u\_{i}(t,x))}{\partial t}+\mathcal{L}^{i}\_{x}(-u\_{i}(t,x))+f(t,x,i)+\lambda\sum\_{j\neq i}\phi\_{M}\left(\frac{(-u\_{j}(t,x))-(-u\_{i}(t,x))-g\_{ij}}{\lambda}\right) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | =âˆ’K+fâ€‹(t,x,i)+Î»â€‹âˆ‘jâ‰ iÏ•Mâ€‹(âˆ’giâ€‹jÎ»)â‰¤0,âˆ€(t,x)âˆˆğ’ŸN,\displaystyle=-K+f(t,x,i)+\lambda\sum\_{j\neq i}\phi\_{M}\left(-\frac{g\_{ij}}{\lambda}\right)\leq 0,\quad\forall(t,x)\in{\cal D}\_{N}, |  | (3.10) |

and âˆ’uiâ€‹(t,x)â‰¤ViÎ»â€‹(t,x)-u\_{i}(t,x)\leq V\_{i}^{\lambda}(t,x) for all (t,x)âˆˆâˆ‚ğ’ŸN(t,x)\in\partial{\cal D}\_{N}. Invoking Theorem 2.1 in kusano1965first, we obtain that system ([3.7](https://arxiv.org/html/2512.04697v1#S3.E7 "In 3 Exploratory Formulation under Entropy Regularization â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")) has a classical solution (V1M,N,â‹¯,VmM,N)(V\_{1}^{M,N},\cdots,V\_{m}^{M,N}), with ViM,NâˆˆC1+Î´â€‹(ğ’ŸÂ¯N)V\_{i}^{M,N}\in C^{1+\delta}(\overline{{\cal D}}\_{N}) for any Î´âˆˆ(0,1)\delta\in(0,1) and ViM,NâˆˆC2+Î±â€‹(ğ’ŸÂ¯N)V\_{i}^{M,N}\in C^{2+\alpha}(\overline{{\cal D}}\_{N}). Furthermore, we deduce from the comparison theorem (Theorem 1.3 in kusano1965first) that

|  |  |  |  |
| --- | --- | --- | --- |
|  | |ViM,Nâ€‹(t,x)|â‰¤uiâ€‹(t,x)=Kâ€‹(Tâˆ’t)+Kf,h,âˆ€(i,t,x)âˆˆğ•€mÃ—ğ’ŸÂ¯N,\displaystyle|V\_{i}^{M,N}(t,x)|\leq u\_{i}(t,x)=K(T-t)+K\_{f,h},\quad\forall(i,t,x)\in\mathbb{I}\_{m}\times\overline{{\cal D}}\_{N}, |  | (3.11) |

which implies that ViM,Nâ€‹(t,x)V\_{i}^{M,N}(t,x) is bounded. Thus, by choosing some MM large enough, for each iâˆˆğ•€mi\in\mathbb{I}\_{m}, ViN:=ViM,NV^{N}\_{i}:=V^{M,N}\_{i} solves the following PDE

|  |  |  |  |
| --- | --- | --- | --- |
|  | {âˆ‚ViNâ€‹(t,x)âˆ‚t+â„’xiâ€‹ViNâ€‹(t,x)+fâ€‹(t,x,i)+Î»â€‹âˆ‘jâ‰ iexpâ¡(VjNâ€‹(t,x)âˆ’ViNâ€‹(t,x)âˆ’giâ€‹jÎ»)=0,(t,x)âˆˆğ’ŸN,ViNâ€‹(t,x)=Kâ€‹(Tâˆ’t)+hâ€‹(x),(t,x)âˆˆâˆ‚ğ’ŸN.\displaystyle\begin{cases}\displaystyle\frac{\partial V\_{i}^{N}(t,x)}{\partial t}+\mathcal{L}^{i}\_{x}V\_{i}^{N}(t,x)+f(t,x,i)+\lambda\sum\_{j\neq i}\exp\left(\frac{V^{N}\_{j}(t,x)-V^{N}\_{i}(t,x)-g\_{ij}}{\lambda}\right)=0,\quad(t,x)\in{\cal D}\_{N},\\[10.00002pt] \displaystyle V\_{i}^{N}(t,x)=K(T-t)+h(x),\quad(t,x)\in\partial{\cal D}\_{N}.\end{cases} |  | (3.12) |

First, we apply Lemma 2 in kusano1965first to the problem ([3.12](https://arxiv.org/html/2512.04697v1#S3.E12 "In 3 Exploratory Formulation under Entropy Regularization â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")) to derive for any Î´âˆˆ(0,1)\delta\in(0,1),

|  |  |  |
| --- | --- | --- |
|  | â€–ViNâ€–C1+Î´â€‹(ğ’ŸN)â‰¤Câ€‹(1+â€–fâ€‹(â‹…,â‹…,i)â€–C0â€‹(ğ’ŸN)+â€–hâ€–C2â€‹(ğ’ŸN)).\displaystyle||V\_{i}^{N}||\_{C^{1+\delta}({\cal D}\_{N})}\leq C\left(1+||f(\cdot,\cdot,i)||\_{C^{0}({\cal D}\_{N})}+||h||\_{C^{2}({\cal D}\_{N})}\right). |  |

In particular, â€–ViNâ€–CÎ±â€‹(ğ’ŸN)||V\_{i}^{N}||\_{C^{\alpha}({\cal D}\_{N})} are bounded independently of NN. We then apply Lemma 1 in kusano1965first to the problem ([3.12](https://arxiv.org/html/2512.04697v1#S3.E12 "In 3 Exploratory Formulation under Entropy Regularization â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")), obtaining

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–ViNâ€–C2+Î±â€‹(ğ’ŸN)\displaystyle||V\_{i}^{N}||\_{C^{2+\alpha}({\cal D}\_{N})} | â‰¤Câ€‹(1+â€–fâ€‹(â‹…,â‹…,i)â€–CÎ±â€‹(ğ’ŸN)+â€–hâ€–C2+Î±â€‹(ğ’ŸN))\displaystyle\leq C\left(1+||f(\cdot,\cdot,i)||\_{C^{\alpha}({\cal D}\_{N})}+||h||\_{C^{2+\alpha}({\cal D}\_{N})}\right) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤Câ€‹(1+â€–fâ€‹(â‹…,â‹…,i)â€–CÎ±â€‹(ğ’Ÿ)+â€–hâ€–C2+Î±â€‹(ğ’Ÿ)).\displaystyle\leq C\left(1+||f(\cdot,\cdot,i)||\_{C^{\alpha}({\cal D})}+||h||\_{C^{2+\alpha}({\cal D})}\right). |  |

Consequently, we can extract from {ViNâ€‹(t,x)}\{V\_{i}^{N}(t,x)\} a subsequence converging uniformly in ğ’ŸÂ¯\overline{{\cal D}} together with the first xx , tt-derivatives and second xx-derivatives to a limit function ViÎ»V\_{i}^{\lambda}, which is a solution to the HJB system ([3.4](https://arxiv.org/html/2512.04697v1#S3.E4 "In 3 Exploratory Formulation under Entropy Regularization â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")). The uniqueness of the solution follows from Theorem 1.3 in kusano1965first. Thus, we complete the proof of the theorem.
âˆ

By the proof of Lemma [3.2](https://arxiv.org/html/2512.04697v1#S3.Thmtheorem2 "Lemma 3.2. â€£ 3 Exploratory Formulation under Entropy Regularization â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes"), for any Î»>0\lambda>0, the system of HJB equations ([3.4](https://arxiv.org/html/2512.04697v1#S3.E4 "In 3 Exploratory Formulation under Entropy Regularization â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")) admits a classical solution (V1Î»,V2Î»,â‹¯,VmÎ»)(V^{\lambda}\_{1},V^{\lambda}\_{2},\cdots,V^{\lambda}\_{m}) satisfying

|  |  |  |  |
| --- | --- | --- | --- |
|  | |ViÎ»â€‹(t,x)|â‰¤Kâ€‹(Tâˆ’t)+Kf,h,âˆ€(i,t,x)âˆˆğ•€mÃ—ğ’ŸÂ¯,\displaystyle|V\_{i}^{\lambda}(t,x)|\leq K(T-t)+K\_{f,h},\quad\forall(i,t,x)\in\mathbb{I}\_{m}\times\overline{{\cal D}}, |  | (3.13) |

where the constant K>0K>0 is given by ([3.8](https://arxiv.org/html/2512.04697v1#S3.E8 "In 3 Exploratory Formulation under Entropy Regularization â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")). We now prove that this bounded classical solution is unique and coincides with the value function.

###### Proposition 3.3 (Verification Theorem).

Suppose Assumptions [2.1](https://arxiv.org/html/2512.04697v1#S2.Thmtheorem1 "Assumption 2.1. â€£ 2 Classical Optimal Switching Problem â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes"), [2.2](https://arxiv.org/html/2512.04697v1#S2.Thmtheorem2 "Assumption 2.2. â€£ 2 Classical Optimal Switching Problem â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes"), and [3.1](https://arxiv.org/html/2512.04697v1#S3.Thmtheorem1 "Assumption 3.1. â€£ 3 Exploratory Formulation under Entropy Regularization â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes") hold, and let (V1Î»,V2Î»,â‹¯,VmÎ»)(V\_{1}^{\lambda},V\_{2}^{\lambda},\cdots,V\_{m}^{\lambda}) be a bounded classical solution to system ([3.4](https://arxiv.org/html/2512.04697v1#S3.E4 "In 3 Exploratory Formulation under Entropy Regularization â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")), as provided by Lemma [3.2](https://arxiv.org/html/2512.04697v1#S3.Thmtheorem2 "Lemma 3.2. â€£ 3 Exploratory Formulation under Entropy Regularization â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes"). We define a set of feedback functions by

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ï€iâ€‹jâˆ—â€‹(t,x)=expâ¡(VjÎ»â€‹(t,x)âˆ’giâ€‹jâˆ’ViÎ»â€‹(t,x)Î»),jâˆˆğ•€mâˆ–{i},(t,x)âˆˆğ’ŸÂ¯,\displaystyle\pi\_{ij}^{\*}(t,x)=\exp\left(\frac{V^{\lambda}\_{j}(t,x)-g\_{ij}-V^{\lambda}\_{i}(t,x)}{\lambda}\right),\quad j\in\mathbb{I}\_{m}\setminus\{i\},\penalty 10000\ (t,x)\in\overline{{\cal D}}, |  | (3.14) |

and

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ï€iâ€‹iâˆ—â€‹(t,x)=âˆ’âˆ‘jâ‰ iÏ€iâ€‹jâˆ—â€‹(t,x),(t,x)âˆˆğ’ŸÂ¯.\displaystyle\pi\_{ii}^{\*}(t,x)=-\sum\_{j\neq i}\pi\_{ij}^{\*}(t,x),\quad(t,x)\in\overline{{\cal D}}. |  | (3.15) |

Consider the process Xâˆ—X^{\*} governed by the dynamics ([3.1](https://arxiv.org/html/2512.04697v1#S3.E1 "In 3 Exploratory Formulation under Entropy Regularization â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")), where the generator of the process Iâˆ—I^{\*} is given by ğ›‘âˆ—=(Ï€tiâ€‹j,âˆ—)i,jâˆˆğ•€m,tâˆˆ[0,T]{\bm{\pi}}^{\*}=(\pi^{ij,\*}\_{t})\_{i,j\in\mathbb{I}\_{m},t\in[0,T]} and Ï€tiâ€‹j,âˆ—=Ï€iâ€‹jâˆ—â€‹(t,Xtâˆ—)\pi\_{t}^{ij,\*}=\pi\_{ij}^{\*}(t,X\_{t}^{\*}). Then, for each iâˆˆğ•€mi\in\mathbb{I}\_{m}, the function ViÎ»V\_{i}^{\lambda} is the value function for problem ([3.3](https://arxiv.org/html/2512.04697v1#S3.E3 "In 3 Exploratory Formulation under Entropy Regularization â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")), and the policy ğ›‘âˆ—{\bm{\pi}}^{\*} is optimal.

###### Proof.

For (i,t,x)âˆˆğ•€mÃ—ğ’ŸÂ¯(i,t,x)\in\mathbb{I}\_{m}\times\overline{{\cal D}}, ğ…âˆˆğ•Œt{\bm{\pi}}\in\mathbb{U}\_{t} and sâˆˆ[t,T]s\in[t,T], using ItÃ´â€™s rule, we obtain

|  |  |  |  |
| --- | --- | --- | --- |
|  | VIsÎ»â€‹(s,Xs)\displaystyle V\_{I\_{s}}^{\lambda}(s,X\_{s}) | =ViÎ»â€‹(t,x)+âˆ«ts(âˆ‚VIâ„“Î»â€‹(â„“,Xâ„“)âˆ‚t+â„’xiâ€‹VIâ„“Î»â€‹(â„“,Xâ„“))â€‹ğ‘‘â„“+âˆ«ts(Dxâ€‹VIâ„“Î»â€‹(â„“,Xâ„“))âŠ¤â€‹Ïƒâ€‹(â„“,Xâ„“,Iâ„“)â€‹ğ‘‘Wâ„“\displaystyle=V\_{i}^{\lambda}(t,x)+\int\_{t}^{s}\left(\frac{\partial V\_{I\_{\ell}}^{\lambda}(\ell,X\_{\ell})}{\partial t}+\mathcal{L}^{i}\_{x}V\_{I\_{\ell}}^{\lambda}(\ell,X\_{\ell})\right)d\ell+\int\_{t}^{s}(D\_{x}V\_{I\_{\ell}}^{\lambda}(\ell,X\_{\ell}))^{\top}\sigma(\ell,X\_{\ell},I\_{\ell})dW\_{\ell} |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | +âˆ«tsâˆ‘jâ‰ Iâ„“(Ï€â„“iâ€‹jâ€‹(VjÎ»â€‹(â„“,Xâ„“)âˆ’VIâ„“Î»â€‹(â„“,Xâ„“)))â€‹dâ€‹â„“.\displaystyle\quad+\int\_{t}^{s}\sum\_{j\neq I\_{\ell}}\left(\pi\_{\ell}^{ij}(V^{\lambda}\_{j}(\ell,X\_{\ell})-V^{\lambda}\_{I\_{\ell}}(\ell,X\_{\ell}))\right)d\ell. |  | (3.16) |

Taking the expectation on both sides of Eq. ([3](https://arxiv.org/html/2512.04697v1#S3.Ex7 "3 Exploratory Formulation under Entropy Regularization â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")), it follows from ([3.4](https://arxiv.org/html/2512.04697v1#S3.E4 "In 3 Exploratory Formulation under Entropy Regularization â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")) that

|  |  |  |  |
| --- | --- | --- | --- |
|  | ViÎ»â€‹(t,x)â‰¥ğ”¼t,x,iâ€‹[âˆ«tsfâ€‹(â„“,Xâ„“,Iâ„“)â€‹ğ‘‘sâˆ’âˆ‘k=1âˆgÎºkâˆ’1â€‹Îºkâ€‹ğŸ{Ï„kâ‰¤s}+Î»â€‹âˆ«tsRâ€‹(ğ…â„“,Iâ„“)â€‹ğ‘‘s+VIsÎ»â€‹(s,Xs)]\displaystyle V\_{i}^{\lambda}(t,x)\geq\mathbb{E}\_{t,x,i}\bigg[\int^{s}\_{t}f(\ell,X\_{\ell},I\_{\ell})ds-\sum\_{k=1}^{\infty}g\_{\kappa\_{k-1}\kappa\_{k}}{\bf 1}\_{\{\tau\_{k}\leq s\}}+\lambda\int^{s}\_{t}R({\bm{\pi}}\_{\ell},I\_{\ell})ds+V\_{I\_{s}}^{\lambda}(s,X\_{s})\bigg] |  | (3.17) |

Letting sâ†’Ts\to T in ([3.17](https://arxiv.org/html/2512.04697v1#S3.E17 "In 3 Exploratory Formulation under Entropy Regularization â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")), we get from ViÎ»=hâ€‹(i,t,x)V\_{i}^{\lambda}=h(i,t,x) and the dominated convergence theorem that

|  |  |  |  |
| --- | --- | --- | --- |
|  | ViÎ»â€‹(t,x)â‰¥ğ”¼t,x,iâ€‹[âˆ«tTfâ€‹(â„“,Xâ„“,Iâ„“)â€‹ğ‘‘sâˆ’âˆ‘k=1âˆgÎºkâˆ’1â€‹Îºkâ€‹ğŸ{Ï„kâ‰¤T}+Î»â€‹âˆ«tTRâ€‹(ğ…â„“,Iâ„“)â€‹ğ‘‘s+hâ€‹(XT)].\displaystyle V\_{i}^{\lambda}(t,x)\geq\mathbb{E}\_{t,x,i}\bigg[\int^{T}\_{t}f(\ell,X\_{\ell},I\_{\ell})ds-\sum\_{k=1}^{\infty}g\_{\kappa\_{k-1}\kappa\_{k}}{\bf 1}\_{\{\tau\_{k}\leq T\}}+\lambda\int^{T}\_{t}R({\bm{\pi}}\_{\ell},I\_{\ell})ds+h(X\_{T})\bigg]. |  | (3.18) |

The inequality ([3.18](https://arxiv.org/html/2512.04697v1#S3.E18 "In 3 Exploratory Formulation under Entropy Regularization â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")) holds for any ğ…âˆˆğ•Œt{\bm{\pi}}\in\mathbb{U}\_{t} and becomes an equality when ğ…=ğ…âˆ—{\bm{\pi}}={\bm{\pi}^{\*}}. Furthermore, Theorem 2.6 in nguyen2025hybrid guarantees the existence and uniqueness of the strong solution (Xâˆ—,Iâˆ—)(X^{\*},I^{\*}) to the SDE ([3.1](https://arxiv.org/html/2512.04697v1#S3.E1 "In 3 Exploratory Formulation under Entropy Regularization â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")). Thus, we complete the proof of the theorem.
âˆ

## 4 Policy Iteration and Convergence

The goal of this section is to study the policy iteration using the characterization in ([3.5](https://arxiv.org/html/2512.04697v1#S3.E5 "In 3 Exploratory Formulation under Entropy Regularization â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")). In particular, in the context of optimal regime switching, we aim to show the policy improvement and the convergence of policy iterations, which demonstrate that each policy update guarantees the performance enhancement and the repeated iterations will lead to the desired optimal policy when the model is known. We also examine the connection between our exploratory formulation and the classical optimal switching problem by analyzing the limit of the vanishing regularization.

We first focus on the rule of policy iteration. Given a feedback strategy ğ…nâ€‹(t,x)=(Ï€iâ€‹jnâ€‹(t,x))i,jâˆˆğ•€m{\bm{\pi}}^{n}(t,x)=(\pi\_{ij}^{n}(t,x))\_{i,j\in\mathbb{I}\_{m}}, the corresponding value function (V1n,â‹¯,Vmn)(V^{n}\_{1},\cdots,V^{n}\_{m}) satisfies the following PDE system: for iâˆˆğ•€mi\in\mathbb{I}\_{m},

|  |  |  |  |
| --- | --- | --- | --- |
|  | {âˆ‚Vinâ€‹(t,x)âˆ‚t+â„’xiâ€‹Vinâ€‹(t,x)+fâ€‹(t,x,i)+Hiâ€‹(ğ…inâ€‹(t,x),V1nâ€‹(t,x),â‹¯,Vmnâ€‹(t,x))=0,Vinâ€‹(T,x)=hâ€‹(x),\displaystyle\begin{cases}\displaystyle\frac{\partial V\_{i}^{n}(t,x)}{\partial t}+\mathcal{L}^{i}\_{x}V\_{i}^{n}(t,x)+f(t,x,i)+H\_{i}({\bm{\pi}}^{n}\_{i}(t,x),V\_{1}^{n}(t,x),\cdots,V\_{m}^{n}(t,x))=0,\\ \displaystyle V\_{i}^{n}(T,x)=h(x),\end{cases} |  | (4.1) |

Here, the Hamiltomian Hiâ€‹(ğ…i,ğ’š):â„mÃ—â„mâ†’â„H\_{i}({\bm{\pi}}\_{i},{\bm{y}}):\mathds{R}^{m}\times\mathds{R}^{m}\to\mathds{R} is defined by

|  |  |  |  |
| --- | --- | --- | --- |
|  | Hiâ€‹(ğ…i,ğ’š)=âˆ‘jâ‰ iÏ€iâ€‹jâ€‹(yjâˆ’giâ€‹jâˆ’yi)+Î»â€‹âˆ‘jâ‰ i(Ï€iâ€‹jâˆ’Ï€iâ€‹jâ€‹logâ¡Ï€iâ€‹j).\displaystyle H\_{i}({\bm{\pi}}\_{i},{\bm{y}})=\sum\_{j\neq i}\pi\_{ij}(y\_{j}-g\_{ij}-y\_{i})+\lambda\sum\_{j\neq i}(\pi\_{ij}-\pi\_{ij}\log\pi\_{ij}). |  | (4.2) |

Having the value function pair (V1n,â‹¯,Vmn)(V^{n}\_{1},\cdots,V\_{m}^{n}), one can construct a feedback strategy ğ…n+1{\bm{\pi}}^{n+1} satisfying

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ï€iâ€‹jn+1â€‹(t,x)=expâ¡(Vjnâ€‹(t,x)âˆ’giâ€‹jâˆ’Vinâ€‹(t,x)Î»),i,jâˆˆğ•€m,jâ‰ i.\displaystyle\pi\_{ij}^{n+1}(t,x)=\exp\left(\frac{V^{n}\_{j}(t,x)-g\_{ij}-V^{n}\_{i}(t,x)}{\lambda}\right),\penalty 10000\ i,j\in\mathbb{I}\_{m},j\neq i. |  | (4.3) |

We continue this iteration, generating a sequence of strategy-value function pairs. The following theorem states that each iteration improves the value function.

###### Proposition 4.1.

Let Assumptions [2.1](https://arxiv.org/html/2512.04697v1#S2.Thmtheorem1 "Assumption 2.1. â€£ 2 Classical Optimal Switching Problem â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes"), [2.2](https://arxiv.org/html/2512.04697v1#S2.Thmtheorem2 "Assumption 2.2. â€£ 2 Classical Optimal Switching Problem â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes") and [3.1](https://arxiv.org/html/2512.04697v1#S3.Thmtheorem1 "Assumption 3.1. â€£ 3 Exploratory Formulation under Entropy Regularization â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes") hold. Give any initial guess (V10,â‹¯,Vm0)(V^{0}\_{1},\cdots,V^{0}\_{m}) with Vi0âˆˆC0â€‹(ğ’ŸÂ¯)V^{0}\_{i}\in C^{0}(\overline{{\cal D}}) for iâˆˆğ•€mi\in\mathbb{I}\_{m}. {(Vin,Ï€iâ€‹jn)i,jâˆˆğ•€m}n=1,2,â€¦\{(V^{n}\_{i},\pi^{n}\_{ij})\_{i,j\in\mathbb{I}\_{m}}\}\_{n=1,2,\ldots} are defined iteratively according to ([4.1](https://arxiv.org/html/2512.04697v1#S4.E1 "In 4 Policy Iteration and Convergence â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")) and ([4.3](https://arxiv.org/html/2512.04697v1#S4.E3 "In 4 Policy Iteration and Convergence â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")). Then, we have that Vinâ‰¤Vin+1â‰¤ViÎ»V^{n}\_{i}\leq V^{n+1}\_{i}\leq V\_{i}^{\lambda} for iâˆˆğ•€mi\in\mathbb{I}\_{m} and n=1,2,â€¦n=1,2,\ldots.

###### Proof.

For nâ‰¥1n\geq 1, let Î”inâ€‹(t,x):=Vin+1â€‹(t,x)âˆ’Vinâ€‹(t,x)\Delta\_{i}^{n}(t,x):=V\_{i}^{n+1}(t,x)-V\_{i}^{n}(t,x), for iâˆˆğ•€mi\in\mathbb{I}\_{m} and (t,x)âˆˆğ’ŸÂ¯(t,x)\in\overline{{\cal D}}. By using ([4.1](https://arxiv.org/html/2512.04697v1#S4.E1 "In 4 Policy Iteration and Convergence â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")), Î”inâ€‹(t,x)\Delta\_{i}^{n}(t,x) satisfies

|  |  |  |  |
| --- | --- | --- | --- |
|  | âˆ‚Î”inâ€‹(t,x)âˆ‚t+â„’xiâ€‹Î”iÎ»â€‹(t,x)+\displaystyle\frac{\partial\Delta\_{i}^{n}(t,x)}{\partial t}+\mathcal{L}^{i}\_{x}\Delta\_{i}^{\lambda}(t,x)+ | Hiâ€‹(ğ…in+1â€‹(t,x),V1n+1â€‹(t,x),â‹¯,Vmn+1â€‹(t,x))\displaystyle H\_{i}({\bm{\pi}}^{n+1}\_{i}(t,x),V\_{1}^{n+1}(t,x),\cdots,V\_{m}^{n+1}(t,x)) |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | âˆ’\displaystyle- | Hiâ€‹(ğ…inâ€‹(t,x),V1nâ€‹(t,x),â‹¯,Vmnâ€‹(t,x))=0,forâ€‹(t,x)âˆˆğ’Ÿ,\displaystyle H\_{i}({\bm{\pi}}^{n}\_{i}(t,x),V\_{1}^{n}(t,x),\cdots,V\_{m}^{n}(t,x))=0,\quad\text{for}\penalty 10000\ (t,x)\in{\cal D}, |  | (4.4) |

with the terminal condition Î”inâ€‹(T,x)=0\Delta\_{i}^{n}(T,x)=0 for xâˆˆâ„nx\in\mathds{R}^{n}. From ([4.3](https://arxiv.org/html/2512.04697v1#S4.E3 "In 4 Policy Iteration and Convergence â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")), we can see

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ…in+1â€‹(t,x)=argâ€‹maxğ…iâ€‹Hiâ€‹(ğ…i,V1nâ€‹(t,x),â‹¯,Vmnâ€‹(t,x)).\displaystyle{\bm{\pi}}\_{i}^{n+1}(t,x)=\underset{{\bm{\pi}}\_{i}}{\mathrm{arg\,max\,}}H\_{i}({\bm{\pi}}\_{i},V\_{1}^{n}(t,x),\cdots,V\_{m}^{n}(t,x)). |  | (4.5) |

It follows from ([4](https://arxiv.org/html/2512.04697v1#S4.Ex1 "4 Policy Iteration and Convergence â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")) and ([4.5](https://arxiv.org/html/2512.04697v1#S4.E5 "In 4 Policy Iteration and Convergence â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")) that, for (t,x)âˆˆğ’Ÿ(t,x)\in{\cal D},

|  |  |  |  |
| --- | --- | --- | --- |
|  |  | âˆ‚Î”inâ€‹(t,x)âˆ‚t+â„’xiâ€‹Î”inâ€‹(t,x)+âˆ‘jâ‰ iÏ€iâ€‹jn+1â€‹(t,x)â€‹Î”jnâ€‹(t,x)âˆ’âˆ‘jâ‰ iÏ€iâ€‹jn+1â€‹(t,x)â€‹Î”inâ€‹(t,x)\displaystyle\frac{\partial\Delta\_{i}^{n}(t,x)}{\partial t}+\mathcal{L}^{i}\_{x}\Delta\_{i}^{n}(t,x)+\sum\_{j\neq i}\pi\_{ij}^{n+1}(t,x)\Delta\_{j}^{n}(t,x)-\sum\_{j\neq i}\pi\_{ij}^{n+1}(t,x)\Delta\_{i}^{n}(t,x) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =âˆ’Hiâ€‹(ğ…in+1â€‹(t,x),V1n+1â€‹(t,x),â‹¯,Vmn+1â€‹(t,x))âˆ’âˆ‘jâ‰ iÏ€iâ€‹jn+1â€‹(t,x)â€‹(Î”jnâ€‹(t,x)âˆ’Î”inâ€‹(t,x))\displaystyle=-H\_{i}({\bm{\pi}}\_{i}^{n+1}(t,x),V\_{1}^{n+1}(t,x),\cdots,V\_{m}^{n+1}(t,x))-\sum\_{j\neq i}\pi\_{ij}^{n+1}(t,x)(\Delta\_{j}^{n}(t,x)-\Delta\_{i}^{n}(t,x)) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +Hiâ€‹(ğ…inâ€‹(t,x),V1nâ€‹(t,x),â‹¯,Vmnâ€‹(t,x))\displaystyle\quad+H\_{i}({\bm{\pi}}^{n}\_{i}(t,x),V\_{1}^{n}(t,x),\cdots,V\_{m}^{n}(t,x)) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =Hiâ€‹(ğ…inâ€‹(t,x),V1nâ€‹(t,x),â‹¯,Vmnâ€‹(t,x))âˆ’Hiâ€‹(ğ…in+1â€‹(t,x),V1nâ€‹(t,x),â‹¯,Vmnâ€‹(t,x))\displaystyle=H\_{i}({\bm{\pi}}^{n}\_{i}(t,x),V\_{1}^{n}(t,x),\cdots,V\_{m}^{n}(t,x))-H\_{i}({\bm{\pi}}^{n+1}\_{i}(t,x),V\_{1}^{n}(t,x),\cdots,V\_{m}^{n}(t,x)) |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | â‰¤0.\displaystyle\leq 0. |  | (4.6) |

By applying Theorem 1.3 in kusano1965first, we deduce that Î”inâ€‹(t,x)â‰¥0\Delta^{n}\_{i}(t,x)\geq 0, that is, Vin+1â€‹(t,x)â‰¥Vinâ€‹(t,x)V\_{i}^{n+1}(t,x)\geq V\_{i}^{n}(t,x), for all iâˆˆğ•€mi\in\mathbb{I}\_{m} and (t,x)âˆˆğ’ŸÂ¯(t,x)\in\overline{{\cal D}}.

On the other hand, for nâ‰¥1n\geq 1, let Î”~inâ€‹(t,x):=ViÎ»â€‹(t,x)âˆ’Vinâ€‹(t,x)\tilde{\Delta}\_{i}^{n}(t,x):=V\_{i}^{\lambda}(t,x)-V\_{i}^{n}(t,x), for iâˆˆğ•€mi\in\mathbb{I}\_{m} and (t,x)âˆˆğ’ŸÂ¯(t,x)\in\overline{{\cal D}}. In a similar fashion, it can be shown that, for (t,x)âˆˆğ’Ÿ(t,x)\in{\cal D},

|  |  |  |  |
| --- | --- | --- | --- |
|  |  | âˆ‚Î”~inâ€‹(t,x)âˆ‚t+â„’xiâ€‹Î”~inâ€‹(t,x)+âˆ‘jâ‰ iÏ€iâ€‹jn+1â€‹(t,x)â€‹Î”~jnâ€‹(t,x)âˆ’âˆ‘jâ‰ iÏ€iâ€‹jn+1â€‹(t,x)â€‹Î”~inâ€‹(t,x)\displaystyle\frac{\partial\tilde{\Delta}\_{i}^{n}(t,x)}{\partial t}+\mathcal{L}^{i}\_{x}\tilde{\Delta}\_{i}^{n}(t,x)+\sum\_{j\neq i}\pi\_{ij}^{n+1}(t,x)\tilde{\Delta}\_{j}^{n}(t,x)-\sum\_{j\neq i}\pi\_{ij}^{n+1}(t,x)\tilde{\Delta}\_{i}^{n}(t,x) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =Hiâ€‹(ğ…inâ€‹(t,x),V1Î»â€‹(t,x),â‹¯,VmÎ»â€‹(t,x))âˆ’Hiâ€‹(ğ…iâˆ—â€‹(t,x),V1Î»â€‹(t,x),â‹¯,VmÎ»â€‹(t,x))\displaystyle=H\_{i}({\bm{\pi}}^{n}\_{i}(t,x),V\_{1}^{\lambda}(t,x),\cdots,V\_{m}^{\lambda}(t,x))-H\_{i}({\bm{\pi}}^{\*}\_{i}(t,x),V\_{1}^{\lambda}(t,x),\cdots,V\_{m}^{\lambda}(t,x)) |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | â‰¤0,\displaystyle\leq 0, |  | (4.7) |

and Î”~inâ€‹(T,x)=0\tilde{\Delta}\_{i}^{n}(T,x)=0 for xâˆˆâ„nx\in\mathds{R}^{n}. By applying Theorem 1.3 in kusano1965first again, Î”~inâ€‹(t,x)â‰¥0\tilde{\Delta}\_{i}^{n}(t,x)\geq 0, i.e., ViÎ»â€‹(t,x)â‰¥Vinâ€‹(t,x)V\_{i}^{\lambda}(t,x)\geq V\_{i}^{n}(t,x), for all iâˆˆğ•€mi\in\mathbb{I}\_{m} and (t,x)âˆˆğ’ŸÂ¯(t,x)\in\overline{{\cal D}}, which then completes the proof.
âˆ

The following theorem, as the first main result of this paper, establishes a fundamental convergence guarantee for our policy iteration method, demonstrating that the sequence of value functions (V1n,â‹¯,Vmn)(V\_{1}^{n},\cdots,V\_{m}^{n}) generated through successive iterations converges uniformly to the optimal value functions (V1Î»â€‹â‹¯,VmÎ»)(V\_{1}^{\lambda}\cdots,V^{\lambda}\_{m}) of our exploratory optimal switching problem. Moreover, we can obtain the explicit convergence rate for the policy iteration.

###### Theorem 4.2.

Let Assumptions [2.1](https://arxiv.org/html/2512.04697v1#S2.Thmtheorem1 "Assumption 2.1. â€£ 2 Classical Optimal Switching Problem â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes"), [2.2](https://arxiv.org/html/2512.04697v1#S2.Thmtheorem2 "Assumption 2.2. â€£ 2 Classical Optimal Switching Problem â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes") and [3.1](https://arxiv.org/html/2512.04697v1#S3.Thmtheorem1 "Assumption 3.1. â€£ 3 Exploratory Formulation under Entropy Regularization â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes") hold. Give any initial guess (V10,â‹¯,Vm0)(V^{0}\_{1},\cdots,V^{0}\_{m}) with Vi0âˆˆC0â€‹(ğ’ŸÂ¯)V^{0}\_{i}\in C^{0}(\overline{{\cal D}}) for iâˆˆğ•€mi\in\mathbb{I}\_{m}. {(Vin,Ï€iâ€‹jn)i,jâˆˆğ•€m}n=1,2,â€¦\{(V^{n}\_{i},\pi^{n}\_{ij})\_{i,j\in\mathbb{I}\_{m}}\}\_{n=1,2,\ldots} are defined iteratively according to ([4.1](https://arxiv.org/html/2512.04697v1#S4.E1 "In 4 Policy Iteration and Convergence â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")) and ([4.3](https://arxiv.org/html/2512.04697v1#S4.E3 "In 4 Policy Iteration and Convergence â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")). Then, we have that, for all nâ‰¥1n\geq 1,

|  |  |  |  |
| --- | --- | --- | --- |
|  | supiâˆˆğ•€msup(t,x)âˆˆğ’ŸÂ¯|Vinâ€‹(t,x)âˆ’ViÎ»â€‹(t,x)|â‰¤C1â€‹C2nn!,\displaystyle\sup\_{i\in\mathbb{I}\_{m}}\sup\_{(t,x)\in\overline{{\cal D}}}|V\_{i}^{n}(t,x)-V\_{i}^{\lambda}(t,x)|\leq C\_{1}\frac{C\_{2}^{n}}{n!}, |  | (4.8) |

where C1,C2>0C\_{1},C\_{2}>0 are constants independent of nn.

###### Proof.

For nâ‰¥0n\geq 0, let us introduce the function Fn:[0,T]â†’â„+F^{n}:[0,T]\to\mathds{R}\_{+} given by

|  |  |  |  |
| --- | --- | --- | --- |
|  | Fnâ€‹(t):=supiâˆˆğ•€msupxâˆˆâ„n|Vinâ€‹(t,x)âˆ’ViÎ»â€‹(t,x)|.\displaystyle F^{n}(t):=\sup\_{i\in\mathbb{I}\_{m}}\sup\_{x\in\mathds{R}^{n}}|V\_{i}^{n}(t,x)-V\_{i}^{\lambda}(t,x)|. |  | (4.9) |

By the proof of Lemma [3.2](https://arxiv.org/html/2512.04697v1#S3.Thmtheorem2 "Lemma 3.2. â€£ 3 Exploratory Formulation under Entropy Regularization â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes"), we can obtain

|  |  |  |  |
| --- | --- | --- | --- |
|  | |ViÎ»â€‹(t,x)|â‰¤Kâ€‹(Tâˆ’t)+Kf,hâ‰¤Kâ€‹T+Kf,h,âˆ€(i,t,x)âˆˆğ•€mÃ—ğ’ŸÂ¯,\displaystyle|V\_{i}^{\lambda}(t,x)|\leq K(T-t)+K\_{f,h}\leq KT+K\_{f,h},\quad\forall(i,t,x)\in\mathbb{I}\_{m}\times\overline{{\cal D}}, |  | (4.10) |

where the constant KK is given by ([3.8](https://arxiv.org/html/2512.04697v1#S3.E8 "In 3 Exploratory Formulation under Entropy Regularization â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")). This implies the boundedness of ViÎ»â€‹(t,x)V\_{i}^{\lambda}(t,x), which in turn implies that the policy ğ…âˆ—{\bm{\pi}}^{\*} from ([3.5](https://arxiv.org/html/2512.04697v1#S3.E5 "In 3 Exploratory Formulation under Entropy Regularization â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")) is bounded. Similarly, by using Theorem [4.1](https://arxiv.org/html/2512.04697v1#S4.Thmtheorem1 "Proposition 4.1. â€£ 4 Policy Iteration and Convergence â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes") and ([4.3](https://arxiv.org/html/2512.04697v1#S4.E3 "In 4 Policy Iteration and Convergence â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")), we can deduce that the sequence of functions Vinâ€‹(t,x)V^{n}\_{i}(t,x) and the corresponding policies ğ…nâ€‹(t,x){\bm{\pi}}^{n}(t,x) are uniformly bounded for nâ‰¥1n\geq 1. Then, it follows from ([3.5](https://arxiv.org/html/2512.04697v1#S3.E5 "In 3 Exploratory Formulation under Entropy Regularization â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")), ([4.2](https://arxiv.org/html/2512.04697v1#S4.E2 "In 4 Policy Iteration and Convergence â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")) and ([4.3](https://arxiv.org/html/2512.04697v1#S4.E3 "In 4 Policy Iteration and Convergence â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")) that

|  |  |  |  |
| --- | --- | --- | --- |
|  |  | |Hiâ€‹(ğ…in+1,V1Î»,â‹¯,VmÎ»)âˆ’Hiâ€‹(ğ…iâˆ—,V1Î»,â‹¯,VmÎ»)|\displaystyle\left|H\_{i}({\bm{\pi}}\_{i}^{n+1},V\_{1}^{\lambda},\cdots,V\_{m}^{\lambda})-H\_{i}({\bm{\pi}}\_{i}^{\*},V\_{1}^{\lambda},\cdots,V\_{m}^{\lambda})\right| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤|âˆ‘jâ‰ iÏ€iâ€‹jnâ€‹(VjÎ»âˆ’giâ€‹jâˆ’ViÎ»)+Î»â€‹âˆ‘jâ‰ i(Ï€iâ€‹jnâˆ’Ï€iâ€‹jnâ€‹logâ¡Ï€iâ€‹jn)|\displaystyle\leq\left|\sum\_{j\neq i}\pi\_{ij}^{n}(V\_{j}^{\lambda}-g\_{ij}-V\_{i}^{\lambda})+\lambda\sum\_{j\neq i}(\pi\_{ij}^{n}-\pi\_{ij}^{n}\log\pi\_{ij}^{n})\right| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +|(âˆ‘jâ‰ iÏ€iâ€‹jâˆ—â€‹(VjÎ»âˆ’giâ€‹jâˆ’ViÎ»)+Î»â€‹âˆ‘jâ‰ i(Ï€iâ€‹jâˆ—âˆ’Ï€iâ€‹jâˆ—â€‹logâ¡Ï€iâ€‹jâˆ—))|\displaystyle\quad+\left|\left(\sum\_{j\neq i}\pi\_{ij}^{\*}(V\_{j}^{\lambda}-g\_{ij}-V\_{i}^{\lambda})+\lambda\sum\_{j\neq i}(\pi\_{ij}^{\*}-\pi\_{ij}^{\*}\log\pi\_{ij}^{\*})\right)\right| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤âˆ‘jâ‰ iÏ€iâ€‹jnâ€‹|VjÎ»âˆ’Vjn|+|ViÎ»âˆ’Vin|â€‹âˆ‘jâ‰ iÏ€iâ€‹jn\displaystyle\leq\sum\_{j\neq i}\pi^{n}\_{ij}|V\_{j}^{\lambda}-V\_{j}^{n}|+|V\_{i}^{\lambda}-V\_{i}^{n}|\sum\_{j\neq i}\pi^{n}\_{ij} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +Î»â€‹âˆ‘jâ‰ i|expâ¡(Vjnâˆ’giâ€‹jâˆ’VinÎ»)âˆ’expâ¡(VjÎ»âˆ’giâ€‹jâˆ’ViÎ»Î»)|\displaystyle\quad+\lambda\sum\_{j\neq i}\left|\exp\left(\frac{V^{n}\_{j}-g\_{ij}-V^{n}\_{i}}{\lambda}\right)-\exp\left(\frac{V\_{j}^{\lambda}-g\_{ij}-V\_{i}^{\lambda}}{\lambda}\right)\right| |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | â‰¤Câˆ—â€‹Fnâ€‹(t),\displaystyle\leq C^{\*}F^{n}(t), |  | (4.11) |

where Câˆ—>0C^{\*}>0 is a constant independent of nn. For nâ‰¥0n\geq 0, we define the function win:ğ’ŸÂ¯â†’â„w\_{i}^{n}:\overline{{\cal D}}\to\mathds{R} for iâˆˆğ•€mi\in\mathbb{I}\_{m} as

|  |  |  |
| --- | --- | --- |
|  | winâ€‹(t,x):=ViÎ»â€‹(t,x)âˆ’Vin+1â€‹(t,x)âˆ’Câˆ—â€‹âˆ«tTFnâ€‹(s)â€‹ğ‘‘s,(t,x)âˆˆğ’ŸÂ¯.\displaystyle w\_{i}^{n}(t,x):=V\_{i}^{\lambda}(t,x)-V\_{i}^{n+1}(t,x)-C^{\*}\int\_{t}^{T}F^{n}(s)ds,\quad(t,x)\in\overline{{\cal D}}. |  |

By using ([4](https://arxiv.org/html/2512.04697v1#S4.Ex8 "4 Policy Iteration and Convergence â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")), it holds that for any (t,x)âˆˆğ’Ÿ(t,x)\in{\cal D},

|  |  |  |
| --- | --- | --- |
|  | âˆ‚winâ€‹(t,x)âˆ‚t+â„’xiâ€‹winâ€‹(t,x)+âˆ‘jâ‰ iÏ€iâ€‹jn+1â€‹(t,x)â€‹wjnâ€‹(t,x)âˆ’âˆ‘jâ‰ iÏ€iâ€‹jn+1â€‹(t,x)â€‹winâ€‹(t,x)\displaystyle\frac{\partial w\_{i}^{n}(t,x)}{\partial t}+\mathcal{L}^{i}\_{x}w\_{i}^{n}(t,x)+\sum\_{j\neq i}\pi\_{ij}^{n+1}(t,x)w\_{j}^{n}(t,x)-\sum\_{j\neq i}\pi\_{ij}^{n+1}(t,x)w\_{i}^{n}(t,x) |  |
|  |  |  |
| --- | --- | --- |
|  | =Hiâ€‹(ğ…in+1â€‹(t,x),V1Î»â€‹(t,x),â‹¯,VmÎ»â€‹(t,x))âˆ’Hiâ€‹(ğ…iâˆ—â€‹(t,x),V1Î»â€‹(t,x),â‹¯,VmÎ»â€‹(t,x))+Câˆ—â€‹Fnâ€‹(t)â‰¥0,\displaystyle=H\_{i}({\bm{\pi}}\_{i}^{n+1}(t,x),V\_{1}^{\lambda}(t,x),\cdots,V\_{m}^{\lambda}(t,x))-H\_{i}({\bm{\pi}}^{\*}\_{i}(t,x),V\_{1}^{\lambda}(t,x),\cdots,V\_{m}^{\lambda}(t,x))+C^{\*}F^{n}(t)\geq 0, |  |

and winâ€‹(T,x)â‰¥0w\_{i}^{n}(T,x)\geq 0 for xâˆˆâ„nx\in\mathds{R}^{n}. By virtue of Theorem 1.3 in kusano1965first, we deduce winâ€‹(t,x)â‰¥0w^{n}\_{i}(t,x)\geq 0. That is,

|  |  |  |  |
| --- | --- | --- | --- |
|  | ViÎ»â€‹(t,x)âˆ’Vin+1â€‹(t,x)â‰¤Câˆ—â€‹âˆ«tTFnâ€‹(s)â€‹ğ‘‘s,âˆ€(i,t,x)âˆˆğ•€mÃ—ğ’ŸÂ¯.\displaystyle V\_{i}^{\lambda}(t,x)-V\_{i}^{n+1}(t,x)\leq C^{\*}\int\_{t}^{T}F^{n}(s)ds,\quad\forall(i,t,x)\in\mathbb{I}\_{m}\times\overline{{\cal D}}. |  | (4.12) |

This yields the inequality

|  |  |  |  |
| --- | --- | --- | --- |
|  | Fn+1â€‹(t)â‰¤Câˆ—â€‹âˆ«tTFnâ€‹(s)â€‹ğ‘‘s,âˆ€tâˆˆ[0,T],\displaystyle F^{n+1}(t)\leq C^{\*}\int\_{t}^{T}F^{n}(s)ds,\quad\forall t\in[0,T], |  | (4.13) |

from which we deduce that

|  |  |  |  |
| --- | --- | --- | --- |
|  | Fnâ€‹(t)â‰¤(Câˆ—)nâ€‹Tnn!â€‹F1â€‹(t),âˆ€tâˆˆ[0,T].\displaystyle F^{n}(t)\leq\frac{(C^{\*})^{n}T^{n}}{n!}F^{1}(t),\quad\forall t\in[0,T]. |  | (4.14) |

Because F1â€‹(t)F^{1}(t) is bounded, let C1=Câˆ—â€‹TC\_{1}=C^{\*}T and C2=suptâˆˆ[0,T]F1â€‹(t)C\_{2}=\sup\_{t\in[0,T]}F^{1}(t). Then we obtain that desired result.
âˆ

To establish a connection between our exploratory formulation and the classical optimal switching problem, we next rigorously analyze the convergence result of the exploratory solution as the temperature parameter Î»\lambda approaches zero. Unlike the existing results in tang2022exploratory for regular control problem that focus on a single PDE problem, the nature of problem with multiple regime states calls for some distinct analysis to investigate the system of PDEs in our setting.
In particular, we employ some stability analysis of viscosity solutions to the PDE system to examine the limit of vanishing entropy regularization. The mathematical goal is to show that the solution of the system of PDE will converge to the solution of the system of variational inequalities as Î»â†’0\lambda\rightarrow 0.

Let us introduce the upper and lower weak limits of functions (V1Î»,â‹¯,VmÎ»)(V\_{1}^{\lambda},\cdots,V\_{m}^{\lambda}) defined as follows: for iâˆˆğ•€mi\in\mathbb{I}\_{m} and (t,x)âˆˆğ’ŸÂ¯(t,x)\in\overline{{\cal D}},

|  |  |  |  |
| --- | --- | --- | --- |
|  | VÂ¯iâ€‹(t,x):={lim supÎ»â†’0(s,y)â†’(t,x),(s,y)âˆˆğ’ŸViÎ»â€‹(s,y),(t,x)âˆˆğ’Ÿ,hâ€‹(x),t=T,xâˆˆâ„n,\displaystyle\overline{V}\_{i}(t,x):=\begin{cases}\displaystyle\limsup\_{\lambda\to 0\atop(s,y)\to(t,x),(s,y)\in{\cal D}}V\_{i}^{\lambda}(s,y),&(t,x)\in{\cal D},\\[10.00002pt] \displaystyle\qquad h(x),&t=T,\penalty 10000\ x\in\mathds{R}^{n},\end{cases} |  | (4.15) |

and

|  |  |  |  |
| --- | --- | --- | --- |
|  | VÂ¯iâ€‹(t,x):={lim infÎ»â†’0,(s,y)â†’(t,x),(s,y)âˆˆğ’ŸViÎ»â€‹(s,y),(t,x)âˆˆğ’Ÿ,hâ€‹(x),t=T,xâˆˆâ„n.\displaystyle\underline{V}\_{i}(t,x):=\begin{cases}\displaystyle\liminf\_{\lambda\to 0,\atop(s,y)\to(t,x),(s,y)\in{\cal D}}V\_{i}^{\lambda}(s,y),&(t,x)\in{\cal D},\\[10.00002pt] \displaystyle\qquad h(x),&t=T,\penalty 10000\ x\in\mathds{R}^{n}.\end{cases} |  | (4.16) |

The next lemma plays a crucial role in establishing the convergence of the value functions (V1Î»,â‹¯,VmÎ»)(V\_{1}^{\lambda},\cdots,V\_{m}^{\lambda}) as the temperature parameter Î»\lambda tends to zero. By defining the upper and lower weak limits, we capture the limiting behavior of these functions. The result asserts that these limits are bounded and satisfy the viscosity solution properties for the system of HJB equations ([2.7](https://arxiv.org/html/2512.04697v1#S2.E7 "In 2 Classical Optimal Switching Problem â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")). Specifically, the upper weak limits form a viscosity subsolution, and the lower weak limits form a viscosity supersolution.

###### Lemma 4.3.

Let Assumptions [2.1](https://arxiv.org/html/2512.04697v1#S2.Thmtheorem1 "Assumption 2.1. â€£ 2 Classical Optimal Switching Problem â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes"), [2.2](https://arxiv.org/html/2512.04697v1#S2.Thmtheorem2 "Assumption 2.2. â€£ 2 Classical Optimal Switching Problem â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes"), and [3.1](https://arxiv.org/html/2512.04697v1#S3.Thmtheorem1 "Assumption 3.1. â€£ 3 Exploratory Formulation under Entropy Regularization â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes") hold. Consider the upper and lower weak limits of the functions (V1Î»,â‹¯,VmÎ»)(V\_{1}^{\lambda},\cdots,V\_{m}^{\lambda}), defined by ([4.15](https://arxiv.org/html/2512.04697v1#S4.E15 "In 4 Policy Iteration and Convergence â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")) and ([4.16](https://arxiv.org/html/2512.04697v1#S4.E16 "In 4 Policy Iteration and Convergence â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")), respectively. Then the tuple of upper weak limits (VÂ¯1,â‹¯,VÂ¯m)(\overline{V}\_{1},\cdots,\overline{V}\_{m}) is a bounded viscosity subsolution of system ([2.7](https://arxiv.org/html/2512.04697v1#S2.E7 "In 2 Classical Optimal Switching Problem â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")), while the tuple of lower weak limits (VÂ¯1,â‹¯,VÂ¯m)(\underline{V}\_{1},\cdots,\underline{V}\_{m}) is a bounded viscosity supersolution of system ([2.7](https://arxiv.org/html/2512.04697v1#S2.E7 "In 2 Classical Optimal Switching Problem â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")).

###### Proof.

It follows from ([3.13](https://arxiv.org/html/2512.04697v1#S3.E13 "In 3 Exploratory Formulation under Entropy Regularization â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")) and Assumption [2.2](https://arxiv.org/html/2512.04697v1#S2.Thmtheorem2 "Assumption 2.2. â€£ 2 Classical Optimal Switching Problem â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")-(ii) that

|  |  |  |
| --- | --- | --- |
|  | |ViÎ»â€‹(t,x)|â‰¤Kf,hâ€‹(1+T)+Î»â€‹supiâˆˆğ•€m(âˆ‘jâ‰ iexpâ¡(âˆ’giâ€‹jÎ»))â€‹Tâ‰¤Kf,hâ€‹(1+T)+Î»â€‹(mâˆ’1)â€‹T\displaystyle|V\_{i}^{\lambda}(t,x)|\leq K\_{f,h}(1+T)+\lambda\sup\_{i\in\mathbb{I}\_{m}}\left(\sum\_{j\neq i}\exp\left(-\frac{g\_{ij}}{\lambda}\right)\right)T\leq K\_{f,h}(1+T)+\lambda(m-1)T |  |

for all Î»>0\lambda>0 and (i,t,x)âˆˆğ•€mÃ—ğ’ŸÂ¯(i,t,x)\in\mathbb{I}\_{m}\times\overline{{\cal D}}.
This implies that VÂ¯i\overline{V}\_{i} and VÂ¯i\underline{V}\_{i} for iâˆˆğ•€mi\in\mathbb{I}\_{m} are bounded functions. Applying Lemma 1.5 in Chapter V of bardi1997optimal, VÂ¯i\overline{V}\_{i} is upper-semicontinuous on ğ’Ÿ{\cal D} while VÂ¯i\underline{V}\_{i} is lower-semicontinuous on ğ’Ÿ{\cal D} for every iâˆˆğ•€mi\in\mathbb{I}\_{m}.

We next show that the tuple of upper weak limits (VÂ¯1,â‹¯,VÂ¯m)(\overline{V}\_{1},\cdots,\overline{V}\_{m}) is a viscosity subsolution of system ([2.7](https://arxiv.org/html/2512.04697v1#S2.E7 "In 2 Classical Optimal Switching Problem â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")) using the contradiction argument. For iâˆˆğ•€mi\in\mathbb{I}\_{m}, let (t0,x0)âˆˆğ’Ÿ(t\_{0},x\_{0})\in{\cal D} and the test function Ï†iâˆˆC1,2â€‹(ğ’Ÿ)\varphi\_{i}\in C^{1,2}({\cal D}) such that (t0,x0)(t\_{0},x\_{0}) is a local maximum of VÂ¯iâˆ’Ï†i\overline{V}\_{i}-\varphi\_{i}. Assume that

|  |  |  |  |
| --- | --- | --- | --- |
|  | min{\displaystyle\min\Bigg\{ | âˆ’âˆ‚Ï†iâ€‹(t0,x0)âˆ‚tâˆ’â„’xiâ€‹Ï†iâ€‹(t0,x0)âˆ’fâ€‹(t0,x0,i),\displaystyle-\frac{\partial\varphi\_{i}(t\_{0},x\_{0})}{\partial t}-\mathcal{L}^{i}\_{x}\varphi\_{i}(t\_{0},x\_{0})-f(t\_{0},x\_{0},i), |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | VÂ¯i(t0,x0)âˆ’maxjâ‰ i(VÂ¯j(t0,x0)âˆ’giâ€‹j)}>0.\displaystyle\qquad\qquad\quad\overline{V}\_{i}(t\_{0},x\_{0})-\max\_{j\neq i}(\overline{V}\_{j}(t\_{0},x\_{0})-g\_{ij})\Bigg\}>0. |  | (4.17) |

That is,

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î´:=âˆ’âˆ‚Ï†iâ€‹(t0,x0)âˆ‚tâˆ’â„’xiâ€‹Ï†iâ€‹(t0,x0)âˆ’fâ€‹(t0,x0,i)>0,\displaystyle\delta:=-\frac{\partial\varphi\_{i}(t\_{0},x\_{0})}{\partial t}-\mathcal{L}^{i}\_{x}\varphi\_{i}(t\_{0},x\_{0})-f(t\_{0},x\_{0},i)>0, |  | (4.18) |
|  |  |  |  |
| --- | --- | --- | --- |
|  | Îµ:=VÂ¯iâ€‹(t0,x0)âˆ’maxjâ‰ iâ¡(VÂ¯jâ€‹(t0,x0)âˆ’giâ€‹j)>0.\displaystyle\varepsilon:=\overline{V}\_{i}(t\_{0},x\_{0})-\max\_{j\neq i}(\overline{V}\_{j}(t\_{0},x\_{0})-g\_{ij})>0. |  | (4.19) |

In view of Lemma 1.6 in Chapter V of bardi1997optimal, there exists a sequence {(tn,xn)}nâ‰¥1\{(t\_{n},x\_{n})\}\_{n\geq 1} with (tn,xn)âˆˆğ’Ÿ(t\_{n},x\_{n})\in{\cal D} and a sequence {Î»n}nâ‰¥1\{\lambda\_{n}\}\_{n\geq 1} with Î»n>0\lambda\_{n}>0, limnâ†’âˆÎ»n=0\lim\_{n\to\infty}\lambda\_{n}=0 such that (tn,xn)(t\_{n},x\_{n}) is a local maximum point of ViÎ»nâˆ’Ï†iV^{\lambda\_{n}}\_{i}-\varphi\_{i} and

|  |  |  |  |
| --- | --- | --- | --- |
|  | limnâ†’âˆ(tn,xn)=(t0,x0),limnâ†’âˆViÎ»nâ€‹(tn,xn)=VÂ¯iâ€‹(t0,x0).\displaystyle\lim\_{n\to\infty}(t\_{n},x\_{n})=(t\_{0},x\_{0}),\quad\lim\_{n\to\infty}V^{\lambda\_{n}}\_{i}(t\_{n},x\_{n})=\overline{V}\_{i}(t\_{0},x\_{0}). |  | (4.20) |

Lemma [3.2](https://arxiv.org/html/2512.04697v1#S3.Thmtheorem2 "Lemma 3.2. â€£ 3 Exploratory Formulation under Entropy Regularization â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes") implies that for any Î»>0\lambda>0, (V1Î»,V2Î»,â‹¯,VmÎ»)(V^{\lambda}\_{1},V^{\lambda}\_{2},\cdots,V^{\lambda}\_{m}) is a classical solution to the system of of HJB equations ([3.4](https://arxiv.org/html/2512.04697v1#S3.E4 "In 3 Exploratory Formulation under Entropy Regularization â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")), thus ViÎ»nV^{\lambda\_{n}}\_{i} is a viscosity subsolution of the following PDE:

|  |  |  |
| --- | --- | --- |
|  | âˆ’âˆ‚ViÎ»nâ€‹(t,x)âˆ‚tâˆ’â„’xiâ€‹ViÎ»nâ€‹(t,x)âˆ’fâ€‹(t,x,i)âˆ’Î»â€‹âˆ‘jâ‰ iexpâ¡(VjÎ»nâ€‹(t,x)âˆ’giâ€‹jâˆ’ViÎ»nâ€‹(t,x)Î»)=0.\displaystyle-\frac{\partial V^{\lambda\_{n}}\_{i}(t,x)}{\partial t}-\mathcal{L}^{i}\_{x}V^{\lambda\_{n}}\_{i}(t,x)-f(t,x,i)-\lambda\sum\_{j\neq i}\exp\left(\frac{V^{\lambda\_{n}}\_{j}(t,x)-g\_{ij}-V^{\lambda\_{n}}\_{i}(t,x)}{\lambda}\right)=0. |  |

Consequently, we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | âˆ’âˆ‚Ï†iâ€‹(tn,xn)âˆ‚tâˆ’â„’xiâ€‹Ï†iâ€‹(tn,xn)âˆ’fâ€‹(tn,xn,i)âˆ’Î»nâ€‹âˆ‘jâ‰ iexpâ¡(VjÎ»nâ€‹(tn,xn)âˆ’giâ€‹jâˆ’ViÎ»nâ€‹(tn,xn)Î»)â‰¤0\displaystyle-\frac{\partial\varphi\_{i}(t\_{n},x\_{n})}{\partial t}-\mathcal{L}^{i}\_{x}\varphi\_{i}(t\_{n},x\_{n})-f(t\_{n},x\_{n},i)-\lambda\_{n}\sum\_{j\neq i}\exp\left(\frac{V^{\lambda\_{n}}\_{j}(t\_{n},x\_{n})-g\_{ij}-V^{\lambda\_{n}}\_{i}(t\_{n},x\_{n})}{\lambda}\right)\leq 0 |  | (4.21) |

for any nâ‰¥1n\geq 1.

From ([4.18](https://arxiv.org/html/2512.04697v1#S4.E18 "In 4 Policy Iteration and Convergence â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")), ([4.19](https://arxiv.org/html/2512.04697v1#S4.E19 "In 4 Policy Iteration and Convergence â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")) and ([4.20](https://arxiv.org/html/2512.04697v1#S4.E20 "In 4 Policy Iteration and Convergence â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")), it follows that there exists some n1>0n\_{1}>0 such that for all nâ‰¥n1n\geq n\_{1},

|  |  |  |
| --- | --- | --- |
|  | âˆ’âˆ‚Ï†iâ€‹(tn,xn)âˆ‚tâˆ’â„’xiâ€‹Ï†iâ€‹(tn,xn)âˆ’fâ€‹(tn,xn,i)â‰¥Î´2,\displaystyle-\frac{\partial\varphi\_{i}(t\_{n},x\_{n})}{\partial t}-\mathcal{L}^{i}\_{x}\varphi\_{i}(t\_{n},x\_{n})-f(t\_{n},x\_{n},i)\geq\frac{\delta}{2}, |  |

and for any jâˆˆğ•€m,jâ‰ ij\in\mathbb{I}\_{m},j\neq i,

|  |  |  |
| --- | --- | --- |
|  | VjÎ»nâ€‹(tn,xn)âˆ’giâ€‹jâˆ’ViÎ»nâ€‹(tn,xn)â‰¤âˆ’Îµ2.\displaystyle V^{\lambda\_{n}}\_{j}(t\_{n},x\_{n})-g\_{ij}-V^{\lambda\_{n}}\_{i}(t\_{n},x\_{n})\leq-\frac{\varepsilon}{2}. |  |

Selecting n2n\_{2} such that for all nâ‰¥n2n\geq n\_{2}, Î»nâ€‹expâ¡(âˆ’Îµ2â€‹Î»n)<Î´2â€‹(mâˆ’1)\lambda\_{n}\exp(-\frac{\varepsilon}{2\lambda\_{n}})<\frac{\delta}{2(m-1)}, then for nâ‰¥maxâ¡{n1,n2}n\geq\max\{n\_{1},n\_{2}\}, we get that

|  |  |  |  |
| --- | --- | --- | --- |
|  |  | âˆ’âˆ‚Ï†iâ€‹(tn,xn)âˆ‚tâˆ’â„’xiâ€‹Ï†iâ€‹(tn,xn)âˆ’fâ€‹(tn,xn,i)âˆ’Î»nâ€‹âˆ‘jâ‰ iexpâ¡(VjÎ»nâ€‹(tn,xn)âˆ’giâ€‹jâˆ’ViÎ»nâ€‹(tn,xn)Î»)\displaystyle-\frac{\partial\varphi\_{i}(t\_{n},x\_{n})}{\partial t}-\mathcal{L}^{i}\_{x}\varphi\_{i}(t\_{n},x\_{n})-f(t\_{n},x\_{n},i)-\lambda\_{n}\sum\_{j\neq i}\exp\left(\frac{V^{\lambda\_{n}}\_{j}(t\_{n},x\_{n})-g\_{ij}-V^{\lambda\_{n}}\_{i}(t\_{n},x\_{n})}{\lambda}\right) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¥âˆ’âˆ‚Ï†iâ€‹(tn,xn)âˆ‚tâˆ’â„’xiâ€‹Ï†iâ€‹(tn,xn)âˆ’fâ€‹(tn,xn,i)âˆ’Î»nâ€‹âˆ‘jâ‰ iexpâ¡(âˆ’Îµ2â€‹Î»n)\displaystyle\geq-\frac{\partial\varphi\_{i}(t\_{n},x\_{n})}{\partial t}-\mathcal{L}^{i}\_{x}\varphi\_{i}(t\_{n},x\_{n})-f(t\_{n},x\_{n},i)-\lambda\_{n}\sum\_{j\neq i}\exp\left(-\frac{\varepsilon}{2\lambda\_{n}}\right) |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | â‰¥Î´2âˆ’Î»nâ€‹(mâˆ’1)â€‹expâ¡(âˆ’Îµ2â€‹Î»n)>0.\displaystyle\geq\frac{\delta}{2}-\lambda\_{n}(m-1)\exp\left(-\frac{\varepsilon}{2\lambda\_{n}}\right)>0. |  | (4.22) |

The inequalities ([4.21](https://arxiv.org/html/2512.04697v1#S4.E21 "In 4 Policy Iteration and Convergence â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")) and ([4](https://arxiv.org/html/2512.04697v1#S4.Ex21 "4 Policy Iteration and Convergence â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")) are contradictory. Therefore, we conclude that the assumption ([4](https://arxiv.org/html/2512.04697v1#S4.Ex17 "4 Policy Iteration and Convergence â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")) is note true, which implies that (VÂ¯1,â‹¯,VÂ¯m)(\overline{V}\_{1},\cdots,\overline{V}\_{m}) is a viscosity subsolution of system ([2.7](https://arxiv.org/html/2512.04697v1#S2.E7 "In 2 Classical Optimal Switching Problem â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")).

We next show that the tuple of lower weak limits (VÂ¯1,â‹¯,VÂ¯m)(\underline{V}\_{1},\cdots,\underline{V}\_{m}) is a viscosity supersolution of system ([2.7](https://arxiv.org/html/2512.04697v1#S2.E7 "In 2 Classical Optimal Switching Problem â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")) by contradiction. For iâˆˆğ•€mi\in\mathbb{I}\_{m}, let (t0,x0)âˆˆğ’Ÿ(t\_{0},x\_{0})\in{\cal D} and the test function Ï†iâˆˆC1,2â€‹(ğ’Ÿ)\varphi\_{i}\in C^{1,2}({\cal D}) such that (t0,x0)(t\_{0},x\_{0}) is a local minimum of VÂ¯iâˆ’Ï†i\overline{V}\_{i}-\varphi\_{i}. Assume that

|  |  |  |  |
| --- | --- | --- | --- |
|  | min{\displaystyle\min\Bigg\{ | âˆ’âˆ‚Ï†iâ€‹(t0,x0)âˆ‚tâˆ’â„’xiâ€‹Ï†iâ€‹(t0,x0)âˆ’fâ€‹(t0,x0,i),\displaystyle-\frac{\partial\varphi\_{i}(t\_{0},x\_{0})}{\partial t}-\mathcal{L}^{i}\_{x}\varphi\_{i}(t\_{0},x\_{0})-f(t\_{0},x\_{0},i), |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | VÂ¯i(t0,x0)âˆ’maxjâ‰ i(VÂ¯j(t0,x0)âˆ’giâ€‹j)}<0.\displaystyle\qquad\qquad\quad\underline{V}\_{i}(t\_{0},x\_{0})-\max\_{j\neq i}(\underline{V}\_{j}(t\_{0},x\_{0})-g\_{ij})\Bigg\}<0. |  | (4.23) |

Using Lemma 1.6 in Chapter V of bardi1997optimal again, there exists a sequence {(tn,xn)}nâ‰¥1\{(t\_{n},x\_{n})\}\_{n\geq 1} with (tn,xn)âˆˆğ’Ÿ(t\_{n},x\_{n})\in{\cal D} and a sequence {Î»n}nâ‰¥1\{\lambda\_{n}\}\_{n\geq 1} with Î»n>0\lambda\_{n}>0, limnâ†’âˆÎ»n=0\lim\_{n\to\infty}\lambda\_{n}=0 such that (tn,xn)(t\_{n},x\_{n}) is a local minimum point of ViÎ»nâˆ’Ï†iV^{\lambda\_{n}}\_{i}-\varphi\_{i} and

|  |  |  |  |
| --- | --- | --- | --- |
|  | limnâ†’âˆ(tn,xn)=(t0,x0),limnâ†’âˆViÎ»nâ€‹(tn,xn)=VÂ¯iâ€‹(t0,x0).\displaystyle\lim\_{n\to\infty}(t\_{n},x\_{n})=(t\_{0},x\_{0}),\quad\lim\_{n\to\infty}V^{\lambda\_{n}}\_{i}(t\_{n},x\_{n})=\underline{V}\_{i}(t\_{0},x\_{0}). |  | (4.24) |

By Lemma [3.2](https://arxiv.org/html/2512.04697v1#S3.Thmtheorem2 "Lemma 3.2. â€£ 3 Exploratory Formulation under Entropy Regularization â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes"), for any Î»>0\lambda>0, (V1Î»,V2Î»,â‹¯,VmÎ»)(V^{\lambda}\_{1},V^{\lambda}\_{2},\cdots,V^{\lambda}\_{m}) is a classical solution to the system of of HJB equations ([3.4](https://arxiv.org/html/2512.04697v1#S3.E4 "In 3 Exploratory Formulation under Entropy Regularization â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")), thus ViÎ»nV^{\lambda\_{n}}\_{i} is a viscosity supersolution of the following PDE:

|  |  |  |
| --- | --- | --- |
|  | âˆ’âˆ‚ViÎ»nâ€‹(t,x)âˆ‚tâˆ’â„’xiâ€‹ViÎ»nâ€‹(t,x)âˆ’fâ€‹(t,x,i)âˆ’Î»â€‹âˆ‘jâ‰ iexpâ¡(VjÎ»nâ€‹(t,x)âˆ’giâ€‹jâˆ’ViÎ»nâ€‹(t,x)Î»)=0.\displaystyle-\frac{\partial V^{\lambda\_{n}}\_{i}(t,x)}{\partial t}-\mathcal{L}^{i}\_{x}V^{\lambda\_{n}}\_{i}(t,x)-f(t,x,i)-\lambda\sum\_{j\neq i}\exp\left(\frac{V^{\lambda\_{n}}\_{j}(t,x)-g\_{ij}-V^{\lambda\_{n}}\_{i}(t,x)}{\lambda}\right)=0. |  |

Therefore we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | âˆ’âˆ‚Ï†iâ€‹(tn,xn)âˆ‚tâˆ’â„’xiâ€‹Ï†iâ€‹(tn,xn)âˆ’fâ€‹(tn,xn,i)âˆ’Î»nâ€‹âˆ‘jâ‰ iexpâ¡(VjÎ»nâ€‹(tn,xn)âˆ’giâ€‹jâˆ’ViÎ»nâ€‹(tn,xn)Î»)â‰¥0\displaystyle-\frac{\partial\varphi\_{i}(t\_{n},x\_{n})}{\partial t}-\mathcal{L}^{i}\_{x}\varphi\_{i}(t\_{n},x\_{n})-f(t\_{n},x\_{n},i)-\lambda\_{n}\sum\_{j\neq i}\exp\left(\frac{V^{\lambda\_{n}}\_{j}(t\_{n},x\_{n})-g\_{ij}-V^{\lambda\_{n}}\_{i}(t\_{n},x\_{n})}{\lambda}\right)\geq 0 |  | (4.25) |

for any nâ‰¥1n\geq 1. We consider two cases for the inequality ([4](https://arxiv.org/html/2512.04697v1#S4.Ex23 "4 Policy Iteration and Convergence â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")).

Case 1. Assume that

|  |  |  |  |
| --- | --- | --- | --- |
|  | âˆ’âˆ‚Ï†iâ€‹(t0,x0)âˆ‚tâˆ’â„’xiâ€‹Ï†iâ€‹(t0,x0)âˆ’fâ€‹(t0,x0,i)<0.\displaystyle-\frac{\partial\varphi\_{i}(t\_{0},x\_{0})}{\partial t}-\mathcal{L}^{i}\_{x}\varphi\_{i}(t\_{0},x\_{0})-f(t\_{0},x\_{0},i)<0. |  | (4.26) |

By ([4.25](https://arxiv.org/html/2512.04697v1#S4.E25 "In 4 Policy Iteration and Convergence â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")), we have

|  |  |  |
| --- | --- | --- |
|  | âˆ’âˆ‚Ï†iâ€‹(tn,xn)âˆ‚tâˆ’â„’xiâ€‹Ï†iâ€‹(tn,xn)âˆ’fâ€‹(tn,xn,i)â‰¥Î»nâ€‹âˆ‘jâ‰ iexpâ¡(VjÎ»nâ€‹(tn,xn)âˆ’giâ€‹jâˆ’ViÎ»nâ€‹(tn,xn)Î»)â‰¥0,\displaystyle-\frac{\partial\varphi\_{i}(t\_{n},x\_{n})}{\partial t}-\mathcal{L}^{i}\_{x}\varphi\_{i}(t\_{n},x\_{n})-f(t\_{n},x\_{n},i)\geq\lambda\_{n}\sum\_{j\neq i}\exp\left(\frac{V^{\lambda\_{n}}\_{j}(t\_{n},x\_{n})-g\_{ij}-V^{\lambda\_{n}}\_{i}(t\_{n},x\_{n})}{\lambda}\right)\geq 0, |  |

which yields

|  |  |  |
| --- | --- | --- |
|  | âˆ’âˆ‚Ï†iâ€‹(t0,x0)âˆ‚tâˆ’â„’xiâ€‹Ï†iâ€‹(t0,x0)âˆ’fâ€‹(t0,x0,i)\displaystyle-\frac{\partial\varphi\_{i}(t\_{0},x\_{0})}{\partial t}-\mathcal{L}^{i}\_{x}\varphi\_{i}(t\_{0},x\_{0})-f(t\_{0},x\_{0},i) |  |
|  |  |  |
| --- | --- | --- |
|  | =limnâ†’âˆ(âˆ’âˆ‚Ï†iâ€‹(tn,xn)âˆ‚tâˆ’â„’xiâ€‹Ï†iâ€‹(tn,xn)âˆ’fâ€‹(tn,xn,i))â‰¥0.\displaystyle=\lim\_{n\to\infty}\left(-\frac{\partial\varphi\_{i}(t\_{n},x\_{n})}{\partial t}-\mathcal{L}^{i}\_{x}\varphi\_{i}(t\_{n},x\_{n})-f(t\_{n},x\_{n},i)\right)\geq 0. |  |

Thus, we obtain a contradiction.

Case 2. Assume that

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î´:=âˆ’âˆ‚Ï†iâ€‹(t0,x0)âˆ‚tâˆ’â„’xiâ€‹Ï†iâ€‹(t0,x0)âˆ’fâ€‹(t0,x0,i)â‰¥0,\displaystyle\delta:=-\frac{\partial\varphi\_{i}(t\_{0},x\_{0})}{\partial t}-\mathcal{L}^{i}\_{x}\varphi\_{i}(t\_{0},x\_{0})-f(t\_{0},x\_{0},i)\geq 0, |  | (4.27) |

and

|  |  |  |  |
| --- | --- | --- | --- |
|  | Îµ:=âˆ’(VÂ¯iâ€‹(t0,x0)âˆ’maxjâ‰ iâ¡(VÂ¯jâ€‹(t0,x0)âˆ’giâ€‹j))=VÂ¯kâ€‹(t0,x0)âˆ’giâ€‹kâˆ’VÂ¯iâ€‹(t0,x0)>0.\displaystyle\varepsilon:=-(\underline{V}\_{i}(t\_{0},x\_{0})-\max\_{j\neq i}(\underline{V}\_{j}(t\_{0},x\_{0})-g\_{ij}))=\underline{V}\_{k}(t\_{0},x\_{0})-g\_{ik}-\underline{V}\_{i}(t\_{0},x\_{0})>0. |  | (4.28) |

By ([4.24](https://arxiv.org/html/2512.04697v1#S4.E24 "In 4 Policy Iteration and Convergence â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")), ([4.27](https://arxiv.org/html/2512.04697v1#S4.E27 "In 4 Policy Iteration and Convergence â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")) and ([4.28](https://arxiv.org/html/2512.04697v1#S4.E28 "In 4 Policy Iteration and Convergence â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")), there exists some n1>0n\_{1}>0 such that for all nâ‰¥n1n\geq n\_{1},

|  |  |  |
| --- | --- | --- |
|  | âˆ’âˆ‚Ï†iâ€‹(tn,xn)âˆ‚tâˆ’â„’xiâ€‹Ï†iâ€‹(tn,xn)âˆ’fâ€‹(tn,xn,i)â‰¤3â€‹Î´2,\displaystyle-\frac{\partial\varphi\_{i}(t\_{n},x\_{n})}{\partial t}-\mathcal{L}^{i}\_{x}\varphi\_{i}(t\_{n},x\_{n})-f(t\_{n},x\_{n},i)\leq\frac{3\delta}{2}, |  |

and

|  |  |  |
| --- | --- | --- |
|  | VkÎ»nâ€‹(tn,xn)âˆ’giâ€‹kâˆ’ViÎ»nâ€‹(tn,xn)â‰¥Îµ2.\displaystyle V^{\lambda\_{n}}\_{k}(t\_{n},x\_{n})-g\_{ik}-V^{\lambda\_{n}}\_{i}(t\_{n},x\_{n})\geq\frac{\varepsilon}{2}. |  |

Selecting n2n\_{2} such that for all nâ‰¥n2n\geq n\_{2}, Î»nâ€‹expâ¡(Îµ2â€‹Î»n)>3â€‹Î´2\lambda\_{n}\exp(\frac{\varepsilon}{2\lambda\_{n}})>\frac{3\delta}{2}, then for nâ‰¥maxâ¡{n1,n2}n\geq\max\{n\_{1},n\_{2}\}, it holds that

|  |  |  |  |
| --- | --- | --- | --- |
|  |  | âˆ’âˆ‚Ï†iâ€‹(tn,xn)âˆ‚tâˆ’â„’xiâ€‹Ï†iâ€‹(tn,xn)âˆ’fâ€‹(tn,xn,i)âˆ’Î»nâ€‹âˆ‘jâ‰ iexpâ¡(VjÎ»nâ€‹(tn,xn)âˆ’giâ€‹jâˆ’ViÎ»nâ€‹(tn,xn)Î»)\displaystyle-\frac{\partial\varphi\_{i}(t\_{n},x\_{n})}{\partial t}-\mathcal{L}^{i}\_{x}\varphi\_{i}(t\_{n},x\_{n})-f(t\_{n},x\_{n},i)-\lambda\_{n}\sum\_{j\neq i}\exp\left(\frac{V^{\lambda\_{n}}\_{j}(t\_{n},x\_{n})-g\_{ij}-V^{\lambda\_{n}}\_{i}(t\_{n},x\_{n})}{\lambda}\right) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤âˆ’âˆ‚Ï†iâ€‹(tn,xn)âˆ‚tâˆ’â„’xiâ€‹Ï†iâ€‹(tn,xn)âˆ’fâ€‹(tn,xn,i)âˆ’Î»nâ€‹expâ¡(VkÎ»nâ€‹(tn,xn)âˆ’giâ€‹kâˆ’ViÎ»nâ€‹(tn,xn)Î»)\displaystyle\leq-\frac{\partial\varphi\_{i}(t\_{n},x\_{n})}{\partial t}-\mathcal{L}^{i}\_{x}\varphi\_{i}(t\_{n},x\_{n})-f(t\_{n},x\_{n},i)-\lambda\_{n}\exp\left(\frac{V^{\lambda\_{n}}\_{k}(t\_{n},x\_{n})-g\_{ik}-V^{\lambda\_{n}}\_{i}(t\_{n},x\_{n})}{\lambda}\right) |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | â‰¤3â€‹Î´2âˆ’Î»nâ€‹expâ¡(Îµ2â€‹Î»n)<0.\displaystyle\leq\frac{3\delta}{2}-\lambda\_{n}\exp\left(\frac{\varepsilon}{2\lambda\_{n}}\right)<0. |  | (4.29) |

The inequalities ([4.25](https://arxiv.org/html/2512.04697v1#S4.E25 "In 4 Policy Iteration and Convergence â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")) and ([4](https://arxiv.org/html/2512.04697v1#S4.Ex30 "4 Policy Iteration and Convergence â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")) are contradictory.

Combining the arguments in two cases above, we conclude that assertion ([4](https://arxiv.org/html/2512.04697v1#S4.Ex23 "4 Policy Iteration and Convergence â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")) does not hold. This implies that (VÂ¯1,â‹¯,VÂ¯m)(\overline{V}\_{1},\cdots,\overline{V}\_{m}) is a viscosity supersolution of system ([2.7](https://arxiv.org/html/2512.04697v1#S2.E7 "In 2 Classical Optimal Switching Problem â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")), which completes the proof.
âˆ

As the second main result of this paper, the next theorem
shows the convergence result towards the classical optimal switching problem as the entropy regularization vanishes.

###### Theorem 4.4.

Let Assumptions [2.1](https://arxiv.org/html/2512.04697v1#S2.Thmtheorem1 "Assumption 2.1. â€£ 2 Classical Optimal Switching Problem â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes"), [2.2](https://arxiv.org/html/2512.04697v1#S2.Thmtheorem2 "Assumption 2.2. â€£ 2 Classical Optimal Switching Problem â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes") and [3.1](https://arxiv.org/html/2512.04697v1#S3.Thmtheorem1 "Assumption 3.1. â€£ 3 Exploratory Formulation under Entropy Regularization â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes") hold. Consider the value functions (V1,â‹¯,Vm)(V\_{1},\cdots,V\_{m}) of the classical optimal switching problem defined by ([2.6](https://arxiv.org/html/2512.04697v1#S2.E6 "In 2 Classical Optimal Switching Problem â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")), and the value functions (V1Î»,â‹¯,VmÎ»)(V\_{1}^{\lambda},\cdots,V\_{m}^{\lambda}) of the exploratory optimal switching problem defined by ([3.3](https://arxiv.org/html/2512.04697v1#S3.E3 "In 3 Exploratory Formulation under Entropy Regularization â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")). Then for any iâˆˆğ•€mi\in\mathbb{I}\_{m} and (t,x)âˆˆğ’ŸÂ¯(t,x)\in\overline{{\cal D}},

|  |  |  |  |
| --- | --- | --- | --- |
|  | limÎ»â†’0ViÎ»â€‹(t,x)=Viâ€‹(t,x).\displaystyle\lim\_{\lambda\to 0}V\_{i}^{\lambda}(t,x)=V\_{i}(t,x). |  | (4.30) |

###### Proof.

By using Lemma [4.3](https://arxiv.org/html/2512.04697v1#S4.Thmtheorem3 "Lemma 4.3. â€£ 4 Policy Iteration and Convergence â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes") and Lemma [2.3](https://arxiv.org/html/2512.04697v1#S2.Thmtheorem3 "Lemma 2.3 (Comparison Principle). â€£ 2 Classical Optimal Switching Problem â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes"), we have

|  |  |  |
| --- | --- | --- |
|  | VÂ¯iâ€‹(t,x)â‰¤VÂ¯iâ€‹(t,x),âˆ€iâˆˆğ•€m,(t,x)âˆˆğ’ŸÂ¯.\displaystyle\overline{V}\_{i}(t,x)\leq\underline{V}\_{i}(t,x),\quad\forall i\in\mathbb{I}\_{m},(t,x)\in\overline{{\cal D}}. |  |

On the other hand, it follows from the definition of upper and lower weak limits that VÂ¯iâ€‹(t,x)â‰¥VÂ¯iâ€‹(t,x)\overline{V}\_{i}(t,x)\geq\underline{V}\_{i}(t,x), for any iâˆˆğ•€mi\in\mathbb{I}\_{m} and (t,x)âˆˆğ’ŸÂ¯(t,x)\in\overline{{\cal D}}. Thus, VÂ¯iâ€‹(t,x)=VÂ¯iâ€‹(t,x)\overline{V}\_{i}(t,x)=\underline{V}\_{i}(t,x), then denotes by

|  |  |  |
| --- | --- | --- |
|  | Viâˆ—â€‹(t,x)=VÂ¯iâ€‹(t,x)=VÂ¯iâ€‹(t,x)forâ€‹iâˆˆğ•€m,(t,x)âˆˆğ’ŸÂ¯.\displaystyle V^{\*}\_{i}(t,x)=\overline{V}\_{i}(t,x)=\underline{V}\_{i}(t,x)\quad\text{for}\penalty 10000\ i\in\mathbb{I}\_{m},(t,x)\in\overline{{\cal D}}. |  |

It follows from ([4.15](https://arxiv.org/html/2512.04697v1#S4.E15 "In 4 Policy Iteration and Convergence â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")), ([4.16](https://arxiv.org/html/2512.04697v1#S4.E16 "In 4 Policy Iteration and Convergence â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")) and Lemma [4.3](https://arxiv.org/html/2512.04697v1#S4.Thmtheorem3 "Lemma 4.3. â€£ 4 Policy Iteration and Convergence â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes") that (V1âˆ—,â‹¯,Vmâˆ—)(V\_{1}^{\*},\cdots,V\_{m}^{\*}) is a bounded viscosity solution of system ([2.7](https://arxiv.org/html/2512.04697v1#S2.E7 "In 2 Classical Optimal Switching Problem â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")) satisfying Viâˆ—â€‹(t,x)=limÎ»â†’0ViÎ»â€‹(t,x)V^{\*}\_{i}(t,x)=\lim\_{\lambda\to 0}V\_{i}^{\lambda}(t,x). We deduce from Theorem [2.4](https://arxiv.org/html/2512.04697v1#S2.Thmtheorem4 "Theorem 2.4. â€£ 2 Classical Optimal Switching Problem â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes") that

|  |  |  |  |
| --- | --- | --- | --- |
|  | Viâ€‹(t,x)=Viâˆ—â€‹(t,x)=limÎ»â†’0ViÎ»â€‹(t,x).\displaystyle V\_{i}(t,x)=V^{\*}\_{i}(t,x)=\lim\_{\lambda\to 0}V\_{i}^{\lambda}(t,x). |  | (4.31) |

Thus, we complete the proof of the theorem.
âˆ

Theorem [4.4](https://arxiv.org/html/2512.04697v1#S4.Thmtheorem4 "Theorem 4.4. â€£ 4 Policy Iteration and Convergence â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes") justifies the use of the exploratory formulation as a well-founded mathematical relaxation: as the exploration effect diminishes (as the temperature parameter Î»â†’0\lambda\to 0), the value function of the exploratory formulation indeed converges towards the value function of the classical optimal switching problem. Mathematically speaking, it is interesting to observe that the solution to the system of PDEs will converge to the solution of system of variational inequalities. Therefore, our exploratory formulation can also be regarded as a penalization approach to study a system of variational inequalities, under which we only need to handle the existence and regularity of solution to a system of PDEs.

## 5 Reinforcement Learning Algorithm

In this section, we design a RL algorithm to solve the exploratory optimal switching problem when the model is unknown. The core of our approach lies in a key reformulation: we have transformed the original optimal switching problem into a standard optimal control problem where we control the generator of the finite-state Markov chain that characterizes the switching regimes. The primary distinction from classical problems is that the agent now actively controls the transition rates between regimes, adding a continuous layer of decision-making on top of the discrete switching choices.

Our choice of the randomization and the exploratory form leads to an explicit characterization of the optimal policy that depends on the value functions, without involving their derivatives. Leveraging this solution structure, we adopt the policy evaluation (PE) method based on the martingale characterization method similar to jia2022policy, which consider two alternative methods based on a martingale characterization: minimizing a martingale loss function, which provides the best mean-square approximation of the true value function, and solving a system of martingale orthogonality condition with test functions. In what follows, we design the PE algorithm by the martingale orthogonality condition and the established policy improvement result in Proposition [4.1](https://arxiv.org/html/2512.04697v1#S4.Thmtheorem1 "Proposition 4.1. â€£ 4 Policy Iteration and Convergence â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes").

Recall that given a feedback strategy ğ…â€‹(t,x)=(Ï€iâ€‹jâ€‹(t,x))i,jâˆˆğ•€m{\bm{\pi}}(t,x)=(\pi\_{ij}(t,x))\_{i,j\in\mathbb{I}\_{m}}, then the corresponding value function (v1ğ…,â‹¯,vmğ…)(v\_{1}^{{\bm{\pi}}},\cdots,v\_{m}^{{\bm{\pi}}}) satisfies the PDE system that for iâˆˆğ•€mi\in\mathbb{I}\_{m},

|  |  |  |  |
| --- | --- | --- | --- |
|  | {âˆ‚viğ…â€‹(t,x)âˆ‚t+â„’xiâ€‹viğ…â€‹(t,x)+fâ€‹(t,x,i)+Hiâ€‹(ğ…iâ€‹(t,x),v1ğ…â€‹(t,x),â‹¯,vmğ…â€‹(t,x))=0,viğ…â€‹(T,x)=hâ€‹(x),\displaystyle\begin{cases}\displaystyle\frac{\partial v\_{i}^{{\bm{\pi}}}(t,x)}{\partial t}+\mathcal{L}^{i}\_{x}v\_{i}^{{\bm{\pi}}}(t,x)+f(t,x,i)+H\_{i}({\bm{\pi}}\_{i}(t,x),v\_{1}^{{\bm{\pi}}}(t,x),\cdots,v^{{\bm{\pi}}}\_{m}(t,x))=0,\\ \displaystyle v^{{\bm{\pi}}}\_{i}(T,x)=h(x),\end{cases} |  | (5.1) |

where the the Hamiltomian HiH\_{i} is given by ([4.2](https://arxiv.org/html/2512.04697v1#S4.E2 "In 4 Policy Iteration and Convergence â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")). For simplicity, we omit the superscript ğ…{{\bm{\pi}}} and denote the value function as:

|  |  |  |  |
| --- | --- | --- | --- |
|  | vâ€‹(t,x,i)=viğ…â€‹(t,x),forÂ â€‹iâˆˆğ•€m,(t,x)âˆˆğ’ŸÂ¯,\displaystyle v(t,x,i)=v^{{\bm{\pi}}}\_{i}(t,x),\quad\text{for }i\in\mathbb{I}\_{m},\penalty 10000\ (t,x)\in\overline{{\cal D}}, |  | (5.2) |

and denote by I=(It)â€‹tâ‰¥0I=(I\_{t}){t\geq 0} a continuous-time finite-state Markov chain with generator ğ…=(Ï€iâ€‹j)i,jâˆˆğ•€m{\bm{\pi}}=(\pi^{ij})\_{i,j\in\mathbb{I}\_{m}}. Let us introduce the process M=(Mt)tâˆˆ[0,T]M=(M\_{t})\_{t\in[0,T]} given by

|  |  |  |  |
| --- | --- | --- | --- |
|  | Mt:=vâ€‹(t,Xt,It)+âˆ«0t(fâ€‹(s,Xs,Is)+Î»â€‹Râ€‹(ğ…s,Is))â€‹ğ‘‘sâˆ’âˆ‘k=1âˆgÎºkâˆ’1â€‹Îºkâ€‹ğŸ{Ï„kâ‰¤t},tâˆˆ[0,T].\displaystyle M\_{t}:=v(t,X\_{t},I\_{t})+\int\_{0}^{t}\left(f(s,X\_{s},I\_{s})+\lambda R({\bm{\pi}}\_{s},I\_{s})\right)ds-\sum\_{k=1}^{\infty}g\_{\kappa\_{k-1}\kappa\_{k}}{\bf 1}\_{\{\tau\_{k}\leq t\}},\quad t\in[0,T]. |  | (5.3) |

The next lemma gives the martingale characterization that lays the foundation for the loss function and the policy evaluation RL algorithm.

###### Lemma 5.1.

Let ğ›‘â€‹(t,x)=(Ï€iâ€‹jâ€‹(t,x))i,jâˆˆğ•€m{\bm{\pi}}(t,x)=(\pi\_{ij}(t,x))\_{i,j\in\mathbb{I}\_{m}} be a feedback strategy and vâ€‹(t,x,i)v(t,x,i) be the corresponding value function given by ([5.2](https://arxiv.org/html/2512.04697v1#S5.E2 "In 5 Reinforcement Learning Algorithm â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")). Then the process M=(Mt)tâˆˆ[0,T]M=(M\_{t})\_{t\in[0,T]} given by ([5.3](https://arxiv.org/html/2512.04697v1#S5.E3 "In 5 Reinforcement Learning Algorithm â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")) is a squar-intergrable martingale.

###### Proof.

Using ItÃ´â€™s rule to vâ€‹(s,Xs,Is)v(s,X\_{s},I\_{s}) from tâ€²t^{\prime} to tt, we obtain

|  |  |  |  |
| --- | --- | --- | --- |
|  | vâ€‹(t,Xt,It)\displaystyle v(t,X\_{t},I\_{t}) | =vâ€‹(tâ€²,Xtâ€²,Itâ€²)+âˆ«tâ€²t(Dxâ€‹vâ€‹(s,Xs,Is))âŠ¤â€‹Ïƒâ€‹(s,Xs,Is)â€‹ğ‘‘Ws\displaystyle=v(t^{\prime},X\_{t^{\prime}},I\_{t^{\prime}})+\int\_{t^{\prime}}^{t}(D\_{x}v(s,X\_{s},I\_{s}))^{\top}\sigma(s,X\_{s},I\_{s})dW\_{s} |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | +âˆ«tâ€²t(âˆ‚vâ€‹(s,Xs,Is)âˆ‚t+â„’xIsâ€‹vâ€‹(s,Xs,Is)+âˆ‘jâ‰ Is(Ï€sIsâ€‹jâ€‹(vâ€‹(s,Xs,j)âˆ’vâ€‹(s,Xs,Is))))â€‹ğ‘‘s.\displaystyle\qquad+\int\_{t^{\prime}}^{t}\left(\frac{\partial v(s,X\_{s},I\_{s})}{\partial t}+\mathcal{L}^{I\_{s}}\_{x}v(s,X\_{s},I\_{s})+\sum\_{j\neq I\_{s}}\left(\pi\_{s}^{I\_{s}j}(v(s,X\_{s},j)-v(s,X\_{s},I\_{s}))\right)\right)ds. |  | (5.4) |

It follows from ([5.1](https://arxiv.org/html/2512.04697v1#S5.E1 "In 5 Reinforcement Learning Algorithm â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")), ([5](https://arxiv.org/html/2512.04697v1#S5.Ex1 "5 Reinforcement Learning Algorithm â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")) and ([5.3](https://arxiv.org/html/2512.04697v1#S5.E3 "In 5 Reinforcement Learning Algorithm â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")) that

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[Mt|â„±tâ€²]\displaystyle\mathbb{E}[M\_{t}|\mathcal{F}\_{t^{\prime}}] |  |
|  |  |  |
| --- | --- | --- |
|  | =ğ”¼â€‹[vâ€‹(t,Xt,It)+âˆ«0t(fâ€‹(s,Xs,Is)+Î»â€‹Râ€‹(ğ…s,Is))â€‹ğ‘‘sâˆ’âˆ‘k=1âˆgÎºkâˆ’1â€‹Îºkâ€‹ğŸ{Ï„kâ‰¤t}|â„±tâ€²]\displaystyle=\mathbb{E}\left[v(t,X\_{t},I\_{t})+\int\_{0}^{t}\left(f(s,X\_{s},I\_{s})+\lambda R({\bm{\pi}}\_{s},I\_{s})\right)ds-\sum\_{k=1}^{\infty}g\_{\kappa\_{k-1}\kappa\_{k}}{\bf 1}\_{\{\tau\_{k}\leq t\}}\Big|\mathcal{F}\_{t^{\prime}}\right] |  |
|  |  |  |
| --- | --- | --- |
|  | =Mtâ€²+ğ”¼â€‹[vâ€‹(t,Xt,It)âˆ’vâ€‹(tâ€²,Xtâ€²,Itâ€²)+âˆ«tâ€²t(fâ€‹(s,Xs,Is)+Î»â€‹Râ€‹(ğ…s,Is))â€‹ğ‘‘sâˆ’âˆ«tâ€²tâˆ‘jâ‰ IsgIsâ€‹jâ€‹Ï€sIsâ€‹jâ€‹dâ€‹s|â„±tâ€²]\displaystyle=M\_{t^{\prime}}+\mathbb{E}\left[v(t,X\_{t},I\_{t})-v(t^{\prime},X\_{t^{\prime}},I\_{t^{\prime}})+\int\_{t^{\prime}}^{t}\left(f(s,X\_{s},I\_{s})+\lambda R({\bm{\pi}}\_{s},I\_{s})\right)ds-\int\_{t^{\prime}}^{t}\sum\_{j\neq I\_{s}}g\_{I\_{s}j}\pi\_{s}^{I\_{s}j}ds\Big|\mathcal{F}\_{t^{\prime}}\right] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | =Mtâ€²+ğ”¼â€‹[âˆ«tâ€²tâˆ«tâ€²t(Dxâ€‹vâ€‹(s,Xs,Is))âŠ¤â€‹Ïƒâ€‹(s,Xs,Is)â€‹ğ‘‘Ws|â„±tâ€²]=Mtâ€².\displaystyle=M\_{t^{\prime}}+\mathbb{E}\left[\int\_{t^{\prime}}^{t}\int\_{t^{\prime}}^{t}(D\_{x}v(s,X\_{s},I\_{s}))^{\top}\sigma(s,X\_{s},I\_{s})dW\_{s}\Big|\mathcal{F}\_{t^{\prime}}\right]=M\_{t^{\prime}}. |  | (5.5) |

Thus, we get the desired result.
âˆ

Let us introduce the notation L2â€‹([0,T])L^{2}([0,T]) as the space of all processes K=(Kt)tâˆˆ[0,T]K=(K\_{t})\_{t\in[0,T]} that KK is ğ”½\mathbb{F}-progressively measurable and satisfies ğ”¼â€‹[âˆ«0T|Kt|2â€‹ğ‘‘t]<âˆ\mathbb{E}[\int\_{0}^{T}|K\_{t}|^{2}dt]<\infty. For any semimartingale N=(Ns)sâˆˆ[0,T]N=(N\_{s})\_{s\in[0,T]}, we denote L2â€‹([0,T];N)L^{2}([0,T];N) the space of all processes K=(Kt)tâˆˆ[0,T]K=(K\_{t})\_{t\in[0,T]} that KK is ğ”½\mathbb{F}-progressively measurable and satisfies

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[âˆ«0T|Kt|2â€‹dâ€‹âŸ¨NâŸ©t]<âˆ,\displaystyle\mathbb{E}\left[\int\_{0}^{T}|K\_{t}|^{2}d\left<N\right>\_{t}\right]<\infty, |  |

where âŸ¨NâŸ©t\left<N\right>\_{t} is the quadratic variation process of NN. It follows from the martingale orthogonality condition that, for any test process Ï‚=(Ï‚t)tâˆˆ[0,T]âˆˆL2â€‹([0,T];M)\varsigma=(\varsigma\_{t})\_{t\in[0,T]}\in L^{2}([0,T];M),

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â€‹[âˆ«0âˆÏ‚tâ€‹ğ‘‘Mt]=0.\displaystyle\mathbb{E}\left[\int\_{0}^{\infty}\varsigma\_{t}dM\_{t}\right]=0. |  | (5.6) |

In fact, the following result shows that this is a necessary and sufficient condition for martingale.

###### Proposition 5.2 (Proposition 4 in jia2022policy).

A diffusion process NN is a martingale if and only if

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â€‹[âˆ«0âˆÏ‚tâ€‹ğ‘‘Nt]=0\displaystyle\mathbb{E}\left[\int\_{0}^{\infty}\varsigma\_{t}dN\_{t}\right]=0 |  | (5.7) |

for any Ï‚âˆˆL2â€‹([0,T];N)\varsigma\in L^{2}([0,T];N).

Given a feedback strategy ğ…â€‹(t,x)=(Ï€iâ€‹jâ€‹(t,x))i,jâˆˆğ•€m{\bm{\pi}}(t,x)=(\pi\_{ij}(t,x))\_{i,j\in\mathbb{I}\_{m}}, we parameterize the value function using a family of functions vÎ¾â€‹(t,x,i)v^{\xi}(t,x,i) satisfying vÎ¾â€‹(T,x,i)=hâ€‹(x)v^{\xi}(T,x,i)=h(x), where Î¾âˆˆÎ˜âŠ‚â„LÎ¾\xi\in\Theta\subset\mathbb{R}^{L\_{\xi}} and LÎ¾L\_{\xi} is the dimension of the parameter vector. Let MÎ¾=(MtÎ¾)tâˆˆ[0,T]M^{\xi}=(M\_{t}^{\xi})\_{t\in[0,T]} be the parameterized version of the martingale process MM. Proposition [5.2](https://arxiv.org/html/2512.04697v1#S5.Thmtheorem2 "Proposition 5.2 (Proposition 4 in jia2022policy). â€£ 5 Reinforcement Learning Algorithm â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes") establishes that finding the optimal parameters Î¾\xi reduces to solving the martingale orthogonality equation ([5.7](https://arxiv.org/html/2512.04697v1#S5.E7 "In Proposition 5.2 (Proposition 4 in jia2022policy). â€£ 5 Reinforcement Learning Algorithm â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")). This can be implemented through stochastic approximation with the parameter update:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î¾â†Î¾+Î±Î¾â€‹âˆ«0TÏ‚sâ€‹ğ‘‘MsÎ¾,\displaystyle\xi\leftarrow\xi+\alpha\_{\xi}\int\_{0}^{T}\varsigma\_{s}dM\_{s}^{\xi}, |  | (5.8) |

where Î±Î¾>0\alpha\_{\xi}>0 is the learning rate.

However, the update rule ([5.8](https://arxiv.org/html/2512.04697v1#S5.E8 "In 5 Reinforcement Learning Algorithm â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")) involves a continuous-time integral that cannot be directly implemented computationally. To address this, we develop a discrete-time approximation of the martingale orthogonality condition. Let Kâˆˆâ„•K\in\mathbb{N} be the number of time intervals and Î”â€‹t=T/K\Delta t=T/K be the step size. Consider the discrete partition 0=t0<t1<t2<â‹¯<tK=T0=t\_{0}<t\_{1}<t\_{2}<\cdots<t\_{K}=T with tkâˆ’tkâˆ’1=Î”â€‹tt\_{k}-t\_{k-1}=\Delta t for k=1,â€¦,Kk=1,\dots,K. Motivated by the continuous-time update ([5.8](https://arxiv.org/html/2512.04697v1#S5.E8 "In 5 Reinforcement Learning Algorithm â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")), we choose the test process Ï‚t=âˆ‚vÎ¾âˆ‚Î¾â€‹(t,Xt,It)\varsigma\_{t}=\frac{\partial v^{\xi}}{\partial\xi}(t,X\_{t},I\_{t}) and propose the following discretized update rule to update parameters after a whole episode (offline):

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î¾â†Î¾+Î±Î¾â€‹âˆ‘k=0Kâˆ’1âˆ‚vÎ¾âˆ‚Î¾â€‹(tk,Xtk,Itk)â€‹Î”â€‹Î¾k\displaystyle\xi\leftarrow\xi+\alpha\_{\xi}\sum\_{k=0}^{K-1}\frac{\partial v^{\xi}}{\partial\xi}(t\_{k},X\_{t\_{k}},I\_{t\_{k}})\Delta\xi\_{k} |  | (5.9) |

or to update parameters at every time step (online):

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î¾â†Î¾+Î±Î¾â€‹âˆ‚vÎ¾âˆ‚Î¾â€‹(tk,Xtk,Itk)â€‹Î”â€‹Î¾k.\displaystyle\xi\leftarrow\xi+\alpha\_{\xi}\frac{\partial v^{\xi}}{\partial\xi}(t\_{k},X\_{t\_{k}},I\_{t\_{k}})\Delta\xi\_{k}. |  | (5.10) |

Here Î”â€‹Î¾k\Delta\xi\_{k} for k=0,1,,..,Kâˆ’1k=0,1,,..,K-1 is given by

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î”â€‹Î¾k=vÎ¾â€‹(tk+1,Xtk+1,Itk+1)âˆ’vÎ¾â€‹(tk,Xtk,Itk)+(fâ€‹(tk,Xtk,Itk)+Î»â€‹Râ€‹(ğ…tkÎ¾,Itk))â€‹Î”â€‹tâˆ’gItkâ€‹Itk+1,\displaystyle\Delta\xi\_{k}=v^{\xi}(t\_{k+1},X\_{t\_{k+1}},I\_{t\_{k+1}})-v^{\xi}(t\_{k},X\_{t\_{k}},I\_{t\_{k}})+\left(f(t\_{k},X\_{t\_{k}},I\_{t\_{k}})+\lambda R({\bm{\pi}}^{\xi}\_{t\_{k}},I\_{t\_{k}})\right)\Delta t-g\_{I\_{t\_{k}}I\_{t\_{k+1}}}, |  | (5.11) |

where the parameterized strategy ğ…Î¾â€‹(t,x)=(Ï€iâ€‹jÎ¾â€‹(t,x))i,jâˆˆğ•€m{\bm{\pi}}^{\xi}(t,x)=(\pi^{\xi}\_{ij}(t,x))\_{i,j\in\mathbb{I}\_{m}} is given by

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ï€iâ€‹jÎ¾â€‹(t,x)=expâ¡(vÎ¾â€‹(t,x,j)âˆ’giâ€‹jâˆ’vÎ¾â€‹(t,x,i)Î»),jâ‰ i,\displaystyle\pi\_{ij}^{\xi}(t,x)=\exp\left(\frac{v^{\xi}(t,x,j)-g\_{ij}-v^{\xi}(t,x,i)}{\lambda}\right),\penalty 10000\ j\neq i, |  | (5.12) |

and Ï€iâ€‹iÎ¾â€‹(t,x)=âˆ’âˆ‘jâ‰ iÏ€iâ€‹jÎ¾â€‹(t,x)\pi\_{ii}^{\xi}(t,x)=-\sum\_{j\neq i}\pi\_{ij}^{\xi}(t,x).

Based on the above updating rules, we can present the pseudo-code of the offline PE algorithm in Algorithm [1](https://arxiv.org/html/2512.04697v1#alg1 "Algorithm 1 â€£ 5 Reinforcement Learning Algorithm â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes"). The online PE algorithm can be devised in a similar fashion and is omitted.

Algorithm 1  Policy Evaluation Algorithm (Offline)

Input:
Initial state (x0,i0)(x\_{0},i\_{0}), horizon TT, number of regimes mm, time step Î”â€‹t\Delta t, number of episodes NN, number of mesh grids KK, initial learning rates Î±Î¾â€‹(â‹…)\alpha\_{\xi}(\cdot) (a function of the number of episodes), functional forms of parameterized value function vÎ¾â€‹(â‹…)v^{\xi}(\cdot), policy ğ…Î¾â€‹(â‹…)\bm{\pi}^{\xi}(\cdot), regime switching costs (giâ€‹j)i,jâˆˆğ•€m(g\_{ij})\_{i,j\in\mathbb{I}\_{m}} and temperature parameter Î»\lambda.
  
â€Š Required Program: an environment simulator (xâ€²,iâ€²,fâ€²)=(x^{\prime},i^{\prime},f^{\prime})= Environment (t,x,i,j)Î”â€‹t{}\_{\Delta t}(t,x,i,j) that takes current time-state pair (t,x,i)(t,x,i) and action jj (the regime to switch to; if j=ij=i, no switching occurs) as inputs and generates state xâ€²x^{\prime}, iâ€²=ji^{\prime}=j and reward fâ€²f^{\prime} at time t+Î”â€‹tt+\Delta t as outputs .
  
â€Š Learning Procedure:

  

1:Initialize Î¾\xi, and â„“=1\ell=1.

2:while â„“<N\ell<N do

3:â€ƒâ€‚Initialize k=0k=0. Observe initial state x0,i0x\_{0},i\_{0} and store (xt0,it0)â†(x0,i0)(x\_{t\_{0}},i\_{t\_{0}})\leftarrow(x\_{0},i\_{0}).

4:â€ƒâ€‚while k<Kk<K do

5:â€ƒâ€ƒâ€ƒ Generate action jtkj\_{t\_{k}} by ğ…Î¾â€‹(tk,xtk)\bm{\pi}^{\xi}\left(t\_{k},x\_{t\_{k}}\right).

6:â€ƒâ€ƒâ€ƒ Apply jtkj\_{t\_{k}} to environment simulator (x,i,f)=(x,i,f)= Environment (tk,xtk,itk,jtk)Î”â€‹t{}\_{\Delta t}(t\_{k},x\_{t\_{k}},i\_{t\_{k}},j\_{t\_{k}}).

7:â€ƒâ€ƒâ€ƒ Observe new state xx and ii as output. Store xtk+1â†xx\_{t\_{k+1}}\leftarrow x, itk+1â†ii\_{t\_{k+1}}\leftarrow i and ftkâ†ff\_{t\_{k}}\leftarrow f.

8:â€ƒâ€ƒâ€ƒ Update kâ†k+1k\leftarrow k+1.

9:â€ƒâ€‚end while

10:â€ƒâ€‚For every k=0,1,â€¦,Kâˆ’1k=0,1,...,K-1, compute

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î”â€‹Î¾k\displaystyle\Delta\xi\_{k} | =vÎ¾â€‹(tk+1,xtk+1,itk+1)âˆ’vÎ¾â€‹(tk,xtk,itk)+(ftk+Î»â€‹Râ€‹(ğ…Î¾â€‹(tk,xtk),itk))â€‹Î”â€‹tâˆ’gitkâ€‹itk+1.\displaystyle=v^{\xi}(t\_{k+1},x\_{t\_{k+1}},i\_{t\_{k+1}})-v^{\xi}(t\_{k},x\_{t\_{k}},i\_{t\_{k}})+\left(f\_{t\_{k}}+\lambda R({\bm{\pi}}^{\xi}(t\_{k},x\_{t\_{k}}),i\_{t\_{k}})\right)\Delta t-g\_{i\_{t\_{k}}i\_{t\_{k+1}}}. |  |

11:â€ƒâ€‚Update Î¾\xi by

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î¾\displaystyle\xi | â†Î¾+Î±Î¾â€‹(â„“)â€‹âˆ‘k=0Kâˆ’1âˆ‚vÎ¾âˆ‚Î¾â€‹(tk,xtk,itk)â€‹Î”â€‹Î¾k,\displaystyle\leftarrow\xi+\alpha\_{\xi}(\ell)\sum\_{k=0}^{K-1}\frac{\partial v^{\xi}}{\partial\xi}\left(t\_{k},x\_{t\_{k}},i\_{t\_{k}}\right)\Delta\xi\_{k}, |  |

12:â€ƒâ€‚Update â„“â†â„“+1\ell\leftarrow\ell+1.

13:end while

Proposition [4.1](https://arxiv.org/html/2512.04697v1#S4.Thmtheorem1 "Proposition 4.1. â€£ 4 Policy Iteration and Convergence â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes") and Theorem [4.2](https://arxiv.org/html/2512.04697v1#S4.Thmtheorem2 "Theorem 4.2. â€£ 4 Policy Iteration and Convergence â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes") confirm the improvement and convergence results of the policy iteration. Meanwhile, Lemma [5.1](https://arxiv.org/html/2512.04697v1#S5.Thmtheorem1 "Lemma 5.1. â€£ 5 Reinforcement Learning Algorithm â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes") and Proposition [5.2](https://arxiv.org/html/2512.04697v1#S5.Thmtheorem2 "Proposition 5.2 (Proposition 4 in jia2022policy). â€£ 5 Reinforcement Learning Algorithm â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes") show that policy evaluation can be performed by solving the martingale orthogonality condition via stochastic approximation. A natural question arises: what can be said about the convergence of Algorithm [1](https://arxiv.org/html/2512.04697v1#alg1 "Algorithm 1 â€£ 5 Reinforcement Learning Algorithm â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")? To address this, we next turn to an analysis of the error estimates for Algorithm [1](https://arxiv.org/html/2512.04697v1#alg1 "Algorithm 1 â€£ 5 Reinforcement Learning Algorithm â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes").

We reformulate the update rule in equation ([5.9](https://arxiv.org/html/2512.04697v1#S5.E9 "In 5 Reinforcement Learning Algorithm â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")) as follows:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î¾i+1â†Î¾i+Î±Î¾â€‹(i)â€‹Î¨â€‹(Î¾i;X,I,ğ…Î¾i),iâ‰¥1,\displaystyle\xi\_{i+1}\leftarrow\xi\_{i}+\alpha\_{\xi}(i)\Psi(\xi\_{i};X,I,{\bm{\pi}}^{\xi\_{i}}),\quad i\geq 1, |  | (5.13) |

where

|  |  |  |
| --- | --- | --- |
|  | Î¨â€‹(Î¾i;X,I,ğ…Î¾i)=âˆ‘k=0Kâˆ’1âˆ‚vÎ¾âˆ‚Î¾â€‹(tk,Xtk,Itk)â€‹Î”â€‹Î¾k,\displaystyle\Psi(\xi\_{i};X,I,{\bm{\pi}}^{\xi\_{i}})=\sum\_{k=0}^{K-1}\frac{\partial v^{\xi}}{\partial\xi}(t\_{k},X\_{t\_{k}},I\_{t\_{k}})\Delta\xi\_{k}, |  |

with Î”â€‹Î¾k\Delta\xi\_{k} defined in equation ([5.11](https://arxiv.org/html/2512.04697v1#S5.E11 "In 5 Reinforcement Learning Algorithm â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")). For notational convenience, we introduce the shorthand Yi+1=(X,I,ğ…Î¾i)Y\_{i+1}=(X,I,{\bm{\pi}}^{\xi\_{i}}) for iâ‰¥1i\geq 1. We further define the expected update function as Ïˆâ€‹(Î¾):=ğ”¼â€‹[Î¨â€‹(Î¾;Y)]\psi(\xi):=\mathbb{E}[\Psi(\xi;Y)]. To establish convergence guarantees, we make the following technical assumptions.

###### Assumption 5.3.

* (i)

  The ordinary differential equation Î¾â€²â€‹(t)=Ïˆâ€‹(Î¾â€‹(t))\xi^{\prime}(t)=\psi(\xi(t)) has a unique stable equilibrium point Î¾âˆ—\xi^{\*}.
* (ii)

  There exists a constant C>0C>0 such that ğ”¼â€‹[|Î¨â€‹(Î¾i;Yi+1)|2|Î¾i]â‰¤Câ€‹(1+|Î¾i|2)\mathbb{E}[|\Psi(\xi\_{i};Y\_{i+1})|^{2}|\xi\_{i}]\leq C(1+|\xi\_{i}|^{2}) for all iterations.
* (iii)

  There exists Îº>0\kappa>0 such that (Î¾âˆ’Î¾âˆ—)â‹…Ïˆâ€‹(Î¾)â‰¤âˆ’Îºâ€‹|Î¾âˆ’Î¾âˆ—|2(\xi-\xi^{\*})\cdot\psi(\xi)\leq-\kappa|\xi-\xi^{\*}|^{2} for all Î¾âˆˆâ„LÎ¾\xi\in\mathbb{R}^{L\_{\xi}}.
* (iv)

  There exist constants Ï,C>0\rho,C>0 such that supjâˆˆğ•€m|vÎ¾â€‹(â‹…,j)âˆ’vÎ¾âˆ—â€‹(â‹…,j)|C0â€‹(ğ’ŸÂ¯)â‰¤Câ€‹|Î¾âˆ’Î¾âˆ—|Ï\sup\_{j\in\mathbb{I}\_{m}}|v^{\xi}(\cdot,j)-v^{\xi^{\*}}(\cdot,j)|\_{C^{0}(\overline{{\cal D}})}\leq C|\xi-\xi^{\*}|^{\rho} for all Î¾âˆˆâ„LÎ¾\xi\in\mathbb{R}^{L\_{\xi}}.

Under these conditions, we now present the main convergence result, which provides the explicit error bound for Algorithm [1](https://arxiv.org/html/2512.04697v1#alg1 "Algorithm 1 â€£ 5 Reinforcement Learning Algorithm â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes").

###### Theorem 5.4.

Let Assumption [5.3](https://arxiv.org/html/2512.04697v1#S5.Thmtheorem3 "Assumption 5.3. â€£ 5 Reinforcement Learning Algorithm â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes") hold. Set Î±Î¾â€‹(i)=AiÎ½+B\alpha\_{\xi}(i)=\frac{A}{i^{\nu}+B} for some Î½â‰¤1\nu\leq 1, A>Î½2â€‹ÎºA>\frac{\nu}{2\kappa} and B>0B>0, and let Ïµ>0\epsilon>0. Then there exists C>0C>0 (independent of n,Ïµn,\epsilon) such that with probability of at least 1âˆ’Ïµ1-\epsilon,

|  |  |  |  |
| --- | --- | --- | --- |
|  | supjâˆˆğ•€m|vÎ¾iâ€‹(â‹…,j)âˆ’vâ€‹(â‹…,j)|C0â€‹(ğ’ŸÂ¯)â‰¤supjâˆˆğ•€m|vâ€‹(â‹…,j)âˆ’vÎ¾âˆ—â€‹(â‹…,j)|C0â€‹(ğ’ŸÂ¯)+CÏµÏÎ¾/2â€‹iâˆ’Î½â€‹ÏÎ¾2.\displaystyle\sup\_{j\in\mathbb{I}\_{m}}|v^{\xi\_{i}}(\cdot,j)-v(\cdot,j)|\_{C^{0}({\overline{{\cal D}}})}\leq\sup\_{j\in\mathbb{I}\_{m}}|v(\cdot,j)-v^{\xi^{\*}}(\cdot,j)|\_{C^{0}({\overline{{\cal D}}})}+\frac{C}{\epsilon^{\rho\_{\xi}/2}}i^{-\frac{\nu\rho\_{\xi}}{2}}. |  | (5.14) |

###### Proof.

Under Assumptions [5.3](https://arxiv.org/html/2512.04697v1#S5.Thmtheorem3 "Assumption 5.3. â€£ 5 Reinforcement Learning Algorithm â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes") (i)â€“(iii) and the step-size condition on Î±Î¾â€‹(i)\alpha\_{\xi}(i), an application of Theorem 22 in benveniste2012adaptive yields

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[|Î¾iâˆ’Î¾âˆ—|2]â‰¤Câ€‹iâˆ’Î½\displaystyle\mathbb{E}[|\xi\_{i}-\xi^{\*}|^{2}]\leq Ci^{-\nu} |  |

where C>0C>0 is a constant independent of nn. This bound in turn implies that

|  |  |  |
| --- | --- | --- |
|  | |Î¾iâˆ’Î¾âˆ—|2â‰¤Câ€‹Ïµâˆ’12â€‹iâˆ’Î½2\displaystyle|\xi\_{i}-\xi^{\*}|^{2}\leq C\epsilon^{-\frac{1}{2}}i^{-\frac{\nu}{2}} |  |

with probability at least 1âˆ’Ïµ1-\epsilon. Then, invoking Assumption [5.3](https://arxiv.org/html/2512.04697v1#S5.Thmtheorem3 "Assumption 5.3. â€£ 5 Reinforcement Learning Algorithm â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes") (iv), we deduce that with probability at least 1âˆ’Ïµ1-\epsilon,

|  |  |  |
| --- | --- | --- |
|  | supjâˆˆğ•€m|vÎ¾iâ€‹(â‹…,j)âˆ’vâ€‹(â‹…,j)|C0â€‹(ğ’ŸÂ¯)\displaystyle\sup\_{j\in\mathbb{I}\_{m}}|v^{\xi\_{i}}(\cdot,j)-v(\cdot,j)|\_{C^{0}({\overline{{\cal D}}})} |  |
|  |  |  |
| --- | --- | --- |
|  | â‰¤supjâˆˆğ•€m|vâ€‹(â‹…,j)âˆ’vÎ¾âˆ—â€‹(â‹…,j)|C0â€‹(ğ’ŸÂ¯)+supjâˆˆğ•€m|vÎ¾âˆ—â€‹(â‹…,j)âˆ’vÎ¾iâ€‹(â‹…,j)|C0â€‹(ğ’ŸÂ¯)\displaystyle\leq\sup\_{j\in\mathbb{I}\_{m}}|v(\cdot,j)-v^{\xi^{\*}}(\cdot,j)|\_{C^{0}({\overline{{\cal D}}})}+\sup\_{j\in\mathbb{I}\_{m}}|v^{\xi^{\*}}(\cdot,j)-v^{\xi\_{i}}(\cdot,j)|\_{C^{0}({\overline{{\cal D}}})} |  |
|  |  |  |
| --- | --- | --- |
|  | â‰¤supjâˆˆğ•€m|vÎ¾âˆ—(â‹…,)âˆ’v(â‹…,j)|C0â€‹(ğ’ŸÂ¯)+CÏµÏÎ¾/2iâˆ’Î½â€‹ÏÎ¾2.\displaystyle\leq\sup\_{j\in\mathbb{I}\_{m}}|v^{\xi^{\*}}(\cdot,)-v(\cdot,j)|\_{C^{0}({\overline{{\cal D}}})}+\frac{C}{\epsilon^{\rho\_{\xi}/2}}i^{-\frac{\nu\rho\_{\xi}}{2}}. |  |

This completes the proof of the theorem.
âˆ

Theorem [5.4](https://arxiv.org/html/2512.04697v1#S5.Thmtheorem4 "Theorem 5.4. â€£ 5 Reinforcement Learning Algorithm â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes") establishes a comprehensive error analysis for Algorithm [1](https://arxiv.org/html/2512.04697v1#alg1 "Algorithm 1 â€£ 5 Reinforcement Learning Algorithm â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes"), providing both theoretical guarantees and practical insights into its convergence behavior. The result demonstrates that the policy evaluation error can be systematically decomposed into two distinct components: the approximation error of the parametric function class and the algorithmic error arising from the stochastic approximation procedure. The first term, supjâˆˆğ•€m|vâ€‹(â‹…,j)âˆ’vÎ¾âˆ—â€‹(â‹…,j)|C0â€‹(ğ’ŸÂ¯)\sup\_{j\in\mathbb{I}\_{m}}|v(\cdot,j)-v^{\xi^{\*}}(\cdot,j)|\_{C^{0}({\overline{{\cal D}}})}, represents the inherent approximation capability of our chosen parametric family. This bias term is independent of the learning algorithm and reflects how well the optimal parameter Î¾âˆ—\xi^{\*} can approximate the true value function within the selected function class. The second term, Câ€‹iâˆ’Î½â€‹ÏÎ¾2/ÏµÏÎ¾/2Ci^{-\frac{\nu\rho\_{\xi}}{2}}/\epsilon^{\rho\_{\xi}/2}, exhibits a polynomial decay with respect to the iteration number ii and vanishes asymptotically as the number of iterations increases, demonstrating the algorithmâ€™s convergence to the optimal parameter configuration within the chosen function class.

## 6 Numerical Examples

This section presents some numerical experiments to demonstrate the practical efficacy of the proposed RL algorithm. We first examine a bounded regulator problem to analyze the algorithmâ€™s convergence property and policy behavior. Subsequently, we apply the algorithm to a put option selection problem involving the optimal switching between risky assets, showcasing its effectiveness in a more complex, multi-regime setting with some financial interpretations.

### 6.1 Bounded Regulator Problem

To establish a performance benchmark for our algorithm, we consider a finite-horizon optimal switching problem with two regimes, conceptualized as a bounded regulator. This classic problem provides a tractable yet non-trivial testbed where the optimal policy has an intuitive structure, allowing for clear interpretation of the algorithmâ€™s learned strategy.

The system state X=(Xt)tâˆˆ[0,T]X=(X\_{t})\_{t\in[0,T]} evolves according to regime-specific stochastic dynamics:

|  |  |  |  |
| --- | --- | --- | --- |
|  | dâ€‹Xt=Î¼iâ€‹dâ€‹t+Ïƒâ€‹dâ€‹Wt,iâˆˆ{0,1},tâˆˆ[0,T],\displaystyle dX\_{t}=\mu\_{i}dt+\sigma dW\_{t},\quad i\in\{0,1\},\penalty 10000\ t\in[0,T], |  | (6.1) |

with initial condition X0=xâˆˆâ„X\_{0}=x\in\mathbb{R}. Here, W=(Wt)tâˆˆ[0,T]W=(W\_{t})\_{t\in[0,T]} is a standard Brownian motion. The parameters are chosen with symmetry: the drift coefficients are Î¼0=âˆ’2\mu\_{0}=-2 and Î¼1=2\mu\_{1}=2, and the volatility is Ïƒ=0.5\sigma=0.5. This symmetric setup induces a natural switching logic to correct the stateâ€™s deviation.

The controllerâ€™s objective is to maximize the expected total reward over the horizon [0,T][0,T], which comprises a running reward and a terminal reward:

|  |  |  |
| --- | --- | --- |
|  | fâ€‹(x)=2â€‹eâˆ’2â€‹x2âˆ’0.1,hâ€‹(x)=2â€‹eâˆ’2â€‹x2,xâˆˆâ„.\displaystyle f(x)=2e^{-2x^{2}}-0.1,\quad h(x)=2e^{-2x^{2}},\quad x\in\mathds{R}. |  |

The Gaussian bump shape of the functions ff and hh creates a strong incentive to maintain the state XtX\_{t} near zero, as the reward attains its maximum value at x=0x=0. Each switch between regimes incurs a cost, specified as g01=g10=0.5g\_{01}=g\_{10}=0.5. This cost penalizes excessive control actions, forcing the optimal policy to strategically balance the benefit of corrective switching against the incurred cost.

We use a discrete version of ([6.1](https://arxiv.org/html/2512.04697v1#S6.E1 "In 6.1 Bounded Regulator Problem â€£ 6 Numerical Examples â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")) for t=0,Î”â€‹t,â€¦,Kâ€‹Î”â€‹tt=0,\Delta t,...,K\Delta t with K=100K=100 and Î”â€‹t=T/K\Delta t=T/K. The value function and policy are approximated by a neural network in the PyTorch framework
with the architecture and parameters summarized in Table [1](https://arxiv.org/html/2512.04697v1#S6.T1 "Table 1 â€£ 6.1 Bounded Regulator Problem â€£ 6 Numerical Examples â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes").

Table 1: Neural Network Architecture and Training Parameters for the Regulator Problem

|  |  |
| --- | --- |
| Component | Specification |
| Network Architecture | 2 hidden layers |
| Activation Functions | ReLU (Layer 1), Tanh (Layer 2) |
| Hidden Dimension | 128 |
| Batch Size | 64 |
| Optimizer | Adam |
| Learning Rate | 1Ã—10âˆ’31\times 10^{-3} |
| Training Episodes | 1000 |

The training progression under the temperature parameter Î»=0.2\lambda=0.2 is shown in Figure [1](https://arxiv.org/html/2512.04697v1#S6.F1 "Figure 1 â€£ 6.1 Bounded Regulator Problem â€£ 6 Numerical Examples â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")-(a). The loss function decreases efficiently and stabilizes after approximately 400 episodes, indicating the robust convergence of the policy iteration in the RL algorithm. Figure [1](https://arxiv.org/html/2512.04697v1#S6.F1 "Figure 1 â€£ 6.1 Bounded Regulator Problem â€£ 6 Numerical Examples â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes")-(b) depicts the learned value functions and the corresponding switching probabilities at t=0.5t=0.5. The near symmetry between the value functions for regime 0 (blue line) and regime 1 (orange line) is a direct consequence of the symmetric problem parameters. The switching probabilitiesâ€”from regime 0 to 1 (green line) and from regime 1 to 0 (yellow line)â€”are calculated from the optimal intensity Ï€\pi.

![Refer to caption](convergence.png)

![Refer to caption](value.png)

Figure 1: (a): Convergence of the training loss for the bounded regulator problem with Î»=0.2\lambda=0.2. (b): Learned value functions and switching probabilities at t=0.5t=0.5 for Î»=0.2\lambda=0.2.

A central theoretical result is the convergence of the exploratory solution to the classical optimal switching policy as the temperature parameter Î»\lambda tends to zero. We validate this numerically.
Figure [2](https://arxiv.org/html/2512.04697v1#S6.F2 "Figure 2 â€£ 6.1 Bounded Regulator Problem â€£ 6 Numerical Examples â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes") shows that the training loss decreases for different values of Î»\lambda, with convergence achieved in all cases. More importantly, Figure [3](https://arxiv.org/html/2512.04697v1#S6.F3 "Figure 3 â€£ 6.1 Bounded Regulator Problem â€£ 6 Numerical Examples â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes") illustrates the fundamental transformation of the optimal policy. For a larger Î»\lambda (e.g., 0.2), the switching probability is a smooth function of the state, reflecting exploratory randomization. As Î»\lambda decreases to 0.01, the probability curve becomes sharp and nearly binary, approaching a deterministic threshold-based policy. This visual evidence strongly supports the theoretical finding that the solutions of the exploratory HJB equations converge to the solution of the classical variational inequalities as Î»â†’0\lambda\to 0.

![Refer to caption](compare_lambda_convergence.png)


Figure 2: Training convergence for different temperature parameters Î»\lambda.

![Refer to caption](compare_lambda_probability.png)


Figure 3: Evolution of the switching probability from regime 0 to 1 as Î»\lambda decreases.

### 6.2 Put Option Selection Problem

To demonstrate the algorithmâ€™s applicability in finance, we model an investor who aims to optimally switch an investment decision between three regimes: two European put options on different assets and a risk-free savings account. The investorâ€™s wealth can be allocated to one of three regimes during the finite horizon [0,T][0,T]:

* â€¢

  regime 0: a put option on Stock AA.
* â€¢

  regime 1: a put option on Stock BB.
* â€¢

  regime 2: the risk-free savings account.

The underlying stock prices follow the geometric Brownian motion:

|  |  |  |
| --- | --- | --- |
|  | dâ€‹StA=Î¼Aâ€‹StAâ€‹dâ€‹t+ÏƒAâ€‹StAâ€‹dâ€‹Wt,dâ€‹StB=Î¼Bâ€‹StBâ€‹dâ€‹t+ÏƒBâ€‹StBâ€‹dâ€‹Wt,tâˆˆ(0,T],\displaystyle dS^{A}\_{t}=\mu^{A}S^{A}\_{t}dt+\sigma^{A}S^{A}\_{t}dW\_{t},\quad dS^{B}\_{t}=\mu^{B}S^{B}\_{t}dt+\sigma^{B}S^{B}\_{t}dW\_{t},\quad t\in(0,T], |  |

with S0A=sAâˆˆ[0,âˆ),S0B=sBâˆˆ[0,âˆ)S^{A}\_{0}=s^{A}\in[0,\infty),S^{B}\_{0}=s^{B}\in[0,\infty). Here the parameters are set by (Î¼A,ÏƒA)=(0.1,0.2)(\mu^{A},\sigma^{A})=(0.1,0.2) and (Î¼B,ÏƒB)=(0.05,0.1)(\mu^{B},\sigma^{B})=(0.05,0.1), and W=(Wt)tâˆˆ[0,T]W=(W\_{t})\_{t\in[0,T]} is a standard Brownian motion. The risk free rate is r=0.05r=0.05. For any time tâˆˆ[0,T]t\in[0,T], the investor decides a action Itâˆˆ{0,1,2}I\_{t}\in\{0,1,2\}, which determines the regime in which the investorâ€™s wealth is allocated. Switching between regimes incurs transaction costs given by the matrix:

|  |  |  |
| --- | --- | --- |
|  | G=(giâ€‹j)0â‰¤i,jâ‰¤2=[00.020.010.0200.010.020.020].\displaystyle G=(g\_{ij})\_{0\leq i,j\leq 2}=\begin{bmatrix}0&0.02&0.01\\ 0.02&0&0.01\\ 0.02&0.02&0\end{bmatrix}. |  |

The investorâ€™s objective is to maximize the expected total reward over the horizon [0,T][0,T], where the running reward function is given by

|  |  |  |
| --- | --- | --- |
|  | fâ€‹(sA,sB,i)={(SKâˆ’sA)+,i=0,(SKâˆ’sB)+,i=1,râ€‹SK,i=2,\displaystyle f(s^{A},s^{B},i)=\begin{cases}(S\_{K}-s^{A})^{+},&i=0,\\ (S\_{K}-s^{B})^{+},&i=1,\\ rS\_{K},&i=2,\end{cases} |  |

with the strike price SK=1S\_{K}=1 and (x)+:=maxâ¡{x,0}(x)^{+}:=\max\{x,0\} for xâˆˆâ„x\in\mathds{R}. The terminal reward function is assumed to be 0.

We set the time horizon T=1T=1, the number of time intervals K=50K=50, the step size Î”â€‹t=T/K=0.02\Delta t=T/K=0.02, and the temperature parameter Î»=0.1\lambda=0.1. The value function and policy are approximated by a neural network with the architecture and parameters summarized in Table [2](https://arxiv.org/html/2512.04697v1#S6.T2 "Table 2 â€£ 6.2 Put Option Selection Problem â€£ 6 Numerical Examples â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes"). The model was implemented within the PyTorch framework.

Table 2: Neural Network Architecture and Training Parameters for the Regulator Problem

|  |  |
| --- | --- |
| Component | Specification |
| Network Architecture | 2 hidden layers |
| Activation Functions | Tanh (Layer 1), Tanh (Layer 2) |
| Hidden Dimension | 128 |
| Batch Size | 512 |
| Optimizer | Adam |
| Learning Rate | 1Ã—10âˆ’41\times 10^{-4} |
| Training Episodes | 1000 |

According to Figure [4](https://arxiv.org/html/2512.04697v1#S6.F4 "Figure 4 â€£ 6.2 Put Option Selection Problem â€£ 6 Numerical Examples â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes"), at the beginning of training, the loss exhibits a oscillation, and the convergence is very pronounced. It becomes stable when the number of episodes exceeds 800. Figure [5](https://arxiv.org/html/2512.04697v1#S6.F5 "Figure 5 â€£ 6.2 Put Option Selection Problem â€£ 6 Numerical Examples â€£ Continuous-time reinforcement learning for optimal switching over multiple regimes") shows the allocation of the asset at time t=0.5t=0.5. We can find that, when stock price of AA and BB large enough, the investor will put all in bank. When stock B has lower price, she tends to hold put AA; When stock AA has lower price, she tends to hold put BB.

![Refer to caption](loss_option.png)


Figure 4: The training loss for the put option selection problem.

![Refer to caption](asset_allocation.png)


Figure 5: The optimal asset allocation policy at t=0.5t=0.5 as a function of stock prices SAS^{A} and SBS^{B}.

Acknowledgements: Yijie Huang, Mengge Li and Xiang Yu are supported by the Hong Kong RGC General Research Fund (GRF) under grant no. 15211524 and the Hong Kong Polytechnic University research grant under no. P0045654.