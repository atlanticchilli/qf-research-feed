---
authors:
- Dimitrios Roxanas
doc_id: arxiv:2512.22109v1
family_id: arxiv:2512.22109
is_current: true
taxonomy:
  alpha_families: []
  asset_classes: []
  horizons: []
  themes: []
title: Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse
  Modelling and Uncertainty Quantification
url_abs: http://arxiv.org/abs/2512.22109v1
url_html: https://arxiv.org/html/2512.22109v1
venue: arXiv q-fin
version: 1
year: 2025
---


Dimitrios Roxanas

###### Abstract

We study the construction and rebalancing of sparse index-tracking portfolios from an operational research perspective, with explicit emphasis on uncertainty quantification and implementability. The decision variables are portfolio weights constrained to sum to one; the aims are to track a reference index closely while controlling the number of names and the turnover induced by rebalancing. We cast index tracking as a high-dimensional linear regression of index returns on constituent returns, and employ a sparsity-inducing Laplace prior on the weights. A single global shrinkage parameter controls the trade-off between tracking error and sparsity, and is calibrated by an empirical-Bayes stochastic approximation scheme. Conditional on this calibration, we approximate the posterior distribution of the portfolio weights using proximal Langevin-type Markov chain Monte Carlo algorithms tailored to the budget constraint. This yields posterior uncertainty on tracking error, portfolio composition and prospective rebalancing moves. Building on these posterior samples, we propose rules for rebalancing that gate trades through magnitude-based thresholds and posterior activation probabilities, thereby trading off expected tracking error against turnover and portfolio size. A case study on tracking the S&P¬†500 index is carried out to showcase how our tools shape the decision process from portfolio construction to rebalancing.

Keywords: Portfolio optimization; Index tracking; Uncertainty Quantification; Proximal MCMC; Rebalancing.

## 1 Introduction

Index-tracking funds seek to reproduce the performance of a market index
using a portfolio of its constituents. In practice, investors rarely hold all the names: they impose constraints on the number of assets, turnover, sector exposures, etc. This leads to an optimisation problem where one aims to minimise tracking error (TE) subject to budget and implementability constraints, and possibly with an explicit sparsity requirement on the weights.

A vast literature formulates index tracking as a deterministic optimisation problem, often using mixed-integer programming or convex relaxations; see, for example, (?, ?, ?, ?, ?, ?, ?, ?) and the references therein. The classical approaches deliver point portfolios but do not quantify uncertainty about the weights, the realised TE, or the need for rebalancing. Moreover, the cardinality of the tracking portfolio is either predetermined or controlled through a tuning parameter. In the latter case, a TE vs sparsity trade-off curve is commonly computed to help choose a good operating point, resulting in a large number of optimisation problems needing to be solved.

In parallel, Bayesian methods in portfolio optimisation have explored priors on weights and analytical posterior calculations in conjugate settings (e.g., (?, ?)). However, full posterior sampling over portfolio weights remains rare, particularly for index tracking, where the combination of high dimension, constraints and sparsity makes classical Markov chain Monte Carlo (MCMC) challenging. While a Bayesian *interpretation* is sometimes given for norm-constrained or penalised, e.g., with an ‚Ñì1\ell\_{1} penalty augmentation, portfolios, e.g., (?, ?, ?, ?, ?, ?), *inference* on portfolio weights in a Bayesian framework is rarely employed. Existing sampling work is practically non-existent, and only focuses on subset selection, where one samples subsets of assets and fits weights conditionally, rather than sampling the full weight vector (?, ?, ?).

We address this gap by developing a *Bayesian* index-tracking formulation: a Gaussian regression likelihood, a sparsity-inducing Laplace prior on weights, and a soft budget constraint modelled as a Gaussian pseudo-observation. By sampling a posterior *over weights* directly, we obtain uncertainty quantification (UQ) on both magnitudes and support, enabling risk-aware decisions and hyperparameter learning within one probabilistic framework.

In particular, our approach will be based on the posterior distribution of weights ww, based on the observed data on index and asset returns,

|  |  |  |
| --- | --- | --- |
|  | œÄ‚Äã(w):=p‚Äã(w|y)‚àùp‚Äã(y|w)‚Äãp‚Äã(w)\pi(w):=p(w|y)\propto\displaystyle p(y|w)\;p(w) |  |

All the models that we will consider will be of the form (up to a normalising factor)

|  |  |  |
| --- | --- | --- |
|  | œÄ‚Äã(w)‚àùe‚àífy‚Äã(w)‚àíŒ∏‚Äãg‚Äã(w),\pi(w)\propto e^{-f\_{y}(w)-\theta g(w)}, |  |

the first factor corresponding to the likelihood (or model) part, while the second corresponds to the prior (here chosen to promote sparsity).

The contribution of this paper is to develop and study a *sampling-based* framework for sparse index tracking that connects modern optimisation and MCMC with decision support for portfolio construction and rebalancing. Our starting point is a regression formulation of index tracking, combined with a sparsity-inducing Laplace prior on the weights and a soft budget constraint introduced through a Gaussian pseudo-observation.

In particular, we

1. (C1)

   adopt an empirical-Bayes perspective and estimate a single global sparsity parameter by stochastic approximation (SAPG) (?, ?), rather than imposing it by hand;
2. (C2)

   approximate the resulting posterior distribution using proximal
   Langevin-type MCMC (MYULA and preconditioned MALA) (?, ?, ?), which leverage
   Moreau‚ÄìYosida regularisation and proximal mappings to handle the
   nonsmooth Laplace term under constraints;
3. (C3)

   use posterior samples to define simple, interpretable rules that
   connect UQ to implementable decisions: support selection, long-only portfolio construction and TE/turnover-aware rebalancing.

We illustrate the approach on a case study tracking the S&P¬†500 index
using a universe of several hundred constituents, over multiple fitting and holding periods. The case study highlights: (i) the quality of the sparse trackers obtained after posterior-informed selection, (ii) how posterior uncertainty on weight changes can be used to gate rebalancing decisions, and (iii) the trade-off between tracking error and turnover induced by different choices of thresholds.

We remark that the methodology can be easily extended beyond index tracking. It can also be applied to other sparse feature selection, and linear inverse problems with linear equality constraints, where one wishes to combine regularised optimisation, empirical-Bayes tuning of penalty scales and UQ-informed decision rules.

The rest of the paper is organised as follows. [SectionÀú2](https://arxiv.org/html/2512.22109v1#S2 "2 Model considerations ‚Ä£ Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification") introduces the Bayesian regression model, the soft budget constraint and the Laplace prior, and briefly describes empirical-Bayes calibration. [SectionÀú3](https://arxiv.org/html/2512.22109v1#S3 "3 Proximal MCMC for the posterior ‚Ä£ Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification") outlines the proximal MCMC algorithms used for posterior approximation. [SectionÀú4](https://arxiv.org/html/2512.22109v1#S4 "4 Posterior‚Äìinformed support selection ‚Ä£ Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification") discusses the construction of a toolset based on posterior information, which is later used in [SectionÀú5](https://arxiv.org/html/2512.22109v1#S5 "5 Construction of a tradeable portfolio ‚Ä£ Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification") to construct a sparse, tradeable portfolio. [SectionÀú6](https://arxiv.org/html/2512.22109v1#S6 "6 Rebalancing ‚Ä£ Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification") presents a rebalancing
formulation based on weight adjustments and uncertainty-based gating
rules. [SectionÀú7](https://arxiv.org/html/2512.22109v1#S7 "7 A case study: tracking the S&P 500 ‚Ä£ Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification") documents a full case study on S&P¬†500 tracking. [SectionÀú8](https://arxiv.org/html/2512.22109v1#S8 "8 Discussion ‚Ä£ Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification") summarises the findings and outlines extensions.
Theoretical and technical details, as well as implementation guidelines and diagnostics, are left for the Appendices.

##### Acknowledgements:

The author wishes to thank K. Triantafyllopoulos (University of Sheffield) and K. Zygalakis (University of Edinburgh) for useful discussions.

## 2 Model considerations

### 2.1 Regression view and soft budget constraint

Let yty\_{t} denote the returns of the index at time tt and
rt‚àà‚Ñùpr\_{t}\in\mathbb{R}^{p} the vector of returns of the pp constituents,
for t=1,‚Ä¶,Tt=1,\dots,T. We collect the data in y‚àà‚ÑùTy\in\mathbb{R}^{T} and
R‚àà‚ÑùT√ópR\in\mathbb{R}^{T\times p}, and consider the regression

|  |  |  |  |
| --- | --- | --- | --- |
|  | yt=rt‚ä§‚Äãw+Œµt,Œµt‚àºùí©‚Äã(0,œÉ2),y\_{t}=r\_{t}^{\top}w+\varepsilon\_{t},\qquad\varepsilon\_{t}\sim\mathcal{N}(0,\sigma^{2}), |  | (1) |

or in vector form y=R‚Äãw+Œµy=Rw+\varepsilon with Œµ‚àºùí©‚Äã(0,œÉ2‚ÄãIT)\varepsilon\sim\mathcal{N}(0,\sigma^{2}I\_{T}).
The portfolio weights w‚àà‚Ñùpw\in\mathbb{R}^{p} must satisfy a budget constraint
1‚ä§‚Äãw=11^{\top}w=1, and we enforce long-only constraints wj‚â•0w\_{j}\geq 0 when constructing the final tradeable portfolio.

Working with a *hard* budget constraint inside the prior is natural
but makes empirical-Bayes updates and proximal MCMC more cumbersome.
Instead, we follow a pseudo-observation approach: we introduce
a Gaussian penalty that softly enforces the budget,

|  |  |  |  |
| --- | --- | --- | --- |
|  | fy‚Äã(w)=12‚ÄãœÉ2‚Äã‚Äñy‚àíR‚Äãw‚Äñ22+Œõ‚Äã(1‚ä§‚Äãw‚àí1)2,f\_{y}(w)=\frac{1}{2\sigma^{2}}\|y-Rw\|\_{2}^{2}+\Lambda\bigl(1^{\top}w-1\bigr)^{2}, |  | (2) |

where Œõ=1/(2‚ÄãœÑc2)\Lambda=1/(2\tau\_{c}^{2}) and œÑc>0\tau\_{c}>0 encodes a tolerated
deviation from the exact budget. This yields the likelihood

|  |  |  |  |
| --- | --- | --- | --- |
|  | p‚Äã(y‚à£w)‚àùexp‚Å°{‚àífy‚Äã(w)}.p(y\mid w)\propto\exp\{-f\_{y}(w)\}. |  | (3) |

The budget constraint is enforced *exactly* at the portfolio
construction and rebalancing stages, but appears here as a soft penalty
to keep the prior separable.

### 2.2 Sparsity-inducing prior

We promote sparsity in ww through a weighted Laplace prior

|  |  |  |  |
| --- | --- | --- | --- |
|  | p‚Äã(w‚à£Œ∏)‚àùexp‚Å°(‚àíŒ∏‚Äã‚àëj=1pŒ±j‚Äã|wj|),Œ∏>0,p(w\mid\theta)\propto\exp\,\!\Bigl(-\theta\sum\_{j=1}^{p}\alpha\_{j}|w\_{j}|\Bigr),\qquad\theta>0, |  | (4) |

where Œ∏\theta is a global scale controlling shrinkage and
the weights Œ±j>0\alpha\_{j}>0 encode per-asset sensitivity.

Following ideas from regularised regression, we choose
per-coordinate scales based on the columns of RR, for example

|  |  |  |  |
| --- | --- | --- | --- |
|  | sj=‚ÄñR‚ãÖj‚Äñ2T,Œ±j=max‚Å°{sj,Œµ}1p‚Äã‚àëk=1pmax‚Å°{sk,Œµ},s\_{j}=\frac{\|R\_{\cdot j}\|\_{2}}{\sqrt{T}},\qquad\alpha\_{j}=\frac{\max\{s\_{j},\varepsilon\}}{\frac{1}{p}\sum\_{k=1}^{p}\max\{s\_{k},\varepsilon\}}, |  | (5) |

with a small Œµ>0\varepsilon>0 to avoid numerical issues.
This yields a prior penalty Œ∏‚Äã‚àëjŒ±j‚Äã|wj|\theta\sum\_{j}\alpha\_{j}|w\_{j}|
that is roughly balanced across coordinates.

Combining likelihood and prior, the unnormalised posterior reads

|  |  |  |  |
| --- | --- | --- | --- |
|  | œÄ‚Äã(w‚à£y,Œ∏)‚àùexp‚Å°{‚àífy‚Äã(w)‚àíŒ∏‚Äãg‚Äã(w)},g‚Äã(w)=‚àëj=1pŒ±j‚Äã|wj|.\pi(w\mid y,\theta)\propto\exp\{-f\_{y}(w)-\theta g(w)\},\qquad g(w)=\sum\_{j=1}^{p}\alpha\_{j}|w\_{j}|. |  | (6) |

### 2.3 Role and effect of the likelihood parameters

The design of the likelihood part, as reflected in ([2](https://arxiv.org/html/2512.22109v1#S2.E2 "Equation 2 ‚Ä£ 2.1 Regression view and soft budget constraint ‚Ä£ 2 Model considerations ‚Ä£ Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification")), involves the parameters Œõ\Lambda and œÉ2.\sigma^{2}.

Our feasible set is ùíû:={w‚àà‚Ñùp: 1p‚ä§‚Äãw=1},\mathcal{C}:=\{w\in\mathbb{R}^{p}:\ \mathbf{1}\_{p}^{\top}w=1\}, and one could incorporate this constraint by including the indicator function Œπùíû‚Äã(w)\iota\_{\mathcal{C}}(w) into the penalty/prior term. However, this creates complications in applying the SAPG algorithm of (?, ?). In short, the issue is that the constraint breaks the homogeneity of the Laplace prior (in a scaling sense, see the above work for more information). Akin to the constrained optimisation idea of using Lagrange multipliers, we incorporate the constraint via the term Œõ‚Äã(ùüè‚ä§‚Äãw‚àí1)2\Lambda\,(\mathbf{1}^{\top}w-1)^{2}, which will force near-satisfaction of the constraint.111We can tune it as desired: larger Œõ\Lambda tightens sampling along the ‚Äúbudget axis‚Äù and shifts the MAP toward budget fidelity. However, we saw rather insubstantial differences in our experiments, so in the end we decided to keep Œõ\Lambda fixed to avoid the need for excessive tuning, and to decouple it from the sparsity parameter selection.
We readjust at a later time by projecting the allocation vector to ùíû\mathcal{C} to satisfy the constraint exactly. We set Œõ=12‚ÄãœÑc2,\Lambda\;=\;\frac{1}{2\tau\_{c}^{2}}, with œÑc\tau\_{c} a fixed user-defined allowed deviation from the hard constraint. This way, we treat the budget as a soft constraint via a quadratic penalty and enforce it with a convex and smooth term. The penalty can then be interpreted as a Gaussian ‚Äúpseudo-measurement‚Äù on b=ùüè‚ä§‚Äãw‚àí1b=\mathbf{1}^{\top}w-1, making œÑc\tau\_{c} a target standard deviation of the budget residual.

The final consideration when it comes to the likelihood is the *œÉ2\sigma^{2} factor*. From a modelling viewpoint, this can be seen as the noise variance or as a goodness-of-fit indicator in the regression with the index and its constituent asset returns. Therefore, any regression approach would yield very small œÉ2\sigma^{2}-values (for us, of the order of 10‚àí910^{-9}, 10‚àí710^{-7} at best). The role of œÉ2\sigma^{2} is much more pronounced in the rebalancing stage, where we construct the new portfolio wneww\_{\text{new}} by using the previous one, woldw\_{\text{old}}, as a baseline and focus on the vector of modifications, Œî‚Äãw,\Delta w, which, once selected, will yield wnew=wold+Œî‚Äãw,‚ÄÑ1‚ä§‚ÄãŒî‚Äãw=0.w\_{\text{new}}=w\_{\text{old}}+\Delta w,\;\mathbf{1}^{\top}\Delta w=0. Given that the rebalancing window is treated as a new data block, the posterior sees woldw\_{\text{old}} merely as a prior guess, and appears to be quite willing to move away from it when the data suggest that a richer combination can reduce TE. From a rebalancing or feature‚Äìadjustment perspective, this is exactly the opposite of the desired narrative: we want the *old* solution to be treated as ‚Äúnearly optimal‚Äù, with only a small number of carefully chosen, posterior‚Äìjustified tweaks. This suggests that for rebalancing, it can be more meaningful to use a *deliberately larger* œÉŒî‚Äãw2\sigma^{2}\_{\Delta w} that encodes the idea that pushing TE below a certain threshold is not worth additional turnover. Our method for this tuning is explained in Section [6](https://arxiv.org/html/2512.22109v1#S6 "6 Rebalancing ‚Ä£ Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification").

From a *computational* viewpoint, the effect of both parameters is reflected in the MCMC timestep. For stability, the timesteps we are allowed to use have to be smaller than 1/Lf,1/L\_{f}, where

|  |  |  |
| --- | --- | --- |
|  | Lf=1œÉ2‚ÄãŒªmax‚Äã(RT‚ÄãR+2‚ÄãŒõ‚Äãùüèùüè‚ä§).L\_{f}=\frac{1}{\sigma^{2}}\,\lambda\_{\max}(R^{T}R+2\Lambda\mathbf{1}\mathbf{1}^{\top}). |  |

Here LfL\_{f} is the Lipschitz constant from the likelihood part. It is an easy calculation to verify that when the soft constraint is absent from the likelihood, Lf‚Ä≤=1œÉ2‚ÄãŒªmax‚Äã(RT‚ÄãR),L\_{f}^{\prime}=\frac{1}{\sigma^{2}}\,\lambda\_{\max}(R^{T}R), where Œªm‚Äãa‚Äãx\lambda\_{max} is the largest eigenvalue of the RT‚ÄãRR^{T}R matrix. In contrast, adding the quadratic soft-constraint term in the likelihood results in Lf=1œÉ2‚ÄãŒªmax‚Äã(RT‚ÄãR+2‚ÄãŒõ‚Äãùüèùüè‚ä§).L\_{f}=\frac{1}{\sigma^{2}}\,\lambda\_{\max}(R^{T}R+2\Lambda\mathbf{1}\mathbf{1}^{\top}). It turns out that, at least in the data sets we explored, the quadratic budget penalty adds a relatively small amount to the dominant eigenvalue. The main driver of stiffness is the combination of a very small œÉ2\sigma^{2}, and to a lesser extent, the spectrum of R‚ä§‚ÄãRR^{\top}R, rather than Œõ\Lambda itself. It is exactly the smallness of œÉ2\sigma^{2} that results in a very small timestep and slow mixing. We resort to preconditioning to counteract these issues.

## 3 Proximal MCMC for the posterior

(Theoretical and technical details, as well as implementation specifics, are provided in the Appendix.)

### 3.1 Moreau‚ÄìYosida smoothing and proximal gradient structure

The posterior œÄ‚Äã(w‚à£y,Œ∏‚ãÜ)\pi(w\mid y,\theta\_{\star}) combines a smooth quadratic
term fyf\_{y} with a convex but nonsmooth ‚Ñì1\ell\_{1} term. Proximal MCMC
methods exploit this structure by replacing gg with its (differentiable) Moreau‚ÄìYosida envelope gŒªg\_{\lambda},

|  |  |  |  |
| --- | --- | --- | --- |
|  | gŒª‚Äã(w)=minu‚àà‚Ñùp‚Å°{g‚Äã(u)+12‚ÄãŒª‚Äã‚Äñu‚àíw‚Äñ22},for a chosen‚ÄãŒª>0g\_{\lambda}(w)=\min\_{u\in\mathbb{R}^{p}}\Bigl\{g(u)+\frac{1}{2\lambda}\|u-w\|\_{2}^{2}\Bigr\},\quad\text{for a chosen}\;\lambda>0 |  | (7) |

whose gradient is given by

|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚àágŒª‚Äã(w)=1Œª‚Äã(w‚àíproxŒª‚Äãg‚Å°(w)),\nabla g\_{\lambda}(w)=\frac{1}{\lambda}\Bigl(w-\operatorname{prox}\_{\lambda g}(w)\Bigr), |  | (8) |

with proxŒª‚Äãg\operatorname{prox}\_{\lambda g} the proximal mapping of gg.
For weighted ‚Ñì1\ell\_{1} penalties, the prox reduces to componentwise soft-thresholding. Interest in these convex analysis tools originated from the popularity of non-smooth regularisers in applications to optimisation and statistical learning. It turns out that many of these regularisers have unique proximal maps that either have explicit formulas (as our example above) or can be computed efficiently (?, ?).

We work with a smoothed potential

|  |  |  |  |
| --- | --- | --- | --- |
|  | Œ¶Œª‚Äã(w)=fy‚Äã(w)+Œ∏‚ãÜ‚ÄãgŒª‚Äã(w),\Phi\_{\lambda}(w)=f\_{y}(w)+\theta\_{\star}g\_{\lambda}(w), |  | (9) |

whose gradient is Lipschitz continuous with constant LfL\_{f} that can be
bounded in terms of R‚ä§‚ÄãRR^{\top}R, the budget penalty Œõ\Lambda and Œª\lambda.
This allows Langevin-type discretisations with principled step-size choices.

### 3.2 MYULA and preconditioned MALA

The Moreau‚ÄìYosida Unadjusted Langevin Algorithm (MYULA) (e.g., (?, ?, ?, ?, ?)) targets an
approximation œÄŒª‚Äã(w‚à£y)\pi\_{\lambda}(w\mid y) to the posterior by iterating

|  |  |  |  |
| --- | --- | --- | --- |
|  | w(k+1)=w(k)‚àíŒ¥‚Äã‚àáŒ¶Œª‚Äã(w(k))+2‚ÄãŒ¥‚ÄãŒæ(k),Œæ(k)‚àºùí©‚Äã(0,Ip),w^{(k+1)}=w^{(k)}-\delta\nabla\Phi\_{\lambda}(w^{(k)})+\sqrt{2\delta}\,\xi^{(k)},\qquad\xi^{(k)}\sim\mathcal{N}(0,I\_{p}), |  | (10) |

with step size Œ¥>0\delta>0. For a fixed Œ∏‚ãÜ,\theta\_{\star}, the smoothed gradient for Œ∏‚ãÜ‚Äãg\theta\_{\star}g is

|  |  |  |
| --- | --- | --- |
|  | ‚àáŒ¶Œª,Œ∏‚ãÜ=‚àáfy‚Äã(w)+1Œª‚Äã(w‚àíproxŒª‚ÄãŒ∏‚ãÜ‚Äãg‚Å°(w)),\nabla\Phi\_{\lambda,\,\theta\_{\star}}=\nabla f\_{y}(w)+\frac{1}{\lambda}\bigl(w-\operatorname{prox}\_{{\lambda\,\theta\_{\star}}g}(w)\bigr), |  |

We choose Œ¥=0.9/(2‚ÄãLf)\delta=0.9/(2L\_{f}) and
Œª=1/Lf\lambda=1/L\_{f}, which provides a good compromise between stability
and mixing in our experiments. In this work, MYULA is used both as an inner kernel
within SAPG and as a fast approximate sampler for exploratory runs.

To obtain higher-quality samples for reporting and UQ, we use a
preconditioned Metropolis-adjusted Langevin algorithm (MALA) targeting
œÄŒª‚Äã(w‚à£y)\pi\_{\lambda}(w\mid y) more accurately. The proposal takes the form

|  |  |  |  |
| --- | --- | --- | --- |
|  | w‚Ä≤=w‚àíŒ¥‚ÄãP2‚Äã‚àáŒ¶Œª‚Äã(w)+2‚ÄãŒ¥‚ÄãP‚ÄãŒæ,Œæ‚àºùí©‚Äã(0,Ip),w^{\prime}=w-\delta P^{2}\nabla\Phi\_{\lambda}(w)+\sqrt{2\delta}\,P\xi,\qquad\xi\sim\mathcal{N}(0,I\_{p}), |  | (11) |

where PP is a Jacobi preconditioner built from the diagonal of
R‚ä§‚ÄãR/œÉ2R^{\top}R/\sigma^{2} (rescaled to balance coordinates), given by

|  |  |  |
| --- | --- | --- |
|  | P=D‚àí1/2,where,D=diag‚Å°(1œÉ2‚Äãdiag‚Å°(R‚ä§‚ÄãR)+‚ÄÑ2‚ÄãŒõ‚Äãp).P=D^{-1/2},\quad\text{where,}\quad D=\operatorname{diag}\!\Big(\frac{1}{\sigma^{2}}\operatorname{diag}(R^{\top}R)\;+\;2\Lambda\,p\Big). |  |

that can equalise the curvature across coordinates (significantly improving mixing) without introducing too much complexity. In our experiments, the use of this preconditioning shrank the workable Lipschitz constant from ùí™‚Äã(109)\mathcal{O}(10^{9}) to ùí™‚Äã(102)\mathcal{O}(10^{2}), allowing proposal steps
Œ¥‚âà0.9/(2‚ÄãLpre)\delta\approx 0.9/(2L\_{\mathrm{pre}}) in the 10‚àí310^{-3}‚Äì10‚àí210^{-2} range instead of 10‚àí1110^{-11}.

To target bias, we complement the above with a MH correction using the usual Gaussian forward/backward densities. We use a short tuning phase to adjust Œ¥\delta to yield an acceptance rate around 0.60,0.60, which is considered desirable.

### 3.3 Empirical-Bayes calibration of the sparsity parameter

The global scale Œ∏\theta in ([4](https://arxiv.org/html/2512.22109v1#S2.E4 "Equation 4 ‚Ä£ 2.2 Sparsity-inducing prior ‚Ä£ 2 Model considerations ‚Ä£ Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification")) controls sparsity and is crucial for the trade-off between TE and the number of active assets. Rather than fixing Œ∏\theta in ad hoc fashion, we adopt an empirical-Bayes approach and automatically estimate Œ∏‚ãÜ\theta\_{\star} by maximising the marginal likelihood p‚Äã(y‚à£Œ∏)p(y\mid\theta).

Direct optimisation of p‚Äã(y‚à£Œ∏)p(y\mid\theta) is intractable because of the
Laplace prior in the presence of the budget constraint, and the high-dimensional integral. We therefore apply
the stochastic approximation proximal gradient (SAPG) scheme, developed
in (?, ?), to iteratively update Œ∏\theta using Monte Carlo
estimates of the gradient of the log-marginal likelihood.

The SAPG algorithm is summarised in [AlgorithmÀú1](https://arxiv.org/html/2512.22109v1#alg1 "In 3.3 Empirical-Bayes calibration of the sparsity parameter ‚Ä£ 3 Proximal MCMC for the posterior ‚Ä£ Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification"). At iteration kk,
we generate a short Markov chain targeting (an approximation of)
œÄ(‚ãÖ‚à£y,Œ∏k)\pi(\cdot\mid y,\theta\_{k}) and use the resulting sample to build a noisy
estimate of the gradient with respect to Œ∑k\eta\_{k}. A Robbins‚ÄìMonro
step-size schedule drives the updates, and a weighted average over
the tail of the run yields the empirical-Bayes estimate Œ∏‚ãÜ\theta\_{\star}.

Algorithm 1  Empirical-Bayes calibration of Œ∏\theta via SAPG (schematic)

1:Choose initial Œ∏0>0\theta\_{0}>0 from a simple moment-matching rule and bounds

2:0<Œ∏min‚â§Œ∏max0<\theta\_{\min}\leq\theta\_{\max}; set Œ∑0=log‚Å°Œ∏0\eta\_{0}=\log\theta\_{0}.

3:for k=0,1,‚Ä¶,Kk=0,1,\dots,K do

4:‚ÄÉ‚ÄÇRun a short MCMC chain targeting œÄ(‚ãÖ‚à£y,Œ∏k)\pi(\cdot\mid y,\theta\_{k}), and obtain a sample
w(1),‚Ä¶,w(mk)w^{(1)},\dots,w^{(m\_{k})}.

5:‚ÄÉ‚ÄÇForm a Monte Carlo estimate of the gradient
Œîk‚âà‚àÇŒ∑log‚Å°p‚Äã(y‚à£Œ∏)|Œ∏=Œ∏k\Delta\_{k}\approx\partial\_{\eta}\log p(y\mid\theta)\big|\_{\theta=\theta\_{k}}.

6:‚ÄÉ‚ÄÇUpdate

|  |  |  |
| --- | --- | --- |
|  | Œ∑k+1=Œ†[log‚Å°Œ∏min,log‚Å°Œ∏max]‚Äã{Œ∑k+œÅk‚ÄãŒîk},œÅk=ck+k0,\eta\_{k+1}=\Pi\_{[\log\theta\_{\min},\log\theta\_{\max}]}\bigl\{\eta\_{k}+\rho\_{k}\Delta\_{k}\bigr\},\qquad\rho\_{k}=\frac{c}{k+k\_{0}}, |  |

and set Œ∏k+1=exp‚Å°(Œ∑k+1)\theta\_{k+1}=\exp(\eta\_{k+1}).

7:end for

8:Return a Polyak‚ÄìRuppert average of the iterates as Œ∏‚ãÜ\theta\_{\star}.

In more detail:

##### Heuristic initialisation.

In theory, the choice of the initial Œ∏0\theta\_{0} for the SAPG iteration will not matter (asymptotically), but in practice, we saw that this wasn‚Äôt always the case. For this reason, based on an initial least-squares solution and a moment-matching argument we defined, for a small œµ>0,\epsilon>0,

|  |  |  |
| --- | --- | --- |
|  | Œ∏0=max‚Å°{p‚àëj=1pŒ±j‚Äã|(wr‚Äãe‚Äãf)j|,œµ},\theta\_{0}=\max\!\left\{\frac{p}{\sum\_{j=1}^{p}\alpha\_{j}|(w\_{ref})\_{j}|},\,\epsilon\right\}, |  |

using a reference solution wr‚Äãe‚Äãfw\_{ref} (see Appendix [A.1](https://arxiv.org/html/2512.22109v1#A1.SS1 "A.1 Heuristic choice of the initial scale ùúÉ‚ÇÄ ‚Ä£ Appendix A Stochastic Approximation Proximal Gradient algorithm (SAPG) ‚Ä£ Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification") for details).
We define Œò=[Œ∏0/10,10‚ÄãŒ∏0]\Theta=[\theta\_{0}/10,10\,\theta\_{0}] and enforce Œ∏‚ààŒò\theta\in\Theta to prevent numerically extreme values.

##### MYULA kernel for ww.

For a fixed Œ∏\theta, we approximate the posterior p‚Äã(w‚à£Œ∏)p(w\mid\theta) using the MYULA kernel on a Moreau‚ÄìYosida smoothed potential. We define the weighted proximal map (soft-thresholding)

|  |  |  |
| --- | --- | --- |
|  | proxŒªMY‚ÄãŒ∏(w)j=sign(wj)max{|wj|‚àíŒªMYŒ∏Œ±j,‚Äâ0}.\operatorname{prox}\_{\lambda\_{\mathrm{MY}}\,\theta}(w)\_{j}=\operatorname{sign}(w\_{j})\,\max\bigl\{|w\_{j}|-\lambda\_{\mathrm{MY}}\,\theta\,\alpha\_{j},\,0\bigr\}. |  |

The MYULA step used inside SAPG is

|  |  |  |
| --- | --- | --- |
|  | w(k+1)=w(k)+Œ¥MYULA‚Äã(‚àí‚àáfy‚Äã(w(k))‚àí1ŒªMY‚Äã(w(k)‚àíproxŒªMY‚ÄãŒ∏‚Å°(w(k))))+2‚ÄãŒ¥MYULA‚ÄãŒæ(k),w^{(k+1)}=w^{(k)}+\delta\_{\mathrm{MYULA}}\Bigl(-\nabla f\_{y}(w^{(k)})-\tfrac{1}{\lambda\_{\mathrm{MY}}}(w^{(k)}-\operatorname{prox}\_{\lambda\_{\mathrm{MY}}\theta}(w^{(k)}))\Bigr)+\sqrt{2\delta\_{\mathrm{MYULA}}}\,\xi^{(k)}, |  |

with Œæ(k)‚àºùí©‚Äã(0,Ip)\xi^{(k)}\sim\mathcal{N}(0,I\_{p}).

We warm-start the chain with Œ∏=Œ∏0\theta=\theta\_{0} before the SAPG updates.

##### SAPG update for Œ∏\theta.

Let

|  |  |  |
| --- | --- | --- |
|  | g‚Äã(w)=‚àëj=1pŒ±j‚Äã|wj|g(w)=\sum\_{j=1}^{p}\alpha\_{j}|w\_{j}| |  |

be the (scaled) ‚Ñì1\ell\_{1} mass. We work on a logarithmic scale for the updates, and set Œ∑=log‚Å°Œ∏\eta=\log\theta:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Œ∑k+1=Œ†[log‚Å°Œ∏min,log‚Å°Œ∏max]‚Äã(Œ∑k+œÅk‚Äã(p‚àíŒ∏k‚ÄãS‚Äã(w(k)))),Œ∏k=eŒ∑k,\eta\_{k+1}=\Pi\_{[\log\theta\_{\min},\log\theta\_{\max}]}\Bigl(\eta\_{k}+\rho\_{k}(p-\theta\_{k}S(w^{(k)}))\Bigr),\qquad\theta\_{k}=e^{\eta\_{k}}, |  | (12) |

with step‚Äìsize schedule œÅk=c/(k+k0)\rho\_{k}=c/(k+k\_{0}). The chain {w(k)}\{w^{(k)}\} is driven by the MYULA kernel described above with the current Œ∏k\theta\_{k}.

After a burn‚Äìin of kburnk\_{\texttt{burn}} iterations, we compute a Polyak‚ÄìRuppert weighted average

|  |  |  |  |
| --- | --- | --- | --- |
|  | Œ∑¬Ø=‚àëk>kburnœâk‚ÄãŒ∑k‚àëk>kburnœâk,œâk‚àù(k‚àíkburn)q,\bar{\eta}=\displaystyle\frac{\sum\_{k>k\_{\mathrm{burn}}}\omega\_{k}\,\eta\_{k}}{\sum\_{k>k\_{\mathrm{burn}}}\omega\_{k}},\qquad\omega\_{k}\propto(k-k\_{\mathrm{burn}})^{q}, |  | (13) |

and define the empirical‚ÄìBayes estimate

|  |  |  |
| --- | --- | --- |
|  | Œ∏‚ãÜ=eŒ∑¬Ø.\theta\_{\star}=e^{\bar{\eta}}. |  |

Once Œ∏‚ãÜ\theta\_{\star} has been obtained, the posterior for ww becomes

|  |  |  |
| --- | --- | --- |
|  | œÄ‚Äã(w‚à£y):=œÄ‚Äã(w‚à£y,Œ∏‚ãÜ),\pi(w\mid y):=\pi(w\mid y,\theta\_{\star}), |  |

and all subsequent steps (posterior sampling, support selection,
rebalancing) are conditional on this empirical-Bayes choice.

## 4 Posterior‚Äìinformed support selection

Unsurprisingly, given the choice of the prior, in our case studies, the MAP estimator has a significant number of very small (both positive and negative) but nonzero weights. From a practical point of view, an investor clearly would not want to take too many small positions, especially with no guarantees that they are indispensable for a low TE. We therefore seek to determine cut-off threshold(s) to determine which nonzero weights will automatically be set to zero. This applies to the process for designing both the original and the rebalanced portfolio.

### 4.1 Decoupling shrinkage from selection

Raw cardinality (the number of nonzero elements in the MAP) can overstate ‚Äúeffective‚Äù exposure when many coefficients are extremely small, but not exactly zero. Regardless of what prior is used, to go from the posterior to a sparse (enough) point estimate will typically require additional post-processing. As the authors of (?, ?) remark, thresholding rules provide an imperfect tool for cases where continuous (non-point mass) priors are placed on the regressors. We embrace this philosophy, also reflected in works on ‚Äúprojective inference‚Äù (e.g., see (?, ?) and later research). Essentially, a possibly not-extremely-sparse model that predicts well is constructed first, and then one proceeds (e.g., by either thresholding rules or ‚Äúprojection onto submodels‚Äù) by finding a sparser subset of the features that will characterise the predictions.

In our pipeline, we too decouple shrinkage and selection. First, we use the weighted-Laplace prior for sparsity, and then we allow the data to inform the selection of the parameter controlling the amount of sparsity enforced. From a Decision Theory point of view, this is equivalent to Bayesian model selection. Then, we craft decision rules for selection, based on UQ metrics obtained from sampling with this model. This allows us to optimise out-of-sample predictive scores under a size penalty in a principled, posterior-informed way.

### 4.2 Effective support

We now show how the results from the long MALA run can inform the formation of the holdable portfolio, and we do this by developing two tools to complement the MAP cardinality information. Unless otherwise specified, the same process applies to rebalancing, but we demonstrate the approach using the notation for the first portfolio.

#### 4.2.1 A noise-floor threshold

The first *effective sparsity* measure is based on a ‚Äúnoise-floor threshold‚Äù, œÑpost,\tau\_{\mathrm{post}}, based on which we will prune the very small MAP weights, by only keeping

|  |  |  |
| --- | --- | --- |
|  | SMAP={j:|wMAP,j|‚â•œÑpost}.S\_{\mathrm{MAP}}=\{j:|w\_{\mathrm{MAP},j}|\geq\tau\_{\mathrm{post}}\}. |  |

From the MALA samples, we estimate per-coordinate posterior standard deviations

|  |  |  |
| --- | --- | --- |
|  | s^j=sd‚Å°(wj(m)),j=1,‚Ä¶,p,\hat{s}\_{j}=\operatorname{sd}\bigl(w^{(m)}\_{j}\bigr),\qquad j=1,\dots,p, |  |

and define the posterior scale threshold

|  |  |  |  |
| --- | --- | --- | --- |
|  | œÑpost=k‚ãÖmedianjs^j\tau\_{\mathrm{post}}=k\cdot\operatorname\*{median}\_{j}\hat{s}\_{j} |  | (14) |

with a default k=2.5k=2.5.

This is in the same spirit as posterior-median thresholding/spike-and-slab rules (e.g., (?, ?)), where ‚Äúeffectively zero‚Äù is defined in terms of the posterior distribution, rather than arbitrary absolute cutoffs (e.g., posterior median shrinks small coefficients to zero and leaves big ones unchanged.)

In this case, ([14](https://arxiv.org/html/2512.22109v1#S4.E14 "Equation 14 ‚Ä£ 4.2.1 A noise-floor threshold ‚Ä£ 4.2 Effective support ‚Ä£ 4 Posterior‚Äìinformed support selection ‚Ä£ Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification")) provides a robust way of estimating the ‚Äútypical‚Äù posterior scale directly from the sampler, and reports it in ‚Äúsigma‚Äù units. The decision to report a single, global œÑpost,\tau\_{\mathrm{post}}, rather than one per coordinate, is mostly for simplicity: notice that our choice ensures a monotone, with-a-single-parameter rule of controlling sparsity. Increasing kk always reduces the cardinality of the support, SS. Choosing k‚âà2.5k\approx 2.5 is more conservative than a loose 2‚ÄãœÉ2\sigma rule (to avoid spurious tiny positions), but not as extreme as 3‚ÄãœÉ3\sigma, which tends to prune more aggressively.

#### 4.2.2 Activation probabilities

We see œÑpost\tau\_{\mathrm{post}} as an indicator of what is ‚Äúmeaningfully non-zero‚Äù (relative to the posterior noise). Motivated to explicitly incorporate uncertainty quantification, we also compute ‚Äúactivation‚Äù probabilities

|  |  |  |  |
| --- | --- | --- | --- |
|  | œÄ^j=Pr‚Å°(|wj|‚â•œÑpost‚à£data)‚âà1M‚Äã‚àëm=1Mùüè‚Äã{|wj(m)|‚â•œÑpost}.\hat{\pi}\_{j}=\Pr\bigl(|w\_{j}|\geq\tau\_{\mathrm{post}}\mid\text{data}\bigr)\approx\frac{1}{M}\sum\_{m=1}^{M}\mathbf{1}\{|w^{(m)}\_{j}|\geq\tau\_{\mathrm{post}}\}. |  | (15) |

Unlike spike‚Äìslab formulations with an explicit inclusion indicator
Œ≥j\gamma\_{j} and posterior inclusion probabilities P‚Äã(Œ≥j=1‚à£y)P(\gamma\_{j}=1\mid y), our
Laplace/MYULA setup works with a fully continuous posterior on the weights.

Metric ([15](https://arxiv.org/html/2512.22109v1#S4.E15 "Equation 15 ‚Ä£ 4.2.2 Activation probabilities ‚Ä£ 4.2 Effective support ‚Ä£ 4 Posterior‚Äìinformed support selection ‚Ä£ Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification")) can be viewed as an inclusion probability
for the event ‚Äúcoefficient jj is meaningfully non-zero‚Äù, in the sense that its magnitude exceeds a data-adaptive multiple of the typical posterior standard deviation. This is analogous
in spirit to classical posterior inclusion probabilities (PIP), but with the ‚Äúinclusion event‚Äù defined via a posterior scale threshold rather than a latent spike at zero.

#### 4.2.3 Selection rule

Our selection rule combines the above two tools, i.e., a *magnitude constraint*

|  |  |  |
| --- | --- | --- |
|  | |wjMAP|‚â•œÑpost|w\_{j}^{\mathrm{MAP}}|\geq\tau\_{\mathrm{post}} |  |

with a *certainty constraint*

|  |  |  |
| --- | --- | --- |
|  | P‚Äã(|wj|‚â•œÑpost‚à£y)‚â•œÄ‚àó,P\bigl(|w\_{j}|\geq\tau\_{\mathrm{post}}\mid y\bigr)\geq\pi^{\ast}, |  |

for some œÄ‚àó‚àà(0,1).\pi^{\ast}\in(0,1).

For a user‚Äìchosen activation threshold œÄ‚ãÜ\pi^{\star}, we define:

|  |  |  |  |
| --- | --- | --- | --- |
|  | SUQ={j:|wMAP,j|‚â•œÑpost,œÄ^j‚â•œÄ‚ãÜ}.S\_{\mathrm{UQ}}=\{j:|w\_{\mathrm{MAP},j}|\geq\tau\_{\mathrm{post}},\;\hat{\pi}\_{j}\geq\pi^{\star}\}. |  | (16) |

Thus, the selected support consists of
*weights that are both large enough (in sigma units) and have high
posterior evidence for being that large*. This is in line with the
use of posterior quantities such as local false sign or discovery
rates for decision-making (see, e.g., (?, ?) for an empirical-Bayes treatment of such quantities in large-scale testing).

### 4.3 Short positions

In our baseline experiments, we ultimately implement long‚Äêonly tracking portfolios, even though the Bayesian machinery is formulated on the unconstrained long‚Äìshort space (via the choice of the symmetric Laplace prior). The long-only decision reflects the practical mandate and operational reality of plain index trackers: maintaining short positions requires margin, additional broker arrangements and monitoring, which are rarely justifiable for passive, infrequently rebalanced products. We therefore use the full long‚Äìshort posterior only as a statistical engine‚Äîto estimate activation probabilities, posterior scales and sign probabilities‚Äîand then apply a simple long‚Äêonly decision rule at the end: negative weights are set to zero (or, in the *rebalancing step* where negative adjustments are acceptable, any proposed move that would cross through zero is truncated at zero); we then restore the budget over the remaining active names. Note that our original machinery, which allows shorts, can be beneficial both in applications of *enhanced index tracking* and other portfolio optimisation problems.

## 5 Construction of a tradeable portfolio

In this section, we elaborate on our method for designing a tracking portfolio for the first trading period. We remark that our method is general enough to apply to other sparse feature selection problems practically as is.

##### Centering and scaling

To avoid having to include an intercept (and for numerical stability), we centre both index and regressors on the estimation window (which we refer to as the FIT‚Äì1 period):

|  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- |
|  | yŒº\displaystyle y\_{\mu} | =1T‚Äã‚àët=1Tyt,\displaystyle=\frac{1}{T}\sum\_{t=1}^{T}y\_{t}, | RŒº\displaystyle R\_{\mu} | ‚àà‚Ñùp,(RŒº)j=1T‚Äã‚àët=1TRt‚Äãj,\displaystyle\in\mathbb{R}^{p},\quad(R\_{\mu})\_{j}=\frac{1}{T}\sum\_{t=1}^{T}R\_{tj}, |  |
|  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- |
|  | yc\displaystyle y\_{c} | =y‚àíyŒº‚ÄãùüèT,\displaystyle=y-y\_{\mu}\mathbf{1}\_{T}, | Rc\displaystyle R\_{c} | =Rfit‚àíùüèT‚ÄãRŒº‚ä§,\displaystyle=R\_{\mathrm{fit}}-\mathbf{1}\_{T}R\_{\mu}^{\top}, |  |

where TT is the length of the FIT‚Äì1 window. We then work with the centred Gaussian likelihood,

|  |  |  |
| --- | --- | --- |
|  | yc‚à£w‚àºùí©‚Äã(Rc‚Äãw,œÉ2‚ÄãIT).y\_{c}\mid w\sim\mathcal{N}(R\_{c}w,\;\sigma^{2}I\_{T}). |  |

where œÉ2\sigma^{2} is the noise variance estimated on (yc,Rc).(y\_{c},R\_{c}).

### 5.1 Noise variance estimation

We start by pre-estimating (see Appendix [C](https://arxiv.org/html/2512.22109v1#A3 "Appendix C Noise variance estimation ‚Ä£ Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification")) the noise variance œÉ2\sigma^{2}, which appears as a parameter in the likelihood. We tested several estimators that operate on the residuals of the regression model, and we chose the *median absolute deviation* (MAD) estimator (?, ?) because of its robustness to outliers; we report œÉ^2=œÉ^MAD2.\widehat{\sigma}^{2}\;=\;\widehat{\sigma}\_{\mathrm{MAD}}^{2}. We note that the budget constraint
ùüè‚ä§‚Äãw=1\mathbf{1}^{\top}w=1
*does not enter* the variance estimation at all: we work with the unconstrained OLS fit and its residuals. The rationale is that
œÉ^2\widehat{\sigma}^{2} should reflect the scale of the *tracking
error* yt‚àírt‚ä§‚Äãwy\_{t}-r\_{t}^{\top}w under a purely data-driven fit, without being distorted by how we choose to enforce the budget constraint. The constraint is imposed later, at the level of the prior
and posterior geometry, but not in the noise-scale estimation stage.

### 5.2 The MAP estimator

Having estimated œÉ^2=œÉ^MAD2,\widehat{\sigma}^{2}\;=\;\widehat{\sigma}\_{\mathrm{MAD}}^{2}, we are in a position to calculate the timestep for the MCMC chain ([10](https://arxiv.org/html/2512.22109v1#S3.E10 "Equation 10 ‚Ä£ 3.2 MYULA and preconditioned MALA ‚Ä£ 3 Proximal MCMC for the posterior ‚Ä£ Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification")), and employ the SAPG algorithm described in subsection [3.3](https://arxiv.org/html/2512.22109v1#S3.SS3 "3.3 Empirical-Bayes calibration of the sparsity parameter ‚Ä£ 3 Proximal MCMC for the posterior ‚Ä£ Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification").

Given Œ∏‚ãÜ,\theta\_{\star}, the output of SAPG, we first
compute a maximum a posteriori (MAP) estimator of ww,

|  |  |  |  |
| --- | --- | --- | --- |
|  | w^MAP‚ààarg‚Äãminw‚àà‚Ñùp‚Å°{fy‚Äã(w)+Œ∏‚ãÜ‚Äãg‚Äã(w)},\hat{w}\_{\text{MAP}}\in\operatorname\*{arg\,min}\_{w\in\mathbb{R}^{p}}\bigl\{f\_{y}(w)+\theta\_{\star}g(w)\bigr\}, |  | (17) |

using FISTA (?, ?). The smooth part fyf\_{y} is treated
by a gradient step with step size 1/Lf1/L\_{f}, while gg is handled via the
weighted soft-thresholding prox. The algorithm stops when the objective difference falls below a relative tolerance threshold. This yields a dense but heavily shrunk vector of weights.

### 5.3 Long MALA run

For posterior summaries, we run a long Markov chain with Œ∏‚ãÜ\theta\_{\star} fixed, targeting the MY smoothed posterior

|  |  |  |
| --- | --- | --- |
|  | œÄ~‚Äã(w)‚àùexp‚Å°(‚àíŒ¶Œª,Œ∏‚ãÜ‚Äã(w)),\tilde{\pi}(w)\propto\exp\bigl(-\Phi\_{\lambda,\,\theta\_{\star}}(w)\bigr), |  |

as described in Subsection [3.2](https://arxiv.org/html/2512.22109v1#S3.SS2 "3.2 MYULA and preconditioned MALA ‚Ä£ 3 Proximal MCMC for the posterior ‚Ä£ Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification"). In particular, we run a long MALA chain with the preconditioned kernel starting from wMAPw\_{\mathrm{MAP}} and, after discarding the samples of a burn-in stage, we form

|  |  |  |
| --- | --- | --- |
|  | {w(m)}m=1M‚â°Wlong‚àà‚ÑùM√óp.\{w^{(m)}\}\_{m=1}^{M}\equiv W\_{\mathrm{long}}\in\mathbb{R}^{M\times p}. |  |

During this stage, we monitor the trace *log‚Å°œÄ‚Äã(w)=Œ¶Œª,Œ∏‚ãÜ‚Äã(w)\log\pi(w)=\Phi\_{\lambda,\theta\_{\star}}(w)* for stationarity, and also the ACF decay and effective sample sizes (ESS) for selected coordinates as indicators of the quality of mixing.

### 5.4 Posterior-informed support selection and the tradeable portfolio

Recall from Section [4.2.3](https://arxiv.org/html/2512.22109v1#S4.SS2.SSS3 "4.2.3 Selection rule ‚Ä£ 4.2 Effective support ‚Ä£ 4 Posterior‚Äìinformed support selection ‚Ä£ Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification") that our selection rule combines two tools, i.e., a *magnitude constraint*

|  |  |  |
| --- | --- | --- |
|  | |wjMAP|‚â•œÑpost|w\_{j}^{\mathrm{MAP}}|\geq\tau\_{\mathrm{post}} |  |

with a *certainty constraint*

|  |  |  |
| --- | --- | --- |
|  | P‚Äã(|wj|‚â•œÑpost‚à£y)‚â•œÄ‚àó,P\bigl(|w\_{j}|\geq\tau\_{\mathrm{post}}\mid y\bigr)\geq\pi^{\ast}, |  |

for some œÄ‚àó‚àà(0,1)\pi^{\ast}\in(0,1). For a chosen activation threshold œÄ‚ãÜ\pi^{\star}, we collect the indices of the kept weights in the set

|  |  |  |  |
| --- | --- | --- | --- |
|  | S0={j:|wMAP,j|‚â•œÑpost,œÄ^j‚â•œÄ‚ãÜ}.S\_{0}=\{j:|w\_{\mathrm{MAP},j}|\geq\tau\_{\mathrm{post}},\;\hat{\pi}\_{j}\geq\pi^{\star}\}. |  | (18) |

##### Long-only refinement.

For passive index tracking with infrequent rebalancing, we prefer to avoid explicit short positions in the *inception* portfolio even if the underlying prior is symmetric. To this end, we refine the set S0S\_{0} to a long-only active set

|  |  |  |  |
| --- | --- | --- | --- |
|  | S={j:|wMAP,j|‚â•œÑpost,œÄ^j‚â•œÄ‚ãÜ;wj‚â•0}.S=\{j:|w\_{\mathrm{MAP},j}|\geq\tau\_{\mathrm{post}},\;\hat{\pi}\_{j}\geq\pi^{\star};w\_{j}\geq 0\}. |  | (19) |

### Construction of a tradeable portfolio

We refer to the weights of the assets corresponding to the above SS by

|  |  |  |
| --- | --- | --- |
|  | (wpruned)j={(wMAP)j,j‚ààS,0,j‚àâS.(w\_{\mathrm{pruned}})\_{j}=\begin{cases}(w\_{\mathrm{MAP}})\_{j},&j\in S,\\[1.99997pt] 0,&j\notin S.\end{cases} |  |

In what follows, Rc,SR\_{c,S} is the submatrix of RcR\_{c} with columns in SS.

Based on these weights, one can form and invest in several sparse portfolios; some examples are given below:

1. (a)

   Pruned portfolio with budget projection.
   We project wprunedw\_{\mathrm{pruned}} back to the budget hyperplane by an equal shift222While this is not the only option, note that we don‚Äôt want to simply project wprunedw\_{\mathrm{pruned}} onto the constraint set as this reintroduces non-zero entries in coordinates that were found to be (or made) zero.
   on SS:

   |  |  |  |  |
   | --- | --- | --- | --- |
   |  | wpruned,proj,j={(wpruned)j+Œ¥,j‚ààS,0,j‚àâS,Œ¥=1‚àí‚àëj‚ààS(wpruned)j|S|.w\_{\mathrm{pruned,proj},j}=\begin{cases}(w\_{\mathrm{pruned}})\_{j}+\delta,&j\in S,\\[1.99997pt] 0,&j\notin S,\end{cases}\qquad\delta=\frac{1-\sum\_{j\in S}(w\_{\mathrm{pruned}})\_{j}}{|S|}. |  | (20) |
2. (b)

   Refitting on SS with FISTA - (this is the one we will hold)

   To allow the penalty to reshape the weights on SS, we re‚Äìsolve the MAP problem restricted to SS. However, even on the long-only SS FISTA can still produce negative entries, unless we explicitly enforce wj‚â•0.w\_{j}\geq 0. So we now solve,

   |  |  |  |  |
   | --- | --- | --- | --- |
   |  | minwS‚àà‚Ñù|S|‚Å°12‚ÄãœÉ2‚Äã‚Äñyc‚àíRc,S‚ÄãwS‚Äñ22+Œõ‚Äã(1‚ä§‚ÄãwS‚àí1)2+Œ∏‚ãÜ‚Äã‚àëj‚ààSŒ±j‚Äã|(wS)j|+Œπw‚â•0‚Äã(w),\min\_{w\_{S}\in\mathbb{R}^{|S|}}\frac{1}{2\sigma^{2}}\,\|y\_{c}-R\_{c,S}w\_{S}\|\_{2}^{2}+\Lambda(1^{\top}w\_{S}-1)^{2}+\theta\_{\star}\sum\_{j\in S}\alpha\_{j}|(w\_{S})\_{j}|+\iota\_{w\geq 0}(w), |  | (21) |

   where w‚â•0w\geq 0 is interpreted entry-wise, and Œπw‚â•0\iota\_{w\geq 0} is the corresponding indicator function. Then the proximal step (with the notation g+g\_{+} to signify the incorporation of the constraint) becomes

   |  |  |  |
   | --- | --- | --- |
   |  | proxt‚Äãg+‚Å°(w)=arg‚Äãminz‚â•0‚Å°12‚Äã‚Äñz‚àíw‚Äñ2+t‚ÄãŒ∏‚ãÜ‚Äã‚àëj‚ààSŒ±j‚Äã|zj|.\operatorname{prox}\_{tg\_{+}}(w)=\operatorname\*{arg\,min}\_{z\geq 0}\frac{1}{2}\|z-w\|^{2}+t\;\theta\_{\star}\sum\_{j\in S}\alpha\_{j}|z\_{j}|. |  |

   As the constraint is separable and |wj|=wj,for‚Äãwj‚â•0,|w\_{j}|=w\_{j},\;\text{for}\,w\_{j}\geq 0, this prox is just positive soft-thresholding, i.e., coordinate-wise:

   |  |  |  |
   | --- | --- | --- |
   |  | proxt‚Äãg+(w))j=max{wj‚àítŒ∏‚ãÜŒ±j,0}.\operatorname{prox}\_{tg\_{+}}(w))\_{j}=\max\{w\_{j}-t\,\theta\_{\star}\,{\alpha}\_{j},0\}. |  |

   A separate Lipschitz constant LSL\_{S} is estimated for the restricted Hessian

   |  |  |  |
   | --- | --- | --- |
   |  | ‚àá2fS=1œÉ2‚ÄãRc,S‚ä§‚ÄãRc,S+2‚ÄãŒõ‚Äã‚Äâ11‚ä§,\nabla^{2}f\_{S}=\frac{1}{\sigma^{2}}R\_{c,S}^{\top}R\_{c,S}+2\Lambda\,11^{\top}, |  |

   and we run FISTA to obtain wS,FISTAw\_{S,\mathrm{FISTA}}. A final optional budget correction shift, i.e., as in ([20](https://arxiv.org/html/2512.22109v1#S5.E20 "Equation 20 ‚Ä£ Item (a) ‚Ä£ Construction of a tradeable portfolio ‚Ä£ 5 Construction of a tradeable portfolio ‚Ä£ Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification")), wS,FISTAw\_{S,\mathrm{FISTA}} to enforce ùüè‚ä§‚Äãùê∞ùêí,FISTA=1{\bf 1^{\top}w\_{S,\mathrm{FISTA}}}=1 exactly. Embedding back to ‚Ñùp\mathbb{R}^{p} with zeros off SS gives

   |  |  |  |
   | --- | --- | --- |
   |  | wFISTA‚Äã\_‚ÄãS‚àà‚Ñùp.w\_{\mathrm{FISTA\\_S}}\in\mathbb{R}^{p}. |  |
3. (c)

   De-biased on SS with exact budget.
   If the long-only constraint were not imposed, we would solve the constrained least‚Äìsquares problem

   |  |  |  |  |
   | --- | --- | --- | --- |
   |  | minu‚àà‚Ñù|S|‚Å°12‚ÄãœÉ^2‚Äã‚Äñyc‚àíRc,S‚Äãu‚Äñ22s.t.1‚ä§‚Äãu=1.\min\_{u\in\mathbb{R}^{|S|}}\frac{1}{2\hat{\sigma}^{2}}\,\|y\_{c}-R\_{c,S}u\|\_{2}^{2}\quad\text{s.t.}\quad 1^{\top}u=1. |  | (22) |

   This is done via a KKT system

   |  |  |  |
   | --- | --- | --- |
   |  | [H11‚ä§0]‚Äã[uŒº]=[Rc,S‚ä§‚Äãyc/œÉ21],H=1œÉ2‚ÄãRc,S‚ä§‚ÄãRc,S+œÅ‚ÄãI,\begin{bmatrix}H&1\\[1.00006pt] 1^{\top}&0\end{bmatrix}\begin{bmatrix}u\\ \mu\end{bmatrix}=\begin{bmatrix}R\_{c,S}^{\top}\,y\_{c}\,/\,\sigma^{2}\\ 1\end{bmatrix},\qquad H=\frac{1}{\sigma^{2}}R\_{c,S}^{\top}R\_{c,S}+\rho I, |  |

   with a very small (user-defined) ridge œÅ>0\rho>0 and Lagrange multiplier Œº\mu.The resulting uu is embedded into ‚Ñùp\mathbb{R}^{p} by zero‚Äìpadding to yield wdebiasw\_{\mathrm{debias}}. The idea is to, after selecting the active set SS, de-bias on that support under the exact budget constraint to remove shrinkage bias.

   However, with the non-negativity constraint, we need to solve

   |  |  |  |  |
   | --- | --- | --- | --- |
   |  | minu‚àà‚Ñù|S|‚Å°12‚ÄãœÉ^2‚Äã‚Äñyc‚àíRc,S‚Äãu‚Äñ22s.t.1‚ä§‚Äãu=1,w‚â•0,\min\_{u\in\mathbb{R}^{|S|}}\frac{1}{2\hat{\sigma}^{2}}\,\|y\_{c}-R\_{c,S}u\|\_{2}^{2}\quad\text{s.t.}\quad 1^{\top}u=1,\quad w\geq 0, |  | (23) |

   which is a convex quadratic program with no closed-form KKT anymore. In this case (but we don‚Äôt pursue it here), one could instead use, e.g., gradient descent on the smooth part and project each iterate onto the simplex (computationally cheap).

For single-period strategies or myopic formulations with linear/fixed costs, this would be the last step of the workflow.

## 6 Rebalancing

Our focus here is passive index tracking, therefore our working assumption is that the investor follows a buy‚Äìand‚Äìhold over a trading window with no daily rebalancing. At the end of the trading period (typically four, six or twelve months), one would evaluate the performance of the held portfolio and compute structural diagnostics that will inform their next steps. In this work, we focus on maintaining a low TE rather than trying to beat the index in terms of returns or drawdown. We warn against trying to aggressively shrink the TE, as this is very likely to result in overfitting and suboptimal out-of-sample results.

We frame the rebalancing problem as an optimisation problem with adjustment vector Œî‚Äãw\Delta w playing the role of the decision variable. The new allocation vector will be

|  |  |  |
| --- | --- | --- |
|  | wnew=wold+Œî‚Äãw,w\_{\text{new}}=w\_{\text{old}}+\Delta w, |  |

where woldw\_{\text{old}} is the portfolio held up to this point. To maintain the budget constraint on wneww\_{\text{new}}
we enforce the hard sum‚Äìzero constraint

|  |  |  |
| --- | --- | --- |
|  | 1‚ä§‚ÄãŒî‚Äãw=01^{\top}\Delta w=0 |  |

for Œî‚Äãw\Delta w so that the budget constraint is preserved exactly.

### 6.1 Tracking error and sparsity considerations for rebalancing

##### The fitting window

Using the same length as before, TT, we construct a second fitting window ending at the evaluation date of the first portfolio; we refer to this as the ‚ÄúFIT‚Äì2‚Äù window. We operate with the vector y2‚ààRTy\_{2}\in R^{T} of index returns, and the matrix R2‚àà‚ÑùT√ópR\_{2}\in\mathbb{R}^{T\times p} of asset returns.

As before, we centre:

|  |  |  |
| --- | --- | --- |
|  | y2,Œº=1T‚Äã‚àëty2,t,R2,Œº,j=1T‚Äã‚àëtR2,t‚Äãj,j‚àà{1,‚Ä¶,p}y2,c=y2‚àíy2,Œº‚ÄãùüèT,R2,c=R2‚àíùüèT‚ÄãR2,Œº‚ä§,\begin{split}y\_{2,\mu}&=\frac{1}{T}\sum\_{t}y\_{2,t},\quad R\_{2,\mu,j}=\frac{1}{T}\sum\_{t}R\_{2,tj},\qquad j\in\{1,\dots,p\}\\ y\_{2,c}&=y\_{2}-y\_{2,\mu}\mathbf{1}\_{T},\qquad R\_{2,c}=R\_{2}-\mathbf{1}\_{T}R\_{2,\mu}^{\top},\end{split} |  |

and build a new set of scales Œ±(2)\alpha^{(2)} based on R2,cR\_{2,c}. Define the residual target

|  |  |  |
| --- | --- | --- |
|  | y2,res=y2,c‚àíR2,c‚Äãwold.y\_{2,\mathrm{res}}\;=\;y\_{2,c}-R\_{2,c}w\_{\text{old}}. |  |

Then, for any Œî‚Äãw\Delta w we have the algebraic identity

|  |  |  |
| --- | --- | --- |
|  | y2,c‚àíR2,c‚Äã(wold+Œî‚Äãw)=y2,res‚àíR2,c‚ÄãŒî‚Äãw.y\_{2,c}-R\_{2,c}(w\_{\text{old}}+\Delta w)\;=\;y\_{2,\mathrm{res}}-R\_{2,c}\Delta w. |  |

##### Laplace prior and choice of weights on Œî‚Äãw\Delta w.

On FIT‚Äì2 we build a weighted Laplace prior on Œî‚Äãw\Delta w,

|  |  |  |
| --- | --- | --- |
|  | p‚Äã(Œî‚Äãw‚à£Œ∫)‚àùexp‚Å°(‚àíŒ∫‚ÄãSŒî‚Äã(Œî‚Äãw))‚ÄãŒπ{ùüè‚ä§‚ÄãŒî‚Äãw=0},SŒî‚Äã(Œî‚Äãw)=‚àëj=1pŒ±j(Œî)‚Äã|Œî‚Äãwj|.p(\Delta w\mid\kappa)\;\propto\;\exp\!\Bigl(-\kappa\,S\_{\Delta}(\Delta w)\Bigr)\;\mathbb{\iota}\_{\{\mathbf{1}^{\top}\Delta w=0\}},\qquad S\_{\Delta}(\Delta w)\;=\;\sum\_{j=1}^{p}\alpha^{(\Delta)}\_{j}\,|\Delta w\_{j}|. |  |

The base scales are recomputed from R2,cR\_{2,c} using the same
column-norm recipe as in FIT‚Äì1,

|  |  |  |
| --- | --- | --- |
|  | Œ±~j=‚ÄñR2,c(:,j)‚Äñ2T,Œ±~j‚ÜêŒ±~j/Œ±~¬Ø,\tilde{\alpha}\_{j}\;=\;\frac{\|R\_{2,c}^{(:,j)}\|\_{2}}{\sqrt{T}},\qquad\tilde{\alpha}\_{j}\leftarrow\tilde{\alpha}\_{j}/\overline{\tilde{\alpha}}, |  |

and we then *square* them for the rebalancing prior,

|  |  |  |
| --- | --- | --- |
|  | Œ±j(Œî)=(Œ±~j)2.\alpha^{(\Delta)}\_{j}\;=\;\bigl(\tilde{\alpha}\_{j}\bigr)^{2}. |  |

This decision deliberately penalises high-volatility names more strongly:
if Œ±~j\tilde{\alpha}\_{j} is large (volatile column), then Œ±j(Œî)\alpha^{(\Delta)}\_{j} is
larger still, so the Laplace penalty discourages frequent sign changes or
small adjustments in such components. Intuitively, this introduces a
‚Äúhysteresis‚Äù effect: noisy names must present a clearer signal before
their Œî‚Äãwj\Delta w\_{j} is moved away from zero.

##### Target and constraint domain.

The working posterior on FIT--2 is

|  |  |  |  |
| --- | --- | --- | --- |
|  | œÄŒ∫‚Äã(Œî‚Äãw‚à£data)‚àùexp‚Å°{‚àí12‚ÄãœÉŒî‚Äãw2‚Äã‚Äñy2,res‚àíR2,c‚ÄãŒî‚Äãw‚Äñ22‚àíŒ∫‚ÄãSŒî‚Äã(Œî‚Äãw)}‚ÄãŒπ{ùüè‚ä§‚ÄãŒî‚Äãw=0}.\pi\_{\kappa}(\Delta w\mid\text{data})\;\propto\;\exp\!\left\{-\frac{1}{2\sigma^{2}\_{\Delta w}}\,\bigl\|y\_{2,\mathrm{res}}-R\_{2,c}\Delta w\bigr\|\_{2}^{2}-\kappa\,S\_{\Delta}(\Delta w)\right\}\;\iota\_{\{\mathbf{1}^{\top}\Delta w=0\}}. |  | (24) |

We work on the sum-zero subspace

|  |  |  |
| --- | --- | --- |
|  | H={Œî‚Äãw‚àà‚Ñùp:ùüè‚ä§‚ÄãŒî‚Äãw=0},H\;=\;\{\Delta w\in\mathbb{R}^{p}:\mathbf{1}^{\top}\Delta w=0\}, |  |

which is a subspace of dimension d=p‚àí1d=p-1.
Thanks to the 1‚Äìhomogeneity of SŒîS\_{\Delta}, it follows that, under the prior alone,
ùîºŒ∫‚Äã[SŒî‚Äã(Œî‚ÄãW)]=d/Œ∫.\mathbb{E}\_{\kappa}[S\_{\Delta}(\Delta W)]=d/\kappa.
This leads to the mean-zero score

|  |  |  |
| --- | --- | --- |
|  | g‚Äã(Œ∑;Œî‚Äãw)=d‚àíŒ∫‚ÄãSŒî‚Äã(Œî‚Äãw),Œ∑=log‚Å°Œ∫,g(\eta;\Delta w)\;=\;d-\kappa\,S\_{\Delta}(\Delta w),\qquad\eta=\log\kappa, |  |

which we use inside SAPG. In practical terms, this allows for the application of the most straightforward of the SAPG variants put forward in (?, ?).

##### Treating the noise scale as a TE‚Äìrelated parameter.

We estimate œÉ2\sigma^{2} for the new period using the MAD estimator as before. However, as expected, the baseline estimator œÉŒî‚Äãw,base2\sigma^{2}\_{\Delta w,\mathrm{base}} is numerically too small to be trusted as a direct description of the out‚Äìof‚Äìsample TE: it merely reflects the in‚Äìsample fit of an aggressively optimised constrained LS portfolio. As discussed in Section [2.3](https://arxiv.org/html/2512.22109v1#S2.SS3 "2.3 Role and effect of the likelihood parameters ‚Ä£ 2 Model considerations ‚Ä£ Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification"), in practice we wish to *choose* a ‚Äúnoise level‚Äù that is consistent with the realised TE over the FIT‚Äì2 window for the existing portfolio woldw\_{\mathrm{old}}, but still allows the prior to exert substantial influence.

To this end we introduce a scalar multiplier c>0c>0 and consider the
family

|  |  |  |
| --- | --- | --- |
|  | œÉŒî‚Äãw2‚Äã(c)=c‚ÄãœÉŒî‚Äãw,base2,c‚ààùíû0,\sigma^{2}\_{\Delta w}(c)\;=\;c\,\sigma^{2}\_{\Delta w,\mathrm{base}},\qquad c\in\mathcal{C}\_{0}, |  |

for a grid ùíû0\mathcal{C}\_{0} of candidate values (e.g.,
ùíû0={1,20,50,‚Ä¶,200}\mathcal{C}\_{0}=\{1,20,50,\dots,200\}). For each cc the likelihood
in¬†([7.4](https://arxiv.org/html/2512.22109v1#S7.Ex93 "7.4 Rebalancing ‚Ä£ 7 A case study: tracking the S&P 500 ‚Ä£ Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification")) is scaled accordingly; in particular
increasing cc flattens the likelihood, which in turn gives SAPG more
freedom to drive the posterior towards sparser solutions.

We measure tracking performance on FIT‚Äì2 via the usual RMS tracking error (on the uncentred returns)

|  |  |  |
| --- | --- | --- |
|  | TEFIT2‚Äã(w)=(1T‚Äã‚àët‚ààFIT‚Äì2(yt‚àí(Rt‚Äãw))2)1/2,\mathrm{TE}\_{\mathrm{FIT2}}(w)\;=\;\biggl(\frac{1}{T}\sum\_{t\in\text{FIT--2}}\bigl(y\_{t}-(R\_{t}w)\bigr)^{2}\biggr)^{1/2}, |  |

and we denote by TEold\mathrm{TE}\_{\mathrm{old}} the TE of
woldw\_{\mathrm{old}} on FIT‚Äì2. This quantity is used to anchor the acceptable range of TE for the rebalanced portfolio.

##### Empirical Bayes learning of Œ∫‚Äã(c)\kappa(c) via SAPG.

For each candidate c‚ààùíûc\in\mathcal{C} we treat œÉŒî‚Äãw2‚Äã(c)\sigma^{2}\_{\Delta w}(c)
as fixed, and we estimate Œ∫‚ãÜ‚Äã(c)\kappa\_{\star}(c) by SAPG on the smoothed posterior œÄ‚Äã(Œî‚Äãw‚à£c,Œ∫)\pi(\Delta w\mid c,\kappa)
associated with¬†([7.4](https://arxiv.org/html/2512.22109v1#S7.Ex93 "7.4 Rebalancing ‚Ä£ 7 A case study: tracking the S&P 500 ‚Ä£ Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification")). Of course, we can only do this because of how fast SAPG and FISTA are; in our experiments, we computed 10 pairs of (Œ∫(c),ŒîwM‚ÄãA‚ÄãP(Œ∫(c))(\kappa(c),\Delta w\_{MAP}(\kappa(c)) in less than two minutes. Writing
Œ∑=log‚Å°Œ∫\eta=\log\kappa, the SAPG update has the generic form

|  |  |  |
| --- | --- | --- |
|  | Œ∑k+1=Œ†[log‚Å°Œ∫min,log‚Å°Œ∫max]‚Äã(Œ∑k+œÅk‚ÄãŒî‚Äãgk),\eta\_{k+1}\;=\;\Pi\_{[\log\kappa\_{\min},\,\log\kappa\_{\max}]}\Bigl(\eta\_{k}+\rho\_{k}\,\Delta g\_{k}\Bigr), |  |

where, as in FIT‚Äì1, œÅk=cSAPG/(k+k0)\rho\_{k}=c\_{\mathrm{SAPG}}/(k+k\_{0}) is a decaying step size
and Œî‚Äãgk\Delta g\_{k} is a noisy estimate of the derivative of the
log‚Äìmarginal likelihood with respect Œ∑\eta. In our implementation, the inner kernel is a MYULA‚Äìtype chain on
Œî‚Äãw\Delta w targeting the smoothed posterior, and at iteration kk we
compute

|  |  |  |
| --- | --- | --- |
|  | uk=proxŒªMY‚Äãg‚Äã(Œî‚Äãwk),Sk=‚àëj=1pŒ±jŒî‚Äã|uk,j|,u\_{k}\;=\;\mathrm{prox}\_{\lambda\_{\mathrm{MY}}g}(\Delta w\_{k}),\qquad S\_{k}\;=\;\sum\_{j=1}^{p}\alpha^{\Delta}\_{j}|u\_{k,j}|, |  |

from which a simple moment condition suggests the update direction
Œî‚Äãgk‚âàd‚àíŒ∫k‚ÄãSk\Delta g\_{k}\approx d-\kappa\_{k}S\_{k}, with d=p‚àí1d=p-1 the dimension
of the sum‚Äìzero hyperplane (as explained in the Appendix). Polyak‚ÄìRuppert averaging of the Œ∑k\eta\_{k}
sequence over the tail of the run yields an empirical‚ÄìBayes estimate
Œ∫‚ãÜ‚Äã(c)\kappa\_{\star}(c) for that particular noise scale c.c.

##### MAP estimation and effective sparsity.

Once Œ∫‚ãÜ‚Äã(c)\kappa\_{\star}(c) has been learned, we fix the pair
(œÉŒî‚Äãw2‚Äã(c),Œ∫‚ãÜ‚Äã(c))(\sigma^{2}\_{\Delta w}(c),\kappa\_{\star}(c)) and compute the
smoothed MAP

|  |  |  |
| --- | --- | --- |
|  | Œî‚ÄãwMAP‚Äã(c)=arg‚Å°minŒî‚Äãw‚àà‚Ñùp‚Å°Œ¶‚Äã(Œî‚Äãw;Œ∫‚ãÜ‚Äã(c),œÉŒî‚Äãw2‚Äã(c)),\Delta w\_{\mathrm{MAP}}(c)\;=\;\arg\min\_{\Delta w\in\mathbb{R}^{p}}\Phi\bigl(\Delta w;\,\kappa\_{\star}(c),\sigma^{2}\_{\Delta w}(c)\bigr), |  |

using a FISTA scheme on¬†([7.4](https://arxiv.org/html/2512.22109v1#S7.Ex93 "7.4 Rebalancing ‚Ä£ 7 A case study: tracking the S&P 500 ‚Ä£ Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification")) with the
budget‚Äìconstrained proximal map
proxŒªMY‚Äãg\mathrm{prox}\_{\lambda\_{\mathrm{MY}}g}.

The rebalanced portfolio
is then
wnew‚Äã(c)=wold+Œî‚ÄãwMAP‚Äã(c)w\_{\mathrm{new}}(c)=w\_{\mathrm{old}}+\Delta w\_{\mathrm{MAP}}(c),
automatically satisfying ùüè‚ä§‚Äãwnew‚Äã(c)=1\mathbf{1}^{\top}w\_{\mathrm{new}}(c)=1.

Because the smoothed objective does not produce exact zeros, we define
an *effective* cardinality

|  |  |  |
| --- | --- | --- |
|  | nnzeff‚Äã(c)=#‚Äã{j‚àà{1,‚Ä¶,p}:|Œî‚ÄãwMAPj‚Äã(c)|‚â•œÑeff},\mathrm{nnz}\_{\mathrm{eff}}(c)\;=\;\#\Bigl\{j\in\{1,\dots,p\}:\bigl|\Delta{w\_{\mathrm{MAP}}}\_{j}(c)\bigr|\geq\tau\_{\mathrm{eff}}\Bigr\}, |  |

where œÑeff>0\tau\_{\mathrm{eff}}>0 is a small fixed threshold chosen to
ignore numerically negligible weight adjustments. We also record the
raw cardinality
nnzraw‚Äã(c)=#‚Äã{j:Œî‚ÄãwMAPj‚Äã(c)‚â†0}\mathrm{nnz}\_{\mathrm{raw}}(c)=\#\{j:\Delta{w\_{\mathrm{MAP}}}\_{j}(c)\neq 0\}
for diagnostic purposes, but all decisions are based on
nnzeff‚Äã(c)\mathrm{nnz}\_{\mathrm{eff}}(c).

##### A scalar decision metric over the cc‚Äìgrid.

For each c‚ààùíû0c\in\mathcal{C}\_{0} we evaluate both the TE
TEFIT2‚Äã(wnew‚Äã(c))\mathrm{TE}\_{\mathrm{FIT2}}(w\_{\mathrm{new}}(c)) and the effective
adjustment size nnzeff‚Äã(c)\mathrm{nnz}\_{\mathrm{eff}}(c). We then define a
simple scalar score

|  |  |  |
| --- | --- | --- |
|  | d‚Äã(c)=œïTE‚Äã(c)‚Äãwnnz‚Äã(c),d(c)\;=\;\phi\_{\mathrm{TE}}(c)\,w\_{\mathrm{nnz}}(c), |  |

where:

* ‚Ä¢

  We specify lower and upper fractions
  0<Œ≥lo<1<Œ≥hi0<\gamma\_{\mathrm{lo}}<1<\gamma\_{\mathrm{hi}} and set

  |  |  |  |
  | --- | --- | --- |
  |  | œïTE‚Äã(c)={1,if¬†‚ÄãŒ≥lo‚ÄãTEold‚â§TEFIT2‚Äã(wreb‚Äã(c))‚â§Œ≥hi‚ÄãTEold,0,otherwise.\phi\_{\mathrm{TE}}(c)\;=\;\begin{cases}1,&\text{if }\gamma\_{\mathrm{lo}}\,\mathrm{TE}\_{\mathrm{old}}\leq\mathrm{TE}\_{\mathrm{FIT2}}(w^{\mathrm{reb}}(c))\leq\gamma\_{\mathrm{hi}}\,\mathrm{TE}\_{\mathrm{old}},\\[2.5pt] 0,&\text{otherwise.}\end{cases} |  |

  In our experiments we use Œ≥lo=0.2\gamma\_{\mathrm{lo}}=0.2 and
  Œ≥hi=1.2\gamma\_{\mathrm{hi}}=1.2, reflecting the view that (i) TE values
  much smaller than 0.2‚ÄãTEold0.2\,\mathrm{TE}\_{\mathrm{old}} are likely to
  correspond to overfitting on FIT‚Äì2, whereas (ii) TE values much larger than 1.2‚ÄãTEold1.2\,\mathrm{TE}\_{\mathrm{old}} indicate ineffective
  tracking.
* ‚Ä¢

  *Gaussian preference over effective cardinality.*
  Let nnzprev\mathrm{nnz}\_{\mathrm{prev}} denote the cardinality of the
  existing portfolio woldw^{\mathrm{old}}, and set a target adjustment
  size

  |  |  |  |
  | --- | --- | --- |
  |  | n‚ãÜ=Œ≥nnz‚Äãnnzprev,n\_{\star}\;=\;\gamma\_{\mathrm{nnz}}\,\mathrm{nnz}\_{\mathrm{prev}}, |  |

  with Œ≥nnz‚âà0.25\gamma\_{\mathrm{nnz}}\approx 0.25: we prefer to adjust only
  a moderate fraction of the currently held names. Given a scale
  parameter œÉnnz>0\sigma\_{\mathrm{nnz}}>0 (in units of ‚Äúnames‚Äù),
  we define

  |  |  |  |
  | --- | --- | --- |
  |  | wnnz‚Äã(c)=exp‚Å°(‚àí12‚Äã(nnzeff‚Äã(c)‚àín‚ãÜœÉnnz)2).w\_{\mathrm{nnz}}(c)\;=\;\exp\!\left(-\frac{1}{2}\left(\frac{\mathrm{nnz}\_{\mathrm{eff}}(c)-n\_{\star}}{\sigma\_{\mathrm{nnz}}}\right)^{2}\right). |  |

The score d‚Äã(c)d(c) is thus nonzero only for settings where the TE lies
in a plausible range, and among those, it favours configurations where
the effective number of adjusted names is neither too small nor too
large. To provide a sense of the scales involved, in the S&P 500 experiment of our Case Study, this procedure selects a value
c‚àó=60c^{\*}=60, with
TEFIT2‚Äã(wnew‚Äã(c‚àó))\mathrm{TE}\_{\mathrm{FIT2}}(w\_{\mathrm{new}}(c^{\*})) slightly lower than
TEold\mathrm{TE}\_{\mathrm{old}} and an effective adjustment size of
nnzeff‚Äã(c‚àó)=34\mathrm{nnz}\_{\mathrm{eff}}(c^{\*})=34 starting from
nnzold=155\mathrm{nnz}\_{\mathrm{old}}=155. The above is a crude example of a possible metric, that nevertheless captures well our intentions; refinements are of course possible.

##### Rebalancing parameters for UQ.

Once c‚àóc^{\*} has been selected on the grid, we lock in the corresponding
noise variance and sparsity level,

|  |  |  |
| --- | --- | --- |
|  | œÉŒî‚Äãw,final2=œÉŒî‚Äãw2‚Äã(c‚àó),Œ∫‚ãÜ,final=Œ∫‚ãÜ‚Äã(c‚àó),\sigma^{2}\_{\Delta w,\mathrm{final}}\;=\;\sigma^{2}\_{\Delta w}(c^{\*}),\qquad{\kappa\_{\star}}\_{,{\text{final}}}\;=\;\kappa\_{\star}(c^{\*}), |  |

together with the associated smoothed MAP rebalancing move
Œî‚ÄãwMAP=Œî‚ÄãwMAP‚Äã(c‚àó)\Delta w\_{\mathrm{MAP}}=\Delta w\_{\mathrm{MAP}}(c^{\*}) and
rebalanced portfolio wnew=wnew‚Äã(c‚àó)w\_{\mathrm{new}}=w\_{\mathrm{new}}(c^{\*}).
These now define a fixed target posterior for uncertainty quantification
on FIT‚Äì2. In particular, the subsequent preconditioned MALA sampler in
Section¬†[6.2](https://arxiv.org/html/2512.22109v1#S6.SS2 "6.2 Long MALA run for rebalancing ‚Ä£ 6 Rebalancing ‚Ä£ Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification") will be constructed around
Œ¶‚Äã(Œî‚Äãw;Œ∫‚ãÜ,final,œÉŒî‚Äãw,final2)\Phi\left(\Delta w;{\kappa\_{\star}}\_{,{\text{final}}},\sigma^{2}\_{\Delta w,\mathrm{final}}\right)
and initialised at Œî‚ÄãwMAP\Delta w\_{\mathrm{MAP}}.

### 6.2 Long MALA run for rebalancing

We mirror the MALA construction for ww, now targeting the smoothed posterior for Œî‚Äãw\Delta w.

##### Preconditioner and smoothing.

We build a Jacobi preconditioner

|  |  |  |
| --- | --- | --- |
|  | Pj(Œî)=((R2,c‚ä§‚ÄãR2,c)j‚ÄãjœÉŒî‚Äãw2)‚àí1/2,P^{(\Delta)}\_{j}=\left(\frac{(R\_{2,c}^{\top}R\_{2,c})\_{jj}}{\sigma^{2}\_{\Delta w}}\right)^{-1/2}, |  |

and estimate Lpre(Œî)‚âàŒªmax‚Äã(P(Œî)‚Äã(R2,c‚ä§‚ÄãR2,c/œÉŒî‚Äãw2)‚ÄãP(Œî))L^{(\Delta)}\_{\mathrm{pre}}\approx\lambda\_{\max}(P^{(\Delta)}(R\_{2,c}^{\top}R\_{2,c}/\sigma^{2}\_{\Delta w})P^{(\Delta)}). The MY smoothing and step‚Äìsize are chosen as

|  |  |  |
| --- | --- | --- |
|  | ŒªMY,pre(Œî)=1Lpre(Œî),Œ¥(Œî)=0.92‚ÄãLpre(Œî)\lambda^{(\Delta)}\_{\mathrm{MY,pre}}=\frac{1}{L^{(\Delta)}\_{\mathrm{pre}}},\qquad\delta^{(\Delta)}=\frac{0.9}{2L^{(\Delta)}\_{\mathrm{pre}}} |  |

with small adaptive adjustments to target a MH acceptance rate around 0.600.60.

##### MALA kernel.

Let Œ¶Œî,Œª\Phi\_{\Delta,\lambda} denote the smoothed potential, using the sum‚Äìzero constrained prox (the same as in the MAP step). The preconditioned MALA proposal for Œî‚Äãw\Delta w is

|  |  |  |
| --- | --- | --- |
|  | Œî‚Äãw‚Ä≤=Œî‚Äãw‚àíŒ¥(Œî)‚Äã(P(Œî))2‚Äã‚àáŒ¶Œî,Œª‚Äã(Œî‚Äãw)+2‚ÄãŒ¥(Œî)‚ÄãP(Œî)‚ÄãŒæ,Œæ‚àºùí©‚Äã(0,Ip),\Delta w^{\prime}=\Delta w-\delta^{(\Delta)}(P^{(\Delta)})^{2}\nabla\Phi\_{\Delta,\lambda}(\Delta w)+\sqrt{2\delta^{(\Delta)}}\,P^{(\Delta)}\xi,\quad\xi\sim\mathcal{N}(0,I\_{p}), |  |

with MH accept/reject. The chain is initialised at Œî‚ÄãwMAP\Delta w\_{\mathrm{MAP}} and run for a long horizon; an initial number of draws is discarded as burn‚Äìin.

Post‚Äìburn we retain

|  |  |  |
| --- | --- | --- |
|  | {Œî‚Äãw(m)}m=1Mpost‚â°Wpost(Œî).\{\Delta w^{(m)}\}\_{m=1}^{M\_{\mathrm{post}}}\equiv W^{(\Delta)}\_{\mathrm{post}}. |  |

We assess mixing via coordinate‚Äìwise ESS and ACF for selected Œî‚Äãwj\Delta w\_{j}.

### 6.3 Posterior-informed rebalancing rules

The long MALA run on the smoothed Œî‚Äãw\Delta w posterior provides, for each coordinate j=1,‚Ä¶,pj=1,\dots,p, an empirical posterior standard deviation
sd^j\widehat{\mathrm{sd}}\_{j} and an activation probability

|  |  |  |
| --- | --- | --- |
|  | œÄ^j=‚Ñô‚Äã(|Œî‚Äãwj|‚â•œÑpost|yFIT2),\hat{\pi}\_{j}\;=\;\mathbb{P}\bigl(|\Delta w\_{j}|\geq\tau\_{\mathrm{post}}\,\big|\,y\_{\text{FIT2}}\bigr), |  |

estimated from post-burn MCMC samples. Following the scale-based thresholding idea discussed earlier, we define a global posterior
scale threshold

|  |  |  |  |
| --- | --- | --- | --- |
|  | œÑpost=k‚ãÖmedian‚Äã(sd^1,‚Ä¶,sd^p),k>0,\tau\_{\mathrm{post}}\;=\;k\cdot\mathrm{median}\bigl(\widehat{\mathrm{sd}}\_{1},\dots,\widehat{\mathrm{sd}}\_{p}\bigr),\qquad k>0, |  | (25) |

and use both œÑpost\tau\_{\mathrm{post}} and the activation probabilities
œÄ^j\hat{\pi}\_{j} to gate which coordinates are eligible for rebalancing.

##### Scale and probability gates.

Let Œî‚ÄãwMAP\Delta w\_{\mathrm{MAP}} denote the smoothed MAP solution for the
rebalancing model at the selected noise scale c‚àóc^{\ast}.
We define:

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | SœÑ\displaystyle S\_{\tau} | ={j:|Œî‚ÄãwMAPj|‚â•œÑpost},\displaystyle=\bigl\{j:|\Delta{w\_{\mathrm{MAP}}}\_{j}|\,\geq\,\tau\_{\mathrm{post}}\bigr\}, |  | (26) |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | SœÄ‚Äã(œÄ‚ãÜ)\displaystyle S\_{\pi}(\pi^{\star}) | ={j:œÄ^j‚â•œÄ‚ãÜ},\displaystyle=\bigl\{j:\hat{\pi}\_{j}\,\geq\,\pi^{\star}\bigr\}, |  | (27) |

where œÄ‚ãÜ‚àà(0,1)\pi^{\star}\in(0,1) is a user‚Äìchosen activation probability
threshold. The intersection

|  |  |  |  |
| --- | --- | --- | --- |
|  | Srule‚Äã(œÄ‚ãÜ)=SœÑ‚à©SœÄ‚Äã(œÄ‚ãÜ)S\_{\mathrm{rule}}(\pi^{\star})\;=\;S\_{\tau}\,\cap\,S\_{\pi}(\pi^{\star}) |  | (28) |

collects those coordinates that are simultaneously ‚Äúlarge‚Äù in the MAP
sense and frequently active under the posterior.

Finally, while negative entries in Œî‚Äãw\Delta w are meaningful, under the mandate of no-shorts, one also needs to impose that wn‚Äãe‚Äãw,j=wo‚Äãl‚Äãd,j+(Œî‚ÄãwMAP)j‚â•0,w\_{new,j}=w\_{old,j}+(\Delta{w\_{\mathrm{MAP}}})\_{j}\geq 0, clipping to zero if not.

##### Implementable rebalancing within the active set.

Let SruleS\_{\mathrm{rule}} have cardinality m‚â•2m\geq 2 and define
ùíÆ=Srule\mathcal{S}=S\_{\mathrm{rule}} for brevity. We start from the
smoothed MAP increment Œî‚ÄãwMAP\Delta w^{\mathrm{MAP}} and restrict it to the
active coordinates,

|  |  |  |
| --- | --- | --- |
|  | Œî‚Äãw~j={Œî‚ÄãwMAPj,j‚ààùíÆ,0,j‚àâùíÆ,\tilde{\Delta w}\_{j}=\begin{cases}\Delta{w\_{\mathrm{MAP}}}\_{j},&j\in\mathcal{S},\\ 0,&j\notin\mathcal{S},\end{cases} |  |

so that only assets in ùíÆ\mathcal{S} are eligible for adjustment.
Because the original MAP increment Œî‚ÄãwMAP\Delta w\_{\mathrm{MAP}} satisfies
the budget constraint ‚àëjŒî‚ÄãwMAPj=0\sum\_{j}\Delta{w\_{\mathrm{MAP}}}\_{j}=0 on the full
universe, the restricted vector Œî‚Äãw~\tilde{\Delta w} will in general have
a nonzero sum over ùíÆ\mathcal{S}. To preserve the budget while
avoiding new nonzero positions (and consequently more trades and fees) outside ùíÆ\mathcal{S} we apply (for example) a
simple recentering within the active set:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Œ¥¬Ø=1m‚Äã‚àëj‚ààùíÆŒî‚Äãw~j,Œî‚Äãwjimpl={Œî‚Äãw~j‚àíŒ¥¬Ø,j‚ààùíÆ,0,j‚àâùíÆ.\bar{\delta}\;=\;\frac{1}{m}\sum\_{j\in\mathcal{S}}\tilde{\Delta w}\_{j},\qquad\Delta w^{\mathrm{impl}}\_{j}=\begin{cases}\tilde{\Delta w}\_{j}-\bar{\delta},&j\in\mathcal{S},\\[2.5pt] 0,&j\notin\mathcal{S}.\end{cases} |  | (29) |

By construction ‚àëjŒî‚Äãwjimpl=0\sum\_{j}\Delta w^{\mathrm{impl}}\_{j}=0 and the
support of Œî‚Äãwimpl\Delta w^{\mathrm{impl}} is exactly ùíÆ\mathcal{S}. The new
portfolio used for reporting and out‚Äìof‚Äìsample testing is then

|  |  |  |  |
| --- | --- | --- | --- |
|  | wnew=wold+Œî‚Äãwimpl.w\_{\mathrm{new}}\;=\;w\_{\mathrm{old}}+\Delta w^{\mathrm{impl}}. |  | (30) |

This rule ensures that rebalancing decisions are driven jointly by the
MAP magnitude and the posterior activation probabilities, and that the
resulting trades remain sparse and interpretable: only a small set of assets is modified, and the budget constraint is enforced by a local
correction within that set, rather than by a global projection that
would introduce many tiny, nonzero positions.

## 7 A case study: tracking the S&P 500

### 7.1 Data

We obtained freely available data from *Yahoo Finance* from *January 1st 2017 to January 1st 2020*, a recent, not-too-turbulent window. The data are in the form of daily adjusted closing prices for the constituent assets of the S&P 500, and the index itself. Our experiments intentionally restrict attention to a fixed universe of assets (478) that survive the entire study horizon. This design isolates the methodological contributions from confounds due to changing constituent sets. A consequence is *survivorship bias*: the sample excludes delisted or newly listed names. In practice, this may understate real-world turnover and TE when reconstitutions occur. We make two mitigations explicit: (i) we use fixed-length (500 trading days) rolling fit windows and fixed hold periods (125 days) so that all estimates are *out-of-sample* relative to the subsequent hold; and (ii) we report implementability proxies (turnover, active names) alongside TE. Extending the pipeline to live universes with entry/exit events is feasible (rebuild the design matrix RR each window and carry inherited positions through reconstitution dates), but is outside our proof-of-concept scope.

All experiments were conducted on a MacBook Air (Apple M1 Chip: 8-core CPU), running Python 3.12.

### 7.2 Creating a tradeable portfolio

### FIT and HOLD windows

We split our data into three fitting and two holding periods, with some overlaps as outlined and shown schematically below:

* ‚Ä¢

  FIT‚Äì1 period: *2017-01-03 to 2018-12-31*, 500 trading days, that is used for the design of the first tradeable portfolio.
* ‚Ä¢

  HOLD‚Äì1 period: *2019-01-02 to 2019-06-28*, 128 trading days, when the above portfolio is held out-of-sample for the first time immediately following its design. Upon evaluation at the end of this period, we focus on rebalancing, working with data in the
* ‚Ä¢

  FIT‚Äì2 period: *2017-07-03 to 2019-06-28*, 500 trading days, ending on the last HOLD‚Äì1 day. We rebalance the first portfolio and hold it throughout the out-of-sample
* ‚Ä¢

  HOLD‚Äì2 period: *2019-07-01 to 2019-12-31*, 124 days. Again, upon evaluation, we use one last fitting period,
* ‚Ä¢

  FIT‚Äì3 period: *2018-01-04 to 2019-12-31*, 500 trading days, ending on the last HOLD‚Äì2 day.

calendar time2017201820192020FIT‚Äì1FIT‚Äì2FIT‚Äì3HOLD‚Äì1HOLD‚Äì2FIT window (in-sampleHOLD window (out-of-sample evaluation)


Figure 1: Timeline of fitting (FIT) and holding (HOLD) periods used in the empirical study.

### Model setup on FIT‚Äì1

We use a rolling window of length T=500T=500 for all the FIT periods. The same 478 assets (all are S&P 500 constituents for the full 2017-2020 period) are considered.

We first centre both the index and the regressors returns on FIT‚Äì1. Moreover, we incorporate the budget constraint through a Gaussian pseudo-observation term. The corresponding log-likelihood is

|  |  |  |
| --- | --- | --- |
|  | fy‚Äã(w)=12‚ÄãœÉ2‚Äã‚Äñyc‚àíRc‚Äãw‚Äñ22+Œõ‚Äã(ùüè‚ä§‚Äãw‚àí1)2,f\_{y}(w)\;=\;\frac{1}{2\sigma^{2}}\,\|y\_{c}-R\_{c}w\|\_{2}^{2}\;+\;\Lambda\bigl(\mathbf{1}^{\top}w-1\bigr)^{2}, |  |

where Œõ=12‚ÄãœÑc2.\Lambda\;=\;\frac{1}{2\tau\_{c}^{2}}. The parameter œÑc\tau\_{c} is a fixed, user-defined allowed deviation from the hard constraint, and the first of only two parameters a user would have to input for FIT‚Äì1 (the other being œÄ‚àó\pi^{\ast} at the final selection stage).

Here we set œÑc=2‚ãÖ10‚àí3,\tau\_{c}=2\cdot 10^{-3}, which in turn yields Œõ=1.25‚ãÖ105.\Lambda=1.25\cdot 10^{5}.

Next, we calculate the MAD estimator for the noise variance as described in Section [C](https://arxiv.org/html/2512.22109v1#A3 "Appendix C Noise variance estimation ‚Ä£ Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification"). For this window we calculated œÉ^2=3.26‚ãÖ10‚àí9.\hat{\sigma}^{2}=3.26\cdot 10^{-9}.

For the prior, following the discussion from Section [2.2](https://arxiv.org/html/2512.22109v1#S2.SS2 "2.2 Sparsity-inducing prior ‚Ä£ 2 Model considerations ‚Ä£ Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification"),
we calculate per‚Äìasset scaling factors for the weighted ‚Ñì1\ell\_{1} penalty; pp is the number of assets, here p=478.p=478. We set

|  |  |  |  |
| --- | --- | --- | --- |
|  | sj=‚ÄñRc,‚ãÖj‚Äñ2T,Œ±j=max‚Å°{sj,Œµ}1p‚Äã‚àëk=1pmax‚Å°{sk,Œµ},s\_{j}\;=\;\frac{\big\|R\_{c,\cdot j}\big\|\_{2}}{\sqrt{T}},\qquad\alpha\_{j}\;=\;\frac{\max\{s\_{j},\varepsilon\}}{\frac{1}{p}\displaystyle\sum\_{k=1}^{p}\max\{s\_{k},\varepsilon\}}, |  | (31) |

(by default, Œµ=10‚àí8\varepsilon=10^{-8}). We recorded the minimal, maximal, and mean weights as 0.51,3.492,10.51,3.492,1 respectively.

### SAPG to select Œ∏‚ãÜ\theta\_{\star}

We have seen that for stability, the MYULA timestep is selected to satisfy it

|  |  |  |
| --- | --- | --- |
|  | Œ¥MYULA=0.92‚ÄãLf,\delta\_{\mathrm{MYULA}}=\frac{0.9}{2L\_{f}}, |  |

where

|  |  |  |
| --- | --- | --- |
|  | Lf=1œÉ^2‚ÄãŒªmax‚Äã(Rc‚ä§‚ÄãRc+2‚ÄãŒõ‚Äãùüèùüè‚ä§).L\_{f}=\frac{1}{\hat{\sigma}^{2}}\,\lambda\_{\max}(R\_{c}^{\top}R\_{c}+2\Lambda\mathbf{1}\mathbf{1}^{\top}). |  |

We estimate the maximal eigenvalue of Rc‚ä§‚ÄãRc+2‚ÄãŒõ‚Äãùüèùüè‚ä§R\_{c}^{\top}R\_{c}+2\Lambda\mathbf{1}\mathbf{1}^{\top} (with the Œõ\Lambda found earlier) using the power method, and with the previously computed value for œÉ^2\hat{\sigma}^{2}, we find

|  |  |  |
| --- | --- | --- |
|  | Lf=5.044‚ãÖ109,ŒªM‚ÄãY=1.982‚ãÖ10‚àí10,Œ¥M‚ÄãY‚ÄãU‚ÄãL‚ÄãA=8.921‚ãÖ10‚àí11.L\_{f}=5.044\cdot 10^{9},\qquad\lambda\_{MY}=1.982\cdot 10^{-10},\qquad\delta\_{MYULA}=8.921\cdot 10^{-11}. |  |

Before SAPG, we warmstart the MYULA chain with a fixed value Œ∏0\theta\_{0}, whose selection we explain in Appendix [A.1](https://arxiv.org/html/2512.22109v1#A1.SS1 "A.1 Heuristic choice of the initial scale ùúÉ‚ÇÄ ‚Ä£ Appendix A Stochastic Approximation Proximal Gradient algorithm (SAPG) ‚Ä£ Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification"). Based on the value of Œ∏0,\theta\_{0}, we also define the admissibility set Œò=[Œ∏0/10,10‚ÄãŒ∏0]\Theta=[\theta\_{0}/10,10\,\theta\_{0}]. We find Œ∏0=198.7,\theta\_{0}=198.7, and then set Œò=[19.87,1987].\Theta=[19.87,1987]. We plot the trace of the log-posterior (up to a constant) as an indication of stationarity in Fig. [2](https://arxiv.org/html/2512.22109v1#S7.F2 "Figure 2 ‚Ä£ SAPG to select ùúÉ_‚ãÜ ‚Ä£ 7 A case study: tracking the S&P 500 ‚Ä£ Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification"). It stabilises very quickly, evidence that we are indeed working with a stable timestep.

We are now ready to run SAPG using iteration ([12](https://arxiv.org/html/2512.22109v1#S3.E12 "Equation 12 ‚Ä£ SAPG update for ùúÉ. ‚Ä£ 3.3 Empirical-Bayes calibration of the sparsity parameter ‚Ä£ 3 Proximal MCMC for the posterior ‚Ä£ Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification")), in logarithmic coordinates. The projection on Œò\Theta is implemented by a simple ‚Äúclipping‚Äù should an iterate exceed its bounds.

We ran 20000 iterations, of which the first 4000 were discarded as burn-in. For the SAPG timestep œÅk\rho\_{k} we used c=1c=1, k0=200k\_{0}=200. Based on the kept samples, we compute a Polyak-Ruppert weighted average as in ([13](https://arxiv.org/html/2512.22109v1#S3.E13 "Equation 13 ‚Ä£ SAPG update for ùúÉ. ‚Ä£ 3.3 Empirical-Bayes calibration of the sparsity parameter ‚Ä£ 3 Proximal MCMC for the posterior ‚Ä£ Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification")) using q=1.q=1. The output of this process is what we report as Œ∏‚ãÜ,\theta\_{\star}, which in this case assumed the value 391.1.391.1. The graph below captures the evolution of Œ∏\theta; after an initial jump, it stabilises very quickly, Fig. [3](https://arxiv.org/html/2512.22109v1#S7.F3 "Figure 3 ‚Ä£ SAPG to select ùúÉ_‚ãÜ ‚Ä£ 7 A case study: tracking the S&P 500 ‚Ä£ Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification").

![[Uncaptioned image]](myula_logpost_warmup.png)



Figure 2: MYULA warm-up with Œ∏0\theta\_{0}.

![[Uncaptioned image]](sapg_theta_trace.png)



Figure 3: SAPG evolution of Œ∏\theta.

We note that the above (solving for Œ∏0\theta\_{0}, warm-starting the chain with Œ∏0\theta\_{0} and SAPG) combined took 8 seconds to run.

### FISTA for MAP

With (œÉ^2,Œõ,Œ∏‚ãÜ)(\hat{\sigma}^{2},\Lambda,\theta\_{\star}) set as above, we compute the full‚Äìsupport maximum a posteriori (MAP) estimator

|  |  |  |
| --- | --- | --- |
|  | w^MAP‚ààarg‚Å°minw‚àà‚Ñùp‚Å°12‚ÄãœÉ^2‚Äã‚Äñyc‚àíRc‚Äãw‚Äñ2+Œõ‚Äã(1p‚ä§‚Äãw‚àí1)2+Œ∏‚ãÜ‚Äã[‚àëjŒ±j‚Äã|wj|].\hat{w}\_{\mathrm{MAP}}\in\arg\min\_{w\in\mathbb{R}^{p}}\frac{1}{2\hat{\sigma}^{2}}\|y\_{c}-R\_{c}w\|^{2}+\Lambda(1\_{p}^{\top}w-1)^{2}\;+\;\theta\_{\star}\Big[\sum\_{j}\alpha\_{j}|w\_{j}|\Big]. |  |

We solve this convex problem with FISTA: the smooth part (data fit and budget constraint) is handled in the gradient step; the ‚Ñì1\ell\_{1} uses a weighted soft-thresholding proximal operator. The time-step is chosen to be 1/Lf.1/L\_{f}.

Writing gŒ∏‚ãÜ‚Äã(w)=Œ∏‚ãÜ‚Äã‚àëjŒ±j‚Äã|wj|g\_{\theta\_{\star}}(w)=\theta\_{\star}\sum\_{j}\alpha\_{j}|w\_{j}|, the FISTA iteration is

|  |  |  |  |
| --- | --- | --- | --- |
|  | w(k+1)\displaystyle w^{(k+1)} | =prox1Lf‚ÄãgŒ∏‚ãÜ‚Å°(z(k)‚àí1Lf‚Äã‚àáfy‚Äã(z(k))),\displaystyle=\operatorname{prox}\_{\frac{1}{L\_{f}}g\_{\theta\_{\star}}}\!\Bigl(z^{(k)}-\tfrac{1}{L\_{f}}\nabla f\_{y}(z^{(k)})\Bigr), |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | tk+1\displaystyle t\_{k+1} | =1+1+4‚Äãtk22,\displaystyle=\frac{1+\sqrt{1+4t\_{k}^{2}}}{2}, |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | z(k+1)\displaystyle z^{(k+1)} | =w(k+1)+tk‚àí1tk+1‚Äã(w(k+1)‚àíw(k)),\displaystyle=w^{(k+1)}+\frac{t\_{k}-1}{t\_{k+1}}\bigl(w^{(k+1)}-w^{(k)}\bigr), |  |

starting from w(0)=0w^{(0)}=0, z(0)=0z^{(0)}=0, t0=1t\_{0}=1, and stopping when the objective difference falls below a relative tolerance threshold.

![Refer to caption](FISTA_for_MAP_iterations.png)


Figure 4: Trace: FISTA iterations for MAP. Stopped after 1385 iterations, which took less than 1 sec.

For reference, the unprocessed MAP recorded an in-sample TE of 7.1754‚Äãùêû‚àíùüéùüì.{\bf 7.1754e-05}. As seen in the histograms below, most of the weights are very small, but only 454/478 of its weights are nonzero. About 30% (139) of the MAP entries are negative.

![[Uncaptioned image]](hist_MAP.png)



Figure 5: Histogram of the MAP

![[Uncaptioned image]](w_hist_abs.png)



Figure 6: Visualising the sizes of the MAP weights.

### The long MALA run (with preconditioning)

Note that one can first run MALA and then solve for the MAP (or at the same time, although FISTA takes a few seconds (at most) and MALA takes about 10 minutes because of the MH step). We opted for this order because it is convenient to initialise MALA (at the burn-in stage) from the MAP.

### Tuning timestep for acceptance ratio

We first tune the stepsize Œ¥\delta to target an acceptance rate in the 0.550.55‚Äì0.650.65 band, using a sequence of short, fixed-length pilot runs. For MALA, in many regimes, the asymptotically optimal acceptance rate is known to be ‚âà0.574\approx 0.574.

Starting from an initial guess

|  |  |  |
| --- | --- | --- |
|  | Œ¥0=0.90Lpre+1/ŒªMY,pre,\delta\_{0}\;=\;\frac{0.90}{L\_{\mathrm{pre}}+1/\lambda\_{\mathrm{MY,pre}}}\!, |  |

based on the preconditioned Lipschitz constant LpreL\_{\mathrm{pre}} and the MY smoothing parameter ŒªMY,pre\lambda\_{\mathrm{MY,pre}}, we iterate the following scheme:

1. 1.

   Run a short MALA chain (e.g. 10001000 proposals) at the current step size Œ¥\delta.
2. 2.

   Compute the empirical acceptance rate a^‚Äã(Œ¥)\hat{a}(\delta).
3. 3.

   Update Œ¥\delta multiplicatively according to

   |  |  |  |
   | --- | --- | --- |
   |  | Œ¥‚Üê{1.25‚ÄãŒ¥,if¬†‚Äãa^‚Äã(Œ¥)>target+0.05,0.50‚ÄãŒ¥,if¬†‚Äãa^‚Äã(Œ¥)<target‚àí0.10,Œ¥,otherwise,\delta\leftarrow\begin{cases}1.25\,\delta,&\text{if }\hat{a}(\delta)>\text{target}+0.05,\\[3.00003pt] 0.50\,\delta,&\text{if }\hat{a}(\delta)<\text{target}-0.10,\\[3.00003pt] \delta,&\text{otherwise},\end{cases} |  |

   where target=0.60\text{target}=0.60 in our experiments.

If the acceptance rate falls within the band [target‚àí0.10,target+0.05][\text{target}-0.10,\text{target}+0.05]
(approximately [0.50,0.65][0.50,0.65] with the current thresholds), we stop and take the
current value as the tuned step size Œ¥‚ãÜ\delta\_{\star}. Importantly, this adaptation is performed only in a preliminary tuning phase. The final long MALA run for inference is then conducted with a fixed step size
Œ¥‚ãÜ\delta\_{\star}, so the production chain is time-homogeneous and standard MCMC theory applies to the resulting samples.

We summarise the tuning step below

|  |  |  |
| --- | --- | --- |
|  | TimestepAcc. Rate1.465‚Äãe‚àí020.931.832‚Äãe‚àí020.912.290‚Äãe‚àí020.882.862‚Äãe‚àí020.823.578‚Äãe‚àí020.764.472‚Äãe‚àí020.675.590‚Äãùêû‚àíùüéùüê0.56\begin{array}[]{r r}\hline\cr\hline\cr\text{\bf Timestep}&\text{\bf Acc. Rate}\\ \hline\cr\hline\cr 1.465e-02&0.93\\ \hline\cr 1.832e-02&0.91\\ \hline\cr 2.290e-02&0.88\\ \hline\cr 2.862e-02&0.82\\ \hline\cr 3.578e-02&0.76\\ \hline\cr 4.472e-02&0.67\\ \hline\cr{\bf 5.590e-02}&0.56\\ \hline\cr\hline\cr\end{array} |  |

### Effective sample size diagnostics

We monitor mixing of the MALA chains both on a function of direct interest
(the tracking error, TE) and on a set of portfolio weights.
Diagnostic summaries are based on effective sample sizes (ESS), computed from
empirical autocorrelation functions.

### ESS for a scalar Markov chain

Let x(1),‚Ä¶,x(M)x^{(1)},\dots,x^{(M)} denote a scalar time series extracted from the
kept MCMC draws after thinning (for example, a single portfolio weight at
successive iterations, or the TE evaluated at each draw). Denote by
r^k\widehat{r}\_{k} the empirical autocorrelation at lag kk:

|  |  |  |
| --- | --- | --- |
|  | r^k=‚àëm=1M‚àík(x(m)‚àíx¬Ø)‚Äã(x(m+k)‚àíx¬Ø)‚àëm=1M(x(m)‚àíx¬Ø)2,x¬Ø=1M‚Äã‚àëm=1Mx(m).\widehat{r}\_{k}\;=\;\frac{\sum\_{m=1}^{M-k}(x^{(m)}-\bar{x})(x^{(m+k)}-\bar{x})}{\sum\_{m=1}^{M}(x^{(m)}-\bar{x})^{2}},\qquad\bar{x}=\frac{1}{M}\sum\_{m=1}^{M}x^{(m)}. |  |

We estimate the integrated autocorrelation time by

|  |  |  |
| --- | --- | --- |
|  | œÑ=‚ÄÑ1+2‚Äã‚àëk=1Kr^k,\tau\;=\;1+2\sum\_{k=1}^{K}\widehat{r}\_{k}, |  |

where we truncate the sum at the first non-positive autocorrelation,

|  |  |  |
| --- | --- | --- |
|  | K=min‚Å°{k‚â•1:r^k<0},K=\min\{k\geq 1:\widehat{r}\_{k}<0\}, |  |

or at a fixed maximum lag if no sign change is observed. The effective sample
size for the series is then defined as

|  |  |  |
| --- | --- | --- |
|  | ESS‚Äã(x)=MœÑ.\mathrm{ESS}(x)\;=\;\frac{M}{\tau}. |  |

### ESS for the tracking error, ESS‚Äã(TE)\mathrm{ESS}(\mathrm{TE})

Let w(m)‚àà‚Ñùpw^{(m)}\in\mathbb{R}^{p} denote the portfolio weights at the mm-th kept
draw from the chain, and let yrawy\_{\mathrm{raw}} and RrawR\_{\mathrm{raw}} denote
the raw (uncentred) index and asset returns on the fit window, with centering
statistics (yŒº,RŒº)(y\_{\mu},R\_{\mu}). For each draw mm we compute a tracking error

|  |  |  |
| --- | --- | --- |
|  | TE(m)=TE‚Äã(yraw,Rraw,w(m),yŒº,RŒº)=1T‚Äã‚àët=1T(yt‚àíy^t‚Äã(w(m)))2,\mathrm{TE}^{(m)}\;=\;\mathrm{TE}\!\left(y\_{\mathrm{raw}},R\_{\mathrm{raw}},w^{(m)},y\_{\mu},R\_{\mu}\right)\;=\;\sqrt{\frac{1}{T}\sum\_{t=1}^{T}\bigl(y\_{t}-\hat{y}\_{t}(w^{(m)})\bigr)^{2}}, |  |

where

|  |  |  |
| --- | --- | --- |
|  | y^t‚Äã(w)=(Rraw,t‚Å£‚ãÖ‚àíRŒº)‚Äãw+yŒº\hat{y}\_{t}(w)\;=\;\bigl(R\_{\mathrm{raw},t\cdot}-R\_{\mu}\bigr)w+y\_{\mu} |  |

is the portfolio return implied by ww at time tt.

We then compute

|  |  |  |
| --- | --- | --- |
|  | ESS‚Äã(TE)=ESS‚Äã(TE(1),‚Ä¶,TE(M))\mathrm{ESS}(\mathrm{TE})\;=\;\mathrm{ESS}\!\bigl(\mathrm{TE}^{(1)},\dots,\mathrm{TE}^{(M)}\bigr) |  |

using the generic scalar ESS logic above. This quantity measures how many
*independent* draws from the posterior would provide the same amount of
Monte Carlo information about the TE as the correlated MALA chain delivers.

A closely related quantity, useful for interpretation, is the Monte Carlo
standard error of the posterior mean tracking error

|  |  |  |
| --- | --- | --- |
|  | TE¬Ø=1M‚Äã‚àëm=1MTE(m).\bar{\mathrm{TE}}=\frac{1}{M}\sum\_{m=1}^{M}\mathrm{TE}^{(m)}. |  |

If sd^‚Äã(TE)\widehat{\mathrm{sd}}(\mathrm{TE}) is the empirical standard deviation of
the TE(m)\mathrm{TE}^{(m)} samples, then

|  |  |  |
| --- | --- | --- |
|  | MCSE‚Äã(TE¬Ø)‚âàsd^‚Äã(TE)ESS‚Äã(TE).\mathrm{MCSE}(\bar{\mathrm{TE}})\;\approx\;\frac{\widehat{\mathrm{sd}}(\mathrm{TE})}{\sqrt{\mathrm{ESS}(\mathrm{TE})}}. |  |

A large ESS‚Äã(TE)\mathrm{ESS}(\mathrm{TE}) therefore translates directly into a
small Monte Carlo uncertainty on the TE summary that is used for reporting
and for comparing different chains or different priors.

### ESS for coordinates and ESSmin‚Äã(S)\mathrm{ESS}\_{\min}(S)

In addition to TE, we monitor mixing on a subset of portfolio weights that
are most relevant for sparsity and rebalancing decisions. Let wMAPw\_{\mathrm{MAP}}
denote the MAP weights under the chosen prior, and define a sentinel index set
SS by selecting the |S||S| largest coordinates in magnitude,

|  |  |  |
| --- | --- | --- |
|  | S={j:j¬†among the top¬†|S|¬†indices by¬†‚Äã|wMAP,j|}.S=\left\{j:\text{$j$ among the top $|S|$ indices by }|w\_{\mathrm{MAP},j}|\right\}. |  |

For each j‚ààSj\in S we consider the scalar series

|  |  |  |
| --- | --- | --- |
|  | wj(1),‚Ä¶,wj(M),w\_{j}^{(1)},\dots,w\_{j}^{(M)}, |  |

and compute an effective sample size

|  |  |  |
| --- | --- | --- |
|  | ESSj=ESS‚Äã(wj(1),‚Ä¶,wj(M)).\mathrm{ESS}\_{j}\;=\;\mathrm{ESS}\!\bigl(w\_{j}^{(1)},\dots,w\_{j}^{(M)}\bigr). |  |

As a conservative scalar summary of mixing on these active coordinates, we
report the minimum effective sample size

|  |  |  |
| --- | --- | --- |
|  | ESSmin‚Äã(S)=minj‚ààS‚Å°ESSj.\mathrm{ESS}\_{\min}(S)\;=\;\min\_{j\in S}\mathrm{ESS}\_{j}. |  |

We summarise the run with the following tables

|  |  |  |
| --- | --- | --- |
|  | SamplesBurn-inESS‚Äã(TE)ESSmin‚Äã(S)sd‚Äã(TE)MCSE‚Äã(TE¬Ø)Run durationAcceptance25000020000837.1624.71.3943‚Äãe‚àí064.8190‚Äãe‚àí081033.4‚Äãs0.57\begin{array}[]{rrrrrrrr}\hline\cr\hline\cr\text{Samples}&\text{Burn-in}&\mathrm{ESS}(\mathrm{TE})&\mathrm{ESS}\_{\min}(S)&\mathrm{sd}(\mathrm{TE})&\mathrm{MCSE}(\bar{\mathrm{TE}})&\text{Run duration}&\text{Acceptance}\\ \hline\cr 250000&20000&837.1&624.7&1.3943e-06&4.8190e-08&1033.4s&0.57\\ \hline\cr\hline\cr\end{array} |  |

Here MCSE‚Äã(TE¬Ø)\mathrm{MCSE}(\bar{\mathrm{TE}}) is computed as described above.

We also record the ESS for certain coordinates

|  |  |  |
| --- | --- | --- |
|  | Coordinate Index154107160213266319372425478E‚ÄãS‚ÄãS628.1639.3632.8628.9628.1629.9631.8632.6668.7632.4\begin{array}[]{lrrrrrrrrrr}\hline\cr\hline\cr\text{Coordinate Index}&1&54&107&160&213&266&319&372&425&478\\ \hline\cr ESS&628.1&639.3&632.8&628.9&628.1&629.9&631.8&632.6&668.7&632.4\\ \hline\cr\hline\cr\end{array} |  |

### Posterior-informed support selection and the tradeable portfolio

We have solved for the MAP and have collected the samples from a long MALA run. We can proceed now with constructing candidate portfolios. We look at their sparsity levels, and also report experiments by looking at the effect of changing the parameters œÑp‚Äão‚Äãs‚Äãt\tau\_{post} and œÄ‚àó.\pi^{\ast}.

In this experiment, we will start from the support determined by the MAP as obtained from FISTA, i.e.,

|  |  |  |
| --- | --- | --- |
|  | S‚Ä≤:={i‚àà{1,‚Ä¶,p}:wj‚â†0}.S^{\prime}:=\{i\in\{1,\dots,p\}:w\_{j}\neq 0\}. |  |

We assess the effect of our gating rules on the support, applying one rule at a time. Based on this thresholding, we declare the weight of certain assets to be zero, and form the new support, which we generically refer to as SS. Finally, we impose the non-negativity condition For every such support, we form three portfolios (as described in Section [5.4](https://arxiv.org/html/2512.22109v1#S5.SS4 "5.4 Posterior-informed support selection and the tradeable portfolio ‚Ä£ 5 Construction of a tradeable portfolio ‚Ä£ Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification")) on the new support, and then compare their in-sample TE.

One caveat for the reporting below is that the D‚ÄãE‚ÄãB‚ÄãI‚ÄãA‚ÄãSDEBIAS portfolio is not completely ‚Äúhonest‚Äù (hence the ‚àó\ast attached to it in the tables). By that we mean, that we debiased on the long-only support but didn‚Äôt enforce the non-negativity constraint in the optimisation and this gave five negative entries (though, none of them in the top-25 entries). We kept it here for comparison only. In contrast, the refit-on-S with FISTA was checked before enforcing non-negativity in the FISTA solve and it had produced four negative-entries (also not in the top-25). Once the constraint has been incorporated into FISTA, all entries are non-negative.

1. (a)

   We vary kk in the posterior scale threshold, œÑp‚Äão‚Äãs‚Äãt‚Äã(k)=k‚ãÖmedianjs^j,\tau\_{post}(k)=k\cdot\operatorname\*{median}\_{j}\hat{s}\_{j},
   based on the MALA estimates of the, per-coordinate, posterior standard deviations

   |  |  |  |
   | --- | --- | --- |
   |  | s^j=sd‚Å°(wj(m)),j=1,‚Ä¶,p.\hat{s}\_{j}=\operatorname{sd}\bigl(w^{(m)}\_{j}\bigr),\qquad j=1,\dots,p. |  |

   The choice of our default kk is not unique, merely pragmatic. If the posterior for each wjw\_{j} were roughly Gaussian, then the
   condition

   |  |  |  |
   | --- | --- | --- |
   |  | |wj|‚â≥k‚ãÖsd^j|w\_{j}|\;\gtrsim\;k\cdot\widehat{\mathrm{sd}}\_{j} |  |

   is akin to demanding a |z||z|-score of about kk:

   * ‚Ä¢

     k=2k=2 corresponds roughly to the familiar ‚Äú95%95\%-ish‚Äù significance threshold; on the other hand,
   * ‚Ä¢

     k=3k=3 corresponds to a very exacting ‚Äú3‚ÄãœÉ3\sigma‚Äù rule.

   The results are reported in Table [1](https://arxiv.org/html/2512.22109v1#S7.T1 "Table 1 ‚Ä£ Item (a) ‚Ä£ Posterior-informed support selection and the tradeable portfolio ‚Ä£ 7 A case study: tracking the S&P 500 ‚Ä£ Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification") below

   | kk | |S||S| | massk‚Äãe‚Äãp‚Äãt\text{mass}\_{kept} | TEp‚Äãr‚Äãu‚Äãn‚Äãe‚Äãd\text{TE}\_{pruned} | TEd‚Äãe‚Äãb‚Äãi‚Äãa‚Äãs‚àó\text{TE}\_{debias}^{\ast} | TEF‚ÄãI‚ÄãS‚ÄãT‚ÄãA\text{TE}\_{FISTA} |
   | --- | --- | --- | --- | --- | --- |
   | 2.00 | 176 | 0.709 | 8.960e-04 | 2.392e-04 | 2.423e-04 |
   | 2.25 | 166 | 0.693 | 7.406e-04 | 2.583e-04 | 2.612e-04 |
   | 2.50 | 155 | 0.674 | 6.039e-04 | 2.792e-04 | 2.809e-04 |
   | 3.00 | 130 | 0.628 | 5.714e-04 | 3.308e-04 | 3.322e-04 |

   Table 1: We vary kk to change the posterior scale threshold œÑp‚Äão‚Äãs‚Äãt=k‚ãÖmedianjs^j\tau\_{post}=k\cdot\operatorname\*{median}\_{j}\hat{s}\_{j}.

   The ‚Äúmass kept‚Äù by SS in the MAP tail is reported as

   |  |  |  |
   | --- | --- | --- |
   |  | mass kept=‚àëj‚ààS|wMAP,j|‚àëj=1p|wMAP,j|.\text{mass kept}=\frac{\sum\_{j\in S}|w\_{\mathrm{MAP},j}|}{\sum\_{j=1}^{p}|w\_{\mathrm{MAP},j}|}. |  |
2. (b)

   We fix k=2.5k=2.5 for œÑp‚Äão‚Äãs‚Äãt,\tau\_{post}, and then vary the ‚Äúactivation probabilities‚Äù œÄ‚àó\pi^{\ast}. We report the results in Table [2](https://arxiv.org/html/2512.22109v1#S7.T2 "Table 2 ‚Ä£ Item (b) ‚Ä£ Posterior-informed support selection and the tradeable portfolio ‚Ä£ 7 A case study: tracking the S&P 500 ‚Ä£ Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification").

   | œÄ‚àó\pi^{\ast} | |S||S| | massk‚Äãe‚Äãp‚Äãt\text{mass}\_{kept} | TEp‚Äãr‚Äãu‚Äãn‚Äãe‚Äãd\text{TE}\_{pruned} | TEd‚Äãe‚Äãb‚Äãi‚Äãa‚Äãs‚àó\text{TE}\_{debias}^{\ast} | TEF‚ÄãI‚ÄãS‚ÄãT‚ÄãA\text{TE}\_{FISTA} |
   | --- | --- | --- | --- | --- | --- |
   | 0.50 | 159 | 0.683 | 6.541e-04 | 2.704e-04 | 2.722e-04 |
   | 0.60 | 157 | 0.679 | 6.341e-04 | 2.774e-04 | 2.790e-04 |
   | 0.65 | 155 | 0.674 | 6.039e-04 | 2.792e-04 | 2.809e-04 |
   | 0.70 | 149 | 0.663 | 5.394e-04 | 2.943e-04 | 2.953e-04 |
   | 0.75 | 146 | 0.656 | 5.164e-04 | 3.007e-04 | 3.014e-04 |
   | 0.80 | 144 | 0.652 | 5.154e-04 | 3.033e-04 | 3.042e-04 |

   Table 2: Changing the active set by varying œÄ‚àó\pi^{\ast} and measuring TE of the portfolios.

From the above tables, we see that the moderate choices (k,œÄ‚àó)=(2.5,0.65)(k,\pi^{\ast})=(2.5,0.65) (the framed row) give a good balance between sparsity and TE. Of course, the decision to use these values should not be seen as ‚Äúoptimised‚Äù, this is merely for a proof of concept demonstration. It is clear, though, that we can‚Äôt expect to squeeze more sparsity through œÄ‚àó\pi^{\ast}, and trying to do so through a higher value of kk is likely to negatively affect TE, and more seriously, lead to overfitting.

We summarise the results in the following table

| Parameters and results (FIT‚Äì1 window) | |
| --- | --- |
| |S||S| | 155 |
| #‚ÄãwMAP<0\#w\_{\text{MAP}}<0 | 139 |
| #‚Äãn‚Äãn‚Äãz‚ÄãwMAP\#\;nnz\,w\_{\text{MAP}} | 454 |
| T‚ÄãEMAP{TE}\_{\text{MAP}} | 7.156e-05 |
| T‚ÄãEPRUNED{TE}\_{\text{PRUNED}} | 6.039e-04 |
| T‚ÄãEDEBIAS‚àó{TE}\_{\text{DEBIAS}^{\ast}} | 2.792e-04 |
| T‚ÄãEFISTA{TE}\_{\text{FISTA}} | 2.809e-04 |

Table 3: Summary of the long-only active set SS and tracking-error diagnostics on the fit window.

We also calculate the sum of the weights after the estimation of the MAP. Recall that we have enforced the budget constraint in a soft way, happy to compromise for an error of the order of 10‚àí310^{-3} (set through œÑc\tau\_{c}); For the MAP itself, the sum of weights is 0.9980.998. The three portfolios based on the MAP (after thresholding) are already incorporating the hard constraint, so it comes as no surprise to report that the sum of the weights for all three is exactly 11.

Before filtering out the short positions from the active set, we checked its cardinality (corresponding to the same thresholds (k,œÄ‚àó)=(2.5,0.65),(k,\pi^{\ast})=(2.5,0.65), and recorded Swith negative=196.S\_{\text{with negative}}=196. In other words, the sign filter eliminated an additional 4141 names. For comparison, we also record the TE for the three portfolios when we allow negative weights.

| TE when allowing shorts (FIT‚Äì1 window) | |
| --- | --- |
| #‚Äãn‚Äãn‚Äãz‚ÄãwMAP\#\;nnz\,w\_{\text{MAP}} | 454 |
| |S||S| | 196 |
| #‚ÄãwMAP<0\#w\_{\text{MAP}}<0 | 139 |
| T‚ÄãEMAP{TE}\_{\text{MAP}} | 7.156e-05 |
| T‚ÄãEPRUNED{TE}\_{\text{PRUNED}} | 9.877e-04 |
| T‚ÄãEDEBIAS{TE}\_{\text{DEBIAS}} | 2.342e-04 |
| T‚ÄãEFISTA{TE}\_{\text{FISTA}} | 2.361e-04 |

Table 4: Summary of the long-short active set SS and tracking-error diagnostics on the fit window.

It is quite clear that eliminating the shorts and refitting on the new active set results in a small increase in TE, providing further support to the decision to produce long-only portfolios. Compared to the MAP, the increase in TE is significant, but of course, this is the price to pay for sparsity.

For the rest of this case study we will hold (and later rebalance) the refit-on-S-with-FISTA, wF‚ÄãI‚ÄãS‚ÄãT‚ÄãAw\_{FISTA} for short (but not to be confused with the unprocessed MAP, also found by FISTA). Below is a list of the top-25 assets as determined by our approach

| rank | ticker | w |
| --- | --- | --- |
| 1 | AAPL | +4.055e-02 |
| 2 | MSFT | +3.292e-02 |
| 3 | AMZN | +2.685e-02 |
| 4 | GOOGL | +2.556e-02 |
| 5 | UNH | +2.119e-02 |
| 6 | HD | +2.014e-02 |
| 7 | KO | +1.879e-02 |
| 8 | DIS | +1.831e-02 |
| 9 | META | +1.725e-02 |
| 10 | CVX | +1.667e-02 |
| 11 | WFC | +1.636e-02 |
| 12 | VZ | +1.582e-02 |
| 13 | MRK | +1.502e-02 |

| rank | ticker | w |
| --- | --- | --- |
| 14 | BRK-B | +1.384e-02 |
| 15 | MA | +1.355e-02 |
| 16 | MDT | +1.275e-02 |
| 17 | DD | +1.257e-02 |
| 18 | BAC | +1.241e-02 |
| 19 | JNJ | +1.231e-02 |
| 20 | JPM | +1.221e-02 |
| 21 | COST | +1.202e-02 |
| 22 | XOM | +1.165e-02 |
| 23 | INTC | +1.133e-02 |
| 24 | CSCO | +1.116e-02 |
| 25 | ADBE | +1.031e-02 |

Table 5: Top ww holdings (FISTA on SS).

### Performance out-of-sample

We now hold the constructed portfolio for 6 months (128 trading days) for what we refer to as the ‚ÄúHOLD-1‚Äù period, which starts on the first trading day after the end of the ‚ÄúFIT-1‚Äù window used for the construction. At the start of HOLD-1 we select wFISTAw\_{\mathrm{FISTA}} and hold it buy‚Äìand‚Äìhold over [tH1,start,tH1,end][t\_{\mathrm{H1,start}},t\_{\mathrm{H1,end}}]. We look at the realised TE (daily, RMSE of TE on a rolling 20-day window, and cumulative returns).

These are summarised in the following figures3331‚Äãbp=10‚àí41\,\text{bp}\,=10^{-4} in return units.

![[Uncaptioned image]](hold1_cum_returns.png)



Figure 7: Index tracking (cumulative returns).



![Refer to caption](RMSE_rolling_bp_hold1.png)


Figure 8: Rolling RMSE TE (20-days window) in bp units.

![Refer to caption](daily_TE_bp_hold1.png)


Figure 9: Daily TE in bp units.

We can clearly see excellent tracking performance, especially for the first 5 months.

For single-period strategies or myopic formulations with linear/fixed costs, this would be the last step of the workflow before investing in the portfolio. Additional considerations, such as different rules for thresholding for more/less sparsity, net and gross exposures via simple caps, etc, are possible, if desired.

To keep the focus on the computational and methodological aspects of our approach, we don‚Äôt pursue these further; nevertheless, all the information to post-process the portfolio for all of the above goals is already available from the previous steps. If more sparsity is desired, there are three main parameters (and combinations thereof) one can further tune: *first*, the gating parameter œÄ‚àó\pi^{\ast}: increasing it corresponds to requiring higher levels of confidence and will further sift the remaining assets. *Second*, the magnitude constraint parameter œÑp‚Äão‚Äãs‚Äãt\tau\_{post} which similarly decreases the active support when increased. We do, however, note that here we selected this parameter automatically learning from the long-MALA run, and we didn‚Äôt set it in an ad hoc way. *Third*, treating œÉ2\sigma^{2} as a TE-related parameter rather than noise variance (as we do in the rebalancing step), one could increase it by 1-2 orders of magnitude. SAPG will adapt to the new reality and select a parameter Œ∏^‚ãÜ\hat{\theta}\_{\star} that enforces even more sparsity

### 7.3 What if we don‚Äôt rebalance?

Before proceeding with the rebalancing step, we look at the performance of the original portfolio on HOLD-2. This is the second trading period, whose first day is the first trading day after the end of HOLD-1.

These are summarised in the following figures showing both periods

![[Uncaptioned image]](hold12_te_rmse20_bp.png)



Figure 10: Rolling RMSE TE in bp units.

![[Uncaptioned image]](daily_TE_bp_hold12.png)



Figure 11: Daily TE in bp units.



![[Uncaptioned image]](HOLD12_cum_returns.png)



Figure 12: Index tracking (cumulative returns).

We have clearly labelled the start of the HOLD-2 period, and replicated the performance plots from the first period.

The above graphs suggest that even if we had chosen not to rebalance, the tracking performance of the original portfolio remains extremely good, a year after its design. Of course, one should not rush to conclude that this will always be the case, after all, this was a rather non-turbulent period. However, it does suggest that our method is quite robust, and it also supports the rebalancing strategy we advocate for here, i.e., to construct new portfolios built upon the originally held.

### 7.4 Rebalancing

We frame the rebalancing problem as an optimisation problem with adjustment vector Œî‚Äãw\Delta w playing the role of the decision variable. The new allocation vector will be

|  |  |  |
| --- | --- | --- |
|  | wnew=wold+Œî‚Äãw,w\_{\text{new}}=w\_{\text{old}}+\Delta w, |  |

where woldw\_{\text{old}} is the portfolio held up to this point. To maintain the budget constraint on wneww\_{\text{new}}
we enforce the hard sum‚Äìzero constraint

|  |  |  |
| --- | --- | --- |
|  | 1‚ä§‚ÄãŒî‚Äãw=01^{\top}\Delta w=0 |  |

for Œî‚Äãw\Delta w so that the budget constraint is preserved exactly.

Using the same length as before, T=500T=500, we construct a second fitting window ending at the evaluation date of the first portfolio; we refer to this as the ‚ÄúFIT‚Äì2 ‚Äù window. We operate with the vector y2‚ààRTy\_{2}\in R^{T} of index returns, and the matrix R2‚àà‚ÑùT√ópR\_{2}\in\mathbb{R}^{T\times p} of asset returns.

As before, we centre:

|  |  |  |
| --- | --- | --- |
|  | y2,Œº=1T‚Äã‚àëty2,t,R2,Œº,j=1T‚Äã‚àëtR2,t‚Äãj,j‚àà{1,‚Ä¶,p}y2,c=y2‚àíy2,Œº‚ÄãùüèT,R2,c=R2‚àíùüèT‚ÄãR2,Œº‚ä§,\begin{split}y\_{2,\mu}&=\frac{1}{T}\sum\_{t}y\_{2,t},\quad R\_{2,\mu,j}=\frac{1}{T}\sum\_{t}R\_{2,tj},\qquad j\in\{1,\dots,p\}\\ y\_{2,c}&=y\_{2}-y\_{2,\mu}\mathbf{1}\_{T},\qquad R\_{2,c}=R\_{2}-\mathbf{1}\_{T}R\_{2,\mu}^{\top},\end{split} |  |

and build a new set of scales Œ±(2)\alpha^{(2)} based on R2,cR\_{2,c}. On FIT‚Äì2, we build a weighted Laplace prior on Œî‚Äãw\Delta w,

|  |  |  |
| --- | --- | --- |
|  | p‚Äã(Œî‚Äãw‚à£Œ∫)‚àùexp‚Å°(‚àíŒ∫‚ÄãSŒî‚Äã(Œî‚Äãw))‚ÄãŒπ{ùüè‚ä§‚ÄãŒî‚Äãw=0},SŒî‚Äã(Œî‚Äãw)=‚àëj=1pŒ±j(Œî)‚Äã|Œî‚Äãwj|.p(\Delta w\mid\kappa)\;\propto\;\exp\!\Bigl(-\kappa\,S\_{\Delta}(\Delta w)\Bigr)\;\mathbb{\iota}\_{\{\mathbf{1}^{\top}\Delta w=0\}},\qquad S\_{\Delta}(\Delta w)\;=\;\sum\_{j=1}^{p}\alpha^{(\Delta)}\_{j}\,|\Delta w\_{j}|. |  |

The base scales are recomputed from R2,cR\_{2,c} using the same
column-norm recipe as in FIT‚Äì1,

|  |  |  |
| --- | --- | --- |
|  | Œ±~j=‚ÄñR2,c(:,j)‚Äñ2T,Œ±~j‚ÜêŒ±~j/Œ±~¬Ø,\tilde{\alpha}\_{j}\;=\;\frac{\|R\_{2,c}^{(:,j)}\|\_{2}}{\sqrt{T}},\qquad\tilde{\alpha}\_{j}\leftarrow\tilde{\alpha}\_{j}/\overline{\tilde{\alpha}}, |  |

and we then *square* them for the rebalancing prior,

|  |  |  |
| --- | --- | --- |
|  | Œ±j(Œî)=(Œ±~j)2.\alpha^{(\Delta)}\_{j}\;=\;\bigl(\tilde{\alpha}\_{j}\bigr)^{2}. |  |

As before, we recorded the minimal, maximal, and mean weights as 0.297,12.011,10.297,12.011,1 respectively.

The working posterior on FIT--2 is

|  |  |  |
| --- | --- | --- |
|  | œÄŒ∫‚Äã(Œî‚Äãw‚à£data)‚àùexp‚Å°{‚àí12‚ÄãœÉŒî‚Äãw2‚Äã‚Äñy2,res‚àíR2,c‚ÄãŒî‚Äãw‚Äñ22‚àíŒ∫‚ÄãSŒî‚Äã(Œî‚Äãw)}‚ÄãŒπ{ùüè‚ä§‚ÄãŒî‚Äãw=0}.\pi\_{\kappa}(\Delta w\mid\text{data})\;\propto\;\exp\!\left\{-\frac{1}{2\sigma^{2}\_{\Delta w}}\,\bigl\|y\_{2,\mathrm{res}}-R\_{2,c}\Delta w\bigr\|\_{2}^{2}-\kappa\,S\_{\Delta}(\Delta w)\right\}\;\iota\_{\{\mathbf{1}^{\top}\Delta w=0\}}. |  |

Next, we calculate the MAD estimator for the (baseline) noise variance as described in Section [C](https://arxiv.org/html/2512.22109v1#A3 "Appendix C Noise variance estimation ‚Ä£ Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification"). For this window we calculated œÉ^Œî‚Äãw2=2.510‚ãÖ10‚àí9.\hat{\sigma}\_{\Delta w}^{2}=2.510\cdot 10^{-9}. Mirroring the earlier procedure (MAD, then SAPG, then MAP), we compute Œî‚ÄãwM‚ÄãA‚ÄãP\Delta w\_{MAP} for the baseline case. From the figure and the table below, we see that the smoothed objective does not produce exact zeros,

![[Uncaptioned image]](dw_map_hist_nz.png)



Figure 13:  Œî‚ÄãwM‚ÄãA‚ÄãP\Delta w\_{MAP}.



| bin |  | ‚Ñì1\ell\_{1} mass in bin | share (%) |
| --- | --- | --- | --- |
| 1 | [6.030√ó10‚àí9,‚Äâ3.090√ó10‚àí8][6.030\times 10^{-9},\,3.090\times 10^{-8}] | 2.661√ó10‚àí82.661\times 10^{-8} | 0.00 |
| 2 | [3.090√ó10‚àí8,‚Äâ1.583√ó10‚àí7][3.090\times 10^{-8},\,1.583\times 10^{-7}] | 7.870√ó10‚àí77.870\times 10^{-7} | 0.00 |
| 3 | [1.583√ó10‚àí7,‚Äâ8.110√ó10‚àí7][1.583\times 10^{-7},\,8.110\times 10^{-7}] | 2.704√ó10‚àí52.704\times 10^{-5} | 0.09 |
| 4 | [8.110√ó10‚àí7,‚Äâ4.155√ó10‚àí6][8.110\times 10^{-7},\,4.155\times 10^{-6}] | 4.395√ó10‚àí44.395\times 10^{-4} | 1.51 |
| 5 | [4.155√ó10‚àí6,‚Äâ2.129√ó10‚àí5][4.155\times 10^{-6},\,2.129\times 10^{-5}] | 1.421√ó10‚àí31.421\times 10^{-3} | 4.88 |
| 6 | [2.129√ó10‚àí5,‚Äâ1.091√ó10‚àí4][2.129\times 10^{-5},\,1.091\times 10^{-4}] | 4.824√ó10‚àí44.824\times 10^{-4} | 1.66 |
| 7 | [1.091√ó10‚àí4,‚Äâ5.589√ó10‚àí4][1.091\times 10^{-4},\,5.589\times 10^{-4}] | 5.527√ó10‚àí35.527\times 10^{-3} | 18.97 |
| 8 | [5.589√ó10‚àí4,‚Äâ2.863√ó10‚àí3][5.589\times 10^{-4},\,2.863\times 10^{-3}] | 2.123√ó10‚àí22.123\times 10^{-2} | 72.89 |

Table 6: Baseline (c=1)(c=1) ‚Ñì1\ell\_{1} mass share by |Œî‚Äãw||\Delta w| bin for the Œî‚Äãw\Delta w MAP estimate. We also record
  
(min|Œîw|=6.030e‚àí09,max|Œîw|=2.863e‚àí03)\min|\Delta w|=6.030e-09,\max|\Delta w|=2.863e-03).

Recall that we wish to *choose* a ‚Äúnoise level‚Äù that is consistent with the realised TE over the FIT‚Äì2 window for the existing portfolio woldw\_{\mathrm{old}}, but still
allows the prior to exert substantial influence. To this end we introduce a scalar multiplier c>0c>0 and consider the
family

|  |  |  |
| --- | --- | --- |
|  | œÉŒî‚Äãw2‚Äã(c)=c‚ÄãœÉŒî‚Äãw,base2,c‚ààùíû0,\sigma^{2}\_{\Delta w}(c)\;=\;c\,\sigma^{2}\_{\Delta w,\mathrm{base}},\qquad c\in\mathcal{C}\_{0}, |  |

for a grid ùíû0\mathcal{C}\_{0} of candidate values. For each value of candidate c,c, we employ SAPG to find the corresponding Œ∫‚Äã(c)\kappa(c). Note that c‚ÄãœÉ2c\,\sigma^{2} feeds into the Lipschitz constant of the likelihood term and thus directly affects the timestep of the MYULA chains involved. For the SAPG runs, we used 15000 iterations with the first 4000 treated as burn-in.

We then fix the pair
(œÉŒî‚Äãw2‚Äã(c),Œ∫‚ãÜ‚Äã(c))(\sigma^{2}\_{\Delta w}(c),\kappa\_{\star}(c)) and use FISTA (4000 iterations at most) to compute the corresponding
smoothed MAP

|  |  |  |
| --- | --- | --- |
|  | Œî‚ÄãwMAP‚Äã(c)=arg‚Å°minŒî‚Äãw‚àà‚Ñùp‚Å°Œ¶‚Äã(Œî‚Äãw;Œ∫‚ãÜ‚Äã(c),œÉŒî‚Äãw2‚Äã(c)),\Delta w\_{\mathrm{MAP}}(c)\;=\;\arg\min\_{\Delta w\in\mathbb{R}^{p}}\Phi\bigl(\Delta w;\,\kappa\_{\star}(c),\sigma^{2}\_{\Delta w}(c)\bigr), |  |

using a FISTA scheme on¬†([7.4](https://arxiv.org/html/2512.22109v1#S7.Ex93 "7.4 Rebalancing ‚Ä£ 7 A case study: tracking the S&P 500 ‚Ä£ Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification")) with the
budget‚Äìconstrained proximal map
proxŒªMY‚Äãg\mathrm{prox}\_{\lambda\_{\mathrm{MY}}g}.

The rebalanced portfolio
is then
wnew‚Äã(c)=wold+Œî‚ÄãwMAP‚Äã(c)w\_{\mathrm{new}}(c)=w\_{\mathrm{old}}+\Delta w\_{\mathrm{MAP}}(c),
automatically satisfying ùüè‚ä§‚Äãwnew‚Äã(c)=1\mathbf{1}^{\top}w\_{\mathrm{new}}(c)=1.

As before, we define
an *effective* cardinality

|  |  |  |
| --- | --- | --- |
|  | nnzeff‚Äã(c)=#‚Äã{j‚àà{1,‚Ä¶,p}:|Œî‚ÄãwMAPj‚Äã(c)|‚â•œÑeff},\mathrm{nnz}\_{\mathrm{eff}}(c)\;=\;\#\Bigl\{j\in\{1,\dots,p\}:\bigl|\Delta{w\_{\mathrm{MAP}}}\_{j}(c)\bigr|\geq\tau\_{\mathrm{eff}}\Bigr\}, |  |

where here œÑeff=10‚àí4\tau\_{\mathrm{eff}}=10^{-4} is a small fixed threshold chosen to ignore numerically negligible weight adjustments. We also record the
raw cardinality
nnzraw‚Äã(c)=#‚Äã{j:Œî‚ÄãwMAPj‚Äã(c)‚â†0}\mathrm{nnz}\_{\mathrm{raw}}(c)=\#\{j:\Delta{w\_{\mathrm{MAP}}}\_{j}(c)\neq 0\}
for diagnostic purposes, but all decisions are based on
nnzeff‚Äã(c)\mathrm{nnz}\_{\mathrm{eff}}(c).

For each c‚ààùíû0c\in\mathcal{C}\_{0} we evaluate both the TE
TEFIT2‚Äã(wnew‚Äã(c))\mathrm{TE}\_{\mathrm{FIT2}}(w\_{\mathrm{new}}(c)) and the effective
adjustment size nnzeff‚Äã(c)\mathrm{nnz}\_{\mathrm{eff}}(c). We then use our
scalar score

|  |  |  |
| --- | --- | --- |
|  | d‚Äã(c)=œïTE‚Äã(c)‚Äãwnnz‚Äã(c),d(c)\;=\;\phi\_{\mathrm{TE}}(c)\,w\_{\mathrm{nnz}}(c), |  |

where:

* ‚Ä¢

  We specify lower and upper fractions
  0<Œ≥lo<1<Œ≥hi0<\gamma\_{\mathrm{lo}}<1<\gamma\_{\mathrm{hi}} and set

  |  |  |  |
  | --- | --- | --- |
  |  | œïTE‚Äã(c)={1,if¬†‚ÄãŒ≥lo‚ÄãTEold‚â§TEFIT2‚Äã(wreb‚Äã(c))‚â§Œ≥hi‚ÄãTEold,0,otherwise.\phi\_{\mathrm{TE}}(c)\;=\;\begin{cases}1,&\text{if }\gamma\_{\mathrm{lo}}\,\mathrm{TE}\_{\mathrm{old}}\leq\mathrm{TE}\_{\mathrm{FIT2}}(w\_{\mathrm{reb}}(c))\leq\gamma\_{\mathrm{hi}}\,\mathrm{TE}\_{\mathrm{old}},\\[2.5pt] 0,&\text{otherwise.}\end{cases} |  |

  In our experiments we use Œ≥lo=0.2\gamma\_{\mathrm{lo}}=0.2 and
  Œ≥hi=1.2\gamma\_{\mathrm{hi}}=1.2, reflecting the view that (i) TE values
  much smaller than 0.2‚ÄãTEold0.2\,\mathrm{TE}\_{\mathrm{old}} are likely to
  correspond to overfitting on FIT‚Äì2, whereas (ii) TE values much larger than 1.2‚ÄãTEold1.2\,\mathrm{TE}\_{\mathrm{old}} indicate ineffective
  tracking.
* ‚Ä¢

  *Gaussian preference over effective cardinality.*
  Let nnzprev\mathrm{nnz}\_{\mathrm{prev}} denote the cardinality of the
  existing portfolio woldw^{\mathrm{old}}, and set a target adjustment
  size

  |  |  |  |
  | --- | --- | --- |
  |  | n‚ãÜ=Œ≥nnz‚Äãnnzprev,n\_{\star}\;=\;\gamma\_{\mathrm{nnz}}\,\mathrm{nnz}\_{\mathrm{prev}}, |  |

  with Œ≥nnz‚âà0.25\gamma\_{\mathrm{nnz}}\approx 0.25: we prefer to adjust only
  a moderate fraction of the currently held names. Given a scale
  parameter œÉnnz=5\sigma\_{\mathrm{nnz}}=5 (in units of ‚Äúnames‚Äù),
  we define

  |  |  |  |
  | --- | --- | --- |
  |  | wnnz‚Äã(c)=exp‚Å°(‚àí12‚Äã(nnzeff‚Äã(c)‚àín‚ãÜœÉnnz)2).w\_{\mathrm{nnz}}(c)\;=\;\exp\!\left(-\frac{1}{2}\left(\frac{\mathrm{nnz}\_{\mathrm{eff}}(c)-n\_{\star}}{\sigma\_{\mathrm{nnz}}}\right)^{2}\right). |  |

The score d‚Äã(c)d(c) is thus nonzero only for settings where the TE lies
in a plausible range, and among those, it favours configurations where
the effective number of adjusted names is neither too small nor too
large. For this run, we used the grid

|  |  |  |
| --- | --- | --- |
|  | ùíû0=[1.0,15,25,35,45,50,55,60,65,70.0,80.0,90.0,100.0].\mathcal{C}\_{0}=[1.0,15,25,35,45,50,55,60,65,70.0,80.0,90.0,100.0]. |  |

The results are reported in Table [7](https://arxiv.org/html/2512.22109v1#S7.T7 "Table 7 ‚Ä£ 7.4 Rebalancing ‚Ä£ 7 A case study: tracking the S&P 500 ‚Ä£ Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification").

| cc | œÉŒî‚Äãw2‚Äã(c)\sigma\_{\Delta w}^{2}(c) | Œ∫EB‚Äã(c)\kappa\_{\mathrm{EB}}(c) | TEFIT2\mathrm{TE}\_{\mathrm{FIT2}} | #‚Äã{Œî‚Äãw‚â†0}\#\{\Delta w\neq 0\} | nnzeff\mathrm{nnz}\_{\mathrm{eff}} | œïTE\phi\_{\mathrm{TE}} | wnnzw\_{\mathrm{nnz}} | score |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 1.000 | 2.510e-09 | 7.014e+02 | 7.8032e-05 | 478 | 436 | 0.000 | 0.000 | 0.000 |
| 15.000 | 3.765e-08 | 9.616e+02 | 1.8504e-04 | 478 | 243 | 1.000 | 0.000 | 0.000 |
| 25.000 | 6.275e-08 | 1.267e+03 | 2.5863e-04 | 478 | 152 | 1.000 | 0.000 | 0.000 |
| 35.000 | 8.784e-08 | 1.442e+03 | 3.0548e-04 | 478 | 93 | 1.000 | 0.000 | 0.000 |
| 45.000 | 1.129e-07 | 1.694e+03 | 3.4440e-04 | 478 | 62 | 1.000 | 0.000 | 0.000 |
| 50.000 | 1.255e-07 | 1.699e+03 | 3.5506e-04 | 478 | 54 | 1.000 | 0.010 | 0.010 |
| 55.000 | 1.380e-07 | 1.671e+03 | 3.6336e-04 | 478 | 45 | 1.000 | 0.458 | 0.458 |
| 60.000 | 1.506e-07 | 1.712e+03 | 3.7475e-04 | 478 | 34 | 1.000 | 0.637 | 0.637 |
| 65.000 | 1.631e-07 | 1.713e+03 | 3.8197e-04 | 478 | 25 | 1.000 | 0.023 | 0.023 |
| 70.000 | 1.757e-07 | 1.714e+03 | 3.8711e-04 | 478 | 21 | 1.000 | 0.002 | 0.002 |
| 80.000 | 2.008e-07 | 1.713e+03 | 3.9469e-04 | 478 | 16 | 1.000 | 0.000 | 0.000 |
| 90.000 | 2.259e-07 | 1.714e+03 | 4.0019e-04 | 478 | 7 | 1.000 | 0.000 | 0.000 |
| 100.000 | 2.510e-07 | 1.714e+03 | 4.0385e-04 | 478 | 4 | 1.000 | 0.000 | 0.000 |

Table 7: Grid over noise scale factors cc for the rebalancing model on the FIT‚Äì2 window. For each cc we set œÉŒî‚Äãw2‚Äã(c)=c‚ÄãœÉŒî‚Äãw,base2\sigma\_{\Delta w}^{2}(c)=c\,\sigma\_{\Delta w,\mathrm{base}}^{2}, recompute Œ∫E‚ÄãB‚Äã(c)\kappa\_{EB}(c) via SAPG on the smoothed objective, and solve the corresponding MAP problem. The score combines a TE-acceptance window and a Gaussian preference for moderate effective adjustment size nnzeff‚Äã(Œî‚Äãw)\mathrm{nnz}\_{\mathrm{eff}}(\Delta w).

Once c‚àóc^{\*} has been selected on the grid, we lock in the corresponding
noise variance and sparsity level,

|  |  |  |
| --- | --- | --- |
|  | œÉŒî‚Äãw,final2=œÉŒî‚Äãw2‚Äã(c‚àó)=1.506‚Äãe‚àí07,Œ∫‚ãÜ,final=Œ∫‚ãÜ‚Äã(c‚àó)=1.712‚Äãe+03,\sigma^{2}\_{\Delta w,\mathrm{final}}\;=\;\sigma^{2}\_{\Delta w}(c^{\*})=1.506e-07,\qquad{\kappa\_{\star}}\_{,{\text{final}}}\;=\;\kappa\_{\star}(c^{\*})=1.712e+03, |  |

together with the associated smoothed MAP rebalancing move
Œî‚ÄãwMAP=Œî‚ÄãwMAP‚Äã(c‚àó)\Delta w\_{\mathrm{MAP}}=\Delta w\_{\mathrm{MAP}}(c^{\*}) and
rebalanced portfolio wnew=wnew‚Äã(c‚àó)w\_{\mathrm{new}}=w\_{\mathrm{new}}(c^{\*}).
These now define a fixed target posterior for uncertainty quantification
on FIT‚Äì2. In particular, the subsequent (again preconditioned) MALA sampler is constructed around
Œ¶‚Äã(Œî‚Äãw;Œ∫‚ãÜ,final,œÉŒî‚Äãw,final2)\Phi\left(\Delta w;{\kappa\_{\star}}\_{,{\text{final}}},\sigma^{2}\_{\Delta w,\mathrm{final}}\right)
and initialised at Œî‚ÄãwMAP\Delta w\_{\mathrm{MAP}}.

We use œÉ2=1.506‚Äãe‚àí07,Œ∫=1.712‚Äãe+03,ŒªM‚ÄãY=7.997‚Äãe‚àí09,\sigma^{2}=1.506e-07,\kappa=1.712e+03,\lambda\_{MY}=7.997e-09, and from the preconditioning, we calculate Lp‚Äãr‚Äãe=1.415‚Äãe+02L\_{pre}=1.415e+02 and with it a baseline timestep Œ≥0(p‚Äãr‚Äãe)=3.180‚Äãe‚àí03.{\gamma\_{0}}\_{(pre)}=3.180e-03. As before, we run a few short chains to determine a timestep that will result in a good acceptance rate ‚àº0.60.\sim 0.60.

| Timestep | Acceptance Rate |
| --- | --- |
| 1.240√ó10‚àí31.240\times 10^{-3} | 0.891 |
| 1.550√ó10‚àí31.550\times 10^{-3} | 0.865 |
| 1.937√ó10‚àí31.937\times 10^{-3} | 0.829 |
| 2.422√ó10‚àí32.422\times 10^{-3} | 0.779 |
| 3.027√ó10‚àí33.027\times 10^{-3} | 0.715 |
| 3.784√ó10‚àí33.784\times 10^{-3} | 0.630 |

Table 8: Tuning (Œ≥\gamma) and acceptance rate.

We run a long (250000 (thin=6), 50000 burn-in) MALA chain using a timestep of 3.784√ó10‚àí33.784\times 10^{-3}, which resulted in an acceptance rate of 0.6330.633. As before, we record ACF/ESS for mixing purposes, e.g., see the figure below for some ACF plots.

![Refer to caption](dw_mala_acf_TE_postburn.png)


(a) ACF of TE.

![Refer to caption](dw_mala_acf_dw_j148_postburn.png)


(b) ACF of asset 148.

![Refer to caption](dw_mala_acf_dw_j359_postburn.png)


(c) ACF of asset 359.

Figure 14: Post-burn ACF diagnostics for the Œî‚Äãw\Delta w MALA.

Following the scale-based thresholding idea discussed earlier, we define a global posterior
scale threshold

|  |  |  |
| --- | --- | --- |
|  | œÑpost=k‚ãÖmedian‚Äã(sd^1,‚Ä¶,sd^p),k>0,\tau\_{\mathrm{post}}\;=\;k\cdot\mathrm{median}\bigl(\widehat{\mathrm{sd}}\_{1},\dots,\widehat{\mathrm{sd}}\_{p}\bigr),\qquad k>0, |  |

and use both œÑpost\tau\_{\mathrm{post}} and the activation probabilities
œÄ^j\hat{\pi}\_{j} to gate which coordinates are eligible for rebalancing.

Let Œî‚ÄãwMAP\Delta w\_{\mathrm{MAP}} denote the smoothed MAP solution for the
rebalancing model at the selected noise scale c‚àóc^{\ast}.
We recall the definitions

|  |  |  |  |
| --- | --- | --- | --- |
|  | SœÑ\displaystyle S\_{\tau} | ={j:|Œî‚ÄãwMAPj|‚â•œÑpost},\displaystyle=\bigl\{j:|\Delta{w\_{\mathrm{MAP}}}\_{j}|\,\geq\,\tau\_{\mathrm{post}}\bigr\}, |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | SœÄ‚Äã(œÄ‚ãÜ)\displaystyle S\_{\pi}(\pi^{\star}) | ={j:œÄ^j‚â•œÄ‚ãÜ},\displaystyle=\bigl\{j:\hat{\pi}\_{j}\,\geq\,\pi^{\star}\bigr\}, |  |

where œÄ‚ãÜ‚àà(0,1)\pi^{\star}\in(0,1) is a user‚Äìchosen activation probability
threshold. The intersection

|  |  |  |
| --- | --- | --- |
|  | Srule‚Äã(œÄ‚ãÜ)=SœÑ‚à©SœÄ‚Äã(œÄ‚ãÜ)S\_{\mathrm{rule}}(\pi^{\star})\;=\;S\_{\tau}\,\cap\,S\_{\pi}(\pi^{\star}) |  |

collects those coordinates that are simultaneously ‚Äúlarge‚Äù in the MAP
sense and frequently active under the posterior.444Other ideas are also possible: for example, one could form ‚Äúconfidence intervals (CI)‚Äù based on the samples obtained and design a rule such as ‚Äúif the 95% CI for asset jj contains 0, we declare Œî‚Äãwj=0.\Delta w\_{j}=0. Finally, while negative entries in Œî‚Äãw\Delta w are meaningful, under the mandate of no-shorts, one also needs to impose that wn‚Äãe‚Äãw,j=wo‚Äãl‚Äãd,j+(Œî‚ÄãwMAP)j‚â•0,w\_{new,j}=w\_{old,j}+(\Delta{w\_{\mathrm{MAP}}})\_{j}\geq 0, clipping to zero if not.

In our experiments, we fix k=2.5k=2.5 in the definition of
œÑpost\tau\_{\mathrm{post}} and use œÄ‚ãÜ=0.8\pi^{\star}=0.8 as a high‚Äìconfidence
activation threshold. The corresponding posterior scale
œÑpost\tau\_{\mathrm{post}} and the resulting cardinalities
#‚ÄãSœÑ,#‚ÄãSœÄ‚Äã(œÄ‚ãÜ)\#S\_{\tau},\#S\_{\pi}(\pi^{\star}) and
#‚Äã(SœÑ‚à©SœÄ‚Äã(œÄ‚ãÜ))\#\bigl(S\_{\tau}\cap S\_{\pi}(\pi^{\star})\bigr) are summarised in
Table¬†[9](https://arxiv.org/html/2512.22109v1#S7.T9 "Table 9 ‚Ä£ 7.4 Rebalancing ‚Ä£ 7 A case study: tracking the S&P 500 ‚Ä£ Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification").

| œÄ‚ãÜ\pi^{\star} | kk in œÑpost\tau\_{\mathrm{post}} | #‚ÄãSœÑ\#S\_{\tau} | #‚ÄãSœÄ‚ãÜ\#S\_{\pi^{\star}} | #‚Äã(SœÑ‚àßSœÄ‚ãÜ)\#(S\_{\tau}\wedge S\_{\pi^{\star}}) |
| --- | --- | --- | --- | --- |
| 0.60 | 2.50 | 2 | 1 | 1 |
| 0.70 | 2.50 | 2 | 0 | 0 |
| 0.80 | 2.50 | 2 | 0 | 0 |
| 0.90 | 2.50 | 2 | 0 | 0 |

Table 9: Summary of activation sets for the Œî‚Äãw\Delta w rebalancing step. The posterior scale threshold is œÑpost=k‚Äãmedian‚Äã(sd^j)\tau\_{\mathrm{post}}=k\,\mathrm{median}(\widehat{\mathrm{sd}}\_{j}) with kk given in the table. For each œÄ‚ãÜ\pi^{\star} we report three cardinalities: the œÑ\tau-only rule, the œÄ‚ãÜ\pi^{\star}-only rule, and the intersection œÑ‚àßœÄ‚ãÜ\tau\wedge\pi^{\star}.



| Ticker | jj | wjoldw\_{j}^{\mathrm{old}} | Œî‚ÄãwjMAP\Delta w\_{j}^{\mathrm{MAP}} | wjneww\_{j}^{\mathrm{new}} | œÄ^j\hat{\pi}\_{j} |
| --- | --- | --- | --- | --- | --- |
| DD | 148 | +1.2572e-02 | -2.8633e-03 | +9.7083e-03 | 0.656 |
| PG | 359 | +0.0000e+00 | +2.6046e-03 | +2.6046e-03 | 0.588 |

Table 10: Active coordinates under the œÑpost\tau\_{\mathrm{post}}-only rule for the Œî‚Äãw\Delta w rebalancing model. The columns report the ticker, coordinate index, previous weight wjoldw\_{j}^{\mathrm{old}}, proposed change Œî‚ÄãwjMAP\Delta w\_{j}^{\mathrm{MAP}}, updated weight wjnew=wjold+Œî‚ÄãwjMAPw\_{j}^{\mathrm{new}}=w\_{j}^{\mathrm{old}}+\Delta w\_{j}^{\mathrm{MAP}}, and the posterior activation probability œÄ^j=‚Ñô‚Äã(|Œî‚Äãwj|‚â•œÑpost)\hat{\pi}\_{j}=\mathbb{P}(|\Delta w\_{j}|\geq\tau\_{\mathrm{post}}).

In this instance, our gating metrics have identified two tickers for rebalancing consideration: there is a proposed reduction in the weight of the asset with index 148 (backed up by a probability of ‚âà66%\approx 66\%), and a proposal for opening a position in the (previously inactive) asset with index 359.

For demonstration only, and to show how to make the process fully automatic (up to the input of œÄ‚ãÜ\pi^{\star}), we adopt the following *UQ‚Äìgated rule* for rebalancing in the
Œî‚Äãw\Delta w stage:

* ‚Ä¢

  Let Srule=Srule‚Äã(œÄ‚ãÜ)S\_{\mathrm{rule}}=S\_{\mathrm{rule}}(\pi^{\star}) with
  œÄ‚ãÜ=0.8\pi^{\star}=0.8.
* ‚Ä¢

  With the sum-to-zero constraint, a nontrivial rebalance needs at least two names (decrease one, increase another) unless you allow cash/leverage. If #‚ÄãSrule<2\#S\_{\mathrm{rule}}<2, we declare the posterior evidence too weak or too concentrated to justify a structural change and
  perform no rebalance, i.e. we keep wnew=woldw\_{\mathrm{new}}=w\_{\mathrm{old}}.
* ‚Ä¢

  If #‚ÄãSrule‚â•2\#S\_{\mathrm{rule}}\geq 2, we construct an implementable
  Œî‚Äãw\Delta w confined to SruleS\_{\mathrm{rule}} and adjust the portfolio
  only on that active set.

In this case, we are not confident enough in the proposed moves, so our rules will preserve the current portfolio. This is, of course, up to the trader to decide, but the rationale here was to discourage many moves without ‚Äústrong signals‚Äù.

| Ticker | jj | wjoldw\_{j}^{\mathrm{old}} | Œî‚ÄãwjMAP\Delta w\_{j}^{\mathrm{MAP}} | wjneww\_{j}^{\mathrm{new}} | œÄ^j\hat{\pi}\_{j} |
| --- | --- | --- | --- | --- | --- |
| *No active coordinates under this gating rule.* | | | | | |

Table 11: Active coordinates under the œÑpost‚àßœÄ‚ãÜ=0.80\tau\_{\mathrm{post}}\wedge\pi^{\star}=0.80 rule for the Œî‚Äãw\Delta w rebalancing model. The columns report the ticker, coordinate index, previous weight wjoldw\_{j}^{\mathrm{old}}, proposed change Œî‚ÄãwjMAP\Delta w\_{j}^{\mathrm{MAP}}, updated weight wjnew=wjold+Œî‚ÄãwjMAPw\_{j}^{\mathrm{new}}=w\_{j}^{\mathrm{old}}+\Delta w\_{j}^{\mathrm{MAP}}, and the posterior activation probability œÄ^j=‚Ñô‚Äã(|Œî‚Äãwj|‚â•œÑpost)\hat{\pi}\_{j}=\mathbb{P}(|\Delta w\_{j}|\geq\tau\_{\mathrm{post}}).

We highlight here the important role of the UQ metrics in the decision-making stage, which in turn necessitates high-quality samples.

### 7.5 One more trading period. What if we rebalanced?

At the end of the HOLD‚Äì1 period, based on the above rebalancing considerations, we decide on whether and how to adjust our portfolio before the next trading period, HOLD‚Äì2.

As our rules indicated, no changes were made, and our portfolio remains wF‚ÄãI‚ÄãS‚ÄãT‚ÄãAw\_{FISTA}, which was designed using FIT‚Äì1 data, held in HOLD‚Äì1, and has not been adjusted based on the FIT‚Äì2 data or our gating rules. However, we decided also to hold a second portfolio, the one formed by following the weak rebalancing suggestions from before. Given that the magnitude of the proposed changes for the two assets is nearly the same and in the opposite direction, we simply open a position in ‚ÄúPG‚Äù and reduce the weight of ‚ÄúDD‚Äù by the same amount. This ensures that the budget constraint is satisfied, but now this portfolio, referred to as ‚ÄúRebalanced‚Äù, has an additional asset.

These are summarised in the following figures

![[Uncaptioned image]](hold2_cumret_overlay.png)



Figure 15: Index tracking (cumulative returns).



![Refer to caption](hold2_te_rmse20_bp.png)


Figure 16: Rolling RMSE TE (20-day window) in bp units.

![Refer to caption](hold2_te_daily_bp.png)


Figure 17: Daily TE in bp units.

We see that the rebalanced portfolio is nearly identical to the original one in terms of tracking performance, suggesting that the effect of the moves was minimal in both TE and returns.

### 7.6 Rebalancing considerations for the next period

At the end of HOLD‚Äì2, we create again a T=500T=500 window ending on the last day of HOLD‚Äì2 (we refer to this period as ‚ÄúFIT‚Äì3‚Äù), and submit both of these portfolios to the same pipeline to determine the need for rebalancing. As this mirrors very closely the steps shown for FIT‚Äì2, we only show the more interesting results and the output of the gating rules. We abuse notation and reuse ‚ÄúŒî‚Äãw\Delta w‚Äù to signify the vector of adjustments from the current portfolio.

### The original portfolio

| œÄ‚ãÜ\pi^{\star} | kk in œÑpost\tau\_{\mathrm{post}} | #‚ÄãSœÑ\#S\_{\tau} | #‚ÄãSœÄ‚ãÜ\#S\_{\pi^{\star}} | #‚Äã(SœÑ‚àßSœÄ‚ãÜ)\#(S\_{\tau}\wedge S\_{\pi^{\star}}) |
| --- | --- | --- | --- | --- |
| 0.60 | 2.50 | 11 | 1 | 1 |
| 0.70 | 2.50 | 11 | 0 | 0 |
| 0.80 | 2.50 | 11 | 0 | 0 |
| 0.90 | 2.50 | 11 | 0 | 0 |

Table 12: Summary of activation sets for the Œî‚Äãw\Delta w rebalancing step on the FIT‚Äì3 window. The posterior scale threshold is œÑpost=k‚Äãmedian‚Äã(sd^j)\tau\_{\mathrm{post}}=k\,\mathrm{median}(\widehat{\mathrm{sd}}\_{j}) with kk given in the table. For each œÄ‚ãÜ\pi^{\star} we report three cardinalities: the œÑ\tau-only rule, the œÄ‚ãÜ\pi^{\star}-only rule, and the intersection œÑ‚àßœÄ‚ãÜ\tau\wedge\pi^{\star}.



| Ticker | jj | wjoldw\_{j}^{\mathrm{old}} | Œî‚ÄãwjMAP\Delta w\_{j}^{\mathrm{MAP}} | wjneww\_{j}^{\mathrm{new}} | œÄ^j\hat{\pi}\_{j} |
| --- | --- | --- | --- | --- | --- |
| AFL | 8 | +6.9100e-03 | -4.2257e-03 | +2.6843e-03 | 0.521 |
| AMT | 27 | +2.2742e-03 | +2.7703e-03 | +5.0445e-03 | 0.352 |
| KO | 110 | +1.8787e-02 | -2.1971e-03 | +1.6590e-02 | 0.489 |
| DD | 148 | +1.2572e-02 | -3.3547e-03 | +9.2170e-03 | 0.668 |
| XOM | 176 | +1.1652e-02 | -3.6003e-03 | +8.0522e-03 | 0.290 |
| GPC | 201 | +5.8994e-03 | -2.4848e-03 | +3.4145e-03 | 0.414 |
| MKC | 289 | +0.0000e+00 | +2.5596e-03 | +2.5596e-03 | 0.317 |
| MCD | 290 | +2.3230e-03 | +3.4824e-03 | +5.8054e-03 | 0.493 |
| PG | 359 | +0.0000e+00 | +3.4678e-03 | +3.4678e-03 | 0.571 |
| SNPS | 407 | +0.0000e+00 | +3.2882e-03 | +3.2882e-03 | 0.140 |
| TRV | 428 | +0.0000e+00 | +3.4885e-03 | +3.4885e-03 | 0.432 |

Table 13: Active coordinates under the œÑpost\tau\_{\mathrm{post}}-only rule for the Œî‚Äãw\Delta w rebalancing model on FIT‚Äì3. The columns report the ticker, coordinate index, previous weight wjoldw\_{j}^{\mathrm{old}}, proposed change Œî‚ÄãwjMAP\Delta w\_{j}^{\mathrm{MAP}}, updated weight wjnew=wjold+Œî‚ÄãwjMAPw\_{j}^{\mathrm{new}}=w\_{j}^{\mathrm{old}}+\Delta w\_{j}^{\mathrm{MAP}}, and the posterior activation probability œÄ^j=‚Ñô‚Äã(|Œî‚Äãwj|‚â•œÑpost)\hat{\pi}\_{j}=\mathbb{P}(|\Delta w\_{j}|\geq\tau\_{\mathrm{post}}).

Interestingly, the two assets we decided not to adjust (for the original portfolio), namely ‚ÄúDD‚Äù and ‚ÄúPG‚Äù are again begging to be considered. Still, under our playbook, there is not enough evidence to change their weights (or any of the others).

### The rebalanced portfolio

We assume now that we have been holding the ‚Äúrebalanced‚Äù portfolio, for which, as an experiment, we previously decided to act on somewhat weak rebalancing suggestions, and we reduced the weight of ‚ÄúDD‚Äù while taking a long position in ‚ÄúPG‚Äù.

| œÄ‚ãÜ\pi^{\star} | kk | #‚ÄãSœÑ\#S\_{\tau} | #‚ÄãSœÄ‚ãÜ\#S\_{\pi^{\star}} | #‚Äã(SœÑ‚àßSœÄ‚ãÜ)\#(S\_{\tau}\wedge S\_{\pi^{\star}}) |
| --- | --- | --- | --- | --- |
| 0.60 | 2.50 | 11 | 1 | 1 |
| 0.70 | 2.50 | 11 | 0 | 0 |
| 0.80 | 2.50 | 11 | 0 | 0 |
| 0.90 | 2.50 | 11 | 0 | 0 |

Table 14: Summary of activation sets for the Œî‚Äãw\Delta w rebalancing step on the FIT‚Äì3 window. The posterior scale threshold is œÑpost=k‚Äãmedian‚Äã(sd^j)\tau\_{\mathrm{post}}=k\,\mathrm{median}(\widehat{\mathrm{sd}}\_{j}). For each œÄ‚ãÜ\pi^{\star} we report the œÑ\tau-only rule, the œÄ‚ãÜ\pi^{\star}-only rule, and the intersection œÑ‚àßœÄ‚ãÜ\tau\wedge\pi^{\star}.



| Ticker | jj | wjoldw\_{j}^{\mathrm{old}} | Œî‚ÄãwjMAP\Delta w\_{j}^{\mathrm{MAP}} | wjneww\_{j}^{\mathrm{new}} | œÄ^j\hat{\pi}\_{j} |
| --- | --- | --- | --- | --- | --- |
| AFL | 8 | +6.9100e-03 | -4.2257e-03 | +2.6843e-03 | 0.635 |
| AMT | 27 | +2.2742e-03 | +2.7703e-03 | +5.0445e-03 | 0.403 |
| KO | 110 | +1.8787e-02 | -2.1971e-03 | +1.6590e-02 | 0.566 |
| DD | 148 | +9.9678e-03 | -3.3547e-03 | +6.6132e-03 | 0.216 |
| XOM | 176 | +1.1652e-02 | -3.6003e-03 | +8.0522e-03 | 0.301 |
| GPC | 201 | +5.8994e-03 | -2.4848e-03 | +3.4145e-03 | 0.341 |
| MKC | 289 | +0.0000e+00 | +2.5596e-03 | +2.5596e-03 | 0.312 |
| MCD | 290 | +2.3230e-03 | +3.4824e-03 | +5.8054e-03 | 0.528 |
| PG | 359 | +2.6038e-03 | +3.4678e-03 | +6.0716e-03 | 0.338 |
| SNPS | 407 | +0.0000e+00 | +3.2882e-03 | +3.2882e-03 | 0.202 |
| TRV | 428 | +0.0000e+00 | +3.4885e-03 | +3.4885e-03 | 0.519 |

Table 15: Active coordinates under the œÑpost\tau\_{\mathrm{post}}-only rule for the Œî‚Äãw\Delta w rebalancing model on FIT‚Äì3. Columns: ticker, index jj, wjoldw\_{j}^{\mathrm{old}}, Œî‚ÄãwjMAP\Delta w\_{j}^{\mathrm{MAP}}, wjneww\_{j}^{\mathrm{new}}, and œÄ^j\hat{\pi}\_{j}.

Once more, we see no strong indication to rebalance, based on the reported œÄ^j\hat{\pi}\_{j}.

It is unsurprising that after this long period, there are more suggestions for rebalancing (for both portfolios), but encouragingly, involving the same names for two closely-related portfolios. At the same time, the suggested moves are small and not supported by high ‚Äúactivation probabilities.‚Äù Ultimately, this comes down to the data and the market conditions, and of course, the trading mentality of individual investors. For a more complete analysis, the explicit incorporation of transaction costs and net returns should be embedded in this process.

## 8 Discussion

We have proposed a Bayesian framework for sparse index tracking that
combines empirical-Bayes calibration, proximal MCMC and UQ-informed
decision rules for portfolio construction and rebalancing. From an
operational-research perspective, the main advantages are:

* ‚Ä¢

  *Integrated tuning and inference.* The sparsity parameter is
  learned from the data via SAPG, avoiding external cross-validation or
  ad hoc grid searches, and yielding a coherent probabilistic model.
* ‚Ä¢

  *Uncertainty-aware decisions.* Posterior samples inform both
  the choice of active assets and the gating of rebalancing moves. This
  supports decisions about whether to trade at all, and by how much, in a
  transparent way.
* ‚Ä¢

  *Practical implementability.* The final portfolios satisfy
  hard budget constraints and long-only requirements, with explicit control
  over the number of names and the size of adjustments, linking directly
  to transaction costs and operational complexity.

The S&P¬†500 case study shows that the method delivers competitive
tracking performance with relatively sparse portfolios and, in the
period considered, suggests a cautious rebalancing stance: posterior
uncertainty does not justify large structural changes. Of course, in more turbulent periods or with different tuning choices, the same framework would support more active rebalancing.

Several extensions are natural. First, transaction costs and net returns
could be incorporated explicitly, for example via linear or fixed cost
terms in the likelihood or prior, leading to cost-aware rebalancing
rules. Second, alternative sparsity priors‚Äîsuch as group penalties,
structured sparsity or spike-and-slab formulations‚Äîcould be explored
within the same proximal MCMC and SAPG framework. Third, the approach
extends directly to other regression-based feature selection, or inverse problems with equality constraints, beyond portfolio optimisation, where one wishes to combine regularisation, empirical-Bayes calibration and uncertainty quantification.

Finally, our focus here has been on an empirical-Bayes treatment of the
global sparsity scale. Fully hierarchical specifications, where Œ∏\theta
and related hyperparameters are assigned priors and sampled jointly
with ww, can offer richer uncertainty statements at the cost of additional computational complexity. This is the subject of ongoing work.

## Appendix A Stochastic Approximation Proximal Gradient algorithm (SAPG)

How to appropriately select a regularisation parameter is a well-known problem in the treatment of ill-posed inverse problems (but also in feature selection in statistical settings). For optimisation approaches using an ‚Ñì1\ell\_{1}-penalty (e.g., such as in (?, ?, ?, ?, ?), the parameter is typically chosen in a way that guarantees the required level of sparsity. I.e., one decides in advance what cardinality to target for their portfolio, and adjusts the tuning parameter so that this is the sparsity level enforced. For the index tracking problem,
a TE vs sparsity trade-off curve is commonly computed to help choose a good operating point. Either way, a large number of optimisation problems need to be solved.

Under an empirical Bayesian paradigm, the regularization parameter Œ∏‚ààŒò,\theta\in\Theta, (for some convex compact set Œò\Theta) is estimated directly from the observed data yy, for example, by maximum marginal likelihood estimation. That is, we compute

|  |  |  |  |
| --- | --- | --- | --- |
|  | Œ∏‚ãÜ‚ààargmaxŒ∏‚ààŒò‚Äãp‚Äã(y|Œ∏),\theta\_{\star}\in\underset{\theta\in\Theta}{\mathrm{argmax}}~p(y|\theta)\,, |  | (32) |

where, for any Œ∏‚ààŒò,\theta\in\Theta, the marginal likelihood p‚Äã(y|Œ∏)p(y|\theta) is given, by

|  |  |  |  |
| --- | --- | --- | --- |
|  | p‚Äã(y|Œ∏)=‚à´‚Ñùpp‚Äã(y|w)‚Äãp‚Äã(w|Œ∏)‚Äãùëëw.p(y|\theta)=\int\_{\mathbb{R}^{p}}p(y|w)\,p(w|\theta)\,dw\;. |  | (33) |

Given Œ∏‚ãÜ\theta\_{\star}, empirical Bayesian approaches base inferences on the pseudo-posterior distribution w‚Ü¶p‚Äã(w|y,Œ∏‚ãÜ)w\mapsto p(w|y,\theta\_{\star}), which, for any w‚àà‚Ñùpw\in\mathbb{R}^{p} is given by

|  |  |  |  |
| --- | --- | --- | --- |
|  | p(w|y,Œ∏‚ãÜ)=exp[‚àífy(w)‚àíŒ∏‚ãÜg(w)]/‚à´‚Ñùpexp[‚àífy(w)‚àíŒ∏‚ãÜg(w)]dw.p(w|y,\theta\_{\star})=\left.\exp[-f\_{y}(w)-\theta\_{\star}g(w)]\middle/\int\_{\mathbb{R}^{p}}\exp[-f\_{y}(w)-\theta\_{\star}g(w)]\;dw\right.\;. |  | (34) |

To be more specific, once SAPG has found Œ∏‚ãÜ\theta\_{\star}, we will calculate using FISTA (?, ?), the MAP, namely,

|  |  |  |
| --- | --- | --- |
|  | w^Œ∏‚ãÜ,MAP‚ààarg‚Äãminw‚àà‚Ñùp‚Å°{fy‚Äã(w)+Œ∏‚ãÜ‚Äãg‚Äã(w)},\hat{w}\_{\theta\_{\star},\,\text{MAP}}\in\operatorname\*{arg\,min}\_{w\in\mathbb{R}^{p}}\left\{f\_{y}(w)+\theta\_{\star}\;g(w)\right\}, |  |

before exploring the posterior and reporting estimates. UQ-gating and size-informed hard thresholding will build upon the MAP to produce the tracking portfolio to hold.

We adopt the empirical Bayesian approach of (?, ?)
for the automatic, completely unsupervised selection of the scalar parameter controlling the sparsity level. The maximum marginal likelihood estimation SAPG is a stochastic proximal gradient algorithm driven by proximal Markov chain Monte Carlo samplers of MYULA-type. The method is highly efficient, easy to implement, and comes with theoretical guarantees (?, ?). The motivation behind SAPG is that one could try to find the maximiser using the projected gradient algorithm (?, ?), which is given by (Œ∏n)n‚àà‚Ñï,(\theta\_{n})\_{n\in\mathbb{N}}, with Œ∏0‚ààŒò\theta\_{0}\in\Theta and associated with the following recursion

|  |  |  |  |
| --- | --- | --- | --- |
|  | Œ∏n+1=Œ†Œò‚Äã[Œ∏n+Œ¥n‚Äã‚àáŒ∏log‚Å°p‚Äã(y|Œ∏n)],\theta\_{n+1}=\Pi\_{\Theta}\,\left[\theta\_{n}+\delta\_{n}\nabla\_{\theta}\log~p(y|\theta\_{n})\,\right]\;, |  | (35) |

where Œ†Œò\Pi\_{\Theta} is the projection onto Œò\Theta and (Œ¥n)n‚àà‚Ñï(\delta\_{n})\_{n\in\mathbb{N}} is a sequence of non-increasing step-sizes. The problem is that typically, the marginal likelihood Œ∏‚Ü¶p‚Äã(y|Œ∏)\theta\mapsto p(y|\theta) is intractable. The authors of (?, ?) manage to express

|  |  |  |
| --- | --- | --- |
|  | Œ∏‚Ü¶‚àáŒ∏log‚Å°p‚Äã(y|Œ∏n)\theta\mapsto\nabla\_{\theta}\log~p(y|\theta\_{n}) |  |

of ([35](https://arxiv.org/html/2512.22109v1#A1.E35 "Equation 35 ‚Ä£ Appendix A Stochastic Approximation Proximal Gradient algorithm (SAPG) ‚Ä£ Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification")) through expectations, making the problem amenable to MCMC approaches, in essence, a stochastic gradient descent.

It is shown that for any Œ∏‚ààŒò\theta\in\Theta

|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚àáŒ∏log‚Å°p‚Äã(y|Œ∏)=‚à´‚Ñùpp‚Äã(w|y,Œ∏)‚Äã‚àáŒ∏log‚Å°p‚Äã(w,y|Œ∏)‚Äãùëëw=‚àí‚à´‚Ñùpg‚Äã(w)‚Äãp‚Äã(w|y,Œ∏)‚Äãùëëw‚àí‚àáŒ∏log‚Å°(Z‚Äã(Œ∏)),\begin{split}\nabla\_{\theta}\log p(y|\theta)&=\int\_{\mathbb{R}^{p}}p(w|y,\theta)\,\nabla\_{\theta}\log p(w,y|\theta)\,dw\\ &=-\int\_{\mathbb{R}^{p}}g(w)p(w|y,\theta)\,dw-\nabla\_{\theta}\log(\mathrm{Z}(\theta))\;,\end{split} |  | (36) |

where Z‚Äã(Œ∏)\mathrm{Z}(\theta) is the normalizing constant of the prior distribution p‚Äã(w|Œ∏)p(w|\theta), i.e.,

|  |  |  |  |
| --- | --- | --- | --- |
|  | Z‚Äã(Œ∏)=‚à´‚Ñùpexp‚Å°(‚àíŒ∏‚Äãg‚Äã(w))‚Äãùëëx.\mathrm{Z}(\theta)=\int\_{\mathbb{R}^{p}}\exp(-\theta\,g(w))dx\;. |  | (37) |

In particular, the expectation ‚à´‚Ñùpg‚Äã(w)‚Äãp‚Äã(w|y,Œ∏)‚Äãùëëw\int\_{\mathbb{R}^{p}}g(w)\,p(w|y,\theta)\,dw is replaced by a Monte Carlo estimator targeting the prior, leading to the following gradient estimate, for any Œ∏‚ààŒò,\theta\in\Theta,

|  |  |  |  |
| --- | --- | --- | --- |
|  | Œîm,Œ∏=1m‚Äã‚àëk=1m‚àáŒ∏log‚Å°p‚Äã(Xk,y|Œ∏)=‚àí‚àáŒ∏log‚Å°Z‚Äã(Œ∏)‚àí1m‚Äã‚àëk=1mg‚Äã(Xk),\Delta\_{m,\theta}=\frac{1}{m}\sum\_{k=1}^{m}\nabla\_{\theta}\log p(X\_{k},y|\theta)=-\nabla\_{\theta}\log\mathrm{Z}(\theta)-\frac{1}{m}\sum\_{k=1}^{m}g(X\_{k})\;, |  | (38) |

where (Xk)k‚àà{0,‚Ä¶,m}(X\_{k})\_{k\in\{0,\dots,m\}} is a sample of size m‚àà‚Ñïm\in\mathbb{N} generated by using a Markov Chain targeting p‚Äã(w|y,Œ∏)=p‚Äã(w,y|Œ∏)/p‚Äã(y|Œ∏)p(w|y,\theta)=p(w,y|\theta)/p(y|\theta), (in our MYULA scheme, this would actually be a regularized approximation of this density). Therefore, to compute Œ∏‚ãÜ\theta\_{\star}, we can build a new sequence (Œ∏n)n‚àà‚Ñï(\theta\_{n})\_{n\in\mathbb{N}} associated with the following recursion

|  |  |  |  |
| --- | --- | --- | --- |
|  | Œ∏n+1=Œ†Œò‚Äã[Œ∏n+Œ¥n+1‚ÄãŒîmn,Œ∏n],Œîmn,Œ∏n=‚àí‚àáŒ∏log‚Å°Z‚Äã(Œ∏n)‚àí1mn‚Äã‚àëk=1mng‚Äã(Xkn),\theta\_{n+1}=\Pi\_{\Theta}\,[\theta\_{n}+\delta\_{n+1}\Delta\_{m\_{n},\theta\_{n}}\,]\;,\qquad\Delta\_{m\_{n},\theta\_{n}}=-\nabla\_{\theta}\log\mathrm{Z}(\theta\_{n})-\frac{1}{m\_{n}}\sum\_{k=1}^{m\_{n}}g(X\_{k}^{n})\;, |  | (39) |

starting from some Œ∏0‚ààŒò\theta\_{0}\in\Theta, and where (mn)n‚àà‚Ñï(m\_{n})\_{n\in\mathbb{N}} is a sequence of non-decreasing sample sizes. Under some assumptions on (mn)n‚àà‚Ñï,(Œ¥n)n‚àà‚Ñï(m\_{n})\_{n\in\mathbb{N}},\,(\delta\_{n})\_{n\in\mathbb{N}} and on the Markov kernels, the errors in the gradient estimates asymptotically average out and the algorithm converges to a maximizer of Œ∏‚Ü¶p‚Äã(y|Œ∏)\theta\mapsto p(y|\theta). More precisely, as is standard (e.g., Polyak-Ruppert ideas) in stochastic approximation algorithms, given N‚àà‚ÑïN\in\mathbb{N}, a sequence of non-increasing weights (œân)n‚àà‚Ñï(\omega\_{n})\_{n\in\mathbb{N}}, and a sequence (Œ∏n)n=0N‚àí1(\theta\_{n})\_{n=0}^{N-1} generated using ([39](https://arxiv.org/html/2512.22109v1#A1.E39 "Equation 39 ‚Ä£ Appendix A Stochastic Approximation Proximal Gradient algorithm (SAPG) ‚Ä£ Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification")), an approximate solution of ([32](https://arxiv.org/html/2512.22109v1#A1.E32 "Equation 32 ‚Ä£ Appendix A Stochastic Approximation Proximal Gradient algorithm (SAPG) ‚Ä£ Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification")) can be obtained by calculating the weighted average

|  |  |  |  |
| --- | --- | --- | --- |
|  | Œ∏¬ØN=‚àën=0N‚àí1œânŒ∏n/‚àën=0N‚àí1œân.\bar{\theta}\_{N}=\left.\sum\_{n=0}^{N-1}\omega\_{n}\theta\_{n}\middle/\sum\_{n=0}^{N-1}\omega\_{n}\right.\;. |  | (40) |

which converges asymptotically to a solution of ([32](https://arxiv.org/html/2512.22109v1#A1.E32 "Equation 32 ‚Ä£ Appendix A Stochastic Approximation Proximal Gradient algorithm (SAPG) ‚Ä£ Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification")) as N‚Üí‚àûN\rightarrow\infty.

The SAPG optimisation scheme in this work makes use of the MYULA approach described earlier. Accordingly, to draw samples from the posterior p‚Äã(w|y,Œ∏)=p‚Äã(w,y|Œ∏)/p‚Äã(y|Œ∏)p(w|y,\theta)=p(w,y|\theta)/p(y|\theta), we will define a Markov chain (Xk)k‚àà‚Ñï(X\_{k})\_{k\in\mathbb{N}}, starting from X0‚àà‚ÑùpX\_{0}\in\mathbb{R}^{p}, given by the recursion

|  |  |  |  |
| --- | --- | --- | --- |
|  | Xk+1=Xk‚àíŒ¥‚Äã‚àáwf‚Äã(Xk)‚àíŒ¥Œª‚Äã{Xk‚àíproxŒ∏‚ÄãgŒª‚Å°(Xk)}+2‚ÄãŒ¥‚ÄãBk+1,X\_{k+1}=X\_{k}-\delta\nabla\_{w}f(X\_{k})-\frac{\delta}{\lambda}\left\{X\_{k}-\operatorname{prox}\_{\theta g}^{\lambda}(X\_{k})\right\}+\sqrt{2\delta}\,B\_{k+1}\;, |  | (41) |

where proxŒ∏‚ÄãgŒª\operatorname{prox}\_{\theta g}^{\lambda} defined as in
([7](https://arxiv.org/html/2512.22109v1#S3.E7 "Equation 7 ‚Ä£ 3.1 Moreau‚ÄìYosida smoothing and proximal gradient structure ‚Ä£ 3 Proximal MCMC for the posterior ‚Ä£ Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification")), Œª‚àà‚Ñù+\lambda\in\mathbb{R}^{+} is
the smoothing parameter for the Moreau-Yosida envelope of g, Œ¥‚àà‚Ñù+\delta\in\mathbb{R}^{+} is the discretisation step-size
and (Bk)k‚àà‚Ñï‚àó(B\_{k})\_{k\in\mathbb{N}^{\*}} is a
sequence of i.i.d. pp-dimensional zero-mean Gaussian
random variables with an identity covariance matrix.

Last but not least, we would like to draw attention to the fact that to use ([39](https://arxiv.org/html/2512.22109v1#A1.E39 "Equation 39 ‚Ä£ Appendix A Stochastic Approximation Proximal Gradient algorithm (SAPG) ‚Ä£ Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification")) it is necessary to evaluate Œ∏‚Ü¶‚àáŒ∏log‚Å°Z‚Äã(Œ∏)\theta\mapsto\nabla\_{\theta}\log\mathrm{Z}(\theta). Generally, this cannot be computed exactly and has to be approximated; the authors in (?, ?) propose three different strategies to address such a calculation/approximation depending on whether gg is a homogeneous function or not. We will adapt their version for scalar parameters, multiplying a 1-homogeneous regulariser.555A function gg is Œ±\alpha-positively homogeneous if there exists Œ±‚àà‚Ñù\{0}\alpha\in\mathbb{R}\backslash\{0\} such that for any x‚àà‚Ñùpx\in\mathbb{R}^{p} and t>0t>0, g‚Äã(t‚Äãx)=tŒ±‚Äãg‚Äã(x)g(tx)=t^{\alpha}g(x). In this case, an easy calculation shows that

|  |  |  |  |
| --- | --- | --- | --- |
|  | dd‚ÄãŒ∏‚Äãlog‚Å°Z‚Äã(Œ∏)=‚àípŒ∏.\frac{\textrm{d}}{\textrm{d}{\theta}}\log\mathrm{Z}(\theta)=-\frac{p}{\theta}. |  | (42) |

### A.1 Heuristic choice of the initial scale Œ∏0\theta\_{0}

This section records the calculations that justify the heuristic (‚Äúmethod-of-moments‚Äù) choice of
Œ∏0.\theta\_{0}.

On the FIT window, we compute an initial least‚Äìsquares solution

|  |  |  |
| --- | --- | --- |
|  | wL‚ÄãS‚Ä≤=Rc‚Ä†‚Äãyc,w^{\prime}\_{LS}=R\_{c}^{\dagger}\;y\_{c}, |  |

which we project down666It is easy to see that the projector has the form PC‚Äã(w)=w‚àí(ùüè‚ä§‚Äãw‚àí1)p‚ÄãùüèP\_{C}(w)=w-\frac{(\mathbf{1}^{\top}w-1)}{p}\mathbf{1} to the affine hyperplane

|  |  |  |
| --- | --- | --- |
|  | ùíû={w:ùüè‚ä§‚Äãw=1}\mathcal{C}=\{w:\mathbf{1}^{\top}w=1\} |  |

to obtain

|  |  |  |  |
| --- | --- | --- | --- |
|  | wLS=(wL‚ÄãS)‚àí(ùüè‚ä§‚ÄãwL‚ÄãS‚àí1)p‚Äã‚Äâ1,w\_{\mathrm{LS}}=(w\_{LS})-\frac{\bigl(\mathbf{1}^{\top}w\_{LS}-1\bigr)}{p}\,\mathbf{1}, |  | (43) |

which satisfies the budget constraint.

Consider a weighted Laplace prior with independent coordinates,

|  |  |  |
| --- | --- | --- |
|  | œÄ‚Äã(w‚à£Œ∏)‚àù‚àèj=1pŒ∏‚ÄãŒ±j2‚Äãexp‚Å°(‚àíŒ∏‚ÄãŒ±j‚Äã|wj|),\pi(w\mid\theta)\;\propto\;\prod\_{j=1}^{p}\frac{\theta\alpha\_{j}}{2}\exp\!\bigl(-\theta\alpha\_{j}|w\_{j}|\bigr), |  |

and ‚Äúprecision‚Äù
Œªj=Œ∏‚ÄãŒ±j\lambda\_{j}=\theta\alpha\_{j}. For a scalar Laplace random variable
W‚àºLaplace‚Äã(0,1/Œª)W\sim\mathrm{Laplace}(0,1/\lambda) we have

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Äã|W|=1Œª‚áíùîº‚Äã[Œª‚Äã|W|]=1.\mathbb{E}|W|\;=\;\frac{1}{\lambda}\quad\Rightarrow\quad\mathbb{E}[\lambda\;|W|]=1. |  |

Thus, under the prior above,

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Äã[Œ∏‚ÄãŒ±j‚Äã|wj|]=1‚áíùîº‚Äã[Œ∏‚Äã‚àëj=1pŒ±j‚Äã|wj|]‚âàp.\mathbb{E}\bigl[\theta\alpha\_{j}|w\_{j}|\bigr]=1\quad\Rightarrow\quad\mathbb{E}\Bigl[\theta\sum\_{j=1}^{p}\alpha\_{j}|w\_{j}|\Bigr]\;\approx\;p. |  |

Informally: a ‚Äútypical‚Äù draw from the prior satisfies

|  |  |  |
| --- | --- | --- |
|  | Œ∏‚Äã‚àëj=1pŒ±j‚Äã|wj|‚âàp.\theta\sum\_{j=1}^{p}\alpha\_{j}|w\_{j}|\approx p. |  |

Let wLSw\_{\mathrm{LS}} be a reference solution, e.g., ([43](https://arxiv.org/html/2512.22109v1#A1.E43 "Equation 43 ‚Ä£ A.1 Heuristic choice of the initial scale ùúÉ‚ÇÄ ‚Ä£ Appendix A Stochastic Approximation Proximal Gradient algorithm (SAPG) ‚Ä£ Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification")). We choose Œ∏0\theta\_{0} so that wLSw\_{\mathrm{LS}} looks
like a *typical* draw from the Laplace prior in the sense of the
penalty scale. Concretely, we impose

|  |  |  |
| --- | --- | --- |
|  | Œ∏0‚Äã‚àëj=1pŒ±j‚Äã|(wLS)j|‚âàp,\theta\_{0}\sum\_{j=1}^{p}\alpha\_{j}|(w\_{\mathrm{LS}})\_{j}|\;\approx\;p, |  |

which yields the closed-form choice

|  |  |  |
| --- | --- | --- |
|  | Œ∏0‚âàp‚àëj=1pŒ±j‚Äã|(wLS)j|.\theta\_{0}\;\approx\;\frac{p}{\sum\_{j=1}^{p}\alpha\_{j}|(w\_{\mathrm{LS}})\_{j}|}. |  |

Because the weights Œ±j\alpha\_{j} have been normalized to have mean
1p‚Äã‚àëj=1pŒ±j‚âà1\frac{1}{p}\sum\_{j=1}^{p}\alpha\_{j}\approx 1, this can be seen as an
‚Äúaverage-scale‚Äù matching condition: by that we mean that, on average,
Œ±j‚Äã|(wLS)j|\alpha\_{j}|(w\_{\mathrm{LS}})\_{j}| will roughly be of the same order as the
prior mean absolute value 1/Œ∏01/\theta\_{0}.

A slightly different interpretation is to say that we are
choosing Œ∏0\theta\_{0} so that there is about one unit of Laplace
penalty per coordinate:

|  |  |  |
| --- | --- | --- |
|  | Œ∏0‚Äã‚àëjŒ±j‚Äã|(wLS)j|‚âàp.\theta\_{0}\sum\_{j}\alpha\_{j}|(w\_{\mathrm{LS}})\_{j}|\approx p. |  |

In practice we do not use Œ∏0\theta\_{0} completely unconstrained, but
regularise it as

|  |  |  |
| --- | --- | --- |
|  | Œ∏0=max‚Å°{p‚àëj=1pŒ±j‚Äã|(wLS)j|,‚Äâ10‚àí6},\theta\_{0}\;=\;\max\!\left\{\frac{p}{\sum\_{j=1}^{p}\alpha\_{j}|(w\_{\mathrm{LS}})\_{j}|},\,10^{-6}\right\}, |  |

and then constrain the SAPG updates to the box

|  |  |  |
| --- | --- | --- |
|  | Œ∏‚ààŒò=[Œ∏0/10,‚Äâ10‚ÄãŒ∏0].\theta\in\Theta=[\theta\_{0}/10,\,10\,\theta\_{0}]. |  |

The lower bound 10‚àí610^{-6} prevents Œ∏0\theta\_{0} from becoming
numerically very small in cases where
‚àëjŒ±j‚Äã|(wLS)j|\sum\_{j}\alpha\_{j}|(w\_{\mathrm{LS}})\_{j}| is very large (for example,
if wLSw\_{\mathrm{LS}} is noisy or poorly identified), which would
flatten the prior, reduce effective regularisation, and harm both
the identifiability of Œ∏\theta and SAPG stability.

The interval [Œ∏0/10,10‚ÄãŒ∏0][\theta\_{0}/10,10\,\theta\_{0}] encodes the belief that the
LS-matched Œ∏0\theta\_{0} is accurate up to roughly one order of
magnitude. Within this range, the SAPG algorithm can adapt the scale
parameter to the data, but it is prevented from drifting to
*extremely* small values (prior almost flat, very high effective
dimension, ill-conditioned geometry) or to *extremely* large
values (e.g., prior overly spiky, weights nearly all shrunk to zero, inhomogeneous
posterior geometry).

## Appendix B Preconditioning and Metropolis-Hastings for the long MALA run

### B.1 Preconditioning

For future reference, ignoring constants, we record here the negative log‚Äìposterior with (œÉ^2,Œõ,Œ∏‚ãÜ)(\hat{\sigma}^{2},\Lambda,\theta\_{\star}):

|  |  |  |  |
| --- | --- | --- | --- |
|  | Œ¶Œ∏‚ãÜ‚Äã(w)=fy‚Äã(w)+g‚Äã(w;Œ∏‚ãÜ)=12‚ÄãœÉ^2‚Äã‚Äñyc‚àíRc‚Äãw‚Äñ22+Œõ‚Äã(1‚ä§‚Äãw‚àí1)2+Œ∏‚ãÜ‚Äã‚àëj=1pŒ±j‚Äã|wj|,\Phi\_{\theta\_{\star}}(w)=f\_{y}(w)+g(w;\theta\_{\star})=\frac{1}{2\hat{\sigma}^{2}}\,\|y\_{c}-R\_{c}w\|\_{2}^{2}+\Lambda(1^{\top}w-1)^{2}+\theta\_{\star}\sum\_{j=1}^{p}\alpha\_{j}|w\_{j}|, |  | (44) |

and with Œ¶Œª,Œ∏‚ãÜ\Phi\_{\lambda,\,\theta\_{\star}} its smoothed analogue, defined by replacing gg with its MY‚Äìenvelope,

|  |  |  |
| --- | --- | --- |
|  | gŒª‚Äã(w)=minu‚Å°{12‚ÄãŒª‚Äã‚Äñw‚àíu‚Äñ22+Œ∏‚ãÜ‚Äã‚àëjŒ±j‚Äã|uj|},i.e.‚ÄãŒ¶Œª,Œ∏‚ãÜ‚Äã(w)=fy‚Äã(w)+gŒª,Œ∏‚ãÜ‚Äã(w).g\_{\lambda}(w)=\min\_{u}\left\{\frac{1}{2\lambda}\|w-u\|\_{2}^{2}+\theta\_{\star}\sum\_{j}\alpha\_{j}|u\_{j}|\right\},\quad\text{i.e.}\;\;\Phi\_{\lambda,\,\theta\_{\star}}(w)=f\_{y}(w)+g\_{\lambda,\,\theta\_{\star}}(w). |  |

To counteract the slow mixing in MALA, due to the stiff likelihood, we adopt a diagonal (Jacobi) preconditioner, P,P,

|  |  |  |
| --- | --- | --- |
|  | P=D‚àí1/2,where,D=diag‚Å°(1œÉ^2‚Äãdiag‚Å°(R‚ä§‚ÄãR)+‚ÄÑ2‚ÄãŒõ).P=D^{-1/2},\quad\text{where,}\quad D=\operatorname{diag}\!\Big(\frac{1}{\hat{\sigma}^{2}}\operatorname{diag}(R^{\top}R)\;+\;2\Lambda\Big). |  |

This essentially results in a variable-metric MYULA update with metric P‚àí2P^{-2}, which amounts to rescaling the gradient and the Moreau term, coordinate-wise. We *retune* the steps by recomputing the Lipschitz bound in the preconditioned geometry,

|  |  |  |
| --- | --- | --- |
|  | Lpre=Œªmax‚Äã(P‚ÄãA‚ÄãP),A=1œÉ^2‚ÄãR‚ä§‚ÄãR+2‚ÄãŒõ‚Äã‚Äâ11‚ä§,L\_{\mathrm{pre}}=\lambda\_{\max}(PAP),\quad A=\tfrac{1}{\hat{\sigma}^{2}}R^{\top}R+2\Lambda\,\bm{1}\bm{1}^{\top}, |  |

with the same principled formulas as with our main MYULA approach, i.e.,

|  |  |  |
| --- | --- | --- |
|  | ŒªMYpre=1Lpre,Œ¥pre=0.92‚ÄãLpre.\lambda\_{\mathrm{MY}}^{\mathrm{pre}}=\frac{1}{L\_{\mathrm{pre}}},\qquad\delta^{\mathrm{pre}}=\frac{0.9}{2L\_{\mathrm{pre}}}. |  |

This preserves the target posterior and helps in substantially increasing the ESS, and also exhibits much improved autocorrelation decay. Moreover, being diagonal, it is cheap to compute.

With this choice, the preconditioned MALA proposal reads

|  |  |  |
| --- | --- | --- |
|  | w‚Ä≤=w‚àíŒ¥pre‚ÄãP2‚Äã‚àáŒ¶Œª,Œ∏‚ãÜ‚Äã(w)+2‚ÄãŒ¥pre‚ÄãP‚ÄãŒæ,Œæ‚àºùí©‚Äã(0,Ip).w^{\prime}=w-\delta^{\mathrm{pre}}P^{2}\nabla\Phi\_{\lambda,\,\theta\_{\star}}(w)+\sqrt{2\delta^{\mathrm{pre}}}\,P\,\xi,\quad\xi\sim\mathcal{N}(0,I\_{p}). |  |

### B.2 MALA‚Äìstyle proposal

Given the current state ww, the proposal is

|  |  |  |
| --- | --- | --- |
|  | w‚Ä≤‚àºùí©‚Äã(m‚Äã(w),‚Äâ2‚ÄãŒ¥pre‚ÄãP2),m‚Äã(w)=w‚àíŒ¥pre‚ÄãP2‚Äã‚àáŒ¶Œª‚Äã(w).w^{\prime}\;\sim\;\mathcal{N}\!\Big(m(w),\,2\delta^{\mathrm{pre}}\,P^{2}\Big),\qquad m(w)\;=\;w-\delta^{\mathrm{pre}}\,P^{2}\,\nabla\Phi\_{\lambda}(w). |  |

This is the preconditioned MYULA step wrapped in a Metropolis‚ÄìHastings correction so that the chain is *exact* for œÄŒª,Œ∏‚ãÜ\pi\_{\lambda,\theta\_{\star}}.

Let q‚Äã(w‚Ä≤‚à£w)=ùí©‚Äã(w‚Ä≤;m‚Äã(w),‚Äâ2‚ÄãŒ¥‚ÄãP2)q(w^{\prime}\mid w)=\mathcal{N}(w^{\prime};\,m(w),\,2\delta P^{2}). The MH acceptance ratio is

|  |  |  |
| --- | --- | --- |
|  | log‚Å°Œ±‚Äã(w,w‚Ä≤)=‚àíŒ¶Œª,Œ∏‚ãÜ‚Äã(w‚Ä≤)+Œ¶Œª,Œ∏‚ãÜ‚Äã(w)‚àí14‚ÄãŒ¥‚Äã(‚Äñw‚àím‚Äã(w‚Ä≤)‚Äñ(P2)‚àí12‚àí‚Äñw‚Ä≤‚àím‚Äã(w)‚Äñ(P2)‚àí12),\log\alpha(w,w^{\prime})\;=\;-\Phi\_{\lambda,\,\theta\_{\star}}(w^{\prime})+\Phi\_{\lambda,\,\theta\_{\star}}(w)\;-\;\frac{1}{4\delta}\,\Big(\|w-m(w^{\prime})\|\_{\left(P^{2}\right)^{-1}}^{2}-\|w^{\prime}-m(w)\|\_{(P^{2})^{-1}}^{2}\Big), |  |

where ‚Äñv‚Äñ(P2)‚àí12=‚àëjvj2/pj2\|v\|\_{(P^{2})^{-1}}^{2}=\sum\_{j}v\_{j}^{2}/p\_{j}^{2} is the (squared) Mahalanobis norm in metric (P2)‚àí1(P^{2})^{-1}.
We accept with probability Œ±‚Äã(w,w‚Ä≤)=min‚Å°{1,exp‚Å°(log‚Å°Œ±)}\alpha(w,w^{\prime})=\min\{1,\exp(\log\alpha)\}.

## Appendix C Noise variance estimation

As part of the model setup, we pre-estimate the noise variance œÉ2\sigma^{2}, which appears as a parameter in the likelihood. We tested several estimators and narrowed the choice down to the *median absolute deviation* (MAD) estimator (?, ?) and the *classical residual variance estimator*; we chose the former because of its robustness to outliers. Both operate on the residuals of the regression model. However, note that the budget constraint
1‚ä§‚Äãw=11^{\top}w=1
*does not enter* the variance estimation at all: we work with the unconstrained OLS fit and its residuals. The rationale is that
œÉ^2\widehat{\sigma}^{2} should reflect the scale of the *tracking
error* yt‚àírt‚ä§‚Äãwy\_{t}-r\_{t}^{\top}w under a purely data-driven fit, without being distorted by how we choose to enforce the budget constraint. The constraint is imposed later, at the level of the prior
and posterior geometry, but not in the noise-scale estimation.

#### Median Absolute Deviation estimator (MAD)-based variance estimation

We work with the centred index-tracking regression

|  |  |  |  |
| --- | --- | --- | --- |
|  | yt=rt‚ä§‚Äãw+Œµt,t=1,‚Ä¶,T,y\_{t}\;=\;r\_{t}^{\top}w\;+\;\varepsilon\_{t},\qquad t=1,\dots,T, |  | (45) |

where yty\_{t} is the (centred) index return, rt‚àà‚Ñùpr\_{t}\in\mathbb{R}^{p} is the (centred) vector of asset returns,
w‚àà‚Ñùpw\in\mathbb{R}^{p} is the portfolio weight vector, and the noise is assumed i.i.d.

|  |  |  |  |
| --- | --- | --- | --- |
|  | Œµt‚àºùí©‚Äã(0,œÉ2).\varepsilon\_{t}\sim\mathcal{N}(0,\sigma^{2}). |  | (46) |

Let y‚àà‚ÑùTy\in\mathbb{R}^{T} and R‚àà‚ÑùT√ópR\in\mathbb{R}^{T\times p} denote the stacked observations
after centring,

|  |  |  |
| --- | --- | --- |
|  | yc=(y1,‚Ä¶,yT)‚ä§,Rc=[r1‚ä§‚ãÆrT‚ä§],y\_{c}=(y\_{1},\dots,y\_{T})^{\top},\qquad R\_{c}=\begin{bmatrix}r\_{1}^{\top}\\ \vdots\\ r\_{T}^{\top}\end{bmatrix}, |  |

so that the model can be written compactly as

|  |  |  |  |
| --- | --- | --- | --- |
|  | yc=Rc‚Äãw+Œµ,Œµ‚àºùí©‚Äã(0,œÉ2‚ÄãIT).y\_{c}\;=\;R\_{c}w+\varepsilon,\qquad\varepsilon\sim\mathcal{N}(0,\sigma^{2}I\_{T}). |  | (47) |

##### Step 1: unconstrained least-squares fit.

The variance estimator is based on residuals from the *unconstrained*
ordinary least-squares (OLS) fit,

|  |  |  |  |
| --- | --- | --- | --- |
|  | w^OLS‚ààarg‚Å°minw‚àà‚Ñùp‚Å°12‚Äã‚à•yc‚àíRc‚Äãw‚à•22.\widehat{w}\_{\mathrm{OLS}}\;\in\;\arg\min\_{w\in\mathbb{R}^{p}}\frac{1}{2}\,\lVert y\_{c}-R\_{c}w\rVert\_{2}^{2}. |  | (48) |

In practice we compute w^OLS\widehat{w}\_{\mathrm{OLS}}
via a pseudoinverse or least-squares solver, e.g.

|  |  |  |  |
| --- | --- | --- | --- |
|  | w^OLS=Rc‚Ä†‚Äãyc=(Rc‚ä§‚ÄãRc)‚àí1‚ÄãRc‚ä§‚Äãyc.\widehat{w}\_{\mathrm{OLS}}=R\_{c}^{\dagger}\,y\_{c}=(R\_{c}^{\top}R\_{c})^{-1}R\_{c}^{\top}\,y\_{c}. |  | (49) |

##### Step 2: residuals.

Define the OLS residuals

|  |  |  |  |
| --- | --- | --- | --- |
|  | r^t=yt‚àírt‚ä§‚Äãw^OLS,t=1,‚Ä¶,T,\widehat{r}\_{t}\;=\;y\_{t}-r\_{t}^{\top}\widehat{w}\_{\mathrm{OLS}},\qquad t=1,\dots,T, |  | (50) |

and collect them into r^‚àà‚ÑùT\widehat{r}\in\mathbb{R}^{T}.
Under the Gaussian model, and ignoring estimation error in
w^OLS\widehat{w}\_{\mathrm{OLS}}, these residuals behave approximately like
realizations of Œµt\varepsilon\_{t} and are therefore informative about œÉ2\sigma^{2}.

##### Step 3: MAD and a robust scale estimate.

The MAD of a sample
x1,‚Ä¶,xTx\_{1},\dots,x\_{T} is

|  |  |  |  |
| --- | --- | --- | --- |
|  | MAD‚Äã(x1,‚Ä¶,xT)=median1‚â§t‚â§T|xt‚àímedian1‚â§s‚â§Txs|.\mathrm{MAD}(x\_{1},\dots,x\_{T})\;=\;\operatorname\*{median}\_{1\leq t\leq T}\,\bigl|\;x\_{t}-\operatorname\*{median}\_{1\leq s\leq T}x\_{s}\;\bigr|. |  | (51) |

If X‚àºùí©‚Äã(0,œÉ2)X\sim\mathcal{N}(0,\sigma^{2}) then

|  |  |  |
| --- | --- | --- |
|  | MAD‚Äã(X1,‚Ä¶,XT)‚âàœÉ‚ÄãŒ¶‚àí1‚Äã(0.75),\mathrm{MAD}(X\_{1},\dots,X\_{T})\;\approx\;\sigma\,\Phi^{-1}(0.75), |  |

where Œ¶‚àí1\Phi^{-1} is the standard normal quantile function.
Thus a consistent estimator of œÉ\sigma is

|  |  |  |  |
| --- | --- | --- | --- |
|  | œÉ^MAD=cMAD‚ÄãMAD‚Äã(r^1,‚Ä¶,r^T),cMAD=1Œ¶‚àí1‚Äã(0.75)‚âà1.4826.\widehat{\sigma}\_{\mathrm{MAD}}\;=\;c\_{\mathrm{MAD}}\;\mathrm{MAD}(\widehat{r}\_{1},\dots,\widehat{r}\_{T}),\qquad c\_{\mathrm{MAD}}=\frac{1}{\Phi^{-1}(0.75)}\approx 1.4826. |  | (52) |

Our working variance estimate is then

|  |  |  |  |
| --- | --- | --- | --- |
|  | œÉ^2=œÉ^MAD2=(1.4826√óMAD‚Äã(r^1,‚Ä¶,r^T))2.\widehat{\sigma}^{2}\;=\;\widehat{\sigma}\_{\mathrm{MAD}}^{2}\;=\;\bigl(1.4826\times\mathrm{MAD}(\widehat{r}\_{1},\dots,\widehat{r}\_{T})\bigr)^{2}. |  | (53) |