---
authors:
- Felipe J. P. Antunes
- Yuri F. Saporito
- Sebastian Jaimungal
doc_id: arxiv:2512.14967v1
family_id: arxiv:2512.14967
is_current: true
taxonomy:
  alpha_families: []
  asset_classes: []
  horizons: []
  themes: []
title: Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise
url_abs: http://arxiv.org/abs/2512.14967v1
url_html: https://arxiv.org/html/2512.14967v1
venue: arXiv q-fin
version: 1
year: 2025
---


Felipe J. P. Antunes
School of Applied Mathematics, Getulio Vargas Foundation, Brazil
[felipe.antunes@fgv.br](mailto:felipe.antunes@fgv.br)
, 
Yuri F. Saporito
School of Applied Mathematics, Getulio Vargas Foundation, Brazil
[yuri.saporito@fgv.br](mailto:yuri.saporito@fgv.br)
 and 
Sebastian Jaimungal
Department of Statistical Sciences, University of Toronto, Canada and
  
Oxford-Man Institute for Quantitative Finance, University of Oxford
[sebastian.jaimungal@utoronto.ca](mailto:sebastian.jaimungal@utoronto.ca)

###### Abstract.

We present a novel numerical method for solving McKean-Vlasov forward-backward stochastic differential equations (MV-FBSDEs) with common noise, combining Picard iterations, elicitability and deep learning. The key innovation involves elicitability to derive a path-wise loss function, enabling efficient training of neural networks to approximate both the backward process and the conditional expectations arising from common noise â€” without requiring computationally expensive nested Monte Carlo simulations. The mean-field interaction term is parameterized via a recurrent neural network trained to minimize an elicitable score, while the backward process is approximated through a feedforward network representing the decoupling field. We validate the algorithm on a systemic risk inter-bank borrowing and lending model, where analytical solutions exist, demonstrating accurate recovery of the true solution. We further extend the model to quantileâ€“mediated interactions, showcasing the flexibility of the elicitability framework beyond conditional means or moments. Finally, we apply the method to a non-stationary Aiyagariâ€“Bewleyâ€“Huggett economic growth model with endogenous interest rates, illustrating its applicability to complex mean-field games without closed-form solutions.

## 1. Introduction

McKean-Vlasov Forward-Backward Stochastic Differential Equations (MV-FBSDEs) arise naturally in the study of stochastic control problems where the dynamics or cost functions depend on the law of the solution process. These equations combine the forward-backward structure characteristic of stochastic maximum principles with mean-field interactions through the law of the process. Formally, a MV-FBSDE system consists of a forward diffusion process (Xt)tâˆˆ[0,T](X\_{t})\_{t\in[0,T]} coupled with backward processes (Yt,Zt)tâˆˆ[0,T](Y\_{t},Z\_{t})\_{t\in[0,T]}, where the coefficients of the differential equations depend on both the current state and the law of XtX\_{t}, typically, though not always, through its conditional moments. The coupling between the forward and backward components, together with the dependence on the law of the solution, make these equations particularly challenging from both theoretical and numerical perspectives. Of special interest is the case which the system is subject to common noise, introducing additional structure in the mean-field interaction through conditioning on a common source of randomness. Such systems play a fundamental role in the probabilistic analysis of mean-field games and mean-field type control problems, where they characterize Nash equilibria and optimal controls, respectively [carmona2018probabilistic].

The method we propose addresses two significant challenges in solving MV-FBSDEs with common noise: the forward-backward coupling and the dependence on the stochastic measure flow. We parameterize the mean-field term as a function of the common noise through an auxiliary recurrent neural network (to allow non-Markovian encoding), trained to minimize a score that elicits the required dependence on the measure flow. This approach eliminates the need for nested Monte Carlo simulation while maintaining adaptedness of the solution to the filtration generated by the common noise [coache2023conditionally].

We demonstrate the effectiveness of our method through numerical experiments on benchmark problems, showing particular advantages in the common noise case where traditional approaches struggle with computational efficiency. We also apply our proposed method to a more challenging MV-FBSDE arising from the economic growth model presented in [achdou2022income].

A survey of numerical methods for BSDEs can be found in [chessari2023numerical], including methods using deep learning.
A numerical method for MV-FBSDEs using recursive Picard iterations for small time intervals can be found in [chassagneux2019numerical]. Their method is based in splitting the time interval of the system into small time intervals, and calculating recursively empirical conditional expectations for YY at each time interval, whereas our method differs by performing a single Picard iteration for the whole time interval, using elicitability to calculate the conditional expectation for YY.

The works of [germain2022numerical, zhou2024deep, han2024learning] present various numerical methods for MV-FBSDEs using neural networks.
[germain2022numerical] uses deep learning to approximate YY, with the loss calculated over the terminal value YTY\_{T}. The mean field interaction is calculated empirically over sample paths.
[zhou2024deep] proposes a method based on solving a characteristic system of ODEs. The method uses score functions and density estimation to avoid sampling from Brownian motions.
[han2024learning] uses deep learning to estimate the dependence on the law of the processes, then uses Deep BSDE to solve the backwards component.
In comparison, our method uses elicitability to estimate the dependence on the law. This allows us to consider the case with common noise without costly sampling. Moreover, we use elicitability to solve for YtY\_{t}, creating a loss function defined over the whole time interval and allows us to easily incorporate mean-field interaction that go beyond moments, e.g., those that depend on quantiles or tail risk.

## 2. Problem Formulation

We work on a complete probability space (Î©,ğ”‰,â„™)(\Omega,\mathfrak{F},\mathbb{P}) on which we have two independent, dd-dimensional Brownian motions (Wt)tâˆˆ[0,T](W\_{t})\_{t\in[0,T]} and (Wt0)tâˆˆ[0,T](W^{0}\_{t})\_{t\in[0,T]} over a time horizon T>0T>0, where W0W^{0} and WW represent common and idiosyncratic noise, respectively. We denote the space of square-integrable ğ’œ\mathcal{A}â€“measurable random variables by ğ•ƒ2â€‹[ğ’œ]{\mathbb{L}}^{2}[\mathcal{A}] and adopt the following notation:

* âŠ³\vartriangleright

  â„±0=(â„±t0)tâˆˆğ’¯\mathcal{F}^{0}=(\mathcal{F}^{0}\_{t})\_{{t\in\mathcal{T}}}, â„±t0=Ïƒâ€‹(Ws0,sâ‰¤t)\mathcal{F}^{0}\_{t}=\sigma\left(W^{0}\_{s},s\leq t\right), for the filtration generated by the common noise;
* âŠ³\vartriangleright

  â„±=(â„±t)tâˆˆğ’¯\mathcal{F}=(\mathcal{F}\_{t})\_{{t\in\mathcal{T}}}, â„±t=Ïƒâ€‹(Ws,sâ‰¤t)\mathcal{F}\_{t}=\sigma\left(W\_{s},s\leq t\right), for the filtration generated by the idiosyncratic noise.

Our goal is to approximate the solution to the fully-coupled multidimensional MV-FBSDE describing the dynamics of (X,Y)âˆˆâ„â„“Ã—â„m(X,Y)\in\mathbb{R}^{\ell}\times\mathbb{R}^{m}:

|  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- |
|  | dâ€‹Xt\displaystyle{{\rm d}X}\_{t} | =Î¼â€‹(t,Xt,Yt,Zt,Zt0,St)â€‹dâ€‹t+Ïƒâ€‹(t,Xt)â€‹dâ€‹Wt+Ïƒ0â€‹(t,Xt)â€‹dâ€‹Wt0,\displaystyle=\mu(t,X\_{t},Y\_{t},Z\_{t},Z\_{t}^{0},S\_{t})\,{{\rm d}t}+\sigma(t,X\_{t})\,{{\rm d}W}\_{t}+\sigma\_{0}(t,X\_{t})\,{{\rm d}W}^{0}\_{t}, | X0=Î¾,\displaystyle X\_{0}=\xi, |  | (1) |
|  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- |
|  | dâ€‹Yt\displaystyle{{\rm d}Y}\_{t} | =âˆ’fâ€‹(t,Xt,Yt,Zt,Zt0,St)â€‹dâ€‹t+ZtâŠºâ€‹dâ€‹Wt+Zt0âŠºâ€‹dâ€‹Wt0,\displaystyle=-f(t,X\_{t},Y\_{t},Z\_{t},Z\_{t}^{0},S\_{t})\,{{\rm d}t}+Z\_{t}^{\intercal}\,{{\rm d}W}\_{t}+{Z\_{t}^{0}}^{\intercal}\,{{\rm d}W}^{0}\_{t}, | YT=Gâ€‹(XT,ST),\displaystyle Y\_{T}=G(X\_{T},S\_{T}), |  | (2) |

where Î¼:[0,T]Ã—â„â„“Ã—â„mÃ—â„dÃ—mÃ—â„dÃ—mÃ—â„pâŸ¶â„â„“\mu:[0,T]\times\mathbb{R}^{\ell}\times\mathbb{R}^{m}\times\mathbb{R}^{d\times m}\times\mathbb{R}^{d\times m}\times\mathbb{R}^{p}\longrightarrow\mathbb{R}^{\ell}, Ïƒ:[0,T]Ã—â„â„“âŸ¶â„â„“Ã—d\sigma:[0,T]\times\mathbb{R}^{\ell}\longrightarrow\mathbb{R}^{\ell\times d}, Ïƒ0:[0,T]Ã—â„â„“âŸ¶â„â„“Ã—d\sigma\_{0}:[0,T]\times\mathbb{R}^{\ell}\longrightarrow\mathbb{R}^{\ell\times d}, f:[0,T]Ã—â„â„“Ã—â„mÃ—â„dÃ—mÃ—â„dÃ—mÃ—â„pâŸ¶â„mf:[0,T]\times\mathbb{R}^{\ell}\times\mathbb{R}^{m}\times\mathbb{R}^{d\times m}\times\mathbb{R}^{d\times m}\times\mathbb{R}^{p}\longrightarrow\mathbb{R}^{m} and G:â„â„“Ã—â„pâŸ¶â„mG:\mathbb{R}^{\ell}\times\mathbb{R}^{p}\longrightarrow\mathbb{R}^{m}.

The quantity StS\_{t} is any elicitable, pp-dimensional statistic (see [fissler2016higher]) of the state XtX\_{t}, conditioned on the path of the common noise up to time tt, i.e. we assume there exists a score function ğ”–{\mathfrak{S}} such that

|  |  |  |  |
| --- | --- | --- | --- |
|  | St=argminğ”°âˆˆğ•ƒ2â€‹[â„±t0]ğ”¼â€‹[ğ”–â€‹(ğ”°,Xt)].S\_{t}=\operatorname\*{argmin}\_{{\mathfrak{s}}\in{\mathbb{L}}^{2}[\mathcal{F}^{0}\_{t}]}\mathbb{E}\left[{\mathfrak{S}}({\mathfrak{s}},X\_{t})\right]. |  | (3) |

Equivalently, StS\_{t} is an elicitable statistic of the conditional measure Î¼t=â„’â€‹(Xt|â„±t0)\mu\_{t}=\mathcal{L}(X\_{t}|\mathcal{F}^{0}\_{t}).
For example, we may elicit conditional moments of XtX\_{t} by choosing ğ”–â€‹(ğ”°,x)=(Ï•â€‹(x)âˆ’ğ”°)2{\mathfrak{S}}({\mathfrak{s}},x)=(\phi(x)-{\mathfrak{s}})^{2} and conditional Î±\alpha-quantiles of XtX\_{t} by choosing ğ”–â€‹(ğ”°,x)=(1{ğ”°â‰¥x}âˆ’Î±)â€‹(ğ”°âˆ’x){\mathfrak{S}}({\mathfrak{s}},x)=(1\_{\{{\mathfrak{s}}\geq x\}}-\alpha)({\mathfrak{s}}-x). We provide further details on elicitability in Section [2.2.2](https://arxiv.org/html/2512.14967v1#S2.SS2.SSS2 "2.2.2. Estimating ğ‘† through elicitability â€£ 2.2. Detailed description â€£ 2. Problem Formulation â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise") below. This model falls into the setting of FBSDEs in a random environment, [carmona2018probabilistic, Chapter 1, Vol. II], in which such environment is given by the elicitable statistic (St)tâˆˆ[0,T](S\_{t})\_{t\in[0,T]}.

We assume the initial condition Î¾\xi is in ğ•ƒ2â€‹[ğ’¢0]{\mathbb{L}}^{2}[\mathcal{G}\_{0}] and takes values in â„â„“\mathbb{R}^{\ell}, for a given initial information ğ’¢0\mathcal{G}\_{0}. Furthermore, the full information, i.e. the filtration generated by both Brownian motions and the initial condition, is denoted by ğ’¢=(ğ’¢t)tâˆˆğ’¯\mathcal{G}=(\mathcal{G}\_{t})\_{{t\in\mathcal{T}}}, ğ’¢t=â„±tâˆ¨â„±t0âˆ¨Ïƒâ€‹(Î¾)\mathcal{G}\_{t}=\mathcal{F}\_{t}\vee\mathcal{F}^{0}\_{t}\vee\sigma(\xi). Below, we use âˆ¥â‹…âˆ¥\|\cdot\| to denote the appropriate Euclidean norm.

###### Assumption 1 (Lipschitz Continuity).

To guarantee existence and uniqueness of solution for ([1](https://arxiv.org/html/2512.14967v1#S2.E1 "In 2. Problem Formulation â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise"))â€“([2](https://arxiv.org/html/2512.14967v1#S2.E2 "In 2. Problem Formulation â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise")), we assume there exist L,C,L0>0L,C,L\_{0}>0 such that the following properties hold:

1. (i)

   Linear growth:

   |  |  |  |
   | --- | --- | --- |
   |  | â€–Î¼â€‹(t,0,0,0,0,ğ”°)â€–+â€–fâ€‹(t,0,0,0,0,ğ”°)â€–+â€–Ïƒâ€‹(t,0)â€–+â€–Ïƒ0â€‹(t,0)â€–â‰¤Câ€‹(1+â€–ğ”°â€–),\|\mu(t,0,0,0,0,{\mathfrak{s}})\|+\|f(t,0,0,0,0,{\mathfrak{s}})\|+\|\sigma(t,0)\|+\|\sigma\_{0}(t,0)\|\leq C(1+\|{\mathfrak{s}}\|), |  |

   for all tâˆˆ[0,T]t\in[0,T] and ğ”°âˆˆâ„p;{\mathfrak{s}}\in\mathbb{R}^{p};
2. (ii)

   Lipschitz continuous coefficients:

   |  |  |  |
   | --- | --- | --- |
   |  | â€–Î¼â€‹(t,x,y,z,z0,ğ”°)âˆ’Î¼â€‹(t,xâ€²,yâ€²,zâ€²,zâ€²â£0,ğ”°)â€–+â€–fâ€‹(t,x,y,z,z0,ğ”°)âˆ’fâ€‹(t,xâ€²,yâ€²,zâ€²,zâ€²â£0,ğ”°)â€–\displaystyle\|\mu(t,x,y,z,z^{0},{\mathfrak{s}})-\mu(t,x^{\prime},y^{\prime},z^{\prime},z^{\prime 0},{\mathfrak{s}})\|+\|f(t,x,y,z,z^{0},{\mathfrak{s}})-f(t,x^{\prime},y^{\prime},z^{\prime},z^{\prime 0},{\mathfrak{s}})\| |  |
   |  |  |  |
   | --- | --- | --- |
   |  | +âˆ¥Ïƒ(t,x)âˆ’Ïƒ(t,xâ€²)âˆ¥+âˆ¥Ïƒ0(t,x)âˆ’Ïƒ0(t,xâ€²)âˆ¥+âˆ¥G(x,ğ”°))âˆ’G(xâ€²,ğ”°))âˆ¥\displaystyle+\|\sigma(t,x)-\sigma(t,x^{\prime})\|+\|\sigma\_{0}(t,x)-\sigma\_{0}(t,x^{\prime})\|+\|G(x,{\mathfrak{s}}))-G(x^{\prime},{\mathfrak{s}}))\| |  |
   |  |  |  |
   | --- | --- | --- |
   |  | â‰¤Lâ€‹(â€–xâˆ’xâ€²â€–+â€–yâˆ’yâ€²â€–+â€–zâˆ’zâ€²â€–+â€–z0âˆ’zâ€²â£0â€–),\displaystyle\leq L(\|x-x^{\prime}\|+\|y-y^{\prime}\|+\|z-z^{\prime}\|+\|z^{0}-z^{\prime 0}\|), |  |

   for all tâˆˆ[0,T]t\in[0,T] and ğ”°âˆˆâ„p;{\mathfrak{s}}\in\mathbb{R}^{p};
3. (iii)

   Lipschitz continuous YY:

   â€–Ytt,xâˆ’Ytt,xâ€²â€–â‰¤L0â€‹â€–xâˆ’xâ€²â€–\|Y\_{t}^{t,x}-Y\_{t}^{t,x^{\prime}}\|\leq L\_{0}\|x-x^{\prime}\|, for any tâˆˆ[0,T]t\in[0,T] and x,xâ€²âˆˆâ„â„“x,x^{\prime}\in\mathbb{R}^{\ell}, where (Xst,x,Yst,x,(X\_{s}^{t,x},Y\_{s}^{t,x}, Zst,x,Zs0t,x)sâˆˆ[t,T]Z\_{s}^{t,x},Z\_{s}^{0^{t,x}})\_{s\in[t,T]} is a solution for ([1](https://arxiv.org/html/2512.14967v1#S2.E1 "In 2. Problem Formulation â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise"))â€“([2](https://arxiv.org/html/2512.14967v1#S2.E2 "In 2. Problem Formulation â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise")) for deterministic initial condition xx at time tt.

Under Assumption [1](https://arxiv.org/html/2512.14967v1#Thmtheorem1 "Assumption 1 (Lipschitz Continuity). â€£ 2. Problem Formulation â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise"), by [carmona2018probabilistic, Theorem 1.45, Proposition 1.52], there exists a unique solution (X,Y,Z,Z0)tâˆˆ[0,T](X,Y,Z,Z^{0})\_{t\in[0,T]} for ([1](https://arxiv.org/html/2512.14967v1#S2.E1 "In 2. Problem Formulation â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise"))â€“([2](https://arxiv.org/html/2512.14967v1#S2.E2 "In 2. Problem Formulation â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise")) such that

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â€‹[suptâˆˆ[0,T]{â€–Xtâ€–2+â€–Ytâ€–2}+âˆ«0T(â€–Ztâ€–2+â€–Zt0â€–2)â€‹ğ‘‘t]<âˆ.\displaystyle\mathbb{E}\left[\sup\_{t\in[0,T]}\Big\{\|X\_{t}\|^{2}+\|Y\_{t}\|^{2}\Big\}+\int\_{0}^{T}\Big(\|Z\_{t}\|^{2}+\|Z^{0}\_{t}\|^{2}\Big)dt\right]<\infty. |  | (4) |

Moreover, under Assumption [1](https://arxiv.org/html/2512.14967v1#Thmtheorem1 "Assumption 1 (Lipschitz Continuity). â€£ 2. Problem Formulation â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise"), which will be a standing assumption henceforth, the backward process YY admits a representation of the form

|  |  |  |  |
| --- | --- | --- | --- |
|  | Yt=Uâ€‹(t,Xt,St),\displaystyle Y\_{t}=U(t,X\_{t},S\_{t}), |  | (5) |

for some function U:[0,T]Ã—â„â„“Ã—â„pâ†’â„mU:[0,T]\times\mathbb{R}^{\ell}\times\mathbb{R}^{p}\rightarrow\mathbb{R}^{m} known as the decoupling field [carmona2018probabilistic, Proposition 1.46, Remark 1.49].

Integrating ([2](https://arxiv.org/html/2512.14967v1#S2.E2 "In 2. Problem Formulation â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise")), we see that YY admits the following fixed-point representation

|  |  |  |  |
| --- | --- | --- | --- |
|  | Yt=ğ”¼â€‹[Gâ€‹(XT,ST)+âˆ«tTfâ€‹(u,Xu,Yu,Zu,Zu0,Su)â€‹du|ğ’¢t].\displaystyle Y\_{t}=\mathbb{E}\left[\left.G(X\_{T},S\_{T})+\int\_{t}^{T}f(u,X\_{u},Y\_{u},Z\_{u},Z\_{u}^{0},S\_{u}){{\rm d}u}\ \right|\ \mathcal{G}\_{t}\right]. |  | (6) |

We can rewrite this fixed-point representation using the elicitability of the mean (or equivalently the L2L^{2}-projection representation for conditional expectations) via a quadratic score function:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Yt=argminğ”œâˆˆğ•ƒ2â€‹[ğ’¢t]ğ”¼â€‹[(ğ”œâˆ’Gâ€‹(XT,ST)âˆ’âˆ«tTfâ€‹(u,Xu,Yu,Zu,Zu0,Su)â€‹du)2].Y\_{t}=\operatorname\*{argmin}\_{{\mathfrak{Y}}\in{\mathbb{L}}^{2}[\mathcal{G}\_{t}]}\mathbb{E}\left[\left({\mathfrak{Y}}-G(X\_{T},S\_{T})-\int\_{t}^{T}f(u,X\_{u},Y\_{u},Z\_{u},Z\_{u}^{0},S\_{u})\,\,{{\rm d}u}\right)^{2}\right]. |  | (7) |

Moreover, by the decoupling field representation, we know that the minimizer in ([7](https://arxiv.org/html/2512.14967v1#S2.E7 "In 2. Problem Formulation â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise")) must have the form ğ”œâˆ—=Uâ€‹(t,Xt,St){\mathfrak{Y}}^{\*}=U(t,X\_{t},S\_{t}), and applying ItÃ´â€™s lemma and matching terms, we find

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | Zt\displaystyle Z\_{t} | =Ïƒâ€‹(t,Xt)âŠºâ€‹âˆ‚xUâ€‹(t,Xt,St),\displaystyle=\sigma(t,X\_{t})^{\intercal}\partial\_{x}U(t,X\_{t},S\_{t}), |  | (8) |

The process Z0Z^{0} admits a similar representation using the ItÃ´â€™s formula along a flow of conditional measures [carmona2018probabilistic, Vol II, Theorem 4.17]. Using that representation for Z0Z^{0}, however, presents challenges for numerical implementation. Instead, in Section [2.2.4](https://arxiv.org/html/2512.14967v1#S2.SS2.SSS4 "2.2.4. Approximating ğ‘ and ğ‘â‚€ â€£ 2.2. Detailed description â€£ 2. Problem Formulation â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise"), we use another (approximate) representation that elicits it.

### 2.1. Proposed method

We next provide an overview of our methodology, and provide further details in Section [2.2](https://arxiv.org/html/2512.14967v1#S2.SS2 "2.2. Detailed description â€£ 2. Problem Formulation â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise"). Our methodology proceeds iteratively. We fix a time discretization of [0,T][0,T], denoted by ğ’¯={t0,â€¦,tN}{\mathcal{T}}=\{t\_{0},\dots,t\_{N}\}, where 0=t0<â‹¯<tN=T0=t\_{0}<\cdots<t\_{N}=T. Given samples of (Wt,Wt0)tâˆˆğ’¯(W\_{t},W\_{t}^{0})\_{{t\in\mathcal{T}}} and initialization â€” which represents the initial guess for the solution to the MV-FBSDE system â€” (Xt0,Yt0,Zt0,Zt0,0,St0)tâˆˆğ’¯(X^{0}\_{t},Y^{0}\_{t},Z^{0}\_{t},Z^{0,0}\_{t},S^{0}\_{t})\_{{t\in\mathcal{T}}}, we update, at iteration kâˆˆâ„•k\in\mathbb{N}, each entry of (Xtk,Ytk,Ztk,Zt0,k,Stk)tâˆˆğ’¯(X^{k}\_{t},Y^{k}\_{t},Z^{k}\_{t},Z^{0,k}\_{t},S^{k}\_{t})\_{{t\in\mathcal{T}}} sequentially as follows:

1. (i)

   First, we solve the forward SDE ([1](https://arxiv.org/html/2512.14967v1#S2.E1 "In 2. Problem Formulation â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise")) for (Xtk+1)tâˆˆğ’¯(X^{k+1}\_{t})\_{{t\in\mathcal{T}}}, through Picard iterations, with (Wt,Wt0)tâˆˆğ’¯(W\_{t},W\_{t}^{0})\_{{t\in\mathcal{T}}} and (Ytk,Ztk,Zt0,k,Stk)tâˆˆğ’¯(Y^{k}\_{t},Z^{k}\_{t},Z^{0,k}\_{t},S^{k}\_{t})\_{{t\in\mathcal{T}}}
   held fixed. We denote the nn-th Picard iteration by Xtk+1,nX\_{t}^{k+1,n}, set Xtk+1,0=XtkX\_{t}^{k+1,0}=X\_{t}^{k}, and repeat this inner Picard iteration until we achieve the desired convergence error in the L2L^{2}â€“norm, say at iteration ğ”«k+1{\mathfrak{n}}\_{k+1}. We then set Xk+1=Xk+1,ğ”«k+1X^{k+1}=X^{k+1,{\mathfrak{n}}\_{k+1}} resulting in the (k+1)tâ€‹h(k+1)^{th}â€“outer Picard iteration of the process XX.
2. (ii)

   Next, using (Wt,Wt0)tâˆˆğ’¯(W\_{t},W\_{t}^{0})\_{{t\in\mathcal{T}}} and (Xk+1)tâˆˆğ’¯(X^{k+1})\_{{t\in\mathcal{T}}}, we compute Stk+1S^{k+1}\_{t} using elicitability, i.e. we solve the minimization problem ([3](https://arxiv.org/html/2512.14967v1#S2.E3 "In 2. Problem Formulation â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise")) using Xtk+1X^{k+1}\_{t}. We approximate ğ”°âˆˆğ•ƒ2â€‹[â„±t0]{\mathfrak{s}}\in{\mathbb{L}}^{2}[\mathcal{F}^{0}\_{t}] as a recurrent neural network ğ”°â€‹(t,(Ws0)sâ‰¤t){\mathfrak{s}}(t,(W^{0}\_{s})\_{s\leq t}), for tâˆˆğ’¯{t\in\mathcal{T}}.
3. (iii)

   Finally, with (Wt,Wt0)tâˆˆğ’¯(W\_{t},W\_{t}^{0})\_{{t\in\mathcal{T}}}, (Xtk+1,Stk+1)tâˆˆğ’¯(X^{k+1}\_{t},S^{k+1}\_{t})\_{{t\in\mathcal{T}}} and (Ytk,Ztk,Zt0,k)tâˆˆğ’¯(Y^{k}\_{t},Z^{k}\_{t},Z^{0,k}\_{t})\_{{t\in\mathcal{T}}} fixed, we solve for (Ytk+1,Ztk+1,Zt0,k+1)tâˆˆğ’¯(Y^{k+1}\_{t},Z^{k+1}\_{t},Z^{0,k+1}\_{t})\_{t\in\mathcal{T}} by calculating the conditional expectation for Yk+1Y^{k+1} through elicitability using ([7](https://arxiv.org/html/2512.14967v1#S2.E7 "In 2. Problem Formulation â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise")). We parameterize Ytk+1âˆˆğ•ƒ2â€‹[ğ’¢t]Y^{k+1}\_{t}\in{\mathbb{L}}^{2}[\mathcal{G}\_{t}] as Uâ€‹(t,Xtk+1,Stk+1)U(t,X^{k+1}\_{t},S^{k+1}\_{t}) (the decoupling field) and approximate UU as the output of a neural network. Finally, we obtain Ztk+1Z^{k+1}\_{t} through automatic differentiation using the representation ([8](https://arxiv.org/html/2512.14967v1#S2.E8 "In 2. Problem Formulation â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise")) and Zt0,k+1Z^{0,k+1}\_{t} using elicitability as described in Section [2.2.4](https://arxiv.org/html/2512.14967v1#S2.SS2.SSS4 "2.2.4. Approximating ğ‘ and ğ‘â‚€ â€£ 2.2. Detailed description â€£ 2. Problem Formulation â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise").

These steps are summarized in Algorithm [1](https://arxiv.org/html/2512.14967v1#algorithm1 "In 2.1. Proposed method â€£ 2. Problem Formulation â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise") and the code is available at <https://github.com/fjpAntunes/mean-field-tools/tree/main/mean_field_tools/deep_bsde>.

Input: Drift Î¼â€‹(u,x,y,z,z0,s)\mu(u,x,y,z,z^{0},s); diffusion coefficients Ïƒâ€‹(u,x)\sigma(u,x) and Ïƒ0â€‹(u,x)\sigma\_{0}(u,x); Initial condition X0X\_{0}; Terminal condition gâ€‹(x,s)g(x,s); driver function fâ€‹(u,x,y,z,z0,s)f(u,x,y,z,z^{0},s); Time discretization ğ’¯\mathcal{T}; Samples of (Wt,Wt0)tâˆˆğ’¯(W\_{t},W\_{t}^{0})\_{t\in\mathcal{T}}; Initial (Yt0,Zt0,Zt0,0,St0)tâˆˆğ’¯(Y^{0}\_{t},Z^{0}\_{t},Z^{0,0}\_{t},S^{0}\_{t})\_{{t\in\mathcal{T}}}; Number of Picard iterations KK.

Output: Samples of (XtK,YtK,ZtK,Zt0,K,StK)tâˆˆğ’¯(X^{K}\_{t},Y^{K}\_{t},Z^{K}\_{t},Z^{0,K}\_{t},S^{K}\_{t})\_{t\in\mathcal{T}} providing an approximate solution to the system ([1](https://arxiv.org/html/2512.14967v1#S2.E1 "In 2. Problem Formulation â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise"))-([2](https://arxiv.org/html/2512.14967v1#S2.E2 "In 2. Problem Formulation â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise")); Trained neural networks SÎ¸S\_{\theta}, UÎ¸U\_{\theta}, vÎ¸v\_{\theta}.

1

2for *k=1k=1 to KK* do

3

4â€‚Â â€ƒGiven (Xtk,Ytk,Ztk,Zt0,k,Stk)tâˆˆğ’¯(X^{k}\_{t},Y^{k}\_{t},Z^{k}\_{t},Z^{0,k}\_{t},S^{k}\_{t})\_{t\in\mathcal{T}}, use Algorithm [2](https://arxiv.org/html/2512.14967v1#algorithm2 "In 2.2.1. Solving the forward SDE by Picard iterations â€£ 2.2. Detailed description â€£ 2. Problem Formulation â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise") to sample (Xtk+1)tâˆˆğ’¯(X^{k+1}\_{t})\_{t\in\mathcal{T}} ;

5

6â€‚Â â€ƒGiven (Xtk+1)tâˆˆğ’¯(X^{k+1}\_{t})\_{t\in\mathcal{T}}, use Algorithm [3](https://arxiv.org/html/2512.14967v1#algorithm3 "In 2.2.2. Estimating ğ‘† through elicitability â€£ 2.2. Detailed description â€£ 2. Problem Formulation â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise") to sample (St(k+1))tâˆˆğ’¯(S^{(k+1)}\_{t})\_{t\in\mathcal{T}} ;

7

8â€‚Â â€ƒGiven (Xtk+1,Ytk,Ztk,Zt0,k,Stk+1)tâˆˆğ’¯(X^{k+1}\_{t},Y^{k}\_{t},Z^{k}\_{t},Z^{0,k}\_{t},S^{k+1}\_{t})\_{t\in\mathcal{T}}, use Algorithm [4](https://arxiv.org/html/2512.14967v1#algorithm4 "In 2.2.3. Solving the backward SDE through elicitability â€£ 2.2. Detailed description â€£ 2. Problem Formulation â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise") to sample (Ytk+1,Ztk+1)tâˆˆğ’¯(Y^{k+1}\_{t},Z^{k+1}\_{t})\_{t\in\mathcal{T}} ;

9

10â€‚Â â€ƒGiven (Xtk+1,Ytk+1,Ztk+1,Zt0,k,Stk+1)tâˆˆğ’¯(X^{k+1}\_{t},Y^{k+1}\_{t},Z^{k+1}\_{t},Z^{0,k}\_{t},S^{k+1}\_{t})\_{t\in\mathcal{T}}, use Algorithm [5](https://arxiv.org/html/2512.14967v1#algorithm5 "In 2.2.4. Approximating ğ‘ and ğ‘â‚€ â€£ 2.2. Detailed description â€£ 2. Problem Formulation â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise") to sample (Zt0,k+1)tâˆˆğ’¯(Z^{0,k+1}\_{t})\_{t\in\mathcal{T}}.

11return *Samples of (XtK,YtK,ZtK,Zt0,K,StK)tâˆˆğ’¯(X^{K}\_{t},Y^{K}\_{t},Z^{K}\_{t},Z^{0,K}\_{t},S^{K}\_{t})\_{t\in\mathcal{T}}*

AlgorithmÂ 1 Picard Iterations and Elicitability for MV-FBSDE

To improve convergence and stability of the algorithm, we dampen the full iteration by performing â€˜soft-updatesâ€™ of the processes as follows

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | Î¨k+1\displaystyle\Psi^{k+1} | =Î´â€‹Î¨k+(1âˆ’Î´)â€‹Î¨^k+1,\displaystyle=\delta\,\Psi^{k}+(1-\delta)\,{\widehat{\Psi}}^{k+1}, |  | (9) |

where Î¨\Psi is XX, YY, ZZ and Z0Z^{0} and 0<1âˆ’Î´â‰ª10<1-\delta\ll 1 is a damping coefficient and hatted variables represent the updates computed from the steps above.

We continue the main outer iteration until a maximum number of iterations is reached, or the ğ•ƒ2{\mathbb{L}}^{2}â€“norm between samples of successive iterations of XX, YY, ZZ and Z0Z^{0} fall below a specified tolerance.

### 2.2. Detailed description

We next provide a detailed description of our methodology. As outlined above, our method consists in iterating between fixing the backward SDE and solving the forward SDE by Picard iterations, and fixing the forward SDE and solving the BSDE by elicitability. Each step of this process consists in a Picard iteration for the whole MV-FBSDE system. The MV aspect requires estimating StS\_{t}, which is achieved by elicitability as well.

We remind the reader that the outer-Picard iteration of the backward SDE is indexed by kk and, for each kk, the inner-Picard iteration of the forward SDE is indexed by nn.

#### 2.2.1. Solving the forward SDE by Picard iterations

Suppose we are at the ktâ€‹hk^{th} outer iteration, i.e., (Xk,Yk,Zk,Z0,k,Sk)(X^{k},Y^{k},Z^{k},Z^{0,k},S^{k}) are fixed. We define Xk+1,nX^{k+1,n} to the be ntâ€‹hn^{th}â€“inner Picard update of the forward process. To this end, set Xk+1,0=XkX^{k+1,0}=X^{k}, and given Xk+1,nX^{k+1,n}, we obtain Xk+1,n+1X^{k+1,n+1} through

|  |  |  |  |
| --- | --- | --- | --- |
|  | Xtk+1,n+1=X0\displaystyle X^{k+1,n+1}\_{t}=X\_{0} | +âˆ«0tÎ¼â€‹(u,Xuk+1,n,Yuk,Zuk,Zu0,k,Suk)â€‹du\displaystyle+\int\_{0}^{t}\mu(u,X^{k+1,n}\_{u},Y^{k}\_{u},Z^{k}\_{u},Z^{0,k}\_{u},S^{k}\_{u})\,{{\rm d}u} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +âˆ«0tÏƒâ€‹(u,Xuk+1,n)â€‹dWu+âˆ«0tÏƒ0â€‹(u,Xuk+1,n)â€‹dWu0,\displaystyle+\int\_{0}^{t}\sigma(u,X^{k+1,n}\_{u})\,{{\rm d}W}\_{u}+\int\_{0}^{t}\sigma\_{0}(u,X^{k+1,n}\_{u})\,{{\rm d}W}\_{u}^{0}\,, |  |

by numerically evaluating the right-hand side using Euler-Maruyama method with time discretization ğ’¯\mathcal{T} until â€–Xk+1,n+1âˆ’Xk+1,nâ€–ğ•ƒ2â€‹(Î©Ã—[0,T])\left\|X^{k+1,n+1}-X^{k+1,n}\right\|\_{{\mathbb{L}}^{2}(\Omega\times[0,T])} is below a specified tolerance; see Algorithm [2](https://arxiv.org/html/2512.14967v1#algorithm2 "In 2.2.1. Solving the forward SDE by Picard iterations â€£ 2.2. Detailed description â€£ 2. Problem Formulation â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise") for a pseudo-code, where we have suppressed the outer iteration counter, kk, for simplicity.

Input: Drift Î¼â€‹(u,x,y,z,z0,s)\mu(u,x,y,z,z^{0},s); diffusion coefficients Ïƒâ€‹(u,x)\sigma(u,x) and Ïƒ0â€‹(u,x)\sigma\_{0}(u,x); Initial condition X0X\_{0}; Time discretization ğ’¯\mathcal{T}; Tolerance parameter Îµ\varepsilon; MM samples of (Wt,Wt0)tâˆˆğ’¯(W\_{t},W\_{t}^{0})\_{t\in\mathcal{T}}; samples of (Yt,Zt,Zt0,St)tâˆˆğ’¯(Y\_{t},Z\_{t},Z\_{t}^{0},S\_{t})\_{t\in\mathcal{T}}.

Output: MM samples of (Xt)tâˆˆğ’¯(X\_{t})\_{t\in\mathcal{T}} at the next Picard iteration.

1

2Initialize Xt0â‰¡X0X^{0}\_{t}\equiv X\_{0}

3nâ†0n\leftarrow 0;

4
while *ğ”¢â‰¥Îµ{\mathfrak{e}}\geq\varepsilon* do

5â€‚Â â€ƒ
Using Euler-Maruyama in ğ’¯\mathcal{T}, approximate MM samples of

|  |  |  |
| --- | --- | --- |
|  | Xtn+1=X0+âˆ«0tÎ¼â€‹(u,Xun,Yu,Zu,Zu0,Su)â€‹du+âˆ«0tÏƒâ€‹(u,Xun)â€‹dWu+âˆ«0tÏƒ0â€‹(u,Xun)â€‹dWu0;X^{n+1}\_{t}=X\_{0}+\int\_{0}^{t}\mu(u,X^{n}\_{u},Y\_{u},Z\_{u},Z^{0}\_{u},S\_{u})\,\,{{\rm d}u}+\int\_{0}^{t}\sigma(u,X^{n}\_{u})\,{{\rm d}W}\_{u}+\int\_{0}^{t}\sigma\_{0}(u,X^{n}\_{u})\,{{\rm d}W}^{0}\_{u}; |  |

Compute error

|  |  |  |
| --- | --- | --- |
|  | ğ”¢=1Mâ€‹Nâ€‹âˆ‘j=1Nâˆ‘i=1Mâ€–Xtjn+1â€‹(i)âˆ’Xtjnâ€‹(i)â€–2;{\mathfrak{e}}=\frac{1}{MN}\sum\_{j=1}^{N}\sum\_{i=1}^{M}||X^{n+1(i)}\_{t\_{j}}-X^{n(i)}\_{t\_{j}}||^{2}; |  |

nâ†n+1n\leftarrow n+1;

6

return *Xn+1X^{n+1}*

AlgorithmÂ 2 Picard iteration of forward SDE with fixed (Y,Z,Z0,m)(Y,Z,Z^{0},m)

#### 2.2.2. Estimating SS through elicitability

Elicitability provides a framework for computing statistics through minimization of scoring functions [fissler2016higher]. For example, given a random variable XX taking values in â„\mathbb{R}, and a Ïƒ\sigma-algebra ğ’¢\mathcal{G}, the conditional expectation ğ”¼â€‹[Xâˆ£ğ’¢]\mathbb{E}\left[X\mid\mathcal{G}\right] can be characterized as the minimizer of the L2L^{2} loss:

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â€‹[X|ğ’¢]=argminYâˆˆğ•ƒ2â€‹(ğ’¢)ğ”¼â€‹[(Xâˆ’Y)2].\mathbb{E}\left[X|\mathcal{G}\right]=\operatorname\*{argmin}\_{Y\in{\mathbb{L}}^{2}(\mathcal{G})}\mathbb{E}\left[\big(X-Y\big)^{2}\right]. |  | (10) |

This characterization transforms the problem of computing conditional expectations into an minimization problem over ğ’¢\mathcal{G}-measurable random variables with second moment.

At the ktâ€‹hk^{th} outer iteration, we have samples from (Xk,Yk,Zk,Z0,k,Sk)tâˆˆğ’¯(X^{k},Y^{k},Z^{k},Z^{0,k},S^{k})\_{t\in\mathcal{T}} and in the previous section, we have obtained samples from (Xk+1)tâˆˆğ’¯(X^{k+1})\_{t\in\mathcal{T}}. We will next estimate Sk+1S^{k+1}.

We begin by analyzing the cases where the dependency on SS is through some conditional moment St=ğ”¼â€‹[Ï•â€‹(Xt)âˆ£â„±t0]S\_{t}=\mathbb{E}[\phi(X\_{t})\mid\mathcal{F}\_{t}^{0}]. When there is no common noise, Stk+1S^{k+1}\_{t} can be estimated by calculating the empirical average over samples of Ï•â€‹(Xtk+1)\phi(X^{k+1}\_{t}). In the case where there is common noise this would be numerically prohibited, as we need to condition on the state of common noise. Instead, we use elicitability and rewrite the conditional expectation as a minimization problem

|  |  |  |  |
| --- | --- | --- | --- |
|  | Stk+1=argminğ”°âˆˆğ•ƒ2â€‹[â„±t0]ğ”¼â€‹[(Ï•â€‹(Xtk+1)âˆ’ğ”°)2].S^{k+1}\_{t}=\operatorname\*{argmin}\_{{\mathfrak{s}}\in{\mathbb{L}}^{2}[\mathcal{F}^{0}\_{t}]}\mathbb{E}\left[\left(\phi(X\_{t}^{k+1})-{\mathfrak{s}}\right)^{2}\right]. |  | (11) |

More generally, if StS\_{t} is any elicitable statistic for a score function ğ”–{\mathfrak{S}} over the conditional probability measure of XtX\_{t} given â„±t0\mathcal{F}^{0}\_{t}, we may write

|  |  |  |  |
| --- | --- | --- | --- |
|  | Stk+1=argminğ”°âˆˆğ•ƒ2â€‹[â„±t0]ğ”¼â€‹[ğ”–â€‹(Xtk+1,ğ”°)].S\_{t}^{k+1}=\operatorname\*{argmin}\_{{\mathfrak{s}}\in{\mathbb{L}}^{2}[\mathcal{F}^{0}\_{t}]}\mathbb{E}\left[{\mathfrak{S}}(X\_{t}^{k+1},{\mathfrak{s}})\right]. |  | (12) |

For example, we may elicit the (conditional) Î±\alpha-quantile by using the score function

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”–â€‹(x,ğ”°)=(1{xâ‰¥ğ”°}âˆ’Î±)â€‹(xâˆ’ğ”°).{\mathfrak{S}}(x,{\mathfrak{s}})=\left(1\_{\{x\geq{\mathfrak{s}}\}}-\alpha\right)\,\left(x-{\mathfrak{s}}\right). |  | (13) |

On the other hand, Î±\alpha-expected shortfall is not elicitable on its own, but it is 22-elicitable, meaning Î±\alpha-quantile and Î±\alpha-expected shortfall are jointly elicitable. A brief exposition of elicitability can be found in [pesenti2024risk].

In general, the solution of the optimization ([12](https://arxiv.org/html/2512.14967v1#S2.E12 "In 2.2.2. Estimating ğ‘† through elicitability â€£ 2.2. Detailed description â€£ 2. Problem Formulation â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise")) may not be Markovian in W0W^{0}. Therefore, we consider a recurrent neural network (RNN) to the parametrize the function SS, and find an approximate solution for the minimization problem ([11](https://arxiv.org/html/2512.14967v1#S2.E11 "In 2.2.2. Estimating ğ‘† through elicitability â€£ 2.2. Detailed description â€£ 2. Problem Formulation â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise")) through stochastic gradient descent. See Algorithm [3](https://arxiv.org/html/2512.14967v1#algorithm3 "In 2.2.2. Estimating ğ‘† through elicitability â€£ 2.2. Detailed description â€£ 2. Problem Formulation â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise") for a pseudo-code, where we suppress the outer iteration counter, kk, for simplicity.

Input: Score function ğ”–{\mathfrak{S}} to elicit SS; MM samples of (Wt,Wt0)tâˆˆğ’¯(W\_{t},W\_{t}^{0})\_{t\in\mathcal{T}}; MM samples of (Xt)tâˆˆğ’¯(X\_{t})\_{t\in\mathcal{T}}; Batch size II; number of iterations EE.

Output: MM samples of (St)tâˆˆğ’¯(S\_{t})\_{t\in\mathcal{T}} with respect to the input samples of (Xt)tâˆˆğ’¯(X\_{t})\_{t\in\mathcal{T}} and (Wt0)tâˆˆğ’¯(W\_{t}^{0})\_{t\in\mathcal{T}}; Trained recurrent neural network SÎ¸S\_{\theta}.

1

2Initialize recurrent neural network SÎ¸S\_{\theta} with random weights Î¸\theta;

3

4for *iter = 1 to EE* do

5

6â€‚Â â€ƒsample II indexes {k1,â€¦,kI}\{k\_{1},\ldots,k\_{I}\} from {1,â€¦,M}\{1,\ldots,M\} with replacement.

7â€‚Â â€ƒCompute loss:

8â€‚Â â€ƒ
Lâ€‹(Î¸)=1Iâ€‹Nâ€‹âˆ‘tâˆˆğ’¯âˆ‘i=1Iğ”–â€‹(Xt(ki),SÎ¸â€‹(t,(Ws0â€‹(ki))uâ‰¤t,uâˆˆğ’¯))\displaystyle L(\theta)=\frac{1}{IN}\sum\_{{t\in\mathcal{T}}}\sum\_{i=1}^{I}{\mathfrak{S}}(X\_{t}^{(k\_{i})},S\_{\theta}(t,(W\_{s}^{0(k\_{i})})\_{u\leq t,u\in\mathcal{T}}));

9â€‚Â â€ƒ
Update Î¸\theta using Adam;

10

return *Trained recurrent neural network SÎ¸S\_{\theta}*

AlgorithmÂ 3 Estimating SS through elicitability for fixed (X,Y,Z,Z0)(X,Y,Z,Z^{0})

#### 2.2.3. Solving the backward SDE through elicitability

Given (Xk,Yk,Zk,Z0,k,Sk)(X^{k},Y^{k},Z^{k},Z^{0,k},S^{k}), from the previous two steps have estimated Xk+1X^{k+1} and Sk+1S^{k+1}, we next update YY. Based on the representation ([7](https://arxiv.org/html/2512.14967v1#S2.E7 "In 2. Problem Formulation â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise")), we consider the following minimization problem:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ytk+1=argminğ”œâˆˆğ•ƒ2â€‹[ğ’¢t]ğ”¼â€‹[(ğ”œâˆ’(Gâ€‹(XTk+1,STk+1)+âˆ«tTfâ€‹(u,Xuk+1,Yuk,Zuk,Zu0,k,Suk+1)â€‹du))2].Y^{k+1}\_{t}=\operatorname\*{argmin}\_{{\mathfrak{Y}}\in{\mathbb{L}}^{2}[\mathcal{G}\_{t}]}\mathbb{E}\left[\left({\mathfrak{Y}}-\left(G(X^{k+1}\_{T},S^{k+1}\_{T})+\int\_{t}^{T}f\big(u,X^{k+1}\_{u},Y^{k}\_{u},Z^{k}\_{u},Z^{0,k}\_{u},S^{k+1}\_{u}\big)\,{{\rm d}u}\right)\right)^{2}\right]. |  | (14) |

The structure given by the decoupling field in Equation ([5](https://arxiv.org/html/2512.14967v1#S2.E5 "In 2. Problem Formulation â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise")) implies that we may parameterize the backward process YY using a neural network that takes (t,Xt,St)(t,X\_{t},S\_{t}) as inputs.

Our numerical method consists in finding an approximate solution for the minimization problem
([14](https://arxiv.org/html/2512.14967v1#S2.E14 "In 2.2.3. Solving the backward SDE through elicitability â€£ 2.2. Detailed description â€£ 2. Problem Formulation â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise")). Denote the numerical solution of this minimization procedure by UÎ¸U\_{\theta}. Hence, we set Ytk+1=UÎ¸â€‹(t,Xtk+1,Stk+1)Y\_{t}^{k+1}=U\_{\theta}(t,X^{k+1}\_{t},S^{k+1}\_{t}). To improve convergence rates, we calculate the loss with time dependent weights ptp\_{t}, using a higher weight to the terminal condition - typically, pT=N2p\_{T}=\frac{N}{2} and pt=1p\_{t}=1 for t<Tt<T. We provide a summary of this part of the method in Algorithm [4](https://arxiv.org/html/2512.14967v1#algorithm4 "In 2.2.3. Solving the backward SDE through elicitability â€£ 2.2. Detailed description â€£ 2. Problem Formulation â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise"), where we have suppressed the counter kk for simplicity and we are using Î¸\theta to represent the generic neural network parameters throughout the paper.

Input: Terminal condition gg; driver function fâ€‹(u,x,y,z,z0,s)f(u,x,y,z,z^{0},s); Time discretization ğ’¯\mathcal{T};
weights ptp\_{t} and sum of weights P=âˆ‘tâˆˆğ’¯ptP=\sum\_{t\in\mathcal{T}}p\_{t};
MM samples of (Wt,Wt0)tâˆˆğ’¯(W\_{t},W\_{t}^{0})\_{t\in\mathcal{T}}; MM samples of (Xt,St)tâˆˆğ’¯(X\_{t},S\_{t})\_{t\in\mathcal{T}} at the next Picard iteration; MM samples (Yt,Zt,Zt0)tâˆˆğ’¯(Y\_{t},Z\_{t},Z\_{t}^{0})\_{t\in\mathcal{T}} at the current Picard iteration; Batch size II; number of iterations EE.

Output: MM samples of (Yt,Zt)tâˆˆğ’¯(Y\_{t},Z\_{t})\_{t\in\mathcal{T}} at the next Picard iteration; Trained neural network UÎ¸U\_{\theta}.

1

2Initialize neural network UÎ¸U\_{\theta} with random weights Î¸\theta;

3

4for *iter = 1 to EE* do

5

6â€‚Â â€ƒsample II indexes {k1,â€¦,kI}\{k\_{1},\ldots,k\_{I}\} from {1,â€¦,M}\{1,\ldots,M\} with replacement.

7â€‚Â â€ƒSet fu=fâ€‹(u,Xu,Yu,Zu,Zu0,Su)f\_{u}=f(u,X\_{u},Y\_{u},Z\_{u},Z^{0}\_{u},S\_{u}), for uâˆˆğ’¯u\in\mathcal{T} ;

8â€‚Â â€ƒ
Compute the loss

|  |  |  |
| --- | --- | --- |
|  | â„’â€‹(Î¸)=1Iâ€‹Pâ€‹âˆ‘tâˆˆğ’¯âˆ‘i=1Iptâ€‹(UÎ¸â€‹(t,Xt(ki),St(ki))âˆ’Gâ€‹(XT(ki),ST(ki))âˆ’âˆ‘uâˆˆğ’¯,uâ‰¥tfu(ki)â€‹Î”â€‹t)2;\mathcal{L}(\theta)=\frac{1}{IP}\sum\_{{t\in\mathcal{T}}}\sum\_{i=1}^{I}p\_{t}\left(U\_{\theta}(t,X\_{t}^{(k\_{i})},S\_{t}^{(k\_{i})})-G(X\_{T}^{(k\_{i})},S\_{T}^{(k\_{i})})-\sum\_{u\in{\mathcal{T}},u\geq t}f\_{u}^{(k\_{i})}\,\,\Delta t\right)^{2}; |  |

Update Î¸\theta using Adam;

9

10return *Trained neural network uÎ¸u\_{\theta}*

AlgorithmÂ 4 Estimating YY through elicitability with fixed (X,Z,Z0,m)(X,Z,Z^{0},m)

#### 2.2.4. Approximating ZZ and Z0Z\_{0}

We obtain Ztk+1Z^{k+1}\_{t} through automatic differentiation using the representation given in ([8](https://arxiv.org/html/2512.14967v1#S2.E8 "In 2. Problem Formulation â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise")):

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ztk+1=Ïƒâ€‹(t,Xtk+1)âŠºâ€‹âˆ‡xUÎ¸â€‹(t,Xtk+1,Stk+1).Z^{k+1}\_{t}=\sigma(t,X^{k+1}\_{t})^{\intercal}\nabla\_{x}U\_{\theta}(t,X^{k+1}\_{t},S^{k+1}\_{t}). |  | (15) |

A similar representation for Z0Z^{0} would require computing the Lyons derivative of UU, which is numerically prohibitive. Moreover, while YY, and hence ZZ, depend of the conditional law of XX only through SS, Z0Z^{0} may depend fully on Î¼\mu. Hence, we consider the following discretization of the backward dynamics of YY ([2](https://arxiv.org/html/2512.14967v1#S2.E2 "In 2. Problem Formulation â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise")) in the discretized times ğ’¯\mathcal{T}:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î”â€‹Yjk+1â‰”Ytj+1k+1âˆ’Ytjk+1â‰ˆâˆ’fjk+1â€‹(tj+1âˆ’tj)+Ztjk+1âŠºâ€‹Î”â€‹Wj+Ztj0,kâŠºâ€‹Î”â€‹Wj0,\Delta Y\_{j}^{k+1}\coloneqq Y\_{t\_{j+1}}^{k+1}-Y\_{t\_{j}}^{k+1}\approx-f\_{j}^{k+1}\,(t\_{j+1}-t\_{j})+Z\_{t\_{j}}^{k+1\intercal}\Delta W\_{j}+Z\_{t\_{j}}^{0,k\,\intercal}\Delta W^{0}\_{j}, |  | (16) |

where, for j=0,â€¦,Nâˆ’1j=0,\ldots,N-1, fjk+1=fâ€‹(tj,Xtjk+1,Ytjk+1,Ztjk+1,Ztj0,k,Stjk+1)f\_{j}^{k+1}=f(t\_{j},X\_{t\_{j}}^{k+1},Y\_{t\_{j}}^{k+1},Z\_{t\_{j}}^{k+1},Z\_{t\_{j}}^{0,k},S\_{t\_{j}}^{k+1}).
We may then update Ztj0Z^{0}\_{t\_{j}} by computing

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | Ztj0,k+1\displaystyle Z\_{t\_{j}}^{0,k+1} | =ğ”¼â€‹[(Î”â€‹Yjk+1Î”â€‹t+fjk+1)â€‹Î”â€‹Wj0|ğ’¢tj].\displaystyle=\mathbb{E}\left[\left.\left(\frac{\Delta Y\_{j}^{k+1}}{\Delta t}+f\_{j}^{k+1}\right)\Delta W^{0}\_{j}\right|\,\mathcal{G}\_{t\_{j}}\,\right]. |  | (17) |

Thus, as we have samples of (Ytk+1)tâˆˆğ’¯(Y\_{t}^{k+1})\_{t\in\mathcal{T}} and (ftk+1)tâˆˆğ’¯(f\_{t}^{k+1})\_{t\in\mathcal{T}} from the previous steps, we may elicit Z0,k+1Z^{0,k+1} by

|  |  |  |  |
| --- | --- | --- | --- |
|  | Zti0,k+1=argminâ„¨âˆˆğ•ƒ2â€‹[ğ’¢ti]ğ”¼â€‹[(â„¨âˆ’(Î”â€‹Yik+1Î”â€‹t+fik+1)â€‹Î”â€‹Wi0)2].Z\_{t\_{i}}^{0,k+1}=\operatorname\*{argmin}\_{\mathfrak{Z}\in{\mathbb{L}}^{2}[\mathcal{G}\_{t\_{i}}]}\mathbb{E}\left[\left(\mathfrak{Z}-\left(\frac{\Delta Y\_{i}^{k+1}}{\Delta t}+f\_{i}^{k+1}\right)\Delta W^{0}\_{i}\right)^{2}\right]. |  | (18) |

We provide a summary of this
part of the method in Algorithm [5](https://arxiv.org/html/2512.14967v1#algorithm5 "In 2.2.4. Approximating ğ‘ and ğ‘â‚€ â€£ 2.2. Detailed description â€£ 2. Problem Formulation â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise"). As we mentioned, in general, Z0Z^{0} might depend fully on Î¼\mu. Hence, we parametrize Z0Z^{0} as a recurrent neural network that takes (t,Xt,(Ws0)sâ‰¤t)(t,X\_{t},(W\_{s}^{0})\_{s\leq t}) as inputs, to encode this path-dependence.

Input: Driver function fâ€‹(u,x,y,z,z0,s)f(u,x,y,z,z^{0},s); Time discretization ğ’¯\mathcal{T}; MM samples of (Wt,Wt0)tâˆˆğ’¯(W\_{t},W\_{t}^{0})\_{t\in\mathcal{T}}; MM samples of (Xt,St,Yt,Zt)tâˆˆğ’¯(X\_{t},S\_{t},Y\_{t},Z\_{t})\_{t\in\mathcal{T}} at the next Picard iteration; MM samples of (Zt0)tâˆˆğ’¯(Z\_{t}^{0})\_{t\in\mathcal{T}} at the current Picard iteration; Batch size II; number of iterations EE.

Output: MM samples of (Zt0)tâˆˆğ’¯(Z^{0}\_{t})\_{t\in\mathcal{T}} at the next Picard iteration; Trained recurrent neural network vÎ¸v\_{\theta}.

1

2Initialize recurrent neural network vÎ¸v\_{\theta} with random weights Î¸\theta;

3

4for *iter = 1 to EE* do

5

6â€‚Â â€ƒsample II indexes {k1,â€¦,kI}\{k\_{1},\ldots,k\_{I}\} from {1,â€¦,M}\{1,\ldots,M\} with replacement.

7â€‚Â â€ƒSet fu=fâ€‹(u,Xu,Yu,Zu,Zu0,Su)f\_{u}=f(u,X\_{u},Y\_{u},Z\_{u},Z^{0}\_{u},S\_{u}) ;

8â€‚Â â€ƒ
Calculate Î”â€‹Yt(ki)â‰”Yt+Î”â€‹t(ki)âˆ’Yt(ki)\Delta Y^{(k\_{i})}\_{t}\coloneqq Y^{(k\_{i})}\_{t+\Delta t}-Y^{(k\_{i})}\_{t}
Compute the loss

|  |  |  |
| --- | --- | --- |
|  | â„’â€‹(Î¸)=1Iâ€‹Nâ€‹âˆ‘tâˆˆğ’¯âˆ–{T}âˆ‘i=1I(vÎ¸â€‹(t,Xt(ki),(Ws0â€‹(ki))sâ‰¤t,sâˆˆğ’¯)âˆ’(Î”â€‹Yt(ki)Î”â€‹t+ft(ki))â€‹Î”â€‹Wt0â€‹(ki))2;\mathcal{L}(\theta)=\frac{1}{IN}\sum\_{{t\in\mathcal{T}}\setminus\{T\}}\sum\_{i=1}^{I}\left(v\_{\theta}(t,X\_{t}^{(k\_{i})},(W\_{s}^{0(k\_{i})})\_{s\leq t,s\in\mathcal{T}})-\left(\frac{\Delta Y^{(k\_{i})}\_{t}}{\Delta t}+f\_{t}^{(k\_{i})}\right)\,\Delta W\_{t}^{0(k\_{i})}\right)^{2}; |  |

Update Î¸\theta using Adam;

9

10return *Trained recurrent neural network vÎ¸v\_{\theta}*

AlgorithmÂ 5 Estimating Z0Z^{0} through elicitability with fixed (X,Y,Z,m)(X,Y,Z,m)

### 2.3. Sampling after training

After completing Algorithm [1](https://arxiv.org/html/2512.14967v1#algorithm1 "In 2.1. Proposed method â€£ 2. Problem Formulation â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise") to the desired accuracy, we have approximate samples of the processes (X,Y,Z,Z0,S)(X,Y,Z,Z^{0},S) over the discretized times ğ’¯\mathcal{T}. Moreover, as a product of the algorithm, we also have neural networks to approximately sample YY, ZZ, Z0Z^{0}, given samples of (W,W0)(W,W^{0}) and XX. Therefore, if one wishes to sample the full set of processes (X,Y,Z,Z0,S)(X,Y,Z,Z^{0},S) after training, it may be done by discretizing the forward dynamics ([1](https://arxiv.org/html/2512.14967v1#S2.E1 "In 2. Problem Formulation â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise")), as described in Algorithm [6](https://arxiv.org/html/2512.14967v1#algorithm6 "In 2.3. Sampling after training â€£ 2. Problem Formulation â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise").

Input: Drift Î¼â€‹(u,x,y,z,z0,s)\mu(u,x,y,z,z^{0},s); diffusion coefficients Ïƒâ€‹(u,x)\sigma(u,x) and Ïƒ0â€‹(u,x)\sigma\_{0}(u,x); Initial condition X0X\_{0}; Time discretization ğ’¯\mathcal{T}; Samples of (Wt,Wt0)tâˆˆğ’¯(W\_{t},W\_{t}^{0})\_{t\in\mathcal{T}}.

Output: Approximate samples (Xt,Yt,Zt,Zt0,St)tâˆˆğ’¯(X\_{t},Y\_{t},Z\_{t},Z^{0}\_{t},S\_{t})\_{t\in\mathcal{T}}

1

2Set SuÎ¸=SÎ¸â€‹(u,(Ws0)sâ‰¤u)S\_{u}^{\theta}=S\_{\theta}(u,(W\_{s}^{0})\_{s\leq u}) and zÎ¸â€‹(u,x,s)=Ïƒâ€‹(u,x)âŠºâ€‹âˆ‡xUÎ¸â€‹(u,x,s)z\_{\theta}(u,x,s)=\sigma(u,x)^{\intercal}\nabla\_{x}U\_{\theta}(u,x,s);

3

4Using Euler-Maruyama in ğ’¯\mathcal{T}, approximate MM samples of

|  |  |  |  |
| --- | --- | --- | --- |
|  | Xt\displaystyle X\_{t} | =X0+âˆ«0tÎ¼â€‹(u,Xu,UÎ¸â€‹(u,Xu,SuÎ¸),zÎ¸â€‹(u,Xu,SuÎ¸),vÎ¸â€‹(u,Xu,(Ws0â€‹(i))sâ‰¤u),SuÎ¸)â€‹du\displaystyle=X\_{0}+\int\_{0}^{t}\mu(u,X\_{u},U\_{\theta}(u,X\_{u},S^{\theta}\_{u}),z\_{\theta}(u,X\_{u},S^{\theta}\_{u}),v\_{\theta}(u,X\_{u},(W\_{s}^{0(i)})\_{s\leq u}),S^{\theta}\_{u})\,\,{{\rm d}u} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +âˆ«0tÏƒâ€‹(u,Xu)â€‹dWu+âˆ«0tÏƒ0â€‹(u,Xu)â€‹dWu0;\displaystyle+\int\_{0}^{t}\sigma(u,X\_{u})\,{{\rm d}W}\_{u}+\int\_{0}^{t}\sigma\_{0}(u,X\_{u})\,{{\rm d}W}^{0}\_{u}; |  |

5return *Samples of (Xt,UÎ¸â€‹(t,Xt,StÎ¸),zÎ¸â€‹(t,Xt,StÎ¸),vÎ¸â€‹(t,Xt,(Ws0â€‹(i))sâ‰¤t),SÎ¸â€‹(t,(Ws0)sâ‰¤t))tâˆˆğ’¯(X\_{t},U\_{\theta}(t,X\_{t},S^{\theta}\_{t}),z\_{\theta}(t,X\_{t},S^{\theta}\_{t}),v\_{\theta}(t,X\_{t},(W\_{s}^{0(i)})\_{s\leq t}),S\_{\theta}(t,(W\_{s}^{0})\_{s\leq t}))\_{t\in\mathcal{T}}*

AlgorithmÂ 6 Approximate samples of MV-FBSDE after training

## 3. Numerical experiments

In this section, we present numerical experiments to demonstrate the effectiveness of our methodology. The first experiment solves the systemic risk banking model introduced in [carmona2013mean]. This model can be solved analytically, hence it is useful as a benchmark against our numerical methodology.
Building on this foundation, our second experiment modifies the systemic risk model by replacing interaction through the mean of the agents for an interaction through a quantile of the population.
Finally, our third experiment solves a non-stationary Aiyagariâ€“Bewleyâ€“Huggett model of income and wealth distribution [achdou2022income], where the mean field interaction is mediated through the interest rate and the agents are subjected to a common noise.

All experiments have the following neural network configuration:

* âŠ³\vartriangleright

  YY Neural net: 2 layers, 18 nodes per layer, Feed Forward.
* âŠ³\vartriangleright

  Z0Z^{0} neural net: a single GRU layer with a 2-dimensional hidden node, followed by a single Feed Forward layer for output.
* âŠ³\vartriangleright

  SS neural net: a single GRU layer with a 2-dimensional hidden node, followed by a single Feed Forward layer for output.

These NNs are small compared with many choices one can make, however, for the numerical experiments we investigate below, they prove to be expressive enough to approximate the solutions well.

Each Picard iteration consists of E=1,000E=1,000 backpropagation iterations for the YY neural net, E=500E=500 iterations for the Z0Z^{0} neural net, E=1,000E=1,000 iterations for the SS neural net, using batches of I=2,048I=2,048 paths. We use the Adam optimizer with learning rate 0.005 and a decay each 5 steps of 0.9997. Soft updates of neural networks in ([9](https://arxiv.org/html/2512.14967v1#S2.E9 "In 2.1. Proposed method â€£ 2. Problem Formulation â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise")) are performed with Î´=0.5\delta=0.5, following the fictitious play methodology described in [lauriere2021numerical]. Moreover, we discretize the time interval in N=101N=101 timesteps, sample M=10,000M=10,000 paths to train the neural networks, and we perform K=20K=20 (outer) Picard iterations. Each (outer) Picard iteration takes approximately 22 seconds to execute on a T4 machine in Google Colab.

### 3.1. Systemic risk banking model

#### 3.1.1. Problem formulation

As the model presented in [carmona2013mean] is linear-quadratic, it admits a explicit solution and hence provides useful testing ground.
In the model, each bank controls its rate of borrowing/lending to a central bank.
The state XtX\_{t} represents the log-monetary reserve of a representative bank, which reverts to the mean log-monetary reserve StS\_{t}.
The Nash equilibrium is characterized by the solution to the MV-FBSDE system

|  |  |  |  |
| --- | --- | --- | --- |
|  | {dâ€‹Xt=[(a+q)â€‹(Stâˆ’Xt)âˆ’Yt]â€‹dâ€‹t+Ïƒâ€‹dâ€‹Bt,X0=Î¾,dâ€‹Yt=[(a+q)â€‹Yt+(Ïµâˆ’q2)â€‹(Stâˆ’Xt)]â€‹dâ€‹t+Ztâ€‹dâ€‹Wt+Zt0â€‹dâ€‹Wt0,YT=câ€‹(XTâˆ’ST),\displaystyle\left\{\begin{aligned} {{\rm d}X}\_{t}&=\left[(a+q)(S\_{t}-X\_{t})-Y\_{t}\right]\,{{\rm d}t}+\sigma\,{{\rm d}B}\_{t},&X\_{0}=\xi,\\[2.5pt] {{\rm d}Y}\_{t}&=\left[(a+q)Y\_{t}+(\epsilon-q^{2})(S\_{t}-X\_{t})\right]\,{{\rm d}t}+Z\_{t}\,{{\rm d}W}\_{t}+Z\_{t}^{0}\,{{\rm d}W}\_{t}^{0},&Y\_{T}=c\,(X\_{T}-S\_{T}),\end{aligned}\right. |  | (19) |

where Bt=Ïâ€‹Wt0+1âˆ’Ï2â€‹WtB\_{t}=\rho\,W\_{t}^{0}+\sqrt{1-\rho^{2}}\,W\_{t} and SS is the conditional mean:

|  |  |  |  |
| --- | --- | --- | --- |
|  | St=ğ”¼â€‹[Xtâˆ£â„±t0].\displaystyle S\_{t}=\mathbb{E}\left[X\_{t}\mid\mathcal{F}\_{t}^{0}\,\right]. |  | (20) |

The analytic solution for YtY\_{t} is

|  |  |  |  |
| --- | --- | --- | --- |
|  | Yt=âˆ’Î·â€‹(t)â€‹(Stâˆ’Xt),Y\_{t}=-\eta(t)\,(S\_{t}-X\_{t}), |  | (21) |

where Î·\eta is the solution to the Ricatti ODE

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î·Ë™â€‹(t)=2â€‹(a+q)â€‹Î·â€‹(t)+Î·2â€‹(t)âˆ’(Ïµâˆ’q2),\dot{\eta}(t)=2(a+q)\eta(t)+\eta^{2}(t)-(\epsilon-q^{2}), |  | (22) |

which is given by

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î·â€‹(t)=âˆ’(Ïµâˆ’q2)â€‹(e(Î´+âˆ’Î´âˆ’)â€‹(Tâˆ’t)âˆ’2)âˆ’câ€‹(Î´+â€‹e(Î´+âˆ’Î´âˆ’)â€‹(Tâˆ’t)âˆ’Î´âˆ’)(Î´âˆ’â€‹e(Î´+âˆ’Î´âˆ’)â€‹(Tâˆ’t)âˆ’Î´+)âˆ’câ€‹(e(Î´+âˆ’Î´âˆ’)â€‹(Tâˆ’t)âˆ’1),\eta(t)=\frac{-(\epsilon-q^{2})\left(e^{(\delta^{+}-\delta^{-})(T-t)}-2\right)-c\left(\delta^{+}e^{(\delta^{+}-\delta^{-})(T-t)}-\delta^{-}\right)}{\left(\delta^{-}e^{(\delta^{+}-\delta^{-})(T-t)}-\delta^{+}\right)-c\left(e^{(\delta^{+}-\delta^{-})(T-t)}-1\right)}, |  | (23) |

with

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î´Â±=âˆ’(a+q)Â±(a+q)2+(Ïµâˆ’q2).\delta^{\pm}=-(a+q)\pm\sqrt{(a+q)^{2}+(\epsilon-q^{2})}. |  | (24) |

Hence, the dynamics for XtX\_{t} may be written

|  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- |
|  | dâ€‹Xt\displaystyle dX\_{t} | =[(a+q+Î·â€‹(t))â€‹(Stâˆ’Xt)]â€‹dâ€‹t+Ïƒâ€‹dâ€‹Bt,\displaystyle=\left[(a+q+\eta(t))(S\_{t}-X\_{t})\right]\,dt+\sigma\,{{\rm d}B}\_{t}, | X0=Î¾.\displaystyle X\_{0}=\xi. |  | (25) |

Applying conditional expectations on the common noise on ([25](https://arxiv.org/html/2512.14967v1#S3.E25 "In 3.1.1. Problem formulation â€£ 3.1. Systemic risk banking model â€£ 3. Numerical experiments â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise")),
we conclude that St=ğ”¼â€‹[Î¾]+Ïâ€‹Ïƒâ€‹Wt0S\_{t}=\mathbb{E}[\xi]+\rho\,\sigma\,W^{0}\_{t}.
Using the integrating factor Î˜â€‹(t)=âˆ«0tÎ¸â€‹(s)â€‹ğ‘‘s\Theta(t)=\int\_{0}^{t}\theta(s)ds, where Î¸â€‹(t)=a+q+Î·â€‹(t)\theta(t)=a+q+\eta(t), the complete analytical solution for the MV-FBSDE is given by

|  |  |  |  |
| --- | --- | --- | --- |
|  | {St=ğ”¼â€‹[Î¾]+Ïâ€‹Ïƒâ€‹Wt0,Xt=Î¾â€‹eâˆ’Î˜â€‹(t)+âˆ«0tÎ¸â€‹(u)â€‹Suâ€‹eâˆ’(Î˜â€‹(t)âˆ’Î˜â€‹(u))â€‹du+Ïƒâ€‹âˆ«0teâˆ’(Î˜â€‹(t)âˆ’Î˜â€‹(u))â€‹dBu,Yt=âˆ’Î·â€‹(t)â€‹(Stâˆ’Xt),Zt=Ïƒâ€‹Î·â€‹(t),Zt0=0.\begin{cases}\displaystyle S\_{t}=\mathbb{E}[\xi]+\rho\,\sigma\,W^{0}\_{t},\\[2.5pt] \displaystyle X\_{t}=\xi e^{-\Theta(t)}+\int\_{0}^{t}\theta(u)S\_{u}e^{-({\Theta(t)-\Theta(u)})}\,{{\rm d}u}+\sigma\int\_{0}^{t}e^{-({\Theta(t)-\Theta(u)})}\,{{\rm d}B}\_{u},\\[2.5pt] \displaystyle Y\_{t}=-\eta(t)(S\_{t}-X\_{t}),\\[2.5pt] \displaystyle Z\_{t}=\sigma\,\eta(t),\\[2.5pt] \displaystyle Z\_{t}^{0}=0.\end{cases} |  | (26) |

#### 3.1.2. Numerical simulation and parameters

For numerical experiments, we choose parameters values that appear in [carmona2013mean]: a=q=c=Ïƒ=1a=q=c=\sigma=1, Ïµ=10\epsilon=10, and Ï=0.3\rho=0.3. The initial condition Î¾\xi is sampled from the normal distribution with mean 0 and variance 44.

Figure [1](https://arxiv.org/html/2512.14967v1#S3.F1 "Figure 1 â€£ 3.1.2. Numerical simulation and parameters â€£ 3.1. Systemic risk banking model â€£ 3. Numerical experiments â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise")
shows a comparison between approximated and analytical solutions for two realizations, while, in Figure
[2](https://arxiv.org/html/2512.14967v1#S3.F2 "Figure 2 â€£ 3.1.2. Numerical simulation and parameters â€£ 3.1. Systemic risk banking model â€£ 3. Numerical experiments â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise"), we show a sample path as the outer Picard iterations evolve.

![Refer to caption](x1.png)


Figure 1. 
Comparison between analytic solution and approximated solution on two sample paths for the systemic risk with common noise experiment. Solid lines are the numerical solutions, and dotted lines represent the analytical solution. Each color pair (orange and blue, green and red) represents a particular path of each variable of the system.

![Refer to caption](x2.png)


Figure 2. 
The same sample path at iterations 1,2,4,8,16 and 32. As iteration increases, the color becomes darker.

### 3.2. Interaction through quantile

To test our algorithm in more challanging example, we apply the algorithm to a modified version of system ([19](https://arxiv.org/html/2512.14967v1#S3.E19 "In 3.1.1. Problem formulation â€£ 3.1. Systemic risk banking model â€£ 3. Numerical experiments â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise")) where the interaction term StS\_{t} is the Î±\alpha-quantile, with Î±=60%\alpha=60\%, see Equation ([13](https://arxiv.org/html/2512.14967v1#S2.E13 "In 2.2.2. Estimating ğ‘† through elicitability â€£ 2.2. Detailed description â€£ 2. Problem Formulation â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise")). We use the same values for all other parameters.

Figure [3](https://arxiv.org/html/2512.14967v1#S3.F3 "Figure 3 â€£ 3.2. Interaction through quantile â€£ 3. Numerical experiments â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise") shows two sample paths of the variables at Nash equilibrium, along with the elicited quantile Î±=60%\alpha=60\%. The sample paths are subjected to the same Brownian motions WtW\_{t} and Wt0W^{0}\_{t} as in Figure [1](https://arxiv.org/html/2512.14967v1#S3.F1 "Figure 1 â€£ 3.1.2. Numerical simulation and parameters â€£ 3.1. Systemic risk banking model â€£ 3. Numerical experiments â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise"). We can observe a upwards shift in the measure flow, when compared with the case where the interaction is through the mean.
Intuitively, when every agent strives to be better than average, the population as a whole drifts upwards.

![Refer to caption](x3.png)


Figure 3. Modified systemic risk model where interaction is mediated through the 60%60\% quantile. Respective paths (subjected to the same Brownian motion) of the original, mean interaction systemic risk model are shown in lighter shades. Note that the XtX\_{t} variable is greater in the quantile interaction model.

### 3.3. Economic growth model

#### 3.3.1. Problem formulation

Our next example examines the limit of a stochastic economy with NN agents who make consumption-savings decisions over a finite time horizon, such as those described in [achdou2022income], when NN goes to infinity. Each agent starts with random initial capital kik^{i} and chooses their consumption rate to maximize a given utility. The capital dynamics depend on consumption, depreciation, and an endogenous interest rate determined by aggregate capital in the economy. This creates strategic interaction through the interest rate â€” each individual agentâ€™s optimal consumption depends on the aggregate capital level, while the aggregate capital evolves based on all agentsâ€™ consumption choices. We then consider the mean field limit with a continuum of agents distributed according to a probability measure flow. The Nash equilibrium is then characterized by an MV-FBSDE system. The model demonstrates how individual optimization and aggregate dynamics couple through the interest rate mechanism.

Consider an idealized economy where the iith agent capital dynamics, denoted KtiK\_{t}^{i}, is governed by two factors in addition to their consumption cic^{i}:
a fixed depreciation rate Î´\delta and an endogenous interest rate rtr\_{t}.
Moreover, it is subjected to an idiosyncratic noise WiW^{i} and a common noise W0W^{0} that affects all agents. The dynamics is described by the SDE:

|  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- |
|  | dâ€‹Kti\displaystyle{{\rm d}K}\_{t}^{i} | =((rtâˆ’Î´)â€‹Ktiâˆ’cti)â€‹dâ€‹t+Ïƒâ€‹(Ïâ€‹dâ€‹Wt0+1âˆ’Ï2â€‹dâ€‹Wti),\displaystyle=\left(\left(r\_{t}-\delta\right)K\_{t}^{i}-c\_{t}^{i}\right)\,{{\rm d}t}+\sigma(\rho\,{{\rm d}W}\_{t}^{0}+\sqrt{1-\rho^{2}}\,{{\rm d}W}^{i}\_{t}), | K0i=ki.\displaystyle K\_{0}^{i}=k^{i}. |  | (27) |

We further assume that the economyâ€™s aggregate production PtP\_{t} is given by a function FF of the aggregate capital of the economy KÂ¯t\overline{K}\_{t}, that is,

|  |  |  |  |
| --- | --- | --- | --- |
|  | Pt=Fâ€‹(KÂ¯t),whereÂ â€‹KÂ¯t=1Nâ€‹âˆ‘i=1NKti.P\_{t}=F(\overline{K}\_{t}),\quad\text{where }\overline{K}\_{t}=\frac{1}{N}\sum\_{i=1}^{N}K\_{t}^{i}. |  | (28) |

In economic equilibrium, the interest rate rtr\_{t} is given by the marginal effect of capital âˆ‚KP\partial\_{K}P in the aggregate production,
which itself is a function of the aggregate capital in the economy KÂ¯t\overline{K}\_{t}.
This is the source of the mean field interactions in this model.

The agentsâ€™ consumption preferences are governed by the utility function uâ€‹(â‹…)u(\cdot),
and their preference for capital at the end of the time horizon are governed by a terminal utility Ïˆâ€‹(KTi)\psi(K\_{T}^{i}).
Therefore, agents choose their consumption c:[0,T]Ã—Î©âŸ¶Cc:[0,T]\times\Omega\longrightarrow C, with CâŠ‚â„C\subset\mathbb{R}, to maximize the following functional:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Jiâ€‹(ki,ci)=ğ”¼â€‹[âˆ«0Tuâ€‹(cti)â€‹dt+Ïˆâ€‹(KTi)],J^{i}(k^{i},c^{i})=\mathbb{E}\left[\int\_{0}^{T}u(c\_{t}^{i}){{\rm d}t}+\psi(K\_{T}^{i})\right], |  | (29) |

where kik^{i} is the initial capital and KTiK\_{T}^{i} is their capital at time TT when following the control process cic^{i}, and assumed to be sufficiently regular so the functional above is well defined. In all, the optimization problem faced by agent ii is

|  |  |  |  |
| --- | --- | --- | --- |
|  | {maxciâ‰¥0â¡ğ”¼â€‹[âˆ«0Tuâ€‹(csi)â€‹ğ‘‘s+Ïˆâ€‹(KTi)],subject todâ€‹Kti=[(âˆ‚KFâ€‹(KÂ¯t)âˆ’Î´)â€‹Ktiâˆ’cti]â€‹dâ€‹t+Ïƒâ€‹(Ïâ€‹dâ€‹Wt0+1âˆ’Ï2â€‹dâ€‹Wti),K0i=ki.\begin{cases}\displaystyle\max\_{c^{i}\geq 0}\mathbb{E}\left[\int\_{0}^{T}u(c^{i}\_{s})ds+\psi(K^{i}\_{T})\right],\\[2.5pt] \text{subject to}\\[2.5pt] dK\_{t}^{i}=\left[\left(\partial\_{K}F(\bar{K}\_{t})-\delta\right)K\_{t}^{i}-c\_{t}^{i}\right]\,dt+\sigma(\rho\,{{\rm d}W}\_{t}^{0}+\sqrt{1-\rho^{2}}\,{{\rm d}W}^{i}\_{t}),&K\_{0}^{i}=k^{i}.\end{cases} |  | (30) |

We next choose logarithmic utility uâ€‹(c)=logâ¡(c)u(c)=\log(c), and then we assume the consumption takes values in the positive reals C=â„+C=\mathbb{R}\_{+}. Moreover, we consder quadratic terminal cost Ïˆâ€‹(K)=âˆ’12â€‹K2\psi(K)=-\frac{1}{2}K^{2}, and quadratic aggregation function Fâ€‹(K)=C2â€‹K2F(K)=\frac{C}{2}K^{2}. With these choices, the optimal consumption is given by ctâ‹†=1/Ytc\_{t}^{\star}=1/Y\_{t}.
In the mean-field limit, we denote the distribution of KtK\_{t} by Î¼t\mu\_{t} and the average capital by St=âˆ«kâ€‹Î¼tâ€‹(dâ€‹k)S\_{t}=\int k\mu\_{t}({\rm d}k).
In this setting, the interest rate is given by rt=Câ€‹Str\_{t}=C\,S\_{t}.
Given an initial capital distribution Î¼0\mu\_{0}, the mean-field game is the solution to the MV-FBSDE system:

|  |  |  |  |
| --- | --- | --- | --- |
|  | {dâ€‹Kt=((rtâˆ’Î´)â€‹Ktâˆ’1Yt)â€‹dâ€‹t+Ïƒâ€‹(Ïâ€‹dâ€‹Wt0+1âˆ’Ï2â€‹dâ€‹Wt),K0âˆ¼Î¼0,dâ€‹Yt=âˆ’((rtâˆ’Î´)â€‹Yt)â€‹dâ€‹t+Ztâ€‹dâ€‹Wt+Zt0â€‹dâ€‹Wt0,YT=âˆ’KT.\begin{cases}{{\rm d}K}\_{t}=\left(\left(r\_{t}-\delta\right)K\_{t}-\frac{1}{Y\_{t}}\right)\,{{\rm d}t}+\sigma(\rho\,{{\rm d}W}\_{t}^{0}+\sqrt{1-\rho^{2}}\,{{\rm d}W}\_{t}),&K\_{0}\sim\mu\_{0},\\[2.5pt] {{\rm d}Y}\_{t}=-\left(\left(r\_{t}-\delta\right)Y\_{t}\right)\,{{\rm d}t}+Z\_{t}\,{{\rm d}W}\_{t}+Z^{0}\_{t}{{\rm d}W}\_{t}^{0},&Y\_{T}=-K\_{T}.\end{cases} |  | (31) |

The system implicitly describes the probability measure flow Î¼t=â„’â€‹(Kt|â„±t0)\mu\_{t}=\mathcal{L}(K\_{t}|\mathcal{F}^{0}\_{t}),
and the dynamics of this forward-backward system depend on Î¼t\mu\_{t} through rt=Câ€‹St=Câ€‹ğ”¼â€‹[Kt|â„±t0]r\_{t}=C\,S\_{t}=C\,\mathbb{E}[K\_{t}|\mathcal{F}^{0}\_{t}].
Note that the optimal consumption ctâˆ—c^{\*}\_{t} can be written as a function of (t,Kt,rt)(t,K\_{t},r\_{t}). Moreover, from the optimal consumption we can derive the marginal propensity to consume (MPC), given by

|  |  |  |
| --- | --- | --- |
|  | âˆ‚Kctâˆ—=âˆ’1Yt2â€‹ZtÏƒ.\partial\_{K}c^{\*}\_{t}=-\frac{1}{Y\_{t}^{2}}\frac{Z\_{t}}{\sigma}. |  |

This function describes, as a function of time, wealth and interest rate, how a increase in wealth is allocated between savings and consumption.

#### 3.3.2. Numerical simulation and parameters

The initial condition follows a normal distribution with mean 0.50.5 and standard deviation 0.50.5. The system parameters are C=1.5C=1.5, Î´=0.1\delta=0.1, and Ïƒ=0.1\sigma=0.1.
We plot solutions with and without common noise in Figures [4](https://arxiv.org/html/2512.14967v1#S3.F4 "Figure 4 â€£ 3.3.2. Numerical simulation and parameters â€£ 3.3. Economic growth model â€£ 3. Numerical experiments â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise") and [5](https://arxiv.org/html/2512.14967v1#S3.F5 "Figure 5 â€£ 3.3.2. Numerical simulation and parameters â€£ 3.3. Economic growth model â€£ 3. Numerical experiments â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise"), respectively.

In Figures [7](https://arxiv.org/html/2512.14967v1#S3.F7 "Figure 7 â€£ 3.3.2. Numerical simulation and parameters â€£ 3.3. Economic growth model â€£ 3. Numerical experiments â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise") and [7](https://arxiv.org/html/2512.14967v1#S3.F7 "Figure 7 â€£ 3.3.2. Numerical simulation and parameters â€£ 3.3. Economic growth model â€£ 3. Numerical experiments â€£ Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise"), we plot the marginal propensity to consume surface when subjected to common noise, as a function of the interest rate rtr\_{t} and capital KtK\_{t}, at time 0.50.5 and 0.90.9 respectively. Note that at time 0.50.5, MPC is increasing in KtK\_{t}, but concave in rtr\_{t} - the intuition being that higher interest rates increase the payoff of saving behavior. However, the effect of the interest rate is reduced as we approach the end of the time interval.

![[Uncaptioned image]](x4.png)

Figure 4. 
Numerical solution of economic growth model with common noise.

![[Uncaptioned image]](x5.png)

Figure 5. 
Numerical solution of economic growth model without common noise. Note that the model learns that Z0Z\_{0} is irrelevant in this setting.

![Refer to caption](x6.png)

Figure 6. Marginal propensity to consume as a function of KtK\_{t} and rtr\_{t}, for time t=0.5t=0.5.

![Refer to caption](x7.png)

Figure 7. Marginal propensity to consume as a function of KtK\_{t} and rtr\_{t}, for time t=0.9t=0.9.

## 4. Conclusion

We have presented a novel numerical method for solving McKean-Vlasov forward-backward stochastic differential equations with common noise. Our approach combines Picard iterations with elicitability principles and deep learning to address the two fundamental challenges in these systems: the forward-backward coupling and the dependence on the conditional measure flow. The key innovation of our method lies in the use of elicitability to compute conditional expectations, which enables us to handle the common noise case without resorting to computationally expensive nested Monte Carlo simulations.

Our numerical experiments demonstrate the effectiveness of the proposed method across problems of varying complexity. The systemic risk banking model provides validation against known analytical solutions, confirming that our algorithm accurately recovers the true solution processes. The extension to quantile-mediated interactions showcases the flexibility of the elicitability framework in handling statistics beyond conditional means. Finally, the economic growth model illustrates the methodâ€™s applicability to more complex, economically meaningful problems where analytical solutions are unavailable.

## Acknowledgments

SJ would like to acknowledge support from the Natural Sciences and Engineering Research Council of Canada through grant RGPIN-2024-04317. YS was supported by FAPERJ (Brasil) through the Jovem Cientista do Nosso Estado Program (E-26/201.375/2022 (272760)) and by CNPq (Brasil) through the Productivity in Research Scholarship (306695/2021-9). FA was supported by CAPES (Brazil) through Programa Institucional de InternacionalizaÃ§Ã£o (88887.939145/2024-00) and Programa Suporte Ã  PÃ³s-GraduaÃ§Ã£o (88887.705168/2022-00), by FGVâ€™s School of Applied Mathematics and by UofT through a Research Assistant Award.