---
authors:
- Jaisal Patel
- Yunzhe Chen
- Kaiwen He
- Keyi Wang
- David Li
- Kairong Xiao
- Xiao-Yang Liu
doc_id: arxiv:2512.08270v1
family_id: arxiv:2512.08270
is_current: true
taxonomy:
  alpha_families: []
  asset_classes: []
  horizons: []
  themes: []
title: Reasoning Models Ace the CFA Exams
url_abs: http://arxiv.org/abs/2512.08270v1
url_html: https://arxiv.org/html/2512.08270v1
venue: arXiv q-fin
version: 1
year: 2025
---


Jaisal Patel1, Yunzhe Chen2 , Kaiwen He3, Keyi Wang3,
David Li4,
  
Kairong Xiao5, Xiao-Yang Liu Yanglet3
  
1Rensselaer Polytechnic Institute, Troy, NY 12180
  
2University of North Carolina at Chapel Hill, Chapel Hill, NC 27599
  
3SecureFinAI Lab, Columbia University, New York, NY 10027
  
4Department of Mathematics, Columbia University, New York, NY 10027
  
5Business School, Columbia University, New York, NY 10027
  
Emails: XL2427@columbia.edu
  
Co-primary author.Corresponding author.

###### Abstract

Previous research has reported that large language models (LLMs) demonstrate poor performance on the Chartered Financial Analyst (CFA) exams. However, recent reasoning models have achieved strong results on graduate-level academic and professional examinations across various disciplines. In this paper, we evaluate state-of-the-art reasoning models on a set of mock CFA exams consisting of 980 questions across three Level I exams, two Level II exams, and three Level III exams. Using the same pass/fail criteria from prior studies, we find that most models clear all three levels. The models that pass, ordered by overall performance, are Gemini 3.0 Pro, Gemini 2.5 Pro, GPT-5, Grok 4, Claude Opus 4.1, and DeepSeek-V3.1. Specifically, Gemini 3.0 Pro achieves a record score of 97.6% on Level I. Performance is also strong on Level II, led by GPT-5 at 94.3%. On Level III, Gemini 2.5 Pro attains the highest score with 86.4% on multiple-choice questions while Gemini 3.0 Pro achieves 92.0% on constructed-response questions.

## 1 Introduction

The evaluation of large language models (LLMs) on high-stakes, domain-specific examinations has become a critical measure of their advancing capabilities. While impressive results on major benchmarks like the medical USMLE, the mathematical AIME, and the legal Uniform Bar Exam have demonstrated LLMs’ broad knowledge and reasoning, these assessments often test knowledge retrieval and logical deduction in isolation. In contrast, the finance domain [liu2023fingpt](https://arxiv.org/html/2512.08270v1#bib.bib8) ; [xie2024finben](https://arxiv.org/html/2512.08270v1#bib.bib14) ; [yanglet2025multimodal](https://arxiv.org/html/2512.08270v1#bib.bib15)  requires the simultaneous application of precise numerical calculations, qualitative analysis, and ethical judgment.

The Chartered Financial Analyst (CFA) certification is a globally recognized qualification for investment and financial professionals. The CFA program is structured into three levels that test an evolving hierarchy of skills and formats: Level I tests foundational knowledge through individual multiple-choice questions (MCQs); Level II tests application and analysis via case-based multiple-choice item sets (vignettes); and Level III tests complex synthesis and portfolio construction across specialized pathways using a combination of item sets and constructed-response questions (CRQs). This structure provides a detailed method for assessing LLM capabilities, allowing for an evaluation of foundational knowledge, application, and complex synthesis.

Beginning in 2023, research on CFA exams with LLMs has progressed from demonstrating poor performance to achieving passing scores. ([callanan2023can,](https://arxiv.org/html/2512.08270v1#bib.bib4) ) reported that ChatGPT (GPT-3.5-turbo) failed Levels I and II, while GPT-4 passed Level I and failed Level II. In 2024, ([mahfouz-etal-2024-state,](https://arxiv.org/html/2512.08270v1#bib.bib9) ) found that Claude 3 Opus and GPT-4o pass Levels I and II. More recently, ([shetty2025advanced,](https://arxiv.org/html/2512.08270v1#bib.bib12) ) showed that o4-mini, Gemini 2.5 Pro, and DeepSeek-R1 passed Level III. While these studies demonstrate rapid improvement, a single comprehensive evaluation of the recent generation of reasoning models across all three CFA levels remains absent.

In this paper, we first reproduce the results of general LLMs from [callanan2023can](https://arxiv.org/html/2512.08270v1#bib.bib4) ; [mahfouz-etal-2024-state](https://arxiv.org/html/2512.08270v1#bib.bib9)  on a set of mock exams for all levels of the CFA (three Level I, two Level II, and three Level III), consisting of a total of 980 questions, using the same settings and pass/fail criteria to establish a baseline. Second, we evaluate the current state-of-the-art reasoning models, including GPT-5, Gemini 3.0 Pro, DeepSeek-V3.1, and Grok 4, alongside predecessors, such as Gemini 2.5 Pro and Claude Opus 4.1, on the same set of mock exams, settings, and criteria. We find that most models meet the passing threshold across all three levels, as shown in [Table˜1](https://arxiv.org/html/2512.08270v1#S1.T1 "In 1 Introduction ‣ Reasoning Models Ace the CFA Exams"), with detailed accuracy metrics provided in [Table˜5](https://arxiv.org/html/2512.08270v1#S4.T5 "In 4.2 Evaluation of State-of-the-Art Reasoning Models ‣ 4 Experiment Results ‣ Reasoning Models Ace the CFA Exams").

Table 1: Pass/Fail outcomes for LLMs on mock CFA exams. Models are ranked by their average accuracy across all three levels. Black for [callanan2023can](https://arxiv.org/html/2512.08270v1#bib.bib4) ; [mahfouz-etal-2024-state](https://arxiv.org/html/2512.08270v1#bib.bib9)  and blue for ours.

Model Producer
Ranking
Model
Level I
Level II
Level III

![[Uncaptioned image]](images/model_provider_logos/openai.jpg) OpenAI
9
ChatGPT [ouyang2022training](https://arxiv.org/html/2512.08270v1#bib.bib11) 
Fail/Fail
Fail/Fail
Fail

8
GPT-4 [achiam2023gpt](https://arxiv.org/html/2512.08270v1#bib.bib2) 
Pass/Pass
Pass/Pass
Fail

7
GPT-4o [hurst2024gpt](https://arxiv.org/html/2512.08270v1#bib.bib7) 
Pass/Pass
Pass/Pass
Pass

3
GPT-5 [openai2025gpt5](https://arxiv.org/html/2512.08270v1#bib.bib10) 
Pass
Pass
Pass

![[Uncaptioned image]](images/model_provider_logos/google.jpg) Google
2
Gemini 2.5 Pro [comanici2025gemini](https://arxiv.org/html/2512.08270v1#bib.bib5) 
Pass
Pass
Pass

1
Gemini 3.0 Pro
Pass
Pass
Pass

![[Uncaptioned image]](images/model_provider_logos/deepseek.jpg) DeepSeek
6
DeepSeek-V3.1 [deepseekai2024deepseekv3technicalreport](https://arxiv.org/html/2512.08270v1#bib.bib6) 
Pass
Pass
Pass

![[Uncaptioned image]](images/model_provider_logos/xai.jpg) xAI
4
Grok 4 [xai2025grok4](https://arxiv.org/html/2512.08270v1#bib.bib13) 
Pass
Pass
Pass

![[Uncaptioned image]](images/model_provider_logos/anthropic.jpg) Anthropic
5
Claude Opus 4.1 [anthropic2025claude](https://arxiv.org/html/2512.08270v1#bib.bib3) 
Pass
Pass
Pass

## 2 Mock CFA Exam Dataset

### 2.1 Question Set

We compile a set of mock CFA exams across all three levels, with a total of 980 questions. The Level I set consists of three exams totaling 540 independent MCQs (180 per exam). The Level II set consists of two exams totaling 176 MCQs (88 per exam), organized into 22 item sets per exam (4 questions per set). The Level III set consists of three exams totaling 264 questions (88 per exam); each exam follows a hybrid format of 11 item sets (totaling 44 MCQs) and 11 constructed-response case studies (totaling 44 CRQs). Although the precise number and point-weighting of constructed-response questions vary in official CFA exams, these mock exams adhere to a standard, representative structure.

![Refer to caption](x1.png)


Figure 1: Sample mock CFA exam questions by level. Cases are shown in blue, questions in red, and answer choices in green. Examples are illustrative and not actual exam content.

### 2.2 Composition and Reproduction Validity

Data sources. We compile the mock exam dataset from two primary sources: [the official CFA Institute Practice Pack](https://www.cfainstitute.org/programs/cfa-program/candidate-resources/practice-pack) and [AnalystPrep](https://app.analystprep.com). For Levels I and II, we use the CFA Practice Pack from 2024 and 2025, respectively. For Level III, we use AnalystPrep Mock Exams from 2025.

In contrast, prior studies relied on different sources: [callanan2023can](https://arxiv.org/html/2512.08270v1#bib.bib4)  and [mahfouz-etal-2024-state](https://arxiv.org/html/2512.08270v1#bib.bib9)  used AnalystPrep Mock Exams from 2023 and 2024, respectively. Exact data replication is precluded because the questions in [callanan2023can](https://arxiv.org/html/2512.08270v1#bib.bib4)  remain undisclosed, and the datasets in [mahfouz-etal-2024-state](https://arxiv.org/html/2512.08270v1#bib.bib9)  reflect a superseded curriculum. Specifically, the 2024 curriculum update emphasized conceptual application for Levels I and II by shifting foundational calculations to prerequisite readings and substantially revising topics such as Corporate Issuers and Fixed Income, while the 2025 update introduced specialized Pathways for Level III. Furthermore, relying on older datasets increases the risk of benchmark contamination, where model performance reflects training data contamination rather than reasoning capability. Therefore, we ensure validity by utilizing materials that match the current examination standard, preserving the difficulty and relevance of the evaluation.

Topic distribution. The Levels I and II exams in the mock exam dataset cover all ten standard topics. For Level III, the structure aligns with the 2025 curriculum, covering six key areas: Asset Allocation (15–20%), Portfolio Construction (15–20%), Performance Measurement (5–10%), Derivatives & Risk Management (10–15%), Ethical Standards (10–15%), and the specialized Pathways (30–35%) in either Portfolio Management, Private Markets, or Private Wealth.

To validate representativeness, we compare our topic weight distribution with [callanan2023can](https://arxiv.org/html/2512.08270v1#bib.bib4)  and [mahfouz-etal-2024-state](https://arxiv.org/html/2512.08270v1#bib.bib9)  in [Table˜2](https://arxiv.org/html/2512.08270v1#S2.T2.fig1 "In 2.2 Composition and Reproduction Validity ‣ 2 Mock CFA Exam Dataset ‣ Reasoning Models Ace the CFA Exams"). Note that to ensure consistent comparison across studies, we map the topics to the high-level functional domains established in [mahfouz-etal-2024-state](https://arxiv.org/html/2512.08270v1#bib.bib9) , distributing them across Ethical Standards, Investment Tools, Asset Classes, and Portfolio Management. The exact breakdown of topics and question counts covered in the Level III mock exams is provided in [table˜8](https://arxiv.org/html/2512.08270v1#A2.T8 "In B.1 Error Statistics ‣ Appendix B Error Cases ‣ Reasoning Models Ace the CFA Exams") in the appendix.

Table 2: Comparison of mock exam topic weights (percentage) across studies.

|  |  |  |  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
|  | Level I | | | Level II | | | Level III | | |
| Topic Area | [callanan2023can](https://arxiv.org/html/2512.08270v1#bib.bib4) | [mahfouz-etal-2024-state](https://arxiv.org/html/2512.08270v1#bib.bib9) | This Work | [callanan2023can](https://arxiv.org/html/2512.08270v1#bib.bib4) | [mahfouz-etal-2024-state](https://arxiv.org/html/2512.08270v1#bib.bib9) | This Work | [callanan2023can](https://arxiv.org/html/2512.08270v1#bib.bib4) | [mahfouz-etal-2024-state](https://arxiv.org/html/2512.08270v1#bib.bib9) | This Work |
| Ethical Standards | 16.1% | 16.0% | 15.0% | 11.4% | 11.0% | 13.6% | - | 9.0% | 13.6% |
| Investment Tools | 39.2% | 39.0% | 35.4% | 43.1% | 43.0% | 34.0% | - | 0.0% | 0.0% |
| Quantitative Methods | 9.8% | 10.0% | 8.0% | 10.2% | 10.0% | 6.8% | - | - | - |
| Economics | 9.7% | 10.0% | 7.6% | 6.8% | 7.0% | 6.8% | - | - | - |
| Financial Reporting | 13.7% | 14.0% | 12.2% | 15.9% | 16.0% | 13.6% | - | - | - |
| Corporate Issuers | 6.0% | 5.0% | 7.6% | 10.2% | 10.0% | 6.8% | - | - | - |
| Asset Classes | 38.0% | 38.0% | 39.1% | 36.3% | 37.0% | 40.8% | - | 32.0% | 24.3% |
| Equity Investments | 15.9% | 16.0% | 12.4% | 13.6% | 14.0% | 13.6% | - | - | - |
| Fixed Income | 10.3% | 10.0% | 12.0% | 12.5% | 13.0% | 13.6% | - | - | - |
| Derivatives | 3.2% | 3.0% | 8.0% | 6.8% | 7.0% | 6.8% | - | - | - |
| Alternative Investments | 8.6% | 9.0% | 6.7% | 3.4% | 3.0% | 6.8% | - | - | - |
| Portfolio Management | 6.7% | 7.0% | 10.6% | 9.1% | 9.0% | 11.4% | - | 59.0% | - |
| Pathways | - | - | - | - | - | - | - | - | 62.1%†\dagger |
| #Mock exams | 5 | 2 | 3 | 2 | 2 | 2 | - | 2 | 3 |
| #Questions | 180 | 180 | 180 | 88 | 88 | 88 | - | 44\* | 88 |

\* [mahfouz-etal-2024-state](https://arxiv.org/html/2512.08270v1#bib.bib9)  reports 44 questions for Level III, likely referring to a single session. Our evaluation uses full-length exams.
  
†\dagger Level III mock exams in this work are the only ones using the 2025 updated curriculum, which introduces Pathways. This allocation is comparable to the 59.0% Portfolio Management weight in Level III reported by [mahfouz-etal-2024-state](https://arxiv.org/html/2512.08270v1#bib.bib9) .

As shown, the topic distribution broadly mirrors prior datasets. For Level I, the maximum deviation in topic weights is within 4.0 percentage points of both [callanan2023can](https://arxiv.org/html/2512.08270v1#bib.bib4)  and [mahfouz-etal-2024-state](https://arxiv.org/html/2512.08270v1#bib.bib9) . For Level II, we observe a notable shift in Investment Tools (34.0% vs. 43.0% and 43.1% in prior work), consistent with the reduced emphasis on foundational tools. For Level III, comparison with [mahfouz-etal-2024-state](https://arxiv.org/html/2512.08270v1#bib.bib9)  shows a reallocation of weight from Asset Classes to Ethical Standards and Pathways in the mock exams we use. Despite these adjustments, Investment Tools and Asset Classes constitute the majority of the curriculum for Levels I and II, whereas Level III retains its concentration on portfolio application. The functional focus of each level remains consistent across datasets, ensuring a comparable scope for reproduction.

Structural characteristics. We further validate our dataset by comparing the characteristics of Level I and Level II questions with those reported in [callanan2023can](https://arxiv.org/html/2512.08270v1#bib.bib4) . As shown in [Table˜3](https://arxiv.org/html/2512.08270v1#S2.T3 "In 2.2 Composition and Reproduction Validity ‣ 2 Mock CFA Exam Dataset ‣ Reasoning Models Ace the CFA Exams"), we observe a notable shift in question composition; specifically, our Level I dataset exhibits a lower density of calculation-based questions in Quantitative Methods (39.5% vs. 70.5%) and Economics (12.2% vs. 50.6%). This difference, once again, reflects the updated curriculum, which places greater emphasis on conceptual application, as well as variability in topic weighting across different mock exams. By validating models against the active curriculum, we ensure that the evaluation targets relevant professional requirements while preserving the difficulty required for a valid reproduction. This complexity is supported by an increase in information density, where our dataset features significantly longer average prompt lengths across both levels compared to prior work.

Table 3: Levels I and II question characteristics by topics: percentage of questions with numerical calculation, average number of tables per question, and average prompt length in tokens.
Black for [callanan2023can](https://arxiv.org/html/2512.08270v1#bib.bib4)  and
blue for ours.

Level I
Level II

Topic
Calculation
#Tab.
Length
Calculation
#Tab.
Length

Ethics
0.7%/0.0%
0.01/0.06
125/158
0.0%/0.0%
0.00/0.00
1013/1162

Quantitative Methods
70.5%/39.5%
0.26/0.26
131/148
27.8%/50.0%
0.00/1.67
1256/1197

Economics
50.6%/12.2%
0.25/0.12
121/147
66.7%/41.7%
2.00/1.33
1115/1020

Financial Reporting
57.7%/33.3%
0.35/0.32
151/152
53.6%/33.3%
2.79/1.50
1383/1072

Corporate Issuers
59.3%/19.5%
0.28/0.20
120/146
44.4%/58.3%
1.67/2.00
930/1135

Equity Investments
52.5%/28.4%
0.19/0.27
112/150
45.8%/58.3%
1.00/1.50
1053/1048

Fixed Income
43.0%/30.8%
0.06/0.15
87/151
50.0%/58.3%
1.45/1.17
779/1089

Derivatives
20.7%/20.9%
0.00/0.07
65/159
75.0%/58.3%
2.00/1.00
816/1073

Alternative Investments
36.4%/13.9%
0.06/0.11
85/157
66.7%/50.0%
2.00/2.33
840/1212

Portfolio Management
38.3%/24.6%
0.18/0.19
110/152
56.3%/25.0%
2.13/1.40
1077/1100

Overall
42.4%/22.0%
0.17/0.18
116/152
45.5%/40.9%
1.47/1.30
1058/1111

## 3 Evaluation Methodology

### 3.1 Experimental Setup

LLMs. We evaluate three groups of models: i) baselines used in [callanan2023can](https://arxiv.org/html/2512.08270v1#bib.bib4)  (ChatGPT, GPT-4); ii) the subsequent model from [mahfouz-etal-2024-state](https://arxiv.org/html/2512.08270v1#bib.bib9)  (GPT-4o); and iii) the state-of-the-art reasoning models (GPT-5, Gemini 3.0 Pro, DeepSeek-V3.1, Grok 4) alongside predecessors (Gemini 2.5 Pro, Claude Opus 4.1). The exact model identifiers and snapshots (date-stamped versions of models) are provided in [Table˜4](https://arxiv.org/html/2512.08270v1#S3.T4 "In 3.1 Experimental Setup ‣ 3 Evaluation Methodology ‣ Reasoning Models Ace the CFA Exams").

Table 4: Specific model identifiers and version snapshots used in our evaluation.

|  |  |  |  |
| --- | --- | --- | --- |
| Provider | Model | Identifier | Snapshot Date |
| Baselines reproduced from [callanan2023can](https://arxiv.org/html/2512.08270v1#bib.bib4)  and [mahfouz-etal-2024-state](https://arxiv.org/html/2512.08270v1#bib.bib9) | | | |
| OpenAI | ChatGPT (GPT-3.5 Turbo) | gpt-3.5-turbo | 25 Jan 2024 |
| OpenAI | GPT-4 | gpt-4 | 13 Jun 2023 |
| OpenAI | GPT-4o | gpt-4o-2024-08-06 | 06 Aug 2024 |
| Reasoning Models (This Work) | | | |
| OpenAI | GPT-5 | gpt-5-preview | 07 Aug 2025 |
| Google | Gemini 2.5 Pro | gemini-2.5-pro | 17 Jun 2025 |
| Google | Gemini 3.0 Pro | gemini-3-pro-preview | 18 Nov 2025 |
| xAI | Grok 4 | grok-4 | 09 Jul 2025 |
| Anthropic | Claude Opus 4.1 | claude-4.1-opus | 08 Aug 2025 |
| DeepSeek | DeepSeek-V3.1 | deepseek-v3.1 | 28 May 2025 |

Model parameters. To ensure reproducibility and comparability across models, we use provider-default parameters for all API calls, with the temperature set to 0 to minimize generation randomness. Exceptions were made for models where the temperature is not configurable (e.g., GPT-5). Note that a zero-temperature setting may impact the optimal reasoning capability of models. Due to variability across runs, we report results as the average score ±\pm standard deviation.

Prompting. We evaluate model performance under two distinct prompting conditions. The exact prompt templates for each setting are provided in [appendix˜A](https://arxiv.org/html/2512.08270v1#A1 "Appendix A Prompts and Instructions ‣ Reasoning Models Ace the CFA Exams").

* ∙\bullet

  Zero-Shot (ZS): The model is presented with the question context and instructed to output the final answer directly.
* ∙\bullet

  Chain-of-Thought (CoT): We use a Zero-Shot Chain-of-Thought approach, instructing the model to "think step-by-step" and "explain your reasoning" before generating the final answer.

### 3.2 Evaluation Metrics

Scoring. Each MCQ consists of three options, with exactly one correct answer. We report accuracy as the number of correct responses divided by the total question count. For CRQs, we employ o4-mini as an automated evaluator. We provide the model with the case context, question, reference answer, candidate response, and the AnalystPrep grading rubric. The specific prompt structure is detailed in [section˜A.3](https://arxiv.org/html/2512.08270v1#A1.SS3 "A.3 Automated Grading (Meta-Prompt) ‣ Appendix A Prompts and Instructions ‣ Reasoning Models Ace the CFA Exams"). Final CRQ scores are reported as a percentage, calculated as the total points awarded divided by the maximum possible score (132 points per exam).

Pass/Fail criteria. We adopt the following passing thresholds from prior studies to ensure consistent evaluation:

1. Level I.

   Pass if the score is ≥60%\geq 60\% in every individual topic and ≥70%\geq 70\% overall [callanan2023can](https://arxiv.org/html/2512.08270v1#bib.bib4) .
2. Level II.

   Pass if the score is ≥50%\geq 50\% in every individual topic and ≥60%\geq 60\% overall [callanan2023can](https://arxiv.org/html/2512.08270v1#bib.bib4) .
3. Level III.

   Pass if the average of the MCQ and CRQ scores is ≥63%\geq 63\% [300hourscfa](https://arxiv.org/html/2512.08270v1#bib.bib1) .

## 4 Experiment Results

### 4.1 Reproduction of Previous Work

We verify existing results from previous works by re-evaluating ChatGPT, GPT-4, and GPT-4o models under identical settings. Comparing our obtained results with those from [callanan2023can](https://arxiv.org/html/2512.08270v1#bib.bib4) ; [mahfouz-etal-2024-state](https://arxiv.org/html/2512.08270v1#bib.bib9) , such a reproduction serves as a validated baseline for evaluating reasoning models.

Our results align with [callanan2023can](https://arxiv.org/html/2512.08270v1#bib.bib4) , confirming that general-purpose LLMs perform poorly across all levels. ChatGPT consistently fails to meet passing criteria, achieving scores of 58.9%-68.4% on CFA Level I and 43.8%-48.3% on Level II. GPT-4 demonstrates stronger performance, scoring 73.3%–80.9% on Level I and 55.7%–69.9% on Level II, yet still fails to clear the Level II threshold in Zero-Shot settings. We find CoT prompting leads to substantial performance gains for GPT-4 (7.6–14.2 percentage points) and moderate gains for ChatGPT (4.5–5.5 percentage points) without changing the pass/fail outcome.

Consistent with [mahfouz-etal-2024-state](https://arxiv.org/html/2512.08270v1#bib.bib9) , we find that GPT-4o reliably passes Levels I and II. Our evaluation shows scores of 90.6% on Level I and 73.9% on Level II under the CoT setting, closely tracking the 88.1% and 76.7% reported in their evaluation. A notable divergence occurs at Level III, whereas [mahfouz-etal-2024-state](https://arxiv.org/html/2512.08270v1#bib.bib9)  reported a failing constructed-response score of 46.2%, our evaluation shows a passing score of 66.7%. This difference likely stems from the evolution of the mock exam datasets (2024 vs. 2025) and our use of the latest stable model snapshot (gpt-4o-2024-08-06), which offers improvements over the unspecified release evaluated in [mahfouz-etal-2024-state](https://arxiv.org/html/2512.08270v1#bib.bib9) .

### 4.2 Evaluation of State-of-the-Art Reasoning Models

We evaluate the current state-of-the-art reasoning models, specifically GPT-5, Gemini 3.0 Pro, DeepSeek-V3.1, and Grok 4, alongside a subset of predecessor models including Gemini 2.5 Pro, and Claude Opus 4.1.

Previous papers [callanan2023can](https://arxiv.org/html/2512.08270v1#bib.bib4) ; [mahfouz-etal-2024-state](https://arxiv.org/html/2512.08270v1#bib.bib9)  reported that LLMs at the time were unable to pass all three levels of the CFA exam. Our results demonstrate that this is no longer the case. As shown in [Table˜5](https://arxiv.org/html/2512.08270v1#S4.T5 "In 4.2 Evaluation of State-of-the-Art Reasoning Models ‣ 4 Experiment Results ‣ Reasoning Models Ace the CFA Exams"), this entire set of reasoning models not only passes all three CFA levels but also achieves nearly perfect scores in Levels I and II. Gemini 3.0 Pro achieves the highest score on Level I with97.6% (ZS), while GPT-5 leads on Level II with 94.3% (ZS).

On the Level II exam, the predecessor model Gemini 2.5 Pro scores the highest accuracy on multiple-choice questions (86.4%), and the newer Gemini 3.0 Pro demonstrates a significant advantage on constructed-response questions, scoring 92.0% compared to 82.8% for Gemini 2.5 Pro. These results indicate that reasoning models surpass the expertise required of entry-level to mid-level financial analysts and may achieve senior-level financial analyst proficiency in the future. These results indicate that while this class of models has mastered the codified knowledge of Levels I and II, the latest state-of-the-art iterations are specifically extending capabilities in the complex synthesis required for Level III.

Table 5: Overall performance of models on mock CFA exams (Accuracy) in zero-shot (ZS) and chain-of-thought (CoT). Black for [callanan2023can](https://arxiv.org/html/2512.08270v1#bib.bib4) ; [mahfouz-etal-2024-state](https://arxiv.org/html/2512.08270v1#bib.bib9)  and blue for ours.

Model
Setting
Level I
Level II
Level III MCQ
Level III CRQ

ChatGPT [ouyang2022training](https://arxiv.org/html/2512.08270v1#bib.bib11) 
ZS
58.8±\pm0.2/58.9±\pm1.0
46.6±\pm0.6/43.8±\pm3.5
54.5±\pm1.9
44.4±\pm1.9

CoT
58.0±\pm0.2/64.4±\pm2.5
47.2±\pm0.3/48.3±\pm1.0
44.2±\pm6.0/52.3±\pm3.5
17.4±\pm2.1/44.4±\pm1.5

GPT-4 [achiam2023gpt](https://arxiv.org/html/2512.08270v1#bib.bib2) 
ZS
73.2±\pm0.2/73.3±\pm1.0
57.4±\pm1.5/55.7±\pm2.5
59.8±\pm1.9
57.8±\pm1.7

CoT
74.0±\pm0.2/80.9±\pm3.3
61.4±\pm0.9/69.9±\pm4.3
65.9±\pm3.8
58.1±\pm3.2

GPT-4o [hurst2024gpt](https://arxiv.org/html/2512.08270v1#bib.bib7) 
ZS
80.0±\pm0.0
71.6±\pm1.4
63.6±\pm1.7
62.8±\pm5.6

CoT
88.1±\pm0.3/90.6±\pm1.0
76.7±\pm0.7/73.9±\pm5.2
63.4±\pm4.2/68.9±\pm1.0
46.2±\pm3.3/66.7±\pm1.8

GPT-5 [openai2025gpt5](https://arxiv.org/html/2512.08270v1#bib.bib10) 
ZS
96.1±\pm1.0
94.3±\pm2.9
73.5±\pm 2.5
71.8±\pm0.4

CoT
96.7±\pm1.9
92.6±\pm1.4
75.0±\pm3.5
71.8±\pm1.9

Gemini 2.5 Pro [comanici2025gemini](https://arxiv.org/html/2512.08270v1#bib.bib5) 
ZS
95.7±\pm0.0
92.6±\pm1.4
84.1±\pm1.9
78.2±\pm3.0

CoT
96.1±\pm1.7
92.6±\pm1.4
86.4±\pm3.8
82.8±\pm1.5

Gemini 3.0 Pro
ZS
97.6±\pm0.0
93.2±\pm0.0
81.8±\pm0.0
86.6±\pm1.2

CoT
97.0±\pm0.5
92.0±\pm0.0
80.3±\pm0.0
92.0±\pm3.5

DeepSeek-V3.1 [deepseekai2024deepseekv3technicalreport](https://arxiv.org/html/2512.08270v1#bib.bib6) 
ZS
90.9±\pm1.0
85.2±\pm2.5
81.1±\pm1.7
70.8±\pm4.3

CoT
91.3±\pm1.0
85.8±\pm1.4
81.8 ±\pm0.0
72.0±\pm2.6

Grok 4 [xai2025grok4](https://arxiv.org/html/2512.08270v1#bib.bib13) 
ZS
94.8±\pm1.0
85.2±\pm2.5
78.0±\pm1.0
71.2±\pm2.6

CoT
95.9±\pm0.0
86.4 ±\pm1.4
78.0±\pm1.7
80.2±\pm3.3

Claude Opus 4.1 [anthropic2025claude](https://arxiv.org/html/2512.08270v1#bib.bib3) 
ZS
94.6±\pm1.0
89.8±\pm1.4
75.0±\pm1.0
73.4±\pm3.1

CoT
94.8±\pm1.0
89.8±\pm4.3
74.2±\pm3.3
79.0±\pm4.8

### 4.3 Performance Analysis

Impact of prompting strategy. We observe a distinct divergence in the efficacy of Chain-of-Thought (CoT) prompting across model generations. For baseline models, CoT provides substantial gains on Levels I and II, improving GPT-4 accuracy by 7.6–14.2 percentage points and ChatGPT by 4.5–5.5 percentage points. This suggests that explicit reasoning steps are critical for earlier architectures to bridge the gap between knowledge recall and application.

In contrast, reasoning models exhibit inconsistent responses to explicit prompting on multiple-choice questions across all levels. While Grok 4 shows standard improvements, Gemini 3.0 Pro exhibits slight regressions under CoT settings for Level I (-0.6%), Level II (-1.2%), and Level III MCQs (-1.5%). Similarly, GPT-5 shows performance drops on Level II (-1.7%). However, this trend reverses for constructed-response questions, where CoT remains highly effective. For example, Gemini 3.0 Pro’s performance on CRQs jumps from 86.6% (ZS) to 92.0% (CoT) and Claude Opus 4.1 from 73.4% to 79.0%. This suggests that while modern architectures may be approaching a performance ceiling for closed-ended tasks, explicit reasoning appears constructive for the synthesis required in open-ended tasks. We note, however, that this improvement on CRQs may partially reflect the verbosity bias of automated evaluators, a limitation further analyzed in [Section˜5.2](https://arxiv.org/html/2512.08270v1#S5.SS2 "5.2 Automated Scoring Validity ‣ 5 Limitations and Future Work ‣ Reasoning Models Ace the CFA Exams").

Topic-level performance shifts. Our results also show a shift in difficulty distribution compared to [mahfouz-etal-2024-state](https://arxiv.org/html/2512.08270v1#bib.bib9) , which identified quantitative domains as a primary weakness for LLMs. In our evaluation, advanced reasoning models appear to have overcome this bottleneck. For instance, GPT-5 and Grok 4 achieve near-zero error rates on Quantitative Methods, Equity Investments, and Economics across Levels I and II. Conversely, Ethical and Professional Standards remains a persistent challenge, exhibiting the highest relative error rates among the top-performing reasoning models (e.g., ≈\approx17–21% on Level II). The exact breakdown of errors by topic is provided in [appendix˜B](https://arxiv.org/html/2512.08270v1#A2 "Appendix B Error Cases ‣ Reasoning Models Ace the CFA Exams").

Generational trade-offs. We observe a distinct trade-off in the transition from Gemini 2.5 Pro to Gemini 3.0 Pro. While the newer model demonstrates superior capability in constructed-response questions (92.0% vs. 82.8%), it exhibits a slight regression on multiple-choice tasks (80.3% vs. 86.4% on Level III MCQs).

## 5 Limitations and Future Work

### 5.1 CFA Exam Representation

While our Level I and Level II evaluations use official CFA Institute material, our Level III dataset relies on third-party mock exams (AnalystPrep) to maintain consistency with [mahfouz-etal-2024-state](https://arxiv.org/html/2512.08270v1#bib.bib9) . We acknowledge that third-party materials may differ from official examinations in vignette complexity, narrative depth, and distractor subtlety. Future work should prioritize official mock exams to maximize representativeness.

### 5.2 Automated Scoring Validity

For Level III constructed-response questions, we used o4-mini for automated grading based on standardized rubrics. This approach, while necessary for scalable evaluation, introduces potential measurement error. Research on LLM-based evaluation indicates a tendency toward a verbosity bias, where judges favor longer, comprehensive-sounding responses even if they lack specific technical precision. Furthermore, automated evaluators may fail to penalize subtle logical inconsistencies as strictly as human experts. The CRQ scores reported in this work should be interpreted as a model-based approximation. Future research requires validation by qualified CFA charterholders to establish a human-verified ground truth.

### 5.3 Data Contamination Risks

A significant limitation of all LLM evaluations is the risk of training data contamination. While the mock exams used in this study are proprietary, paywalled, and relatively new, we cannot definitively rule out the possibility of indirect leakage. For example, paraphrased reconstructions or derivative discussions of these questions may appear in public training corpora. High performance could therefore partially reflect memorization rather than pure reasoning capability. Establishing a completely contamination-free evaluation environment remains an open challenge in the field.

## 6 Conclusion

In this paper, we present a comprehensive study of state-of-the-art reasoning models on mock CFA exams across all three levels, evaluating their performance against reproduced baselines from [callanan2023can](https://arxiv.org/html/2512.08270v1#bib.bib4)  and [mahfouz-etal-2024-state](https://arxiv.org/html/2512.08270v1#bib.bib9)  on a set of mock exams.

We find that top models achieve near-perfect scores on Level I (exceeding 97%) and demonstrate high proficiency on Level II (over 94%). Specifically, Gemini 3.0 Pro achieves a record score of 97.6% on Level I, while GPT-5 leads on Level II with 94.3%. On Level III, Gemini 2.5 Pro attains the highest score on multiple-choice questions with 86.4%, while Gemini 3.0 Pro achieves 92.0% on constructed-response questions.

These results suggest that current models have now largely mastered the codified knowledge base of Levels I and II. Furthermore, the substantial improvement in constructed-response performance by the latest reasoning generation indicates a growing capability for the complex synthesis required for Level III. Collectively, these findings establish new, unified performance baselines for future research.

## Acknowledgement

Keyi Wang and Xiao-Yang Liu Yanglet acknowledge the support from Columbia’s SIRS and STAR Program, The Tang Family Fund for Research Innovations in FinTech, Engineering, and Business Operations.

## References

* [1]

  300Hours.
  CFA Passing Score: MPS Estimates to Help Your Prep.
  <https://300hours.com/cfa-passing-score/>, 2025.
* [2]

  Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.
  GPT-4 technical report.
  arXiv preprint arXiv:2303.08774, 2023.
* [3]

  Anthropic.
  Claude Opus 4.1.
  <https://www.anthropic.com/news/claude-opus-4-1>, 2025.
* [4]

  Ethan Callanan, Amarachi Mbakwe, Antony Papadimitriou, Yulong Pei, Mathieu Sibue, Xiaodan Zhu, Zhiqiang Ma, Xiaomo Liu, and Sameena Shah.
  Can GPT models be financial analysts? An evaluation of ChatGPT and GPT-4 on mock CFA exams.
  In Proceedings of the Eighth Financial Technology and Natural Language Processing and the 1st Agent AI for Scenario Planning, pages 23–32, Jeju, South Korea, 3 August 2024.
* [5]

  Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al.
  Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities.
  arXiv preprint arXiv:2507.06261, 2025.
* [6]

  DeepSeek-AI.
  DeepSeek-V3 technical report, 2024.
* [7]

  Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al.
  GPT-4o system card.
  arXiv preprint arXiv:2410.21276, 2024.
* [8]

  Xiao-Yang Liu, Guoxuan Wang, Hongyang Yang, and Daochen Zha.
  FinGPT: Democratizing internet-scale data for financial large language models.
  Workshop on Instruction Tuning and Instruction Following, NeurIPS, 2023.
* [9]

  Mahmoud Mahfouz, Ethan Callanan, Mathieu Sibue, Antony Papadimitriou, Zhiqiang Ma, Xiaomo Liu, and Xiaodan Zhu.
  The state of the art of large language models on chartered financial analyst exams.
  In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 1068–1082, Miami, Florida, US, November 2024. Association for Computational Linguistics.
* [10]

  OpenAI.
  Introducing GPT-5.
  <https://openai.com/index/introducing-gpt-5/>, 2025.
* [11]

  Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
  Training language models to follow instructions with human feedback.
  Advances in Neural Information Processing Systems, 35:27730–27744, 2022.
* [12]

  Pranam Shetty, Abhisek Upadhayaya, Parth Mitesh Shah, Srikanth Jagabathula, Shilpi Nayak, and Anna Joo Fee.
  Advanced financial reasoning at scale: A comprehensive evaluation of large language models on CFA Level III.
  arXiv preprint arXiv:2507.02954, 2025.
* [13]

  xAI.
  Grok 4.
  <https://x.ai/news/grok-4>, 2025.
* [14]

  Qianqian Xie, Weiguang Han, Zhengyu Chen, Ruoyu Xiang, Xiao Zhang, Yueru He, Mengxi Xiao, Dong Li, Yongfu Dai, Duanyu Feng, et al.
  FinBen: A holistic financial benchmark for large language models.
  Advances in Neural Information Processing Systems, 37:95716–95743, 2024.
* [15]

  Xiao-Yang Liu Yanglet, Yupeng Cao, and Li Deng.
  Multimodal financial foundation models (MFFMs): Progress, prospects, and challenges.
  arXiv preprint arXiv:2506.01973, 2025.

## Appendix A Prompts and Instructions

### A.1 Multiple Choice Questions (Level I, II, & III MCQ)

{qbox}

System Prompts

Level I (Zero-Shot):
“You are a CFA (chartered financial analyst) taking a test to evaluate your knowledge of finance. You will be given a question along with three possible answers (A, B, and C). Provide only the correct answer (A, B, or C) without any reasoning or explanation.”

Level II & III MCQ (Zero-Shot with Context/Vignette):
“You are a CFA (chartered financial analyst) taking a test to evaluate your knowledge of finance. You will be given a case description and a question along with three possible answers (A, B, and C). Provide only the correct answer (A, B, or C) without any reasoning or explanation.”

Level I (Chain-of-Thought):
“You are a CFA (chartered financial analyst) taking a test to evaluate your knowledge of finance. You will be given a question along with three possible answers (A, B, and C). Before answering, think through the question step-by-step. Explain your reasoning, including any calculations. Indicate the correct answer (A, B, or C).”

Level II & III MCQ (Chain-of-Thought with Context/Vignette):
“You are a CFA (chartered financial analyst) taking a test to evaluate your knowledge of finance. You will be given a case description and a question along with three possible answers (A, B, and C). Before answering, think through the case step-by-step. Explain your reasoning, including any calculations. Indicate the correct answer (A, B, or C).”

{qbox}

User Prompts

Level I Question:

```
Question:
{question}
A. {a}
B. {b}
C. {c}
```

Level II & III Question:

```
Case:
{context}

Question:
{question}
A. {a}
B. {b}
C. {c}
```

### A.2 Level III Constructed-Response Questions (CRQ)

{qbox}

System Prompts

Zero-Shot (ZS):
“You are taking a test for the Chartered Financial Analyst (CFA) program designed to evaluate your knowledge of different topics in finance. You will be given a constructed-response question. Provide a clear, direct answer to the question.”

Chain-of-Thought (CoT):
“You are taking a test for the Chartered Financial Analyst (CFA) program designed to evaluate your knowledge of different topics in finance. You will be given a constructed-response question. Think step-by-step and respond with your thinking and answer the question.”

{qbox}

User Prompts

```
Case:
{context}

Question:
{question}

Answer:
```

### A.3 Automated Grading (Meta-Prompt)

{qbox}

Automated Grading Prompts

These prompts are used to grade CRQ responses.

System Prompt:
“You are a CFA Level 3 examiner tasked with grading constructed-response answers. You will be provided with:

1. 1.

   The complete vignette/case context
2. 2.

   The specific question
3. 3.

   The model answer with detailed explanation
4. 4.

   The grading rubric with specific criteria
5. 5.

   The student’s answer (which may include reasoning process)
6. 6.

   The score range: minimum 0, maximum {total\_points} points

Your task is to assign an integer score based on:

* •

  Technical accuracy
* •

  Reasoning quality
* •

  Relevance to the question
* •

  Communication clarity
* •

  Strict adherence to the grading rubric

Requirements:

* •

  Assign ONLY integer scores within the range 0–{total\_points}
* •

  Follow the rubric criteria exactly for point allocation
* •

  Base scoring on technical accuracy and rubric adherence
* •

  Return format: "Score: X" followed by brief justification”

User Prompt:

```
Please grade this student’s answer strictly according to the rubric:

COMPLETE VIGNETTE/CASE:
{context}

QUESTION:
{question}

MODEL ANSWER:
{explanation}

GRADING RUBRIC:
{grading_rubric}

SCORE RANGE: 0 to {total_points} points (integers only)

STUDENT’S ANSWER:
{student_answer}

Provide your grading as:
Score: [integer from 0 to {total_points}]
Justification: [brief explanation based on rubric criteria]
```

## Appendix B Error Cases

### B.1 Error Statistics

[Tables˜6](https://arxiv.org/html/2512.08270v1#A2.T6 "In B.1 Error Statistics ‣ Appendix B Error Cases ‣ Reasoning Models Ace the CFA Exams"), [7](https://arxiv.org/html/2512.08270v1#A2.T7 "Table 7 ‣ B.1 Error Statistics ‣ Appendix B Error Cases ‣ Reasoning Models Ace the CFA Exams") and [8](https://arxiv.org/html/2512.08270v1#A2.T8 "Table 8 ‣ B.1 Error Statistics ‣ Appendix B Error Cases ‣ Reasoning Models Ace the CFA Exams") provide a quantitative breakdown of model errors across all three CFA MCQ levels. The tables categorize the total number of incorrect answers by topic for each model. Notably, while newer models significantly reduce overall errors, topics like Ethical and Professional Standards remain a persistent challenge for nearly all models on Levels I and II. For Level III, errors are more distributed, though topics like Performance Measurement and Derivatives and Risk Management are difficult for even the most advanced models.

Table 6: Breakdown of Chain of Thought errors on the Level I MCQ exam. Cell format: Error Count (Error Rate %). The error rate is calculated based on the total questions (n) per topic.

Topic (n)
ChatGPT
GPT-4
GPT-4o
GPT-5
G2.5P
G3P
DeepSeek
Grok 4
Claude

Ethics (81)
41 (51%)
24 (30%)
12 (15%)
4 (5%)
7 (9%)
3 (4%)
19 (23%)
10 (12%)
12 (15%)

Quantitative Methods (43)
8 (19%)
5 (12%)
0 (0%)
0 (0%)
1 (2%)
0 (0%)
0 (0%)
0 (0%)
0 (0%)

Economics (41)
10 (24%)
4 (10%)
4 (10%)
3 (7%)
3 (7%)
1 (2%)
2 (5%)
2 (5%)
2 (5%)

Financial Reporting (66)
25 (38%)
14 (21%)
7 (11%)
1 (2%)
2 (3%)
3 (5%)
4 (6%)
2 (3%)
2 (3%)

Corporate Issuers (41)
12 (29%)
4 (10%)
4 (10%)
1 (2%)
1 (2%)
2 (5%)
4 (10%)
1 (2%)
3 (7%)

Equity Investments (67)
28 (42%)
11 (16%)
6 (9%)
2 (3%)
1 (1%)
2 (3%)
5 (7%)
3 (4%)
3 (4%)

Fixed Income (65)
20 (31%)
18 (28%)
8 (12%)
3 (5%)
1 (2%)
0 (0%)
2 (3%)
1 (2%)
4 (6%)

Derivatives (43)
18 (42%)
9 (21%)
5 (12%)
0 (0%)
1 (2%)
2 (5%)
3 (7%)
0 (0%)
0 (0%)

Alternative Investments (36)
9 (25%)
6 (17%)
4 (11%)
2 (6%)
1 (3%)
1 (3%)
2 (6%)
1 (3%)
0 (0%)

Portfolio Management (57)
18 (32%)
8 (14%)
1 (2%)
2 (4%)
3 (5%)
2 (4%)
6 (11%)
2 (4%)
2 (4%)

Total (540)
192 (36%)
103 (19%)
51 (9%)
18 (3%)
21 (4%)
16 (3%)
47 (9%)
22 (4%)
28 (5%)




Table 7: A breakdown of Chain of Thought errors on the Level II MCQ exam. Cell format: Error Count (Error Rate %).

Topic (n)
ChatGPT
GPT-4
GPT-4o
GPT-5
G2.5P
G3P
DeepSeek
Grok 4
Claude

Ethics (24)
13 (54%)
6 (25%)
8 (33%)
4 (17%)
5 (21%)
5 (21%)
11 (46%)
11 (46%)
5 (21%)

Quantitative Methods (12)
2 (17%)
0 (0%)
1 (8%)
0 (0%)
0 (0%)
0 (0%)
0 (0%)
0 (0%)
0 (0%)

Economics (12)
9 (75%)
5 (42%)
4 (33%)
0 (0%)
0 (0%)
0 (0%)
0 (0%)
0 (0%)
0 (0%)

Financial Reporting (24)
13 (54%)
6 (25%)
7 (29%)
3 (13%)
3 (13%)
2 (8%)
4 (17%)
2 (8%)
3 (13%)

Corporate Issuers (12)
6 (50%)
2 (17%)
1 (8%)
1 (8%)
1 (8%)
1 (8%)
1 (8%)
0 (0%)
2 (17%)

Equity Investments (24)
7 (29%)
2 (8%)
1 (4%)
0 (0%)
0 (0%)
2 (8%)
0 (0%)
0 (0%)
0 (0%)

Fixed Income (24)
14 (58%)
8 (33%)
9 (38%)
2 (8%)
2 (8%)
2 (8%)
1 (4%)
3 (13%)
2 (8%)

Derivatives (12)
8 (67%)
5 (42%)
3 (25%)
1 (8%)
0 (0%)
2 (17%)
1 (8%)
2 (17%)
2 (17%)

Alternative Investments (12)
6 (50%)
5 (42%)
3 (25%)
0 (0%)
0 (0%)
0 (0%)
0 (0%)
1 (8%)
0 (0%)

Portfolio Management (20)
6 (30%)
8 (40%)
4 (20%)
1 (5%)
0 (0%)
0 (0%)
4 (20%)
2 (10%)
1 (5%)

Total (176)
91 (52%)
53 (30%)
46 (26%)
13 (7%)
13 (7%)
14 (8%)
25 (14%)
24 (14%)
18 (10%)




Table 8: A breakdown of Chain of Thought errors on the Level III MCQ exam. Cell format: Error Count (Error Rate %). The error rate is calculated based on the total questions (n) per topic.

Topic (n)
ChatGPT
GPT-4
GPT-4o
GPT-5
G2.5P
G3P
DeepSeek
Grok 4
Claude

Asset Allocation (12)
7 (58%)
6 (50%)
4 (33%)
3 (25%)
3 (25%)
3 (25%)
0 (0%)
3 (25%)
3 (25%)

Portfolio Construction (24)
8 (33%)
7 (29%)
12 (50%)
4 (17%)
0 (0%)
6 (25%)
5 (21%)
7 (29%)
5 (21%)

Performance Measurement (12)
9 (75%)
4 (33%)
5 (42%)
9 (75%)
5 (42%)
8 (67%)
7 (58%)
5 (42%)
7 (58%)

Derivatives & Risk (24)
12 (50%)
9 (38%)
10 (42%)
5 (21%)
2 (8%)
3 (13%)
2 (8%)
2 (8%)
7 (29%)

Ethical Standards (12)
8 (67%)
7 (58%)
0 (0%)
4 (33%)
1 (8%)
0 (0%)
3 (25%)
4 (33%)
5 (42%)

Portfolio Management (16)
10 (63%)
5 (31%)
7 (44%)
5 (31%)
4 (25%)
2 (13%)
2 (13%)
6 (38%)
4 (25%)

Private Markets (16)
5 (31%)
3 (19%)
1 (6%)
0 (0%)
1 (6%)
0 (0%)
1 (6%)
1 (6%)
1 (6%)

Private Wealth (16)
4 (25%)
4 (25%)
3 (19%)
3 (19%)
2 (13%)
4 (25%)
4 (25%)
1 (6%)
2 (13%)

Total (132)
63 (48%)
45 (34%)
41 (31%)
33 (25%)
18 (14%)
26 (20%)
24 (18%)
29 (22%)
34 (26%)

### B.2 Error Examples

[Figures˜2](https://arxiv.org/html/2512.08270v1#A2.F2 "In B.2 Error Examples ‣ Appendix B Error Cases ‣ Reasoning Models Ace the CFA Exams"), [3](https://arxiv.org/html/2512.08270v1#A2.F3 "Figure 3 ‣ B.2 Error Examples ‣ Appendix B Error Cases ‣ Reasoning Models Ace the CFA Exams"), [4](https://arxiv.org/html/2512.08270v1#A2.F4 "Figure 4 ‣ B.2 Error Examples ‣ Appendix B Error Cases ‣ Reasoning Models Ace the CFA Exams"), [5](https://arxiv.org/html/2512.08270v1#A2.F5 "Figure 5 ‣ B.2 Error Examples ‣ Appendix B Error Cases ‣ Reasoning Models Ace the CFA Exams") and [6](https://arxiv.org/html/2512.08270v1#A2.F6 "Figure 6 ‣ B.2 Error Examples ‣ Appendix B Error Cases ‣ Reasoning Models Ace the CFA Exams") provide qualitative examples of common errors. Each figure presents a complete problem. This may include a case vignette (for Level II and III), a table, or a grading rubric (for CRQs). Each figure then shows the question (for MCQs, the correct answer choice is bolded), the model’s incorrect response, the ground truth answer, model grading (for CRQs), and a brief error analysis. The following examples illustrate the specific types of logical flaws that persist in GPT-5.

![Refer to caption](x2.png)

Figure 2: Example of a Concept Misapplication error, where the model incorrectly selects between two related propositions.



![Refer to caption](x3.png)

Figure 3: Example of a Rule Application error, where the model misapplies ethical standards to a specific case vignette.



![Refer to caption](x4.png)

Figure 4: Example of a Misinterpretation of Evidence error, where the model incorrectly flags a normal portfolio activity as a sign of a poor benchmark.



![Refer to caption](x5.png)

Figure 5: Example of a Concept Oversimplification error, where the model provides a common but incorrect generalization instead of the nuanced answer.



![Refer to caption](x6.png)

Figure 6: Example of a Calculation Error, where the model uses incorrect base values for calculating the hurdle amount and profit, leading to an incorrect carried interest.