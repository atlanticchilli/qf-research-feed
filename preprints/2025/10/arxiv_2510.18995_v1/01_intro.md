---
authors:
- Alexandre Boumezoued
- Adel Cherchali
- Vincent Lemaire
- Gilles Pag√®s
- Mathieu Truc
doc_id: arxiv:2510.18995v1
family_id: arxiv:2510.18995
is_current: true
taxonomy:
  alpha_families: []
  asset_classes: []
  horizons: []
  themes: []
title: Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for
  Nested Simulations
url_abs: http://arxiv.org/abs/2510.18995v1
url_html: https://arxiv.org/html/2510.18995v1
venue: arXiv q-fin
version: 1
year: 2025
---


Alexandre Boumezoued
&Adel Cherchali11footnotemark: 1
&Vincent Lemaire
&Gilles Pag√®s22footnotemark: 2
&Mathieu Truc
Milliman R&D, 14 avenue de la Grande Arm√©e, Paris, FranceLaboratoire de Probabilit√©s, Statistique et Mod√©lisation (LPSM), 4 rue Jussieu, FranceLaboratoire de Probabilit√©s, Statistique et Mod√©lisation (LPSM), Milliman R&D, 14 avenue de la Grande Arm√©e, Paris, France, email : mathieu.truc@milliman.com

###### Abstract

Estimating risk measures such as large loss probabilities and Value-At-Risk is fundamental in financial risk management and often relies on computationally intensive nested Monte Carlo methods. While Multi-Level Monte Carlo (MLMC) techniques and their weighted variants are typically more efficient, their effectiveness tends to deteriorate when dealing with irregular functions, notably indicator functions, which are intrinsic to these risk measures. We address this issue by introducing a novel MLMC parametrization that significantly improves performance in practical, non-asymptotic settings while maintaining theoretical asymptotic guarantees. We also prove that antithetic sampling of MLMC levels enhances efficiency regardless of the regularity of the underlying function. Numerical experiments motivated by the calculation of economic capital in a life insurance context confirm the practical value of our approach for estimating loss probabilities and quantiles, bridging theoretical advances and practical requirements in financial risk estimation.

## 1 Introduction

Let (Œ©,‚Ñ±,‚Ñô)(\Omega,\mathcal{F},\mathbb{P}) be a probability space on which all considered random variables will be defined in this article. Letting dd and qq be positive integers, we consider two random variables XX valued in ‚Ñùd\mathbb{R}^{d} and UU valued in ‚Ñùq\mathbb{R}^{q} independent from XX. Let F:‚Ñùd√ó‚Ñùq‚ü∂‚ÑùF:\mathbb{R}^{d}\times\mathbb{R}^{q}\longrightarrow\mathbb{R} be a Borel measurable function and denote Z:=F‚Äã(X,U)Z:=F(X,U). We make the general assumption that the distribution of XX is absolutely continuous (with respect to the Lebesgue measure) and that ZZ is square integrable. Let œà:x‚üºùîº‚Äã[F‚Äã(x,U)]\psi:x\longmapsto\mathbb{E}[F(x,U)] be the function from ‚Ñùd\mathbb{R}^{d} to ‚Ñù\mathbb{R} then œà\psi is a regular version of the conditional expectation of ZZ given XX. Finally we define L:=œà‚Äã(X)L:=\psi(X) and consider a measurable function f:‚Ñù‚ü∂‚Ñùf:\mathbb{R}\longrightarrow\mathbb{R}. Note that ‚Ñô\mathbb{P} almost surely (denoted a.s) we have L=ùîº‚Äã[Z|X]L=\mathbb{E}[Z|X].

In many practical applications of financial risk management, we need to estimate, by Monte Carlo
(MC) simulations, quantities of the form

|  |  |  |  |
| --- | --- | --- | --- |
|  | I:=ùîº‚Äã[f‚Äã(L)],I:=\mathbb{E}[f(L)], |  | (1) |

sometimes also quantiles of LL are of interest. Most of the time in practice, the conditional expectation LL cannot be sampled exactly.
In that case, one must rely on using approximate samples of LL which inexorably introduce a bias in the estimation. A general way to construct random variables approximating LL is to consider its MC approximation.

###### Definition 1.

Let K‚àà‚ÑïK\in\mathbb{N}, x‚àà‚Ñùdx\in\mathbb{R}^{d} and (Uk)k‚àà{1,‚Ä¶,K}(U\_{k})\_{k\in\{1,\dots,K\}} a sequence of independent and identically distributed (i.i.d.) copies of UU independent from XX. Then the random variable

|  |  |  |
| --- | --- | --- |
|  | E^K‚Äã(x):=1K‚Äã‚àëk=1KF‚Äã(x,Uk)\hat{E}\_{K}(x):=\frac{1}{K}\sum\_{k=1}^{K}F(x,U\_{k}) |  |

is a *MC approximation* of œà‚Äã(x)\psi(x) with KK inner samples. In addition, the random variable E^K‚Äã(X)\hat{E}\_{K}(X) is a *MC approximation* of LL.

An application of the strong law of large numbers for martingale ([[1](https://arxiv.org/html/2510.18995v1#bib.bib1)], Theorem 2.18) to the martingale (MK)K‚àà‚Ñï(M\_{K})\_{K\in\mathbb{N}} defined for all K‚àà‚ÑïK\in\mathbb{N} by MK:=‚àëk=1KF‚Äã(X,Uk)‚àíùîº‚Äã[F‚Äã(X,U)|X]M\_{K}:=\sum\_{k=1}^{K}F(X,U\_{k})-\mathbb{E}[F(X,U)|X] (with respect to the filtration generated by (X,U1,‚Ä¶,UK)K‚àà‚Ñï(X,U\_{1},\dots,U\_{K})\_{K\in\mathbb{N}}) ensures that E^K‚Äã(X)\hat{E}\_{K}(X) converges a.s to LL as the number of inner samples KK goes to infinity. Using the MC approximation E^K‚Äã(X)\hat{E}\_{K}(X) in place of LL in a MC estimation of II is known as a nested MC framework as it involves two stages of MC approximations. Typically, one would use a nested MC estimator of ([1](https://arxiv.org/html/2510.18995v1#S1.E1 "In 1 Introduction ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")) defined for (J,K)‚àà‚Ñï2(J,K)\in\mathbb{N}^{2} as

|  |  |  |  |
| --- | --- | --- | --- |
|  | IJ,K:=1J‚Äã‚àëj=1Jf‚Äã(1K‚Äã‚àëk=1KF‚Äã(Xk,Uj,k)),I\_{J,K}:=\frac{1}{J}\sum\_{j=1}^{J}f\left(\frac{1}{K}\sum\_{k=1}^{K}F(X\_{k},U\_{j,k})\right)\,, |  | (2) |

where (Xk)k‚àà{1,‚Ä¶,K}(X\_{k})\_{k\in\{1,\dots,K\}} are i.i.d copies of XX and (Uj,k)(j,k)‚àà{1,‚Ä¶‚ÄãJ}√ó{1,‚Ä¶‚ÄãK}(U\_{j,k})\_{(j,k)\in\{1,\dots J\}\times\{1,\dots K\}} are i.i.d copies of UU independent from (Xk)k‚àà{1,‚Ä¶,K}(X\_{k})\_{k\in\{1,\dots,K\}}. Under mild conditions, this estimator converges in a proper way toward II as JJ and KK goes to infinity (see, e.g., [[2](https://arxiv.org/html/2510.18995v1#bib.bib2)]). Borrowing from the nested MC literature in risk management (e.g., Gordy-Juneja [[3](https://arxiv.org/html/2510.18995v1#bib.bib3)], Giles & Haji-Ali [[4](https://arxiv.org/html/2510.18995v1#bib.bib4)], Lemaire-Pag√®s [[5](https://arxiv.org/html/2510.18995v1#bib.bib5)]), we refer to E^K‚Äã(X)\hat{E}\_{K}(X) as the inner expectation, the random variables F‚Äã(X,U1),‚Ä¶,F‚Äã(X,UK)F(X,U\_{1}),\dots,F(X,U\_{K}) as the inner samples (or scenarios) and realizations of XX as outer samples (or scenarios).

To alleviate the notations, for all integer KK, we define YK:=f‚Äã(E^K‚Äã(X))Y\_{K}:=f(\hat{E}\_{K}(X)), the MC approximation of Y:=f‚Äã(L)Y:=f(L) with KK inner samples. For a large class of functions ff the weak error I‚àíùîº‚Äã[YK]I-\mathbb{E}[Y\_{K}] vanishes as KK goes to infinity. For example, Gordy-Juneja [[3](https://arxiv.org/html/2510.18995v1#bib.bib3)] (Proposition 1) provide a first order expansion of the weak error when ff is an indicator function. Giorgi et. al. [[6](https://arxiv.org/html/2510.18995v1#bib.bib6)] (Proposition 4.1 and Proposition 5.1), proved a higher order expansion when ff is a smooth function as well as when ff is an indicator function.

The case when ff is an indicator function is particularly relevant in financial risk management. Indeed if, for u‚àà‚Ñùu\in\mathbb{R}, x‚àà‚Ñùx\in\mathbb{R}, f‚Äã(x)=ùüôx‚â§uf(x)=\mathbbm{1}\_{x\leq u} then I=‚Ñô‚Äã(L‚â§u)=FL‚Äã(u)I=\mathbb{P}(L\leq u)=F\_{L}(u) is the cumulative distribution function (c.d.f.) of LL. Then if LL is represents a future portfolio loss, we are estimating the probability that the portfolio will incur a loss lower than uu. It was studied by Gordy-Juneja in their seminal paper [[3](https://arxiv.org/html/2510.18995v1#bib.bib3)], and later on by many others (e.g., [[7](https://arxiv.org/html/2510.18995v1#bib.bib7)], [[4](https://arxiv.org/html/2510.18995v1#bib.bib4)], [[6](https://arxiv.org/html/2510.18995v1#bib.bib6)]).

In practical applications, the sampling of J‚àà‚ÑïJ\in\mathbb{N} i.i.d copies of YKY\_{K} typically proceeds in two stages. Initially, JJ outer scenarios (X1,‚Ä¶,XJ)(X\_{1},\dots,X\_{J}) are drawn independently from the distribution of XX. Subsequently, J√óKJ\times K i.i.d. samples (Uj,1,‚Ä¶,Uj,K),j‚àà{1,‚Ä¶,J}(U\_{j,1},\dots,U\_{j,K}),\;j\in\{1,\dots,J\} are drawn from the distribution of UU, which are then used to construct the J√óKJ\times K inner scenarios F‚Äã(Xj,Uj,1),‚Ä¶,F‚Äã(Xj,Uj,K),j‚àà{1,‚Ä¶,J}F(X\_{j},U\_{j,1}),\dots,F(X\_{j},U\_{j,K}),\;j\in\{1,\dots,J\}. These are then used to construct JJ samples YKj=f‚Äã(1K‚Äã‚àëk=1KF‚Äã(Xj,Uj,k))Y^{j}\_{K}=f(\frac{1}{K}\sum\_{k=1}^{K}F(X\_{j},U\_{j,k})) of YKY\_{K}. The unit outer sampling cost is, therefore, defined as the cost of sampling a single XX, whereas the unit inner sampling cost is defined as the combined cost of sampling one UU and evaluating F‚Äã(X,U)F(X,U). In this paper, we consequently adopt the following computational cost framework for sampling YKY\_{K}.

###### Definition 2.

For any integer KK we define the computational cost of sampling YKY\_{K} as

|  |  |  |  |
| --- | --- | --- | --- |
|  | Œ≥œÑ‚Äã(K)=Œ∫¬Ø‚Äã(œÑ+K),\gamma\_{\tau}(K)=\bar{\kappa}(\tau+K)\,, |  | (3) |

where Œ∫¬Ø‚àà(0,+‚àû)\bar{\kappa}\in(0,+\infty) denotes the unit inner sampling cost, and œÑ‚àà[0,+‚àû)\tau\in[0,+\infty) is a dimensionless constant representing the unit outer sampling cost expressed in units of Œ∫¬Ø\bar{\kappa}. In other words the cost of sampling XX is œÑ\tau times the combined cost of sampling UU and evaluating FF.

For simplicity, and without loss of generality, we set Œ∫¬Ø=1\bar{\kappa}=1, which means all computational costs are measured relative to the unit inner sampling cost. In the context of MLMC the literature typically neglects the cost of sampling XX (i.e. assumes œÑ=0\tau=0), however in practice œÑ\tau can be large and we show in this paper that this may affect the optimal balance between inner and outer scenarios in non-asymptotic regimes. As an example in a financial risk management context, each sampling of XX may require a calibration of multiple Risk-Neutral models with respect to the drawn risk factors. When complex models are considered this calibration phase can be time consuming and requires to consider larger œÑ\tau (see [[8](https://arxiv.org/html/2510.18995v1#bib.bib8)]).

In financial risk management, a common risk measure to compute is the Value-At-Risk
(VaR, see, e.g., [[9](https://arxiv.org/html/2510.18995v1#bib.bib9)]) with high confidence level Œ±‚àà(0,1)\alpha\in(0,1) :

|  |  |  |  |
| --- | --- | --- | --- |
|  | qL‚Äã(Œ±)=inf{u‚àà‚Ñù|FL‚Äã(u)‚â•Œ±}.q\_{L}(\alpha)=\inf\left\{u\in\mathbb{R}\;|\;F\_{L}(u)\geq\alpha\right\}. |  | (4) |

As a key example, the 1-year Value-At-Risk with Œ±=99.5%\alpha=99.5\% on future own-fund losses serves as a fundamental measure utilized by the European insurance sector to evaluate the financial situation of companies according to Solvency II regulations (see, e.g., [[10](https://arxiv.org/html/2510.18995v1#bib.bib10)]). A closely related risk measure is the evaluation of FL‚Äã(u)F\_{L}(u) the c.d.f. of the loss LL for extreme threshold u‚àà‚Ñùu\in\mathbb{R} (i.e uu close to extreme quantiles of LL).

These two measures are directly linked: for continuous distributions, the VaR at level Œ±\alpha is the smallest uu such that FL‚Äã(u)=Œ±F\_{L}(u)=\alpha. Thus, evaluating FL‚Äã(u)F\_{L}(u) for a given uu gives the probability of losses not exceeding uu, while VaR inverts this relationship to find the threshold corresponding to a given probability level. The standard approach for practitioners to compute these risk measures is to use a nested Monte Carlo estimator : fix an inner sample size KK, generate an i.i.d. sample of E^K‚Äã(X)\hat{E}\_{K}(X) of size JJ, then compute the relevant probability/order statistic (see, e.g., [[11](https://arxiv.org/html/2510.18995v1#bib.bib11)], [[12](https://arxiv.org/html/2510.18995v1#bib.bib12)]). However, nested simulations is known as sample-inefficient when precise estimations are desired. Gordy-Juneja [[3](https://arxiv.org/html/2510.18995v1#bib.bib3)] demonstrate that, to halve the estimation error, the simulation budget must be increased by a factor eight. By comparison, within a conventional unbiased Monte Carlo situation, standard methods require only a fourfold increase in simulation budget to achieve an equivalent reduction in error.

Therefore, practitioners often resort to alternative techniques that involve statistical learning of the conditional expectation function œà\psi (also known as proxy methods). Common approaches include Least-Squares Monte Carlo methods, which assumes that œà\psi can be well-approximated by a polynomial function (see, e.g., [[13](https://arxiv.org/html/2510.18995v1#bib.bib13)], [[14](https://arxiv.org/html/2510.18995v1#bib.bib14)]), and Replicating Portfolio methods, where œà\psi is approximated by a portfolio of vanilla options (see, e.g., [[15](https://arxiv.org/html/2510.18995v1#bib.bib15)]). Nonetheless, we can identify two main shortcomings with these proxy methodologies. First, practical implementation often requires making arbitrary choices regarding the model specification, which can impede the accurate approximation of œà\psi. Second, these methods may suffer from inefficiency in high-dimensional contexts, making them difficult to scale as the complexity of risk modeling increases. For instance, insurance company can have up to hundreds of risk factors. The scalability of these methods in high dimension is an active research topic where neural networks play an important role (see, e.g., [[16](https://arxiv.org/html/2510.18995v1#bib.bib16)], [[17](https://arxiv.org/html/2510.18995v1#bib.bib17)], [[18](https://arxiv.org/html/2510.18995v1#bib.bib18)] or [[19](https://arxiv.org/html/2510.18995v1#bib.bib19)]). Balancing the allocation of inner and outer scenarios as a function of œÑ\tau is also a pertinent consideration in proxy methods. Alfonsi et al. [[20](https://arxiv.org/html/2510.18995v1#bib.bib20)] demonstrate that, as œÑ\tau gets large, computational efficiency can be achieved by increasing the number of inner samples per outer samples in the construction of the Least-Squares Monte Carlo proxy.

A common fully Monte-Carlo approach to improving the estimation of II in biased Monte Carlo frameworks is to employ Multi-level Monte Carlo (MLMC) methods (see [[21](https://arxiv.org/html/2510.18995v1#bib.bib21)]). In his pioneering work [[22](https://arxiv.org/html/2510.18995v1#bib.bib22)], Giles introduced the MLMC framework to address bias arising from the discretization of stochastic differential equations. Subsequently, numerous authors have extended this methodology (e.g., [[5](https://arxiv.org/html/2510.18995v1#bib.bib5)], [[4](https://arxiv.org/html/2510.18995v1#bib.bib4)], [[23](https://arxiv.org/html/2510.18995v1#bib.bib23)], [[24](https://arxiv.org/html/2510.18995v1#bib.bib24)]), including applications in the context of nested Monte Carlo. In the most favorable scenario (specifically, when the function ff in ([1](https://arxiv.org/html/2510.18995v1#S1.E1 "In 1 Introduction ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")) is sufficiently smooth) the MLMC method achieves sample efficiency comparable to what an unbiased Monte Carlo method would achieve. However when II is the evaluation of a c.d.f.,

|  |  |  |  |
| --- | --- | --- | --- |
|  | I=ùîº‚Äã[ùüôL‚â§u]=FL‚Äã(u)I=\mathbb{E}[\mathbbm{1}\_{L\leq u}]=F\_{L}(u) |  | (5) |

for some fixed u‚àà‚Ñùu\in\mathbb{R}, we need to deal with an indicator function in ([1](https://arxiv.org/html/2510.18995v1#S1.E1 "In 1 Introduction ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")) and its irregularity then hinder the efficiency of MLMC methods.

In this paper, we aim at improving the MLMC method for those cases where ff is an indicator function, such as when estimating ([5](https://arxiv.org/html/2510.18995v1#S1.E5 "In 1 Introduction ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")). In particular we will build upon the Multi-level Richardson Romberg (ML2R) method (see Lemaire-Pag√®s [[5](https://arxiv.org/html/2510.18995v1#bib.bib5)]), that adds weights to the traditional MLMC method in order to improve the efficiency, especially for those cases where ff is irregular. Our contributions are as follows: first, we provide a proof that antithetic sampling of MLMC levels is always efficient in the context of nested Monte Carlo, regardless of the regularity of ff, and quantify the reduction in variance. Then we propose a numerical optimization scheme for MLMC parameters that fully accounts for œÑ\tau. This approach yields significant improvements in non-asymptotic performance while maintaining asymptotic theoretical properties. Computational experiments demonstrate that this numerical optimization is crucial in a setting where ff is an indicator function or when œÑ\tau is large. Finally, we show numerically how the ML2R parametrized with our methodology provide an efficient way to estimate ([5](https://arxiv.org/html/2510.18995v1#S1.E5 "In 1 Introduction ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")) and ([4](https://arxiv.org/html/2510.18995v1#S1.E4 "In 1 Introduction ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")) when high precisions are required.

## 2 Multi-level Monte Carlo and antithetic variance reduction

In this section, we start by introducing MLMC estimators for computing expectations of the form I=ùîº‚Äã[f‚Äã(L)]I=\mathbb{E}[f(L)] where f:‚Ñù‚ü∂‚Ñùf:\mathbb{R}\longrightarrow\mathbb{R} is any function satisfying ùîº‚Äã[|f‚Äã(L)|2]<+‚àû\mathbb{E}[|f(L)|^{2}]<+\infty, presenting both the standard MLMC estimator and the ML2R variant. Then, we show that one can always construct an antithetic sampling of the MLMC levels regardless of the regularity of ff and quantify the reduction in variance.

Finally, to facilitate the introduction of our extensions in the subsequent sections, we recall state of the art theoretical properties of these estimators with particular emphasis on their asymptotic performances.

### 2.1 Introduction to Multi-level Monte Carlo estimators

The standard approach for estimating II in the nested MC framework is the nested MC estimator. This method involves first fixing the number of inner samples KK, then computing an empirical average of YKY\_{K} based on JJ i.i.d samples. As mentioned previously, this estimator is sample-inefficient and incurs prohibitive computational costs when high precision is required. To obtain more efficient estimators, we can employ a MLMC method ; it consist of choosing an increasing sequence of inner sample sizes, (Kr)r‚àà{1,‚Ä¶,R}(K\_{r})\_{r\in\{1,\dots,R\}}, for some number of levels R‚àà‚ÑïR\in\mathbb{N}, rather than using a single inner sample size KK as in the nested MC estimator. Observing that

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Äã[YKR]=ùîº‚Äã[YK1]+‚àër=2Rùîº‚Äã[YKr‚àíYKr‚àí1],\mathbb{E}[Y\_{K\_{R}}]=\mathbb{E}[Y\_{K\_{1}}]+\sum\_{r=2}^{R}\mathbb{E}[Y\_{K\_{r}}-Y\_{K\_{r-1}}]\,, |  |

instead of sampling YKRY\_{K\_{R}}, we can sample YK1Y\_{K\_{1}} and the differences YKr‚àíYKr‚àí1Y\_{K\_{r}}-Y\_{K\_{r-1}} for each level r‚àà{1,‚Ä¶,R}r\in\{1,\dots,R\}. This approach allows to allocate more samples to the lower (computationally inexpensive) levels. Indeed, since the upper-level terms involve differences of potentially correlated random variables, they tend to have lower variance. Lemaire-Pag√®s [[5](https://arxiv.org/html/2510.18995v1#bib.bib5)] (Definition 3.2) introduce a general MLMC estimator that encompasses both the standard MLMC and ML2R approaches. Below, we recall the definition adapted to our setting.

###### Definition 3.

Let Œò\Theta denote the parameter space of the Multi-level Monte Carlo estimator, defined as the set

|  |  |  |
| --- | --- | --- |
|  | Œò={(J,q,K,R)‚àà(0,+‚àû)√ó[0,1]‚Ñï√ó(0,+‚àû)√ó‚Ñï:‚àÄr‚àà{1,‚Ä¶,R}‚Äãqr>0,‚àër=1Rqr=1}\Theta=\left\{(J,q,K,R)\in(0,+\infty)\times[0,1]^{\mathbb{N}}\times(0,+\infty)\times\mathbb{N}:\forall r\in\{1,\dots,R\}\;q\_{r}>0,\sum\_{r=1}^{R}q\_{r}=1\right\} |  |

Here,

1. 1.

   JJ is a parameter that serves as a proxy for the total number of outer samples. For each level rr, we define Jr=‚åàJ‚Äãqr‚åâJ\_{r}=\lceil Jq\_{r}\rceil the number of outer samples allocated to level rr. The actual total number of outer samples is given by ‚àër=1RJr\sum\_{r=1}^{R}J\_{r} which might exceed JJ, but the difference is typically negligible in practice,
2. 2.

   qq is a sequence specifying the allocation of outer samples to each of the RR levels (note that qr=0q\_{r}=0 when r>Rr>R),
3. 3.

   KK is a parameter that serves as a proxy for the number of inner samples used in the first level. The actual number of inner samples used is K1=‚åàK‚åâK\_{1}=\lceil K\rceil, the difference is typically negligible in practice. More generally for each level rr, we define Kr=‚åàK‚åâ‚Äã2r‚àí1K\_{r}=\lceil K\rceil 2^{r-1} the number of inner samples used at level rr.

###### Remark 1.

In the above definition we choose a geometric progression for the sequence (Kr)r‚àà{1,‚Ä¶,R}(K\_{r})\_{r\in\{1,\dots,R\}} with ratio 2. This is a standard choice for MLMC methods within the nested Monte Carlo framework (see, e.g., [[21](https://arxiv.org/html/2510.18995v1#bib.bib21)]). Lemaire-Pag√®s [[5](https://arxiv.org/html/2510.18995v1#bib.bib5)] (Theorem 3.12) show that a ratio of 2 is optimal for the ML2R when ff is an irregular function, otherwise other choices could theoretically be more efficient. For simplicity, we adopt the ratio 2 in all cases throughout this paper.

###### Definition 4.

We consider a family of square-integrable random variable (Œî‚ÄãY2‚ÄãN)N‚àà‚Ñï(\Delta Y\_{2N})\_{N\in\mathbb{N}} such that for all N‚àà‚ÑïN\in\mathbb{N}, ùîº‚Äã[Œî‚ÄãY2‚ÄãN]=ùîº‚Äã[Y2‚ÄãN‚àíYN]\mathbb{E}[\Delta Y\_{2N}]=\mathbb{E}[Y\_{2N}-Y\_{N}]. We also assume that Œî‚ÄãY2‚ÄãN\Delta Y\_{2N} has a sampling cost Œ≥œÑ‚Äã(2‚ÄãN)\gamma\_{\tau}(2N).

Let Œ∏=(J,q,K,R)‚ààŒò\theta=(J,q,K,R)\in\Theta, for each r‚àà{1,‚Ä¶,R}r\in\{1,\dots,R\}, let (Œî‚ÄãYKrj)j‚àà{1,‚Ä¶,Jr}(\Delta Y\_{K\_{r}}^{j})\_{j\in\{1,\dots,J\_{r}\}} be i.i.d copies of Œî‚ÄãYKr\Delta Y\_{K\_{r}}. Letting (ArR)r‚àà{1,‚Ä¶,R}(A\_{r}^{R})\_{r\in\{1,\dots,R\}} be a sequence of real numbers, which may depend on RR, we define the general MLMC estimator of II as

|  |  |  |  |
| --- | --- | --- | --- |
|  | I^Œ∏=1J1‚Äã‚àëj=1J1YK1j+‚àër=2RArRJr‚Äã‚àëj=1JrŒî‚ÄãYKrj.\hat{I}\_{\theta}=\frac{1}{J\_{1}}\sum\_{j=1}^{J\_{1}}Y\_{K\_{1}}^{j}+\sum\_{r=2}^{R}\frac{A^{R}\_{r}}{J\_{r}}\sum\_{j=1}^{J\_{r}}\Delta Y\_{K\_{r}}^{j}\,. |  | (6) |

Different choices for the sequence of weights (ArR)r‚àà{2,‚Ä¶,R}(A^{R}\_{r})\_{r\in\{2,\dots,R\}} in this definition lead to different variants of the MLMC estimator. Namely the standard MLMC as introduced by Giles¬†[[22](https://arxiv.org/html/2510.18995v1#bib.bib22)] is obtained when ArR=1,r=1,‚Ä¶,RA^{R}\_{r}=1,\;r=1,\dots,R. The ML2R as introduced by Lemaire-Pag√®s [[5](https://arxiv.org/html/2510.18995v1#bib.bib5)], is obtained when ArR=WrR,r=1,‚Ä¶,RA^{R}\_{r}=W\_{r}^{R},\;r=1,\dots,R, for some specific sequence of weights (WrR)r‚àà{1,‚Ä¶,R}(W\_{r}^{R})\_{r\in\{1,\dots,R\}} (see Appendix [A.1](https://arxiv.org/html/2510.18995v1#A1.SS1 "A.1 Computation of the weights in the ML2R ‚Ä£ Appendix A Appendix ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") for details about their construction). It is worth noting that when R=1R=1, we recover the standard nested estimator ([2](https://arxiv.org/html/2510.18995v1#S1.E2 "In 1 Introduction ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")). In this sense, MLMC generalizes nested MC. In this paper, if we need to discuss specifically one version of the estimator, the standard MLMC will be denoted by I^MLMC\hat{I}^{\textrm{MLMC}}, while the ML2R variant will be denoted by I^ML2R\hat{I}^{\textrm{ML2R}}. We now introduce the standard choice for the family (Œî‚ÄãY2‚ÄãN)N‚àà‚Ñï(\Delta Y\_{2N})\_{N\in\mathbb{N}}.

###### Definition 5.

For any N‚àà‚ÑïN\in\mathbb{N}, we let Œî‚ÄãY2‚ÄãNS\Delta Y\_{2N}^{S} be the standard construction of an MLMC level with K=2‚ÄãNK=2N inner scenarios. It is defined by Œî‚ÄãY2‚ÄãNS:=Y2‚ÄãNf‚àíY2‚ÄãNc\Delta Y^{S}\_{2N}:=Y^{f}\_{2N}-Y\_{2N}^{c} where

|  |  |  |
| --- | --- | --- |
|  | Y2‚ÄãNf:=f‚Äã(E^2‚ÄãNf‚Äã(X)),Y2‚ÄãNc:=f‚Äã(E^2‚ÄãNc‚Äã(X)),Y^{f}\_{2N}:=f(\hat{E}^{f}\_{2N}(X)),\quad Y\_{2N}^{c}:=f(\hat{E}^{c}\_{2N}(X))\,, |  |

with E^2‚ÄãNf‚Äã(X)\hat{E}^{f}\_{2N}(X) and E^2‚ÄãNc(X))\hat{E}^{c}\_{2N}(X)) defined as

|  |  |  |
| --- | --- | --- |
|  | E^2‚ÄãNf‚Äã(X):=12‚ÄãN‚Äã‚àën=12‚ÄãNF‚Äã(X,Un),E^2‚ÄãNc‚Äã(X):=1N‚Äã‚àën=1NF‚Äã(X,Un),\hat{E}^{f}\_{2N}(X):=\frac{1}{2N}\sum\_{n=1}^{2N}F(X,U\_{n}),\quad\hat{E}^{c}\_{2N}(X):=\frac{1}{N}\sum\_{n=1}^{N}F(X,U\_{n})\,, |  |

where (Un)n‚àà{1,‚Ä¶,N}(U\_{n})\_{n\in\{1,\dots,N\}} is an i.i.d samples generated from UU.

Notice that in the above definition, the first NN samples of UU from the fine level estimator E^2‚ÄãNf‚Äã(X)\hat{E}^{f}\_{2N}(X) are re-used in constructing the coarse level estimator E^2‚ÄãNc‚Äã(X)\hat{E}^{c}\_{2N}(X). This approach, as opposed to using independent samples for each level, provides two key advantages. First, it reduces the variance of Œî‚ÄãY2‚ÄãKS\Delta Y^{S}\_{2K} by increasing the correlation between the term of the difference. Second, it lowers the sampling cost from Œ≥œÑ‚Äã(2‚ÄãN)+Œ≥œÑ‚Äã(N)\gamma\_{\tau}(2N)+\gamma\_{\tau}(N) to Œ≥œÑ‚Äã(2‚ÄãN)\gamma\_{\tau}(2N) as only 2‚ÄãN2N inner samples are required. It is straightforward to check that Œî‚ÄãY2‚ÄãNS\Delta Y^{S}\_{2N} is square integrable and that ùîº‚Äã[Œî‚ÄãY2‚ÄãNS]=ùîº‚Äã[Y2‚ÄãN‚àíYN]\mathbb{E}[\Delta Y^{S}\_{2N}]=\mathbb{E}[Y\_{2N}-Y\_{N}], therefore fulfilling the conditions of Definition [4](https://arxiv.org/html/2510.18995v1#Thmdefinition4 "Definition 4. ‚Ä£ 2.1 Introduction to Multi-level Monte Carlo estimators ‚Ä£ 2 Multi-level Monte Carlo and antithetic variance reduction ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations").

The computational cost of an MLMC estimators with R‚àà‚ÑïR\in\mathbb{N} levels is the sum of the computational cost of each level r‚àà{1,‚Ä¶,R}r\in\{1,\dots,R\}, where we assigned an outer simulation budget of Jr=‚åàJ‚Äãqr‚åâJ\_{r}=\lceil Jq\_{r}\rceil with an inner simulation budget of Kr=‚åàK‚Äã2r‚àí1‚åâK\_{r}=\lceil K2^{r-1}\rceil.

###### Definition 6.

Let Œ∏=(J,q,K,R)‚ààŒò\theta=(J,q,K,R)\in\Theta, we define the computational cost of I^Œ∏\hat{I}\_{\theta} as ùíûœÑ‚Äã(Œ∏):=‚àër=1R‚åàJ‚Äãqr‚åâ‚ÄãŒ≥œÑ‚Äã(Kr)\mathcal{C}\_{\tau}(\theta):=\sum\_{r=1}^{R}\lceil Jq\_{r}\rceil\gamma\_{\tau}(K\_{r}).

The cost function defined above is not particularly tractable. However, in practice, ‚åàJ‚Äãqr‚åâ\lceil Jq\_{r}\rceil is typically very close to J‚ÄãqrJq\_{r}. Therefore, it is reasonable to use the following approximation:

|  |  |  |
| --- | --- | --- |
|  | ùíûœÑ‚Äã(Œ∏)‚âàJ√ó‚àër=1Rqr‚ÄãŒ≥œÑ‚Äã(Kr)\mathcal{C}\_{\tau}(\theta)\approx J\times\sum\_{r=1}^{R}q\_{r}\gamma\_{\tau}(K\_{r}) |  |

In this expression, ‚àër=1Rqr‚ÄãŒ≥œÑ‚Äã(Kr)\sum\_{r=1}^{R}q\_{r}\gamma\_{\tau}(K\_{r}) can be interpreted as the computational cost per outer sample.

###### Definition 7.

Let Œ∏=(J,q,K,R)‚ààŒò\theta=(J,q,K,R)\in\Theta, define the computational cost per outer-samples of I^Œ∏\hat{I}\_{\theta} by

|  |  |  |  |
| --- | --- | --- | --- |
|  | Œ∫œÑ‚Äã(q,K,R):=‚àër=1Rqr‚ÄãŒ≥œÑ‚Äã(Kr).\mathcal{\kappa}\_{\tau}(q,K,R):=\sum\_{r=1}^{R}q\_{r}\gamma\_{\tau}(K\_{r})\,. |  | (7) |

and define the approximate computational cost of I^Œ∏\hat{I}\_{\theta} by

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùíû~œÑ‚Äã(Œ∏):=J‚ÄãŒ∫œÑ‚Äã(q,K,R).\tilde{\mathcal{C}}\_{\tau}(\theta):=J\mathcal{\kappa}\_{\tau}(q,K,R)\,. |  | (8) |

As is standard for MLMC estimators, we will measure its precision with the Root Mean Squared Error (RMSE), i.e the L2L^{2}-norm between the target and the MLMC estimation.

###### Definition 8.

Let Œ∏‚ààŒò\theta\in\Theta, the Mean Squared Error (MSE) of I^Œ∏\hat{I}\_{\theta} defined by ‚Ñ≥‚Äã(Œ∏)=ùîº‚Äã[(I‚àíI^Œ∏)2]:=Œº‚Äã(Œ∏)2+ùí±‚Äã(Œ∏)\mathcal{M}(\theta)=\mathbb{E}[(I-\hat{I}\_{\theta})^{2}]:=\mu(\theta)^{2}+\mathcal{V}(\theta) where Œº‚Äã(Œ∏):=ùîº‚Äã[I‚àíI^Œ∏]\mu(\theta):=\mathbb{E}[I-\hat{I}\_{\theta}] is the bias of the estimator and ùí±‚Äã(Œ∏):=Var‚Äã[I^Œ∏]\mathcal{V}(\theta):=\mathrm{Var}[\hat{I}\_{\theta}] is the variance.

For convenience we introduce the following notations.

###### Definition 9.

Let Œ∏=(J,q,K,R)‚ààŒò\theta=(J,q,K,R)\in\Theta, for each r‚àà{1,‚Ä¶,R}r\in\{1,\dots,R\}, we let œÉ2‚Äã(r,K)\sigma^{2}(r,K) be the structural variance of the level rr defined by

|  |  |  |
| --- | --- | --- |
|  | œÉ2‚Äã(r,K):={Var‚Äã[YK1]r=1(ArR)2‚ÄãVar‚Äã[Œî‚ÄãYKr]r‚àà{2,‚Ä¶,R}\sigma^{2}(r,K):=\begin{cases}\mathrm{Var}[Y\_{K\_{1}}]&r=1\\ (A^{R}\_{r})^{2}\mathrm{Var}[\Delta Y\_{K\_{r}}]&r\in\{2,\dots,R\}\end{cases} |  |

From the independence of the levels in ([6](https://arxiv.org/html/2510.18995v1#S2.E6 "In Definition 4. ‚Ä£ 2.1 Introduction to Multi-level Monte Carlo estimators ‚Ä£ 2 Multi-level Monte Carlo and antithetic variance reduction ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")), the variance of an MLMC estimator is controlled by the sum of the variance of its levels. While the variance of a level r‚àà{1,‚Ä¶,R}r\in\{1,\dots,R\} is controlled by the ratio between œÉ2‚Äã(r,K)\sigma^{2}(r,K) its structural variance and JrJ\_{r} its number of allocated outer samples.

###### Proposition 1.

Let Œ∏=(J,q,K,R)‚ààŒò\theta=(J,q,K,R)\in\Theta, then

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùí±‚Äã(Œ∏)=‚àër=1RœÉ2‚Äã(r,K)Jr\mathcal{V}(\theta)=\sum\_{r=1}^{R}\frac{\sigma^{2}(r,K)}{J\_{r}} |  | (9) |

###### Proof.

It follows immediately from the independence of the levels in the definition of an MLMC estimator.
‚àé

### 2.2 Antithetic variance reduction

A quick inspection of ([9](https://arxiv.org/html/2510.18995v1#S2.E9 "In Proposition 1. ‚Ä£ 2.1 Introduction to Multi-level Monte Carlo estimators ‚Ä£ 2 Multi-level Monte Carlo and antithetic variance reduction ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")) reveals that, for a fixed computational budget, introducing additional upper levels to the traditional nested Monte Carlo estimator leads to an increase in variance. For the inclusion of these levels to be beneficial, this variance inflation must be sufficiently small so that, when combined with the reduction in bias, the overall MSE decreases. Each upper level r‚àà{2,‚Ä¶,R}r\in\{2,\dots,R\} contributes a variance term proportional to Var‚Äã[Œî‚ÄãYKr]\mathrm{Var}[\Delta Y\_{K\_{r}}], therefore, ensuring that Var‚Äã[Œî‚ÄãYKr]\mathrm{Var}[\Delta Y\_{K\_{r}}] remains as small as possible is essential for the effectiveness of MLMC estimators.

###### Remark 2.

In contexts where ff in ([1](https://arxiv.org/html/2510.18995v1#S1.E1 "In 1 Introduction ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")) is smooth, for r‚â•2r\geq 2, Var‚Äã[Œî‚ÄãYKr]\mathrm{Var}[\Delta Y\_{K\_{r}}] is often an order of magnitude smaller than Var‚Äã[YK1]\mathrm{Var}[Y\_{K\_{1}}], so the contribution of the upper-levels to the overall variance is already quite limited. However, when ff is an indicator function, it is common for Var‚Äã[Œî‚ÄãYKr]\mathrm{Var}[\Delta Y\_{K\_{r}}], to be the same order of magnitude as Var‚Äã[YK1]\mathrm{Var}[Y\_{K\_{1}}]. Thus, reducing Var‚Äã[Œî‚ÄãYKr]\mathrm{Var}[\Delta Y\_{K\_{r}}] becomes even more critical in cases where ff is an indicator function.

A common technique found in the MLMC literature for reducing Var‚Äã[Œî‚ÄãYKr]\mathrm{Var}[\Delta Y\_{K\_{r}}] when ff is smooth is to construct Œî‚ÄãYKr\Delta Y\_{K\_{r}} as an antithetic variable of YKr‚àíYKr‚àí1Y\_{K\_{r}}-Y\_{K\_{r-1}} by reusing all the samples of the fine level YKrY\_{K\_{r}} in two conditionally independent copies of the coarse level YKr‚àí1Y\_{K\_{r-1}} ; this is possible since Kr=2‚ÄãKr‚àí1K\_{r}=2K\_{r-1} (see for example [[25](https://arxiv.org/html/2510.18995v1#bib.bib25)] or [[26](https://arxiv.org/html/2510.18995v1#bib.bib26)]).

###### Definition 10.

For any N‚àà‚ÑïN\in\mathbb{N}, let

|  |  |  |
| --- | --- | --- |
|  | E^2‚ÄãNf‚Äã(X):=12‚ÄãN‚Äã‚àën=12‚ÄãNF‚Äã(X,Un),E^2‚ÄãNc‚Äã(X):=1N‚Äã‚àën=1NF‚Äã(X,Un),E^2‚ÄãNc‚Ä≤‚Äã(X):=1N‚Äã‚àën=N+12‚ÄãNF‚Äã(X,Un),\hat{E}^{f}\_{2N}(X):=\frac{1}{2N}\sum\_{n=1}^{2N}F(X,U\_{n}),\quad\hat{E}^{c}\_{2N}(X):=\frac{1}{N}\sum\_{n=1}^{N}F(X,U\_{n}),\quad\hat{E}^{c^{\prime}}\_{2N}(X):=\frac{1}{N}\sum\_{n=N+1}^{2N}F(X,U\_{n})\,, |  |

with (Un)n‚àà{1,‚Ä¶,N}(U\_{n})\_{n\in\{1,\dots,N\}} an i.i.d samples generated from UU. We then define the fine level Y2‚ÄãNf:=f‚Äã(E^2‚ÄãNf‚Äã(X))Y^{f}\_{2N}:=f(\hat{E}^{f}\_{2N}(X)), and the two conditionally coarse levels Y2‚ÄãNc:=f‚Äã(E^2‚ÄãNc‚Äã(X))Y^{c}\_{2N}:=f(\hat{E}^{c}\_{2N}(X)) and Y2‚ÄãNc‚Ä≤:=f‚Äã(E^2‚ÄãNc‚Ä≤‚Äã(X))Y^{c^{\prime}}\_{2N}:=f(\hat{E}^{c^{\prime}}\_{2N}(X)).

It is well known (see, e.g., Pag√®s [[27](https://arxiv.org/html/2510.18995v1#bib.bib27)], Proposition 9.3) that if ff is smooth enough and if the variance of Y2‚ÄãK‚àíYKY\_{2K}-Y\_{K} converges to 0 when K‚Üí+‚àûK\rightarrow+\infty, then the antithetic construction improves the rate of variance convergence. More precisely, briefly anticipating the assumption (VarŒ≤)(\mathrm{Var}\_{\beta}) (formally introduced in Section [2.3](https://arxiv.org/html/2510.18995v1#S2.SS3 "2.3 Theoretical guarantees on asymptotic complexity of MLMC estimators ‚Ä£ 2 Multi-level Monte Carlo and antithetic variance reduction ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")) : in the smooth case, this assumption typically holds with a rate of variance decay Œ≤=1\beta=1, while with antithetic sampling it is satisfied for Œ≤>1\beta>1, ensuring a faster convergence of the variance to 0 when KK grows. However when ff is not smooth (such as in the case of an indicator function), this theoretical guarantee generally no longer apply. Nevertheless, some authors still apply antithetic sampling in non-smooth cases (see, [[4](https://arxiv.org/html/2510.18995v1#bib.bib4)], [[28](https://arxiv.org/html/2510.18995v1#bib.bib28)]).

###### Definition 11.

For any N‚àà‚ÑïN\in\mathbb{N}, we let Œî‚ÄãY2‚ÄãNA\Delta Y^{A}\_{2N} be the antithetic construction of an MLMC level with 2‚ÄãN2N inner samples defined by,

|  |  |  |  |
| --- | --- | --- | --- |
|  | Œî‚ÄãY2‚ÄãNA:=Y2‚ÄãNf‚àí12‚Äã(Y2‚ÄãNc+Y2‚ÄãNc‚Ä≤)\Delta Y^{A}\_{2N}:=Y^{f}\_{2N}-\frac{1}{2}(Y^{c}\_{2N}+Y^{c^{\prime}}\_{2N}) |  | (10) |

Clearly, as required by Definition [4](https://arxiv.org/html/2510.18995v1#Thmdefinition4 "Definition 4. ‚Ä£ 2.1 Introduction to Multi-level Monte Carlo estimators ‚Ä£ 2 Multi-level Monte Carlo and antithetic variance reduction ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations"), the sampling cost for Œî‚ÄãY2‚ÄãNA\Delta Y^{A}\_{2N} is Œ≥œÑ‚Äã(2‚ÄãN)\gamma\_{\tau}(2N) since the total number of samples used is still 2‚ÄãN2N. The other requirements, namely the square-integrability and expectation property are straightforward to check.

In Theorem [1](https://arxiv.org/html/2510.18995v1#Thmtheorem1 "Theorem 1. ‚Ä£ 2.2 Antithetic variance reduction ‚Ä£ 2 Multi-level Monte Carlo and antithetic variance reduction ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations"), we provide a general theoretical justification showing that antithetic sampling yields variance reduction regardless of ff and its smoothness, and quantify this reduction. As far as we are aware, this result is new.

###### Theorem 1.

For any N‚àà‚ÑïN\in\mathbb{N},

|  |  |  |
| --- | --- | --- |
|  | Var‚Äã[Œî‚ÄãY2‚ÄãNA]=Var‚Äã[Œî‚ÄãY2‚ÄãNS]‚àí12‚Äãùîº‚Äã[Var‚Äã[YN|X]]\mathrm{Var}[\Delta Y\_{2N}^{A}]=\mathrm{Var}[\Delta Y\_{2N}^{S}]-\frac{1}{2}\mathbb{E}[\mathrm{Var}[Y\_{N}|X]] |  |

or equivalently,

|  |  |  |
| --- | --- | --- |
|  | Var[ŒîY2‚ÄãNS]‚àíVar[ŒîY2‚ÄãNA]=12||YN‚àíùîº[YN|X]||L22=12ùîº[Var[YN|X]]\mathrm{Var}[\Delta Y\_{2N}^{S}]-\mathrm{Var}[\Delta Y\_{2N}^{A}]=\frac{1}{2}||Y\_{N}-\mathbb{E}[Y\_{N}|X]||^{2}\_{L\_{2}}=\frac{1}{2}\mathbb{E}[\mathrm{Var}[Y\_{N}|X]] |  |

This difference is always non-negative and it is positive whenever F‚Äã(X,U)F(X,U) is not independent from XX.

###### Proof.

Let N‚àà‚ÑïN\in\mathbb{N}, then

|  |  |  |
| --- | --- | --- |
|  | Var‚Äã[Y2‚ÄãNS]=Var‚Äã[Y2‚ÄãN]+Var‚Äã[YN]‚àí2‚ÄãCov‚Äã(Y2‚ÄãNf,YNc)\mathrm{Var}[Y\_{2N}^{S}]=\mathrm{Var}[Y\_{2N}]+\mathrm{Var}[Y\_{N}]-2\text{Cov}(Y^{f}\_{2N},Y^{c}\_{N}) |  |

and Var‚Äã[Œî‚ÄãY2‚ÄãNA]=Var‚Äã[Y2‚ÄãN]+14‚ÄãVar‚Äã[YNc+YNc‚Ä≤]‚àíCov‚Äã(Y2‚ÄãN,YNc)‚àíCov‚Äã(Y2‚ÄãN,YNc‚Ä≤)\mathrm{Var}[\Delta Y\_{2N}^{A}]=\mathrm{Var}[Y\_{2N}]+\frac{1}{4}\mathrm{Var}[Y^{c}\_{N}+Y^{c^{\prime}}\_{N}]-\text{Cov}(Y\_{2N},Y\_{N}^{c})-\text{Cov}(Y\_{2N},Y^{c^{\prime}}\_{N}).
Since E^Nc\hat{E}\_{N}^{c} and E^Nc‚Ä≤\hat{E}\_{N}^{c^{\prime}} have the same law, are independent conditionally to XX and play a symmetric role in Cov‚Äã(Y2‚ÄãN,YNc)\text{Cov}(Y\_{2N},Y^{c}\_{N}) we have the identity Cov‚Äã(Y2‚ÄãN,YNc)=Cov‚Äã(Y2‚ÄãN,YNc‚Ä≤)\text{Cov}(Y\_{2N},Y^{c}\_{N})=\text{Cov}(Y\_{2N},Y^{c^{\prime}}\_{N}), which gives Var‚Äã[Œî‚ÄãY2‚ÄãNA]=Var‚Äã[Y2‚ÄãN]+14‚ÄãVar‚Äã[YNc+YNc‚Ä≤]‚àí2‚ÄãCov‚Äã(Y2‚ÄãN,YNc)\mathrm{Var}[\Delta Y\_{2N}^{A}]=\mathrm{Var}[Y\_{2N}]+\frac{1}{4}\mathrm{Var}[Y^{c}\_{N}+Y^{c^{\prime}}\_{N}]-2\text{Cov}(Y\_{2N},Y\_{N}^{c})
and therefore Var‚Äã[Œî‚ÄãY2‚ÄãNS]‚àíVar‚Äã[Œî‚ÄãY2‚ÄãNA]=Var‚Äã[YN]‚àí14‚ÄãVar‚Äã[YNc+YNc‚Ä≤]\mathrm{Var}[\Delta Y\_{2N}^{S}]-\mathrm{Var}[\Delta Y\_{2N}^{A}]=\mathrm{Var}[Y\_{N}]-\frac{1}{4}\mathrm{Var}[Y\_{N}^{c}+Y^{c^{\prime}}\_{N}]. Now expanding Var‚Äã[YNc+YNc‚Ä≤]\mathrm{Var}[Y\_{N}^{c}+Y^{c^{\prime}}\_{N}] in the above equation gives,

|  |  |  |
| --- | --- | --- |
|  | Var‚Äã[Œî‚ÄãY2‚ÄãNS]‚àíVar‚Äã[Œî‚ÄãY2‚ÄãNA]=12‚Äã(Var‚Äã[YN]‚àíCov‚Äã(YNc,YNc‚Ä≤))=12‚Äã(ùîº‚Äã[YN2]‚àíùîº‚Äã[YNc‚ÄãYNc‚Ä≤]).\mathrm{Var}[\Delta Y\_{2N}^{S}]-\mathrm{Var}[\Delta Y\_{2N}^{A}]=\frac{1}{2}\left(\mathrm{Var}[Y\_{N}]-\text{Cov}(Y^{c}\_{N},Y^{c^{\prime}}\_{N})\right)=\frac{1}{2}\left(\mathbb{E}[Y\_{N}^{2}]-\mathbb{E}[Y^{c}\_{N}Y^{c^{\prime}}\_{N}]\right)\,. |  |

From tower property of conditional expectations together with the conditional independence of YNcY^{c}\_{N} and YNc‚Ä≤Y^{c^{\prime}}\_{N} we get ùîº‚Äã[YNc‚ÄãYNc‚Ä≤]=ùîº‚Äã[(ùîº‚Äã[YN|X])2]\mathbb{E}[Y^{c}\_{N}Y^{c^{\prime}}\_{N}]=\mathbb{E}[(\mathbb{E}[Y\_{N}|X])^{2}] which finally gives,

|  |  |  |
| --- | --- | --- |
|  | Var[ŒîY2‚ÄãNS]‚àíVar[ŒîY2‚ÄãNA]=12(ùîº[YN2]‚àíùîº[(ùîº[YN|X])2])=12||YN‚àíùîº[YN|X]||L22.\mathrm{Var}[\Delta Y\_{2N}^{S}]-\mathrm{Var}[\Delta Y\_{2N}^{A}]=\frac{1}{2}\left(\mathbb{E}[Y\_{N}^{2}]-\mathbb{E}[(\mathbb{E}[Y\_{N}|X])^{2}]\right)=\frac{1}{2}||Y\_{N}-\mathbb{E}[Y\_{N}|X]||^{2}\_{L\_{2}}\,. |  |

This completes the proof.
‚àé

Since the case where ff is an indicator function will be of special interest later in this paper, it is worth applying the above proposition to this special case :

###### Corollary 1.

Let u‚àà‚Ñùu\in\mathbb{R}, if for all x‚àà‚Ñùx\in\mathbb{R}, f‚Äã(x)=ùüôx‚â§uf(x)=\mathbbm{1}\_{x\leq u} then,

|  |  |  |
| --- | --- | --- |
|  | Var‚Äã[Œî‚ÄãY2‚ÄãNS]‚àíVar‚Äã[Œî‚ÄãY2‚ÄãNA]=12‚Äãùîº‚Äã[‚Ñô‚Äã(E^N‚â§u|X)‚Äã(1‚àí‚Ñô‚Äã(E^N‚â§u|X))]\mathrm{Var}[\Delta Y\_{2N}^{S}]-\mathrm{Var}[\Delta Y\_{2N}^{A}]=\frac{1}{2}\mathbb{E}[\mathbb{P}(\hat{E}\_{N}\leq u|X)(1-\mathbb{P}(\hat{E}\_{N}\leq u|X))] |  |

###### Proof.

It suffices to remark that when ff is an indicator function Var‚Äã[YN|X]=‚Ñô‚Äã(E^N‚â§Œ∑|X)‚Äã(1‚àí‚Ñô‚Äã(E^N‚â§Œ∑|X))\mathrm{Var}[Y\_{N}|X]=\mathbb{P}(\hat{E}\_{N}\leq\eta|X)(1-\mathbb{P}(\hat{E}\_{N}\leq\eta|X))
‚àé

Note that since Œî‚ÄãYNA\Delta Y^{A}\_{N} and Œî‚ÄãYNS\Delta Y^{S}\_{N} have equal sampling costs, antithetic sampling is always beneficial in a nested MC setting, regardless of the regularity of ff. We will therefore use the antithetic version for upper levels and omit the superscript AA in the notation.

### 2.3 Theoretical guarantees on asymptotic complexity of MLMC estimators

In this section, to establish the context and notation for our subsequent theoretical developments and improvements, we review standard results from Lemaire-Pag√®s [[5](https://arxiv.org/html/2510.18995v1#bib.bib5)] on the asymptotic complexity of both traditional and weighted MLMC estimators. Specifically, under assumptions controlling the bias and variance of the estimators, they quantify the asymptotic computational cost required to obtain an estimate of II with an RMSE Œµ\varepsilon as Œµ‚Üí0\varepsilon\to 0. These results demonstrate how MLMC methods can achieve significant computational savings compared to the traditional nested simulation approach, especially as higher accuracy is desired. They also highlight how the weighted MLMC estimator outperform the standard MLMC estimator in scenarios where ff is an irregular function.

Below, we recall key assumptions required for the complexity theorems.

###### Assumption 1 (Weak Error Expansion).

We say that the weak error expansion (WEŒ±,R)(\mathrm{WE}\_{\alpha,R}) holds for some integer R‚â•1R\geq 1 and coefficient Œ±>0\alpha>0 if there exist real constants c1,c2,‚Ä¶,cRc\_{1},c\_{2},\dots,c\_{R} and a function Œ∑R:‚Ñï‚Üí‚Ñù\eta\_{R}:\mathbb{N}\to\mathbb{R} such that Œ∑R‚Äã(N)‚Üí0\eta\_{R}(N)\to 0 as N‚Üí‚àûN\to\infty, and

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Äã[YN]=ùîº‚Äã[Y]+‚àër=1RcrNŒ±‚Äãr+1NŒ±‚ÄãR‚ãÖŒ∑R‚Äã(N).\mathbb{E}[Y\_{N}]=\mathbb{E}[Y]+\sum\_{r=1}^{R}\frac{c\_{r}}{N^{\alpha r}}+\frac{1}{N^{\alpha R}}\cdot\eta\_{R}(N)\,. |  |

We write (WEŒ±)(\mathrm{WE}\_{\alpha}) when the above expansion holds for all R‚â•1R\geq 1 and a common sequence of constants (cr)r‚àà‚Ñï(c\_{r})\_{r\in\mathbb{N}}. In that case, we define c~‚àû:=limR‚Üí‚àûcR1/R\tilde{c}\_{\infty}:=\lim\_{R\to\infty}c\_{R}^{1/R} assuming that the limit exists.

###### Assumption 2.

We say that the variance decay assumption (VarŒ≤)(\mathrm{Var}\_{\beta}) holds with coefficient Œ≤>0\beta>0 if there exist a constant V1>0V\_{1}>0 such that

|  |  |  |
| --- | --- | --- |
|  | ‚àÄN‚àà‚Ñï,Var‚Äã[Œî‚ÄãY2‚ÄãN]‚â§V1(2‚ÄãN)Œ≤.\forall N\in\mathbb{N},\quad\mathrm{Var}[\Delta Y\_{2N}]\leq\frac{V\_{1}}{(2N)^{\beta}}\,. |  |

and a constant œÉ¬Ø>0\bar{\sigma}>0 such that

|  |  |  |
| --- | --- | --- |
|  | ‚àÄN‚àà‚Ñï,Var‚Äã[YN]‚â§œÉ¬Ø2\forall N\in\mathbb{N},\quad\mathrm{Var}[Y\_{N}]\leq\bar{\sigma}^{2} |  |

The two theorems below are adapted from Lemaire-Pag√®s [[5](https://arxiv.org/html/2510.18995v1#bib.bib5)] (Theorem 3.12), with changes in notation for consistency with our framework.

###### Theorem 2 ([[5](https://arxiv.org/html/2510.18995v1#bib.bib5)], Theorem 3.12, a)).

Assume œÑ=0\tau=0. Consider the case of of the weighted Multi-level estimator I^ML2R\hat{I}^{\mathrm{ML2R}}. Assume that (WEŒ±)(\mathrm{WE}\_{\alpha}) holds for some Œ±>0\alpha>0 and (VarŒ≤)(\mathrm{Var}\_{\beta}) for some Œ≤>0\beta>0. Assume furthermore that

|  |  |  |
| --- | --- | --- |
|  | supR‚àà‚Ñï‚Äãsupk‚àà‚Ñï‚Äã|Œ∑R‚Äã(k)|<+‚àû\underset{R\in\mathbb{N}}{\sup}\;\underset{k\in\mathbb{N}}{\sup}\;|\eta\_{R}(k)|<+\infty |  |

and c~‚àû>0\tilde{c}\_{\infty}>0. Then, there exists a Œµ0>0\varepsilon\_{0}>0, a collection {Œ∏Œµ‚ààŒò:Œµ‚àà(0,Œµ0)}\{\theta\_{\varepsilon}\in\Theta:\varepsilon\in(0,\varepsilon\_{0})\} and a CML2R‚â•0C\_{\mathrm{ML2R}}\geq 0 verifying

|  |  |  |
| --- | --- | --- |
|  | limsupŒµ‚Üí0‚Äãv‚Äã(Œµ,Œ≤)‚ãÖùíû~0‚Äã(Œ∏Œµ)‚â§CML2R\underset{\varepsilon\to 0}{\lim\sup}\;v(\varepsilon,\beta)\cdot\tilde{\mathcal{C}}\_{0}(\theta\_{\varepsilon})\leq C\_{\mathrm{ML2R}} |  |

where,

|  |  |  |  |
| --- | --- | --- | --- |
|  | v‚Äã(Œµ,Œ≤)={Œµ2,ifŒ≤>1,Œµ2‚Äã(log‚Å°(1/Œµ))‚àí1,ifŒ≤=1,Œµ2‚Äãe‚àí1‚àíŒ≤Œ±‚Äã2‚Äãln‚Å°(1/Œµ)‚Äãln‚Å°(2),ifŒ≤<1.v(\varepsilon,\beta)=\begin{cases}\varepsilon^{2},&\text{if}\quad\beta>1,\\ \varepsilon^{2}(\log(1/\varepsilon))^{-1},&\text{if}\quad\beta=1,\\ \varepsilon^{2}e^{-\frac{1-\beta}{\sqrt{\alpha}}\sqrt{2\ln(1/\varepsilon)\ln(2)}},&\text{if}\quad\beta<1.\end{cases} |  | (11) |

and

|  |  |  |
| --- | --- | --- |
|  | limsupŒµ‚Üí0‚ÄãŒµ‚àí2‚Äã‚Ñ≥‚Äã(Œ∏Œµ)‚â§1\underset{\varepsilon\to 0}{\lim\sup}\;\varepsilon^{-2}\mathcal{M}(\theta\_{\varepsilon})\leq 1 |  |

###### Theorem 3 ([[5](https://arxiv.org/html/2510.18995v1#bib.bib5)], Theorem 3.12 b).

Assume œÑ=0\tau=0. Consider the case of of the standard Multi-level estimator I^MLMC\hat{I}^{\mathrm{MLMC}}. Assume that (WEŒ±,1)(\mathrm{WE}\_{\alpha,1}) hold for some Œ±>0\alpha>0 and (VarŒ≤)(\mathrm{Var}\_{\beta}) for some Œ≤>0\beta>0. Assume furthermore that c1‚â†0c\_{1}\neq 0. Then, there exists a Œµ0>0\varepsilon\_{0}>0, a collection {Œ∏Œµ‚ààŒò:Œµ‚àà(0,Œµ0)}\{\theta\_{\varepsilon}\in\Theta:\varepsilon\in(0,\varepsilon\_{0})\} and a CMLMC‚â•0C\_{\mathrm{MLMC}}\geq 0 verifying

|  |  |  |
| --- | --- | --- |
|  | limsupŒµ‚Üí0‚Äãv‚Äã(Œµ,Œ≤)‚ãÖùíû~0‚Äã(Œ∏Œµ)‚â§CMLMC\underset{\varepsilon\to 0}{\lim\sup}\;v(\varepsilon,\beta)\cdot\tilde{\mathcal{C}}\_{0}(\theta\_{\varepsilon})\leq C\_{\mathrm{MLMC}} |  |

where,

|  |  |  |  |
| --- | --- | --- | --- |
|  | v‚Äã(Œµ,Œ≤)={Œµ2,ifŒ≤>1,Œµ2‚Äã(log‚Å°(1/Œµ))‚àí2,ifŒ≤=1,Œµ2+1‚àíŒ≤Œ±,ifŒ≤<1.v(\varepsilon,\beta)=\begin{cases}\varepsilon^{2},&\text{if}\quad\beta>1,\\ \varepsilon^{2}(\log(1/\varepsilon))^{-2},&\text{if}\quad\beta=1,\\ \varepsilon^{2+\frac{1-\beta}{\alpha}},&\text{if}\quad\beta<1.\end{cases} |  | (12) |

and

|  |  |  |
| --- | --- | --- |
|  | limsupŒµ‚Üí0‚ÄãŒµ‚àí2‚Äã‚Ñ≥‚Äã(Œ∏Œµ)‚â§1\underset{\varepsilon\to 0}{\lim\sup}\;\varepsilon^{-2}\mathcal{M}(\theta\_{\varepsilon})\leq 1 |  |

These theorems reveal three regimes of asymptotic complexity, determined by the variance decay rate¬†Œ≤\beta. When Œ≤>1\beta>1, both estimators achieve the target precision¬†Œµ\varepsilon with a computational complexity of O‚Äã(Œµ‚àí2)O(\varepsilon^{-2}), which matches the optimal complexity attainable if unbiased samples of LL were available. In the intermediate regime, where Œ≤=1\beta=1, both estimators require a computational cost of order O(Œµ‚àí2log(Œµ)c)O(\varepsilon^{-2}\log(\varepsilon)^{c}) for some constant c>0c>0; that is, the complexity is essentially O‚Äã(Œµ‚àí2)O(\varepsilon^{-2}) up to a logarithmic factor. In this case, the ML2R estimator already outperforms the standard version. When Œ≤<1\beta<1, the complexity departs further from O‚Äã(Œµ‚àí2)O(\varepsilon^{-2}). Specifically, the ML2R estimator achieves a complexity of O‚Äã(Œµ‚àí2‚àíŒΩ)O(\varepsilon^{-2-\nu}) for any ŒΩ>0\nu>0, while the standard MLMC estimator has complexity O‚Äã(Œµ‚àí2‚àí1‚àíŒ≤Œ±)O\left(\varepsilon^{-2-\frac{1-\beta}{\alpha}}\right). In all cases, MLMC methods provide a significant asymptotic improvement over the traditional nested simulation approach.

###### Remark 3.

The parameter Œ≤\beta is strongly related to the smoothness of the underlying function ff where more smoothness yields higher Œ≤\beta. Typically, when ff is an indicator function Œ≤=12\beta=\frac{1}{2} (see [[4](https://arxiv.org/html/2510.18995v1#bib.bib4)] or [[6](https://arxiv.org/html/2510.18995v1#bib.bib6)])). The regime where Œ≤<1\beta<1 is precisely the case in which the ML2R estimator is significantly more efficient than the standard MLMC, making it the preferred estimator for estimating ([5](https://arxiv.org/html/2510.18995v1#S1.E5 "In 1 Introduction ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")). Numerical evidence in Section [6](https://arxiv.org/html/2510.18995v1#S6 "6 Numerical experiments ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") will back-up this claim.

In Theorem [4](https://arxiv.org/html/2510.18995v1#Thmtheorem4 "Theorem 4. ‚Ä£ 3.1.2 Extension of complexity theorems for arbitrary ùúè ‚Ä£ 3.1 State-of-the-art MLMC parameterization ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") we will extend these results to arbitrary œÑ‚â•0\tau\geq 0, replacing ùíû~0\tilde{\mathcal{C}}\_{0} by ùíû~œÑ\tilde{\mathcal{C}}\_{\tau}. The asymptotic rates are still achieve by the standard parametrization (Œ∏Œµ)Œµ‚àà(0,Œµ0)(\theta\_{\varepsilon})\_{\varepsilon\in(0,\varepsilon\_{0})}. However, for non-asymptotic regimes, the parametrization is sub-optimal. To address this, Section [3](https://arxiv.org/html/2510.18995v1#S3 "3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") introduces an improved parametrization that enhances performance in non-asymptotic settings.

## 3 Optimizing parameters selection for MLMC in non-asymptotic regimes

The objective of this section is to present a new parametrization of MLMC estimators that explicitly incorporates the parameter œÑ\tau and improve complexity in non-asymptotic regimes. As demonstrated by our numerical experiments in Section [6](https://arxiv.org/html/2510.18995v1#S6 "6 Numerical experiments ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations"), this novel parametrization improves performance of MLMC when applied to a nested Monte Carlo framework with indicator function payoffs.

We first review the standard parametrization achieving the computational complexities of Theorem [2](https://arxiv.org/html/2510.18995v1#Thmtheorem2 "Theorem 2 ([5], Theorem 3.12, a)). ‚Ä£ 2.3 Theoretical guarantees on asymptotic complexity of MLMC estimators ‚Ä£ 2 Multi-level Monte Carlo and antithetic variance reduction ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") and [3](https://arxiv.org/html/2510.18995v1#Thmtheorem3 "Theorem 3 ([5], Theorem 3.12 b). ‚Ä£ 2.3 Theoretical guarantees on asymptotic complexity of MLMC estimators ‚Ä£ 2 Multi-level Monte Carlo and antithetic variance reduction ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") ; then prove it generalizes to context with generic œÑ‚â•0\tau\geq 0. Building on this foundation, we introduce a novel parametrization involving a numerical optimization for parameters KK and RR fully accounting for generic œÑ\tau and improving the computational cost for non-asymptotic regimes while preserving theoretical guarantees of Theorem [2](https://arxiv.org/html/2510.18995v1#Thmtheorem2 "Theorem 2 ([5], Theorem 3.12, a)). ‚Ä£ 2.3 Theoretical guarantees on asymptotic complexity of MLMC estimators ‚Ä£ 2 Multi-level Monte Carlo and antithetic variance reduction ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") and [3](https://arxiv.org/html/2510.18995v1#Thmtheorem3 "Theorem 3 ([5], Theorem 3.12 b). ‚Ä£ 2.3 Theoretical guarantees on asymptotic complexity of MLMC estimators ‚Ä£ 2 Multi-level Monte Carlo and antithetic variance reduction ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations"). Finally, we discuss asymptotics of the optimized parameters when œÑ‚Üí+‚àû\tau\to+\infty.

### 3.1 State-of-the-art MLMC parameterization

#### 3.1.1 Plug-and-play parameters

Traditionally, as developed by Giles (see [[21](https://arxiv.org/html/2510.18995v1#bib.bib21)], Algorithm 1), the determination of the optimized parameters Œ∏Œµ\theta\_{\varepsilon} for a prescribed precision Œµ>0\varepsilon>0 in the standard MLMC setting is based on an adaptive algorithm. This algorithm begins with an initial choice for the number of levels and the number of outer samples per level, then iteratively refine these parameters based on ongoing estimates of the bias and variance. In contrast, the method proposed by Lemaire-Pag√®s is more "plug-and-play" (see [[5](https://arxiv.org/html/2510.18995v1#bib.bib5)] Practitioner‚Äôs corner 5.1): after a preliminary phase in which certain structural constants of the problem are estimated, optimal parameters can be computed in closed form. Here we will discuss exclusively the approach of Lemaire-Pag√®s.

The objective is to solve, at least approximately, the following minimization problem

|  |  |  |  |
| --- | --- | --- | --- |
|  | minŒ∏‚ààŒò‚Ñ≥‚Äã(Œ∏)‚â§Œµ2‚Äãùíû~œÑ‚Äã(Œ∏).\underset{\begin{subarray}{c}\theta\in\Theta\\ \mathcal{M}(\theta)\leq\varepsilon^{2}\end{subarray}}{\min}\tilde{\mathcal{C}}\_{\tau}(\theta)\,. |  | (13) |

Lemaire-Pag√®s [[5](https://arxiv.org/html/2510.18995v1#bib.bib5)] deal with the asymptotic regime as Œµ‚Üí0\varepsilon\to 0, with œÑ=0\tau=0. They construct a collection of closed-form optimized parameters {Œ∏Œµ:Œµ‚àà(0,Œµ0]}\{\theta\_{\varepsilon}:\varepsilon\in(0,\varepsilon\_{0}]\} for some Œµ0>0\varepsilon\_{0}>0 such that, under the hypothesis of Theorem [2](https://arxiv.org/html/2510.18995v1#Thmtheorem2 "Theorem 2 ([5], Theorem 3.12, a)). ‚Ä£ 2.3 Theoretical guarantees on asymptotic complexity of MLMC estimators ‚Ä£ 2 Multi-level Monte Carlo and antithetic variance reduction ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") (resp. Theorem [3](https://arxiv.org/html/2510.18995v1#Thmtheorem3 "Theorem 3 ([5], Theorem 3.12 b). ‚Ä£ 2.3 Theoretical guarantees on asymptotic complexity of MLMC estimators ‚Ä£ 2 Multi-level Monte Carlo and antithetic variance reduction ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")) for the ML2R case (resp. standard MLMC case),

|  |  |  |
| --- | --- | --- |
|  | limsupŒµ‚Üí0‚ÄãŒµ‚àí2‚Äã‚Ñ≥‚Äã(Œ∏Œµ)‚â§1,\underset{\varepsilon\to 0}{\lim\sup}\;\varepsilon^{-2}\mathcal{M}(\theta\_{\varepsilon})\leq 1\,, |  |

and

|  |  |  |
| --- | --- | --- |
|  | limsupv‚Äã(Œµ,Œ≤)‚ãÖùíû~0‚Äã(Œ∏Œµ)‚â§CŒ±,Œ≤,\lim\sup v(\varepsilon,\beta)\cdot\tilde{\mathcal{C}}\_{0}(\theta\_{\varepsilon})\leq C\_{\alpha,\beta}\,, |  |

where CŒ±,Œ≤C\_{\alpha,\beta} is a finite constant depending on Œ±,Œ≤\alpha,\beta and the type of estimator (standard MLMC or ML2R) and v‚Äã(Œµ,Œ≤)v(\varepsilon,\beta) is defined in ([11](https://arxiv.org/html/2510.18995v1#S2.E11 "In Theorem 2 ([5], Theorem 3.12, a)). ‚Ä£ 2.3 Theoretical guarantees on asymptotic complexity of MLMC estimators ‚Ä£ 2 Multi-level Monte Carlo and antithetic variance reduction ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")) and ([12](https://arxiv.org/html/2510.18995v1#S2.E12 "In Theorem 3 ([5], Theorem 3.12 b). ‚Ä£ 2.3 Theoretical guarantees on asymptotic complexity of MLMC estimators ‚Ä£ 2 Multi-level Monte Carlo and antithetic variance reduction ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")). We report in Table [1](https://arxiv.org/html/2510.18995v1#S3.T1 "Table 1 ‚Ä£ 3.1.1 Plug-and-play parameters ‚Ä£ 3.1 State-of-the-art MLMC parameterization ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") the explicit values of these parameters. We refer to [[5](https://arxiv.org/html/2510.18995v1#bib.bib5)] (see in particular 5.1 Practitionner‚Äôs corner) for their construction in our nested MC context with a strong-error assumption of type (VarŒ≤)(\mathrm{Var}\_{\beta}).

|  |  |  |
| --- | --- | --- |
| Parameter | Weighted (ML2R) | Classical (MLMC) |
| R‚Äã(Œµ)R(\varepsilon) | ‚åà12+ln2‚Å°(c~1Œ±K¬Ø)+(12+ln2‚Å°(c~1Œ±K¬Ø))2+2‚Äãln2‚Å°(Œµ‚àí1‚Äã1+4‚ÄãŒ±)Œ±‚åâ\left\lceil\frac{1}{2}+\ln\_{2}(\frac{\tilde{c}^{\frac{1}{\alpha}}}{\underline{K}})+\sqrt{(\frac{1}{2}+\ln\_{2}(\frac{\tilde{c}^{\frac{1}{\alpha}}}{\underline{K}}))^{2}+\frac{2\ln\_{2}(\varepsilon^{-1}\sqrt{1+4\alpha})}{\alpha}}\right\rceil | ‚åà1+ln2‚Å°(|c1|1Œ±K¬Ø)+ln2‚Å°(1+2‚ÄãŒ±‚ÄãŒµ‚àí1)Œ±‚åâ\left\lceil 1+\ln\_{2}(\frac{|c\_{1}|^{\frac{1}{\alpha}}}{\underline{K}})+\frac{\ln\_{2}(\sqrt{1+2\alpha}\varepsilon^{-1})}{\alpha}\right\rceil |
| K+‚Äã(Œµ)K^{+}(\varepsilon) | (1+2‚ÄãŒ±‚ÄãR)12‚ÄãŒ±‚ÄãR‚ÄãŒµ‚àí1Œ±‚ÄãR‚Äãc~1Œ±‚Äã2‚àíR‚àí12(1+2\alpha R)^{\frac{1}{2\alpha R}}\varepsilon^{-\frac{1}{\alpha R}}\tilde{c}^{\frac{1}{\alpha}}2^{-\frac{R-1}{2}} | (1+2‚ÄãŒ±)12‚ÄãŒ±‚ÄãŒµ‚àí1Œ±‚Äã|c1|1Œ±‚Äã2‚àí(R‚àí1)(1+2\alpha)^{\frac{1}{2\alpha}}\varepsilon^{-\frac{1}{\alpha}}|c\_{1}|^{\frac{1}{\alpha}}2^{-(R-1)} |
| K‚Äã(Œµ)K(\varepsilon) | K¬Ø‚Äã‚åàK+‚Äã(Œµ)K¬Ø‚åâ\underline{K}\left\lceil\frac{K^{+}(\varepsilon)}{\underline{K}}\right\rceil | |
| q‚Äã(Œµ)q(\varepsilon) | q1‚Äã(Œµ)=œÉ¬ØŒºŒµ‚àóq\_{1}(\varepsilon)=\frac{\bar{\sigma}}{\mu^{\*}\_{\varepsilon}}\quad ‚àÄr=2,‚Ä¶,R‚Äã(Œµ)qr‚Äã(Œµ)=V1‚Äã|Ar,R‚Äã(Œµ)|K¬ØŒ≤2‚Äã2(1+Œ≤)‚Äã(r‚àí1)2‚ÄãŒºŒµ‚àó\forall r=2,\dots,R(\varepsilon)\quad q\_{r}(\varepsilon)=\frac{\sqrt{V\_{1}}|A\_{r,R(\varepsilon)}|}{\underline{K}^{\frac{\beta}{2}}2^{\frac{(1+\beta)(r-1)}{2}}\mu^{\*}\_{\varepsilon}} | |
|  | with ŒºŒµ\mu\_{\varepsilon} such that ‚àër=1R‚Äã(Œµ)qr‚Äã(Œµ)=1\sum\_{r=1}^{R(\varepsilon)}q\_{r}(\varepsilon)=1 | |
| J‚Äã(Œµ)J(\varepsilon) | MŒµ‚ÄãœÉ¬Ø2q1‚Äã(Œµ)+‚àër=2R(ArR‚Äã(Œµ))2‚ÄãV1qr‚Äã(Œµ)‚ÄãK‚Äã(Œµ)Œ≤‚Äã2(r‚àí1)‚ÄãŒ≤Œµ2=MŒµ‚ÄãŒºŒµ‚Äã(œÉ¬Ø+V1‚ÄãK‚Äã(Œµ)‚àíŒ≤2‚Äã‚àër=2R‚Äã(Œµ)|Ar,R‚Äã(Œµ)|‚Äã2(r‚àí1)‚Äã(1‚àíŒ≤)2)Œµ2M\_{\varepsilon}\frac{\frac{\bar{\sigma}^{2}}{q\_{1}(\varepsilon)}+\sum\_{r=2}^{R}\frac{(A\_{r}^{R(\varepsilon)})^{2}V\_{1}}{q\_{r}(\varepsilon)K(\varepsilon)^{\beta}2^{(r-1)\beta}}}{\varepsilon^{2}}=M\_{\varepsilon}\frac{\mu\_{\varepsilon}\left(\bar{\sigma}+\sqrt{V\_{1}}K(\varepsilon)^{-\frac{\beta}{2}}\sum\_{r=2}^{R(\varepsilon)}|A\_{r,R(\varepsilon)}|2^{\frac{(r-1)(1-\beta)}{2}}\right)}{\varepsilon^{2}} | |
|  | with MŒµ=(1+12‚ÄãŒ±‚ÄãR‚Äã(Œµ))M\_{\varepsilon}=(1+\frac{1}{2\alpha R(\varepsilon)}) in the ML2R case and MŒµ=(1+12‚ÄãŒ±)M\_{\varepsilon}=(1+\frac{1}{2\alpha}) in the MLMC case. | |

Table 1: Closed-form optimized parameters for the standard and weighted Multi-level estimators. Where c~\tilde{c} is a constant such that c~>c~‚àû\tilde{c}>\tilde{c}\_{\infty} in (W‚ÄãEŒ±)(WE\_{\alpha}), and K¬Ø\underline{K} is a fixed integer greater or equal to 1.

#### 3.1.2 Extension of complexity theorems for arbitrary œÑ\tau

The objective of this section is to prove Theorem [4](https://arxiv.org/html/2510.18995v1#Thmtheorem4 "Theorem 4. ‚Ä£ 3.1.2 Extension of complexity theorems for arbitrary ùúè ‚Ä£ 3.1 State-of-the-art MLMC parameterization ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") below. That is, we want to show that the asymptotics from Theorem [2](https://arxiv.org/html/2510.18995v1#Thmtheorem2 "Theorem 2 ([5], Theorem 3.12, a)). ‚Ä£ 2.3 Theoretical guarantees on asymptotic complexity of MLMC estimators ‚Ä£ 2 Multi-level Monte Carlo and antithetic variance reduction ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") and Theorem [3](https://arxiv.org/html/2510.18995v1#Thmtheorem3 "Theorem 3 ([5], Theorem 3.12 b). ‚Ä£ 2.3 Theoretical guarantees on asymptotic complexity of MLMC estimators ‚Ä£ 2 Multi-level Monte Carlo and antithetic variance reduction ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") on the complexity of parameters in Table [1](https://arxiv.org/html/2510.18995v1#S3.T1 "Table 1 ‚Ä£ 3.1.1 Plug-and-play parameters ‚Ä£ 3.1 State-of-the-art MLMC parameterization ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") are preserved for arbitrary œÑ‚â•0\tau\geq 0.

###### Theorem 4.

For any real-valued function vv defined on the right neighbourhood of 0 such that limsupŒµ‚Üí0‚Äãv‚Äã(Œµ)‚Äãùíû~0‚Äã(Œ∏Œµ)<+‚àû\underset{\varepsilon\to 0}{\lim\sup}\;v(\varepsilon)\tilde{\mathcal{C}}\_{0}(\theta\_{\varepsilon})<+\infty
then,

|  |  |  |
| --- | --- | --- |
|  | limsupŒµ‚Üí0‚Äãv‚Äã(Œµ)‚Äãùíû~œÑ‚Äã(Œ∏Œµ)‚â§ùí¶Œ≤‚ÄãlimsupŒµ‚Üí0‚Äãv‚Äã(Œµ)‚Äãùíû~0‚Äã(Œ∏Œµ)\underset{\varepsilon\to 0}{\lim\sup}\;v(\varepsilon)\tilde{\mathcal{C}}\_{\tau}(\theta\_{\varepsilon})\leq\mathcal{K}\_{\beta}\;\underset{\varepsilon\to 0}{\lim\sup}\;v(\varepsilon)\tilde{\mathcal{C}}\_{0}(\theta\_{\varepsilon}) |  |

where,

|  |  |  |
| --- | --- | --- |
|  | ùí¶Œ≤:={1Œ≤‚â§1œÑd‚àû+1Œ≤>1\mathcal{K}\_{\beta}:=\begin{cases}1&\beta\leq 1\\ \frac{\tau}{d\_{\infty}}+1&\beta>1\end{cases} |  |

Therefore the complexities of Theorem [2](https://arxiv.org/html/2510.18995v1#Thmtheorem2 "Theorem 2 ([5], Theorem 3.12, a)). ‚Ä£ 2.3 Theoretical guarantees on asymptotic complexity of MLMC estimators ‚Ä£ 2 Multi-level Monte Carlo and antithetic variance reduction ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") and Theorem [3](https://arxiv.org/html/2510.18995v1#Thmtheorem3 "Theorem 3 ([5], Theorem 3.12 b). ‚Ä£ 2.3 Theoretical guarantees on asymptotic complexity of MLMC estimators ‚Ä£ 2 Multi-level Monte Carlo and antithetic variance reduction ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") still hold for generic œÑ‚â•0\tau\geq 0 (when replacing ùíû~0\tilde{\mathcal{C}}\_{0} with ùíû~œÑ\tilde{\mathcal{C}}\_{\tau}), and are still achieved with parameters in Table [1](https://arxiv.org/html/2510.18995v1#S3.T1 "Table 1 ‚Ä£ 3.1.1 Plug-and-play parameters ‚Ä£ 3.1 State-of-the-art MLMC parameterization ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations").

To prove the theorem, we begin with a proposition relating ùíû~œÑ\tilde{\mathcal{C}}\_{\tau} to ùíû~0\tilde{\mathcal{C}}\_{0} for generic œÑ‚â•0\tau\geq 0.

###### Proposition 2.

Let œÑ‚â•0\tau\geq 0 and Œ∏‚ààŒò\theta\in\Theta be fixed. Then

|  |  |  |
| --- | --- | --- |
|  | ùíû~œÑ‚Äã(Œ∏)ùíû~0‚Äã(Œ∏)=1+œÑ‚åàK‚åâ‚Äã‚àër=1Rqr‚Äã2r‚àí1.\frac{\tilde{\mathcal{C}}\_{\tau}(\theta)}{\tilde{\mathcal{C}}\_{0}(\theta)}=1+\frac{\tau}{\lceil K\rceil\sum\_{r=1}^{R}q\_{r}2^{r-1}}\,. |  |

###### Proof.

Let œÑ‚â•0\tau\geq 0 and Œ∏‚ààŒò\theta\in\Theta be fixed. From ([3](https://arxiv.org/html/2510.18995v1#S1.E3 "In Definition 2. ‚Ä£ 1 Introduction ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")), for all r‚àà{1,‚Ä¶,R}r\in\{1,\dots,R\}

|  |  |  |  |
| --- | --- | --- | --- |
|  | Œ≥œÑ‚Äã(Kr)=œÑ+‚åàK‚åâ‚Äã2r‚àí1=œÑ+Œ≥0‚Äã(Kr).\gamma\_{\tau}(K\_{r})=\tau+\lceil K\rceil 2^{r-1}=\tau+\gamma\_{0}(K\_{r})\,. |  | (14) |

Then using ([7](https://arxiv.org/html/2510.18995v1#S2.E7 "In Definition 7. ‚Ä£ 2.1 Introduction to Multi-level Monte Carlo estimators ‚Ä£ 2 Multi-level Monte Carlo and antithetic variance reduction ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")), ([8](https://arxiv.org/html/2510.18995v1#S2.E8 "In Definition 7. ‚Ä£ 2.1 Introduction to Multi-level Monte Carlo estimators ‚Ä£ 2 Multi-level Monte Carlo and antithetic variance reduction ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")) and the above equation we get,

|  |  |  |
| --- | --- | --- |
|  | ùíû~œÑ‚Äã(Œ∏)=J‚Äã‚àër=1Rqr‚ÄãŒ≥œÑ‚Äã(Kr)=œÑ‚ÄãJ‚Äã‚àër=1Rqr+‚àër=1Rqr‚Äã‚åàK‚åâ‚Äã2r‚àí1=œÑ‚ÄãJ+ùíû~0‚Äã(Œ∏).\tilde{\mathcal{C}}\_{\tau}(\theta)=J\sum\_{r=1}^{R}q\_{r}\gamma\_{\tau}(K\_{r})=\tau J\sum\_{r=1}^{R}q\_{r}+\sum\_{r=1}^{R}q\_{r}\lceil K\rceil 2^{r-1}=\tau J+\tilde{\mathcal{C}}\_{0}(\theta)\,. |  |

where we used ‚àër=1Rqr=1\sum\_{r=1}^{R}q\_{r}=1 in the last equality. Therefore,

|  |  |  |
| --- | --- | --- |
|  | ùíû~œÑ‚Äã(Œ∏)ùíû~0‚Äã(Œ∏)=œÑ‚ÄãJC~0‚Äã(Œ∏)+1=œÑ‚àër=1Rqr‚Äã‚åàK‚åâ‚Äã2r‚àí1+1\frac{\tilde{\mathcal{C}}\_{\tau}(\theta)}{\tilde{\mathcal{C}}\_{0}(\theta)}=\frac{\tau J}{\tilde{C}\_{0}(\theta)}+1=\frac{\tau}{\sum\_{r=1}^{R}q\_{r}\lceil K\rceil 2^{r-1}}+1 |  |

proving the result.
‚àé

We observe from this proposition that the relative behavior of ùíû~œÑ‚Äã(Œ∏Œµ)\tilde{\mathcal{C}}\_{\tau}(\theta\_{\varepsilon}) and ùíû~0‚Äã(Œ∏Œµ)\tilde{\mathcal{C}}\_{0}(\theta\_{\varepsilon}) depends on the asymptotic behavior of ‚åàK‚Äã(Œµ)‚åâ‚Äã‚àër=1R‚Äã(Œµ)qr‚Äã(Œµ)‚Äã2r‚àí1\lceil K(\varepsilon)\rceil\sum\_{r=1}^{R(\varepsilon)}q\_{r}(\varepsilon)2^{r-1}, which we analyze in the following proposition. The proof of this result is based on technical lemmas presented in Appendix [A.2](https://arxiv.org/html/2510.18995v1#A1.SS2 "A.2 Technical Lemmas for Section 3.1.2 ‚Ä£ Appendix A Appendix ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations").

###### Proposition 3.

For all Œµ>0\varepsilon>0, let Œ∏Œµ\theta\_{\varepsilon} be the MLMC parameters of Table [1](https://arxiv.org/html/2510.18995v1#S3.T1 "Table 1 ‚Ä£ 3.1.1 Plug-and-play parameters ‚Ä£ 3.1 State-of-the-art MLMC parameterization ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations"). If Œ≤>1\beta>1 then there exists d‚àû‚àà(0,+‚àû)d\_{\infty}\in(0,+\infty) such that

|  |  |  |  |
| --- | --- | --- | --- |
|  | K‚Äã(Œµ)‚Äã‚àër=1R‚Äã(Œµ)qr‚Äã(Œµ)‚Äã2r‚àí1‚Äã‚ü∂Œµ‚Üí0‚Äãd‚àû,K({\varepsilon})\sum\_{r=1}^{R(\varepsilon)}q\_{r}(\varepsilon)2^{r-1}\underset{\varepsilon\to 0}{\longrightarrow}d\_{\infty}\,, |  | (15) |

while if Œ≤‚â§1\beta\leq 1,

|  |  |  |  |
| --- | --- | --- | --- |
|  | K‚Äã(Œµ)‚Äã‚àër=1R‚Äã(Œµ)qr‚Äã(Œµ)‚Äã2r‚àí1‚Äã‚ü∂Œµ‚Üí0+‚àûK(\varepsilon)\sum\_{r=1}^{R(\varepsilon)}q\_{r}(\varepsilon)2^{r-1}\underset{\varepsilon\to 0}{\longrightarrow}+\infty |  | (16) |

###### Proof.

From Lemma [2](https://arxiv.org/html/2510.18995v1#Thmlemma2 "Lemma 2. ‚Ä£ A.2 Technical Lemmas for Section 3.1.2 ‚Ä£ Appendix A Appendix ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") we can consider Œµ>0\varepsilon>0 to be sufficiently small such that K‚Äã(Œµ)=K¬ØK({\varepsilon})=\underline{K}.
Then from the definition of q‚Äã(Œµ)q(\varepsilon),

|  |  |  |
| --- | --- | --- |
|  | qr‚Äã(Œµ)={œÉ¬ØK¬Ø‚ÄãŒºŒµr=1V1‚Äã|ArR‚Äã(Œµ)|K¬Ø1+Œ≤2‚Äã2(1+Œ≤)‚Äã(r‚àí1)2‚ÄãŒºŒµr‚àà{2,‚Ä¶,R‚Äã(Œµ)}q\_{r}(\varepsilon)=\begin{cases}\frac{\bar{\sigma}}{\sqrt{\underline{K}}\mu\_{\varepsilon}}&r=1\\ \frac{\sqrt{V\_{1}}|A\_{r}^{R(\varepsilon)}|}{\underline{K}^{\frac{1+\beta}{2}}2^{\frac{(1+\beta)(r-1)}{2}}\mu\_{\varepsilon}}&r\in\{2,\dots,R(\varepsilon)\}\end{cases} |  |

Therefore,

|  |  |  |
| --- | --- | --- |
|  | K‚Äã(Œµ)‚Äã‚àër=1R‚Äã(Œµ)qr‚Äã(Œµ)‚Äã2r‚àí1=1ŒºŒµ‚Äã[œÉ¬ØK¬Ø+V1K¬Ø1+Œ≤2‚Äã‚àër=2R‚Äã(Œµ)|ArR‚Äã(Œµ)|‚Äã2r‚àí12‚Äã(1‚àíŒ≤)].K(\varepsilon)\sum\_{r=1}^{R(\varepsilon)}q\_{r}(\varepsilon)2^{r-1}=\frac{1}{\mu\_{\varepsilon}}\left[\frac{\bar{\sigma}}{\sqrt{\underline{K}}}+\frac{\sqrt{V\_{1}}}{\underline{K}^{\frac{1+\beta}{2}}}\sum\_{r=2}^{R(\varepsilon)}|A\_{r}^{R(\varepsilon)}|2^{\frac{r-1}{2}(1-\beta)}\right]\,. |  |

Since R‚Äã(Œµ)‚Üí+‚àûR(\varepsilon)\to+\infty as Œµ‚Üí0\varepsilon\to 0 an application of Lemma [5](https://arxiv.org/html/2510.18995v1#Thmlemma5 "Lemma 5. ‚Ä£ A.2 Technical Lemmas for Section 3.1.2 ‚Ä£ Appendix A Appendix ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") yields that ŒºŒµ\mu\_{\varepsilon} converges to a limit Œº‚àû‚àà(0,+‚àû)\mu\_{\infty}\in(0,+\infty). Therefore it suffice to study the convergence of the term

|  |  |  |
| --- | --- | --- |
|  | ‚àër=2R‚Äã(Œµ)|ArR‚Äã(Œµ)|‚Äã2r‚àí12‚Äã(1‚àíŒ≤).\sum\_{r=2}^{R(\varepsilon)}|A\_{r}^{R(\varepsilon)}|2^{\frac{r-1}{2}(1-\beta)}\,. |  |

Assuming Œ≤>1\beta>1, then applying Lemma [3](https://arxiv.org/html/2510.18995v1#Thmlemma3 "Lemma 3 (See [31], Lemma 4.3). ‚Ä£ A.2 Technical Lemmas for Section 3.1.2 ‚Ä£ Appendix A Appendix ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") 2. with Œ≥=‚àí1‚àíŒ≤2>0\gamma=-\frac{1-\beta}{2}>0 gives ([15](https://arxiv.org/html/2510.18995v1#S3.E15 "In Proposition 3. ‚Ä£ 3.1.2 Extension of complexity theorems for arbitrary ùúè ‚Ä£ 3.1 State-of-the-art MLMC parameterization ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")). Assuming Œ≤‚â§1\beta\leq 1, remark that,

|  |  |  |
| --- | --- | --- |
|  | ‚àër=2R‚Äã(Œµ)|ArR‚Äã(Œµ)|‚Äã2r‚àí12‚Äã(1‚àíŒ≤)‚â•|AR‚Äã(Œµ)R‚Äã(Œµ)|‚Äã2R‚Äã(Œµ)‚àí12‚Äã(1‚àíŒ≤).\sum\_{r=2}^{R(\varepsilon)}|A\_{r}^{R(\varepsilon)}|2^{\frac{r-1}{2}(1-\beta)}\geq|A\_{R(\varepsilon)}^{R(\varepsilon)}|2^{\frac{R(\varepsilon)-1}{2}(1-\beta)}\,. |  |

Since R‚Äã(Œµ)‚Üí+‚àûR(\varepsilon)\to+\infty and by Lemma [4](https://arxiv.org/html/2510.18995v1#Thmlemma4 "Lemma 4 (See [31], Lemma 7.1). ‚Ä£ A.2 Technical Lemmas for Section 3.1.2 ‚Ä£ Appendix A Appendix ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations"), |AR‚Äã(Œµ)R‚Äã(Œµ)|‚Üí1|A\_{R(\varepsilon)}^{R(\varepsilon)}|\to 1 as Œµ‚Üí0\varepsilon\to 0, we have |AR‚Äã(Œµ)R‚Äã(Œµ)|‚Äã2R‚Äã(Œµ)‚àí12‚Äã(1‚àíŒ≤)‚Äã‚ü∂Œµ‚Üí0+‚àû|A\_{R(\varepsilon)}^{R(\varepsilon)}|2^{\frac{R(\varepsilon)-1}{2}(1-\beta)}\underset{\varepsilon\to 0}{\longrightarrow}+\infty and get ([16](https://arxiv.org/html/2510.18995v1#S3.E16 "In Proposition 3. ‚Ä£ 3.1.2 Extension of complexity theorems for arbitrary ùúè ‚Ä£ 3.1 State-of-the-art MLMC parameterization ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")).

Finally, assuming that Œ≤=1\beta=1 ; we let œï\phi be the function from ‚Ñï\mathbb{N} to ‚Ñï\mathbb{N} such that for all R‚àà‚ÑïR\in\mathbb{N}, œï‚Äã(R)=R‚àíR\phi(R)=R-\sqrt{R}. On one hand from Lemma [4](https://arxiv.org/html/2510.18995v1#Thmlemma4 "Lemma 4 (See [31], Lemma 7.1). ‚Ä£ A.2 Technical Lemmas for Section 3.1.2 ‚Ä£ Appendix A Appendix ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations"),

|  |  |  |  |
| --- | --- | --- | --- |
|  | limŒµ‚Üí0‚Äãinfj‚àà{1,‚Ä¶,œï‚Äã(R‚Äã(Œµ))}‚Äã|AjR‚Äã(Œµ)|>0.\underset{\varepsilon\to 0}{\lim}\underset{j\in\{1,\dots,\phi(R(\varepsilon))\}}{\inf}|A\_{j}^{R(\varepsilon)}|>0\,. |  | (17) |

On the other hand,

|  |  |  |
| --- | --- | --- |
|  | ‚àër=2R‚Äã(Œµ)|ArR‚Äã(Œµ)|‚Äã2r‚àí12‚Äã(1‚àíŒ≤2)‚â•‚àër=2œï‚Äã(R‚Äã(Œµ))|ArR‚Äã(Œµ)|‚Äã2r‚àí12‚Äã(1‚àíŒ≤2)‚â•infj‚àà{1,‚Ä¶,œï‚Äã(R‚Äã(Œµ))}‚Äã|AjR‚Äã(Œµ)|‚Äã(œï‚Äã(R‚Äã(Œµ))‚àí1).\sum\_{r=2}^{R(\varepsilon)}|A\_{r}^{R(\varepsilon)}|2^{\frac{r-1}{2}(1-\frac{\beta}{2})}\geq\sum\_{r=2}^{\phi(R(\varepsilon))}|A\_{r}^{R(\varepsilon)}|2^{\frac{r-1}{2}(1-\frac{\beta}{2})}\geq\underset{j\in\{1,\dots,\phi(R(\varepsilon))\}}{\inf}|A\_{j}^{R(\varepsilon)}|(\phi(R(\varepsilon))-1)\,. |  |

From ([17](https://arxiv.org/html/2510.18995v1#S3.E17 "In 3.1.2 Extension of complexity theorems for arbitrary ùúè ‚Ä£ 3.1 State-of-the-art MLMC parameterization ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")) and œï‚Äã(RŒµ)‚Üí+‚àû\phi(R\_{\varepsilon})\to+\infty as Œµ‚Üí0\varepsilon\to 0 we get

|  |  |  |
| --- | --- | --- |
|  | infj‚àà{1,‚Ä¶,œï‚Äã(R‚Äã(Œµ))}‚Äã|AjR‚Äã(Œµ)|‚Äã(œï‚Äã(R‚Äã(Œµ))‚àí1)‚Äã‚ü∂Œµ‚Üí0+‚àû,\underset{j\in\{1,\dots,\phi(R(\varepsilon))\}}{\inf}|A\_{j}^{R(\varepsilon)}|(\phi(R(\varepsilon))-1)\underset{\varepsilon\to 0}{\longrightarrow}+\infty\,, |  |

which gives ([16](https://arxiv.org/html/2510.18995v1#S3.E16 "In Proposition 3. ‚Ä£ 3.1.2 Extension of complexity theorems for arbitrary ùúè ‚Ä£ 3.1 State-of-the-art MLMC parameterization ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")).
‚àé

We are now ready to prove Theorem [4](https://arxiv.org/html/2510.18995v1#Thmtheorem4 "Theorem 4. ‚Ä£ 3.1.2 Extension of complexity theorems for arbitrary ùúè ‚Ä£ 3.1 State-of-the-art MLMC parameterization ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations").

###### Proof.

From Proposition [2](https://arxiv.org/html/2510.18995v1#Thmproposition2 "Proposition 2. ‚Ä£ 3.1.2 Extension of complexity theorems for arbitrary ùúè ‚Ä£ 3.1 State-of-the-art MLMC parameterization ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") and Proposition [3](https://arxiv.org/html/2510.18995v1#Thmproposition3 "Proposition 3. ‚Ä£ 3.1.2 Extension of complexity theorems for arbitrary ùúè ‚Ä£ 3.1 State-of-the-art MLMC parameterization ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations"), if Œ≤>1\beta>1,

|  |  |  |
| --- | --- | --- |
|  | ùíû~œÑ‚Äã(Œ∏Œµ)ùíû~0‚Äã(Œ∏Œµ)‚Äã‚ü∂Œµ‚Üí0‚Äã1+œÑd‚àû\frac{\tilde{\mathcal{C}}\_{\tau}(\theta\_{\varepsilon})}{\tilde{\mathcal{C}}\_{0}(\theta\_{\varepsilon})}\underset{\varepsilon\to 0}{\longrightarrow}1+\frac{\tau}{d\_{\infty}} |  |

whereas if Œ≤‚â§1\beta\leq 1,

|  |  |  |
| --- | --- | --- |
|  | ùíû~œÑ‚Äã(Œ∏Œµ)ùíû~0‚Äã(Œ∏Œµ)‚Äã‚ü∂Œµ‚Üí0‚Äã1.\frac{\tilde{\mathcal{C}}\_{\tau}(\theta\_{\varepsilon})}{\tilde{\mathcal{C}}\_{0}(\theta\_{\varepsilon})}\underset{\varepsilon\to 0}{\longrightarrow}1\,. |  |

Therefore in both cases,

|  |  |  |
| --- | --- | --- |
|  | limsupŒµ‚Üí0‚Äãv‚Äã(Œµ)‚Äãùíû~œÑ‚Äã(Œ∏Œµ)=ùí¶Œ≤‚ÄãlimsupŒµ‚Üí0‚Äãv‚Äã(Œµ)‚Äãùíû~0‚Äã(Œ∏Œµ),\underset{\varepsilon\to 0}{\lim\sup}\;v(\varepsilon)\tilde{\mathcal{C}}\_{\tau}(\theta\_{\varepsilon})=\mathcal{K}\_{\beta}\;\underset{\varepsilon\to 0}{\lim\sup}\;v(\varepsilon)\tilde{\mathcal{C}}\_{0}(\theta\_{\varepsilon})\,, |  |

proving the claim.
‚àé

### 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency

The optimized parameters presented in Table [1](https://arxiv.org/html/2510.18995v1#S3.T1 "Table 1 ‚Ä£ 3.1.1 Plug-and-play parameters ‚Ä£ 3.1 State-of-the-art MLMC parameterization ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") are derived using several asymptotic approximations. While these approximations are generally effective and yield robust parameters, they may become problematic in non-asymptotic regimes of precision Œµ>0\varepsilon>0. This issue typically arise when œÑ\tau is large or when ff is an indicator function. In such scenarios, R‚Äã(Œµ)R(\varepsilon) may significantly overestimate the optimal number of levels, leading to suboptimal performances. For instance, for relatively large Œµ\varepsilon, often a standard nested MC (i.e R=1R=1) is more efficient than a MLMC with R‚â•2R\geq 2. However the parameters in Table [1](https://arxiv.org/html/2510.18995v1#S3.T1 "Table 1 ‚Ä£ 3.1.1 Plug-and-play parameters ‚Ä£ 3.1 State-of-the-art MLMC parameterization ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") always yield R‚Äã(Œµ)‚â•2R(\varepsilon)\geq 2.

In this section, we revisit the derivation of optimal parameters from Lemaire-Pag√®s [[5](https://arxiv.org/html/2510.18995v1#bib.bib5)]. Our goal is to generalize their approach to arbitrary œÑ‚â•0\tau\geq 0 and to obtain more robust parameter choices for non-asymptotic regimes, including the option to select R=1R=1 when appropriate. The newly optimized parameters are presented in Table [2](https://arxiv.org/html/2510.18995v1#S3.T2 "Table 2 ‚Ä£ 3.2.4 Complexity analysis for the new optimized parameters ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations"). Among them R‚àó‚Äã(Œµ)R^{\*}(\varepsilon) and K‚àó‚Äã(Œµ)K^{\*}(\varepsilon) are obtained via a fast numerical optimization procedure, while q‚àó‚Äã(Œµ)q^{\*}(\varepsilon) and J‚àó‚Äã(Œµ)J^{\*}(\varepsilon) are given in closed-form.

The main result of this section is Theorem [5](https://arxiv.org/html/2510.18995v1#Thmtheorem5 "Theorem 5. ‚Ä£ 3.2.4 Complexity analysis for the new optimized parameters ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations"), which establishes that the new optimized parameters reduce the overall computational cost of the estimator while preserving its asymptotic MSE behavior. In particular, this ensures that the asymptotic guarantees provided by Theorem [2](https://arxiv.org/html/2510.18995v1#Thmtheorem2 "Theorem 2 ([5], Theorem 3.12, a)). ‚Ä£ 2.3 Theoretical guarantees on asymptotic complexity of MLMC estimators ‚Ä£ 2 Multi-level Monte Carlo and antithetic variance reduction ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") and Theorem [3](https://arxiv.org/html/2510.18995v1#Thmtheorem3 "Theorem 3 ([5], Theorem 3.12 b). ‚Ä£ 2.3 Theoretical guarantees on asymptotic complexity of MLMC estimators ‚Ä£ 2 Multi-level Monte Carlo and antithetic variance reduction ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") remain valid.

In this section, we assume that we have access to accurate values for the structural constants Œ±\alpha, c1c\_{1}, c~>c~‚àû\tilde{c}>\tilde{c}\_{\infty} from assumption (WEŒ±)(\mathrm{WE\_{\alpha}}) and to Œ≤\beta, V1V\_{1}, œÉ¬Ø\bar{\sigma} from assumption (VarŒ≤)(\mathrm{Var}\_{\beta}).
Recall that the objective is to solve the following minimization problem

|  |  |  |  |
| --- | --- | --- | --- |
|  | minŒ∏‚ààŒò‚Ñ≥‚Äã(Œ∏)‚â§Œµ2‚Äãùíû~œÑ‚Äã(Œ∏).\underset{\begin{subarray}{c}\theta\in\Theta\\ \mathcal{M}(\theta)\leq\varepsilon^{2}\end{subarray}}{\min}\tilde{\mathcal{C}}\_{\tau}(\theta)\,. |  | (18) |

While this problem cannot be solve directly, we build upon the approach of Lemaire-Pag√®s [[5](https://arxiv.org/html/2510.18995v1#bib.bib5)] for the derivation of an approximate solution.

#### 3.2.1 Optimization of JJ

We begin with the optimization of JJ while other parameters are fixed.

###### Definition 12.

Let Œ†\Pi be the parameter space defined as

|  |  |  |
| --- | --- | --- |
|  | Œ†:={(q,K,R)‚àà[0,1]‚Ñï√ó(0,+‚àû)√ó‚Ñï:‚àÄr‚àà{1,‚Ä¶,R},qr>0,‚àër=1Rqr=1}\Pi:=\left\{(q,K,R)\in[0,1]^{\mathbb{N}}\times(0,+\infty)\times\mathbb{N}:\forall r\in\{1,\dots,R\},\;q\_{r}>0,\;\sum\_{r=1}^{R}q\_{r}=1\right\} |  |

We will consider that Œ∏=(J,œÄ)‚ààŒò\theta=(J,\pi)\in\Theta where œÄ‚ààŒ†\pi\in\Pi, which is a slight abuse of notation. Letting œÄ‚ààŒ†\pi\in\Pi be fixed, the goal of this step is to find an approximate solution to

|  |  |  |  |
| --- | --- | --- | --- |
|  | minJ‚àà(0,+‚àû)‚Ñ≥‚Äã(J,œÄ)‚â§Œµ2‚Äãùíû~œÑ‚Äã(J,œÄ).\underset{\begin{subarray}{c}J\in(0,+\infty)\\ \mathcal{M}(J,\pi)\leq\varepsilon^{2}\end{subarray}}{\min}\;\tilde{\mathcal{C}}\_{\tau}(J,\pi)\,. |  | (19) |

Since the function (J,œÄ)‚Ü¶‚Ñ≥‚Äã(J,œÄ)(J,\pi)\mapsto\mathcal{M}(J,\pi) is not easily manageable, we begin by introducing an alternative function ‚Ñ≥~\widetilde{\mathcal{M}} to serve as a proxy in optimizing the parameter JJ. Observe that the bias of an MLMC estimator depends solely on the parameters KK and RR. Indeed from Definition [4](https://arxiv.org/html/2510.18995v1#Thmdefinition4 "Definition 4. ‚Ä£ 2.1 Introduction to Multi-level Monte Carlo estimators ‚Ä£ 2 Multi-level Monte Carlo and antithetic variance reduction ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") for all Œ∏=(J,q,K,R)‚ààŒò\theta=(J,q,K,R)\in\Theta,

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Äã[I^Œ∏]=ùîº‚Äã[YK1]+‚àër=2RArR‚Äãùîº‚Äã[Œî‚ÄãYKr]\mathbb{E}[\hat{I}\_{\theta}]=\mathbb{E}[Y\_{K\_{1}}]+\sum\_{r=2}^{R}A\_{r}^{R}\mathbb{E}[\Delta Y\_{K\_{r}}] |  |

Therefore, although Œº\mu is formally defined on the full parameter space Œò\Theta, it depends only on parameters (K,R)(K,R). That is, there exists a function Œº~:(0,+‚àû)√ó‚Ñï‚ü∂+‚àû\tilde{\mu}:(0,+\infty)\times\mathbb{N}\longrightarrow+\infty such that for all Œ∏=(J,K,q,R)‚ààŒò\theta=(J,K,q,R)\in\Theta, we have Œº‚Äã(Œ∏)=Œº~‚Äã(K,R)\mu(\theta)=\tilde{\mu}(K,R). Slightly abusing the notation, we will repeatedly use Œº‚Äã(K,R)\mu(K,R) instead Œº~‚Äã(K,R)\tilde{\mu}(K,R).

To get a more manageable expression of the variance ùí±‚Äã(Œ∏)\mathcal{V}(\theta), we introduce the notion of unit variance of an MLMC estimator which will provide a fully tractable upper bound under (VarŒ≤)(\mathrm{Var}\_{\beta}).

###### Definition 13.

Let v:Œ†‚ü∂‚Ñùv:\Pi\longrightarrow\mathbb{R} be the unit variance function of an MLMC estimator, defined as

|  |  |  |
| --- | --- | --- |
|  | ‚àÄœÄ=(q,K,R)‚ààŒ†,v‚Äã(œÄ):=‚àër=1RœÉ2‚Äã(r,K)qr\forall\pi=(q,K,R)\in\Pi,\quad v(\pi):=\sum\_{r=1}^{R}\frac{\sigma^{2}(r,K)}{q\_{r}} |  |

###### Definition 14.

Let Œ∏=(J,q,K,R)‚ààŒò\theta=(J,q,K,R)\in\Theta, we define

|  |  |  |
| --- | --- | --- |
|  | œÉ¬Ø2‚Äã(r,K):={œÉ¬Ø12r=1(ArR)2‚ÄãV1KrŒ≤r‚àà{2,‚Ä¶,R}\bar{\sigma}^{2}(r,K):=\begin{cases}\bar{\sigma}\_{1}^{2}&r=1\\ \frac{(A^{R}\_{r})^{2}V\_{1}}{K\_{r}^{\beta}}&r\in\{2,\dots,R\}\end{cases} |  |

Let v¬Ø:Œ†‚ü∂‚Ñù\bar{v}:\Pi\longrightarrow\mathbb{R} be defined by

|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚àÄœÄ=(q,K,R)‚ààŒ†,v¬Ø‚Äã(œÄ):=‚àër=1RœÉ¬Ø2‚Äã(r,K)qr.\forall\pi=(q,K,R)\in\Pi,\quad\bar{v}(\pi):=\sum\_{r=1}^{R}\frac{\bar{\sigma}^{2}(r,K)}{q\_{r}}\,. |  | (20) |

Then under (VarŒ≤)(\mathrm{Var}\_{\beta}) we have v¬Ø‚â•v\bar{v}\geq v.

The following proposition justify that we can use v¬Ø‚Äã(œÄ)J\frac{\bar{v}(\pi)}{J} as an appropriate proxy for the variance of an MLMC estimator.

###### Proposition 4.

For all Œ∏=(J,œÄ)‚ààŒò\theta=(J,\pi)\in\Theta we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùí±‚Äã(Œ∏)‚â§v¬Ø‚Äã(œÄ)J\mathcal{V}(\theta)\leq\frac{\bar{v}(\pi)}{J} |  | (21) |

###### Proof.

Let Œ∏=(J,œÄ)‚ààŒò\theta=(J,\pi)\in\Theta. Using Proposition [1](https://arxiv.org/html/2510.18995v1#Thmproposition1 "Proposition 1. ‚Ä£ 2.1 Introduction to Multi-level Monte Carlo estimators ‚Ä£ 2 Multi-level Monte Carlo and antithetic variance reduction ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations"),

|  |  |  |
| --- | --- | --- |
|  | ùí±‚Äã(Œ∏)=‚àër=1RœÉ2‚Äã(r,K)Jr=‚àër=1RœÉ2‚Äã(r,K)‚åàJ‚Äãqr‚åâ‚â§1J‚Äã‚àër=1RœÉ2‚Äã(r,K)qr=v‚Äã(œÄ)J‚â§v¬Ø‚Äã(œÄ)J,\mathcal{V}(\theta)=\sum\_{r=1}^{R}\frac{\sigma^{2}(r,K)}{J\_{r}}=\sum\_{r=1}^{R}\frac{\sigma^{2}(r,K)}{\lceil Jq\_{r}\rceil}\leq\frac{1}{J}\sum\_{r=1}^{R}\frac{\sigma^{2}(r,K)}{q\_{r}}=\frac{v(\pi)}{J}\leq\frac{\bar{v}(\pi)}{J}\,, |  |

which proves the claim.
‚àé

Below we recall with our notations, the result from Lemaire-Pag√®s [[5](https://arxiv.org/html/2510.18995v1#bib.bib5)] giving an asymptotic bias expansion for a general MLMC estimator, under (WEŒ±)(\mathrm{WE}\_{\alpha}) in the ML2R case and under (WEŒ±,1)(\mathrm{WE}\_{\alpha,1}) in the standard MLMC case.

###### Proposition 5 ([[5](https://arxiv.org/html/2510.18995v1#bib.bib5)], Propositon 3.4).

Let R‚â•1R\geq 1 and K‚àà(0,+‚àû)K\in(0,+\infty). In the ML2R case, assume (WEŒ±)(\mathrm{WE}\_{\alpha}) holds and that supR‚àà‚Ñï‚Äãsupk‚àà‚Ñï‚Äã|Œ∑R‚Äã(k)|<+‚àû\underset{R\in\mathbb{N}}{\sup}\;\underset{k\in\mathbb{N}}{\sup}\;|\eta\_{R}(k)|<+\infty. Then there exists a bounded function Œ∑:‚Ñù2‚ü∂‚Ñù\eta:\mathbb{R}^{2}\longrightarrow\mathbb{R} such that

|  |  |  |  |
| --- | --- | --- | --- |
|  | Œº‚Äã(K,R)=(‚àí1)R‚àí1‚ÄãcR‚åàK‚åâŒ±‚ÄãR‚Äã2Œ±‚ÄãR‚Äã(R‚àí1)2‚Äã(1+Œ∑‚Äã(‚åàK‚åâ‚àí1,R))\mu(K,R)=\frac{(-1)^{R-1}c\_{R}}{\lceil K\rceil^{\alpha R}2^{\frac{\alpha R(R-1)}{2}}}\left(1+\eta(\lceil K\rceil^{-1},R)\right) |  | (22) |

In the standard MLMC case, assume that (W‚ÄãEŒ±)(WE\_{\alpha}) holds with c1‚â†0c\_{1}\neq 0, then

|  |  |  |  |
| --- | --- | --- | --- |
|  | Œº‚Äã(K,R)=c1‚åàK‚åâŒ±‚Äã2(R‚àí1)‚ÄãŒ±‚Äã(1+Œ∑1‚Äã(KR‚àí1))\mu(K,R)=\frac{c\_{1}}{\lceil K\rceil^{\alpha}2^{(R-1)\alpha}}\left(1+\eta\_{1}(K\_{R}^{-1})\right) |  | (23) |

It is not possible to evaluate ([22](https://arxiv.org/html/2510.18995v1#S3.E22 "In Proposition 5 ([5], Propositon 3.4). ‚Ä£ 3.2.1 Optimization of ùêΩ ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")) and ([23](https://arxiv.org/html/2510.18995v1#S3.E23 "In Proposition 5 ([5], Propositon 3.4). ‚Ä£ 3.2.1 Optimization of ùêΩ ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")) exactly, as some key quantities are not directly accessible: Œ∑1\eta\_{1} in the MLMC case, and Œ∑\eta and cRc\_{R} in the ML2R case. To overcome this, we approximate the unknown terms by setting Œ∑‚â°0\eta\equiv 0, Œ∑1‚â°0\eta\_{1}\equiv 0, and replacing cRc\_{R} with its asymptotic upper bound c~R\tilde{c}^{R}. These simplifications yield the following tractable formulas for the bias of an MLMC estimator.

###### Definition 15.

For all œÄ0=(K,R)‚ààŒ†0\pi\_{0}=(K,R)\in\Pi\_{0}, let Œº~:Œ†0‚ü∂‚Ñù\tilde{\mu}:\Pi\_{0}\longrightarrow\mathbb{R} be the tractable bias of a general MLMC estimator, defined as

|  |  |  |  |
| --- | --- | --- | --- |
|  | Œº~‚Äã(K,R)=(‚àí1)R‚àí1‚Äãc~R‚åàK‚åâŒ±‚ÄãR‚Äã2Œ±‚ÄãR‚Äã(R‚àí1)2\tilde{\mu}(K,R)=\frac{(-1)^{R-1}\tilde{c}^{R}}{\lceil K\rceil^{\alpha R}2^{\frac{\alpha R(R-1)}{2}}} |  | (24) |

in the ML2R case and

|  |  |  |  |
| --- | --- | --- | --- |
|  | Œº~‚Äã(K,R)=c1‚åàK‚åâŒ±‚Äã2(R‚àí1)‚ÄãŒ±\tilde{\mu}(K,R)=\frac{c\_{1}}{\lceil K\rceil^{\alpha}2^{(R-1)\alpha}} |  | (25) |

in the standard MLMC case.

Combining the variance proxy and the bias proxy give us the proxy for the MSE of the estimator.

###### Definition 16.

We define the tractable proxy for the MSE of the estimator as the function ‚Ñ≥~:Œò‚Üí‚Ñù\widetilde{\mathcal{M}}:\Theta\to\mathbb{R} such that

|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚àÄœÄ=(J,q,K,R),‚Ñ≥~‚Äã(J,œÄ)=v¬Ø‚Äã(œÄ)J+Œº~2‚Äã(K,R).\forall\pi=(J,q,K,R),\quad\widetilde{\mathcal{M}}(J,\pi)=\frac{\bar{v}(\pi)}{J}+\tilde{\mu}^{2}(K,R)\,. |  | (26) |

For fixed œÄ‚ààŒ†\pi\in\Pi, instead of ([19](https://arxiv.org/html/2510.18995v1#S3.E19 "In 3.2.1 Optimization of ùêΩ ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")) we solve the more tractable problem,

|  |  |  |  |
| --- | --- | --- | --- |
|  | minJ‚àà(0,+‚àû)‚Ñ≥~‚Äã(J,œÄ)‚â§Œµ2‚Äãùíû~œÑ‚Äã(Œ∏)\underset{\begin{subarray}{c}J\in(0,+\infty)\\ \widetilde{\mathcal{M}}(J,\pi)\leq\varepsilon^{2}\end{subarray}}{\min}\;\tilde{\mathcal{C}}\_{\tau}(\theta) |  | (27) |

###### Proposition 6.

Problem ([27](https://arxiv.org/html/2510.18995v1#S3.E27 "In 3.2.1 Optimization of ùêΩ ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")) admits a solution if and only if œÄ=(q,K,R)‚ààŒ†\pi=(q,K,R)\in\Pi is such that Œº~‚Äã(K,R)<Œµ\tilde{\mu}(K,R)<\varepsilon. In that case the solution is given by,

|  |  |  |
| --- | --- | --- |
|  | J‚Äã(Œµ,œÄ)=v¬Ø‚Äã(œÄ)Œµ2‚àíŒº~2‚Äã(K,R)J(\varepsilon,\pi)=\frac{\bar{v}(\pi)}{\varepsilon^{2}-\tilde{\mu}^{2}(K,R)} |  |

###### Proof.

For any œÄ‚ààŒ†\pi\in\Pi, from ([8](https://arxiv.org/html/2510.18995v1#S2.E8 "In Definition 7. ‚Ä£ 2.1 Introduction to Multi-level Monte Carlo estimators ‚Ä£ 2 Multi-level Monte Carlo and antithetic variance reduction ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")), J‚Ü¶ùíû~œÑ‚Äã(J,œÄ)J\mapsto\tilde{\mathcal{C}}\_{\tau}(J,\pi) is increasing. While from ([26](https://arxiv.org/html/2510.18995v1#S3.E26 "In Definition 16. ‚Ä£ 3.2.1 Optimization of ùêΩ ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")), J‚Ü¶‚Ñ≥~‚Äã(J,œÄ)J\mapsto\widetilde{\mathcal{M}}(J,\pi) is decreasing. Therefore ([27](https://arxiv.org/html/2510.18995v1#S3.E27 "In 3.2.1 Optimization of ùêΩ ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")) is solved for the JJ saturating the constraint. Clearly a solution exists if and only if œÄ=(q,K,R)‚ààŒ†\pi=(q,K,R)\in\Pi is such that |Œº‚Äã(K,R)|2<Œµ2|\mu(K,R)|^{2}<\varepsilon^{2}. Solving in J‚àà(0,+‚àû)J\in(0,+\infty) the equation
‚Ñ≥~‚Äã(J,œÄ)=Œµ2\widetilde{\mathcal{M}}(J,\pi)=\varepsilon^{2} give the result.
‚àé

It then natural to find the optimized parameter œÄ\pi as a solution to,

|  |  |  |  |
| --- | --- | --- | --- |
|  | minœÄ=(q,K,R)‚ààŒ†|Œº~‚Äã(K,R)|<Œµ‚Äãùíû~œÑ‚Äã(J‚Äã(Œµ,œÄ),œÄ)\underset{\begin{subarray}{c}\pi=(q,K,R)\in\Pi\\ |\tilde{\mu}(K,R)|<\varepsilon\end{subarray}}{\min}\tilde{\mathcal{C}}\_{\tau}(J(\varepsilon,\pi),\pi) |  | (28) |

#### 3.2.2 Optimization of qq

To solve ([28](https://arxiv.org/html/2510.18995v1#S3.E28 "In 3.2.1 Optimization of ùêΩ ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")), building upon Lemaire-Pag√®s [[5](https://arxiv.org/html/2510.18995v1#bib.bib5)], we introduce the notion of effort of an MLMC estimator alongside with its tractable upper bound under (VarŒ≤)(\mathrm{Var}\_{\beta}).

###### Definition 17.

For all œÄ‚ààŒ†\pi\in\Pi we define œïœÑ‚Äã(œÄ)\phi\_{\tau}(\pi) the effort of an MLMC estimator defined by œïœÑ‚Äã(œÄ):=v‚Äã(œÄ)‚ÄãŒ∫œÑ‚Äã(œÄ)\phi\_{\tau}(\pi):=v(\pi)\kappa\_{\tau}(\pi)
and œï¬ØœÑ‚Äã(œÄ)\bar{\phi}\_{\tau}(\pi) its upper bound under (VarŒ≤)(\mathrm{Var}\_{\beta}) defined by œï¬ØœÑ‚Äã(œÄ):=v¬Ø‚Äã(œÄ)‚ÄãŒ∫œÑ‚Äã(œÄ)\bar{\phi}\_{\tau}(\pi):=\bar{v}(\pi)\kappa\_{\tau}(\pi).

Noticing that, for all Œµ>0\varepsilon>0, œÄ‚ààŒ†\pi\in\Pi,

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùíû~œÑ‚Äã(J‚Äã(Œµ,œÄ),œÄ)=J‚Äã(Œµ,œÄ)‚ÄãŒ∫œÑ‚Äã(œÄ)=v¬Ø‚Äã(œÄ)‚ÄãŒ∫œÑ‚Äã(œÄ)Œµ2‚àíŒº~2‚Äã(K,R)=œï¬ØœÑ‚Äã(œÄ)Œµ2‚àíŒº~2‚Äã(K,R),\tilde{\mathcal{C}}\_{\tau}(J(\varepsilon,\pi),\pi)=J(\varepsilon,\pi)\kappa\_{\tau}(\pi)=\frac{\overline{v}(\pi)\kappa\_{\tau}(\pi)}{\varepsilon^{2}-\tilde{\mu}^{2}(K,R)}=\frac{\overline{\phi}\_{\tau}(\pi)}{\varepsilon^{2}-\tilde{\mu}^{2}(K,R)}\,, |  | (29) |

therefore problem ([28](https://arxiv.org/html/2510.18995v1#S3.E28 "In 3.2.1 Optimization of ùêΩ ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")) becomes

|  |  |  |
| --- | --- | --- |
|  | minœÄ=(q,K,R)‚ààŒ†|Œº~‚Äã(K,R)|<Œµ‚Äãœï¬ØœÑ‚Äã(œÄ)Œµ2‚àíŒº~2‚Äã(K,R)\underset{\begin{subarray}{c}\pi=(q,K,R)\in\Pi\\ |\tilde{\mu}(K,R)|<\varepsilon\end{subarray}}{\min}\frac{\overline{\phi}\_{\tau}(\pi)}{\varepsilon^{2}-\tilde{\mu}^{2}(K,R)} |  |

Notice how the parameter qq only impact the numerator of the value function and not the denominator nor the constraint. Consequently, the optimal qq as a function of KK and RR is the solution of the problem

|  |  |  |  |
| --- | --- | --- | --- |
|  | minq‚àà[0,1]‚Ñï‚àÄr‚àà{1,‚Ä¶,R}‚Äãqr>0‚àër=1Rqr=1‚Äãœï¬ØœÑ‚Äã(q,K,R).\underset{\begin{subarray}{c}q\in[0,1]^{\mathbb{N}}\\ \forall r\in\{1,\dots,R\}\,q\_{r}>0\\ \sum\_{r=1}^{R}q\_{r}=1\end{subarray}}{\min}\;\overline{\phi}\_{\tau}(q,K,R)\,. |  | (30) |

For convenience, we denote Œ†0=(0,+‚àû)√ó‚Ñï\Pi\_{0}=(0,+\infty)\times\mathbb{N} the space of MLMC parameters other than JJ and qq. Following Lemaire-Pag√®s [[5](https://arxiv.org/html/2510.18995v1#bib.bib5)], to optimize qq we rely on their following Lemma.

###### Lemma 1 ([[5](https://arxiv.org/html/2510.18995v1#bib.bib5)], Lemma 3.5).

Let R‚àà‚ÑïR\in\mathbb{N} and for all j‚àà{1,‚Ä¶,R}j\in\{1,\dots,R\}, let aj>0a\_{j}>0, bj>0b\_{j}>0 and qj>0q\_{j}>0 such that ‚àër=1Rqr=1\sum\_{r=1}^{R}q\_{r}=1, then

|  |  |  |
| --- | --- | --- |
|  | (‚àëj=1Rajqj)‚Äã(‚àëj=1Rbj‚Äãqj)‚â•(‚àëj=1Raj‚Äãbj)2\left(\sum\_{j=1}^{R}\frac{a\_{j}}{q\_{j}}\right)\left(\sum\_{j=1}^{R}b\_{j}q\_{j}\right)\geq\left(\sum\_{j=1}^{R}\sqrt{a\_{j}b\_{j}}\right)^{2} |  |

and equality holds if and only if qj=aj‚Äãbj‚àí1Œºq\_{j}=\frac{\sqrt{a\_{j}b\_{j}^{-1}}}{\mu} where Œº=‚àër=1Raj‚Äãbj‚àí1\mu=\sum\_{r=1}^{R}\sqrt{a\_{j}b\_{j}^{-1}}

###### Proposition 7.

Let œÄ0=(K,R)‚ààŒ†0\pi\_{0}=(K,R)\in\Pi\_{0}, then ([30](https://arxiv.org/html/2510.18995v1#S3.E30 "In 3.2.2 Optimization of ùëû ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")) admits a unique solution (qr‚Äã(œÄ0))r‚àà{1,‚Ä¶,R}(q\_{r}(\pi\_{0}))\_{r\in\{1,\dots,R\}} defined by

|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚àÄr‚àà{1,‚Ä¶,R},qr‚Äã(œÄ0)=œÉ¬Ø‚Äã(r,K)Œ≥œÑ‚Äã(Kr)‚ÄãŒºœÄ0\forall r\in\{1,\dots,R\},\quad q\_{r}(\pi\_{0})=\frac{\bar{\sigma}(r,K)}{\sqrt{\gamma\_{\tau}(K\_{r})}\mu\_{\pi\_{0}}} |  | (31) |

where

|  |  |  |
| --- | --- | --- |
|  | ŒºœÄ0=‚àër=1RœÉ¬Ø‚Äã(r,K)Œ≥œÑ‚Äã(Kr)\mu\_{\pi\_{0}}=\sum\_{r=1}^{R}\frac{\bar{\sigma}(r,K)}{\sqrt{\gamma\_{\tau}(K\_{r})}} |  |

is a normalizing constant such that ‚àër=1Rqr‚Äã(œÄ0)=1\sum\_{r=1}^{R}q\_{r}(\pi\_{0})=1.
Letting œï¬ØœÑ‚àó:Œ†0‚ü∂‚Ñù\bar{\phi}^{\*}\_{\tau}:\Pi\_{0}\longrightarrow\mathbb{R} be defined as

|  |  |  |
| --- | --- | --- |
|  | ‚àÄœÄ0‚ààŒ†0,œï¬ØœÑ‚àó‚Äã(œÄ0)=(‚àër=1RœÉ¬Ø‚Äã(r,K)‚ÄãŒ≥œÑ‚Äã(Kr))2\forall\pi\_{0}\in\Pi\_{0},\quad\bar{\phi}^{\*}\_{\tau}(\pi\_{0})=\left(\sum\_{r=1}^{R}\bar{\sigma}(r,K)\sqrt{\gamma\_{\tau}(K\_{r})}\right)^{2} |  |

then for all œÄ0‚ààŒ†0\pi\_{0}\in\Pi\_{0},

|  |  |  |  |
| --- | --- | --- | --- |
|  | œï¬ØœÑ‚Äã(q‚Äã(œÄ0),œÄ0)=œï¬ØœÑ‚àó‚Äã(œÄ0)\bar{\phi}\_{\tau}(q(\pi\_{0}),\pi\_{0})=\bar{\phi}^{\*}\_{\tau}(\pi\_{0}) |  | (32) |

###### Proof.

Let (K,R)‚ààŒ†0(K,R)\in\Pi\_{0}. Notice that for q‚àà[0,1]‚Ñïq\in[0,1]^{\mathbb{N}} such that for all r‚àà{1,‚Ä¶,R}r\in\{1,\dots,R\}, qr>0q\_{r}>0 and ‚àër=1Rqr=1\sum\_{r=1}^{R}q\_{r}=1,

|  |  |  |
| --- | --- | --- |
|  | œï¬ØœÑ‚Äã(q,K,R)=v¬Ø‚Äã(q,K,R)‚ÄãŒ∫œÑ‚Äã(q,K,R)=(‚àër=1RœÉ¬Ø2‚Äã(r,K)qr)‚Äã(‚àër=1Rqr‚ÄãŒ≥œÑ‚Äã(Kr)).\bar{\phi}\_{\tau}(q,K,R)=\bar{v}(q,K,R)\kappa\_{\tau}(q,K,R)=\left(\sum\_{r=1}^{R}\frac{\bar{\sigma}^{2}(r,K)}{q\_{r}}\right)\left(\sum\_{r=1}^{R}q\_{r}\gamma\_{\tau}(K\_{r})\right)\,. |  |

Then a direct application of Lemma [1](https://arxiv.org/html/2510.18995v1#Thmlemma1 "Lemma 1 ([5], Lemma 3.5). ‚Ä£ 3.2.2 Optimization of ùëû ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") with aj=œÉ¬Ø2‚Äã(r,K)a\_{j}=\bar{\sigma}^{2}(r,K), j‚àà{1,‚Ä¶,R}j\in\{1,\dots,R\} and bj=Œ≥œÑ‚Äã(Kj)b\_{j}=\gamma\_{\tau}(K\_{j}), j‚àà{1,‚Ä¶,R}j\in\{1,\dots,R\} gives the result.
‚àé

###### Remark 4.

Notice that when œÑ=0\tau=0 we recover the optimal parameters from Table [1](https://arxiv.org/html/2510.18995v1#S3.T1 "Table 1 ‚Ä£ 3.1.1 Plug-and-play parameters ‚Ä£ 3.1 State-of-the-art MLMC parameterization ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations").

The optimal parameter œÄ0=(K,R)‚ààŒ†0\pi\_{0}=(K,R)\in\Pi\_{0} solving ([28](https://arxiv.org/html/2510.18995v1#S3.E28 "In 3.2.1 Optimization of ùêΩ ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")) are then solution to the problem

|  |  |  |  |
| --- | --- | --- | --- |
|  | minœÄ0‚ààŒ†0|Œº~‚Äã(œÄ0)|<Œµ‚Äãœï¬ØœÑ‚àó‚Äã(œÄ0)Œµ2‚àíŒº~2‚Äã(œÄ0)\underset{\begin{subarray}{c}\pi\_{0}\in\Pi\_{0}\\ |\tilde{\mu}(\pi\_{0})|<\varepsilon\end{subarray}}{\min}\frac{\bar{\phi}\_{\tau}^{\*}(\pi\_{0})}{\varepsilon^{2}-\tilde{\mu}^{2}(\pi\_{0})} |  | (33) |

#### 3.2.3 Optimization of KK and RR

This optimization problem can be solved efficiently with a numerical optimization procedure. To do so we proceed in two steps. First fixing R‚àà‚ÑïR\in\mathbb{N} we solve,

|  |  |  |  |
| --- | --- | --- | --- |
|  | minK‚àà(0,+‚àû)|Œº~‚Äã(K,R)|<Œµ‚Å°œï¬ØœÑ‚àó‚Äã(K,R)Œµ2‚àíŒº~2‚Äã(K,R).\min\_{\begin{subarray}{c}K\in(0,+\infty)\\ |\tilde{\mu}(K,R)|<\varepsilon\end{subarray}}\frac{\bar{\phi}\_{\tau}^{\*}(K,R)}{\varepsilon^{2}-\tilde{\mu}^{2}(K,R)}\,. |  | (34) |

Note that for K‚àà(0,+‚àû)K\in(0,+\infty) in the standard MLMC case

|  |  |  |
| --- | --- | --- |
|  | |Œº~‚Äã(K,R)|<Œµ‚ü∫K¬Ø‚Äã(Œµ,R):=‚åä|c1|1Œ±Œµ1Œ±‚Äã2R‚àí1‚åã+1‚â§‚åàK‚åâ|\tilde{\mu}(K,R)|<\varepsilon\Longleftrightarrow\underline{K}(\varepsilon,R):=\left\lfloor\frac{|c\_{1}|^{\frac{1}{\alpha}}}{\varepsilon^{\frac{1}{\alpha}}2^{R-1}}\right\rfloor+1\leq\lceil K\rceil |  |

while in the ML2R case

|  |  |  |
| --- | --- | --- |
|  | |Œº~‚Äã(K,R)|<Œµ‚ü∫K¬Ø‚Äã(Œµ,R):=‚åäc~1Œ±Œµ1Œ±‚ÄãR‚Äã2R‚àí12‚åã+1‚â§‚åàK‚åâ|\tilde{\mu}(K,R)|<\varepsilon\Longleftrightarrow\underline{K}(\varepsilon,R):=\left\lfloor\frac{\tilde{c}^{\frac{1}{\alpha}}}{\varepsilon^{\frac{1}{\alpha R}}2^{\frac{R-1}{2}}}\right\rfloor+1\leq\lceil K\rceil |  |

thus the constraint is explicit and straightforward to implement in practice. Furthermore for all K‚àà(0,+‚àû)K\in(0,+\infty), in the standard MLMC case, the value function can be written as

|  |  |  |
| --- | --- | --- |
|  | œï¬ØœÑ‚àó‚Äã(K,R)Œµ2‚àíŒº~2‚Äã(K,R)=(œÉ¬Ø1‚ÄãœÑ+‚åàK‚åâ+V1‚Äã‚åàK‚åâ‚àíŒ≤2‚Äã‚àër=2RœÑ+‚åàK‚åâ‚Äã2r‚àí1‚Äã2‚àíŒ≤‚Äã(r‚àí1)2)2Œµ2‚àíc12‚åàK‚åâ2‚ÄãŒ±‚Äã4Œ±‚Äã(R‚àí1)\frac{\bar{\phi}\_{\tau}^{\*}(K,R)}{\varepsilon^{2}-\tilde{\mu}^{2}(K,R)}=\frac{\left(\bar{\sigma}\_{1}\sqrt{\tau+\lceil K\rceil}+\sqrt{V\_{1}}\lceil K\rceil^{\frac{-\beta}{2}}\sum\_{r=2}^{R}\sqrt{\tau+\lceil K\rceil 2^{r-1}}2^{\frac{-\beta(r-1)}{2}}\right)^{2}}{\varepsilon^{2}-\frac{c\_{1}^{2}}{\lceil K\rceil^{2\alpha}4^{\alpha(R-1)}}} |  |

and in the ML2R case

|  |  |  |
| --- | --- | --- |
|  | œï¬ØœÑ‚àó‚Äã(K,R)Œµ2‚àíŒº~2‚Äã(K,R)=(œÉ¬Ø1‚ÄãœÑ+‚åàK‚åâ+V1‚Äã‚åàK‚åâ‚àíŒ≤2‚Äã‚àër=2R|WrR|‚ÄãœÑ+‚åàK‚åâ‚Äã2r‚àí1‚Äã2‚àíŒ≤‚Äã(r‚àí1)2)2Œµ2‚àíc~2‚ÄãR‚åàK‚åâ2‚ÄãŒ±‚ÄãR‚Äã2Œ±‚ÄãR‚Äã(R‚àí1).\frac{\bar{\phi}\_{\tau}^{\*}(K,R)}{\varepsilon^{2}-\tilde{\mu}^{2}(K,R)}=\frac{\left(\bar{\sigma}\_{1}\sqrt{\tau+\lceil K\rceil}+\sqrt{V\_{1}}\lceil K\rceil^{\frac{-\beta}{2}}\sum\_{r=2}^{R}|W^{R}\_{r}|\sqrt{\tau+\lceil K\rceil 2^{r-1}}2^{\frac{-\beta(r-1)}{2}}\right)^{2}}{\varepsilon^{2}-\frac{\tilde{c}^{2R}}{\lceil K\rceil^{2\alpha R}2^{\alpha R(R-1)}}}\,. |  |

The value function is therefore fully explicit and easy to implement in practice. Moreover it is constant on each intervals (n,n+1](n,n+1] for integers nn satisfying the constraint. Thus, the search for the optimal KK can therefore be restricted to integers values that satisfy the constraint. Accordingly we rewrite ([34](https://arxiv.org/html/2510.18995v1#S3.E34 "In 3.2.3 Optimization of ùêæ and ùëÖ ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")) as

|  |  |  |  |
| --- | --- | --- | --- |
|  | minK‚àà{K¬Ø‚Äã(Œµ,R),K¬Ø‚Äã(Œµ,R)+1,‚Ä¶}‚Å°œï¬ØœÑ‚àó‚Äã(K,R)Œµ2‚àíŒº~2‚Äã(K,R).\min\_{K\in\{\underline{K}(\varepsilon,R),\underline{K}(\varepsilon,R)+1,\dots\}}\frac{\bar{\phi}\_{\tau}^{\*}(K,R)}{\varepsilon^{2}-\tilde{\mu}^{2}(K,R)}\,. |  | (35) |

Since objective function diverges when KK increases, ([35](https://arxiv.org/html/2510.18995v1#S3.E35 "In 3.2.3 Optimization of ùêæ and ùëÖ ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")) is guaranteed to have a solution. In practice this problem is efficiently solved by standard optimization algorithms, as the objective function is typically unimodal.

Let K‚Äã(Œµ,R)K(\varepsilon,R) denotes a solution of ([34](https://arxiv.org/html/2510.18995v1#S3.E34 "In 3.2.3 Optimization of ùêæ and ùëÖ ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")). We then solve

|  |  |  |  |
| --- | --- | --- | --- |
|  | minR‚àà{R‚àí‚Äã(Œµ),R‚àí‚Äã(Œµ)+1,‚Ä¶,R‚Äã(Œµ)}‚Å°œï¬ØœÑ‚àó‚Äã(K‚Äã(Œµ,R),R)Œµ2‚àíŒº~2‚Äã(K‚Äã(Œµ,R),R)\min\_{R\in\{R\_{-}(\varepsilon),R\_{-}(\varepsilon)+1,\dots,R(\varepsilon)\}}\frac{\bar{\phi}^{\*}\_{\tau}(K(\varepsilon,R),R)}{\varepsilon^{2}-\tilde{\mu}^{2}(K(\varepsilon,R),R)} |  | (36) |

where R‚Äã(Œµ)R(\varepsilon) is the optimized RR found in Table [1](https://arxiv.org/html/2510.18995v1#S3.T1 "Table 1 ‚Ä£ 3.1.1 Plug-and-play parameters ‚Ä£ 3.1 State-of-the-art MLMC parameterization ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") and R‚àí‚Äã(Œµ)R\_{-}(\varepsilon) is such that R‚àí‚Äã(Œµ)‚Üí+‚àûR\_{-}(\varepsilon)\to+\infty as Œµ‚Üí0\varepsilon\to 0 and 1‚â§R‚àí‚Äã(Œµ)‚â§R‚Äã(Œµ)1\leq R\_{-}(\varepsilon)\leq R(\varepsilon). For instance R‚àí‚Äã(Œµ)=‚åàlog10‚Å°(R‚Äã(Œµ))‚åâR\_{-}(\varepsilon)=\lceil\log\_{10}(R(\varepsilon))\rceil satisfy the conditions. This lower bound ensures the asymptotic guarantees stated in Theorem [5](https://arxiv.org/html/2510.18995v1#Thmtheorem5 "Theorem 5. ‚Ä£ 3.2.4 Complexity analysis for the new optimized parameters ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") for the parameters. Nevertheless, in practical computations, the search for the optimal R can be performed over the range 11 to R‚Äã(Œµ)R(\varepsilon). This optimization problem is straightforward to solve, as the objective function can be evaluated efficiently.

###### Remark 5.

In practice, for the ML2R case, using c~R\tilde{c}^{R} as a proxy for cRc\_{R} in ([24](https://arxiv.org/html/2510.18995v1#S3.E24 "In Definition 15. ‚Ä£ 3.2.1 Optimization of ùêΩ ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")) is appropriate only when RR is large. However, since the optimization problems ([34](https://arxiv.org/html/2510.18995v1#S3.E34 "In 3.2.3 Optimization of ùêæ and ùëÖ ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")) and ([36](https://arxiv.org/html/2510.18995v1#S3.E36 "In 3.2.3 Optimization of ùêæ and ùëÖ ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")) also consider small values of RR, a more accurate approximation of for cRc\_{R} is given by c1‚ÄãaRc\_{1}a^{R} for some a>1a>1. The coefficient c1c\_{1} can typically be estimated reliably in a pre-processing phase. Accordingly, all instance of c~\tilde{c} should be replace by c11R‚Äãac\_{1}^{\frac{1}{R}}a in the relevant expressions. Numerical experiments suggests that when dealing with indicator functions, choosing a=2a=2 or 33 yields effective results. Note that in order to guarantee the satisfaction of the MSE constraint is satisfied, it is preferable to overestimate cRc\_{R} rather than to underestimate it. In our numerical experiments of Section [6](https://arxiv.org/html/2510.18995v1#S6 "6 Numerical experiments ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations"), we will adopt this proxy with a=2a=2.

#### 3.2.4 Complexity analysis for the new optimized parameters

In this section we formalize the new optimized parameters and prove Theorem [5](https://arxiv.org/html/2510.18995v1#Thmtheorem5 "Theorem 5. ‚Ä£ 3.2.4 Complexity analysis for the new optimized parameters ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations"). Given a target precision Œµ>0\varepsilon>0, we denote by R‚àó‚Äã(Œµ)R^{\*}(\varepsilon) the solution to the optimization problem¬†([36](https://arxiv.org/html/2510.18995v1#S3.E36 "In 3.2.3 Optimization of ùêæ and ùëÖ ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")). Based on this value, we define K‚àó‚Äã(Œµ):=K‚àó‚Äã(Œµ,R‚àó‚Äã(Œµ))K^{\*}(\varepsilon):=K^{\*}(\varepsilon,R^{\*}(\varepsilon)), and we collect these into the vector œÄ0‚àó‚Äã(Œµ):=(K‚àó‚Äã(Œµ),R‚àó‚Äã(Œµ))\pi^{\*}\_{0}(\varepsilon):=(K^{\*}(\varepsilon),R^{\*}(\varepsilon)). Next, we define q‚àó‚Äã(Œµ):=q‚Äã(Œµ,œÄ0‚àó‚Äã(Œµ))q^{\*}(\varepsilon):=q(\varepsilon,\pi^{\*}\_{0}(\varepsilon)), and aggregate this with the previous parameters as œÄ‚àó‚Äã(Œµ):=(q‚àó‚Äã(Œµ),œÄ0‚Äã(Œµ))\pi^{\*}(\varepsilon):=(q^{\*}(\varepsilon),\pi\_{0}(\varepsilon)). Finally let J‚àó‚Äã(Œµ):=J‚Äã(Œµ,œÄ‚àó‚Äã(Œµ))J^{\*}(\varepsilon):=J(\varepsilon,\pi^{\*}(\varepsilon)). We summarize all the optimized parameters for the target precision Œµ\varepsilon by the vector Œ∏Œµ‚àó:=(J‚àó‚Äã(Œµ),q‚àó‚Äã(Œµ),K‚àó‚Äã(Œµ),R‚àó‚Äã(Œµ))\theta^{\*}\_{\varepsilon}:=(J^{\*}(\varepsilon),q^{\*}(\varepsilon),K^{\*}(\varepsilon),R^{\*}(\varepsilon)). For convenience, all these definitions are summarized in Table¬†[2](https://arxiv.org/html/2510.18995v1#S3.T2 "Table 2 ‚Ä£ 3.2.4 Complexity analysis for the new optimized parameters ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations").

| Parameter | Weighted (ML2R) | Classical (MLMC) |
| --- | --- | --- |
| R‚àó‚Äã(Œµ)R^{\*}(\varepsilon) | arg‚Å°minR‚àà{R‚àí‚Äã(Œµ),‚Ä¶,R‚Äã(Œµ)}‚Äãœï¬ØœÑ‚àó‚Äã(K‚Äã(Œµ,R),R)Œµ2‚àíŒº~2‚Äã(K‚Äã(Œµ,R),R)\underset{{R\in\{R\_{-}(\varepsilon),\dots,R(\varepsilon)\}}}{\arg\min}\;\frac{\bar{\phi}^{\*}\_{\tau}(K(\varepsilon,R),R)}{\varepsilon^{2}-\tilde{\mu}^{2}(K(\varepsilon,R),R)} | |
| K¬Ø‚àó‚Äã(Œµ)\underline{K}^{\*}(\varepsilon) | ‚åäc~1Œ±Œµ1Œ±‚ÄãR‚àó‚Äã(Œµ)‚Äã2R‚àó‚Äã(Œµ)‚àí12‚åã+1\quad\quad\quad\quad\left\lfloor\frac{\tilde{c}^{\frac{1}{\alpha}}}{\varepsilon^{\frac{1}{\alpha R^{\*}(\varepsilon)}}2^{\frac{R^{\*}(\varepsilon)-1}{2}}}\right\rfloor+1\quad\quad\quad\quad | ‚åä|c1|1Œ±Œµ1Œ±‚Äã2R‚àó‚Äã(Œµ)‚àí1‚åã+1\left\lfloor\frac{|c\_{1}|^{\frac{1}{\alpha}}}{\varepsilon^{\frac{1}{\alpha}}2^{R^{\*}(\varepsilon)-1}}\right\rfloor+1 |
| K‚àó‚Äã(Œµ)K^{\*}(\varepsilon) | arg‚Å°minK‚àà{K¬Ø‚àó‚Äã(Œµ),K¬Ø‚àó‚Äã(Œµ)+1,‚Ä¶}‚Äãœï¬ØœÑ‚àó‚Äã(K,R‚àó‚Äã(Œµ))Œµ2‚àíŒº~2‚Äã(K,R‚àó‚Äã(Œµ))\underset{K\in\{\underline{K}^{\*}(\varepsilon),\underline{K}^{\*}(\varepsilon)+1,\dots\}}{\arg\min}\frac{\bar{\phi}\_{\tau}^{\*}(K,R^{\*}(\varepsilon))}{\varepsilon^{2}-\tilde{\mu}^{2}(K,R^{\*}(\varepsilon))} | |
| q‚àó‚Äã(Œµ)q^{\*}(\varepsilon) | q1‚àó‚Äã(Œµ)=œÉ¬Ø1œÑ+‚åàK‚àó‚Äã(Œµ)‚åâ‚ÄãŒºŒµ‚àóq^{\*}\_{1}(\varepsilon)=\frac{\bar{\sigma}\_{1}}{\sqrt{\tau+\lceil K^{\*}(\varepsilon)\rceil}\mu^{\*}\_{\varepsilon}}\quad ‚àÄr=2,‚Ä¶,R‚Äã(Œµ)qr‚àó‚Äã(Œµ)=V1‚Äã|ArR‚àó‚Äã(Œµ)|‚åàK‚àó‚Äã(Œµ)‚åâŒ≤2‚Äã2Œ≤‚Äã(r‚àí1)2‚ÄãœÑ+‚åàK‚àó‚Äã(Œµ)‚åâ‚Äã2r‚àí1‚ÄãŒºŒµ‚àó\forall r=2,\dots,R(\varepsilon)\quad q^{\*}\_{r}(\varepsilon)=\frac{\sqrt{V\_{1}}|A\_{r}^{R^{\*}(\varepsilon)}|}{\left\lceil K^{\*}(\varepsilon)\right\rceil^{\frac{\beta}{2}}2^{\frac{\beta(r-1)}{2}}\sqrt{\tau+\left\lceil K^{\*}(\varepsilon)\right\rceil 2^{r-1}}\mu^{\*}\_{\varepsilon}} | |
|  | with ŒºŒµ‚àó\mu^{\*}\_{\varepsilon} such that ‚àër=1R‚àó‚Äã(Œµ)qr‚àó‚Äã(Œµ)=1\sum\_{r=1}^{R^{\*}(\varepsilon)}q^{\*}\_{r}(\varepsilon)=1 | |
| J‚Äã(Œµ)J(\varepsilon) | v¬Ø‚Äã(q‚àó‚Äã(Œµ),K‚àó‚Äã(Œµ),R‚àó‚Äã(Œµ))Œµ2‚àíŒº~2‚Äã(K‚àó‚Äã(Œµ),R‚àó‚Äã(Œµ))\frac{\bar{v}(q^{\*}(\varepsilon),K^{\*}(\varepsilon),R^{\*}(\varepsilon))}{\varepsilon^{2}-\tilde{\mu}^{2}(K^{\*}(\varepsilon),R^{\*}(\varepsilon))} | |

Table 2: New optimized parameters for the standard and weighted Multi-level estimators. Here c~\tilde{c} is a constant such that c~>c~‚àû\tilde{c}>\tilde{c}\_{\infty} in (W‚ÄãEŒ±)(WE\_{\alpha}).

Our objective is now to prove Theorem [5](https://arxiv.org/html/2510.18995v1#Thmtheorem5 "Theorem 5. ‚Ä£ 3.2.4 Complexity analysis for the new optimized parameters ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations"), which demonstrate a reduction in the computational cost when using Œ∏Œµ‚àó\theta^{\*}\_{\varepsilon} from Table [2](https://arxiv.org/html/2510.18995v1#S3.T2 "Table 2 ‚Ä£ 3.2.4 Complexity analysis for the new optimized parameters ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") in place of Œ∏Œµ\theta\_{\varepsilon} from Table [1](https://arxiv.org/html/2510.18995v1#S3.T1 "Table 1 ‚Ä£ 3.1.1 Plug-and-play parameters ‚Ä£ 3.1 State-of-the-art MLMC parameterization ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations"), while preserving the same asymptotic MSE behavior. The proof of the theorem is based on technical Lemmas presented in Appendix [A.3](https://arxiv.org/html/2510.18995v1#A1.SS3 "A.3 Technical Lemmas for Section 3.2.4 ‚Ä£ Appendix A Appendix ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations").

###### Theorem 5.

Let Œµ>0\varepsilon>0, œÑ‚â•0\tau\geq 0, then ùíû~œÑ‚Äã(Œ∏Œµ‚àó)‚â§ùíû~œÑ‚Äã(Œ∏Œµ)\tilde{\mathcal{C}}\_{\tau}(\theta^{\*}\_{\varepsilon})\leq\tilde{\mathcal{C}}\_{\tau}(\theta\_{\varepsilon}). Assume that (VarŒ≤)(\mathrm{Var}\_{\beta}) holds. In the standard MLMC case assume furthermore that (WEŒ±,1)(\mathrm{WE}\_{\alpha,1}) holds while in the case of the ML2R assume that (WEŒ±)(\mathrm{WE}\_{\alpha}) holds with supR‚àà‚Ñï‚Äãsupk‚àà‚Ñï‚Äã|Œ∑R‚Äã(k)|<+‚àû\underset{R\in\mathbb{N}}{\sup}\;\underset{k\in\mathbb{N}}{\sup}\;|\eta\_{R}(k)|<+\infty instead. Then,

|  |  |  |
| --- | --- | --- |
|  | limsupŒµ‚Üí0‚ÄãŒµ‚àí2‚Äã‚Ñ≥‚Äã(Œ∏Œµ‚àó)‚â§1.\underset{\varepsilon\to 0}{\lim\sup}\;\varepsilon^{-2}\mathcal{M}(\theta^{\*}\_{\varepsilon})\leq 1\,. |  |

###### Proof.

Let Œµ>0\varepsilon>0, œÑ‚â•0\tau\geq 0 and recall that J‚àó‚Äã(Œµ)=J‚Äã(Œµ,œÄ‚àó‚Äã(Œµ))J^{\*}(\varepsilon)=J(\varepsilon,\pi^{\*}(\varepsilon)) and q‚àó‚Äã(Œµ)=q‚Äã(K‚àó‚Äã(Œµ),R‚àó‚Äã(Œµ))q^{\*}(\varepsilon)=q(K^{\*}(\varepsilon),R^{\*}(\varepsilon)). Therefore by ([29](https://arxiv.org/html/2510.18995v1#S3.E29 "In 3.2.2 Optimization of ùëû ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")), ([32](https://arxiv.org/html/2510.18995v1#S3.E32 "In Proposition 7. ‚Ä£ 3.2.2 Optimization of ùëû ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")) and the definition of œÄ0‚àó‚Äã(Œµ)\pi\_{0}^{\*}(\varepsilon) we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùíû~œÑ‚Äã(Œ∏Œµ‚àó)=œï¬ØœÑ‚àó‚Äã(K‚àó‚Äã(Œµ),R‚àó‚Äã(Œµ))Œµ2‚àíŒº~2‚Äã(K‚àó‚Äã(Œµ),R‚àó‚Äã(Œµ))=arg‚Å°minK‚àà(0,+‚àû)R‚àà{R‚àí‚Äã(Œµ),‚Ä¶,R‚Äã(Œµ)}|Œº~‚Äã(K,R)|<Œµ‚Äãœï¬ØœÑ‚àó‚Äã(K,R)Œµ2‚àíŒº~2‚Äã(K,R)\tilde{\mathcal{C}}\_{\tau}(\theta^{\*}\_{\varepsilon})=\frac{\bar{\phi}^{\*}\_{\tau}(K^{\*}(\varepsilon),R^{\*}(\varepsilon))}{\varepsilon^{2}-\tilde{\mu}^{2}(K^{\*}(\varepsilon),R^{\*}(\varepsilon))}=\underset{{\begin{subarray}{c}K\in(0,+\infty)\\ R\in\{R\_{-}(\varepsilon),\dots,R(\varepsilon)\}\\ |\tilde{\mu}(K,R)|<\varepsilon\end{subarray}}}{\arg\min}\;\frac{\bar{\phi}^{\*}\_{\tau}(K,R)}{\varepsilon^{2}-\tilde{\mu}^{2}(K,R)} |  | (37) |

Since from Lemma [6](https://arxiv.org/html/2510.18995v1#Thmlemma6 "Lemma 6. ‚Ä£ A.3 Technical Lemmas for Section 3.2.4 ‚Ä£ Appendix A Appendix ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations"), œÄ0‚Äã(Œµ)=(K‚Äã(Œµ),R‚Äã(Œµ))\pi\_{0}(\varepsilon)=(K(\varepsilon),R(\varepsilon)) is admissible for ([37](https://arxiv.org/html/2510.18995v1#S3.E37 "In 3.2.4 Complexity analysis for the new optimized parameters ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")) we conclude that

|  |  |  |
| --- | --- | --- |
|  | ùíû~œÑ‚Äã(Œ∏Œµ‚àó)‚â§œï¬ØœÑ‚àó‚Äã(K‚Äã(Œµ),R‚Äã(Œµ))Œµ2‚àíŒº~2‚Äã(K‚Äã(Œµ),R‚Äã(Œµ)).\tilde{\mathcal{C}}\_{\tau}(\theta^{\*}\_{\varepsilon})\leq\frac{\bar{\phi}^{\*}\_{\tau}(K(\varepsilon),R(\varepsilon))}{\varepsilon^{2}-\tilde{\mu}^{2}(K(\varepsilon),R(\varepsilon))}\,. |  |

Recall that œï¬ØœÑ‚àó‚Äã(œÄ0‚Äã(Œµ))=œï¬ØœÑ‚Äã(q‚Äã(Œµ,œÄ0‚Äã(Œµ)),œÄ0‚Äã(Œµ))\bar{\phi}^{\*}\_{\tau}(\pi\_{0}(\varepsilon))=\bar{\phi}\_{\tau}(q(\varepsilon,\pi\_{0}(\varepsilon)),\pi\_{0}(\varepsilon))
and by Proposition [7](https://arxiv.org/html/2510.18995v1#Thmproposition7 "Proposition 7. ‚Ä£ 3.2.2 Optimization of ùëû ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations"),

|  |  |  |
| --- | --- | --- |
|  | q‚Äã(Œµ,œÄ0‚Äã(Œµ))=arg‚Å°minq‚àà[0,1]‚Ñï‚àër=1Rqr=1‚Äãœï¬ØœÑ‚Äã(q,œÄ0‚Äã(Œµ)).q(\varepsilon,\pi\_{0}(\varepsilon))=\underset{\begin{subarray}{c}q\in[0,1]^{\mathbb{N}}\\ \sum\_{r=1}^{R}q\_{r}=1\end{subarray}}{\arg\min}\;\bar{\phi}\_{\tau}(q,\pi\_{0}(\varepsilon))\,. |  |

Therefore,

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùíû~œÑ‚Äã(Œ∏Œµ‚àó)‚â§œï¬ØœÑ‚àó‚Äã(K‚Äã(Œµ),R‚Äã(Œµ))Œµ2‚àíŒº~2‚Äã(K‚Äã(Œµ),R‚Äã(Œµ))‚â§œï¬ØœÑ‚Äã(q‚Äã(Œµ),œÄ0‚Äã(Œµ))Œµ2‚àíŒº~2‚Äã(K‚Äã(Œµ),R‚Äã(Œµ)).\tilde{\mathcal{C}}\_{\tau}(\theta^{\*}\_{\varepsilon})\leq\frac{\bar{\phi}^{\*}\_{\tau}(K(\varepsilon),R(\varepsilon))}{\varepsilon^{2}-\tilde{\mu}^{2}(K(\varepsilon),R(\varepsilon))}\leq\frac{\bar{\phi}\_{\tau}(q(\varepsilon),\pi\_{0}(\varepsilon))}{\varepsilon^{2}-\tilde{\mu}^{2}(K(\varepsilon),R(\varepsilon))}\,. |  | (38) |

Recalling that for all œÄ=(q,K,R)‚ààŒ†\pi=(q,K,R)\in\Pi,

|  |  |  |
| --- | --- | --- |
|  | ùíû~œÑ‚Äã(J‚Äã(Œµ,œÄ),œÄ)=œï¬ØœÑ‚Äã(œÄ)Œµ2‚àíŒº~2‚Äã(K,R),\tilde{\mathcal{C}}\_{\tau}(J(\varepsilon,\pi),\pi)=\frac{\bar{\phi}\_{\tau}(\pi)}{\varepsilon^{2}-\tilde{\mu}^{2}(K,R)}\,, |  |

then we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | œï¬ØœÑ‚Äã(q‚Äã(Œµ),œÄ0‚Äã(Œµ))Œµ2‚àíŒº~2‚Äã(K‚Äã(Œµ),R‚Äã(Œµ))=ùíû~œÑ‚Äã(J‚Äã(Œµ,œÄ‚Äã(Œµ)),œÄ‚Äã(Œµ)).\frac{\bar{\phi}\_{\tau}(q(\varepsilon),\pi\_{0}(\varepsilon))}{\varepsilon^{2}-\tilde{\mu}^{2}(K(\varepsilon),R(\varepsilon))}=\tilde{\mathcal{C}}\_{\tau}(J(\varepsilon,\pi(\varepsilon)),\pi(\varepsilon))\,. |  | (39) |

where œÄ‚Äã(Œµ)=(q‚Äã(Œµ),œÄ0‚Äã(Œµ))\pi(\varepsilon)=(q(\varepsilon),\pi\_{0}(\varepsilon)). Since by definition of J‚Äã(Œµ,œÄ‚Äã(Œµ))J(\varepsilon,\pi(\varepsilon)),

|  |  |  |
| --- | --- | --- |
|  | J‚Äã(Œµ,œÄ‚Äã(Œµ))=arg‚Å°minJ‚àà(0,+‚àû)‚Ñ≥~‚Äã(J,œÄ‚Äã(Œµ))‚â§Œµ2‚ÄãC~œÑ‚Äã(J,œÄ‚Äã(Œµ))J(\varepsilon,\pi(\varepsilon))=\underset{\begin{subarray}{c}J\in(0,+\infty)\\ \widetilde{\mathcal{M}}(J,\pi(\varepsilon))\leq\varepsilon^{2}\end{subarray}}{\arg\min}\tilde{C}\_{\tau}(J,\pi(\varepsilon)) |  |

and from Lemma [8](https://arxiv.org/html/2510.18995v1#Thmlemma8 "Lemma 8. ‚Ä£ A.3 Technical Lemmas for Section 3.2.4 ‚Ä£ Appendix A Appendix ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") ‚Ñ≥~‚Äã(J‚Äã(Œµ),œÄ‚Äã(Œµ))‚â§Œµ2\widetilde{\mathcal{M}}(J(\varepsilon),\pi(\varepsilon))\leq\varepsilon^{2} we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùíû~œÑ‚Äã(J‚Äã(Œµ,œÄ‚Äã(Œµ)),œÄ‚Äã(Œµ))‚â§ùíû~œÑ‚Äã(J‚Äã(Œµ),œÄ‚Äã(Œµ))=ùíû~œÑ‚Äã(Œ∏Œµ)\tilde{\mathcal{C}}\_{\tau}(J(\varepsilon,\pi(\varepsilon)),\pi(\varepsilon))\leq\tilde{\mathcal{C}}\_{\tau}(J(\varepsilon),\pi(\varepsilon))=\tilde{\mathcal{C}}\_{\tau}(\theta\_{\varepsilon}) |  | (40) |

Combining ([38](https://arxiv.org/html/2510.18995v1#S3.E38 "In 3.2.4 Complexity analysis for the new optimized parameters ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")), ([39](https://arxiv.org/html/2510.18995v1#S3.E39 "In 3.2.4 Complexity analysis for the new optimized parameters ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")) and ([40](https://arxiv.org/html/2510.18995v1#S3.E40 "In 3.2.4 Complexity analysis for the new optimized parameters ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")) then gives ùíû~œÑ‚Äã(Œ∏Œµ‚àó)‚â§ùíû~œÑ‚Äã(Œ∏Œµ)\tilde{\mathcal{C}}\_{\tau}(\theta^{\*}\_{\varepsilon})\leq\tilde{\mathcal{C}}\_{\tau}(\theta\_{\varepsilon}) which proves the first claim.

We now prove the second claim. Denoting œÄ‚àó‚Äã(Œµ)=(q‚àó‚Äã(Œµ),K‚àó‚Äã(Œµ),R‚àó‚Äã(Œµ))\pi^{\*}(\varepsilon)=(q^{\*}(\varepsilon),K^{\*}(\varepsilon),R^{\*}(\varepsilon)), recall from ([8](https://arxiv.org/html/2510.18995v1#Thmdefinition8 "Definition 8. ‚Ä£ 2.1 Introduction to Multi-level Monte Carlo estimators ‚Ä£ 2 Multi-level Monte Carlo and antithetic variance reduction ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")),

|  |  |  |
| --- | --- | --- |
|  | ‚Ñ≥‚Äã(Œ∏Œµ‚àó)=v‚Äã(œÄ‚àó‚Äã(Œµ))J‚àó‚Äã(Œµ)+Œº2‚Äã(K‚àó‚Äã(Œµ),R‚àó‚Äã(Œµ)).\mathcal{M}(\theta^{\*}\_{\varepsilon})=\frac{v(\pi^{\*}(\varepsilon))}{J^{\*}(\varepsilon)}+\mu^{2}(K^{\*}(\varepsilon),R^{\*}(\varepsilon))\,. |  |

Under assumption (VarŒ≤)(\mathrm{Var}\_{\beta}), v‚Äã(œÄ‚àó‚Äã(Œµ))‚â§v¬Ø‚Äã(œÄ‚àó‚Äã(Œµ))v(\pi^{\*}(\varepsilon))\leq\bar{v}(\pi^{\*}(\varepsilon)). Therefore by definition of J‚àó‚Äã(Œµ)J^{\*}(\varepsilon),

|  |  |  |
| --- | --- | --- |
|  | ‚Ñ≥‚Äã(Œ∏Œµ‚àó)‚â§v¬Ø‚Äã(œÄ‚àó‚Äã(Œµ))J‚àó‚Äã(Œµ)+Œº2‚Äã(œÄ0‚àó‚Äã(Œµ))=Œµ2‚àíŒº~2‚Äã(œÄ0‚àó‚Äã(Œµ))+Œº2‚Äã(œÄ0‚àó‚Äã(Œµ)).\mathcal{M}(\theta^{\*}\_{\varepsilon})\leq\frac{\bar{v}(\pi^{\*}(\varepsilon))}{J^{\*}(\varepsilon)}+\mu^{2}(\pi\_{0}^{\*}(\varepsilon))=\varepsilon^{2}-\tilde{\mu}^{2}(\pi^{\*}\_{0}(\varepsilon))+\mu^{2}(\pi^{\*}\_{0}(\varepsilon))\,. |  |

It suffice now to prove that

|  |  |  |
| --- | --- | --- |
|  | Œµ‚àí2‚Äã|Œº2‚Äã(œÄ0‚àó‚Äã(Œµ))‚àíŒº~2‚Äã(œÄ0‚àó‚Äã(Œµ))|‚Äã‚ü∂Œµ‚Üí0‚Äã0.\varepsilon^{-2}|\mu^{2}(\pi^{\*}\_{0}(\varepsilon))-\tilde{\mu}^{2}(\pi^{\*}\_{0}(\varepsilon))|\underset{\varepsilon\to 0}{\longrightarrow}0\,. |  |

We begin with the standard MLMC case. Under (WEŒ±,1)(\mathrm{WE}\_{\alpha,1}), from ([23](https://arxiv.org/html/2510.18995v1#S3.E23 "In Proposition 5 ([5], Propositon 3.4). ‚Ä£ 3.2.1 Optimization of ùêΩ ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")) and ([25](https://arxiv.org/html/2510.18995v1#S3.E25 "In Definition 15. ‚Ä£ 3.2.1 Optimization of ùêΩ ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")),

|  |  |  |
| --- | --- | --- |
|  | |Œº2‚Äã(œÄ0‚àó‚Äã(Œµ))‚àíŒº~2‚Äã(œÄ0‚àó‚Äã(Œµ))|=Œº~2‚Äã(œÄ0‚àó‚Äã(Œµ))‚Äã|(1+Œ∑1‚Äã(1K‚àó‚Äã(Œµ)‚Äã2(R‚àó‚Äã(Œµ)‚àí1)‚ÄãŒ±))2‚àí1||\mu^{2}(\pi^{\*}\_{0}(\varepsilon))-\tilde{\mu}^{2}(\pi^{\*}\_{0}(\varepsilon))|=\tilde{\mu}^{2}(\pi^{\*}\_{0}(\varepsilon))\left|\left(1+\eta\_{1}\left(\frac{1}{K^{\*}(\varepsilon)2^{(R^{\*}(\varepsilon)-1)\alpha}}\right)\right)^{2}-1\right| |  |

Then since Œº~2‚Äã(œÄ0‚àó‚Äã(Œµ))‚â§Œµ2\tilde{\mu}^{2}(\pi^{\*}\_{0}(\varepsilon))\leq\varepsilon^{2} and R‚àó‚Äã(Œµ)‚â•R‚àí‚Äã(Œµ)‚Äã‚ÜíŒµ‚Üí0+‚àûR^{\*}(\varepsilon)\geq R\_{-}(\varepsilon)\underset{\varepsilon\to 0}{\to}+\infty we have

|  |  |  |
| --- | --- | --- |
|  | Œµ‚àí2‚Äã|Œº2‚Äã(œÄ0‚àó‚Äã(Œµ))‚àíŒº~2‚Äã(œÄ0‚àó‚Äã(Œµ))|‚â§|(1+Œ∑1‚Äã(1K‚àó‚Äã(Œµ)‚Äã2(R‚àó‚Äã(Œµ)‚àí1)‚ÄãŒ±))2‚àí1|‚Äã‚ü∂Œµ‚Üí0‚Äã0.\varepsilon^{-2}\left|\mu^{2}(\pi\_{0}^{\*}(\varepsilon))-\tilde{\mu}^{2}(\pi\_{0}^{\*}(\varepsilon))\right|\leq\left|\left(1+\eta\_{1}\left(\frac{1}{K^{\*}(\varepsilon)2^{(R^{\*}(\varepsilon)-1)\alpha}}\right)\right)^{2}-1\right|\underset{\varepsilon\to 0}{\longrightarrow}0\,. |  |

which prove the result.

We continue with the ML2R case. Under (WEŒ±)(\mathrm{WE}\_{\alpha}) with supR‚àà‚Ñï‚Äãsupk‚àà‚Ñï‚Äã|Œ∑R‚Äã(k)|<+‚àû\underset{R\in\mathbb{N}}{\sup}\;\underset{k\in\mathbb{N}}{\sup}\;|\eta\_{R}(k)|<+\infty
from ([22](https://arxiv.org/html/2510.18995v1#S3.E22 "In Proposition 5 ([5], Propositon 3.4). ‚Ä£ 3.2.1 Optimization of ùêΩ ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")) and ([24](https://arxiv.org/html/2510.18995v1#S3.E24 "In Definition 15. ‚Ä£ 3.2.1 Optimization of ùêΩ ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")),

|  |  |  |
| --- | --- | --- |
|  | |Œº2‚Äã(œÄ0‚àó‚Äã(Œµ))‚àíŒº~2‚Äã(œÄ0‚àó‚Äã(Œµ))|=Œº~2‚Äã(œÄ0‚àó‚Äã(Œµ))‚Äã|(cR‚Äã(Œµ)c~R)2‚Äã(Œ∑‚Äã(œÄ0‚àó‚Äã(Œµ))+1)2‚àí1|.|\mu^{2}(\pi\_{0}^{\*}(\varepsilon))-\tilde{\mu}^{2}(\pi\_{0}^{\*}(\varepsilon))|=\tilde{\mu}^{2}(\pi\_{0}^{\*}(\varepsilon))\left|\left(\frac{c\_{R(\varepsilon)}}{\tilde{c}^{R}}\right)^{2}\left(\eta(\pi^{\*}\_{0}(\varepsilon))+1\right)^{2}-1\right|\,. |  |

Since Œº~2‚Äã(œÄ‚àó‚Äã(Œµ))‚â§Œµ2\tilde{\mu}^{2}(\pi^{\*}(\varepsilon))\leq\varepsilon^{2}, Œ∑\eta is a bounded function, c~>c~‚àû\tilde{c}>\tilde{c}\_{\infty} and R‚àó‚Äã(Œµ)‚Üí+‚àûR^{\*}(\varepsilon)\to+\infty as Œµ‚Üí0\varepsilon\to 0 we get

|  |  |  |
| --- | --- | --- |
|  | Œµ‚àí2|Œº2(œÄ0‚àó(Œµ))‚àíŒº~2(œÄ0‚àó(Œµ))|‚â§|(cR‚Äã(Œµ)c~R)2(Œ∑(œÄ0‚àó(Œµ))+1))2‚àí1|‚ü∂Œµ‚Üí00,\varepsilon^{-2}\left|\mu^{2}(\pi\_{0}^{\*}(\varepsilon))-\tilde{\mu}^{2}(\pi\_{0}^{\*}(\varepsilon))\right|\leq\left|\left(\frac{c\_{R(\varepsilon)}}{\tilde{c}^{R}}\right)^{2}\left(\eta(\pi^{\*}\_{0}(\varepsilon))+1)\right)^{2}-1\right|\underset{\varepsilon\to 0}{\longrightarrow}0\,, |  |

which complete the proof.
‚àé

### 3.3 Influence of the parameter œÑ\tau on the optimized parameters

In this section we present a comprehensive analysis of the parameter œÑ\tau and its effect on the optimized parameters of Table [2](https://arxiv.org/html/2510.18995v1#S3.T2 "Table 2 ‚Ä£ 3.2.4 Complexity analysis for the new optimized parameters ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations"). We recall that œÑ\tau is a unitless parameter representing the ratio between outer sampling cost and inner sampling cost. Its value depends on the specific context, for example in the context of insurance risk management, the outer sampling cost typically correspond to the cost of simulating the risk-factors and calibrating the Risk-Neutrals models to the realization. While the inner sampling cost typically represents the cost of simulating risk-neutral trajectories and computing the insurer profit and losses (P&L) with an Asset Liability Model (ALM). When complex Risk-Neutral models are considered, the cost of their calibration may be substantial and therefore œÑ\tau be large.

For œÑ‚â•0\tau\geq 0, we let gœÑg\_{\tau} be the objective function of the optimization problem ([33](https://arxiv.org/html/2510.18995v1#S3.E33 "In 3.2.2 Optimization of ùëû ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")), namely

|  |  |  |
| --- | --- | --- |
|  | ‚àÄœÄ0=(K,R)‚ààŒ†0,gœÑ‚Äã(œÄ0)=œï¬ØœÑ‚àó‚Äã(œÄ0)Œµ2‚àíŒº~2‚Äã(œÄ0)\forall\pi\_{0}=(K,R)\in\Pi\_{0},\quad g\_{\tau}(\pi\_{0})=\frac{\bar{\phi}\_{\tau}^{\*}(\pi\_{0})}{\varepsilon^{2}-\tilde{\mu}^{2}(\pi\_{0})} |  |

When œÑ\tau grows large, a simple computation shows that for all œÄ0=(K,R)‚ààŒ†0\pi\_{0}=(K,R)\in\Pi\_{0} such that |Œº~‚Äã(œÄ0)|<Œµ|\tilde{\mu}(\pi\_{0})|<\varepsilon

|  |  |  |
| --- | --- | --- |
|  | gœÑ‚Äã(œÄ0)‚Äã‚àºœÑ‚Üí+‚àû‚ÄãœÑ‚Äã(‚àër=1RœÉ¬Ø‚Äã(r,K))2Œµ2‚àíŒº~2‚Äã(œÄ0).g\_{\tau}(\pi\_{0})\underset{\tau\rightarrow+\infty}{\sim}\frac{\tau\left(\sum\_{r=1}^{R}\bar{\sigma}(r,K)\right)^{2}}{\varepsilon^{2}-\tilde{\mu}^{2}(\pi\_{0})}\;. |  |

This expression provides some insights on the role of œÑ\tau. As œÑ\tau increases (particularity for large values), the cost penalty per unit of the "variance-like" term (‚àër=1RœÉ¬Ø‚Äã(r,K))2\left(\sum\_{r=1}^{R}\bar{\sigma}(r,K)\right)^{2} also increases. In other words, a larger œÑ\tau places greater emphasis on minimizing the variance across the levels. This often resulting in a smaller optimal number of levels RR. At the same time, this tends to increase the optimal value for KK as the minimization shifts focus away from the cost per level. The outer sample repartition q‚àó‚Äã(Œµ)q^{\*}(\varepsilon) is also affected by œÑ\tau. Recalling that for all r‚àà{1,‚Ä¶,R‚àó‚Äã(Œµ)}r\in\{1,\dots,R^{\*}(\varepsilon)\}

|  |  |  |
| --- | --- | --- |
|  | qr‚àó‚Äã(Œµ)=œÉ¬Ø‚Äã(r,K‚àó‚Äã(Œµ))œÑ+K‚àó‚Äã(Œµ)‚Äã2r‚àí1‚ÄãŒºŒµ‚àó,q^{\*}\_{r}(\varepsilon)=\frac{\bar{\sigma}(r,K^{\*}(\varepsilon))}{\sqrt{\tau+K^{\*}(\varepsilon)2^{r-1}}\mu^{\*}\_{\varepsilon}}\,, |  |

straightforward computations shows that

|  |  |  |
| --- | --- | --- |
|  | qr‚àó‚Äã(Œµ)‚Äã‚àºœÑ‚Üí+‚àû‚ÄãœÉ¬Ø‚Äã(r,K‚àó‚Äã(Œµ))‚àëi=1R‚àó‚Äã(Œµ)œÉ¬Ø‚Äã(i,K‚àó‚Äã(Œµ)).q^{\*}\_{r}(\varepsilon)\underset{\tau\rightarrow+\infty}{\sim}\frac{\bar{\sigma}(r,K^{\*}(\varepsilon))}{\sum\_{i=1}^{R^{\*}(\varepsilon)}\bar{\sigma}(i,K^{\*}(\varepsilon))}\;. |  |

Therefore as œÑ\tau goes to infinity the outer sample repartition boils down to a measure of the variance contribution of the considered level to the overall variance, with no consideration to its cost. This is expected as when œÑ\tau gets large, the relative cost of each levels gets close to 1. This will also impact the total number of outer samples as a consequence. Recalling that,

|  |  |  |
| --- | --- | --- |
|  | J‚àó‚Äã(Œµ)=v¬Ø‚Äã(œÄ‚àó‚Äã(Œµ))Œµ2‚àíŒº~‚Äã(œÄ0‚àó‚Äã(Œµ))J^{\*}(\varepsilon)=\frac{\bar{v}(\pi^{\*}(\varepsilon))}{\varepsilon^{2}-\tilde{\mu}(\pi^{\*}\_{0}(\varepsilon))} |  |

a straightforward computations shows

|  |  |  |
| --- | --- | --- |
|  | J‚àó‚Äã(Œµ)‚Äã‚àºœÑ‚Üí+‚àû‚Äã(‚àër=1R‚àó‚Äã(Œµ)œÉ¬Ø‚Äã(r,K‚àó‚Äã(Œµ)))2Œµ2‚àíŒº~‚Äã(œÄ0‚àó‚Äã(Œµ)).J^{\*}(\varepsilon)\underset{\tau\to+\infty}{\sim}\frac{\left(\sum\_{r=1}^{R^{\*}(\varepsilon)}\bar{\sigma}(r,K^{\*}(\varepsilon))\right)^{2}}{\varepsilon^{2}-\tilde{\mu}(\pi^{\*}\_{0}(\varepsilon))}\,. |  |

Overall when œÑ\tau grows large, the control of the variance of each level will be prioritized over the control of its sampling cost leading generally to lower RR and larger KK. In turns, this leads to less overall outer samples (lower JJ) allocated more to the higher levels. These effects will be demonstrated in the numerical experiments of Section [6.3](https://arxiv.org/html/2510.18995v1#S6.SS3 "6.3 Influence of ùúè on efficiency. ‚Ä£ 6 Numerical experiments ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations").

## 4 Estimating probability of large losses

In this section, we investigate the application of MLMC estimators to the estimation of the cumulative distribution function (c.d.f.) of LL at a fixed threshold uu. Building on this, we also briefly discuss how MLMC can be used to estimate quantiles. The main point of focus is to discuss how the indicator function arising from pointwise c.d.f. estimation imposes major structural constraints on MLMC estimators.

### 4.1 The indicator function framework

Defining a threshold u‚àà‚Ñùu\in\mathbb{R}, we set f:=ùüô.‚â§uf:=\mathbbm{1}\_{.\leq u}. In that case the target to estimate reads,

|  |  |  |  |
| --- | --- | --- | --- |
|  | I=‚Ñô‚Äã(L‚â§u)=FL‚Äã(u).I=\mathbb{P}(L\leq u)\,=F\_{L}(u). |  | (41) |

corresponding to a pointwise evaluation of the c.d.f. of LL. Gordy-Juneja studied this particular problem in their seminal paper [[3](https://arxiv.org/html/2510.18995v1#bib.bib3)], developing the theory of the nested Monte Carlo estimator. Other notable work in this context can be found in the litterature, such as Giles-Haji-Ali [[4](https://arxiv.org/html/2510.18995v1#bib.bib4)] who studied a stochastic version of the standard MLMC in this context based on ideas from Broadie et. al. [[7](https://arxiv.org/html/2510.18995v1#bib.bib7)].

In this context, when the dimension of risk factors XX is d=1d=1, a result from Giorgi et. al. [[6](https://arxiv.org/html/2510.18995v1#bib.bib6)] (Proposition 5.1) gives theoretical conditions in order to fulfill the assumption (WEŒ±,R)(\mathrm{WE}\_{\alpha,R}) for R‚â•1R\geq 1 and Œ±=1\alpha=1. Based on this, for the rest of this section we will assume that (WEŒ±)(\mathrm{WE}\_{\alpha}) holds with Œ±=1\alpha=1. The indicator function ff being non-smooth, the control of the variance (VarŒ≤)(\text{Var}\_{\beta}) holds with only Œ≤=12\beta=\frac{1}{2} ; one can found theoretical conditions for this to hold in Giles-Haj-Ali [[4](https://arxiv.org/html/2510.18995v1#bib.bib4)] (Proposition 2.2) or Giorgi et. al. [[6](https://arxiv.org/html/2510.18995v1#bib.bib6)] (Proposition 5.2). Based on this, for the rest of this section we assume will that (VarŒ≤)(\mathrm{Var}\_{\beta}) holds with Œ≤=12\beta=\frac{1}{2}. Looking back at Theorems [2](https://arxiv.org/html/2510.18995v1#Thmtheorem2 "Theorem 2 ([5], Theorem 3.12, a)). ‚Ä£ 2.3 Theoretical guarantees on asymptotic complexity of MLMC estimators ‚Ä£ 2 Multi-level Monte Carlo and antithetic variance reduction ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") and [3](https://arxiv.org/html/2510.18995v1#Thmtheorem3 "Theorem 3 ([5], Theorem 3.12 b). ‚Ä£ 2.3 Theoretical guarantees on asymptotic complexity of MLMC estimators ‚Ä£ 2 Multi-level Monte Carlo and antithetic variance reduction ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations"), note that the regime Œ≤<1\beta<1, is the regime in which the ML2R estimator is comparatively much more efficient than the standard MLMC.

The indicator function framework imply important structural constraint on the problem. In this case the variance at the first level œÉ¬Ø2‚Äã(1,K)\bar{\sigma}^{2}(1,K) for K‚àà(0,+‚àû)K\in(0,+\infty) can be developed under the assumption (WE1)(\mathrm{WE}\_{1}). Expanding on the work of Gordy-Juneja [[3](https://arxiv.org/html/2510.18995v1#bib.bib3)] we obtain the following.

###### Proposition 8.

For all K‚àà(0,+‚àû)K\in(0,+\infty) and all R‚àà‚ÑïR\in\mathbb{N},

|  |  |  |
| --- | --- | --- |
|  | œÉ¬Ø2‚Äã(1,K)=I‚Äã(1‚àíI)+(1‚àí2‚ÄãI)‚Äã‚àër=1Rcr‚åàK‚åâr‚àí‚àër=1‚åäR2‚åãcr‚åàK‚åâ2‚Äãr‚àí‚àër=1R‚àí1‚àës=1‚åäR‚àír‚åãcr‚Äãcs‚åàK‚åâr+s+o‚Äã(1‚åàK‚åâR)\bar{\sigma}^{2}(1,K)=I(1-I)+(1-2I)\sum\_{r=1}^{R}\frac{c\_{r}}{\lceil K\rceil^{r}}-\sum\_{r=1}^{\lfloor\frac{R}{2}\rfloor}\frac{c\_{r}}{\lceil K\rceil^{2r}}-\sum\_{r=1}^{R-1}\sum\_{s=1}^{\lfloor R-r\rfloor}\frac{c\_{r}c\_{s}}{\lceil K\rceil^{r+s}}+o\left(\frac{1}{\lceil K\rceil^{R}}\right) |  |

###### Proof.

The proof comes mainly from the fact that in the indicator framework YK1Y\_{K\_{1}} is a Bernoulli random variable therefore,

|  |  |  |
| --- | --- | --- |
|  | œÉ¬Ø2‚Äã(1,K)=Var‚Äã[YK1]=ùîº‚Äã[YK1]‚Äã(1‚àíùîº‚Äã[YK1])\bar{\sigma}^{2}(1,K)=\mathrm{Var}[Y\_{K\_{1}}]=\mathbb{E}[Y\_{K\_{1}}](1-\mathbb{E}[Y\_{K\_{1}}]) |  |

Then using the bias expansion from (WE1)(\mathrm{WE}\_{1}) we get the desired result after collecting the negligible terms.
‚àé

Since in this framework YK1Y\_{K\_{1}} is a Bernoulli random variable we always have œÉ¬Ø2‚Äã(1,K)‚â§14\bar{\sigma}^{2}(1,K)\leq\frac{1}{4}, which gives automatically a bound œÉ¬Ø12\bar{\sigma}\_{1}^{2} in (Var12)(\mathrm{Var}\_{\frac{1}{2}}). However, in practice, this choice may be too loose. For example if I=0.5%I=0.5\% (i.e uu is the 99.5 % quantile of the conditional expectation) the principal term I‚Äã(1‚àíI)I(1-I) will be close to 0.5%0.5\% which is 50 time less than 14\frac{1}{4}. For extreme threshold estimation in practice, our recommendation is to simply take œÉ¬Ø12:=I~‚Äã(1‚àíI~)\bar{\sigma}^{2}\_{1}:=\tilde{I}(1-\tilde{I}) where I~\tilde{I} is a rough a priori estimate of II. A supplementary correction term (1‚àí2‚ÄãI~)‚Äãc1(1-2\tilde{I})c\_{1} could also be added to be more conservative.

Returning to Remark [2](https://arxiv.org/html/2510.18995v1#Thmremark2 "Remark 2. ‚Ä£ 2.2 Antithetic variance reduction ‚Ä£ 2 Multi-level Monte Carlo and antithetic variance reduction ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations"), we emphasize that in the indicator function case, for K‚àà‚ÑïK\in\mathbb{N},

|  |  |  |
| --- | --- | --- |
|  | Var‚Äã[YK]‚âàI‚Äã(1‚àíI),\mathrm{Var}[Y\_{K}]\approx I(1-I)\,, |  |

while the strong irregularity of ff yields Var‚Äã[Œî‚ÄãY2‚ÄãK]\mathrm{Var}[\Delta Y\_{2K}] of the same order of magnitude. One can verify this phenomenon empirically by comparing V1V\_{1} in (VarŒ≤)(\mathrm{Var}\_{\beta}) with I‚Äã(1‚àíI)I(1-I), which are commonly found to be of the same order of magnitude (see Table [4](https://arxiv.org/html/2510.18995v1#S6.T4 "Table 4 ‚Ä£ 6.1.2 Variance structural constants ‚Ä£ 6.1 Structural constants in the framework ‚Ä£ 6 Numerical experiments ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") in Section [6](https://arxiv.org/html/2510.18995v1#S6 "6 Numerical experiments ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")). This seems to not be the case in context where ff is smooth (see for example numerical section of [[5](https://arxiv.org/html/2510.18995v1#bib.bib5)]) where they observe values for V1V\_{1} much smaller than Var‚Äã[YK]\mathrm{Var}[Y\_{K}].

This leads to the following implication : each additional level in the MLMC estimators results in a relatively significant variance increase. Therefore, for an added level to be worthwhile, one need either KK to be large enough to offset V1V\_{1} in (Var12)(\mathrm{Var}\_{\frac{1}{2}}) (which is challenging because of the slow 12\frac{1}{2} rate of variance decay), or the bias reduction achieved must be large enough to compensate the increased variance. Accordingly, our numerical experiments reveal that in this context the ML2R estimator is much more efficient than the standard MLMC estimator, even in the non-asymptotic regime. We attribute this improvement to the strong increase in bias reduction per levels in the ML2R case due to the weights. A second observation is that the optimal number of levels RR are generally low (at most 22 or 33 for most practical computational budget), and typically the ML2R estimator only surpasses the efficiency of the traditional nested MC estimator for large computational budget (or equivalently for high required precisions).

### 4.2 Nested Monte Carlo : closed-form optimized parameters

In the particular case of the standard nested MC estimator (where we consider R‚àó‚Äã(Œµ)=1R^{\*}(\varepsilon)=1), the optimized parameters K‚àó‚Äã(Œµ)K^{\*}(\varepsilon) in Table [2](https://arxiv.org/html/2510.18995v1#S3.T2 "Table 2 ‚Ä£ 3.2.4 Complexity analysis for the new optimized parameters ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") admits an explicit expression.

###### Proposition 9.

For all Œµ>0\varepsilon>0 and all œÑ>0\tau>0 we define KœÑ+‚Äã(Œµ)K^{+}\_{\tau}(\varepsilon) the solution of the optimization problem

|  |  |  |  |
| --- | --- | --- | --- |
|  | minK‚àà‚Ñù+‚àóc12K2‚ÄãŒ±<Œµ2‚ÄãœÉ¬Ø12‚Äã(œÑ+K)Œµ2‚àíc12K2‚ÄãŒ±\underset{\begin{subarray}{c}K\in\mathbb{R}^{\*}\_{+}\\ \frac{c\_{1}^{2}}{K^{2\alpha}}<\varepsilon^{2}\end{subarray}}{\min}\;\frac{\bar{\sigma}\_{1}^{2}(\tau+K)}{\varepsilon^{2}-\frac{c\_{1}^{2}}{K^{2\alpha}}} |  | (42) |

Then

|  |  |  |
| --- | --- | --- |
|  | KœÑ‚àó‚Äã(Œµ)={|c1|Œµ‚Äã2‚Äãcos‚Å°(13‚Äãarccos‚Å°(Œµ‚ÄãœÑ|c1,Œ∑|)),Œµ<|c1,Œ∑|œÑ3‚Äãc1,Œµ=|c1,Œ∑|œÑ(|c1,Œ∑|Œµ)23‚Äã[(œÑ+(œÑ2‚àíc1,Œ∑2Œµ2)12)13+(œÑ‚àí(œÑ2‚àíc1,Œ∑2Œµ2)12)13],Œµ>|c1,Œ∑|œÑK^{\*}\_{\tau}(\varepsilon)=\begin{cases}\frac{|c\_{1}|}{\varepsilon}2\cos\left(\frac{1}{3}\arccos\left(\frac{\varepsilon\tau}{|c\_{1,\eta}|}\right)\right)&,\;\varepsilon<\frac{|c\_{1,\eta}|}{\tau}\\ 3c\_{1}&,\;\varepsilon=\frac{|c\_{1,\eta}|}{\tau}\\ \left(\frac{|c\_{1,\eta}|}{\varepsilon}\right)^{\frac{2}{3}}\left[\left(\tau+\left(\tau^{2}-\frac{c\_{1,\eta}^{2}}{\varepsilon^{2}}\right)^{\frac{1}{2}}\right)^{\frac{1}{3}}+\left(\tau-\left(\tau^{2}-\frac{c\_{1,\eta}^{2}}{\varepsilon^{2}}\right)^{\frac{1}{2}}\right)^{\frac{1}{3}}\right]&,\;\varepsilon>\frac{|c\_{1,\eta}|}{\tau}\end{cases} |  |

Furthermore, observe that the optimization problem¬†([42](https://arxiv.org/html/2510.18995v1#S4.E42 "In Proposition 9. ‚Ä£ 4.2 Nested Monte Carlo : closed-form optimized parameters ‚Ä£ 4 Estimating probability of large losses ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")) is, both in its objective function and its constraint, the continuous analogue of the discrete optimization problem¬†([35](https://arxiv.org/html/2510.18995v1#S3.E35 "In 3.2.3 Optimization of ùêæ and ùëÖ ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")) with R=1R=1. Consequently, in this case, the optimal value K‚àó‚Äã(Œµ)K^{\*}(\varepsilon) is precisely either ‚åàKœÑ+‚Äã(Œµ)‚åâ\lceil K^{+}\_{\tau}(\varepsilon)\rceil or ‚åäKœÑ+‚Äã(Œµ)‚åã\lfloor K^{+}\_{\tau}(\varepsilon)\rfloor.

###### Proof.

See Appendix [A.4](https://arxiv.org/html/2510.18995v1#A1.SS4 "A.4 Proof of Proposition 9 ‚Ä£ Appendix A Appendix ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations").
‚àé

This results can be useful to bypass the numerical optimization procedure when restricting ourselves to the nested Monte Carlo estimator. Noting that KœÑ+‚Äã(Œµ)‚Äã‚àºŒµ‚Üí+‚àû‚ÄãK‚àó‚Äã(Œµ)K^{+}\_{\tau}(\varepsilon)\underset{\varepsilon\to+\infty}{\sim}K^{\*}(\varepsilon), for Œµ\varepsilon small enough, KœÑ+‚Äã(Œµ)K^{+}\_{\tau}(\varepsilon) also provide information on the allocation for asymptotic values of œÑ\tau. Letting

|  |  |  |
| --- | --- | --- |
|  | JœÑ+‚Äã(Œµ):=œÉ¬Ø12Œµ2‚àíc12(KœÑ+‚Äã(Œµ))2J^{+}\_{\tau}(\varepsilon):=\frac{\bar{\sigma}^{2}\_{1}}{\varepsilon^{2}-\frac{c\_{1}^{2}}{(K^{+}\_{\tau}(\varepsilon))^{2}}} |  |

be the associated optimized number of outer samples. When œÑ\tau goes to 0, a straightforward computation shows that for a fixed Œµ>0\varepsilon>0

|  |  |  |
| --- | --- | --- |
|  | KœÑ+‚Äã(Œµ)‚Äã‚ü∂œÑ‚Üí0‚Äã|c1|‚Äã3ŒµK^{+}\_{\tau}(\varepsilon)\underset{\tau\rightarrow 0}{\longrightarrow}\frac{|c\_{1}|\sqrt{3}}{\varepsilon} |  |

and

|  |  |  |
| --- | --- | --- |
|  | JœÑ+‚Äã(Œµ)‚Äã‚ü∂œÑ‚Üí0‚ÄãœÉ¬Ø122‚ÄãŒµ2J^{+}\_{\tau}(\varepsilon)\underset{\tau\rightarrow 0}{\longrightarrow}\frac{\bar{\sigma}^{2}\_{1}}{2\varepsilon^{2}} |  |

which corresponds to the usual allocation for the nested Monte Carlo when œÑ=0\tau=0 (see for example Lemaire-Pag√®s [[5](https://arxiv.org/html/2510.18995v1#bib.bib5)], Proposition 2.3). Similarly, letting œÑ\tau going to infinity we see that

|  |  |  |
| --- | --- | --- |
|  | KœÑ+‚Äã(Œµ)‚Äã‚àºœÑ‚Üí+‚àû‚Äã(2‚ÄãœÑ)13‚Äã(|c1|Œµ)23K^{+}\_{\tau}(\varepsilon)\underset{\tau\rightarrow+\infty}{\sim}(2\tau)^{\frac{1}{3}}\left(\frac{|c\_{1}|}{\varepsilon}\right)^{\frac{2}{3}} |  |

and

|  |  |  |
| --- | --- | --- |
|  | JœÑ+‚Äã(Œµ)‚Äã‚àºœÑ‚Üí+‚àû‚ÄãœÉ¬Ø12Œµ2J^{+}\_{\tau}(\varepsilon)\underset{\tau\rightarrow+\infty}{\sim}\frac{\bar{\sigma}^{2}\_{1}}{\varepsilon^{2}} |  |

which shows that KœÑ+‚Äã(Œµ)K^{+}\_{\tau}(\varepsilon) goes to infinity as œÑ\tau grows and that the proportion of the computational budget allocated to inner samples grows asymptotically in œÑ13\tau^{\frac{1}{3}}.

### 4.3 From probabilty to quantiles

In this section, we briefly discuss how we can use the presented estimators to estimate a quantile of LL. For all Œ±‚àà(0,1)\alpha\in(0,1) we let

|  |  |  |
| --- | --- | --- |
|  | qŒ±=inf{v‚àà‚Ñù:FL‚Äã(v)‚â•Œ±}q\_{\alpha}=\inf\{v\in\mathbb{R}:F\_{L}(v)\geq\alpha\} |  |

Then, when LL is a continuous random variable, qŒ±q\_{\alpha} is the smallest solution to the equation

|  |  |  |
| --- | --- | --- |
|  | FL‚Äã(v)‚â•Œ±F\_{L}(v)\geq\alpha |  |

and furthermore if FLF\_{L} is strictly increasing, it is the unique solution to

|  |  |  |  |
| --- | --- | --- | --- |
|  | FL‚Äã(v)=Œ±.F\_{L}(v)=\alpha\,. |  | (43) |

For simplicity, we will assume that this is the case. A natural idea is to replace FLF\_{L} with an estimate F^\hat{F} constructed via an MLMC estimator. For Œ∏‚ààŒò\theta\in\Theta one can consider, with the notation of Definition [4](https://arxiv.org/html/2510.18995v1#Thmdefinition4 "Definition 4. ‚Ä£ 2.1 Introduction to Multi-level Monte Carlo estimators ‚Ä£ 2 Multi-level Monte Carlo and antithetic variance reduction ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations"),

|  |  |  |
| --- | --- | --- |
|  | ‚àÄv‚àà‚Ñù,F^Œ∏‚Äã(v)=1J1‚Äã‚àëj=1J1YK1j‚Äã(v)+‚àër=2RArRJr‚Äã‚àëj=1JrŒî‚ÄãYKrj‚Äã(v)\forall v\in\mathbb{R},\quad\hat{F}\_{\theta}(v)=\frac{1}{J\_{1}}\sum\_{j=1}^{J\_{1}}Y\_{K\_{1}}^{j}(v)+\sum\_{r=2}^{R}\frac{A^{R}\_{r}}{J\_{r}}\sum\_{j=1}^{J\_{r}}\Delta Y\_{K\_{r}}^{j}(v) |  |

where for all v‚àà‚Ñùv\in\mathbb{R}

|  |  |  |
| --- | --- | --- |
|  | YK1j‚Äã(v)=ùüôE^K1j‚Äã(X)‚â§vY\_{K\_{1}}^{j}(v)=\mathbbm{1}\_{\hat{E}^{j}\_{K\_{1}}(X)\leq v} |  |

and

|  |  |  |
| --- | --- | --- |
|  | Œî‚ÄãYKrj‚Äã(v)=ùüôE^Krf,j‚Äã(X)‚â§v‚àí12‚Äã(ùüôE^Krc,j‚Äã(X)‚â§v+ùüôE^Krc‚Ä≤,j‚Äã(X)‚â§v)\Delta Y\_{K\_{r}}^{j}(v)=\mathbbm{1}\_{\hat{E}^{f,j}\_{K\_{r}}(X)\leq v}-\frac{1}{2}(\mathbbm{1}\_{\hat{E}^{c,j}\_{K\_{r}}(X)\leq v}+\mathbbm{1}\_{\hat{E}^{c^{\prime},j}\_{K\_{r}}(X)\leq v}) |  |

In other words, we fix the samples and let the threshold vv vary. In the particular case where R=1R=1, it is well known that ([43](https://arxiv.org/html/2510.18995v1#S4.E43 "In 4.3 From probabilty to quantiles ‚Ä£ 4 Estimating probability of large losses ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")) admits a closed-form solution, in that case, denoting (E^K1(j))j‚àà{1,‚Ä¶,J1}(\hat{E}\_{K\_{1}}^{(j)})\_{j\in\{1,\dots,J\_{1}\}} the order statistics of the sample (E^K1j)j‚àà{1,‚Ä¶,J1}(\hat{E}\_{K\_{1}}^{j})\_{j\in\{1,\dots,J\_{1}\}} :

|  |  |  |
| --- | --- | --- |
|  | F^Œ∏‚Äã(v)=E^K1(jŒ±)\hat{F}\_{\theta}(v)=\hat{E}\_{K\_{1}}^{(j\_{\alpha})} |  |

where jŒ±=‚åàJ‚ÄãŒ±‚åâj\_{\alpha}=\lceil J\alpha\rceil. However when R‚â•2R\geq 2 no such relation holds and we must numerically solve ([43](https://arxiv.org/html/2510.18995v1#S4.E43 "In 4.3 From probabilty to quantiles ‚Ä£ 4 Estimating probability of large losses ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")). We refer to Algorithm [1](https://arxiv.org/html/2510.18995v1#alg1 "Algorithm 1 ‚Ä£ 6.2 Methodology and Benchmarking ‚Ä£ 6 Numerical experiments ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") for a practical implementation of this quantile estimator.

## 5 Risk measurement in a Life-Insurance toy model

In this section we apply MLMC estimators to a life insurance solvency monitoring context. The European Solvency II (SII) act require all European insurance companies to compute a Solvency Capital Requirement (SCR) which is defined as a 99.5% quantile on their 1-year future own-fund loss L1=O‚ÄãF0‚àíO‚ÄãF1L\_{1}=OF\_{0}-OF\_{1} where O‚ÄãF0OF\_{0} is the current (deterministic) own-fund of the company and O‚ÄãF1OF\_{1} is the (random) own-fund of the company in one year. SII requires insurance companies to evaluate their own-fund with a risk-neutral view and typically O‚ÄãF1=ùîº‚Ñö‚Äã[‚àët=2TF~t|‚Ñ±1]OF\_{1}=\mathbb{E}\_{\mathbb{Q}}[\sum\_{t=2}^{T}\tilde{F}\_{t}|\mathcal{F}\_{1}] where ‚Ñö\mathbb{Q} is a risk-neutral measure, (‚Ñ±t)(\mathcal{F}\_{t}) is a filtration representing market and actuarial informations and (F~t)2‚â§t‚â§T(\tilde{F}\_{t})\_{2\leq t\leq T} are discounted future cash-flows for the company. Since an exact simulation of O‚ÄãF1OF\_{1} is most often impossible insurance companies are in a typical nested MC framework where the conditional expectation must be approximated with a MC procedure.

As full assessment of the 99.5% quantile of L1L\_{1} is computationally demanding, insurance company can rely on a simple "Standard-Formula" approach to the computation of the SCR. It uses predefined stress scenarios to various risk factors (such as market, credit, underwriting, and operational risks) to determine individual SCRs for each risk category. These SCRs are then aggregated into a total SCR using a specified correlation matrix, which accounts for diversification effects between different risk factors (see, e.g., [[29](https://arxiv.org/html/2510.18995v1#bib.bib29)]). However, insurers seeking a more accurate and tailored assessment of their risk profile are required to compute their SCR using their own "Internal Model" for the estimation of the 99.5% quantile.

Our objective is to evaluate the complexities of the different MLMC estimators in the "Internal Model" context And will therefore focus on two metrics. The first is the evaluation of the c.d.f. of L1L\_{1} at a quantile level q99.5%q\_{99.5\%} :

|  |  |  |
| --- | --- | --- |
|  | I=‚Ñô‚Äã(L1‚â§q99.5%)=0.995I=\mathbb{P}(L\_{1}\leq q\_{99.5\%})=0.995 |  |

while the second is the evaluation of the quantile q99.5%q\_{99.5\%} itself. The study of this framework was motivated by existing literature (Alfonsi et. al. [[30](https://arxiv.org/html/2510.18995v1#bib.bib30)]) that already successfully applied MLMC estimators in the context of the Standard Formula. To do so, we introduce a simplified life-insurance (ALM) model, mimicking standard contract characteristics. The simplicity of the model allow us to get closed formulas for q99.5%q\_{99.5\%} for reference.

Our numerical experiments highlight the efficiency of the new MLMC parameters presented in Table [2](https://arxiv.org/html/2510.18995v1#S3.T2 "Table 2 ‚Ä£ 3.2.4 Complexity analysis for the new optimized parameters ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") and the impact of œÑ\tau on the calibrated parameters. We will also observe how ML2R ends up being much more efficient than MLMC and standard nested MC in estimating both type of targets.

### 5.1 Market and mortality dynamics

For life insurance contracts some of the main drivers of risks are financial market risks (stock, bond, credit ‚Ä¶) and mortality risks (longevity, increased death rates, pandemics, ‚Ä¶). In our simple model we will consider a financial market with a constant risk free rate r‚àà‚Ñùr\in\mathbb{R} and a stock index whose value is represented by the stochastic process (St)t‚â•0(S\_{t})\_{t\geq 0}. To simplify we consider a constant mortality rate p‚àà[0,1]p\in[0,1] for any individual (although the model could naturally be extended to actuarial mortality tables). Therefore in this model the only source of randomness is the evolution of the stock index. Accordingly, we define the information available to the insurance company as the filtration (‚Ñ±t)t‚â•0(\mathcal{F}\_{t})\_{t\geq 0} generated by the stochastic process (St)t‚â•0(S\_{t})\_{t\geq 0}. We assume that the stock process follows Black-Scholes dynamics with volatility œÉ>0\sigma>0 and instantaneous return Œº‚àà‚Ñù\mu\in\mathbb{R}. Namely letting (Wt‚Ñô)t‚â•0(W^{\mathbb{P}}\_{t})\_{t\geq 0} be a Brownian motion under ‚Ñô\mathbb{P} we assume that

|  |  |  |
| --- | --- | --- |
|  | ‚àÄt‚â•0,St=s0‚Äãexp‚Å°((Œº‚àíœÉ22)‚Äãt+œÉ‚ÄãWt‚Ñô)\forall t\geq 0,\;S\_{t}=s\_{0}\exp\left(\left(\mu-\frac{\sigma^{2}}{2}\right)t+\sigma W^{\mathbb{P}}\_{t}\right) |  |

We denote ‚Ñö\mathbb{Q} the unique risk-neutral probability measure in this market and (Wt‚Ñö)t‚â•0(W^{\mathbb{Q}}\_{t})\_{t\geq 0} the Brownian under ‚Ñö\mathbb{Q} such that

|  |  |  |
| --- | --- | --- |
|  | ‚àÄt‚â•0,St=s0‚Äãexp‚Å°((r‚àíœÉ22)‚Äãt+œÉ‚ÄãWt‚Ñö)\forall t\geq 0,\;S\_{t}=s\_{0}\exp\left(\left(r-\frac{\sigma^{2}}{2}\right)t+\sigma W^{\mathbb{Q}}\_{t}\right) |  |

### 5.2 Description of the life-insurance contract

We consider a life-insurance savings contract where policyholders make collectively an initial payment M‚ÄãR0‚àà‚Ñù+‚àóMR\_{0}\in\mathbb{R}^{\*}\_{+} constituting the Mathematical Reserve of the company at time t=0t=0. The insurer invests this capital in the stock index (St)t‚â•0(S\_{t})\_{t\geq 0} and offers the following financial guarantees¬†:

* ‚Ä¢

  Minimum guaranteed rate rg‚àà‚Ñùr\_{g}\in\mathbb{R} : This is the minimum rate of appreciation on the savings of the policyholders
* ‚Ä¢

  Profit-sharing rate Œ≥‚àà[0,1]\gamma\in[0,1] : Under french regulation Œ≥=85%\gamma=85\%, this corresponds to the percentage of profits on the stock investments that must be redistributed to policyholders

The savings of the policyholders and any current additional interest (from minimum guaranteed rate or profit-sharing mechanism) are paid once at termination of the contract. The termination is either at a fixed maturity T>0T>0 of the contract or earlier if the policyholder dies. We assume that the contract is monitored yearly, and payment are made at the end of each year t=1,‚Ä¶,Tt=1,\dots,T.

At inception time t=0t=0, the insurer invests M‚ÄãR0MR\_{0} in the stock index which gets him œï0=M‚ÄãR0s0\phi\_{0}=\frac{MR\_{0}}{s\_{0}} shares of the stock. Then each year 1‚â§t‚â§T‚àí11\leq t\leq T-1, the following steps are carried :

1. 1.

   The value of the policyholders‚Äô savings M‚ÄãRt‚àí1MR\_{t-1} are appreciated at a rate

   |  |  |  |
   | --- | --- | --- |
   |  | rs‚Äã(t):=max‚Å°(rg,Œ≥‚Äãln‚Å°(Rt))=rg+(Œ≥‚Äãln‚Å°(Rt)‚àírg)+r\_{s}(t):=\max(r\_{g},\gamma\ln(R\_{t}))=r\_{g}+(\gamma\ln(R\_{t})-r\_{g})^{+} |  |

   where Rt=StSt‚àí1R\_{t}=\frac{S\_{t}}{S\_{t-1}}, t‚àà{1,‚Ä¶,T}t\in\{1,\dots,T\}, satisfying the financial guarantees of the minimum guaranteed rate and the profit sharing mechanism. The appreciated value is then

   |  |  |  |  |
   | --- | --- | --- | --- |
   |  | M‚ÄãR~t:=M‚ÄãRt‚àí1‚Äã(1+rs‚Äã(t))\widetilde{MR}\_{t}:=MR\_{t-1}(1+r\_{s}(t)) |  | (44) |
2. 2.

   A proportion pp of policyholders dies during the year, triggering a termination of their contract. The insurer therefore must pay

   |  |  |  |
   | --- | --- | --- |
   |  | M‚ÄãR~t‚Äãp\widetilde{MR}\_{t}p |  |

   to the deceased policyholders. This requires to sell

   |  |  |  |
   | --- | --- | --- |
   |  | Œî‚Äãœït:=‚àíM‚ÄãR~t‚ÄãpSt\Delta\phi\_{t}:=\frac{-\widetilde{MR}\_{t}p}{S\_{t}} |  |

   share of the stock to provide the liquidity. The insurance company is left with œït:=œït‚àí1+Œî‚Äãœït\phi\_{t}:=\phi\_{t-1}+\Delta\phi\_{t} shares of the stock.
3. 3.

   The value of the remaining policyholders‚Äô savings is then updated by

   |  |  |  |  |
   | --- | --- | --- | --- |
   |  | M‚ÄãRt:=M‚ÄãR~t‚Äã(1‚àíp)MR\_{t}:=\widetilde{MR}\_{t}(1-p) |  | (45) |

At termination date t=Tt=T, the same steps are carried but with p=1p=1 (as all policyholders terminates their contract regardless if they died or not). The shareholders of the insurance company are then left with œïT‚ÄãST‚àà‚Ñù\phi\_{T}S\_{T}\in\mathbb{R} from the liquidation of the remaining asset shares.

### 5.3 Solvency monitoring of the portfolio

An insurance company is said to be solvent if its Own Funds are positive ; here the OF is defined as the discounted risk-neutral expectation of the future cash-flows generated by the contract conditionally to current information :

|  |  |  |
| --- | --- | --- |
|  | ‚àÄ0‚â§t‚â§T,O‚ÄãFt:=ùîº‚Ñö‚Äã[e‚àír‚Äã(T‚àít)‚ÄãœïT‚ÄãST|‚Ñ±t]\forall 0\leq t\leq T,\quad OF\_{t}:=\mathbb{E}\_{\mathbb{Q}}[e^{-r(T-t)}\phi\_{T}S\_{T}|\mathcal{F}\_{t}] |  |

The OF in the Solvency II framework represents the difference between the market value of the assets of the insurance company and the value a rationale agent would consent to bear the responsibility of the liabilities of the company.

At time t=0,‚Ä¶,Tt=0,\dots,T, the SCR can then be defined as a 99.5%99.5\% quantile on the distribution of ‚àíO‚ÄãFt+1-OF\_{t+1} conditionally to ‚Ñ±t\mathcal{F}\_{t} :

|  |  |  |
| --- | --- | --- |
|  | S‚ÄãC‚ÄãRt:=O‚ÄãFt+inf{x‚àà‚Ñù:‚Ñô‚Äã(‚àíO‚ÄãFt+1‚â§x|‚Ñ±t)‚â•99.5%}=q99.5%‚Äã(Lt|‚Ñ±t)SCR\_{t}:=OF\_{t}+\inf\{x\in\mathbb{R}:\mathbb{P}(-OF\_{t+1}\leq x|\mathcal{F}\_{t})\geq 99.5\%\}=q\_{99.5\%}(L\_{t}|\mathcal{F}\_{t}) |  |

where Lt=O‚ÄãFt‚àíO‚ÄãFt+1L\_{t}=OF\_{t}-OF\_{t+1} is the loss in Own Funds over one year in the future. The following proposition show that, in our model, O‚ÄãFtOF\_{t} can be explicitly computed based on the state variables œït\phi\_{t}, M‚ÄãRtMR\_{t}, StS\_{t}, as well as on the trajectory (s0,S1,‚Ä¶,St)(s\_{0},S\_{1},\dots,S\_{t}). The proof of this proposition is based on technical Lemmas presented in Appendix [A.5](https://arxiv.org/html/2510.18995v1#A1.SS5 "A.5 Technical Lemma for Proposition 10 ‚Ä£ Appendix A Appendix ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations"). For convenience we first introduce the following notations :

###### Definition 18.

Let (du)u‚àà{1,‚Ä¶,T}(d\_{u})\_{u\in\{1,\dots,T\}} be the sequence valued in ‚Ñù\mathbb{R} defined by,

|  |  |  |
| --- | --- | --- |
|  | du:={pu‚àà{1,‚Ä¶,T‚àí1}1u=Td\_{u}:=\begin{cases}p&u\in\{1,\dots,T-1\}\\ 1&u=T\end{cases} |  |

and call (du)u‚àà{1,‚Ä¶,T}(d\_{u})\_{u\in\{1,\dots,T\}} the sequence of exit rates. Further we define, z:=1+rg+Œ≥‚ÄãœÉ‚Äã(Œ¶‚Ä≤‚Äã(d)+d‚ÄãŒ¶‚Äã(d))z:=1+r\_{g}+\gamma\sigma(\Phi^{{}^{\prime}}(d)+d\Phi(d)) where d:=r‚àíœÉ22‚àírgŒ≥œÉd:=\frac{r-\frac{\sigma^{2}}{2}-\frac{r\_{g}}{\gamma}}{\sigma}.

###### Proposition 10.

Let t‚àà{0,‚Ä¶,T}t\in\{0,\dots,T\},

|  |  |  |
| --- | --- | --- |
|  | O‚ÄãFt=ùîº‚Ñö‚Äã[e‚àír‚Äã(T‚àít)‚ÄãœïT‚ÄãST|‚Ñ±t]=œït‚ÄãSt‚àíM‚ÄãRt‚Äã[p‚Äã‚àëu=tT‚àí1e‚àír‚Äã(u‚àít)‚Äã(1‚àíp)u‚àít‚àí1‚Äãzu‚àít+e‚àír‚Äã(T‚àít)‚Äã(1‚àíp)T‚àít‚àí1‚ÄãzT‚àít]OF\_{t}=\mathbb{E}\_{\mathbb{Q}}[e^{-r(T-t)}\phi\_{T}S\_{T}|\mathcal{F}\_{t}]=\phi\_{t}S\_{t}-MR\_{t}\left[p\sum\_{u=t}^{T-1}e^{-r(u-t)}(1-p)^{u-t-1}z^{u-t}+e^{-r(T-t)}(1-p)^{T-t-1}z^{T-t}\right] |  |

In particular, letting œàt\psi\_{t} be the function from ‚Ñùt+1\mathbb{R}^{t+1} to ‚Ñù\mathbb{R} defined by,

|  |  |  |
| --- | --- | --- |
|  | ‚àÄx=(x0,x1,‚Ä¶,xt)‚àà(0,+‚àû)t+1,œàt‚Äã(x)=gt‚Äã(x)‚Äãxt‚àíft‚Äã(x)‚Äã[p‚Äã‚àëu=tT‚àí1e‚àír‚Äã(u‚àít)‚Äã(1‚àíp)u‚àít‚àí1‚Äãzu‚àít+e‚àír‚Äã(T‚àít)‚Äã(1‚àíp)T‚àít‚àí1‚ÄãzT‚àít],\forall x=(x\_{0},x\_{1},\dots,x\_{t})\in(0,+\infty)^{t+1},\;\psi\_{t}(x)=g\_{t}(x)x\_{t}-f\_{t}(x)\left[p\sum\_{u=t}^{T-1}e^{-r(u-t)}(1-p)^{u-t-1}z^{u-t}+e^{-r(T-t)}(1-p)^{T-t-1}z^{T-t}\right]\,, |  |

where

|  |  |  |
| --- | --- | --- |
|  | ‚àÄx=(x0,‚Ä¶,xt)‚àà(0,+‚àû)t+1,ft‚Äã(x)=M‚ÄãR0‚Äã‚àèu=1t(1‚àídu)‚Äã(1+œÅ‚Äã(xu‚àí1,xu)),\forall x=(x\_{0},\dots,x\_{t})\in(0,+\infty)^{t+1},\quad f\_{t}(x)=MR\_{0}\prod\_{u=1}^{t}(1-d\_{u})(1+\rho(x\_{u-1},x\_{u}))\,, |  |

|  |  |  |
| --- | --- | --- |
|  | ‚àÄx=(x0,x1,‚Ä¶,xt)‚àà(0,+‚àû)t+1,gt‚Äã(x)=œï0‚àíM‚ÄãR0‚Äã‚àëi=1tdixi‚Äã‚àèj=u+1i‚àí1(1‚àídj)‚Äã‚àèj=u+1i(1+œÅ‚Äã(xj‚àí1,xj)),\forall x=(x\_{0},x\_{1},\dots,x\_{t})\in(0,+\infty)^{t+1},\quad g\_{t}(x)=\phi\_{0}-MR\_{0}\sum\_{i=1}^{t}\frac{d\_{i}}{x\_{i}}\prod\_{j=u+1}^{i-1}(1-d\_{j})\prod\_{j=u+1}^{i}(1+\rho(x\_{j-1},x\_{j}))\,, |  |

and

|  |  |  |
| --- | --- | --- |
|  | ‚àÄ(x,x‚Ä≤)‚àà(0,+‚àû)2,œÅ‚Äã(x,x‚Ä≤)=max‚Å°(rg,Œ≥‚Äãln‚Å°(x‚Ä≤x)),\forall(x,x^{\prime})\in(0,+\infty)^{2},\quad\rho(x,x^{\prime})=\max\left(r\_{g},\gamma\ln\left(\frac{x^{\prime}}{x}\right)\right)\,, |  |

then ùîº‚Ñö‚Äã[e‚àír‚Äã(T‚àít)‚ÄãœïT‚ÄãST|‚Ñ±t]=œàt‚Äã(s0,S1,‚Ä¶,St)\mathbb{E}\_{\mathbb{Q}}[e^{-r(T-t)}\phi\_{T}S\_{T}|\mathcal{F}\_{t}]=\psi\_{t}(s\_{0},S\_{1},\dots,S\_{t}) ‚Äâ.

###### Proof.

For all t‚àà{0,‚Ä¶,T}t\in\{0,\dots,T\}, from Lemma [12](https://arxiv.org/html/2510.18995v1#Thmlemma12 "Lemma 12. ‚Ä£ A.5 Technical Lemma for Proposition 10 ‚Ä£ Appendix A Appendix ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations"),

|  |  |  |
| --- | --- | --- |
|  | œïT‚ÄãSt=œït‚ÄãST‚àíM‚ÄãRt‚Äã‚àëi=t+1Tdi‚ÄãSTSi‚Äã‚àèj=t+1i‚àí1(1‚àídj)‚Äã‚àèj=t+1i(1+œÅ‚Äã(Sj‚àí1,Sj))\phi\_{T}S\_{t}=\phi\_{t}S\_{T}-MR\_{t}\sum\_{i=t+1}^{T}\frac{d\_{i}S\_{T}}{S\_{i}}\prod\_{j=t+1}^{i-1}(1-d\_{j})\prod\_{j=t+1}^{i}(1+\rho(S\_{j-1},S\_{j})) |  |

Now since œït\phi\_{t} and M‚ÄãRtMR\_{t} are ‚Ñ±t\mathcal{F}\_{t}-measurable,

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Ñö‚Äã[œïT‚ÄãST|‚Ñ±t]=œït‚Äãùîº‚Ñö‚Äã[ST|‚Ñ±t]‚àíM‚ÄãRt‚Äã‚àëi=t+1Tdi‚Äã‚àèj=t+1i‚àí1(1‚àídj)‚Äãùîº‚Ñö‚Äã[STSi‚Äã‚àèj=t+1i(1+œÅ‚Äã(Sj‚àí1,Sj))|‚Ñ±t]\mathbb{E}\_{\mathbb{Q}}[\phi\_{T}S\_{T}|\mathcal{F}\_{t}]=\phi\_{t}\mathbb{E}\_{\mathbb{Q}}[S\_{T}|\mathcal{F}\_{t}]-MR\_{t}\sum\_{i=t+1}^{T}d\_{i}\prod\_{j=t+1}^{i-1}(1-d\_{j})\mathbb{E}\_{\mathbb{Q}}\left[\frac{S\_{T}}{S\_{i}}\prod\_{j=t+1}^{i}(1+\rho(S\_{j-1},S\_{j}))|\mathcal{F}\_{t}\right] |  |

A well-known fact in the Black-Scholes model is that

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Ñö‚Äã[ST|‚Ñ±t]=St‚Äãer‚Äã(T‚àít).\mathbb{E}\_{\mathbb{Q}}[S\_{T}|\mathcal{F}\_{t}]=S\_{t}e^{r(T-t)}\,. |  |

Furthermore, for all i‚àà{t+1,‚Ä¶,T}i\in\{t+1,\dots,T\}, StSi‚Äã‚àèj=t+1i(1+œÅ‚Äã(Sj‚àí1,Sj))\frac{S\_{t}}{S\_{i}}\prod\_{j=t+1}^{i}(1+\rho(S\_{j-1},S\_{j})) is independent from ‚Ñ±t\mathcal{F}\_{t}, therefore

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùîº‚Ñö‚Äã[STSi‚Äã‚àèj=t+1i(1+œÅ‚Äã(Sj‚àí1,Sj))|‚Ñ±t]\displaystyle\mathbb{E}\_{\mathbb{Q}}\left[\frac{S\_{T}}{S\_{i}}\prod\_{j=t+1}^{i}(1+\rho(S\_{j-1},S\_{j}))|\mathcal{F}\_{t}\right] | =ùîº‚Ñö‚Äã[STSi‚Äã‚àèj=t+1i(1+œÅ‚Äã(Sj‚àí1,Sj))]\displaystyle=\mathbb{E}\_{\mathbb{Q}}\left[\frac{S\_{T}}{S\_{i}}\prod\_{j=t+1}^{i}(1+\rho(S\_{j-1},S\_{j}))\right] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =ùîº‚Ñö‚Äã[STSi]‚Äã‚àèj=t+1iùîº‚Äã[1+œÅ‚Äã(Sj‚àí1,Sj)]\displaystyle=\mathbb{E}\_{\mathbb{Q}}\left[\frac{S\_{T}}{S\_{i}}\right]\prod\_{j=t+1}^{i}\mathbb{E}[1+\rho(S\_{j-1},S\_{j})] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =eT‚àíi‚Äãzi‚àít\displaystyle=e^{T-i}z^{i-t} |  |

where the second equality comes from the independence of the random variable, and the last from Lemma [13](https://arxiv.org/html/2510.18995v1#Thmlemma13 "Lemma 13. ‚Ä£ A.5 Technical Lemma for Proposition 10 ‚Ä£ Appendix A Appendix ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations"). In turns we get

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Ñö‚Äã[e‚àír‚Äã(T‚àít)‚ÄãœïT‚ÄãST|‚Ñ±t]=œït‚ÄãSt‚àíM‚ÄãRt‚Äã‚àëi=t+1Tdi‚Äãe(i‚àít)‚Äãzi‚àít‚Äã‚àèj=t+1i‚àí1(1‚àídj)\mathbb{E}\_{\mathbb{Q}}[e^{-r(T-t)}\phi\_{T}S\_{T}|\mathcal{F}\_{t}]=\phi\_{t}S\_{t}-MR\_{t}\sum\_{i=t+1}^{T}d\_{i}e^{(i-t)}z^{i-t}\prod\_{j=t+1}^{i-1}(1-d\_{j}) |  |

Using that di=pd\_{i}=p for i‚àà{0,‚Ä¶,T‚àí1}i\in\{0,\dots,T-1\} and dT=1d\_{T}=1 then give the first claim. The second claim is a direct application of the first claim with Lemma [11](https://arxiv.org/html/2510.18995v1#Thmlemma11 "Lemma 11. ‚Ä£ A.5 Technical Lemma for Proposition 10 ‚Ä£ Appendix A Appendix ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") and Lemma [12](https://arxiv.org/html/2510.18995v1#Thmlemma12 "Lemma 12. ‚Ä£ A.5 Technical Lemma for Proposition 10 ‚Ä£ Appendix A Appendix ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations").
‚àé

From now on, to simplify we will set ourselves in the case t=0t=0. Notice then that L1=œà0‚Äã(s0)‚àíœà1‚Äã(s0,S1)L\_{1}=\psi\_{0}(s\_{0})-\psi\_{1}(s\_{0},S\_{1}) and recalling that s0s\_{0} is deterministic we let œà\psi be the (deterministic) function from (0,+‚àû)(0,+\infty) to ‚Ñù\mathbb{R} defined by,

|  |  |  |
| --- | --- | --- |
|  | ‚àÄx‚àà(0,+‚àû),œà‚Äã(x):=œà0‚Äã(s0)‚àíœà1‚Äã(s0,x),\forall x\in(0,+\infty),\quad\psi(x):=\psi\_{0}(s\_{0})-\psi\_{1}(s\_{0},x)\,, |  |

then œà\psi is such that L1=œà‚Äã(S1)‚Äãa.sL\_{1}=\psi(S\_{1})\;a.s.

To relate this framework to the context of the notation of the nested MC framework used in previous sections, let XX be a random variable whose distribution under ‚Ñô\mathbb{P} is the same as that of S1S\_{1} under ‚Ñô\mathbb{P}. Define U=(U1,‚Ä¶,UT‚àí1)U=(U\_{1},\dots,U\_{T-1}) as a random vector, independent of XX, whose distribution under ‚Ñô\mathbb{P} matches that of the independent Gaussian increments (W2‚Ñö‚àíW1‚Ñö,‚Ä¶,WT‚Ñö‚àíWT‚àí1‚Ñö)(W\_{2}^{\mathbb{Q}}-W^{\mathbb{Q}}\_{1},\dots,W\_{T}^{\mathbb{Q}}-W\_{T-1}^{\mathbb{Q}}) under ‚Ñö\mathbb{Q}. For all t>0t>0, we let œïx,t\phi\_{x,t} be the functional mapping tt Gaussian increments to the value of the stock at time tt in the Risk-Neutral Black-Scholes model, starting from xx at time 0. Now, define the function FF for x‚àà(0,+‚àû)x\in(0,+\infty), u=(u1,‚Ä¶‚ÄãuT‚àí1)‚àà(0,+‚àû)T‚àí1u=(u\_{1},\dots u\_{T-1})\in(0,+\infty)^{T-1} by

|  |  |  |
| --- | --- | --- |
|  | F‚Äã(x,u)=œà0‚Äã(s0)‚àíe‚àír‚Äã(T‚àít)‚ÄãgT‚Äã(s0,x,œïx,1‚Äã(u1),‚Ä¶,œïx,T‚àí1‚Äã(u1,‚Ä¶,uT‚àí1))‚Äãœïx,T‚àí1‚Äã(u1,‚Ä¶,uT‚àí1)F(x,u)=\psi\_{0}(s\_{0})-e^{-r(T-t)}g\_{T}(s\_{0},x,\phi\_{x,1}(u\_{1}),\dots,\phi\_{x,T-1}(u\_{1},\dots,u\_{T-1}))\phi\_{x,T-1}(u\_{1},\dots,u\_{T-1}) |  |

where gTg\_{T} (as defined in [55](https://arxiv.org/html/2510.18995v1#A1.E55 "In Lemma 12. ‚Ä£ A.5 Technical Lemma for Proposition 10 ‚Ä£ Appendix A Appendix ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")) maps stock price trajectories to the terminal number of stock shares. With these definitions, we have L1=ùîº‚Äã[F‚Äã(X,U)|X]L\_{1}=\mathbb{E}[F(X,U)|X] where XX and UU are independent. The propositions below give us a closed formula for S‚ÄãC‚ÄãR0SCR\_{0} that we will use as reference value for our numerical experiments.

###### Proposition 11.

The function œà\psi is decreasing on (0,x1](0,x\_{1}] and [x2,+‚àû)[x\_{2},+\infty) where

|  |  |  |
| --- | --- | --- |
|  | {x1:=s0‚ÄãergŒ≥x2:=s0‚ÄãŒ≥‚Äã((1‚àíp)‚Äã[p‚Äã‚àët=1T‚àí2e‚àír‚Äãt‚Äã(1‚àíp)t‚àí1‚Äãzt+e‚àír‚Äã(T‚àí1)‚Äã(1‚àíp)T‚àí2‚ÄãzT‚àí1]+p)\begin{cases}x\_{1}:=s\_{0}e^{\frac{r\_{g}}{\gamma}}\\ x\_{2}:=s\_{0}\gamma\left((1-p)\left[p\sum\_{t=1}^{T-2}e^{-rt}(1-p)^{t-1}z^{t}+e^{-r(T-1)}(1-p)^{T-2}z^{T-1}\right]+p\right)\end{cases} |  |

In particular if x1‚â•x2x\_{1}\geq x\_{2} the value function œà\psi is non-increasing on ‚Ñù+‚àó\mathbb{R}^{\*}\_{+}.

###### Proof.

A straightforward calculation shows that œà\psi is differentiable on (0,x1)(0,x\_{1}) and (x1,+‚àû)(x\_{1},+\infty) with

|  |  |  |
| --- | --- | --- |
|  | œà‚Ä≤‚Äã(x)={‚àíœï0x<x1M‚ÄãR0‚ÄãŒ≥x‚Äã[p‚Äã‚àët=1T‚àí2e‚àír‚Äãt‚Äã(1‚àíp)t‚àí1‚Äãzt+e‚àír‚Äã(T‚àí1)‚Äã(1‚àíp)T‚àí2‚ÄãzT‚àí1]‚àíœï0x1<x\psi^{\prime}(x)=\begin{cases}-\phi\_{0}&x<x\_{1}\\ MR\_{0}\frac{\gamma}{x}\left[p\sum\_{t=1}^{T-2}e^{-rt}(1-p)^{t-1}z^{t}+e^{-r(T-1)}(1-p)^{T-2}z^{T-1}\right]-\phi\_{0}&x\_{1}<x\\ \end{cases} |  |

Therefore when x<x1x<x\_{1}, and when x2‚â•xx\_{2}\geq x we have œà‚Ä≤‚Äã(x)‚â§0\psi^{\prime}(x)\leq 0, noting that œà\psi is continuous at x1x\_{1} we get the result.
‚àé

###### Proposition 12.

With the notations of Proposition [11](https://arxiv.org/html/2510.18995v1#Thmproposition11 "Proposition 11. ‚Ä£ 5.3 Solvency monitoring of the portfolio ‚Ä£ 5 Risk measurement in a Life-Insurance toy model ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations"), if x1‚â•x2x\_{1}\geq x\_{2} we have

|  |  |  |
| --- | --- | --- |
|  | S‚ÄãC‚ÄãR0=q1‚àíŒ±‚Äã(œà‚Äã(S1))=œà‚Äã(s0‚Äãexp‚Å°(Œº‚àíœÉ22+œÉ‚ÄãŒ¶‚àí1‚Äã(Œ±)))SCR\_{0}=q\_{1-\alpha}(\psi(S\_{1}))=\psi\left(s\_{0}\exp\left(\mu-\frac{\sigma^{2}}{2}+\sigma\Phi^{-1}(\alpha)\right)\right) |  |

with Œ±=0.5%\alpha=0.5\%

###### Proof.

This is an immediate consequence of the non-increasing property of x‚Ü¶œà‚Äã(x)x\mapsto\psi(x) and the non-decreasing property of u‚Ü¶s0‚Äãexp‚Å°(Œº‚àíœÉ22+œÉ‚Äãu)u\mapsto s\_{0}\exp\left(\mu-\frac{\sigma^{2}}{2}+\sigma u\right).
‚àé

In our numerical experiments, we will use the parameters reported in Table [3](https://arxiv.org/html/2510.18995v1#S6.T3 "Table 3 ‚Ä£ 6.1.2 Variance structural constants ‚Ä£ 6.1 Structural constants in the framework ‚Ä£ 6 Numerical experiments ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") which yields q99.5%‚âà252.76q\_{99.5\%}\approx 252.76.

In Figure [1](https://arxiv.org/html/2510.18995v1#S5.F1 "Figure 1 ‚Ä£ 5.3 Solvency monitoring of the portfolio ‚Ä£ 5 Risk measurement in a Life-Insurance toy model ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") a) we show an histogram of exact sampling of L1L\_{1} and in Figure [1](https://arxiv.org/html/2510.18995v1#S5.F1 "Figure 1 ‚Ä£ 5.3 Solvency monitoring of the portfolio ‚Ä£ 5 Risk measurement in a Life-Insurance toy model ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") b) we represent œà\psi. We observe that the loss grows as the stock perform poorly during the first year, with a comparatively steep increase when going below the 100 mark. This result in no loss in own-fund for most of scenarios and a comparatively severe loss in extreme downward stock scenarios.

![Refer to caption](images/of_loss_distrib_plot.png)


(a) Density estimation of L1L\_{1}

![Refer to caption](images/own_fund_loss_cond.png)


(b) œà‚Äã(x)\psi(x) as function of xx

Figure 1: Characteristics of the own-fund loss

## 6 Numerical experiments

### 6.1 Structural constants in the framework

In this section we conduct a pre-processing where we estimate the structural constants of the framework. Here we do not restrict ourselves on the simulation budget used to estimate these constants. In practice only a fraction of the total computational budget should be used.

#### 6.1.1 Bias structural constants

Following (WE1)(\mathrm{WE}\_{1}), we begin with the the structural constant c1c\_{1}, based on the relation¬†:

|  |  |  |
| --- | --- | --- |
|  | ‚àÄK‚àà‚Ñï,ùîº‚Äã[Œî‚ÄãY2‚ÄãKA]=ùîº‚Äã[Y2‚ÄãK‚àíYK]‚âà‚àíc12‚ÄãK\forall K\in\mathbb{N},\quad\mathbb{E}[\Delta Y^{A}\_{2K}]=\mathbb{E}[Y\_{2K}-Y\_{K}]\approx-\frac{c\_{1}}{2K} |  |

and with the structural constant c2c\_{2} based on the relation :

|  |  |  |
| --- | --- | --- |
|  | ‚àÄK‚àà‚Ñï,ùîº‚Äã[2‚ÄãY4‚ÄãK‚àí3‚ÄãY2‚ÄãK+YK]‚âà6‚Äãc2(4‚ÄãK)2\forall K\in\mathbb{N},\quad\mathbb{E}[2Y\_{4K}-3Y\_{2K}+Y\_{K}]\approx\frac{6c\_{2}}{(4K)^{2}} |  |

In our setting choosing c1=0.025c\_{1}=0.025 and c2=0.05c\_{2}=0.05 seems empirically appropriate (see Figure [2](https://arxiv.org/html/2510.18995v1#S6.F2 "Figure 2 ‚Ä£ 6.1.2 Variance structural constants ‚Ä£ 6.1 Structural constants in the framework ‚Ä£ 6 Numerical experiments ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") a) and b) ). Going back to Remark [5](https://arxiv.org/html/2510.18995v1#Thmremark5 "Remark 5. ‚Ä£ 3.2.3 Optimization of ùêæ and ùëÖ ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations"), we make the assumption that the sequence of coefficient (cr)r‚àà‚Ñï(c\_{r})\_{r\in\mathbb{N}} follows the parametric form cr=c1‚Äãar‚àí1c\_{r}=c\_{1}a^{r-1} where we choose a=c2c1=2a=\frac{c\_{2}}{c\_{1}}=2. In particular this leads to c~=a=2\tilde{c}=a=2. Notice however that c2c\_{2} is very hard to estimate (as shown by the large confidence interval in Figure [2](https://arxiv.org/html/2510.18995v1#S6.F2 "Figure 2 ‚Ä£ 6.1.2 Variance structural constants ‚Ä£ 6.1 Structural constants in the framework ‚Ä£ 6 Numerical experiments ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") b) ) and is mostly infeasible in practice, therefore one must often blindly choose aa. Our recommendation is to take typically aa to be 22 or 33.

#### 6.1.2 Variance structural constants

The structural constant œÉ¬Ø12\bar{\sigma}\_{1}^{2} is choose based on Proposition [8](https://arxiv.org/html/2510.18995v1#Thmproposition8 "Proposition 8. ‚Ä£ 4.1 The indicator function framework ‚Ä£ 4 Estimating probability of large losses ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations"), namely we consider œÉ¬Ø12‚âàI‚Äã(1‚àíI)‚âà0.5%\bar{\sigma}\_{1}^{2}\approx I(1-I)\approx 0.5\%. Following (Var1/2)(\mathrm{Var}\_{1/2}) the structural constant V1V\_{1} is based on the relation:

|  |  |  |
| --- | --- | --- |
|  | ‚àÄK‚àà‚Ñï,Var‚Äã[Œî‚ÄãY2‚ÄãK]‚âàV12‚ÄãK\forall K\in\mathbb{N},\quad\text{Var}[\Delta Y\_{2K}]\approx\frac{V\_{1}}{\sqrt{2K}} |  |

Note that this constant depend on whether we consider the antithetic version of the level, Œî‚ÄãY2‚ÄãKA\Delta Y\_{2K}^{A} or the non-antithetic version Œî‚ÄãYKrS\Delta Y^{S}\_{K\_{r}}. In Figure [2](https://arxiv.org/html/2510.18995v1#S6.F2 "Figure 2 ‚Ä£ 6.1.2 Variance structural constants ‚Ä£ 6.1 Structural constants in the framework ‚Ä£ 6 Numerical experiments ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") c) we plot the estimated relation in the framework for both the antithetic level and non antithetic level. In the end we see that V1A=0.01V^{A}\_{1}=0.01 with antithetic sampling and V1S=0.02V^{S}\_{1}=0.02 without are appropriate. As an illustration of Theorem [1](https://arxiv.org/html/2510.18995v1#Thmtheorem1 "Theorem 1. ‚Ä£ 2.2 Antithetic variance reduction ‚Ä£ 2 Multi-level Monte Carlo and antithetic variance reduction ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations"), the antithetic sampling divide roughly by 2 the variance on each levels despite the fact that we are using indicator function payoff.

In Table [4](https://arxiv.org/html/2510.18995v1#S6.T4 "Table 4 ‚Ä£ 6.1.2 Variance structural constants ‚Ä£ 6.1 Structural constants in the framework ‚Ä£ 6 Numerical experiments ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") we summarize the structural parameters used in the experimentation. Notice how œÉ¬Ø12\bar{\sigma}\_{1}^{2} and V1V\_{1} are of the same order of magnitude, this is a common phenomenon in indicator function ff framework. This entail that each additional levels in MLMC estimators are (comparatively with the variance on the first level) adding a significant amount of variance. Therefore qualitatively, we expect to use a rather low number of levels.

###### Remark 6.

Since the structural constants are estimated using f‚Äã(x)=ùüôx‚â§q99.5%f(x)=\mathbbm{1}\_{x\leq q\_{99.5\%}}, if we are interested in estimating the quantile in practice a rough a-priori value for q99.5%q\_{99.5\%} is required in order to estimate the strctural constants.

| Parameter | Value |
| --- | --- |
| Risk free rate rr | 5%5\% |
| Stock vol œÉ\sigma | 15%15\% |
| Stock drift Œº\mu | 8%8\% |
| Stock initial value s0s\_{0} | 100100 |
| Horizon TT | 1010 years |
| Minimum guaranteed rate rgr\_{g} | 0%0\% |
| Profit-sharing rate Œ≥\gamma | 85%85\% |
| Death rate pp | 2%2\% |
| Initial Mathematical Reserve M‚ÄãR0MR\_{0} | 1000 |

Table 3: Parameters for the numerical experiments



![Refer to caption](images/c1_plot.png)


(a) First order structural bias plot. In blue the empirical estimation of |ùîº‚Äã(Y2‚ÄãK‚àíYK)||\mathbb{E}(Y\_{2K}-Y\_{K})|. In faded blue the 95% confidence interval. In orange the proxy relation for c1=0.025c\_{1}=0.025.

![Refer to caption](images/c2_plot.png)


(b) Second order structural bias plot. In blue the empirical estimation of |ùîº‚Äã(2‚ÄãY4‚ÄãK‚àí3‚ÄãY2‚ÄãK+YK)||\mathbb{E}(2Y\_{4K}-3Y\_{2K}+Y\_{K})|. In faded blue the 95% confidence interval. In orange the proxy relation for c2=0.05c\_{2}=0.05.

![Refer to caption](images/V1_plot.png)


(c) Variance structural bias plot. In solid blue the empirical estimation of Var‚Äã(Œî‚ÄãY2‚ÄãKA)\mathrm{Var}(\Delta Y^{A}\_{2K}) and in dashed blue the proxy relation for V1A=0.01V\_{1}^{A}=0.01. In solid orange the empirical estimation of Var‚Äã(Œî‚ÄãY2‚ÄãKS)\mathrm{Var}(\Delta Y^{S}\_{2K}) and in dashed orange the proxy relation for V1S=0.02V\_{1}^{S}=0.02.

Figure 2: Structural constants plots.



| Structural constant | Value |
| --- | --- |
| First bias coefficient c1c\_{1} | 0.0250.025 |
| Bias coefficient geometric growth aa | 22 |
| Variance decay constant with antithetic sampling V1AV^{A}\_{1} | 0.0100.010 |
| Variance decay constant without antithetic sampling V1CV^{C}\_{1} | 0.0200.020 |
| Variance at the first level œÉ¬Ø12\bar{\sigma}\_{1}^{2} | 0.5%0.5\% |

Table 4: Summary of the structural constants in the framework.

### 6.2 Methodology and Benchmarking

For all prescribed precision Œµ>0\varepsilon>0, we consider 5 estimators :

1. 1.

   the standard nested Monte Carlo estimator that consist of using the parameters in Table [2](https://arxiv.org/html/2510.18995v1#S3.T2 "Table 2 ‚Ä£ 3.2.4 Complexity analysis for the new optimized parameters ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") where R‚àó‚Äã(Œµ)=1R^{\*}(\varepsilon)=1 is forced regardless of Œµ\varepsilon
2. 2.

   the weighted Multi-level Monte Carlo (ML2R) estimator with the parameters of Table [2](https://arxiv.org/html/2510.18995v1#S3.T2 "Table 2 ‚Ä£ 3.2.4 Complexity analysis for the new optimized parameters ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") left column
3. 3.

   the classical Multi-level Monte Carlo (MLC) estimator with the parameters of Table [2](https://arxiv.org/html/2510.18995v1#S3.T2 "Table 2 ‚Ä£ 3.2.4 Complexity analysis for the new optimized parameters ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") right column
4. 4.

   the ML2R estimator with the standard parameters of Table [1](https://arxiv.org/html/2510.18995v1#S3.T1 "Table 1 ‚Ä£ 3.1.1 Plug-and-play parameters ‚Ä£ 3.1 State-of-the-art MLMC parameterization ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") left column
5. 5.

   the MLMC estimator with the standard parameters of Table [1](https://arxiv.org/html/2510.18995v1#S3.T1 "Table 1 ‚Ä£ 3.1.1 Plug-and-play parameters ‚Ä£ 3.1 State-of-the-art MLMC parameterization ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") right column

The methodology is the following, we consider a sequence of target precision (Œµn)n‚àà{1,‚Ä¶,N}(\varepsilon\_{n})\_{n\in\{1,\dots,N\}}. For each estimator I^\hat{I} and each n‚àà{1,‚Ä¶,N}n\in\{1,\dots,N\} we compute the corresponding parameters Œ∏n=Œ∏Œµn‚ààŒò\theta\_{n}=\theta\_{\varepsilon\_{n}}\in\Theta. The computational cost associated with each set of parameters is then evaluated.

For each set of parameter Œ∏n\theta\_{n}, we perform M‚àà‚ÑïM\in\mathbb{N} independent estimation of II and q99.5%q\_{99.5\%} by generating samples according to the specified parameters. Using these estimations, we calculate the empirical RMSE. Importantly, for each sample, we simultaneously estimate both II and q99.5%q\_{99.5\%} from the same sample. To illustrate we report in Algorithm [1](https://arxiv.org/html/2510.18995v1#alg1 "Algorithm 1 ‚Ä£ 6.2 Methodology and Benchmarking ‚Ä£ 6 Numerical experiments ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations"), the pseudo-code for the simultaneous estimation in the ML2R case with antithetic sampling. Although the root-finding step in Algorithm [1](https://arxiv.org/html/2510.18995v1#alg1 "Algorithm 1 ‚Ä£ 6.2 Methodology and Benchmarking ‚Ä£ 6 Numerical experiments ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") incurs additional computational cost that increases with JJ, we disregard this expense since, in practical applications, it would be negligible compared to the cost of the sampling step.

Algorithm 1  Estimate simultaneously II and q99.5%q\_{99.5\%} with an antithetic ML2R.

Œ∏=(J,q,K,R)‚ààŒò\theta=(J,q,K,R)\in\Theta, uu the c.d.f. evaluation point, pp the quantile level

K1‚Üê‚åàK‚åâK\_{1}\leftarrow\lceil K\rceil

J1‚Üê‚åàJ‚Äãq1‚åâJ\_{1}\leftarrow\lceil Jq\_{1}\rceil

EK1E\_{K\_{1}} be an i.i.d sample of E^K1‚Äã(X)\hat{E}\_{K\_{1}}(X) with size J1J\_{1}

for r=2,‚Ä¶,Rr=2,...,R do

Kr‚Üê‚åàK‚åâ‚Äã2r‚àí1K\_{r}\leftarrow\lceil K\rceil 2^{r-1}

Jr‚Üê‚åàJ‚Äãqr‚åâJ\_{r}\leftarrow\lceil Jq\_{r}\rceil

(EKrf,EKrc,EKrc‚Ä≤)(E^{f}\_{K\_{r}},E\_{K\_{r}}^{c},E\_{K\_{r}}^{c^{\prime}}) be an i.i.d sample of (E^Krf‚Äã(X),E^Krc‚Äã(X),E^Krc‚Ä≤‚Äã(X))(\hat{E}^{f}\_{K\_{r}}(X),\hat{E}^{c}\_{K\_{r}}(X),\hat{E}^{c^{\prime}}\_{K\_{r}}(X)) with size JrJ\_{r}

end for

procedure Evaluate(v)

I^‚Üê1J1‚Äã‚àëj=1J1ùüôEK1‚Äã[j]‚â§v\hat{I}\leftarrow\frac{1}{J\_{1}}\sum\_{j=1}^{J\_{1}}\mathbbm{1}\_{E\_{K\_{1}}[j]\leq v}

for r=2,‚Ä¶,Rr=2,...,R do

I^‚ÜêI^+WrRJr‚Äã‚àëj=1JrùüôEKrf‚Äã[j]‚â§v‚àí12‚Äã(ùüôEKrc‚Äã[j]‚â§v+ùüôEKrc‚Ä≤‚Äã[j]‚â§v)\hat{I}\leftarrow\hat{I}+\frac{W\_{r}^{R}}{J\_{r}}\sum\_{j=1}^{J\_{r}}\mathbbm{1}\_{E^{f}\_{K\_{r}}[j]\leq v}-\frac{1}{2}(\mathbbm{1}\_{E^{c}\_{K\_{r}}[j]\leq v}+\mathbbm{1}\_{E^{c^{\prime}}\_{K\_{r}}[j]\leq v})

end for

return I^\hat{I}

end procedure

I^‚Üê\hat{I}\leftarrow Evaluate(uu)

q^‚Üê\hat{q}\leftarrow a root of v‚Ü¶v\mapsto (Evaluate(vv)‚àíp-p)

return (I^,q^)(\hat{I},\hat{q})

In Figure¬†[3](https://arxiv.org/html/2510.18995v1#S6.F3 "Figure 3 ‚Ä£ 6.2 Methodology and Benchmarking ‚Ä£ 6 Numerical experiments ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations"), we present the computational cost of each estimator as a function of the achieved empirical RMSE for both target quantities. We observe that the performance of each estimator is consistent across the two tasks. The weighted Multi-level Monte Carlo estimator performs clearly better than the other estimators. We also notice that the new parameters from Table [2](https://arxiv.org/html/2510.18995v1#S3.T2 "Table 2 ‚Ä£ 3.2.4 Complexity analysis for the new optimized parameters ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") increase the performance on lower computational budget while keeping the increased performance on the highest budgets. Contrary to the standard parameters of Table [1](https://arxiv.org/html/2510.18995v1#S3.T1 "Table 1 ‚Ä£ 3.1.1 Plug-and-play parameters ‚Ä£ 3.1 State-of-the-art MLMC parameterization ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations"), the new parameters correctly select R=1R=1 when appropriate (here roughly for computational budgets lower than 10710^{7}). In other words the MLMC with optimized parameters are always at least as efficient as a standard Nested MC. Notice that, at the highest achieved precisions, the ML2R estimator with parameters calibrated using Table [2](https://arxiv.org/html/2510.18995v1#S3.T2 "Table 2 ‚Ä£ 3.2.4 Complexity analysis for the new optimized parameters ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") (orange curve) is approximately 3 to 4 times more efficient than the traditional nested Monte Carlo (MC) method (blue curve). Additionally, observe that the MLMC method without weights (red curve) does not outperform the traditional nested MC in this experiment. This supports the theoretical view that the use of weights is crucial when dealing with indicator functions.

![Refer to caption](images/cdf_all_rmse_cost.png)


(a) Pointwise c.d.f. estimation at 99.5% level.

![Refer to caption](images/quantile_all_rmse_cost.png)


(b) 99.5% Quantile estimation.

Figure 3: Comparison of different computational complexity : Empirical RMSE against computational cost. In faded color the 95% confidence interval on the RMSE. In blue, the traditional Nested MC estimator calibrated with Table [2](https://arxiv.org/html/2510.18995v1#S3.T2 "Table 2 ‚Ä£ 3.2.4 Complexity analysis for the new optimized parameters ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") and R = 1. In orange the ML2R estimator calibrated with Table [2](https://arxiv.org/html/2510.18995v1#S3.T2 "Table 2 ‚Ä£ 3.2.4 Complexity analysis for the new optimized parameters ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations"). In green the ML2R estimator calibrated with Table [1](https://arxiv.org/html/2510.18995v1#S3.T1 "Table 1 ‚Ä£ 3.1.1 Plug-and-play parameters ‚Ä£ 3.1 State-of-the-art MLMC parameterization ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations"). In red the standard MLMC estimator calibrated with Table [2](https://arxiv.org/html/2510.18995v1#S3.T2 "Table 2 ‚Ä£ 3.2.4 Complexity analysis for the new optimized parameters ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations"). In purple the standard MLMC estimator calibrated with Table [1](https://arxiv.org/html/2510.18995v1#S3.T1 "Table 1 ‚Ä£ 3.1.1 Plug-and-play parameters ‚Ä£ 3.1 State-of-the-art MLMC parameterization ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations").

### 6.3 Influence of œÑ\tau on efficiency.

In this section, we compare the robustness with respect to the parameter œÑ\tau of an ML2R estimator calibrated with Table [1](https://arxiv.org/html/2510.18995v1#S3.T1 "Table 1 ‚Ä£ 3.1.1 Plug-and-play parameters ‚Ä£ 3.1 State-of-the-art MLMC parameterization ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") to an ML2R calibrated with Table [2](https://arxiv.org/html/2510.18995v1#S3.T2 "Table 2 ‚Ä£ 3.2.4 Complexity analysis for the new optimized parameters ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations"). The methodology consist of considering a fixed computational budget C‚àà‚ÑïC\in\mathbb{N} and an increasing sequence (œÑn)n‚àà{1,‚Ä¶,N}(\tau\_{n})\_{n\in\{1,\dots,N\}} of parameter œÑ\tau. For each n‚àà{1,‚Ä¶,N}n\in\{1,\dots,N\}, since both ùíûœÑ‚Äã(Œ∏Œµ)\mathcal{C}\_{\tau}(\theta\_{\varepsilon}) and ùíûœÑ‚Äã(Œ∏Œµ‚àó)\mathcal{C}\_{\tau}(\theta^{\*}\_{\varepsilon}) are increasing when Œµ\varepsilon decrease, we can find with a root finding algorithm, Œµ‚Äã(œÑ)\varepsilon(\tau) and Œµ‚àó‚Äã(œÑ)\varepsilon^{\*}(\tau) such that ùíûœÑ‚Äã(Œ∏Œµ‚Äã(œÑ))‚âàùíûœÑ‚Äã(Œ∏Œµ‚àó‚Äã(œÑ)‚àó)‚âàC\mathcal{C}\_{\tau}(\theta\_{\varepsilon(\tau)})\approx\mathcal{C}\_{\tau}(\theta^{\*}\_{\varepsilon^{\*}(\tau)})\approx C. These parameters correspond to the best use of the given computational budget for both calibration approaches. We then compute the efficiency eœÑe\_{\tau} of parameters Œ∏Œµ‚àó‚Äã(œÑ)‚àó\theta^{\*}\_{\varepsilon^{\*}(\tau)} with respect to Œ∏‚Äã(ŒµœÑ)\theta(\varepsilon\_{\tau}) with eœÑ:=‚Ñ≥‚Äã(Œ∏‚Äã(ŒµœÑ))‚Ñ≥‚Äã(Œ∏‚àó‚Äã(ŒµœÑ‚àó))e\_{\tau}:=\frac{\mathcal{M}(\theta(\varepsilon\_{\tau}))}{\mathcal{M}(\theta^{\*}(\varepsilon^{\*}\_{\tau}))} where both MSE are estimated empirically. In Figure [4](https://arxiv.org/html/2510.18995v1#S6.F4 "Figure 4 ‚Ä£ 6.3 Influence of ùúè on efficiency. ‚Ä£ 6 Numerical experiments ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") we plot the efficiency for varying values of œÑ\tau (see also Table [5](https://arxiv.org/html/2510.18995v1#S6.T5 "Table 5 ‚Ä£ 6.3 Influence of ùúè on efficiency. ‚Ä£ 6 Numerical experiments ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") and Table [6](https://arxiv.org/html/2510.18995v1#S6.T6 "Table 6 ‚Ä£ 6.3 Influence of ùúè on efficiency. ‚Ä£ 6 Numerical experiments ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")). We observe that the efficiency is increasing when œÑ\tau grows, which confirm that parameters from Table [2](https://arxiv.org/html/2510.18995v1#S3.T2 "Table 2 ‚Ä£ 3.2.4 Complexity analysis for the new optimized parameters ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") are more robust to large œÑ\tau. Furthermore observe in Table [6](https://arxiv.org/html/2510.18995v1#S6.T6 "Table 6 ‚Ä£ 6.3 Influence of ùúè on efficiency. ‚Ä£ 6 Numerical experiments ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") that as œÑ\tau grows, the selected RR from Table [2](https://arxiv.org/html/2510.18995v1#S3.T2 "Table 2 ‚Ä£ 3.2.4 Complexity analysis for the new optimized parameters ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") decreases, confirming the discussion of Section [3.3](https://arxiv.org/html/2510.18995v1#S3.SS3 "3.3 Influence of the parameter ùúè on the optimized parameters ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations"). However, since the selected RR remains greater than 1 even for larger values of œÑ\tau, we see that the weighted MLMC procedure continues to be more efficient than the traditional nested estimator.

![Refer to caption](images/closed_ml2r_vs_optimized_ml2r.png)


Figure 4: Efficiency of the optimized parameters of Table [2](https://arxiv.org/html/2510.18995v1#S3.T2 "Table 2 ‚Ä£ 3.2.4 Complexity analysis for the new optimized parameters ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") against parameters of Table [1](https://arxiv.org/html/2510.18995v1#S3.T1 "Table 1 ‚Ä£ 3.1.1 Plug-and-play parameters ‚Ä£ 3.1 State-of-the-art MLMC parameterization ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") for an ML2R estimator. In faded color, the 90% confidence interval for the efficiency.



| œÑ\tau | JJ | KK | RR | Cost | CDF RMSE |
| --- | --- | --- | --- | --- | --- |
| 0 | 1.37‚ãÖ1071.37\cdot 10^{7} | 1010 | 44 | 5.00‚ãÖ1085.00\cdot 10^{8} | 6.51‚ãÖ10‚àí5‚Äã(6.20‚ãÖ10‚àí5‚àí6.80‚ãÖ10‚àí5)6.51\cdot 10^{-5}\;(6.20\cdot 10^{-5}-6.80\cdot 10^{-5}) |
| 2525 | 8.98‚ãÖ1068.98\cdot 10^{6} | 1010 | 44 | 5.00‚ãÖ1085.00\cdot 10^{8} | 8.24‚ãÖ10‚àí5‚Äã(7.88‚ãÖ10‚àí5‚àí8.59‚ãÖ10‚àí5)8.24\cdot 10^{-5}\;(7.88\cdot 10^{-5}-8.59\cdot 10^{-5}) |
| 5050 | 6.19‚ãÖ1066.19\cdot 10^{6} | 1010 | 44 | 5.00‚ãÖ1085.00\cdot 10^{8} | 9.74‚ãÖ10‚àí5‚Äã(9.32‚ãÖ10‚àí5‚àí1.01‚ãÖ10‚àí4)9.74\cdot 10^{-5}\;(9.32\cdot 10^{-5}-1.01\cdot 10^{-4}) |
| 7575 | 4.73‚ãÖ1064.73\cdot 10^{6} | 1010 | 44 | 5.00‚ãÖ1085.00\cdot 10^{8} | 1.13‚ãÖ10‚àí4‚Äã(1.08‚ãÖ10‚àí4‚àí1.20‚ãÖ10‚àí4)1.13\cdot 10^{-4}\;(1.08\cdot 10^{-4}-1.20\cdot 10^{-4}) |
| 100100 | 3.82‚ãÖ1063.82\cdot 10^{6} | 1010 | 44 | 5.00‚ãÖ1085.00\cdot 10^{8} | 1.25‚ãÖ10‚àí4‚Äã(1.20‚ãÖ10‚àí4‚àí1.30‚ãÖ10‚àí4)1.25\cdot 10^{-4}\;(1.20\cdot 10^{-4}-1.30\cdot 10^{-4}) |

Table 5: Sensitivity of an ML2R calibrated with Table [1](https://arxiv.org/html/2510.18995v1#S3.T1 "Table 1 ‚Ä£ 3.1.1 Plug-and-play parameters ‚Ä£ 3.1 State-of-the-art MLMC parameterization ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") to varying œÑ\tau. In parenthesis are 95% confidence intervals on the estimated RMSE. For conciseness parameter qq is omitted.



| œÑ\tau | JJ | KK | RR | Cost | CDF RMSE |
| --- | --- | --- | --- | --- | --- |
| 0 | 2.23‚ãÖ1072.23\cdot 10^{7} | 1010 | 33 | 5.00‚ãÖ1085.00\cdot 10^{8} | 4.59‚ãÖ10‚àí5‚Äã(4.38‚ãÖ10‚àí5‚àí4.78‚ãÖ10‚àí5)4.59\cdot 10^{-5}\;(4.38\cdot 10^{-5}-4.78\cdot 10^{-5}) |
| 2525 | 6.30‚ãÖ1066.30\cdot 10^{6} | 3838 | 22 | 5.00‚ãÖ1085.00\cdot 10^{8} | 5.60‚ãÖ10‚àí5‚Äã(5.34‚ãÖ10‚àí5‚àí5.85‚ãÖ10‚àí5)5.60\cdot 10^{-5}\;(5.34\cdot 10^{-5}-5.85\cdot 10^{-5}) |
| 5050 | 4.71‚ãÖ1064.71\cdot 10^{6} | 3939 | 22 | 5.00‚ãÖ1085.00\cdot 10^{8} | 6.21‚ãÖ10‚àí5‚Äã(5.95‚ãÖ10‚àí5‚àí6.47‚ãÖ10‚àí5)6.21\cdot 10^{-5}\;(5.95\cdot 10^{-5}-6.47\cdot 10^{-5}) |
| 7575 | 3.72‚ãÖ1063.72\cdot 10^{6} | 4141 | 22 | 5.00‚ãÖ1085.00\cdot 10^{8} | 6.93‚ãÖ10‚àí5‚Äã(6.63‚ãÖ10‚àí5‚àí7.21‚ãÖ10‚àí5)6.93\cdot 10^{-5}\;(6.63\cdot 10^{-5}-7.21\cdot 10^{-5}) |
| 100100 | 3.08‚ãÖ1063.08\cdot 10^{6} | 4343 | 22 | 5.00‚ãÖ1085.00\cdot 10^{8} | 7.30‚ãÖ10‚àí5‚Äã(6.98‚ãÖ10‚àí5‚àí7.59‚ãÖ10‚àí5)7.30\cdot 10^{-5}\;(6.98\cdot 10^{-5}-7.59\cdot 10^{-5}) |

Table 6: Sensitivity of an ML2R calibrated with Table [2](https://arxiv.org/html/2510.18995v1#S3.T2 "Table 2 ‚Ä£ 3.2.4 Complexity analysis for the new optimized parameters ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") to varying œÑ\tau. In parenthesis are 95% confidence intervals on the estimated RMSE. For conciseness parameter qq is omitted.

## References

* [1]

  P¬†Hall and C¬†C Heyde.
  Martingale limit theory and its applications.
  Probability & Mathematical Statistics Monograph. Academic Press, 1981.
* [2]

  Tom Rainforth, Robert Cornish, Hongseok Yang, and Andrew Warrington.
  On nesting monte carlo estimators.
  In International Conference on Machine Learning, 2017.
* [3]

  Michael¬†B. Gordy and Sandeep Juneja.
  Nested Simulation in Portfolio Risk Measurement.
  Management Science, 56(10):1833‚Äì1848, 2010.
* [4]

  Michael¬†B. Giles and Abul-Lateef Haji-Ali.
  Multilevel Nested Simulation for Efficient Risk Estimation.
  SIAM/ASA Journal on Uncertainty Quantification, 7(2):497‚Äì525, 2019.
* [5]

  Vincent Lemaire and Gilles Pag√®s.
  Multilevel Richardson‚ÄìRomberg extrapolation.
  Bernoulli, 23(4A):2643‚Äì2692, 2017.
* [6]

  Daphn√© Giorgi, Vincent Lemaire, and Gilles Pag√®s.
  Weak error for nested multilevel monte carlo.
  Methodology and Computing in Applied Probability, 22(3):1325‚Äì1348, January 2020.
* [7]

  Mark Broadie, Yiping Du, and Ciamac¬†C. Moallemi.
  Efficient Risk Estimation via Nested Sequential Simulation.
  Management Science, 57(6):1172‚Äì1194, June 2011.
* [8]

  Pierre-Edouard Arrouy, Alexandre Boumezoued, Bernard Lapeyre, and Sophian Mehalla.
  Economic scenario generators: a risk management tool for insurance.
  MathematicS In Action, 2022.
* [9]

  Philippe Jorion.
  Value at Risk: The New Benchmark for Managing Financial Risk 3rd Ed.
  McGraw Hill Professional, 2006.
* [10]

  Erik Bolviken.
  Solvency ii in life insurance, 2017.
* [11]

  Daniel Bauer, Andreas Reu√ü, and Daniel¬†D. Singer.
  On the calculation of the solvency capital requirement based on nested simulations.
  Astin Bulletin, 42:453‚Äì499, 2012.
* [12]

  Runhuan Feng, Zhenyu Cui, and Peng Li.
  Nested stochastic modeling for insurance companies.
  Society Of Actuaries, 2016.
* [13]

  Anne-Sophie Krah, Zoran Nikolic, and Ralf Korn.
  A least-squares monte carlo framework in proxy modeling of life insurance companies.
  Risks, 2018.
* [14]

  Hongjun Ha and Daniel Bauer.
  A least-squares monte carlo approach to the estimation of enterprise risk.
  Finance and Stochastics, 26(3):417‚Äì459, July 2022.
* [15]

  Mathieu Cambou and Damir Filipoviƒá.
  Replicating portfolio approach to capital calculation.
  Finance and Stochastics, 22, 01 2018.
* [16]

  Gilberto Castellani, Ugo Fiore, Zelda Marino, Luca Passalacqua, Francesca Perla, Salvatore Scognamiglio, and Paolo Zanetti.
  Machine learning techniques in nested stochastic simulations for life insurance.
  Applied Stochastic Models in Business and Industry, 37:159‚Äì181, 2021.
* [17]

  Patrick Cheridito, John Ery, and Mario¬†V. W√ºthrich.
  Assessing asset-liability risk with neural networks.
  Risks, 2020.
* [18]

  Anne-Sophie Krah, Zoran Nikolic, and Ralf Korn.
  Least-squares monte carlo for proxy modeling in life insurance: Neural networks.
  Risks, 2020.
* [19]

  Francesca Perla, Salvatore Scognamiglio, Andrea Spadaro, and Paolo Zanetti.
  Transformers-based least square monte carlo for solvency calculation in life insurance.
  Insurance: Mathematics and Economics, 2025.
* [20]

  Aur√©lien Alfonsi, Bernard Lapeyre, and J√©r√¥me Lelong.
  How many inner simulations to compute conditional expectations with least-square monte carlo?
  Methodology and Computing in Applied Probability, 25, 2022.
* [21]

  Michael¬†B. Giles.
  Multilevel Monte-Carlo methods.
  Acta Numerica, 24:259‚Äì328, 2015.
* [22]

  Michael¬†B. Giles.
  Multilevel Monte-Carlo Path Simulation.
  Operations Research, 56(3):607‚Äì617, 2008.
* [23]

  Sebastian Krumscheid and Fabio Nobile.
  Multilevel monte carlo approximation of functions.
  SIAM/ASA J. Uncertain. Quantification, 6:1256‚Äì1293, 2018.
* [24]

  St√©phane Cr√©pey, Noufel Frikha, Azar Louzi, and Jonathan Spence.
  Adaptive multilevel stochastic approximation of the value-at-risk, 2024.
* [25]

  K.¬†Bujok, B.¬†M. Hambly, and C.¬†Reisinger.
  Multilevel simulation of functionals of bernoulli random variables with application to basket credit derivatives.
  Methodology and Computing in Applied Probability, 17(3):579‚Äì604, October 2013.
* [26]

  Abdul-Lateef Haji-Ali.
  Pedestrian Flow in the Mean Field Limit.
  PhD thesis, King Abdullah University of Science and Technology, 2012.
* [27]

  Gilles Pag√®s.
  Numerical Probability: An Introduction with Applications to Finance.
  Springer Cham, 2018.
* [28]

  Abdul-Lateef Haji-Ali and Jonathan Spence.
  Nested multilevel monte carlo with biased and antithetic sampling.
  ArXiv, abs/2308.07835, 2023.
* [29]

  Matthias Scherer and Gerhard Stahl.
  The standard formula of solvency ii: a critical discussion.
  European Actuarial Journal, 11:3 ‚Äì 20, 2020.
* [30]

  Aur√©lien Alfonsi, Adel Cherchali, and Jos√© Infante¬†Acevedo.
  A synthetic model for asset-liability management in life insurance, and analysis of the scr with the standard formula.
  European Actuarial Journal, 10, 06 2020.
* [31]

  Daphn√© Giorgi, Vincent Lemaire, and Gilles Pag√®s.
  Limit theorems for weighted and regular multilevel estimators.
  Monte Carlo Methods and Applications, 23(1):43‚Äì70, February 2017.

## Appendix A Appendix

### A.1 Computation of the weights in the ML2R

In this section we follow the derivation of the weights in the ML2R of Pag√®s [[27](https://arxiv.org/html/2510.18995v1#bib.bib27)] Section 9.4 and 9.5.

For Œ±>0\alpha>0, R‚àà‚ÑïR\in\mathbb{N} fixed, we consider (w1,‚Ä¶,wR)‚àà‚ÑùR(w\_{1},\dots,w\_{R})\in\mathbb{R}^{R} the unique solution to the following Vandermonde system of equations :

|  |  |  |  |
| --- | --- | --- | --- |
|  | {‚àëj=1Rwj=1‚àëj=1Rwj(2Œ±‚Äã(j‚àí1))r,r=1,‚Ä¶,R‚àí1.\begin{cases}\sum\_{j=1}^{R}w\_{j}=1\\ \sum\_{j=1}^{R}\frac{w\_{j}}{(2^{\alpha(j-1)})^{r}},r=1,\dots,R-1\end{cases}\,. |  | (46) |

Considering for r=2,‚Ä¶,Rr=2,\dots,R, Kr=K1‚Äã2r‚àí1K\_{r}=K\_{1}2^{r-1} and assuming that (W‚ÄãEŒ±,R)(WE\_{\alpha,R}) holds

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Äã[‚àër=1Rwr‚ÄãYKr]=ùîº‚Äã[Y‚àû]+W¬ØR+1‚ÄãcRK1R+o‚Äã(1K1R)\mathbb{E}\left[\sum\_{r=1}^{R}w\_{r}Y\_{K\_{r}}\right]=\mathbb{E}\left[Y\_{\infty}\right]+\bar{W}\_{R+1}\frac{c\_{R}}{K\_{1}^{R}}+o\left(\frac{1}{K\_{1}^{R}}\right) |  |

with W¬ØR+1=‚àër=1Rwr2Œ±‚Äãr\bar{W}\_{R+1}=\sum\_{r=1}^{R}\frac{w\_{r}}{2^{\alpha r}}. The weights effectively remove the bias up to a term of order RR and the solution (w1,‚Ä¶,wR)‚àà‚ÑùR(w\_{1},\dots,w\_{R})\in\mathbb{R}^{R} has an explicit expression

|  |  |  |
| --- | --- | --- |
|  | ‚àÄi‚àà{1,‚Ä¶,R},wi=(‚àí1)R‚àíi‚àè1‚â§j‚â§Rj‚â†i|1‚àí2Œ±‚Äã(j‚àíi)|.\forall i\in\{1,\dots,R\},w\_{i}=\;\frac{(-1)^{R-i}}{\prod\_{\begin{subarray}{c}1\leq j\leq R\\ j\neq i\end{subarray}}|1-2^{\alpha(j-i)}|}\,. |  |

This gives (see for example [[5](https://arxiv.org/html/2510.18995v1#bib.bib5)], Proposition 2.5)

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùîº‚Äã[‚àër=1Rwr‚ÄãYKr]=ùîº‚Äã[Y‚àû]+(‚àí1)R‚àí1‚ÄãcRK1Œ±‚ÄãR‚Äã2Œ±‚ÄãR‚Äã(R‚àí1)2+o‚Äã(1K1Œ±‚ÄãR)\mathbb{E}\left[\sum\_{r=1}^{R}w\_{r}Y\_{K\_{r}}\right]=\mathbb{E}\left[Y\_{\infty}\right]+(-1)^{R-1}\frac{c\_{R}}{K\_{1}^{\alpha R}2^{\frac{\alpha R(R-1)}{2}}}+o\left(\frac{1}{K\_{1}^{\alpha R}}\right) |  | (47) |

Combining this Richardson-Romberg procedure with a Multi-level procedure gives the weights :

|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚àÄ1‚â§i‚â§R,Wi=wi+wi+1+‚ãØ+wR.\forall 1\leq i\leq R,\;W\_{i}=w\_{i}+w\_{i+1}+\dots+w\_{R}\,. |  | (48) |

Note that in particular W1=1W\_{1}=1. Then using the Abel transform formula:

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Äã[YK1+‚àër=2RWr‚Äã(YKr‚àíYKr‚àí1)]=ùîº‚Äã[‚àër=1Rwr‚ÄãYKr].\mathbb{E}\left[Y\_{K\_{1}}+\sum\_{r=2}^{R}W\_{r}\left(Y\_{K\_{r}}-Y\_{K\_{r-1}}\right)\right]=\mathbb{E}\left[\sum\_{r=1}^{R}w\_{r}Y\_{K\_{r}}\right]\,. |  |

This shows that if Ar=WrA\_{r}=W\_{r}, r=2,‚Ä¶,Rr=2,\dots,R, then under (W‚ÄãEŒ±,R)(WE\_{\alpha,R}), the weighted Multi-level procedure as no bias term up to the order RR. To emphasize the dependence of the weights on RR we will refer to them as (W1R,W2R,‚Ä¶,WRR)(W\_{1}^{R},W\_{2}^{R},\dots,W\_{R}^{R}) in this article.

### A.2 Technical Lemmas for Section [3.1.2](https://arxiv.org/html/2510.18995v1#S3.SS1.SSS2 "3.1.2 Extension of complexity theorems for arbitrary ùúè ‚Ä£ 3.1 State-of-the-art MLMC parameterization ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")

###### Lemma 2.

(See [[31](https://arxiv.org/html/2510.18995v1#bib.bib31)], Proposition 4.1)
There exist a Œµ0>0\varepsilon\_{0}>0 such that for all Œµ‚â§Œµ0\varepsilon\leq\varepsilon\_{0} such that K‚Äã(Œµ)=K¬ØK(\varepsilon)=\underline{K}.

###### Lemma 3 (See [[31](https://arxiv.org/html/2510.18995v1#bib.bib31)], Lemma 4.3).

Let Œ±>0\alpha>0, R‚àà‚ÑïR\in\mathbb{N} and (WjR)j‚àà{1,‚Ä¶,R}(W\_{j}^{R})\_{j\in\{1,\dots,R\}} be the associated ML2R weights defined in ([48](https://arxiv.org/html/2510.18995v1#A1.E48 "In A.1 Computation of the weights in the ML2R ‚Ä£ Appendix A Appendix ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")).

1. 1.

   the collection {WjR:R‚àà‚Ñï,j‚àà{1,‚Ä¶,R}}\{W\_{j}^{R}:R\in\mathbb{N},j\in\{1,\dots,R\}\} is uniformly bounded, that is there exists a constant W‚àû>0W\_{\infty}>0 such that,

   |  |  |  |
   | --- | --- | --- |
   |  | ‚àÄR‚àà‚Ñï,j‚àà{1,‚Ä¶,R},|WjR|‚â§W‚àû,\forall R\in\mathbb{N},\;j\in\{1,\dots,R\},\quad|W\_{j}^{R}|\leq W\_{\infty}\,, |  |
2. 2.

   for all Œ≥>0\gamma>0,

   |  |  |  |
   | --- | --- | --- |
   |  | limR‚Üí+‚àû‚Äã‚àëj=2R|WjR|‚Äã2‚àíŒ≥‚Äã(j‚àí1)=12Œ≥‚àí1.\underset{R\to+\infty}{\lim}\sum\_{j=2}^{R}|W\_{j}^{R}|2^{-\gamma(j-1)}=\frac{1}{2^{\gamma}-1}\,. |  |

###### Lemma 4 (See [[31](https://arxiv.org/html/2510.18995v1#bib.bib31)], Lemma 7.1).

Let œï:‚Ñï‚Üí‚Ñï\phi:\mathbb{N}\to\mathbb{N} such that œï‚Äã(R)‚àà{1,‚Ä¶,R‚àí1}\phi(R)\in\{1,\dots,R-1\} for every R‚â•1R\geq 1, œï‚Äã(R)‚Äã‚ü∂R‚Üí+‚àû+‚àû\phi(R)\underset{R\to+\infty}{\longrightarrow}+\infty, and R‚àíœï‚Äã(R)‚Äã‚ü∂R‚Üí+‚àû+‚àûR-\phi(R)\underset{R\to+\infty}{\longrightarrow}+\infty. Then,

|  |  |  |
| --- | --- | --- |
|  | limR‚Üí+‚àû‚Äãsupj‚àà{1,‚Ä¶,œï‚Äã(R)}‚Äã|WjR‚àí1|=0.\underset{R\to+\infty}{\lim}\underset{j\in\{1,\dots,\phi(R)\}}{\sup}|W\_{j}^{R}-1|=0\,. |  |

In particular for all j‚àà‚Ñïj\in\mathbb{N}, WjR‚Äã‚ü∂R‚Üí+‚àû‚Äã1W\_{j}^{R}\underset{R\to+\infty}{\longrightarrow}1.

###### Lemma 5.

(See [[31](https://arxiv.org/html/2510.18995v1#bib.bib31)], Section 4.4)
Let Œµ>0\varepsilon>0 and Œ∏Œµ‚ààŒò\theta\_{\varepsilon}\in\Theta the MLMC parameter sequence from Table [1](https://arxiv.org/html/2510.18995v1#S3.T1 "Table 1 ‚Ä£ 3.1.1 Plug-and-play parameters ‚Ä£ 3.1 State-of-the-art MLMC parameterization ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations"). Consider, ŒºŒµ=‚àër=1RŒµœÉ¬Ø‚Äã(r,KŒµ)Œ≥œÑ‚Äã(r,KŒµ)\mu\_{\varepsilon}=\sum\_{r=1}^{R\_{\varepsilon}}\frac{\bar{\sigma}(r,K\_{\varepsilon})}{\sqrt{\gamma\_{\tau}(r,K\_{\varepsilon})}} then, ŒºŒµ‚Äã‚ü∂Œµ‚Üí0‚ÄãŒº‚àû‚àà(0,+‚àû)\mu\_{\varepsilon}\underset{\varepsilon\to 0}{\longrightarrow}\mu\_{\infty}\in(0,+\infty).

###### Proof.

Recall that for all Œµ>0\varepsilon>0, K‚Äã(Œµ)=K¬ØK(\varepsilon)=\underline{K} , therefore since ŒºŒµ\mu\_{\varepsilon} is such that ‚àër=1R‚Äã(Œµ)qr‚Äã(Œµ)=1\sum\_{r=1}^{R(\varepsilon)}q\_{r}(\varepsilon)=1 we get,

|  |  |  |
| --- | --- | --- |
|  | ŒºŒµ=œÉ¬ØK¬Ø+V1K¬Ø1+Œ≤2‚Äã‚àër=2R‚Äã(Œµ)|ArR‚Äã(Œµ)|2(r‚àí1)2‚Äã(1+Œ≤).\mu\_{\varepsilon}=\frac{\bar{\sigma}}{\sqrt{\underline{K}}}+\frac{\sqrt{V\_{1}}}{\underline{K}^{\frac{1+\beta}{2}}}\sum\_{r=2}^{R(\varepsilon)}\frac{|A\_{r}^{R(\varepsilon)}|}{2^{{}^{\frac{(r-1)}{2}(1+\beta)}}}\,. |  |

Then since Œ≤>0\beta>0, an application of Lemma [3](https://arxiv.org/html/2510.18995v1#Thmlemma3 "Lemma 3 (See [31], Lemma 4.3). ‚Ä£ A.2 Technical Lemmas for Section 3.1.2 ‚Ä£ Appendix A Appendix ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") 2. with Œ≥=1+Œ≤2\gamma=\frac{1+\beta}{2} yields the result.
‚àé

### A.3 Technical Lemmas for Section [3.2.4](https://arxiv.org/html/2510.18995v1#S3.SS2.SSS4 "3.2.4 Complexity analysis for the new optimized parameters ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")

###### Lemma 6.

Let Œµ>0\varepsilon>0, R‚àà‚ÑïR\in\mathbb{N}, define in the standard MLMC case CŒµ,R:=Œµ1+2‚ÄãŒ±C\_{\varepsilon,R}:=\frac{\varepsilon}{\sqrt{1+2\alpha}}
while in the ML2R case CŒµ,R:=Œµ1+2‚ÄãŒ±‚ÄãRC\_{\varepsilon,R}:=\frac{\varepsilon}{\sqrt{1+2\alpha R}} then in both cases |Œº~‚Äã(K‚Äã(Œµ),R)|‚â§CŒµ,R|\tilde{\mu}(K(\varepsilon),R)|\leq C\_{\varepsilon,R}.

###### Proof.

We begin with the proof in the case of the standard MLMC estimator. Let Œµ>0\varepsilon>0, starting from ([25](https://arxiv.org/html/2510.18995v1#S3.E25 "In Definition 15. ‚Ä£ 3.2.1 Optimization of ùêΩ ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")),

|  |  |  |
| --- | --- | --- |
|  | |Œº~(K(Œµ),R(Œµ)|‚â§|c1|K‚Äã(Œµ)Œ±‚Äã2(R‚Äã(Œ±)‚àí1)‚ÄãŒ±‚â§|c1|K+‚Äã(Œµ)Œ±‚Äã2(R‚Äã(Œ±)‚àí1)‚ÄãŒ±=Œµ1+2‚ÄãŒ±|\tilde{\mu}(K(\varepsilon),R(\varepsilon)|\leq\frac{|c\_{1}|}{K(\varepsilon)^{\alpha}2^{(R(\alpha)-1)\alpha}}\leq\frac{|c\_{1}|}{K^{+}(\varepsilon)^{\alpha}2^{(R(\alpha)-1)\alpha}}=\frac{\varepsilon}{\sqrt{1+2\alpha}} |  |

where in the last equality we used the definition of K+‚Äã(Œµ)K^{+}(\varepsilon) from Table [1](https://arxiv.org/html/2510.18995v1#S3.T1 "Table 1 ‚Ä£ 3.1.1 Plug-and-play parameters ‚Ä£ 3.1 State-of-the-art MLMC parameterization ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations"). We continue with the proof in the case of the ML2R estimator. Let Œµ>0\varepsilon>0, starting from ([24](https://arxiv.org/html/2510.18995v1#S3.E24 "In Definition 15. ‚Ä£ 3.2.1 Optimization of ùêΩ ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")),

|  |  |  |
| --- | --- | --- |
|  | |Œº~(K(Œµ),R(Œµ)|‚â§c~RK‚Äã(Œµ)Œ±‚ÄãR‚Äã2R‚Äã(Œ±)‚Äã(R‚Äã(Œ±)‚àí1)2‚ÄãŒ±‚â§c~RK+‚Äã(Œµ)Œ±‚ÄãR‚Äã2R‚Äã(Œ±)‚Äã(R‚Äã(Œ±)‚àí1)2‚ÄãŒ±=Œµ1+2‚ÄãŒ±‚ÄãR‚Äã(Œµ)|\tilde{\mu}(K(\varepsilon),R(\varepsilon)|\leq\frac{\tilde{c}^{R}}{K(\varepsilon)^{\alpha R}2^{\frac{R(\alpha)(R(\alpha)-1)}{2}\alpha}}\leq\frac{\tilde{c}^{R}}{K^{+}(\varepsilon)^{\alpha R}2^{\frac{R(\alpha)(R(\alpha)-1)}{2}\alpha}}=\frac{\varepsilon}{\sqrt{1+2\alpha R(\varepsilon)}} |  |

where in the last equality we used the definition of K+‚Äã(Œµ)K^{+}(\varepsilon) from Table [1](https://arxiv.org/html/2510.18995v1#S3.T1 "Table 1 ‚Ä£ 3.1.1 Plug-and-play parameters ‚Ä£ 3.1 State-of-the-art MLMC parameterization ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations"). This conclude the proof.
‚àé

###### Lemma 7.

For all Œµ>0\varepsilon>0,

|  |  |  |  |
| --- | --- | --- | --- |
|  | J‚Äã(Œµ)=v¬Ø‚Äã(œÄ‚Äã(Œµ))Œµ2‚àíCŒµ,R‚Äã(Œµ)2J(\varepsilon)=\frac{\bar{v}(\pi(\varepsilon))}{\varepsilon^{2}-C^{2}\_{\varepsilon,R(\varepsilon)}} |  | (49) |

where CŒµ,R‚Äã(Œµ)C\_{\varepsilon,R(\varepsilon)} is defined in Lemma [6](https://arxiv.org/html/2510.18995v1#Thmlemma6 "Lemma 6. ‚Ä£ A.3 Technical Lemmas for Section 3.2.4 ‚Ä£ Appendix A Appendix ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations").

###### Proof.

Let Œµ>0\varepsilon>0 and denote œÄ‚Äã(Œµ)=(q‚Äã(Œµ),K‚Äã(Œµ),R‚Äã(Œµ))\pi(\varepsilon)=(q(\varepsilon),K(\varepsilon),R(\varepsilon)). Recall from ([20](https://arxiv.org/html/2510.18995v1#S3.E20 "In Definition 14. ‚Ä£ 3.2.1 Optimization of ùêΩ ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")) that

|  |  |  |
| --- | --- | --- |
|  | v¬Ø‚Äã(œÄ‚Äã(Œµ))=‚àër=1R‚Äã(Œµ)œÉ¬Ø2‚Äã(r,K‚Äã(Œµ))qr‚Äã(Œµ)=œÉ¬Ø2q1‚Äã(Œµ)+‚àër=2R‚Äã(Œµ)V1‚Äã(ArR‚Äã(Œµ))2K‚Äã(Œµ)Œ≤‚Äã2Œ≤‚Äã(r‚àí1)‚Äãqr‚Äã(Œµ).\bar{v}(\pi(\varepsilon))=\sum\_{r=1}^{R(\varepsilon)}\frac{\bar{\sigma}^{2}(r,K(\varepsilon))}{q\_{r}(\varepsilon)}=\frac{\bar{\sigma}^{2}}{q\_{1}(\varepsilon)}+\sum\_{r=2}^{R(\varepsilon)}\frac{V\_{1}(A^{R(\varepsilon)}\_{r})^{2}}{K(\varepsilon)^{\beta}2^{\beta(r-1)}q\_{r}(\varepsilon)}\,. |  |

Now since,

|  |  |  |
| --- | --- | --- |
|  | 1Œµ2‚àíCŒµ,R‚Äã(Œµ)2=Œµ2MŒµ\frac{1}{\varepsilon^{2}-C^{2}\_{\varepsilon,R(\varepsilon)}}=\frac{\varepsilon^{2}}{M\_{\varepsilon}} |  |

where MŒµM\_{\varepsilon} is defined in Table [1](https://arxiv.org/html/2510.18995v1#S3.T1 "Table 1 ‚Ä£ 3.1.1 Plug-and-play parameters ‚Ä£ 3.1 State-of-the-art MLMC parameterization ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations"), we clearly get that

|  |  |  |
| --- | --- | --- |
|  | v¬Ø‚Äã(œÄ‚Äã(Œµ))Œµ2‚àíCŒµ,R‚Äã(Œµ)2=J‚Äã(Œµ)\frac{\bar{v}(\pi(\varepsilon))}{\varepsilon^{2}-C^{2}\_{\varepsilon,R(\varepsilon)}}=J(\varepsilon) |  |

proving the claim.
‚àé

###### Lemma 8.

For all Œµ>0\varepsilon>0, ‚Ñ≥~‚Äã(Œ∏Œµ)‚â§Œµ2\widetilde{\mathcal{M}}(\theta\_{\varepsilon})\leq\varepsilon^{2}.

###### Proof.

Let Œµ>0\varepsilon>0, denote œÄ‚Äã(Œµ)=(q‚Äã(Œµ),K‚Äã(Œµ),R‚Äã(Œµ))\pi(\varepsilon)=(q(\varepsilon),K(\varepsilon),R(\varepsilon)) and œÄ0‚Äã(Œµ)=(K‚Äã(Œµ),R‚Äã(Œµ))\pi\_{0}(\varepsilon)=(K(\varepsilon),R(\varepsilon)). Recall from ([26](https://arxiv.org/html/2510.18995v1#S3.E26 "In Definition 16. ‚Ä£ 3.2.1 Optimization of ùêΩ ‚Ä£ 3.2 Beyond closed form: Numerical parameter optimization for finite-Sample efficiency ‚Ä£ 3 Optimizing parameters selection for MLMC in non-asymptotic regimes ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")) that

|  |  |  |
| --- | --- | --- |
|  | ‚Ñ≥~‚Äã(Œ∏Œµ)=v¬Ø‚Äã(œÄ‚Äã(Œµ))J‚Äã(Œµ)+Œº~2‚Äã(œÄ0‚Äã(Œµ))\widetilde{\mathcal{M}}(\theta\_{\varepsilon})=\frac{\bar{v}(\pi(\varepsilon))}{J(\varepsilon)}+\tilde{\mu}^{2}(\pi\_{0}(\varepsilon)) |  |

Owing to ([49](https://arxiv.org/html/2510.18995v1#A1.E49 "In Lemma 7. ‚Ä£ A.3 Technical Lemmas for Section 3.2.4 ‚Ä£ Appendix A Appendix ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")) we have,

|  |  |  |
| --- | --- | --- |
|  | J‚Äã(Œµ)=v¬Ø‚Äã(œÄ‚Äã(Œµ))Œµ2‚àíCŒµ,R‚Äã(Œµ)2J(\varepsilon)=\frac{\bar{v}(\pi(\varepsilon))}{\varepsilon^{2}-C^{2}\_{\varepsilon,R(\varepsilon)}} |  |

Therefore, ‚Ñ≥~‚Äã(Œ∏Œµ)=Œµ2‚àíCŒµ,R‚Äã(Œµ)2+Œº~2‚Äã(œÄ0‚Äã(Œµ))\widetilde{\mathcal{M}}(\theta\_{\varepsilon})=\varepsilon^{2}-C^{2}\_{\varepsilon,R(\varepsilon)}+\tilde{\mu}^{2}(\pi\_{0}(\varepsilon)). By Lemma ([6](https://arxiv.org/html/2510.18995v1#Thmlemma6 "Lemma 6. ‚Ä£ A.3 Technical Lemmas for Section 3.2.4 ‚Ä£ Appendix A Appendix ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")), CŒµ,R‚Äã(Œµ)‚â•|Œº~‚Äã(œÄ0‚Äã(Œµ))|C\_{\varepsilon,R(\varepsilon)}\geq|\tilde{\mu}(\pi\_{0}(\varepsilon))| from which we get the result ‚Ñ≥~‚Äã(Œ∏Œµ)‚â§Œµ2\widetilde{\mathcal{M}}(\theta\_{\varepsilon})\leq\varepsilon^{2}.
‚àé

### A.4 Proof of Proposition [9](https://arxiv.org/html/2510.18995v1#Thmproposition9 "Proposition 9. ‚Ä£ 4.2 Nested Monte Carlo : closed-form optimized parameters ‚Ä£ 4 Estimating probability of large losses ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")

To prove the proposition we introduce, for all b‚àà‚Ñùb\in\mathbb{R},

|  |  |  |
| --- | --- | --- |
|  | EŒµ,b={x‚àà(0,+‚àû):bŒµ<x}E\_{\varepsilon,b}=\left\{x\in(0,+\infty):\frac{b}{\varepsilon}<x\right\} |  |

and for all a‚àà‚Ñù+a\in\mathbb{R}^{+}, define the auxiliary function fŒµ,a,bf\_{\varepsilon,a,b} from EŒµ,bE\_{\varepsilon,b} to ‚Ñù\mathbb{R} such that

|  |  |  |
| --- | --- | --- |
|  | ‚àÄx‚ààEŒµ,fŒµ,a,b‚Äã(x)=x+aŒµ2‚àíb2x2=x2‚Äã(x+a)x2‚ÄãŒµ2‚àíb2\forall x\in E\_{\varepsilon},\quad f\_{\varepsilon,a,b}(x)=\frac{x+a}{\varepsilon^{2}-\frac{b^{2}}{x^{2}}}=\frac{x^{2}\left(x+a\right)}{x^{2}\varepsilon^{2}-b^{2}} |  |

###### Lemma 9.

ff is a differentiable, strictly convex and coercive function on x‚ààEŒµx\in E\_{\varepsilon}, therefore it admits a unique global minima on EŒµE\_{\varepsilon}.

###### Proof.

For all Œµ>0\varepsilon>0, a‚àà‚Ñù+a\in\mathbb{R}\_{+}, b‚àà‚Ñùb\in\mathbb{R}, fŒµ,a,bf\_{\varepsilon,a,b} is two time differentiable on EŒµE\_{\varepsilon} and for all x‚ààEŒµx\in E\_{\varepsilon}

|  |  |  |
| --- | --- | --- |
|  | fŒµ,a,b‚Ä≤‚Äã(x)=x‚Äã(‚àí2‚Äãa‚Äãb2+Œµ2‚Äãx3‚àí3‚Äãb2‚Äãx)(x2‚ÄãŒµ2‚àíb2)2,fŒµ,a,b‚Ä≤‚Ä≤‚Äã(x)=2‚Äãb2(x2‚ÄãŒµ2‚àíb2)3‚Äã(a‚Äã(3‚Äãx2‚ÄãŒµ2+b2)+x‚Äã(x2‚ÄãŒµ2+3‚Äãb2))f^{\prime}\_{\varepsilon,a,b}(x)=\frac{x(-2ab^{2}+\varepsilon^{2}x^{3}-3b^{2}x)}{(x^{2}\varepsilon^{2}-b^{2})^{2}},\quad f^{\prime\prime}\_{\varepsilon,a,b}(x)=\frac{2b^{2}}{(x^{2}\varepsilon^{2}-b^{2})^{3}}\left(a\left(3x^{2}\varepsilon^{2}+b^{2}\right)+x\left(x^{2}\varepsilon^{2}+3b^{2}\right)\right) |  |

Therefore we have that f‚Ä≤‚Ä≤‚Äã(x)>0f^{\prime\prime}(x)>0, thus ff is strictly convex on EŒµE\_{\varepsilon}. Furthermore it is easy to check that it is a coercive function.
‚àé

###### Lemma 10.

For all Œµ>0\varepsilon>0, b‚àà‚Ñùb\in\mathbb{R}, if a=0a=0

|  |  |  |
| --- | --- | --- |
|  | fŒµ,0,b‚Ä≤‚Äã(x)=0‚áîx=3‚ÄãbŒµf^{\prime}\_{\varepsilon,0,b}(x)=0\Leftrightarrow x=\frac{\sqrt{3}b}{\varepsilon} |  |

if a>0a>0,

|  |  |  |
| --- | --- | --- |
|  | fŒµ,a,b‚Ä≤‚Äã(x)‚áî{|b|Œµ‚Äã2‚Äãcos‚Å°(13‚Äãarccos‚Å°(Œµ‚Äãa|b|)),Œµ<|b|a3‚Äãb,Œµ=|b|a(|b|Œµ)23‚Äã[(a+(a2‚àíb2Œµ2)12)13+(a‚àí(a2‚àíb2Œµ2)12)13],Œµ>|b|a.f^{\prime}\_{\varepsilon,a,b}(x)\Leftrightarrow\begin{cases}\frac{|b|}{\varepsilon}2\cos\left(\frac{1}{3}\arccos\left(\frac{\varepsilon a}{|b|}\right)\right)&,\;\varepsilon<\frac{|b|}{a}\\ 3b&,\;\varepsilon=\frac{|b|}{a}\\ \left(\frac{|b|}{\varepsilon}\right)^{\frac{2}{3}}\left[\left(a+\left(a^{2}-\frac{b^{2}}{\varepsilon^{2}}\right)^{\frac{1}{2}}\right)^{\frac{1}{3}}+\left(a-\left(a^{2}-\frac{b^{2}}{\varepsilon^{2}}\right)^{\frac{1}{2}}\right)^{\frac{1}{3}}\right]&,\;\varepsilon>\frac{|b|}{a}\,.\end{cases} |  |

###### Proof.

From Lemma [9](https://arxiv.org/html/2510.18995v1#Thmlemma9 "Lemma 9. ‚Ä£ A.4 Proof of Proposition 9 ‚Ä£ Appendix A Appendix ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations"), for all Œµ>0\varepsilon>0, a‚àà‚Ñù+a\in\mathbb{R\_{+}} and b‚àà‚Ñùb\in\mathbb{R}, fŒµ,a,bf\_{\varepsilon,a,b} admits a unique minimum x‚àóx\_{\*} in EŒµ,bE\_{\varepsilon,b} characterized by

|  |  |  |
| --- | --- | --- |
|  | fŒµ,a,b‚Ä≤‚Äã(x‚àó)=0f\_{\varepsilon,a,b}^{\prime}(x\_{\*})=0 |  |

We have that for all x‚ààEŒµ,bx\in E\_{\varepsilon,b}

|  |  |  |
| --- | --- | --- |
|  | fŒµ,a,b‚Ä≤‚Äã(x)=x(x2‚ÄãŒµ2‚àíc12)‚Äã(‚àí2‚ÄãœÑ‚Äãc12+Œµ2‚Äãx3‚àí3‚Äãc12‚Äãx)\displaystyle f\_{\varepsilon,a,b}^{\prime}(x)=\frac{x}{(x^{2}\varepsilon^{2}-c\_{1}^{2})}\left(-2\tau c\_{1}^{2}+\varepsilon^{2}x^{3}-3c\_{1}^{2}x\right) |  |

Therefore,

|  |  |  |
| --- | --- | --- |
|  | fŒµ,a,b‚Ä≤‚Äã(x)=0‚ü∫x3‚àí3‚Äãb2Œµ2‚Äãx‚àí2‚Äãb2‚ÄãaŒµ2=0f\_{\varepsilon,a,b}^{\prime}(x)=0\Longleftrightarrow x^{3}-\frac{3b^{2}}{\varepsilon^{2}}x-\frac{2b^{2}a}{\varepsilon^{2}}=0 |  |

If a=0a=0, then we easily obtain that

|  |  |  |
| --- | --- | --- |
|  | fŒµ,a,b‚Ä≤‚Äã(x)=0‚ü∫x=3‚ÄãbŒµf\_{\varepsilon,a,b}^{\prime}(x)=0\Longleftrightarrow x=\frac{\sqrt{3}b}{\varepsilon} |  |

Otherwise when œÑ>0\tau>0 we consider the polynomial

|  |  |  |
| --- | --- | --- |
|  | P‚Äã[X]=X3‚àí3‚Äãb2Œµ2‚ÄãX‚àí2‚Äãb2‚ÄãaŒµ2P[X]=X^{3}-\frac{3b^{2}}{\varepsilon^{2}}X-\frac{2b^{2}a}{\varepsilon^{2}} |  |

and its discriminant :

|  |  |  |
| --- | --- | --- |
|  | Œî=‚àí(4‚Äã(‚àí3‚Äãb2Œµ2)3+27‚Äã(2‚Äãb2‚ÄãaŒµ2)2)=108‚Äãb4Œµ4‚Äã(b2Œµ2‚àía2)\Delta=-\left(4\left(-\frac{3b^{2}}{\varepsilon^{2}}\right)^{3}+27\left(\frac{2b^{2}a}{\varepsilon^{2}}\right)^{2}\right)=108\frac{b^{4}}{\varepsilon^{4}}\left(\frac{b^{2}}{\varepsilon^{2}}-a^{2}\right) |  |

Therefore,

|  |  |  |
| --- | --- | --- |
|  | {Œî>0‚ü∫Œµ<|b|aŒî=0‚ü∫Œµ=|b|aŒî<0‚ü∫Œµ>|b|a\begin{cases}\Delta>0&\Longleftrightarrow\varepsilon<\frac{|b|}{a}\\ \Delta=0&\Longleftrightarrow\varepsilon=\frac{|b|}{a}\\ \Delta<0&\Longleftrightarrow\varepsilon>\frac{|b|}{a}\end{cases} |  |

To study the root of P‚Äã[X]P[X] we study the three cases separately.

Case 1: We assume that Œµ<|b|a\varepsilon<\frac{|b|}{a}, therefore Œî>0\Delta>0, then from Cardano‚Äôs cubic formulas, P‚Äã[X]P[X] admits three distinct real roots z0,z1,z2z\_{0},z\_{1},z\_{2} such that for k‚àà{0,1,2}k\in\{0,1,2\} :

|  |  |  |
| --- | --- | --- |
|  | zk=2‚Äã|b|Œµ‚Äãcos‚Å°(13‚Äãarccos‚Å°(Œµ‚Äãa|b|)+2‚Äãk‚ÄãœÄ3)z\_{k}=2\frac{|b|}{\varepsilon}\cos\left(\frac{1}{3}\arccos\left(\frac{\varepsilon a}{|b|}\right)+\frac{2k\pi}{3}\right) |  |

Since there exist at least one xx in EŒµE\_{\varepsilon} such that fŒµ,a,b‚Ä≤‚Äã(x)=0f\_{\varepsilon,a,b}^{\prime}(x)=0, we know that one of the root is located in EŒµE\_{\varepsilon} (i.e is such that zk>|b|Œµz\_{k}>\frac{|b|}{\varepsilon}). Remark that

|  |  |  |
| --- | --- | --- |
|  | zk>|b|Œµ‚ü∫cos‚Å°(13‚Äãarccos‚Å°(Œµ‚Äãa|b|)+2‚Äãk‚ÄãœÄ3)>12z\_{k}>\frac{|b|}{\varepsilon}\Longleftrightarrow\cos\left(\frac{1}{3}\arccos\left(\frac{\varepsilon a}{|b|}\right)+\frac{2k\pi}{3}\right)>\frac{1}{2} |  |

From the fact that cos\cos is decreasing on [0,œÄ][0,\pi], 2‚ÄãœÄ2\pi-periodic even and that cos‚Å°(œÄ3)=12\cos(\frac{\pi}{3})=\frac{1}{2} we get that for all x‚àà‚Ñùx\in\mathbb{R}

|  |  |  |  |
| --- | --- | --- | --- |
|  | cos(x)>12‚ü∫‚àíœÄ3<x<œÄ3[2œÄ]\cos(x)>\frac{1}{2}\Longleftrightarrow\frac{-\pi}{3}<x<\frac{\pi}{3}\quad[2\pi] |  | (50) |

Now since arccos(]0,1[)‚äÇ]0,œÄ[\arccos(]0,1[)\subset]0,\pi[ we have that :

|  |  |  |
| --- | --- | --- |
|  | 2‚Äãk‚ÄãœÄ3<13‚Äãarccos‚Å°(Œµ‚Äãa|b|)+2‚Äãk‚ÄãœÄ3<œÄ+2‚Äãk‚ÄãœÄ3\frac{2k\pi}{3}<\frac{1}{3}\arccos\left(\frac{\varepsilon a}{|b|}\right)+\frac{2k\pi}{3}<\pi+\frac{2k\pi}{3} |  |

Among the three intervals ]0,œÄ[]0,\pi[, ]2‚ÄãœÄ3,5‚ÄãœÄ3[\left]\frac{2\pi}{3},\frac{5\pi}{3}\right[, ]4‚ÄãœÄ3,7‚ÄãœÄ3[\left]\frac{4\pi}{3},\frac{7\pi}{3}\right[ only the interval ]0,œÄ[]0,\pi[ contains values that that satisfies Conditions [50](https://arxiv.org/html/2510.18995v1#A1.E50 "In A.4 Proof of Proposition 9 ‚Ä£ Appendix A Appendix ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations"). Therefore we have necessarily that z0‚ààEŒµz\_{0}\in E\_{\varepsilon} and fŒµ,a,b‚Ä≤‚Äã(z0)=0f\_{\varepsilon,a,b}^{\prime}(z\_{0})=0.

Case 2: Now we assume that Œµ=|b|a\varepsilon=\frac{|b|}{a}, therefore Œî=0\Delta=0, in this case from Cardano‚Äôs formulas P‚Äã[X]P[X] admits two real roots (z0,z1)‚àà‚Ñù2(z\_{0},z\_{1})\in\mathbb{R}^{2}

|  |  |  |
| --- | --- | --- |
|  | {z0=3‚Äãbz1=‚àí3‚Äãb\begin{cases}z\_{0}&=3b\\ z\_{1}&=-3b\end{cases} |  |

Since z1<0z\_{1}<0 we have z1‚àâEŒµz\_{1}\notin E\_{\varepsilon}, therefore necessarily z0‚ààEŒµz\_{0}\in E\_{\varepsilon} and fŒµ,a,b‚Ä≤‚Äã(z0)=0f\_{\varepsilon,a,b}^{\prime}(z\_{0})=0.

Case 3: Now we assume that Œµ>|b|a\varepsilon>\frac{|b|}{a}, thus Œî<0\Delta<0. From Cardano‚Äôs formulas P‚Äã[X]P[X] admits one real root z0‚àà‚Ñù2z\_{0}\in\mathbb{R}^{2}

|  |  |  |
| --- | --- | --- |
|  | z0=(bŒµ)23‚Äã[(œÑ+(a2‚àíb2Œµ2)12)13+(a‚àí(a2‚àíb2Œµ2)12)13]z\_{0}=\left(\frac{b}{\varepsilon}\right)^{\frac{2}{3}}\left[\left(\tau+\left(a^{2}-\frac{b^{2}}{\varepsilon^{2}}\right)^{\frac{1}{2}}\right)^{\frac{1}{3}}+\left(a-\left(a^{2}-\frac{b^{2}}{\varepsilon^{2}}\right)^{\frac{1}{2}}\right)^{\frac{1}{3}}\right] |  |

Then a direct application of Lemma [10](https://arxiv.org/html/2510.18995v1#Thmlemma10 "Lemma 10. ‚Ä£ A.4 Proof of Proposition 9 ‚Ä£ Appendix A Appendix ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") with a=œÑa=\tau and b=c1b=c\_{1} yields the result.
‚àé

### A.5 Technical Lemma for Proposition [10](https://arxiv.org/html/2510.18995v1#Thmproposition10 "Proposition 10. ‚Ä£ 5.3 Solvency monitoring of the portfolio ‚Ä£ 5 Risk measurement in a Life-Insurance toy model ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")

In this appendix we present some technical Lemmas used to prove Proposition [10](https://arxiv.org/html/2510.18995v1#Thmproposition10 "Proposition 10. ‚Ä£ 5.3 Solvency monitoring of the portfolio ‚Ä£ 5 Risk measurement in a Life-Insurance toy model ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations") in Section [6](https://arxiv.org/html/2510.18995v1#S6 "6 Numerical experiments ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations").

###### Definition 19.

We define œÅ\rho, the function form (0,+‚àû)2(0,+\infty)^{2} to ‚Ñù\mathbb{R} defined by,

|  |  |  |
| --- | --- | --- |
|  | ‚àÄ(x,x‚Ä≤)‚àà(0,+‚àû)2,œÅ‚Äã(x,x‚Ä≤)=max‚Å°(rg,Œ≥‚Äãln‚Å°(x‚Ä≤x))\forall(x,x^{\prime})\in(0,+\infty)^{2},\quad\rho(x,x^{\prime})=\max\left(r\_{g},\gamma\ln\left(\frac{x^{\prime}}{x}\right)\right) |  |

In particular we have,

|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚àÄt‚àà{1,‚Ä¶,T},rs‚Äã(t)=œÅ‚Äã(St‚àí1,St)\forall t\in\{1,\dots,T\},\quad r\_{s}(t)=\rho(S\_{t-1},S\_{t}) |  | (51) |

###### Lemma 11.

The following holds,

|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚àÄt‚àà{0,‚Ä¶,T},‚àÄu‚àà{0,‚Ä¶,t},M‚ÄãRt=M‚ÄãRu‚Äã‚àèi=u+1t(1‚àídi)‚Äã(1+œÅ‚Äã(Si‚àí1,Si))\forall t\in\{0,\dots,T\},\forall u\in\{0,\dots,t\},\quad MR\_{t}=MR\_{u}\prod\_{i=u+1}^{t}(1-d\_{i})(1+\rho(S\_{i-1},S\_{i})) |  | (52) |

For all t‚àà{0,‚Ä¶,T}t\in\{0,\dots,T\}, let ftf\_{t} be the function from (0,+‚àû)t+1(0,+\infty)^{t+1} to ‚Ñù\mathbb{R} defined by,

|  |  |  |
| --- | --- | --- |
|  | ‚àÄx=(x0,‚Ä¶,xt)‚àà(0,+‚àû)t+1,ft‚Äã(x)=M‚ÄãR0‚Äã‚àèu=1t(1‚àídu)‚Äã(1+œÅ‚Äã(xu‚àí1,xu))\forall x=(x\_{0},\dots,x\_{t})\in(0,+\infty)^{t+1},\quad f\_{t}(x)=MR\_{0}\prod\_{u=1}^{t}(1-d\_{u})(1+\rho(x\_{u-1},x\_{u})) |  |

then,

|  |  |  |  |
| --- | --- | --- | --- |
|  | M‚ÄãRt=ft‚Äã(S0,S1,‚Ä¶,St).MR\_{t}=f\_{t}(S\_{0},S\_{1},\dots,S\_{t})\,. |  | (53) |

In particular M‚ÄãRtMR\_{t} is ‚Ñ±t\mathcal{F}\_{t}-measurable.

###### Proof.

A straightforward induction argument yields the first result. Let t‚àà{0,‚Ä¶,T}t\in\{0,\dots,T\}, clearly ([52](https://arxiv.org/html/2510.18995v1#A1.E52 "In Lemma 11. ‚Ä£ A.5 Technical Lemma for Proposition 10 ‚Ä£ Appendix A Appendix ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")) holds for u=tu=t. Let u‚àà{1,‚Ä¶,t}u\in\{1,\dots,t\} be such that ([52](https://arxiv.org/html/2510.18995v1#A1.E52 "In Lemma 11. ‚Ä£ A.5 Technical Lemma for Proposition 10 ‚Ä£ Appendix A Appendix ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")) holds, then by ([45](https://arxiv.org/html/2510.18995v1#S5.E45 "In item 3 ‚Ä£ 5.2 Description of the life-insurance contract ‚Ä£ 5 Risk measurement in a Life-Insurance toy model ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")) and ([44](https://arxiv.org/html/2510.18995v1#S5.E44 "In item 1 ‚Ä£ 5.2 Description of the life-insurance contract ‚Ä£ 5 Risk measurement in a Life-Insurance toy model ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")),

|  |  |  |
| --- | --- | --- |
|  | M‚ÄãRu=M‚ÄãR~u‚àí1‚Äã(1‚àídu)=M‚ÄãRu‚àí1‚Äã(1+œÅ‚Äã(Su‚àí1,Su))‚Äã(1‚àídu).MR\_{u}=\widetilde{MR}\_{u-1}(1-d\_{u})=MR\_{u-1}(1+\rho(S\_{u-1},S\_{u}))(1-d\_{u})\,. |  |

Then using the induction hypothesis,

|  |  |  |
| --- | --- | --- |
|  | M‚ÄãRt=M‚ÄãRu‚Äã‚àèi=u+1t(1‚àídi)‚Äã(1+œÅ‚Äã(Si‚àí1,Si))=M‚ÄãRu‚àí1‚Äã‚àèi=ut(1‚àídi)‚Äã(1+œÅ‚Äã(Si‚àí1,Si))MR\_{t}=MR\_{u}\prod\_{i=u+1}^{t}(1-d\_{i})(1+\rho(S\_{i-1},S\_{i}))=MR\_{u-1}\prod\_{i=u}^{t}(1-d\_{i})(1+\rho(S\_{i-1},S\_{i})) |  |

proving the fist claim. The second claim is a straightforward application of ([52](https://arxiv.org/html/2510.18995v1#A1.E52 "In Lemma 11. ‚Ä£ A.5 Technical Lemma for Proposition 10 ‚Ä£ Appendix A Appendix ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")) for u=0u=0.
‚àé

###### Lemma 12.

For all t‚àà{0,‚Ä¶,T}t\in\{0,\dots,T\}, and all u‚àà{0,‚Ä¶,t}u\in\{0,\dots,t\}

|  |  |  |  |
| --- | --- | --- | --- |
|  | œït=œïu‚àíM‚ÄãRu‚Äã‚àëi=u+1tdiSi‚Äã‚àèj=u+1i‚àí1(1‚àídj)‚Äã‚àèj=u+1i(1+œÅ‚Äã(Sj‚àí1,Sj)).\phi\_{t}=\phi\_{u}-MR\_{u}\sum\_{i=u+1}^{t}\frac{d\_{i}}{S\_{i}}\prod\_{j=u+1}^{i-1}(1-d\_{j})\prod\_{j=u+1}^{i}(1+\rho(S\_{j-1},S\_{j}))\,. |  | (54) |

Let gtg\_{t} be the function from (0,+‚àû)t+1(0,+\infty)^{t+1} to ‚Ñù\mathbb{R} defined by,

|  |  |  |
| --- | --- | --- |
|  | ‚àÄx=(x0,x1,‚Ä¶,xt)‚àà(0,+‚àû)t+1,gt‚Äã(x)=œï0‚àíM‚ÄãR0‚Äã‚àëi=1tdixi‚Äã‚àèj=u+1i‚àí1(1‚àídj)‚Äã‚àèj=u+1i(1+œÅ‚Äã(xj‚àí1,xj))\forall x=(x\_{0},x\_{1},\dots,x\_{t})\in(0,+\infty)^{t+1},\quad g\_{t}(x)=\phi\_{0}-MR\_{0}\sum\_{i=1}^{t}\frac{d\_{i}}{x\_{i}}\prod\_{j=u+1}^{i-1}(1-d\_{j})\prod\_{j=u+1}^{i}(1+\rho(x\_{j-1},x\_{j})) |  |

then,

|  |  |  |  |
| --- | --- | --- | --- |
|  | œït=gt‚Äã(S0,S1,‚Ä¶,St).\phi\_{t}=g\_{t}(S\_{0},S\_{1},\dots,S\_{t})\,. |  | (55) |

In particular œït\phi\_{t} is ‚Ñ±t\mathcal{F}\_{t}-measurable.

###### Proof.

Let t‚àà{0,‚Ä¶,T}t\in\{0,\dots,T\}, we begin by showing that for all u‚àà{0,‚Ä¶,t}u\in\{0,\dots,t\}

|  |  |  |  |
| --- | --- | --- | --- |
|  | œït=œïu‚àí‚àëi=u+1tM‚ÄãR~iSi\phi\_{t}=\phi\_{u}-\sum\_{i=u+1}^{t}\frac{\widetilde{MR}\_{i}}{S\_{i}} |  | (56) |

A straightforward induction argument prove this claim : clearly ([56](https://arxiv.org/html/2510.18995v1#A1.E56 "In A.5 Technical Lemma for Proposition 10 ‚Ä£ Appendix A Appendix ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")) holds for u=tu=t. Let u‚àà{1,‚Ä¶,t}u\in\{1,\dots,t\} be such that ([56](https://arxiv.org/html/2510.18995v1#A1.E56 "In A.5 Technical Lemma for Proposition 10 ‚Ä£ Appendix A Appendix ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")) holds. Since u‚àà{1,‚Ä¶,t}u\in\{1,\dots,t\},

|  |  |  |
| --- | --- | --- |
|  | œïu=œïu‚àí1+Œî‚Äãœïu=œïu‚àí1‚àíM‚ÄãR~u‚ÄãduSu\phi\_{u}=\phi\_{u-1}+\Delta\phi\_{u}=\phi\_{u-1}-\frac{\widetilde{MR}\_{u}d\_{u}}{S\_{u}} |  |

Therefore from the induction hypothesis we get,

|  |  |  |
| --- | --- | --- |
|  | œït=œïu‚àí‚àëi=u+1tM‚ÄãR~iSi=œïu‚àí1‚àí‚àëi=utM‚ÄãR~iSi\phi\_{t}=\phi\_{u}-\sum\_{i=u+1}^{t}\frac{\widetilde{MR}\_{i}}{S\_{i}}=\phi\_{u-1}-\sum\_{i=u}^{t}\frac{\widetilde{MR}\_{i}}{S\_{i}} |  |

proving the claim. Now, for all i‚àà{u+1,‚Ä¶,t}i\in\{u+1,\dots,t\},

|  |  |  |
| --- | --- | --- |
|  | M‚ÄãR~i=M‚ÄãRi‚àí1‚Äã(1+œÅ‚Äã(Si‚àí1,Si))\widetilde{MR}\_{i}=MR\_{i-1}(1+\rho(S\_{i-1},S\_{i})) |  |

Using ([52](https://arxiv.org/html/2510.18995v1#A1.E52 "In Lemma 11. ‚Ä£ A.5 Technical Lemma for Proposition 10 ‚Ä£ Appendix A Appendix ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")),

|  |  |  |
| --- | --- | --- |
|  | M‚ÄãRi‚àí1=M‚ÄãRu‚Äã‚àèj=u+1i‚àí1(1‚àídj)‚Äã(1+œÅ‚Äã(Sj‚àí1,Sj)).MR\_{i-1}=MR\_{u}\prod\_{j=u+1}^{i-1}(1-d\_{j})(1+\rho(S\_{j-1},S\_{j}))\,. |  |

Therefore,

|  |  |  |  |
| --- | --- | --- | --- |
|  | M‚ÄãR~i=M‚ÄãRu‚Äã‚àèj=u+1i‚àí1(1‚àídj)‚Äã‚àèj=u+1i(1+œÅ‚Äã(Sj‚àí1,Sj)).\widetilde{MR}\_{i}=MR\_{u}\prod\_{j=u+1}^{i-1}(1-d\_{j})\prod\_{j=u+1}^{i}(1+\rho(S\_{j-1},S\_{j}))\,. |  | (57) |

Plugging ([57](https://arxiv.org/html/2510.18995v1#A1.E57 "In A.5 Technical Lemma for Proposition 10 ‚Ä£ Appendix A Appendix ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")) in ([56](https://arxiv.org/html/2510.18995v1#A1.E56 "In A.5 Technical Lemma for Proposition 10 ‚Ä£ Appendix A Appendix ‚Ä£ Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations")) then gives the first claim. The second claim is a straightforward application of the first claim for u=0u=0.
‚àé

###### Lemma 13.

Let t‚àà{1,‚Ä¶,T}t\in\{1,\dots,T\}, we let,

|  |  |  |  |
| --- | --- | --- | --- |
|  | z:=ùîº‚Ñö‚Äã[1+rs‚Äã(t)]=ùîº‚Ñö‚Äã[1+œÅ‚Äã(St‚àí1,St)]z:=\mathbb{E}\_{\mathbb{Q}}[1+r\_{s}(t)]=\mathbb{E}\_{\mathbb{Q}}[1+\rho(S\_{t-1},S\_{t})] |  | (58) |

and

|  |  |  |
| --- | --- | --- |
|  | d:=r‚àíœÉ22‚àírgŒ≥œÉd:=\frac{r-\frac{\sigma^{2}}{2}-\frac{r\_{g}}{\gamma}}{\sigma} |  |

then,

|  |  |  |
| --- | --- | --- |
|  | z=1+rg+Œ≥‚ÄãœÉ‚Äã(Œ¶‚Ä≤‚Äã(d)+d‚ÄãŒ¶‚Äã(d))z=1+r\_{g}+\gamma\sigma(\Phi^{{}^{\prime}}(d)+d\Phi(d)) |  |

where Œ¶\Phi is the c.d.f. of a standard normal random variable.

###### Proof.

Let t‚àà{1,‚Ä¶,T}t\in\{1,\dots,T\}, by definition,

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Ñö‚Äã[rs‚Äã(t)]=rg+ùîº‚Äã[(Œ≥‚Äãln‚Å°(StSt‚àí1)‚àírg)+]\mathbb{E}\_{\mathbb{Q}}[r\_{s}(t)]=r\_{g}+\mathbb{E}\left[\left(\gamma\ln\left(\frac{S\_{t}}{S\_{t-1}}\right)-r\_{g}\right)\_{+}\right] |  |

Now, standard Black-Scholes like calculations gives,

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Ñö‚Äã[(Œ≥‚Äãln‚Å°(StSt‚àí1)‚àírg)+]=Œ≥‚Äãùîº‚Ñö‚Äã[ln‚Å°(StSt‚àí1)‚ÄãùüôŒ≥‚Äãln‚Å°(StSt‚àí1)‚â•rg]‚àírg‚Äã‚Ñö‚Äã(Œ≥‚Äãln‚Å°(StSt‚àí1)‚â•rg)\mathbb{E}\_{\mathbb{Q}}\left[\left(\gamma\ln\left(\frac{S\_{t}}{S\_{t-1}}\right)-r\_{g}\right)\_{+}\right]=\gamma\mathbb{E}\_{\mathbb{Q}}\left[\ln\left(\frac{S\_{t}}{S\_{t-1}}\right)\mathbbm{1}\_{\gamma\ln\left(\frac{S\_{t}}{S\_{t-1}}\right)\geq r\_{g}}\right]-r\_{g}\mathbb{Q}\left(\gamma\ln\left(\frac{S\_{t}}{S\_{t-1}}\right)\geq r\_{g}\right) |  |

Letting NN be a standard normal random variable, then,

|  |  |  |
| --- | --- | --- |
|  | ln‚Å°(StSt‚àí1)‚Äã=Law‚Äãr‚àíœÉ22+œÉ‚ÄãN\ln\left(\frac{S\_{t}}{S\_{t-1}}\right)\overset{\mathrm{Law}}{=}r-\frac{\sigma^{2}}{2}+\sigma N |  |

Therefore,

|  |  |  |
| --- | --- | --- |
|  | ‚Ñö‚Äã(Œ≥‚Äãln‚Å°(StSt‚àí1)‚â•rg)=‚Ñö‚Äã(N‚â•‚àíd)=Œ¶‚Äã(d)\mathbb{Q}\left(\gamma\ln\left(\frac{S\_{t}}{S\_{t-1}}\right)\geq r\_{g}\right)=\mathbb{Q}\left(N\geq-d\right)=\Phi(d) |  |

and,

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Ñö‚Äã[ln‚Å°(StSt‚àí1)‚ÄãùüôŒ≥‚Äãln‚Å°(StSt‚àí1)‚â•rg]=(r‚àíœÉ22)‚ÄãŒ¶‚Äã(d)+œÉ‚Äãùîº‚Ñö‚Äã[N‚ÄãùüôN‚â•‚àíd]=(r‚àíœÉ22)‚ÄãŒ¶‚Äã(d)+œÉ‚ÄãŒ¶‚Ä≤‚Äã(d)\mathbb{E}\_{\mathbb{Q}}\left[\ln\left(\frac{S\_{t}}{S\_{t-1}}\right)\mathbbm{1}\_{\gamma\ln\left(\frac{S\_{t}}{S\_{t-1}}\right)\geq r\_{g}}\right]=\left(r-\frac{\sigma^{2}}{2}\right)\Phi(d)+\sigma\mathbb{E}\_{\mathbb{Q}}[N\mathbbm{1}\_{N\geq-d}]=\left(r-\frac{\sigma^{2}}{2}\right)\Phi(d)+\sigma\Phi^{\prime}(d) |  |

Which yields the result.
‚àé