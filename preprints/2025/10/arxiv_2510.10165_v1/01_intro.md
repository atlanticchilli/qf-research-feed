---
authors:
- Feiyang
- Xu
- Poonacha K. Medappa
- Murat M. Tunc
- Martijn Vroegindeweij
- Jan C. Fransoo
doc_id: arxiv:2510.10165v1
family_id: arxiv:2510.10165
is_current: true
taxonomy:
  alpha_families: []
  asset_classes: []
  horizons: []
  themes: []
title: 1 The workflow of OSS projects. It comprises of a primary branch of a GitHub
  (project), which typically contains the main source code that serves as the foundation
  for new feature development, bug fixes, and updates. Changes to this branch are
  usually controlled through a structured review process conducted by maintainers
  to ensure code quality and prevent issues. To contribute to the project, a PR is
  submitted to propose changes to a project. It allows developers to submit modifications,
  request feedback, and merge updates into the main branch. The review and refinement
  process comprises of two activities that we measure in our study - pull request
  review (PR Review) and pull request rework (PR Rework). Eventually the reviewed
  and reworked changes will either be merged into the main branch or undergo additional
  rounds of review and rework.
url_abs: http://arxiv.org/abs/2510.10165v1
url_html: https://arxiv.org/html/2510.10165v1
venue: arXiv q-fin
version: 1
year: 2025
---

\OneAndAHalfSpacedXI\TheoremsNumberedThrough\ECRepeatTheorems\EquationsNumberedThrough

\TITLE

AI-assisted Programming May Decrease the Productivity of Experienced Developers by Increasing Maintenance Burden

\ARTICLEAUTHORS\AUTHOR

Feiyang (Amber) Xu, Poonacha K. Medappa, Murat M. Tunc
  
Martijn Vroegindeweij, Jan C. Fransoo
\AFFTilburg University, the Netherlands
  
\EMAILf.xu\_1@tilburguniversity.edu, p.k.medappa@tilburguniversity.edu, m.m.tunc@tilburguniversity.edu
  
w.m.vroegindeweij@tilburguniversity.edu, jan.fransoo@tilburguniversity.edu

\ABSTRACT

Generative AI solutions like GitHub Copilot have been shown to increase the productivity of software developers. Yet prior work remains unclear on the quality of code produced and the challenges of maintaining it in software projects. If quality declines as volume grows, experienced developers face increased workloads reviewing and reworking code from less-experienced contributors. We analyze developer activity in Open Source Software (OSS) projects following the introduction of GitHub Copilot. We find that productivity indeed increases. However, the increase in productivity is primarily driven by less-experienced (peripheral) developers. We also find that code written after the adoption of AI requires more rework. Importantly, the added rework burden falls on the more experienced (core) developers, who review 6.5% more code after Copilot’s introduction, but show a 19% drop in their original code productivity. More broadly, this finding raises caution that productivity gains of AI may mask the growing burden of maintenance on a shrinking pool of experts.

\KEYWORDS

Generative AI, GitHub Copilot, Open Source Software, Software Maintenance, Software Quality, Difference-in-Differences

How will AI shape the future of software development? This question has taken on renewed significance with the recent rise of generative AI (GenAI) technologies, which are becoming an integral part of daily operations of software development teams. A prominent example is GitHub Copilot, an AI-powered coding assistant designed to support developers by generating code suggestions and accelerating routine programming tasks. When GitHub launched Copilot, it was introduced as “your AI pair programmer," emphasizing not only its role as an automation tool but also as a team member who partners with the developer to write and test code (Friedman [2021](https://arxiv.org/html/2510.10165v1#bib.bib10)). Unlike earlier code automation tools, the framing of GitHub Copilot as a pair programmer suggests that the future of software development will increasingly involve Human-AI pair programming.

For organizations and communities involved in software development, the addition of AI pair programmers in teams offers the potential for significant productivity gains. Recent research shows that developers who use GitHub Copilot complete their programming tasks 55.8% faster (Peng et al. [2023](https://arxiv.org/html/2510.10165v1#bib.bib18)). Such productivity benefits lead to promises of faster time-to-market and increased revenue for organizations developing software applications. Considering this, major tech organizations have started to increasingly rely on AI in their projects - “more than a quarter of all new code at Google is generated by AI, then reviewed and accepted by engineers," reported Google CEO Sundar Pichai in January, 2025.111https://www.technologyreview.com/2025/01/20/1110180/the-second-wave-of-ai-coding-is-here/ While these productivity gains are promising, they also raise important questions about the quality and maintainability of AI-generated code. Because AI tools can lower the skill barrier for writing code (Dakhel et al. [2023](https://arxiv.org/html/2510.10165v1#bib.bib6)), developers may increasingly rely on them without fully understanding the implications of the code being produced (Barrett et al. [2023](https://arxiv.org/html/2510.10165v1#bib.bib2)). This growing reliance introduces new risks such as security vulnerabilities, bugs, and suboptimal solutions that may undermine the long-term stability and maintainability of software systems.

The potential challenges that AI poses to software quality and security are expected to be especially pronounced in distributed software development teams, such as in Open Source Software (OSS) communities. In these communities, contributors from around the world collaborate, often voluntarily, to develop and maintain software that form the digital infrastructure of our society (e.g., Linux, Apache, LaTeX, Python), making it freely or cheaply available to the public (Eghbal [2020](https://arxiv.org/html/2510.10165v1#bib.bib9), Nagle [2019](https://arxiv.org/html/2510.10165v1#bib.bib14)). Despite the voluntary nature of work in these communities, the value of OSS (the estimated cost for firms to build equivalent software internally) is estimated at $8.8 trillion (Hoffmann et al. [2024](https://arxiv.org/html/2510.10165v1#bib.bib12)). 222These estimates suggests that firms would spend approximately 3.5 times more on software than they currently do if OSS did not exist (Hoffmann et al. [2024](https://arxiv.org/html/2510.10165v1#bib.bib12)). Importantly, OSS is not only a source of low cost software but also a model that many firms now adopt in their own development practices. Microsoft, once a vocal critic of open source, has become a key advocate—open-sourcing its core technologies such as the .NET Framework, TypeScript, PowerShell, and VSCode since the mid-2010s.333https://opensource.microsoft.com/ This shift reflects a broader industry trend toward embracing OSS as a development model for their software applications (Nagle [2019](https://arxiv.org/html/2510.10165v1#bib.bib14)).

Given this critical role for both firms and society, the growing adoption of AI in OSS communities raises important questions about its broader impact. On the one hand, AI tools can lower the barrier for peripheral contributors (contributors who come from the community of users of software; Rullani and Haefliger [2013](https://arxiv.org/html/2510.10165v1#bib.bib19)) to contribute to these software projects. On the other hand, this surge in AI-assisted contributions may introduce new maintenance burdens, particularly around code quality and security (Eghbal [2020](https://arxiv.org/html/2510.10165v1#bib.bib9)). In this study, we examine how AI adoption is reshaping the OSS development workflow by introducing new maintenance challenges, ultimately raising concerns about the long-term sustainability and reliability of the digital infrastructure that our society increasingly depends on.

The quality maintenance of OSS communities is typically carried out by a small group of experienced (core) contributors who take on the role of gatekeepers of the community. They play a crucial role in ensuring the project’s quality, stability, and sustenance by reviewing contributions, managing releases, and guiding the community (Eghbal [2020](https://arxiv.org/html/2510.10165v1#bib.bib9)). These contributors are deeply embedded in the community and often take on the “less desirable" maintenance related activities that are necessary for the upkeep of the community (Eghbal [2020](https://arxiv.org/html/2510.10165v1#bib.bib9), Nagle et al. [2020](https://arxiv.org/html/2510.10165v1#bib.bib15), Nagle [2019](https://arxiv.org/html/2510.10165v1#bib.bib14)). While these core contributors are critical for the OSS community, they are often few, under-incentivized and overworked (Eghbal [2020](https://arxiv.org/html/2510.10165v1#bib.bib9)). Consequently, the current state of much of the OSS infrastructure is characterized by an over-reliance on the efforts of a few core contributors to perform all the maintenance work (Osborne [2024](https://arxiv.org/html/2510.10165v1#bib.bib17)). For example, the backbone of many data science applications is the Python module “pandas”, which is used by over 2.7 million users. The “pandas” OSS project has been developed with contributions from more than 3,400 contributors who have helped build its features over time.444https://github.com/pandas-dev/pandas Despite its wide adoption, the project maintenance has rested on the shoulders of only about 35 core contributors, of whom just 15 are currently active.555https://pandas.pydata.org/about/team.html666This point was raised in a keynote by Fernando Perez, the founder of IPython/Jupyter, at the Euro SciPy conference in 2011, where he remarked that - https://numfocus.org/blog/why-is-numpy-only-now-getting-funded

“Python in science is a great success story, but the entire edifice rests on (often the spare time of)  30 people.”
Now, considering this, imagine a surge of new contributors submitting code to these projects (which might come from the productivity increase brought by AI), requiring the attention of the already overburdened maintainers. An increased workload could lead to delays in reviewing and merging the submitted code, greater difficulty in validating code quality, and potentially a higher risk of introducing vulnerabilities into the critical digital infrastructure.

To understand the impact of the adoption of AI pair programming on the maintenance of OSS communities, we examined how the OSS development workflow changes before and after the deployment of GitHub Copilot. The development workflow of an OSS project is illustrated in Figure [1](https://arxiv.org/html/2510.10165v1#S0.F1 "Figure 1"). A feature of this development workflow is that it allows multiple contributors to work independently on separate branches (or forks) and submit their changes for inclusion in the main project branch. This is done through a pull request (PR), which serves as a formal proposal to merge code. Core or experienced contributors often handle key maintenance tasks: they review submitted PRs for quality and compliance, suggest improvements or corrections, and integrate approved changes into the main project. This workflow enables distributed development and community-driven innovation, allowing peripheral contributors who come from the community of users of the project to contribute. At the same time, it ensures that the code is reviewed, refined, and improved before merging into the main branch. The effectiveness of this review process has helped OSS achieve remarkably high quality, surpassing proprietary software in metrics like bugs per 1,000 lines of code.777https://blogs.worldbank.org/en/opendata/quality-open-source-software-how-many-eyes-are-enough However, the success and effectiveness of this "decentralized-development" approach hinges on the core contributors who actively participate in the review-rework process of the PRs submitted by different contributors (Rullani and Haefliger [2013](https://arxiv.org/html/2510.10165v1#bib.bib19)).

![Refer to caption](x1.jpg)


Figure 1: The workflow of OSS projects. It comprises of a primary branch of a GitHub (project), which typically contains the main source code that serves as the foundation for new feature development, bug fixes, and updates. Changes to this branch are usually controlled through a structured review process conducted by maintainers to ensure code quality and prevent issues. To contribute to the project, a PR is submitted to propose changes to a project. It allows developers to submit modifications, request feedback, and merge updates into the main branch. The review and refinement process comprises of two activities that we measure in our study - pull request review (PR Review) and pull request rework (PR Rework). Eventually the reviewed and reworked changes will either be merged into the main branch or undergo additional rounds of review and rework.

Our main objective is to examine whether the review and rework effort on PRs changed after the introduction of GitHub Copilot. To empirically test this, we exploit the release of GitHub Copilot as a technical preview in June 2021, which included limited programming language endorsement. We focus on OSS projects owned by Microsoft, as the company had exclusive access to OpenAI’s GPT-3, the model powering GitHub Copilot during its technical preview, due to its investment in OpenAI and its prior acquisition of GitHub.888https://www.technologyreview.com/2020/09/23/1008729/openai-is-giving-microsoft-exclusive-access-to-its-gpt-3-language-model/999https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/thomas-dohmke-on-improving-engineering-experience-using-generative-ai The individual users in our dataset are contributors to Microsoft-owned OSS projects. We estimate the effect of Copilot at both the project and contributor levels using a Difference-in-Differences (DiD) design. Treatment and control groups were defined based on the primary programming language: those using Copilot-endorsed languages formed the treatment group, while non-endorsed language users served as the control (Yeverechyahu et al. [2024](https://arxiv.org/html/2510.10165v1#bib.bib23)). For both project and contributor levels, we collected data on programming activities and aggregated them at the monthly level.

We studied the code productivity increase introduced by Copilot by lines of code added, commits101010A commit is the fundamental unit of change on GitHub. Similar to saving a file that’s been edited, a commit records changes to one or more files on GitHub - https://docs.github.com/en/pull-requests/committing-changes-to-your-project/creating-and-editing-commits/about-commits and PRs submitted. Maintenance-related activities were assessed through PR reviews and PR rework, which measures the extent to which contributions initially submitted to the project need to be revised before they are integrated into the project. Based on our analysis of a large-scale panel dataset from GitHub, covering 2,755 projects and 1,699 contributors, we find that while AI adoption leads to productivity gains, they also increase maintenance-related activities due to a higher volume of review and rework needed per PR.
Specifically, our analysis reveals a double-edged effect of GitHub Copilot on OSS development. At the project level, Copilot adoption is associated with a significant boost in productivity: projects that supported Copilot saw increases in lines of code added, commits, and PRs. However, this surge in contributions came also with an increase in PR rework (2.4% more code rework), indicating greater maintenance demands and possible declines in the quality of code initially submitted. At the contributor level, we observe an important redistribution of effort: less experienced, peripheral contributors increased their development activity, taking advantage of Copilot’s ability to lower coding barriers. Specifically, peripheral contributors, particularly those in the bottom percentiles in terms of experience, increased their commit activity by 43.5% and submitted 17.7% more PRs. In contrast, the core contributors reduced their commit activity by 19%, shifting their focus toward reviewing and maintaining code (a 6.5% increase), and shouldering a heavier quality assurance burden. Together, these findings highlight how AI can enable broader participation in OSS, but also raise concerns about the sustainability of these gains and the strain placed on a shrinking pool of experienced contributors who maintain quality in OSS projects.

## 1 Results

### 1.1 Project (Repository) Level Analysis

We conducted a DiD analysis using a panel dataset of OSS projects hosted on GitHub. The technical preview of GitHub Copilot was launched on June 29, 2021, providing a natural experiment for studying Copilot’s effect on code quality and maintainability. We focus on OSS projects owned by Microsoft, as the company had exclusive access to the technology behind GitHub Copilot, and therefore had greater access to the tool during its technical preview.111111In September 2020, Microsoft gained exclusive access to OpenAI’s GPT3. This access allowed Microsoft to repurpose and modify the model for code generation, leading to the development of GitHub Copilot. In June 2021, GitHub Copilot was launched as a technical preview. Anecdotal evidence and our interviews of Microsoft employees indicates that during the technical preview, access to Copilot was restricted to selected GitHub users, specifically employees of Microsoft/GitHub and maintainers of popular projects. This restricted access suggests that the primary users of Copilot during the technical preview were likely Microsoft and GitHub employees, along with selected Github uses who volunteered for the beta testing the software.121212Dog Fooding; is Microsoft speak for internal use of their own software, often to stress test its new software before public release - https://devblogs.microsoft.com/oldnewthing/20110802-00/?p=10003131313https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/thomas-dohmke-on-improving-engineering-experience-using-generative-ai We defined our observation period as the 12 months before and after this date, spanning from July 2020 to July 2022. During the technical preview of Copilot, it was endorsed for five programming languages — Python, JavaScript, Ruby, TypeScript, and Go.141414https://github.blog/news-insights/product-news/introducing-github-copilot-ai-pair-programmer/ At the project level, we define the treatment group as repositories whose primary programming language was among those endorsed by GitHub Copilot during the technical preview, while repositories using non-endorsed languages (e.g., C, C#, C++, Java, PHP, R, Scala) serve as the control group. Our project-level dataset consists of 2,755 repositories, with 1,660 in the Copilot-endorsed treatment group and 1,095 in the non-endorsed control group.

On GitHub, a PR is a vehicle of contribution through which contributors participate in the development process. A PR typically contains one or more commits and often represents a "patch" or feature addition to the project. Any individual can submit a PR (a request to merge their contribution into the project), which is then (peer) reviewed by the core contributors, who have write access to the source code. The core contributors reviewing the PR can decide to merge the PR into the main project, request modifications or, reject it. If modifications are requested for a PR, the author of the PR can address the comments and modification requests and re-submit the code in the form of follow-up commits for another round of review. This rework process continues until all issues with the code are resolved and the code can be merged, or until the idea behind the code is no longer in alignment with the project goals and the PR is rejected. Considering this, we can measure the amount of rework done on a PR submitted by a contributor by identifying the number of commits that are added to the PR after its initial submission. We use the PR rework effort measure as a proxy to determine the initial quality of code contributions submitted to the project and the maintenance-related efforts associated with the code contributions.

We analyzed the number of lines of code added, commits, and PRs as code development activities, and PR rework as maintenance related activities. Table [1](https://arxiv.org/html/2510.10165v1#S1.T1 "Table 1 ‣ 1.1 Project (Repository) Level Analysis ‣ 1 Results") presents the estimations. After the introduction of GitHub Copilot, repositories whose primary programming language was endorsed by Copilot (treatment) experienced a significant increase in code development activities, such as the number of lines of code added (β=0.163\beta=0.163, S​E=0.06SE=0.06; t=2.73t=2.73; p<0.01p<0.01; 95%95\% C​ICI, (0.046,0.28)(0.046,0.28)), commits (β=0.04\beta=0.04, S​E=0.022SE=0.022; t=1.84t=1.84; p<0.1p<0.1; 95%95\% C​ICI, (−0.003,0.082)(-0.003,0.082)), and PRs (β=0.042\beta=0.042, S​E=0.015SE=0.015; t=2.81t=2.81; p<0.01p<0.01; 95%95\% C​ICI, (0.013,0.071)(0.013,0.071)). This increase of productivity aligns with the analysis of industry experts (Peng et al. [2023](https://arxiv.org/html/2510.10165v1#bib.bib18)) and the findings of recent academic research (Yeverechyahu et al. [2024](https://arxiv.org/html/2510.10165v1#bib.bib23)).

Table 1: The impact of technical preview on development and maintenance activities.

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
| Activity: | Development | | | Maintenance |
| DV: | Lines of Added Code | Commits | Pull Requests | PR Rework |
| Copilot | 0.163\*\*\* | 0.04\* | 0.042\*\*\* | 0.024\*\* |
|  | (0.06) | (0.022) | (0.015) | (0.01) |
| Project FE | ✓ | ✓ | ✓ | ✓ |
| Month FE | ✓ | ✓ | ✓ | ✓ |
| PR Controls |  |  |  | ✓ |
| N | 66,120 | 66,120 | 66,168 | 66,168 |
| Adj. R2R^{2} | 0.516 | 0.603 | 0.691 | 0.81 |

* *Note:* All DVs are log-transformed. Robust standard errors clustered at the project level are presented in parentheses. ∗p<<0.1; ∗∗p<<0.05; ∗∗∗p<<0.01

More importantly, we find that the submitted PRs requires greater amount of rework, indicating a likely decrease in the quality of the submitted PRs. When analyzing the PRs submitted, the increased amount of code resubmissions remains significant (β=0.024\beta=0.024, S​E=0.01SE=0.01; t=2.35t=2.35; p<0.05p<0.05; 95%95\% C​ICI, (0.004,0.043)(0.004,0.043)), even when controlling for the number of PRs, indicating an increased demand for code reviews per unit of code development. Specifically, we find that treatment repositories experienced 2.4% more code rework, keeping the number of PR submitted constant. This increase likely reflects a higher volume of lower-quality code submissions to the project.

Figure [2](https://arxiv.org/html/2510.10165v1#S1.F2 "Figure 2 ‣ 1.1 Project (Repository) Level Analysis ‣ 1 Results") presents the event time analysis of the treatment effect on PR rework (leads-lags estimates). The coefficients for the pre-treatment periods are statistically insignificant, indicating that the parallel trends assumption holds (Angrist and Pischke [2009](https://arxiv.org/html/2510.10165v1#bib.bib1)). The leads-lags estimate also indicate a long-term effect of Copilot. With greater deployment and adoption of AI, we expect increased contributor engagement and more frequent usage of Copilot. There is an upward trend in the post-treatment coefficients for PR rework that gradually become more positive over time (regression results are provided in the Supplementary Materials, Section 5.2). The observed pattern suggests a growing trend of increased code rework in repositories that supported AI coding partners.

![Refer to caption](x2.png)


Figure 2: Parallel trends and dynamic effects of the Copilot treatment for pull request rework. The horizontal axis represents the months relative to introduction of Copilot, while the vertical axis shows the estimated coefficients with confidence intervals (95%). The coefficients for the pre-treatment periods (leads) are statistically insignificant, indicating that there are no systematic differences in trends between the treated and control groups before the introducing of Copilot. This suggests that the parallel trends assumption holds, supporting the validity of our DiD estimation.

### 1.2 Contributor Level Analysis:

To understand how AI adoption affected contributors to OSS projects, we analyzed their contribution behavior before and after the introduction of GitHub Copilot. At the contributor level, we define treated contributors as those whose primary development activity is in Copilot-endorsed languages, while contributors working primarily in non-endorsed languages serve as the control group. Similar to our project level analysis, we use the technical preview of Copilot as our treatment period and perform DiD analysis at the contributor level. In total, we collected an individual level dataset consisting of 1,699 contributors from GitHub, with 1,186 in the Copilot-supported treatment group and 513 in the non-Copilot-supported control group.151515There are 37,334 contributors who made at least one contribution to the repositories in our sample. The contributions include, for example, posting a comment, submitting a commit, or conducting a pull request review. Among them, we filtered out 5,308 contributors who participated in more than three of the repositories we studied (we selected three repos to ensure sufficient variation for our PR reviewed repositories measure) . Then, we applied a programming language filter to construct a comparable treatment and control group for the individual-level analysis, and the data set was reduced to 1,699 contributors (who were users of the treated and control programming languages). Following the conceptualization of core contributors based on activity from past literature (Setia et al. [2012](https://arxiv.org/html/2510.10165v1#bib.bib20), Crowston et al. [2006](https://arxiv.org/html/2510.10165v1#bib.bib5)), we used contributors’ level of activity during the pretreatment period, measured by the number of commits, to classify contributors into core contributors (top 25%) and peripheral contributors (rest 75%). From Figure [3](https://arxiv.org/html/2510.10165v1#S1.F3 "Figure 3 ‣ 1.2 Contributor Level Analysis: ‣ 1 Results"), we observe that core contributors perform the majority of development activities (both in terms of commits and PR) in the projects.

![Refer to caption](Figures_202406/hist_commit_bw.png)

![Refer to caption](Figures_202406/hist_pr_bw.png)

Figure 3: Histogram of contributions for core and peripheral contributors: The core contributors decreased the development activities after the deployment of Copilot. The peripheral contributors displayed opposite behaviour.

We next examined commits and PRs as code development activities, and PR reviews (PR controlled) and PR reviewed repositories (PR controlled) as maintenance related activities. Based on our DiD analysis (Table 2), we find that core contributors engage in fewer development activities and in more maintenance activities after the launch of Copilot. The core contributors performed significant less code commits (β=−0.357\beta=-0.357, S​E=0.052SE=0.052; t=−6.83t=-6.83; p<0.01p<0.01; 95%95\% C​ICI, (−0.46,−0.255)(-0.46,-0.255)) submissions. At the same time, the core contributors reviewed more PRs and do so across a broader set of repositories (β=0.045\beta=0.045, S​E=0.018SE=0.018; t=2.42t=2.42; p<0.05p<0.05; 95%95\% C​ICI, (0.008,0.081)(0.008,0.081)). This shift suggests that core contributors are not only spending more of their limited time on maintenance tasks but also spreading themselves thinner across more repositories. The resulting burden of maintenance may come at the cost of reduced productivity among more experienced contributors.

To further examine the shift in workload across different contributor groups, we conducted a serious of subgroup DiD analyses based on the volume of contributions made by GitHub contributors. We calculated the quantity of commits submitted during the pretreatment period, and split the contributors accordingly into four percentile groups: 0 to 25%, 25% to 50%, 50% to 75% and 75% to 100%. The results are presented in Figure [4](https://arxiv.org/html/2510.10165v1#S1.F4 "Figure 4 ‣ 1.2 Contributor Level Analysis: ‣ 1 Results"). The statistical results are presented in supplementary analysis (Section 4.2).

Compared to the control group, we observe a consistent decline in commit volume among core contributors, while peripheral contributors show the opposite trend. Core contributors shifted towards more maintenance work, with a 19% decrease in commits and a 6.5% increase in PR reviews. In contrast, peripheral contributors, particularly those in the bottom percentiles, increased their commit activity by 43.5% and submitted 17.7% more PRs. This pattern suggests that while Copilot lowered the barriers for peripheral contributors to participate, it placed a greater maintenance burden on core contributors, redirecting their efforts from development-related to maintenance-related activities.

Table 2: The impact of technical preview on GitHub’s core contributor behaviour.

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
| Activity: | Development | | Maintenance | |
| DV: | Commits | Pull Request | PR Review | PR Reviewed Repos |
| Copilot | 0.142\*\*\* | 0.091\*\*\* | 0.04 | 0.008 |
|  | (0.408) | (0.03) | (0.023) | (0.012) |
| Core Contributor | -0.357\*\*\* | -0.027 | 0.06\* | 0.045\*\* |
| ×\times Copilot | (0.052) | (0.042) | (0.035) | (0.018) |
| Individual FE | ✓ | ✓ | ✓ | ✓ |
| Month FE | ✓ | ✓ | ✓ | ✓ |
| PR Controlled |  |  | ✓ | ✓ |
| N | 51,408 | 51,408 | 51,408 | 51,408 |
| Adj. R2R^{2} | 0.643 | 0.607 | 0.799 | 0.757 |

* *Note:* All DVs are log-transformed. Robust standard errors clustered at the individual level are presented in parentheses. ∗p<<0.1; ∗∗p<<0.05; ∗∗∗p<<0.01



![Refer to caption](Figures_202406/maintainer_commits.png)


(a) Commits.

![Refer to caption](Figures_202406/maintainer_PR.png)


(b) Pull Request.

![Refer to caption](Figures_202406/maintainer_PRR.png)


(c) PR Reviews.

![Refer to caption](Figures_202406/maintainer_PRRR.png)


(d) PR Reviewed Repos.

Figure 4: Contribution activities analysis by contributor subgroup. Panels show estimated coefficients (converted to %) from DiD regressions with 95% confidence intervals, capturing the relative change in activity post-Copilot exposure compared to control repositories. (a) Commits: Conversely, commit activity declines progressively with contributor experience, with the top 25% experiencing a 19% reduction, suggesting reduced hands-on coding engagement. (b) Pull Requests: The most peripheral contributors (0–25%) significantly increase their PR submissions (17.7%), highlighting increased participation from less experienced developers. (c) PR Reviews: The top 25% of contributors (core) exhibit a significant increase in review activity, suggesting a shift of responsibility towards the core contributors. (d) PR Reviewed Repositories: Similarly, only the core contributor group shows a meaningful rise in the number of distinct repositories reviewed, indicating a broader oversight role.

## 2 Discussion

Recent research suggests that the adoption of GenAI technologies brings not only productivity gains but also a host of unintended secondary effects. These include anchoring bias (Chen and Chan [2024](https://arxiv.org/html/2510.10165v1#bib.bib4)), reinforcement of societal and algorithmic biases (Williams-Ceci et al. [2025](https://arxiv.org/html/2510.10165v1#bib.bib22), Nicoletti and Bass [2023](https://arxiv.org/html/2510.10165v1#bib.bib16)), reduced collective diversity (Doshi and Hauser [2024](https://arxiv.org/html/2510.10165v1#bib.bib7)), dialect prejudice (Hofmann et al. [2024](https://arxiv.org/html/2510.10165v1#bib.bib13)), and snowball effects that amplify initial biases over time (Glickman and Sharot [2025](https://arxiv.org/html/2510.10165v1#bib.bib11)). Meta-analyses further show that in decision-making tasks, human-AI teams can perform worse than either alone, despite outperforming in content generation tasks (Vaccaro et al. [2024](https://arxiv.org/html/2510.10165v1#bib.bib21)).

In software development communities like OSS, the key concern is how AI affects code quality and software security. By lowering the skill barrier for writing code (Dakhel et al. [2023](https://arxiv.org/html/2510.10165v1#bib.bib6)), AI tools can encourage broader participation but may also lead developers to rely on AI-generated code without fully understanding its implications (Barrett et al. [2023](https://arxiv.org/html/2510.10165v1#bib.bib2)). This growing dependency increases the risk of bugs, security vulnerabilities, and fragile or suboptimal code being introduced into production. In the face of unprecedented demand for OSS infrastructure (Nagle [2019](https://arxiv.org/html/2510.10165v1#bib.bib14)), the cost of such quality-related issues can be severe.
As a case in point, in 2011 a major vulnerability (nicknamed Heartbleed) in an OpenSSL OSS project was overlooked and included in an update of the project. It went unnoticed for years, allowing any sophisticated hacker to capture secure information being passed to vulnerable web servers, including passwords, credit card information, and other sensitive data. The widely held view on Heartbleed underscores the risks of under-resourced maintenance related activities in OSS projects: :161616https://mashable.com/archive/heartbleed-bug-websites-affected

> “The mystery is not that a few overworked volunteers missed this bug; the mystery is why it hasn’t happened more often” (Eghbal [2016](https://arxiv.org/html/2510.10165v1#bib.bib8), p. 13).

Our study sheds light on the hidden maintenance burdens AI introduces into OSS development communities. While AI-assisted coding lowers entry barriers and attracts peripheral (often less experienced) contributors, potentially fueling innovation, it also introduces substantial maintenance challenges. Chief among these are longer rework cycles and declining code quality, which heighten the risk of future security issues. More critically, the burden of reviewing and correcting AI-generated code increasingly falls on experienced core contributors diverting their attention from creative tasks like writing new code to routine and reactive maintenance work such as reviewing and reworking others’ contributions (Eghbal [2020](https://arxiv.org/html/2510.10165v1#bib.bib9), [2016](https://arxiv.org/html/2510.10165v1#bib.bib8)).

In sum, this research contributes to our understanding of AI driven programming tools in three key ways. First, we find that the introduction of Copilot increased programmer productivity, resulting in a higher volume of code contributions, measured through commits and PRs, at the repository level. At the individual level, OSS contributors submit more PRs after the adoption of GitHub Copilot. This finding aligns with the industry study conducted by Microsoft (Peng et al. [2023](https://arxiv.org/html/2510.10165v1#bib.bib18)). Second, and more importantly, we provide empirical evidence illustrating how AI adoption can also subsequently increase the need for maintenance related activities in OSS communities. We find that the code submissions (PRs) after the introduction of AI require more rework. This increase in rework adds complexity to the review process before PRs can be merged, potentially undermining the project’s quality. Poor-quality code can result in more bugs, increased rework, and eventual large-scale refactoring of the entire project (Barrett et al. [2023](https://arxiv.org/html/2510.10165v1#bib.bib2)). Lastly, we examine the heterogeneous effects of AI adoption across different groups of contributors. Our results show that core contributors, not only review more PRs while contributing fewer commits, but also extend their maintenance related activities across a wider range of repositories. This suggests that AI-assisted code submissions from less experienced developers may shift the burden onto core maintainers, ultimately undermining their productivity. As these key contributors become overextended, it may increase the risk of unresolved bugs and security vulnerabilities, threatening the long-term sustainability of OSS projects.

To illustrate the scale of Copilot’s impact on OSS communities, Microsoft core contributors in our dataset conduct on average, 976 commits, 160 PRs, and 166 PR reviews annually before its introduction. The increased volume of code associated with Copilot adoption results in an additional workload – each core contributor is expected to review approximately 10 more pull requests annually. This added maintenance burden corresponds to a reduction of 164 commits and 9 pull request contributions per year per core contributor.
More critically, GitHub’s 2024 surveys reveal that more than one-third of contributors to the 10 most popular OSS projects made their first contribution after signing up for GitHub Copilot, highlighting a significant influx of new and often less experienced developers171717https://github.blog/news-insights/octoverse/octoverse-2024/.
With annual contributions to OSS projects approaching 1 billion, this surge in participation significantly increases the burden on core contributors, who take on the maintenance related tasks in the project. As a result, maintainers are compelled to reallocate their time toward reviewing and managing code submissions instead of writing new code.

Prior beliefs have largely emphasized the productivity and efficiency gains of AI pair programming, fueling expectations that tools like Copilot would significantly accelerate software development (Peng et al. [2023](https://arxiv.org/html/2510.10165v1#bib.bib18)). While these benefits are real, our results reveal a more complex picture. Specifically, we find that Copilot also amplifies software maintenance challenges—particularly for core contributors—as evidenced by our individual-level analysis. The reliance on AI tools may also impact the foundational learning of peripheral developers. With AI providing quick solutions, there is a risk that developers may not fully engage with the underlying principles of coding nor the best coding practices, leading to a superficial understanding. This gap in comprehension can result in code that is functional but lacks robustness, making future maintenance more challenging. This concern aligns with findings from AI-assisted customer support settings, where less experienced workers see large productivity gains, while more skilled workers experience only modest improvements (Brynjolfsson et al. [2025](https://arxiv.org/html/2510.10165v1#bib.bib3)).

The implications of AI adoption in OSS communities extend well beyond the boundaries of OSS development. As OSS practices and tools are increasingly integrated into corporate and institutional workflows, the quality, reliability, and sustainability challenges observed in OSS are likely to appear in other areas of software development. Many firms now rely heavily on OSS libraries, frameworks, and platforms, and increasingly adopt the open-source approach to develop software and products (Nagle [2019](https://arxiv.org/html/2510.10165v1#bib.bib14)). As a result, any decline in code quality or strain on maintenance capacity within OSS can cascade into broader software ecosystems, affecting enterprise systems, digital services, and public-sector infrastructure that depend on these components.

As a concluding remark, the challenges observed in OSS, such as quality concerns and increased maintenance burdens driven by productivity gains among lower-skilled contributors, should serve as an early warning for similar risks in other domains where AI is being promoted to boost productivity and innovation.

## 3 Methods

### 3.1 Research Design

Data startsJuly 2020GitHub Copilot Technical Preview June 29, 2021Data ends July 2022Pre-Treatment PeriodPost-Treatment PeriodTreatment Group: Python, JavaScript, Ruby, TypeScript and GoControl Group: R, C, C#, C++, Java, PHP and Scala


Figure 5: The timeline of our study period: the technical preview of GitHub Copilot was launched on June 29, 2021, initially endorsing five programming languages – Python, JavaScript, Ruby, TypeScript, and Go. We designed our study to include the 12 months before and the 12 months after Copilot’s introduction.

In June 29, 2021, developed on OpenAI’s GPT-3 model, GitHub launched a technical preview version of Copilot – the very first AI pair programmer. This early version of GitHub Copilot was not open to the public and endorsed only five programming languages: Python, JavaScript, Ruby, TypeScript, and Go.181818https://github.blog/news-insights/product-news/introducing-github-Copilot-ai-pair-programmer/ Copilot was later launched to the public in June 2022, with contributors required to pay a monthly fee to subscribe to the AI pair programming service. With the public release in June 2022, Copilot gradually added endorsement for more languages such as, C and Java. As shown in Figure [5](https://arxiv.org/html/2510.10165v1#S3.F5 "Figure 5 ‣ 3.1 Research Design ‣ 3 Methods"), we define our observation period as 12 months before and 12 months after its introduction. The measures were aggregated to create a monthly panel dataset spanning from July 2020 to July 2022.

Our study leverages this natural experiment created by the launch of GitHub Copilot, an AI-powered LLM designed to assist with coding. Specifically, we take advantage of the early stage Copilot’s limited language endorsement, which included languages of Python, JavaScript, Ruby, TypeScript, and Go, while excluding others comparable languages of R, C, C#, C++, Java, PHP and Scala. We select these five language as the non-Copilot-endorsed languages because of their comparable functionality and also they are the most frequently used languages for Microsoft-owned repositories. Our identification strategy by programming language between treatment and control group are similar to those in past studies (Yeverechyahu et al. [2024](https://arxiv.org/html/2510.10165v1#bib.bib23)).

Anecdotal evidence191919https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/thomas-dohmke-on-improving-engineering-experience-using-generative-ai and our interviews of Microsoft employees indicates that during the technical preview, access to Copilot was restricted to selected GitHub contributors, specifically employees of Microsoft/GitHub and maintainers of popular repositories. This restricted access suggests that the primary contributors of Copilot during the technical preview were likely Microsoft and GitHub employees, ensuring that the tool was stress tested (Dog Fooding; is Microsoft speak for internal use of their own software202020https://devblogs.microsoft.com/oldnewthing/20110802-00/?p=10003) internally before its broader release.

Since Copilot usage cannot be identified at the individual level, we focus on Microsoft-owned repositories and contributors who actively contributed to these repositories during the observation period. By doing so, we aim to capture changes in OSS contributor behaviour driven by Copilot, as contributors to Microsoft-owned repositories are more likely to have free access to the tool.

We use GitHub’s API service to collect all the data for this research, enabling efficient and precise querying of repository/individual activities. This approach enables us to gather detailed information on measurements such as pull requests, commits, reviews, authors, and repository/individual metadata, providing a comprehensive dataset for analyzing the impact of AI-assisted coding on OSS project development and maintenance.

We report the summary statistics for key variables used in our analysis. For the project level analysis, the binary variable Copilot indicates the treatment (treated repository\*post copilot), with a mean of 0.30 and a standard deviation of 0.46. The mean log of lines of code added is 1.33 (SD = 2.88). The mean log of the commit count is 0.47 (SD = 1.05) and the mean log of PRs is 0.36 (SD = 0.83). The average log of PR rework – defined as the number of commits added to a PR after its submission – is 0.27 with a standard deviation of 0.89.

For the individual level analysis, the binary indicator for Copilot indicates the treatment (treated contributor\*post Copilot), with a mean of 0.366 and a standard deviation of 0.482. The log-transformed number of commits submitted by each contributor per month has a mean of 1.75 (SD = 1.69), ranging from 0 to a maximum of 10.12. The number of PRs submitted, also log-transformed, has a mean of 0.96 (SD = 1.16), with a maximum value of 6.20. The log number of PR reviews conducted has a mean of 0.79 (SD = 1.21), reaching a maximum of 6.03. Lastly, the number of repositories where users conducted PR reviews, log-transformed, has a mean of 0.45 (SD = 0.66), with a maximum value of 5.21.

Table 3: Descriptive statistics for repository level analysis.

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
| Variable | Mean | Std. Dev. | Min | Max |
| Copilot (Dummy) | 0.301 | 0.459 | 0 | 1 |
| Code Added (log) | 1.328 | 2.883 | 0 | 17.034 |
| Commits (log) | 0.466 | 1.052 | 0 | 8.075 |
| PRs (log) | 0.358 | 0.834 | 0 | 6.789 |
| PR Rework (log) | 0.273 | 0.894 | 0 | 7.536 |




Table 4: Descriptive Statistics for individual level analysis.

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
| Variable | Mean | Std. Dev. | Min | Max |
| Copilot (Dummy) | 0.366 | 0.482 | 0 | 1 |
| Commits (log) | 1.750 | 1.694 | 0 | 10.116 |
| PRs (log) | 0.955 | 1.155 | 0 | 6.201 |
| PR Reviews (log) | 0.789 | 1.212 | 0 | 6.028 |
| PR Reviewed Repos (log) | 0.454 | 0.664 | 0 | 5.215 |

### 3.2 Statistical analysis

To estimate the effect of Copilot on repositories and individuals, we used two difference in difference (DiD) regression model. The project level DiD model (repo i month t) is provided below:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Repository Level Effecti,t=β0+β1​C​o​p​i​l​o​ti,t+γi+δt+ϵi,t\textit{Repository Level Effect}\_{i,t}=\beta\_{0}+\beta\_{1}Copilot\_{i,t}+\gamma\_{i}+\delta\_{t}+\epsilon\_{i,t} |  | (1) |

The individual level DiD model (contributor i month t) is provided below:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Individual Level Effecti,t=β0+β1​C​o​p​i​l​o​ti,t+β2​Core Contributori×C​o​p​i​l​o​ti,t+γi+δt+ϵi,t\begin{split}\textit{Individual Level Effect}\_{i,t}=\beta\_{0}+\beta\_{1}Copilot\_{i,t}+\beta\_{2}\textit{Core Contributor}\_{i}\times Copilot\_{i,t}+\gamma\_{i}+\delta\_{t}+\epsilon\_{i,t}\end{split} |  | (2) |

where yi,ty\_{i,t} refers to the outcome measures (development and maintenance) for project / individual ii in month tt. C​o​p​i​l​o​ti,tCopilot\_{i,t} is the independent variable and a binary indicator that turns to 1 when the Copilot is released and functioned as our treatment.
C​o​r​e​C​o​n​t​r​i​b​u​t​o​riCoreContributor\_{i} is the moderator and a binary indicator that turns to 1 when one individual is identified as core contributor by pretreatment code contribution behaviour. The project-level / individual-level fixed effects are represented by γi\gamma\_{i}, and δt\delta\_{t} represents the monthly fixed effects. ϵi,t\epsilon\_{i,t} is the robust standard error clustered at the project / individual level to account for the potential heteroskedasticity of the errors.

### 3.3 Matching results

As a robustness check, we employed Coarsened Exact Matching (CEM) to mitigate concerns about potential selection bias and ensure a more balanced comparison between treatment and control groups. CEM allows us to pre-process the data by matching units on a set of covariates that may influence both treatment assignment and outcomes. We matched repositories based on pretreatment code development characteristics, including Code Added, Commits, and Pull Request. After matching, the repository level dataset consisting of n = 2,510 repositories, with 1,486 in the treatment group and 1,024 in the control group. The treatment and control groups were more closely aligned in their baseline characteristics, reducing imbalance across key variables. We then re-estimated the DiD models using the matched sample. The results are listed in the table below. The results remain consistent with our main findings, reinforcing the conclusion that the integration of GitHub Copilot is associated with increased rework and maintenance related activities.

Table 5: Balance Statistics: Unmatched and Matched Samples

|  |  |  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Variable | Unmatched Sample | | | | Matched Sample | | | |
| Treated | Control | t-stat | p | Treated | Control | t-stat | p |
| Code Added (log) | 3.6703.670 | 4.1434.143 | −2.648-2.648 | 0.0080.008 | 3.2983.298 | 3.5193.519 | −1.253-1.253 | 0.2100.210 |
| Commits (log) | 1.4701.470 | 1.6641.664 | −2.524-2.524 | 0.0120.012 | 1.3201.320 | 1.4021.402 | −1.105-1.105 | 0.2690.269 |
| Pull Request (log) | 1.0661.066 | 1.2021.202 | −2.149-2.149 | 0.0320.032 | 1.0091.009 | 0.9740.974 | −0.565-0.565 | 0.5720.572 |




Table 6: The impact of technical preview on project development and code quality measures after CEM Matching.

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
| Concept: | Development | | | Maintenance |
| DV: | Code Added | Commit | Pull Request | PR Rework |
| Copilot | 0.202\*\*\* | 0.051\*\* | 0.038\*\*\* | 0.019\*\* |
|  | (0.06) | (0.021) | (0.015) | (0.01) |
| Project FE | ✓ | ✓ | ✓ | ✓ |
| Month FE | ✓ | ✓ | ✓ | ✓ |
| PR Controls |  |  |  | ✓ |
| N | 60,240 | 60,240 | 60,240 | 60,240 |
| Adj. R2R^{2} | 0.463 | 0.517 | 0.634 | 0.779 |

* *Note:* All DVs are log-transformed. Robust standard errors clustered at the project level are presented in parentheses. ∗p<<0.1; ∗∗p<<0.05; ∗∗∗p<<0.01

## References

* Angrist and Pischke (2009)

  Angrist JD, Pischke JS (2009) *Mostly Harmless Econometrics: An Empiricist’s Companion* (Princeton University Press).
* Barrett et al. (2023)

  Barrett C, Boyd B, Bursztein E, Carlini N, Chen B, Choi J, Chowdhury AR, Christodorescu M, Datta A, Feizi S, et al. (2023) Identifying and Mitigating the Security Risks of Generative AI. *Foundations and Trends® in Privacy and Security* 6(1):1–52.
* Brynjolfsson et al. (2025)

  Brynjolfsson E, Li D, Raymond LR (2025) Generative AI at Work. *The Quarterly Journal of Economics* 140(2):889––942.
* Chen and Chan (2024)

  Chen Z, Chan J (2024) Large Language Model in Creative Work: The Role of Collaboration Modality and User Expertise. *Management Science* 70(12):9101–9117.
* Crowston et al. (2006)

  Crowston K, Wei K, Li Q, Howison J (2006) Core and periphery in free/libre and open source software team communications. *Proceedings of the 39th annual Hawaii international conference on system sciences (HICSS’06)*, volume 6, 118a–118a (IEEE).
* Dakhel et al. (2023)

  Dakhel AM, Majdinasab V, Nikanjam A, Khomh F, Desmarais MC, Jiang ZM (2023) GitHub Copilot AI Pair Programmer: Asset or Liability? *Journal of Systems and Software* 203:111734.
* Doshi and Hauser (2024)

  Doshi AR, Hauser OP (2024) Generative AI Enhances Individual Creativity but Reduces the Collective Diversity of Novel Content. *Science Advances* 10(28):eadn5290.
* Eghbal (2016)

  Eghbal N (2016) *Roads and Bridges: The Unseen Labor Behind Our Digital Infrastructure* (Ford Foundation).
* Eghbal (2020)

  Eghbal N (2020) *Working in Public: The Making and Maintenance of Open Source Software* (Stripe Press).
* Friedman (2021)

  Friedman N (2021) Introducing GitHub Copilot: Your AI Pair Programmer. URL https://github.blog/news-insights/product-news/introducing-github-copilot-ai-pair-programmer/.
* Glickman and Sharot (2025)

  Glickman M, Sharot T (2025) How Human–AI Feedback Loops Alter Human Perceptual, Emotional and Social judgements. *Nature Human Behaviour* 9(2):345–359.
* Hoffmann et al. (2024)

  Hoffmann M, Nagle F, Zhou Y (2024) The value of open source software. *Harvard Business School Strategy Unit Working Paper No. 24-038* .
* Hofmann et al. (2024)

  Hofmann V, Kalluri PR, Jurafsky D, King S (2024) AI Generates Covertly Racist Decisions about People Based on Their Dialect. *Nature* 633(8028):147–154.
* Nagle (2019)

  Nagle F (2019) Open Source Software and Firm Productivity. *Management Science* 65(3):1191–1215.
* Nagle et al. (2020)

  Nagle F, Wheeler DA, Lifshitz-Assaf H, Ham H, Hoffman JL (2020) Report on the 2020 FOSS Contributor Survey. Technical report, The Linux Foundation.
* Nicoletti and Bass (2023)

  Nicoletti L, Bass D (2023) Humans Are Biased. Generative AI Is Even Worse. *Bloomberg* URL https://www.bloomberg.com/graphics/2023-generative-ai-bias/.
* Osborne (2024)

  Osborne C (2024) Public-private Funding Models in Open Source Software Development: A Case Study on Scikit-learn. *arXiv preprint* 2404.06484.
* Peng et al. (2023)

  Peng S, Kalliamvakou E, Cihon P, Demirer M (2023) The Impact of AI on Developer Productivity: Evidence from Github Copilot. *arXiv preprint arXiv:2302.06590* .
* Rullani and Haefliger (2013)

  Rullani F, Haefliger S (2013) The Periphery on Stage: The Intra-organizational Dynamics in Online Communities of Creation. *Research Policy* 42(4):941–953.
* Setia et al. (2012)

  Setia P, Rajagopalan B, Sambamurthy V, Calantone R (2012) How Peripheral Developers Contribute to Open-Source Software Development. *Information Systems Research* 23(1):144–163.
* Vaccaro et al. (2024)

  Vaccaro M, Almaatouq A, Malone T (2024) When Combinations of Humans and AI are Useful: A Systematic Review and Meta-Analysis. *Nature Human Behaviour* 8(12):2293–2303.
* Williams-Ceci et al. (2025)

  Williams-Ceci S, Jakesch M, Bhat A, Kadoma K, Zalmanson L, Naaman M (2025) Biased AI Writing Assistants Shift Users’ Attitudes on Societal Issues. *PsyArXiv Preprints* .
* Yeverechyahu et al. (2024)

  Yeverechyahu D, Mayya R, Oestreicher-Singer G (2024) The Impact of Large Language Models on Open-Source Software Innovation: Evidence from GitHub Copilot. *SSRN Electronic Journal* 4684662.

## 4 Supplementary Materials

### 4.1 Project Level Lead Lag Analysis

We conducted a lead-lag analysis to examine the dynamic effects of GitHub Copilot adoption on repositories’ performance over time. Using a 24-month window centered around the technical preview release: 12 months before and 12 months after, we estimated the monthly treatment effects relative to the month of launch. The coefficients of the analysis is listed below:

Table 7: Regression Results for PR Rework

|  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- |
| Variable | Coefficient | Std. Err. | tt | P>|t|P>|t| | 95% CI (Lower) | 95% CI (Upper) |
| b12 | 0.0058 | 0.0216 | 0.27 | 0.789 | -0.0366 | 0.0482 |
| b11 | 0.0099 | 0.0207 | 0.48 | 0.632 | -0.0306 | 0.0504 |
| b10 | -0.0080 | 0.0223 | -0.36 | 0.720 | -0.0518 | 0.0358 |
| b9 | -0.0004 | 0.0222 | -0.02 | 0.987 | -0.0438 | 0.0431 |
| b8 | -0.0115 | 0.0210 | -0.55 | 0.586 | -0.0527 | 0.0298 |
| b7 | -0.0313 | 0.0228 | -1.37 | 0.170 | -0.0759 | 0.0134 |
| b6 | -0.0335 | 0.0235 | -1.43 | 0.154 | -0.0796 | 0.0125 |
| b5 | -0.0289 | 0.0218 | -1.33 | 0.184 | -0.0715 | 0.0138 |
| b4 | -0.0192 | 0.0233 | -0.82 | 0.411 | -0.0648 | 0.0265 |
| b3 | -0.0315 | 0.0232 | -1.36 | 0.174 | -0.0770 | 0.0140 |
| b2 | 0.0190 | 0.0219 | 0.87 | 0.385 | -0.0239 | 0.0619 |
| b1 | Baseline | | | | | |
| a0 | 0.0005 | 0.0240 | 0.02 | 0.983 | -0.0466 | 0.0477 |
| a1 | 0.0342 | 0.0256 | 1.34 | 0.181 | -0.0159 | 0.0843 |
| a2 | 0.0124 | 0.0266 | 0.47 | 0.641 | -0.0398 | 0.0647 |
| a3 | -0.0094 | 0.0268 | -0.35 | 0.725 | -0.0620 | 0.0431 |
| a4 | 0.0250 | 0.0265 | 0.94 | 0.346 | -0.0269 | 0.0769 |
| a5 | 0.0456 | 0.0246 | 1.85 | 0.064 | -0.0027 | 0.0940 |
| a6 | 0.0767 | 0.0259 | 2.96 | 0.003 | 0.0260 | 0.1274 |
| a7 | 0.0543 | 0.0269 | 2.02 | 0.044 | 0.0015 | 0.1072 |
| a8 | 0.0393 | 0.0286 | 1.37 | 0.169 | -0.0167 | 0.0953 |
| a9 | 0.0770 | 0.0267 | 2.88 | 0.004 | 0.0247 | 0.1294 |
| a10 | 0.0957 | 0.0278 | 3.44 | 0.001 | 0.0412 | 0.1503 |
| a11 | 0.0551 | 0.0298 | 1.85 | 0.065 | -0.0034 | 0.1136 |
| \_cons | 0.2416 | 0.0186 | 12.98 | 0.000 | 0.2051 | 0.2781 |

### 4.2 Individual Level Sub Group Analysis

The individual level subgroup DiD model (contributor i month t) is provided below:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Subgroup Individual Level Effecti,t=β0+β1​C​o​p​i​l​o​ti,t×S​u​b​G​r​o​u​pi+δt+ϵi,t\begin{split}\textit{Subgroup Individual Level Effect}\_{i,t}=\beta\_{0}+\beta\_{1}Copilot\_{i,t}\times SubGroup\_{i}+\delta\_{t}+\epsilon\_{i,t}\end{split} |  | (3) |

where yi,ty\_{i,t} refers to the outcome measures (development and maintenance) for individual ii in month tt. C​o​p​i​l​o​ti,tCopilot\_{i,t} is the IV and a binary indicator that turns to 1 when the Copilot is released and functioned as our treatment.
S​u​b​G​r​o​u​piSubGroup\_{i} is the moderator and a binary indicator that turns to 1 when one individual belongs to a subset by pretreatment code contribution behaviour. The individual-level fixed effects are represented by γi\gamma\_{i}, and δt\delta\_{t} represents the monthly fixed effects. ϵi,t\epsilon\_{i,t} is the robust standard error clustered at the individual level to account for the potential heteroskedasticity of the errors.

We estimate the effect of Copilot on four subgroup of contributors based on the pretreatment contribution: 0 to 25%, 25% to 50%, 50% to 75% and 75% to 100%. The analysis are conducted for each of our outcome measures: PR Reviews, PR Reviewed Repos, commits and PRs. The PR Reviews, PR Reviewed Repos have PR controlled. The statistical results are presented in the following tables.

Table 8: Regression Results for PR Reviews (PR controlled)

|  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- |
|  | Coefficient | Std. Err. | tt-value | P>|t|P>|t| | 95% CI (Lower) | 95% CI (Upper) |
| Subgroup 0%-25% | 0.0099 | 0.0243 | 0.41 | 0.685 | -0.0379 | 0.0577 |
| Subgroup 25%-50% | 0.0117 | 0.0300 | 0.39 | 0.697 | -0.0472 | 0.0706 |
| Subgroup 50%-75% | -0.0073 | 0.0352 | -0.21 | 0.835 | -0.0764 | 0.0617 |
| Subgroup 75%-100% | 0.0628 | 0.0366 | 1.72 | 0.086 | -0.0089 | 0.1345 |




Table 9: Regression Results for PR Reviewed Repos (PR controlled)

|  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- |
|  | Coefficient | Std. Err. | tt-value | P>|t|P>|t| | 95% CI (Lower) | 95% CI (Upper) |
| Subgroup 0%-25% | 0.0061 | 0.0143 | 0.43 | 0.670 | -0.0220 | 0.0342 |
| Subgroup 25%-50% | 0.0078 | 0.0163 | 0.48 | 0.630 | -0.0241 | 0.0397 |
| Subgroup 50%-75% | 0.0125 | 0.0179 | 0.70 | 0.486 | -0.0227 | 0.0477 |
| Subgroup 75%-100% | 0.0530 | 0.0192 | 2.76 | 0.006 | 0.0154 | 0.0906 |




Table 10: Regression Results for Commits

|  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- |
|  | Coefficient | Std. Err. | tt-value | P>|t|P>|t| | 95% CI (Lower) | 95% CI (Upper) |
| Subgroup 0%-25% | 0.3614 | 0.0447 | 8.08 | 0.000 | 0.2737 | 0.4492 |
| Subgroup 25%-50% | 0.1115 | 0.0555 | 2.01 | 0.045 | 0.0026 | 0.2205 |
| Subgroup 50%-75% | -0.0381 | 0.0610 | -0.62 | 0.532 | -0.1578 | 0.0815 |
| Subgroup 75%-100% | -0.2149 | 0.0551 | -3.90 | 0.000 | -0.3230 | -0.1068 |




Table 11: Regression Results for PR

|  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- |
|  | Coefficient | Std. Err. | tt-value | P>|t|P>|t| | 95% CI (Lower) | 95% CI (Upper) |
| Subgroup 0%-25% | 0.1630 | 0.0317 | 5.15 | 0.000 | 0.1009 | 0.2252 |
| Subgroup 25%-50% | 0.0756 | 0.0399 | 1.89 | 0.059 | -0.0027 | 0.1539 |
| Subgroup 50%-75% | 0.0373 | 0.0465 | 0.80 | 0.423 | -0.0540 | 0.1285 |
| Subgroup 75%-100% | 0.0644 | 0.0444 | 1.45 | 0.147 | -0.0227 | 0.1515 |

## 5 Replication Instructions

All data and code used in this study are publicly available at the following GitHub repository: https://github.com/NATHUMBEHAV-25073099/Replication. The repository includes Stata .do files to reproduce all results in the manuscript, processed datasets, and GraphQL queries used for data collection from GitHub. Detailed instructions are provided in the README.md file.