---
authors:
- Gabriele Visentin
- Patrick Cheridito
doc_id: arxiv:2510.15458v1
family_id: arxiv:2510.15458
is_current: true
taxonomy:
  alpha_families: []
  asset_classes: []
  horizons: []
  themes: []
title: "Robust Optimization in Causal Models and \U0001D43A-Causal Normalizing Flows"
url_abs: http://arxiv.org/abs/2510.15458v1
url_html: https://arxiv.org/html/2510.15458v1
venue: arXiv q-fin
version: 1
year: 2025
---


Gabriele Visentin
  
Department of Mathematics
  
ETH Zurich
  
Zurich, Switzerland
  
gabriele.visentin@math.ethz.ch
  
&Patrick Cheridito
  
Department of Mathematics
  
ETH Zurich
  
Zurich, Switzerland
  
patrick.cheridito@math.ethz.ch

###### Abstract

In this paper, we show that interventionally robust optimization problems in causal models are continuous under the GG-causal Wasserstein distance, but may be discontinuous under the standard Wasserstein distance. This highlights the importance of using generative models that respect the causal structure when augmenting data for such tasks. To this end, we propose a new normalizing flow architecture that satisfies a universal approximation property for causal structural models and can be efficiently trained to minimize the GG-causal Wasserstein distance. Empirically, we demonstrate that our model outperforms standard (non-causal) generative models in data augmentation for causal regression and mean-variance portfolio optimization in causal factor models.

## 1 Introduction

Solving optimization problems often requires generative data augmentation (Chen etÂ al., [2024](https://arxiv.org/html/2510.15458v1#bib.bib8); Zheng etÂ al., [2023](https://arxiv.org/html/2510.15458v1#bib.bib26)), particularly when out-of-sample distributional shifts are expected to be frequent and severe, as in the case of financial applications. In such cases, only the most recent data points are representative enough to be used in solving downstream tasks (such as hedging, regression or portfolio selection), resulting in small datasets that require generative data augmentation to avoid overfitting (Bailey etÂ al., [2017](https://arxiv.org/html/2510.15458v1#bib.bib4)). However, when using generative models for data augmentation, it is essential to choose their training loss in a way that is compatible with the downstream tasks, so as to guarantee good and stable performance.

It is well-known, for instance, that multi-stage stochastic optimization problems are continuous under the *adapted* Wasserstein distance, while they may be discontinuous under the standard Wasserstein distance (Pflug & Pichler, [2012](https://arxiv.org/html/2510.15458v1#bib.bib19); [2014](https://arxiv.org/html/2510.15458v1#bib.bib20); Backhoff-Veraguas etÂ al., [2020](https://arxiv.org/html/2510.15458v1#bib.bib3)). This insight prompted several authors to propose new time-series generative models that attempt to minimize the adapted Wasserstein distance, either partially (Xu etÂ al., [2020](https://arxiv.org/html/2510.15458v1#bib.bib24)) or its one-sided111Also known in the literature as the causal Wasserstein distance, because it respects the temporal flow of information in the causal direction (from past to present). This terminology conflicts with the way the term â€œcausalâ€ is used in causal modelling. To avoid misunderstandings we talk of the â€œGG-causalâ€ Wasserstein distance and refer to the causal Wasserstein distance as the â€œone-sidedâ€ adapted Wasserstein distance. version (Acciaio etÂ al., [2024](https://arxiv.org/html/2510.15458v1#bib.bib1)).

In this paper we prove a generalization of this result for causal models. Specifically, we show that causal optimization problems (i.e. problems in which the control variables can depend only on the parents of the state variables in the underlying causal DAG GG) are continuous with respect to the GG-causal Wasserstein distance (Cheridito & Eckstein, [2025](https://arxiv.org/html/2510.15458v1#bib.bib9)).

Furthermore, we prove that solutions to GG-causal optimization problems are always interventionally robust. This means that causal optimization can be understood as a way of performing Distributionally Robust Optimization (DRO) (Chen etÂ al., [2020](https://arxiv.org/html/2510.15458v1#bib.bib7); Kuhn etÂ al., [2025](https://arxiv.org/html/2510.15458v1#bib.bib14)) by taking into account the problemâ€™s causal structure.

Next, we address the challenge of designing a generative model capable of good approximations under the GG-causal Wasserstein distance. We radically depart from existing approaches for the adapted Wasserstein distance and propose a novel GG-causal normalizing flow model based on invertible neural couplings that respect the causal structure of the data. We prove a universal approximation property for this model class and that maximum likelihood training indeed leads to distributions that are close to the target distribution in the GG-causal Wasserstein distance. Since the standard, adapted and CO-OT Wasserstein distances are all special cases of the GG-causal Wasserstein distance, this model family provides optimal generative augmentation models for a vast class of empirical applications.

Contributions. Our main contributions are the following:

* â€¢

  We prove that causal optimization problems (i.e. problems in which optimizers must be functions of the state variablesâ€™ parents in the causal DAG GG) are continuous under the GG-causal Wasserstein distance, but may be discontinuous under the standard Wasserstein distance.
* â€¢

  We prove that solutions to GG-causal optimization problems are always interventionally robust.
* â€¢

  We introduce GG-causal normalizing flows and we prove that they satisfy a universal approximation property for causal structural models under very mild conditions.
* â€¢

  We prove that GG-causal normalizing flows minimize the GG-causal Wasserstein distance between data and model distribution by simple likelihood maximization.
* â€¢

  We show empirically that GG-causal normalizing flows outperform non-causal generative models (such as variational auto-encoders, standard normalizing flows, and nearest-neighbor KDE) when used to perform generative data augmentation in two empirical setups: causal regression and mean-variance portfolio optimization in causal factor models.

## 2 Background

Notation. We denote by âˆ¥â‹…âˆ¥\|\cdot\| the Euclidean norm on â„d\mathbb{R}^{d} and by Lpâ€‹(Î¼)L^{p}(\mu) the space Lpâ€‹(â„d,â„¬â€‹(â„d),Î¼)L^{p}(\mathbb{R}^{d},\mathcal{B}(\mathbb{R}^{d}),\mu) equipped with the norm â€–fâ€–Lpâ€‹(Î¼):=(âˆ«â„dâ€–fâ€‹(z)â€–pâ€‹Î¼â€‹(dâ€‹z))1/p\|f\|\_{L^{p}(\mu)}:=\left(\int\_{\mathbb{R}^{d}}\|f(z)\|^{p}\mu(dz)\right)^{1/p}. ğ’«â€‹(â„d)\mathcal{P}(\mathbb{R}^{d}) denotes the space of all Borel probability measures on â„d\mathbb{R}^{d}. ğ’©â€‹(Î¼,Î£)\mathcal{N}(\mu,\Sigma) is the multivariate Gaussian distribution with mean Î¼\mu and covariance matrix Î£\Sigma, ğ’°â€‹([0,1]d)\mathcal{U}([0,1]^{d}) is the uniform distribution on the dd-dimensional hypercube, IdI\_{d} denotes the dÃ—dd\times d identity matrix.

We use set-indices to slice vectors, i.e. if x=(x1,â€¦,xd)âˆˆâ„dx=(x\_{1},\ldots,x\_{d})\in\mathbb{R}^{d} and AâŠ†{1,â€¦,d}A\subseteq\{1,\ldots,d\}, then xA:=(xi,iâˆˆA)âˆˆâ„|A|x\_{A}:=(x\_{i},i\in A)\in\mathbb{R}^{|A|}. If Î¼âˆˆğ’«â€‹(â„d)\mu\in\mathcal{P}(\mathbb{R}^{d}) and X=(X1,â€¦,Xd)âˆ¼Î¼X=(X\_{1},\ldots,X\_{d})\sim\mu, then the regular conditional distribution of XAX\_{A} given XBX\_{B} is denoted by Î¼â€‹(dâ€‹xA|xB)\mu(dx\_{A}|x\_{B}), for all A,BâŠ†{1,â€¦,d}A,B\subseteq\{1,\ldots,d\} with Aâˆ©B=âˆ…A\cap B=\emptyset.

### 2.1 Structural Causal Models

We assume throughout that G=(V,E)G=(V,E) is a given directed acyclic graph (DAG) with a finite index set V={1,â€¦,d}V=\{1,\ldots,d\}, which we assume, without loss of generality, to be sorted (i.e. (i,j)âˆˆE(i,j)\in E, then i<ji<j). If AâŠ†VA\subseteq V, we denote by PAâ€‹(A):={iâˆˆVâˆ–A|âˆƒjâˆˆA|(i,j)âˆˆE}\text{PA}(A):=\{i\in V\setminus A\>|\>\exists j\in A\>|\>(i,j)\in E\} the set of parents of the vertices in AA (notice that PAâ€‹(A)âŠ†Vâˆ–A\text{PA}(A)\subseteq V\setminus A by definition).

In this paper, we work with structural causal models, as presented in Peters etÂ al. ([2017](https://arxiv.org/html/2510.15458v1#bib.bib18)).

###### Definition 2.1 (Structural Causal Model (SCM)).

Given a DAG G=(V,E)G=(V,E), a Structural Causal Model (SCM) is a collection of assignments

|  |  |  |
| --- | --- | --- |
|  | Xi:=fiâ€‹(XPAâ€‹(i),Ui),for allÂ i=1,â€¦,d,X\_{i}:=f\_{i}(X\_{\text{PA}(i)},U\_{i}),\quad\text{for all $i=1,\ldots,d$}, |  |

where the noise variables (Ui,i=1,â€¦,d)(U\_{i},i=1,\ldots,d) are mutually independent.

### 2.2 GG-causal Wasserstein distance

###### Definition 2.2 (G-compatible distribution).

A distribution Î¼âˆˆğ’«â€‹(â„d)\mu\in\mathcal{P}(\mathbb{R}^{d}) is said to be GG-compatible, and we denote it by Î¼âˆˆğ’«Gâ€‹(â„d)\mu\in\mathcal{P}\_{G}(\mathbb{R}^{d}), if any of the following equivalent conditions holds:

1. 1.

   there exist a random vector X=(X1,â€¦,Xd)âˆ¼Î¼X=(X\_{1},\ldots,X\_{d})\sim\mu together with measurable functions fi:â„|PAâ€‹(i)|Ã—â„â†’â„f\_{i}:\mathbb{R}^{|\text{PA}(i)|}\times\mathbb{R}\to\mathbb{R}, (i=1,â€¦,ni=1,\ldots,n), and mutually independent random variables (Ui,i=1,â€¦,d)(U\_{i},i=1,\ldots,d) such that
   X\_i = f\_i(X\_PA(i), U\_i), â€ƒfor all i=1,â€¦,di=1,\ldots,d.
2. 2.

   For every Xâˆ¼Î¼X\sim\mu, one has
   X\_i âŸ‚â€‹â€‹â€‹ âŸ‚X\_1:i-1 â€… â€” â€…X\_PA(i), â€ƒfor all i=2,â€¦,di=2,\ldots,d.
3. 3.

   The distribution Î¼\mu admits the following disintegration:
   Î¼(dx\_1, â€¦, dx\_d) = âˆ\_i=1^d Î¼(dx\_i â€… â€” â€…x\_PA(i)).

For a proof of the equivalence of these three conditions, see Cheridito & Eckstein ([2025](https://arxiv.org/html/2510.15458v1#bib.bib9), Remark 3.2).

###### Definition 2.3 (GG-bicausal couplings).

A coupling Ï€âˆˆÎ â€‹(Î¼,Î½)\pi\in\Pi(\mu,\nu) between two distributions Î¼,Î½âˆˆğ’«Gâ€‹(â„d)\mu,\nu\in\mathcal{P}\_{G}(\mathbb{R}^{d}) is GG-causal if there exist (X,Xâ€²)âˆ¼Ï€(X,X^{\prime})\sim\pi such that

|  |  |  |
| --- | --- | --- |
|  | Xiâ€²=giâ€‹(Xi,XPAâ€‹(i),XPAâ€‹(i)â€²,Ui)X^{\prime}\_{i}=g\_{i}(X\_{i},X\_{\text{PA}(i)},X^{\prime}\_{\text{PA}(i)},U\_{i}) |  |

for some measurable mappings (gi)i=1d(g\_{i})\_{i=1}^{d} and mutually independent random variables (Ui)i=1d(U\_{i})\_{i=1}^{d}. If also the distribution of (Xâ€²,X)(X^{\prime},X) is GG-causal, then we say that Ï€\pi is GG-bicausal. We denote by Î Gbcâ€‹(Î¼,Î½)\Pi\_{G}^{\text{bc}}(\mu,\nu) the set of all GG-bicausal couplings between Î¼\mu and Î½\nu.

###### Definition 2.4 (GG-causal Wasserstein distance).

Denote by ğ’«G,1â€‹(â„d)\mathcal{P}\_{G,1}(\mathbb{R}^{d}) the space of all GG-compatible distributions with finite first moments. Then the GG-causal Wasserstein distance between Î¼,Î½âˆˆğ’«G,1â€‹(â„d)\mu,\nu\in\mathcal{P}\_{G,1}(\mathbb{R}^{d}) is defined as:

|  |  |  |
| --- | --- | --- |
|  | WGâ€‹(Î¼,Î½):=infÏ€âˆˆÎ Gbcâ€‹(Î¼,Î½)âˆ«â„dÃ—â„dâ€–xâˆ’xâ€²â€–â€‹Ï€â€‹(dâ€‹x,dâ€‹xâ€²).W\_{G}(\mu,\nu):=\inf\_{\pi\in\Pi\_{G}^{\text{bc}}(\mu,\nu)}\int\_{\mathbb{R}^{d}\times\mathbb{R}^{d}}\|x-x^{\prime}\|\,\pi(dx,dx^{\prime}). |  |

Furthermore, WGW\_{G} defines a semi-metric on the space ğ’«G,1â€‹(â„d)\mathcal{P}\_{G,1}(\mathbb{R}^{d}) (Cheridito & Eckstein, [2025](https://arxiv.org/html/2510.15458v1#bib.bib9), Proposition 4.3).

## 3 Robust optimization in Structural Causal Models

Suppose we are given an SCM Xâˆ¼Î¼âˆˆğ’«Gâ€‹(â„d)X\sim\mu\in\mathcal{P}\_{G}(\mathbb{R}^{d}) on a DAG G=(V,E)G=(V,E) and we want to solve a stochastic optimization problem in which the state variables XTX\_{T} are specified by a vertex subset TâŠ†VT\subseteq V (called the *target set*) and the control variables can potentially be all remaining vertices in the graph, i.e. XVâˆ–TX\_{V\setminus T}. To avoid feedback loops between state and control variables, we will need the following technical assumption.

###### Assumption 3.1.

The DAG G=(V,E)G=(V,E) and the target set TâŠ†VT\subseteq V are such that GG quotiened by the partition {T}âˆª{{i},iâˆˆVâˆ–T}\{T\}\cup\{\{i\},i\in V\setminus T\} is a DAG.

###### Remark 3.2.

[3.1](https://arxiv.org/html/2510.15458v1#S3.Thmtheorem1 "Assumption 3.1. â€£ 3 Robust optimization in Structural Causal Models â€£ Robust Optimization in Causal Models and ğº-Causal Normalizing Flows") is quite mild and is equivalent to asking that if i,jâˆˆTi,j\in T, then XiX\_{i} cannot be the parent of a parent of XjX\_{j}. This guarantees that PAâ€‹(T)âˆ©CHâ€‹(T)=âˆ…\text{PA}(T)\cap\text{CH}(T)=\emptyset, which is nothing but asking that XTX\_{T} be part of a valid SCM *as a random vector*, see [Fig.Â 2](https://arxiv.org/html/2510.15458v1#S3.F2 "In 3 Robust optimization in Structural Causal Models â€£ Robust Optimization in Causal Models and ğº-Causal Normalizing Flows") and [2](https://arxiv.org/html/2510.15458v1#S3.F2 "Figure 2 â€£ 3 Robust optimization in Structural Causal Models â€£ Robust Optimization in Causal Models and ğº-Causal Normalizing Flows").

![Refer to caption](x1.png)


Figure 1: DAG GG before quotienting (target set TT highlighted).

![Refer to caption](x2.png)


Figure 2: DAG GG after quotienting (vertex set PAâ€‹(T)\text{PA}(T) highlighted).

###### Definition 3.3 (GG-causal function).

Given a target set TâŠ†VT\subseteq V, we say that a function h:â„|Vâˆ–T|â†’â„|T|h:\mathbb{R}^{|V\setminus T|}\to\mathbb{R}^{|T|} is GG-causal (with respect to TT) if hh depends only on the parents of XTX\_{T}, i.e. hâ€‹(x)=hâ€‹(xPAâ€‹(T))h(x)=h(x\_{\text{PA}(T)}), for all xâˆˆâ„|Vâˆ–T|x\in\mathbb{R}^{|V\setminus T|}.

###### Definition 3.4 (GG-causal optimization problem).

Let G=(V,E)G=(V,E) be a sorted DAG, Xâˆ¼Î¼âˆˆğ’«Gâ€‹(â„d)X\sim\mu\in\mathcal{P}\_{G}(\mathbb{R}^{d}) and let TâŠ†VT\subseteq V be a target set.
If Q:â„|T|Ã—â„|Vâˆ–T|â†’â„Â¯Q:\mathbb{R}^{|T|}\times\mathbb{R}^{|V\setminus T|}\to\overline{\mathbb{R}} is a function to be optimized, then a GG-causal optimization problem (with respect to TT) is an optimization problem of the following form:

|  |  |  |  |
| --- | --- | --- | --- |
|  | minh:â„|Vâˆ–T|â†’â„|T|hÂ isÂ G-causalâ¡ğ”¼Î¼â€‹[Qâ€‹(XT,hâ€‹(XVâˆ–T))].\min\_{\begin{subarray}{c}h:\mathbb{R}^{|V\setminus T|}\to\mathbb{R}^{|T|}\\ \text{$h$ is $G$-causal}\end{subarray}}\mathbb{E}^{\mu}\left[Q(X\_{T},h(X\_{V\setminus T}))\right]. |  | (1) |

Any minimizer of ([1](https://arxiv.org/html/2510.15458v1#S3.E1 "Equation 1 â€£ Definition 3.4 (ğº-causal optimization problem). â€£ 3 Robust optimization in Structural Causal Models â€£ Robust Optimization in Causal Models and ğº-Causal Normalizing Flows")) is called a GG-causal optimizer.

The following result shows that GG-causal optimizers are always interventionally robust. This underscores the desirability of GG-causal optimizers when we expect the data distribution to undergo distributional shifts due to interventions between training and testing time.

###### Theorem 3.5 (Robustness of GG-causal optimizers).

Let hâˆ—h^{\*} be a solution of the problem in [Eq.Â 1](https://arxiv.org/html/2510.15458v1#S3.E1 "In Definition 3.4 (ğº-causal optimization problem). â€£ 3 Robust optimization in Structural Causal Models â€£ Robust Optimization in Causal Models and ğº-Causal Normalizing Flows"). Then:

|  |  |  |
| --- | --- | --- |
|  | hâˆ—âˆˆargâ€‹minh:â„|Vâˆ–T|â†’â„|T|â€‹supÎ½âˆˆâ„â€‹(Î¼)ğ”¼Î½â€‹[Qâ€‹(XT,hâ€‹(XVâˆ–T))],h^{\*}\in\operatorname\*{arg\,min}\_{h:\mathbb{R}^{|V\setminus T|}\to\mathbb{R}^{|T|}}\sup\_{\nu\in\mathcal{I}(\mu)}\mathbb{E}^{\nu}\left[Q(X\_{T},h(X\_{V\setminus T}))\right], |  |

where

|  |  |  |
| --- | --- | --- |
|  | â„â€‹(Î¼):={Î½âˆˆğ’«â€‹(â„d)|Î½â€‹(dâ€‹xT|xPAâ€‹(T))=Î¼â€‹(dâ€‹xT|xPAâ€‹(T))Â andÂ suppâ€‹(Î½â€‹(dâ€‹xPAâ€‹(T)))âŠ†suppâ€‹(Î¼â€‹(dâ€‹xPAâ€‹(T)))}\mathcal{I}(\mu):=\{\nu\in\mathcal{P}(\mathbb{R}^{d})\;\>|\>\;\text{$\nu(dx\_{T}|x\_{\text{PA}(T)})=\mu(dx\_{T}|x\_{\text{PA}(T)})$ and $\mathrm{supp}(\nu(dx\_{\text{PA}(T)}))\subseteq\mathrm{supp}(\mu(dx\_{\text{PA}(T)}))$}\} |  |

is the set of all interventional distributions that leave the causal mechanism of XTX\_{T} unchanged.

###### Proof.

Itâ€™s enough to show that for any h:â„|Vâˆ–T|â†’â„|T|h:\mathbb{R}^{|V\setminus T|}\to\mathbb{R}^{|T|} and any Î½âˆˆâ„â€‹(Î¼)\nu\in\mathcal{I}(\mu), there exists a Î½â€²âˆˆâ„â€‹(Î¼)\nu^{\prime}\in\mathcal{I}(\mu) such that ğ”¼Î½â€²â€‹[Qâ€‹(XT,hâ€‹(XVâˆ–T))]â‰¥ğ”¼Î½â€‹[Qâ€‹(XT,hâˆ—â€‹(XPAâ€‹(T)))]\mathbb{E}^{\nu^{\prime}}\left[Q(X\_{T},h(X\_{V\setminus T}))\right]\geq\mathbb{E}^{\nu}\left[Q(X\_{T},h^{\*}(X\_{\text{PA}(T)}))\right].

Given Î½âˆˆâ„â€‹(Î¼)\nu\in\mathcal{I}(\mu), define Î½â€²â€‹(dâ€‹x):=Î½â€‹(dâ€‹xVâˆ–(TâˆªPAâ€‹(T)))â€‹Î½â€‹(dâ€‹xPAâ€‹(T),dâ€‹xT)\nu^{\prime}(dx):=\nu(dx\_{V\setminus(T\cup\text{PA}(T))})\nu(dx\_{\text{PA}(T)},dx\_{T}). Then:

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼Î½â€²â€‹[Qâ€‹(XT,hâ€‹(XVâˆ–T))]\displaystyle\mathbb{E}^{\nu^{\prime}}\left[Q(X\_{T},h(X\_{V\setminus T}))\right] | =âˆ«Î½â€‹(dâ€‹xVâˆ–(TâˆªPAâ€‹(T)))â€‹âˆ«Î½â€‹(dâ€‹xPAâ€‹(T),dâ€‹xT)â€‹Qâ€‹(xT,hâ€‹(xVâˆ–T))\displaystyle=\int\nu(dx\_{V\setminus(T\cup\text{PA}(T))})\int\nu(dx\_{\text{PA}(T)},dx\_{T})Q(x\_{T},h(x\_{V\setminus T})) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =âˆ«Î½â€‹(dâ€‹xVâˆ–(TâˆªPAâ€‹(T)))â€‹âˆ«Î½â€‹(xPAâ€‹(T))â€‹âˆ«Î¼â€‹(dâ€‹xT|xPAâ€‹(T))â€‹Qâ€‹(xT,hâ€‹(xVâˆ–T))\displaystyle=\int\nu(dx\_{V\setminus(T\cup\text{PA}(T))})\int\nu(x\_{\text{PA}(T)})\int\mu(dx\_{T}\>|\>x\_{\text{PA}(T)})Q(x\_{T},h(x\_{V\setminus T})) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¥âˆ«Î½â€‹(dâ€‹xVâˆ–(TâˆªPAâ€‹(T)))â€‹âˆ«Î½â€‹(xPAâ€‹(T))â€‹âˆ«Î¼â€‹(dâ€‹xT|xPAâ€‹(T))â€‹Qâ€‹(xT,hâˆ—â€‹(xPAâ€‹(T)))\displaystyle\geq\int\nu(dx\_{V\setminus(T\cup\text{PA}(T))})\int\nu(x\_{\text{PA}(T)})\int\mu(dx\_{T}\>|\>x\_{\text{PA}(T)})Q(x\_{T},h^{\*}(x\_{\text{PA}(T)})) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =ğ”¼Î½â€‹[Qâ€‹(XT,hâˆ—â€‹(XPAâ€‹(T)))]\displaystyle=\mathbb{E}^{\nu}\left[Q(X\_{T},h^{\*}(X\_{\text{PA}(T)}))\right] |  |

where the second equality follows from Î½âˆˆâ„â€‹(Î¼)\nu\in\mathcal{I}(\mu) and the inequality follows from [Eq.Â 1](https://arxiv.org/html/2510.15458v1#S3.E1 "In Definition 3.4 (ğº-causal optimization problem). â€£ 3 Robust optimization in Structural Causal Models â€£ Robust Optimization in Causal Models and ğº-Causal Normalizing Flows"), [LemmaÂ A.1](https://arxiv.org/html/2510.15458v1#A1.Thmtheorem1 "Lemma A.1 (Interchangeability principle). â€£ Appendix A Auxiliary results â€£ Robust Optimization in Causal Models and ğº-Causal Normalizing Flows"), and suppâ€‹(Î½â€‹(dâ€‹xPAâ€‹(Y)))âŠ†suppâ€‹(Î¼â€‹(dâ€‹xPAâ€‹(Y)))\mathrm{supp}(\nu(dx\_{\text{PA}(Y)}))\subseteq\mathrm{supp}(\mu(dx\_{\text{PA}(Y)})).
âˆ

###### Remark 3.6.

The theorem above is a generalization of (Rojas-Carulla etÂ al., [2018](https://arxiv.org/html/2510.15458v1#bib.bib22), Theorem 4), which covered the mean squared loss only. We explicitly added the assumption suppâ€‹(Î½â€‹(dâ€‹xPAâ€‹(Y)))âŠ†suppâ€‹(Î¼â€‹(dâ€‹xPAâ€‹(Y)))\mathrm{supp}(\nu(dx\_{\text{PA}(Y)}))\subseteq\mathrm{supp}(\mu(dx\_{\text{PA}(Y)})), for all Î½âˆˆâ„â€‹(Î¼)\nu\in\mathcal{I}(\mu), which is needed also for their theorem to hold.

The next theorem shows that the value functionals of GG-causal optimization problems are continuous with respect to the GG-causal Wasserstein distance, while they may fail to be continuous with respect to the standard Wasserstein distance (as we show in [ExampleÂ 3.8](https://arxiv.org/html/2510.15458v1#S3.Thmtheorem8 "Example 3.8. â€£ 3 Robust optimization in Structural Causal Models â€£ Robust Optimization in Causal Models and ğº-Causal Normalizing Flows") below). This proves that the GG-causal Wasserstein distance is the right distance to control errors in causal optimization problems and, in particular, interventionally robust optimization problems.

###### Theorem 3.7 (Continuity of GG-causal optimization problems).

Let G=(V,E)G=(V,E) be a sorted DAG, Xâˆ¼Î¼âˆˆğ’«Gâ€‹(â„d)X\sim\mu\in\mathcal{P}\_{G}(\mathbb{R}^{d}) and let TâŠ†VT\subseteq V be a target set, such that [3.1](https://arxiv.org/html/2510.15458v1#S3.Thmtheorem1 "Assumption 3.1. â€£ 3 Robust optimization in Structural Causal Models â€£ Robust Optimization in Causal Models and ğº-Causal Normalizing Flows") holds. If Q:â„|T|Ã—â„|Vâˆ–T|â†’â„Â¯Q:\mathbb{R}^{|T|}\times\mathbb{R}^{|V\setminus T|}\to\overline{\mathbb{R}} is such that xâ†¦Qâ€‹(x,h)x\mapsto Q(x,h) is locally LL-Lipschitz (uniformly in hh) and hâ†¦Qâ€‹(x,h)h\mapsto Q(x,h) is convex, then the value functional

|  |  |  |
| --- | --- | --- |
|  | Î¼â†¦ğ’±â€‹(Î¼):=minh:â„|Vâˆ–T|â†’â„|T|hÂ isÂ G-causalâ¡ğ”¼Î¼â€‹[Qâ€‹(XT,hâ€‹(XVâˆ–T))]\displaystyle\mu\mapsto\mathcal{V}(\mu):=\min\_{\begin{subarray}{c}h:\mathbb{R}^{|V\setminus T|}\to\mathbb{R}^{|T|}\\ \text{$h$ is $G$-causal}\end{subarray}}\mathbb{E}^{\mu}\left[Q(X\_{T},h(X\_{V\setminus T}))\right] |  |

is continuous with respect to the GG-causal Wasserstein distance.

###### Proof.

See proof in [SectionÂ B.1](https://arxiv.org/html/2510.15458v1#A2.SS1 "B.1 Proof of Theorem 3.7 â€£ Appendix B Proofs â€£ Robust Optimization in Causal Models and ğº-Causal Normalizing Flows").
âˆ

###### Example 3.8.

Define Î¼Îµâˆˆğ’«Gâ€‹(â„2)\mu\_{\varepsilon}\in\mathcal{P}\_{G}(\mathbb{R}^{2}) as the following SCM:

|  |  |  |
| --- | --- | --- |
|  | {Y:=sgnâ€‹(X),X:=Îµâ‹…U,whereÂ Uâˆ¼Raâ€‹(1/2),\begin{cases}Y:=\text{sgn}(X),\\ X:=\varepsilon\cdot U,\end{cases}\quad\text{where $U\sim\text{Ra}(1/2)$}, |  |

where Raâ€‹(p)\text{Ra}(p) denoted the Rademacher distribution pâ€‹Î´1+(1âˆ’p)â€‹Î´âˆ’1p\delta\_{1}+(1-p)\delta\_{-1}, and consider the following GG-causal regression problem:

|  |  |  |
| --- | --- | --- |
|  | ğ’±â€‹(Î¼)=infh:â„â†’â„hÂ G-causalğ”¼Î¼â€‹[(Yâˆ’hâ€‹(X))2].\mathcal{V}(\mu)=\inf\_{\begin{subarray}{c}h:\mathbb{R}\to\mathbb{R}\\ \text{$h$ $G$-causal}\end{subarray}}\mathbb{E}^{\mu}\left[(Y-h(X))^{2}\right]. |  |

Then as Îµâ†’0\varepsilon\to 0 we have that Î¼Îµ=12â€‹Î´(Îµ,1)+12â€‹Î´(âˆ’Îµ,âˆ’1)\mu\_{\varepsilon}=\frac{1}{2}\delta\_{(\varepsilon,1)}+\frac{1}{2}\delta\_{(-\varepsilon,-1)} converges to Î¼:=12â€‹Î´(0,1)+12â€‹Î´(0,âˆ’1)=Î´0âŠ—Raâ€‹(1/2)\mu:=\frac{1}{2}\delta\_{(0,1)}+\frac{1}{2}\delta\_{(0,-1)}=\delta\_{0}\otimes\text{Ra}(1/2) under the standard Wasserstein distance, but limÎµâ†’0ğ’±â€‹(Î¼Îµ)=0â‰ 1=ğ’±â€‹(Î¼)\lim\_{\varepsilon\to 0}\mathcal{V}(\mu\_{\varepsilon})=0\neq 1=\mathcal{V}(\mu).

## 4 Proposed method: GG-causal normalizing flows

[TheoremÂ 3.7](https://arxiv.org/html/2510.15458v1#S3.Thmtheorem7 "Theorem 3.7 (Continuity of ğº-causal optimization problems). â€£ 3 Robust optimization in Structural Causal Models â€£ Robust Optimization in Causal Models and ğº-Causal Normalizing Flows") and [ExampleÂ 3.8](https://arxiv.org/html/2510.15458v1#S3.Thmtheorem8 "Example 3.8. â€£ 3 Robust optimization in Structural Causal Models â€£ Robust Optimization in Causal Models and ğº-Causal Normalizing Flows") imply that generative augmentation models that are not trained under the GG-causal Wasserstein distance may lead to optimizers that severely underperform on GG-causal downstream tasks. To solve this issue, we propose a novel normalizing flow architecture capable of minimizing the GG-causal Wasserstein distance from any data distribution Î¼âˆˆğ’«Gâ€‹(â„d)\mu\in\mathcal{P}\_{G}(\mathbb{R}^{d}). Since the standard, adapted and CO-OT Wasserstein distances are all special cases of the GG-causal Wasserstein distance, this model family provides optimal generative augmentation models for a vast class of empirical applications.

A GG-causal normalizing flow T^=T^(d)âˆ˜â‹¯âˆ˜T^(1)\hat{T}=\hat{T}^{(d)}\circ\cdots\circ\hat{T}^{(1)} is a composition of dd neural coupling flows T^(k):â„dâ†’â„d\hat{T}^{(k)}:\mathbb{R}^{d}\to\mathbb{R}^{d} of the following form:

|  |  |  |  |
| --- | --- | --- | --- |
|  | T^i(k)â€‹(x)={gâ€‹(xi;Î¸â€‹(xPAâ€‹(i)))ifÂ i=kidifÂ iâ‰ k\hat{T}^{(k)}\_{i}(x)=\begin{cases}g(x\_{i};\theta(x\_{\text{PA}(i)}))&\text{if $i=k$}\\ \text{id}&\text{if $i\neq k$}\end{cases} |  | (2) |

where g:â„Ã—Î˜â€‹(n)â†’â„g:\mathbb{R}\times\Theta(n)\to\mathbb{R} is a shallow MLP of the form:

|  |  |  |  |
| --- | --- | --- | --- |
|  | gâ€‹(x,Î¸)=âˆ‘i=1nwi(2)â€‹Ïâ€‹(wi(1)â€‹x+bi(1))+b(2)g(x,\theta)=\sum\_{i=1}^{n}w^{(2)}\_{i}\rho(w^{(1)}\_{i}x+b^{(1)}\_{i})+b^{(2)} |  | (3) |

with parameters Î¸:=(w(1),b(1),w(2),b(2))âˆˆÎ˜â€‹(n):=â„>0nÃ—â„nÃ—â„>0nÃ—â„\theta:=(w^{(1)},b^{(1)},w^{(2)},b^{(2)})\in\Theta(n):=\mathbb{R}\_{>0}^{n}\times\mathbb{R}^{n}\times\mathbb{R}\_{>0}^{n}\times\mathbb{R} and custom activation function222Recall that the LeakyReLU activation function is defined as LeakyReLUÎ±â€‹(x):=xâ€‹ğŸ™{xâ‰¥0}+Î±â€‹xâ€‹ğŸ™{x<0}\text{LeakyReLU}\_{\alpha}(x):=x\mathds{1}\_{\{x\geq 0\}}+\alpha x\mathds{1}\_{\{x<0\}}.:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ïâ€‹(x)=12â€‹LeakyReLUÎ±âˆ’1â€‹(1+x)âˆ’12â€‹LeakyReLUÎ±âˆ’1â€‹(1âˆ’x),Î±âˆˆ(0,1).\rho(x)=\frac{1}{2}\text{LeakyReLU}\_{\alpha-1}(1+x)-\frac{1}{2}\text{LeakyReLU}\_{\alpha-1}(1-x),\quad\alpha\in(0,1). |  | (4) |

We denote by IncrMLP(nn) the class of all MLPs with nn hidden neurons and parameter space Î˜â€‹(n)\Theta(n). It is easy to see that IncrMLPâ€‹(n)\text{IncrMLP}(n) contains only continuous, piecewise linear, strictly increasing (and, therefore, *invertible*) functions, thanks to the choice of activation function333One cannot just take Ïâ€‹(x)=ReLUâ€‹(x)\rho(x)=\text{ReLU}(x), because gg could fail to be strictly increasing, nor Ïâ€‹(x)=LeakyReLUÎ±â€‹(x)\rho(x)=\text{LeakyReLU}\_{\alpha}(x), because then gg would be constrained to be convex, which harms model capacity. and parameter space. The inverse of gg and its derivative can be computed efficiently, which allows the coupling flow in [Eq.Â 2](https://arxiv.org/html/2510.15458v1#S4.E2 "In 4 Proposed method: ğº-causal normalizing flows â€£ Robust Optimization in Causal Models and ğº-Causal Normalizing Flows") to be easily implemented in a normalizing flow model (see code in the supplementary material).

In [Eq.Â 2](https://arxiv.org/html/2510.15458v1#S4.E2 "In 4 Proposed method: ğº-causal normalizing flows â€£ Robust Optimization in Causal Models and ğº-Causal Normalizing Flows") we specify the parameters of gg in terms of a function Î¸â€‹(xPAâ€‹(i))\theta(x\_{\text{PA}(i)}), which we take to be an MLP444In practice, we enforce Î¸â€‹(xPAâ€‹(i))âˆˆÎ˜â€‹(n)\theta(x\_{\text{PA}(i)})\in\Theta(n) by constraining its outputs corresponding to the weights w(1)w^{(1)} and w(2)w^{(2)} to be strictly positive, either by using a ReLU activation function or by taking their absolute value.. The particular choice of MLP class does not matter, as long as the assumptions of (Leshno etÂ al., [1993](https://arxiv.org/html/2510.15458v1#bib.bib15), Theorem 1) are satisfied555The activation function must be non-polynomial and locally essentially bounded on â„\mathbb{R}. All commonly used activation functions (including ReLU) satisfy this. and we denote by MLP any such class. Since the outputs of Î¸â€‹(â‹…)âˆˆMLP\theta(\cdot)\in\text{MLP} are used as parameters for another MLP, gâ€‹(â‹…)g(\cdot), it is common to say that Î¸â€‹(â‹…)\theta(\cdot) is a *hypernetwork* (Chauhan etÂ al., [2024](https://arxiv.org/html/2510.15458v1#bib.bib6)). Therefore we say that the coupling flow in [Eq.Â 2](https://arxiv.org/html/2510.15458v1#S4.E2 "In 4 Proposed method: ğº-causal normalizing flows â€£ Robust Optimization in Causal Models and ğº-Causal Normalizing Flows") is a hypercoupling flow and we denote by HyperCplâ€‹(n,Î¸â€‹(â‹…))\text{HyperCpl}(n,\theta(\cdot)) the class of hypercoupling flows with gâ€‹(â‹…)âˆˆIncrMLPâ€‹(n)g(\cdot)\in\text{IncrMLP}(n) and parameter hypernetwork Î¸â€‹(â‹…)âˆˆMLP\theta(\cdot)\in\text{MLP}.

Since each hypercoupling flow in a GG-causal normalizing flow acts only on a subset of the input coordinates it effectively functions as a scale in a multi-scale architecture, thus reducing the computational burden by exploiting our a priori knowledge of the causal DAG GG.

###### Remark 4.1.

We emphasize that the DAG GG is an *input* of our model, not an output. We assume, therefore, that the modeler has estimated the causal skeleton GG, using any of the available methods for causal discovery Nogueira etÂ al. ([2022](https://arxiv.org/html/2510.15458v1#bib.bib17)); Zanga etÂ al. ([2022](https://arxiv.org/html/2510.15458v1#bib.bib25)). On the other hand, we do not require any knowledge of the functional form of the causal mechanisms, which our model will learn directly from data.

Next, we turn to the task of proving that GG-causal normalizing flows are universal approximators for structural causal models.

###### Definition 4.2 (GG-compatible transformation.).

Let GG be a sorted DAG. A map T:â„dâ†’â„dT:\mathbb{R}^{d}\to\mathbb{R}^{d} is a GG-compatible transformation if each coordinate Tiâ€‹(x)T\_{i}(x) is a function of (xi,xPAâ€‹(i))(x\_{i},x\_{\text{PA}(i)}), for all i=1,â€¦,di=1,\ldots,d. Furthermore, a GG-compatible transformation TT is called (strictly) increasing if each coordinate TiT\_{i} is (strictly) increasing in xix\_{i}.

###### Theorem 4.3.

Let Î¼âˆˆğ’«Gâ€‹(â„d)\mu\in\mathcal{P}\_{G}(\mathbb{R}^{d}) be an absolutely continuous distribution. Then there exists a GG-compatible, strictly increasing transformation T:â„dâ†’â„dT:\mathbb{R}^{d}\to\mathbb{R}^{d}, such that T#â€‹ğ’°â€‹([0,1]d)=Î¼{T}\_{\#}\mathcal{U}([0,1]^{d})=\mu.

Furthermore, TT is of the form T:=T(d)âˆ˜â‹¯âˆ˜T(1)T:=T^{(d)}\circ\cdots\circ T^{(1)}, where each T(k):â„dâ†’â„dT^{(k)}:\mathbb{R}^{d}\to\mathbb{R}^{d} is defined as:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ti(k)â€‹(x)={Fiâˆ’1â€‹(xi|xPAâ€‹(i))i=k,idiâ‰ k.(k=1,â€¦,d)T^{(k)}\_{i}(x)=\begin{cases}F^{-1}\_{i}(x\_{i}\>|\>x\_{\text{PA}(i)})&i=k,\\ \text{id}&i\neq k.\end{cases}\quad(k=1,\ldots,d) |  | (5) |

where Fiâˆ’1F^{-1}\_{i} is the (conditional) quantile function of the random variable Xiâˆ¼Î¼â€‹(dâ€‹xi)X\_{i}\sim\mu(dx\_{i}) given its parents XPAâ€‹(i)âˆ¼Î¼â€‹(dâ€‹xPAâ€‹(i))X\_{\text{PA}(i)}\sim\mu(dx\_{\text{PA}(i)}).

###### Proof.

It is easy to check that TT, as defined, is indeed a GG-compatible, increasing transformation. The absolute continuity of Î¼\mu implies that all conditional distributions admit a density (Jacod & Protter, [2004](https://arxiv.org/html/2510.15458v1#bib.bib13), Theorem 12.2), therefore a continuous cdf and a strictly monotone quantile function (McNeil etÂ al., [2015](https://arxiv.org/html/2510.15458v1#bib.bib16), Proposition A.3 (ii)).

Next, we show that T#â€‹ğ’°â€‹([0,1]d)=Î¼{T}\_{\#}\mathcal{U}([0,1]^{d})=\mu. By [DefinitionÂ 2.2](https://arxiv.org/html/2510.15458v1#S2.Thmtheorem2 "Definition 2.2 (G-compatible distribution). â€£ 2.2 ğº-causal Wasserstein distance â€£ 2 Background â€£ Robust Optimization in Causal Models and ğº-Causal Normalizing Flows") we know that there exists Xâˆ¼Î¼X\sim\mu and measurable functions fif\_{i} such that Xi=fiâ€‹(XPAâ€‹(i),Ui)X\_{i}=f\_{i}(X\_{\text{PA}}(i),U\_{i}) where U=(U1,â€¦,Ud)U=(U\_{1},\ldots,U\_{d}) is a random vector of mutually independent random variables. Without loss of generality, we can take Uâˆ¼ğ’°â€‹([0,1]d)U\sim\mathcal{U}([0,1]^{d}) and set Xi=Fiâˆ’1â€‹(Ui|XPAâ€‹(i))X\_{i}=F^{-1}\_{i}(U\_{i}|X\_{\text{PA}}(i)) (McNeil etÂ al., [2015](https://arxiv.org/html/2510.15458v1#bib.bib16), Proposition A.6)).
âˆ

###### Theorem 4.4 (Universal Approximation Property (UAP) for GG-causal normalizing flows).

Let Î¼âˆˆğ’«G,1â€‹(â„d)\mu\in\mathcal{P}\_{G,1}(\mathbb{R}^{d}) be an absolutely continuous distribution with compact support and assume that the conditional cdfs (xk,xPAâ€‹(k))â†¦Fkâ€‹(xk|xPAâ€‹(k))(x\_{k},x\_{\text{PA}(k)})\mapsto F\_{k}(x\_{k}\>|\>x\_{\text{PA}(k)}) belong to C1â€‹(â„Ã—â„|PAâ€‹(k)|)C^{1}(\mathbb{R}\times\mathbb{R}^{|\text{PA}(k)|}), for all k=1,â€¦,dk=1,\ldots,d.

Then GG-causal normalizing flows with base distribution ğ’°â€‹([0,1]d)\mathcal{U}([0,1]^{d}) are dense in the semi-metric space (ğ’«G,1â€‹(â„d),WG)(\mathcal{P}\_{G,1}(\mathbb{R}^{d}),W\_{G}), i.e. for every Îµ>0\varepsilon>0, there exists a GG-causal normalizing flow T^\hat{T} such that

|  |  |  |
| --- | --- | --- |
|  | WG(Î¼,T^#ğ’°([0,1]d)â‰¤Îµ.W\_{G}(\mu,{\hat{T}}\_{\#}\mathcal{U}([0,1]^{d})\leq\varepsilon. |  |

###### Proof.

See proof in [SectionÂ B.2](https://arxiv.org/html/2510.15458v1#A2.SS2 "B.2 Proof of Theorem 4.4 â€£ Appendix B Proofs â€£ Robust Optimization in Causal Models and ğº-Causal Normalizing Flows").
âˆ

###### Remark 4.5.

The theorem holds for base distributions other than ğ’°â€‹([0,1]d)\mathcal{U}([0,1]^{d}). In fact any absolutely continuous distribution on â„d\mathbb{R}^{d} with mutually independent coordinates (such as the standard multivariate Gaussian ğ’©â€‹(0,Id)\mathcal{N}(0,I\_{d})) would work, provided we add a non-trainable layer between the base distribution and the first flow that maps â„d\mathbb{R}^{d} into the base distributionâ€™s quantiles (for ğ’©â€‹(0,Id)\mathcal{N}(0,I\_{d}), such a map is just Î¦âŠ—d\Phi^{\otimes d}, where Î¦\Phi is the standard Gaussian cdf).

In practice GG-causal normalizing flows are trained using likelihood maximization (or, equivalently, KL minimization), so it is important to make sure that minimizing this loss guarantees that the GG-causal Wasserstein distance between data and model distribution is also minimized. The following result proves exactly this and is a generalization of Acciaio etÂ al. ([2024](https://arxiv.org/html/2510.15458v1#bib.bib1), Lemma 2.3) and Eckstein & Pammer ([2024](https://arxiv.org/html/2510.15458v1#bib.bib11), Lemma 3.5), which established an analogous claim for the adapted Wasserstein distance.

###### Theorem 4.6 (WGW\_{G} training via KL minimization).

Let Î¼,Î½âˆˆğ’«Gâ€‹(K)\mu,\nu\in\mathcal{P}\_{G}(K) for some compact KâŠ†â„dK\subseteq\mathbb{R}^{d}. Then:

|  |  |  |
| --- | --- | --- |
|  | WGâ€‹(Î¼,Î½)â‰¤Câ€‹12â€‹ğ’ŸKâ€‹Lâ€‹(Î¼|Î½),W\_{G}(\mu,\nu)\leq C\sqrt{\frac{1}{2}\mathcal{D}\_{KL}(\mu\>|\>\nu)}, |  |

for a constant C>0C>0.

###### Proof.

See proof in [SectionÂ B.3](https://arxiv.org/html/2510.15458v1#A2.SS3 "B.3 Proof of Theorem 4.6 â€£ Appendix B Proofs â€£ Robust Optimization in Causal Models and ğº-Causal Normalizing Flows").
âˆ

## 5 Numerical experiments

### 5.1 Causal regression

We study a multivariate causal regression problem of the form:

|  |  |  |  |
| --- | --- | --- | --- |
|  | minh:â„|Vâˆ–T|â†’â„|T|hÂ isÂ G-causalâ¡ğ”¼Î¼â€‹[(XTâˆ’hâ€‹(XVâˆ–T))2],\min\_{\begin{subarray}{c}h:\mathbb{R}^{|V\setminus T|}\to\mathbb{R}^{|T|}\\ \text{$h$ is $G$-causal}\end{subarray}}\mathbb{E}^{\mu}\left[(X\_{T}-h(X\_{V\setminus T}))^{2}\right], |  | (6) |

where Î¼âˆˆğ’«Gâ€‹(â„d)\mu\in\mathcal{P}\_{G}(\mathbb{R}^{d}) is a randomly generated linear Gaussian SCM (Peters etÂ al., [2017](https://arxiv.org/html/2510.15458v1#bib.bib18), Section 7.1.3) with coefficients uniformly sampled in (âˆ’1,1)(-1,1) and homoscedastic noise with unit variance. The sorted DAG GG is obtained by randomly sampling an Erdos-Renyi graph on dd vertices with edge probability pp and eliminating all edges (i,j)(i,j) with i>ji>j.

According to [TheoremÂ 3.5](https://arxiv.org/html/2510.15458v1#S3.Thmtheorem5 "Theorem 3.5 (Robustness of ğº-causal optimizers). â€£ 3 Robust optimization in Structural Causal Models â€£ Robust Optimization in Causal Models and ğº-Causal Normalizing Flows"), any solution to problem ([6](https://arxiv.org/html/2510.15458v1#S5.E6 "Equation 6 â€£ 5.1 Causal regression â€£ 5 Numerical experiments â€£ Robust Optimization in Causal Models and ğº-Causal Normalizing Flows")) is interventionally robust. In order to showcase this robustness property of the GG-causal regressor, we compare its performance with that of a standard (i.e. non-causal) regressor when tested out-of-sample on a large number of random soft666A soft intervention at a node iâˆˆVi\in V leaves its parents and noise distribution unaltered, but changes the functional form of its causal mechanism. interventions. Each intervention is obtained by randomly sampling a node iâˆˆVâˆ–Ti\in V\setminus T and substituting its causal mechanism, fâ€‹(XPAâ€‹(i),Ui)f(X\_{\text{PA}(i)},U\_{i}), with a new one, f~â€‹(X|PA(i),Ui)\tilde{f}(X\_{|\text{PA}(i)},U\_{i}). We consider only linear interventions and quantify their interventional strength by computing the following L1L^{1}-norm:

|  |  |  |
| --- | --- | --- |
|  | âˆ«âˆ«|fâ€‹(xPAâ€‹(i),u)âˆ’f~â€‹(xPAâ€‹(i),u)|â€‹Î¼â€‹(dâ€‹xPAâ€‹(i))â€‹Î»â€‹(dâ€‹u),\int\int|f(x\_{\text{PA}(i)},u)-\tilde{f}(x\_{\text{PA}(i)},u)|\mu(dx\_{\text{PA}(i)})\lambda(du), |  |

where Î¼\mu is the original distribution (before intervention) and Î»\lambda is the noise distribution. Interventional strength, therefore, quantifies the out-of-sample variation of the regressorâ€™s inputs under the intervention.

We implement a multivariate regression with d=10d=10, p=0.5p=0.5 and T={5,6}T=\{5,6\}. We report in [Fig.Â 8](https://arxiv.org/html/2510.15458v1#S5.F8 "In 5.1 Causal regression â€£ 5 Numerical experiments â€£ Robust Optimization in Causal Models and ğº-Causal Normalizing Flows") and [Fig.Â 8](https://arxiv.org/html/2510.15458v1#S5.F8 "In 5.1 Causal regression â€£ 5 Numerical experiments â€£ Robust Optimization in Causal Models and ğº-Causal Normalizing Flows") the worst-case performance of a GG-causal regressor and of a non-causal regressor (in terms of MSE and R2R^{2}, respectively) as a function of the interventional strength. At small interventional strengths the non-causal regressor benefits from the information contained in non-parent nodes (which are not available as inputs to the GG-causal optimizer). These non-parent nodes may belong to the Markov blanket of the target nodes in GG and therefore be statistically informative, but their usefulness crucially depends on the stability of their causal mechanisms. As the interventional strength is increased the worst-case performance of the non-causal regressor rapidly deteriorates, while that of the GG-causal regressor remains stable, as shown in the figures.

![Refer to caption](x3.png)


Figure 3: Worst-case MSE vs interventional strength.

![Refer to caption](x4.png)


Figure 4: Worst-case R2R^{2} vs interventional strength.

In [Fig.Â 6](https://arxiv.org/html/2510.15458v1#S5.F6 "In 5.1 Causal regression â€£ 5 Numerical experiments â€£ Robust Optimization in Causal Models and ğº-Causal Normalizing Flows") and [Fig.Â 6](https://arxiv.org/html/2510.15458v1#S5.F6 "In 5.1 Causal regression â€£ 5 Numerical experiments â€£ Robust Optimization in Causal Models and ğº-Causal Normalizing Flows") we deepen the comparison by plotting the distribution of the performance metrics (MSE and R2R^{2}, respectively) for both estimators. Notice how interventions deteriorate the performance of the non-causal regressor starting from the least favorable quantiles, while the entire distribution of the performance metrics of the GG-causal remains stable. These figures also show that the median performance of the causal regressor is, after all, not strongly affected by the linear random interventions we consider. In this sense, non-causal optimizers can still be approximately optimal in applications where distributional shifts are expected to be mild.

![Refer to caption](x5.png)


Figure 5: Median and (75%-95%) CI of MSE vs interventional strength.

![Refer to caption](x6.png)


Figure 6: Median and (75%-95%) CI of R2R^{2} vs interventional strength.

Finally, we investigate the performance of our GG-causal normalizing flow model when used for generative data augmentation. We therefore train several augmentation models (both non-causal and GG-causal) on a training set of n=10000n=10000 samples from Î¼\mu. We then use them to generate of synthetic training set of n=10000n=10000 samples and we train a causal optimizer on it.

As shown in [Fig.Â 8](https://arxiv.org/html/2510.15458v1#S5.F8 "In 5.1 Causal regression â€£ 5 Numerical experiments â€£ Robust Optimization in Causal Models and ğº-Causal Normalizing Flows") and [Fig.Â 8](https://arxiv.org/html/2510.15458v1#S5.F8 "In 5.1 Causal regression â€£ 5 Numerical experiments â€£ Robust Optimization in Causal Models and ğº-Causal Normalizing Flows"), causal optimizers trained using non-causal augmentation models (e.g. RealNVP and VAE) are indeed robust under interventions, but their worst-case metrics are significantly worse than when causal augmentation is used. This is an empirical validation of the fact that the loss used for training the augmentation model plays a crucial role in downstream performance.

![Refer to caption](x7.png)


Figure 7: Worst-case MSE after generative data augmentation vs interventional strength.

![Refer to caption](x8.png)


Figure 8: Worst-case R2R^{2} after generative data augmentation vs interventional strength.

### 5.2 Conditional mean-variance portfolio optimization

We look at the following conditional mean-variance portfolio optimization problem:

|  |  |  |
| --- | --- | --- |
|  | ğ’±â€‹(Î¼)=infh:â„|Vâˆ–T|â†’â„|T|hÂ isÂ G-causal{âˆ’ğ”¼Î¼â€‹[âŸ¨XT,hâ€‹(XVâˆ–T)âŸ©]+Î³2â€‹VarÎ¼â€‹(âŸ¨XT,hâ€‹(XVâˆ–T)âŸ©)},\mathcal{V}(\mu)=\inf\_{\begin{subarray}{c}h:\mathbb{R}^{|V\setminus T|}\to\mathbb{R}^{|T|}\\ \text{$h$ is $G$-causal}\end{subarray}}\left\{-\mathbb{E}^{\mu}\left[\langle X\_{T},h(X\_{V\setminus T})\rangle\right]+\frac{\gamma}{2}\mathrm{Var}^{\mu}\left(\langle X\_{T},h(X\_{V\setminus T})\rangle\right)\right\}, |  |

where Xâˆ¼Î¼âˆˆğ’«Gâ€‹(â„d)X\sim\mu\in\mathcal{P}\_{G}(\mathbb{R}^{d}) is a linear Gaussian SCM, with bipartite DAG GG with partition {T,Vâˆ–T}\{T,V\setminus T\} and random uniform coefficients in (âˆ’1,1)(-1,1), and Î³\gamma is a given risk aversion parameter. The target variables XTX\_{T} represent stock returns, while XVâˆ–TX\_{V\setminus T} are market factors or trading signals. We present the results for a high-dimensional example with |T|=100|T|=100 stocks and |Vâˆ–T|=20|V\setminus T|=20 factors.

We sample random linear interventions exactly as done in the case of causal regression and study empirically the robustness of the GG-causal portfolio in terms of its Sharpe ratio as the interventional strength increases.

[Fig.Â 10](https://arxiv.org/html/2510.15458v1#S5.F10 "In 5.2 Conditional mean-variance portfolio optimization â€£ 5 Numerical experiments â€£ Robust Optimization in Causal Models and ğº-Causal Normalizing Flows") and [Fig.Â 10](https://arxiv.org/html/2510.15458v1#S5.F10 "In 5.2 Conditional mean-variance portfolio optimization â€£ 5 Numerical experiments â€£ Robust Optimization in Causal Models and ğº-Causal Normalizing Flows") show that the Sharpe ratio of the GG-causal portfolio is indeed robust to a wide range of interventions, while the performance of non-causal portfolios deteriorates rapidly, starting from the least favorable quantiles.

![Refer to caption](x9.png)


Figure 9: Worst-case Sharpe ratio vs interventional strength

![Refer to caption](x10.png)


Figure 10: Median and (75%-95%) CI of Sharpe ratio vs interventional strength

Reproducibility statement. All results can be reproduced using the source code provided in the Supplimentary Materials. Demo notebooks of the numerical experiments will be made available in a paper-related GitHub repository upon publication.

## References

* Acciaio etÂ al. (2024)

  Beatrice Acciaio, Stephan Eckstein, and Songyan Hou.
  Time-Causal VAE: Robust Financial Time Series Generator.
  *arXiv preprint arXiv:2411.02947*, 2024.
* Aubin & Frankowska (2009)

  Jean-Pierre Aubin and HÃ©lÃ¨ne Frankowska.
  Set-Valued Analysis, 2009.
* Backhoff-Veraguas etÂ al. (2020)

  Julio Backhoff-Veraguas, Daniel Bartl, Mathias BeiglbÃ¶ck, and Manu Eder.
  Adapted wasserstein distances and stability in mathematical finance.
  *Finance and Stochastics*, 24(3):601â€“632,
  2020.
* Bailey etÂ al. (2017)

  David Bailey, Jonathan Borwein, Marcos LopezÂ de Prado, and QijiÂ Jim Zhu.
  The probability of backtest overfitting.
  *The Journal of Computational Finance*, 20(4):39â€“69, 2017.
* Bogachev (2007)

  VladimirÂ I Bogachev.
  *Measure theory*.
  Springer, 2007.
* Chauhan etÂ al. (2024)

  VinodÂ Kumar Chauhan, Jiandong Zhou, Ping Lu, Soheila Molaei, and DavidÂ A
  Clifton.
  A brief review of hypernetworks in deep learning.
  *Artificial Intelligence Review*, 57(9):250,
  2024.
* Chen etÂ al. (2020)

  Ruidi Chen, IoannisÂ Ch Paschalidis, etÂ al.
  Distributionally robust learning.
  *Foundations and TrendsÂ® in Optimization*,
  4(1-2):1â€“243, 2020.
* Chen etÂ al. (2024)

  Yunhao Chen, Zihui Yan, and Yunjie Zhu.
  A comprehensive survey for generative data augmentation.
  *Neurocomputing*, 600:128167, 2024.
* Cheridito & Eckstein (2025)

  Patrick Cheridito and Stephan Eckstein.
  Optimal transport and Wasserstein distances for causal models.
  *Bernoulli*, 31(2):1351â€“1376, 2025.
* Eckstein & Nutz (2022)

  Stephan Eckstein and Marcel Nutz.
  Quantitative stability of regularized optimal transport and
  convergence of sinkhornâ€™s algorithm.
  *SIAM Journal on Mathematical Analysis*, 54(6):5922â€“5948, 2022.
* Eckstein & Pammer (2024)

  Stephan Eckstein and Gudmund Pammer.
  Computational methods for adapted optimal transport.
  *The Annals of Applied Probability*, 34(1A):675â€“713, 2024.
* Folland (1999)

  GeraldÂ B. Folland.
  *Real Analysis: Modern Techniques and their Applications*.
  John Wiley & Sons, 1999.
* Jacod & Protter (2004)

  Jean Jacod and Philip Protter.
  *Probability Essentials*.
  Springer Science & Business Media, 2004.
* Kuhn etÂ al. (2025)

  Daniel Kuhn, Soroosh Shafiee, and Wolfram Wiesemann.
  Distributionally robust optimization.
  *Acta Numerica*, 34:579â€“804, 2025.
* Leshno etÂ al. (1993)

  Moshe Leshno, VladimirÂ Ya Lin, Allan Pinkus, and Shimon Schocken.
  Multilayer feedforward networks with a nonpolynomial activation
  function can approximate any function.
  *Neural networks*, 6(6):861â€“867, 1993.
* McNeil etÂ al. (2015)

  AlexanderÂ J McNeil, RÃ¼diger Frey, and Paul Embrechts.
  *Quantitative Risk Management: Concepts, Techniques and Tools
  (Revised Edition)*.
  Princeton university press, 2015.
* Nogueira etÂ al. (2022)

  AnaÂ Rita Nogueira, Andrea Pugnana, Salvatore Ruggieri, Dino Pedreschi, and
  JoÃ£o Gama.
  Methods and tools for causal discovery and causal inference.
  *Wiley interdisciplinary reviews: data mining and knowledge
  discovery*, 12(2):e1449, 2022.
* Peters etÂ al. (2017)

  Jonas Peters, Dominik Janzing, and Bernhard SchÃ¶lkopf.
  *Elements of Causal Inference: Foundations and Learning
  Algorithms*.
  The MIT press, 2017.
* Pflug & Pichler (2012)

  GeorgÂ Ch Pflug and Alois Pichler.
  A distance for multistage stochastic optimization models.
  *SIAM Journal on Optimization*, 22(1):1â€“23,
  2012.
* Pflug & Pichler (2014)

  GeorgÂ Ch Pflug and Alois Pichler.
  *Multistage Stochastic Optimization*, volume 1104.
  Springer, 2014.
* Rockafellar & Wets (1998)

  R.Â Tyrrell Rockafellar and Roger J.Â B. Wets.
  *Variational Analysis*.
  Springer, 1998.
* Rojas-Carulla etÂ al. (2018)

  Mateo Rojas-Carulla, Bernhard SchÃ¶lkopf, Richard Turner, and Jonas Peters.
  Invariant Models for Causal Transfer Learning.
  *Journal of Machine Learning Research*, 19(36):1â€“34, 2018.
* Schumaker (2007)

  Larry Schumaker.
  *Spline Functions: Basic Theory*.
  Cambridge University Press, 2007.
* Xu etÂ al. (2020)

  Tianlin Xu, LiÂ Kevin Wenliang, Michael Munn, and Beatrice Acciaio.
  Cot-gan: Generating sequential data via causal optimal transport.
  *Advances in neural information processing systems*,
  33:8798â€“8809, 2020.
* Zanga etÂ al. (2022)

  Alessio Zanga, Elif Ozkirimli, and Fabio Stella.
  A survey on causal discovery: theory and practice.
  *International Journal of Approximate Reasoning*, 151:101â€“129, 2022.
* Zheng etÂ al. (2023)

  Chenyu Zheng, Guoqiang Wu, and Chongxuan Li.
  Toward understanding generative data augmentation.
  *Advances in neural information processing systems*,
  36:54046â€“54060, 2023.

## Appendix A Auxiliary results

###### Lemma A.1 (Interchangeability principle).

Let (Î©,â„±,â„™)(\Omega,\mathcal{F},\mathbb{P}) be a probability space and let f:Î©Ã—â„dâ†’â„Â¯f:\Omega\times\mathbb{R}^{d}\to\overline{\mathbb{R}} be an â„±\mathcal{F}-measurable normal integrand. Then:

|  |  |  |
| --- | --- | --- |
|  | âˆ«minxâˆˆâ„dâ¡fâ€‹(Ï‰,x)â€‹â„™â€‹(dâ€‹Ï‰)=minXâˆˆmâ€‹â„±â€‹âˆ«fâ€‹(Ï‰,Xâ€‹(Ï‰))â€‹â„™â€‹(dâ€‹Ï‰),\int\min\_{x\in\mathbb{R}^{d}}f(\omega,x)\mathbb{P}(d\omega)=\min\_{X\in m\mathcal{F}}\int f(\omega,X(\omega))\mathbb{P}(d\omega), |  |

provided that the right-hand side is not âˆ\infty.

Furthermore, if both sides are not âˆ’âˆ-\infty, then:

|  |  |  |
| --- | --- | --- |
|  | Xâˆ—âˆˆargâ€‹minXâˆˆmâ€‹â„±â€‹âˆ«fâ€‹(Ï‰,Xâ€‹(Ï‰))â€‹â„™â€‹(dâ€‹Ï‰)âŸºXâˆ—â€‹(Ï‰)âˆˆargâ€‹minxâˆˆâ„dâ¡fâ€‹(Ï‰,x),(Î¼-almost surely)X^{\*}\in\operatorname\*{arg\,min}\_{X\in m\mathcal{F}}\int f(\omega,X(\omega))\mathbb{P}(d\omega)\Longleftrightarrow X^{\*}(\omega)\in\operatorname\*{arg\,min}\_{x\in\mathbb{R}^{d}}f(\omega,x),\>\>\text{($\mu$-almost surely)} |  |

###### Proof.

See Rockafellar & Wets ([1998](https://arxiv.org/html/2510.15458v1#bib.bib21), Theorem 14.60).
âˆ

###### Lemma A.2 (Composition lemma).

Let (ğ’³,âˆ¥â‹…âˆ¥)(\mathcal{X},\|\cdot\|) be a Banach space with its Borel Ïƒ\sigma-algebra and let Î¼(0),â€¦,Î¼(d)\mu^{(0)},\ldots,\mu^{(d)} be measures defined on it. Given measurable maps T^(k):ğ’³â†’ğ’³\hat{T}^{(k)}:\mathcal{X}\to\mathcal{X} and T(k):ğ’³â†’ğ’³T^{(k)}:\mathcal{X}\to\mathcal{X} such that T(k)#â€‹Î¼(kâˆ’1)=Î¼(k){T^{(k)}}\_{\#}\mu^{(k-1)}=\mu^{(k)} (for k=1,â€¦,dk=1,\ldots,d), if the following two conditions hold:

1. i)

   T^(k)\hat{T}^{(k)} is LkL\_{k}-Lipschitz,
2. ii)

   â€–T(k)âˆ’T^(k)â€–Lpâ€‹(Î¼(kâˆ’1))â‰¤Îµk\|T^{(k)}-\hat{T}^{(k)}\|\_{L^{p}(\mu^{(k-1)})}\leq\varepsilon\_{k},

then:

|  |  |  |
| --- | --- | --- |
|  | â€–T(d)âˆ˜â‹¯âˆ˜T(1)âˆ’T^(d)âˆ˜â‹¯âˆ˜T^(1)â€–Lpâ€‹(Î»)â‰¤âˆ‘k=1dÎµkâ€‹âˆj=k+1dLj,\|T^{(d)}\circ\cdots\circ T^{(1)}-\hat{T}^{(d)}\circ\cdots\circ\hat{T}^{(1)}\|\_{L^{p}(\lambda)}\leq\sum\_{k=1}^{d}\varepsilon\_{k}\prod\_{j=k+1}^{d}L\_{j}, |  |

with the convention that âˆjâˆˆâˆ…Lj:=1.\prod\_{j\in\emptyset}L\_{j}:=1.

###### Proof.

The claim follows by induction. It is obviously true for d=1d=1. Assume that it holds for dâˆ’1d-1, then for dd:

|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â€–T(d)âˆ˜â‹¯âˆ˜T(1)âˆ’T^(d)âˆ˜â‹¯âˆ˜T^(1)â€–Lpâ€‹(Î¼(0))\displaystyle\|T^{(d)}\circ\cdots\circ T^{(1)}-\hat{T}^{(d)}\circ\cdots\circ\hat{T}^{(1)}\|\_{L^{p}(\mu^{(0)})} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤\displaystyle\leq | â€–T(d)âˆ˜T(dâˆ’1)âˆ˜â‹¯âˆ˜T(1)âˆ’T^(d)âˆ˜T(dâˆ’1)âˆ˜â‹¯âˆ˜T(1)â€–Lpâ€‹(Î¼(0))+\displaystyle\|T^{(d)}\circ T^{(d-1)}\circ\cdots\circ T^{(1)}-\hat{T}^{(d)}\circ T^{(d-1)}\circ\cdots\circ T^{(1)}\|\_{L^{p}(\mu^{(0)})}+ |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â€–T^(d)âˆ˜T(dâˆ’1)âˆ˜â‹¯âˆ˜T(1)âˆ’T^(d)âˆ˜T^(dâˆ’1)âˆ˜â‹¯âˆ˜T^(1)â€–Lpâ€‹(Î¼(0))\displaystyle\|\hat{T}^{(d)}\circ T^{(d-1)}\circ\cdots\circ T^{(1)}-\hat{T}^{(d)}\circ\hat{T}^{(d-1)}\circ\cdots\circ\hat{T}^{(1)}\|\_{L^{p}(\mu^{(0)})} |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | â‰¤\displaystyle\leq | â€–T(d)âˆ’T^(d)â€–Lpâ€‹(Î¼(dâˆ’1))+Ldâ€‹â€–T(dâˆ’1)âˆ˜â‹¯âˆ˜T(1)âˆ’T^(dâˆ’1)âˆ˜â‹¯âˆ˜T^(1)â€–Lpâ€‹(Î¼(0))\displaystyle\|T^{(d)}-\hat{T}^{(d)}\|\_{L^{p}(\mu^{(d-1)})}+L\_{d}\|T^{(d-1)}\circ\cdots\circ T^{(1)}-\hat{T}^{(d-1)}\circ\cdots\circ\hat{T}^{(1)}\|\_{L^{p}(\mu^{(0)})} |  | (Change of variable + Lipschitz) |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | â‰¤\displaystyle\leq | Îµd+Ldâ‹…âˆ‘k=1dâˆ’1Îµkâ€‹âˆj=k+1dâˆ’1Lj\displaystyle\varepsilon\_{d}+L\_{d}\cdot\sum\_{k=1}^{d-1}\varepsilon\_{k}\prod\_{j=k+1}^{d-1}L\_{j} |  | (claim holds of dâˆ’1d-1) |
|  |  |  |  |
| --- | --- | --- | --- |
|  | =\displaystyle= | âˆ‘k=1dÎµkâ€‹âˆj=k+1dLj\displaystyle\sum\_{k=1}^{d}\varepsilon\_{k}\prod\_{j=k+1}^{d}L\_{j} |  |

âˆ

###### Lemma A.3.

Let gâˆˆIncrMLPâ€‹(n)g\in\text{IncrMLP}(n) with parameter space Î˜â€‹(n)\Theta(n). Then the map Î¸â†¦gâ€‹(â‹…;Î¸)\theta\mapsto g(\cdot;\theta) from Î˜â€‹(n)\Theta(n) to L1â€‹([0,1])L^{1}([0,1]) is continuous.

###### Proof.

It is a direct application of Lebesgueâ€™s dominated convergence theorem (Bogachev, [2007](https://arxiv.org/html/2510.15458v1#bib.bib5), Theorem 2.8.1), so we just verify that the assumptions of the theorem hold. Let Î¸kâ†’Î¸âˆˆÎ˜\theta\_{k}\to\theta\in\Theta be any convergent sequence. Since Î¸â†¦gâ€‹(u;Î¸)\theta\mapsto g(u;\theta) is continuous, we have that gâ€‹(u;Î¸k)â†’gâ€‹(u;Î¸)g(u;\theta\_{k})\to g(u;\theta) for all uâˆˆ[0,1]u\in[0,1]. Furthermore, the functions gâ€‹(â‹…;Î¸k)g(\cdot;\theta\_{k}) are uniformly bounded:

|  |  |  |  |
| --- | --- | --- | --- |
|  | supkâˆˆâ„•|gâ€‹(u;Î¸k)|\displaystyle\sup\_{k\in\mathbb{N}}|g(u;\theta\_{k})| | â‰¤supkâˆˆâ„•supuâˆˆ[0,1]|gâ€‹(u;Î¸k)|\displaystyle\leq\sup\_{k\in\mathbb{N}}\sup\_{u\in[0,1]}|g(u;\theta\_{k})| |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | â‰¤supkâˆˆâ„•maxâ¡{|gâ€‹(0;Î¸k)|,|gâ€‹(1;Î¸k)|}\displaystyle\leq\sup\_{k\in\mathbb{N}}\max\{|g(0;\theta\_{k})|,|g(1;\theta\_{k})|\} |  | (uâ†¦gâ€‹(u;Î¸)u\mapsto g(u;\theta) is increasing) |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤supÎ¸âˆˆKmaxâ¡{|gâ€‹(0;Î¸)|,|gâ€‹(1;Î¸)|}\displaystyle\leq\sup\_{\theta\in K}\max\{|g(0;\theta)|,|g(1;\theta)|\} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | <+âˆ\displaystyle<+\infty |  |

where KâŠ†Î˜K\subseteq\Theta is any compact containing the sequence {Î¸k,kâˆˆâ„•}\{\theta\_{k},k\in\mathbb{N}\} (which exists because the sequence is convergent) and the last inequality follows from the fact that Î¸â†¦maxâ¡{|gâ€‹(0;Î¸)|,|gâ€‹(1;Î¸)|}\theta\mapsto\max\{|g(0;\theta)|,|g(1;\theta)|\} is continuous (itâ€™s the minimum of two continuous functions) and therefore bounded on KK.
âˆ

###### Lemma A.4.

Let RâŠ†â„kR\subseteq\mathbb{R}^{k} be a compact set and let the functions fâ€‹(â‹…,x):[a,b]â†’â„f(\cdot,x):[a,b]\to\mathbb{R} be continuous, linear splines on a common grid a=u1<â€¦<un+1=ba=u\_{1}<\ldots<u\_{n+1}=b, for every xâˆˆRx\in R. Then there exists a subset Î˜âŠ†Î˜â€‹(n)\Theta\subseteq\Theta(n) (which depends only on the grid) such that the set-valued function Î¸~:Râ‡‰Î˜\tilde{\theta}:R\rightrightarrows\Theta, defined by

|  |  |  |
| --- | --- | --- |
|  | Î¸~(xPAâ€‹(k)):=argâ€‹minÎ¸â€²âˆˆÎ˜âˆ¥f^(â‹…,xPAâ€‹(k))âˆ’g(â‹…,Î¸â€²))âˆ¥L1â€‹([0,1]),âˆ€xPAâ€‹(k)âˆˆR\tilde{\theta}(x\_{\text{PA}(k)}):=\operatorname\*{arg\,min}\_{\theta^{\prime}\in\Theta}\;\|\hat{f}(\cdot,x\_{\text{PA}(k)})-g(\cdot,\theta^{\prime}))\|\_{L^{1}([0,1])},\quad\forall x\_{\text{PA}(k)}\in R |  |

admits a continuous selection Î¸:Râ†’Î˜\theta:R\to\Theta, such that gâ€‹(u,Î¸â€‹(x))=f^â€‹(u,x)g(u,\theta(x))=\hat{f}(u,x) for all uâˆˆ[0,1]u\in[0,1].

###### Proof.

The existence of a continuous selection follows from Michaelâ€™s theorem (Aubin & Frankowska, [2009](https://arxiv.org/html/2510.15458v1#bib.bib2), Theorem 9.1.2), provided we can show that Î¸~\tilde{\theta} is lower semi-continuous with closed and convex values.

Lower-semicontinuity actually holds regardless of the choice of the set Î˜\Theta, so we prove it first. It follows from the fact that that (xPAâ€‹(k),Î¸)â†¦âˆ¥f^(u,xPAâ€‹(k))âˆ’g(u;Î¸â€²))âˆ¥L1â€‹([0,1])(x\_{\text{PA}(k)},\theta)\mapsto\|\hat{f}(u,x\_{\text{PA}(k)})-g(u;\theta^{\prime}))\|\_{L^{1}([0,1])} is a CarathÃ©odory function (for a definition, see Rockafellar & Wets ([1998](https://arxiv.org/html/2510.15458v1#bib.bib21), Example 14.29)) and therefore a normal integrand (Rockafellar & Wets, [1998](https://arxiv.org/html/2510.15458v1#bib.bib21), Definition 14.27, Proposition 14.28). Indeed:

* â€¢

  Since (u,xPAâ€‹(k))â†¦|f^(u,xPAâ€‹(k))âˆ’g(u;Î¸))|(u,x\_{\text{PA}(k)})\mapsto|\hat{f}(u,x\_{\text{PA}(k)})-g(u;\theta))| is measurable (even continuous) for all Î¸âˆˆÎ˜\theta\in\Theta, Tonelliâ€™s theorem (Folland, [1999](https://arxiv.org/html/2510.15458v1#bib.bib12), Theorem 2.37) implies that xâ†¦âˆ¥f^(â‹…,x)âˆ’g(â‹…;Î¸))âˆ¥L1â€‹([0,1])x\mapsto\|\hat{f}(\cdot,x)-g(\cdot;\theta))\|\_{L^{1}([0,1])} is measurable.
* â€¢

  The map Î¸â†¦hâ€‹(xPAâ€‹(k),Î¸)\theta\mapsto h(x\_{\text{PA}(k)},\theta) is continuous for all xPAâ€‹(k)âˆˆâ„|PAâ€‹(k)|x\_{\text{PA}(k)}\in\mathbb{R}^{|\text{PA}(k)|} because itâ€™s the composition of two continuous maps: Î¸â†¦gâ€‹(â‹…;Î¸)âˆˆL1â€‹([0,1])\theta\mapsto g(\cdot;\theta)\in L^{1}([0,1]), which is continuous by [LemmaÂ A.3](https://arxiv.org/html/2510.15458v1#A1.Thmtheorem3 "Lemma A.3. â€£ Appendix A Auxiliary results â€£ Robust Optimization in Causal Models and ğº-Causal Normalizing Flows"), and g(â‹…;Î¸)â†¦âˆ¥f^(â‹…,x)âˆ’g(â‹…;Î¸))âˆ¥L1â€‹([0,1])g(\cdot;\theta)\mapsto\|\hat{f}(\cdot,x)-g(\cdot;\theta))\|\_{L^{1}([0,1])}, which is continuous because the norm is a continuous function.

We will now show that Î¸~\tilde{\theta} is actually singleton valued (which, of course, implies that it is closed and convex valued), by constructing a suitable set Î˜âŠ†Î˜â€‹(n)\Theta\subseteq\Theta(n). The main strategy is to realize that the weights and biases of the first layer (w(1)w^{(1)} and b(1)b^{(1)}) can be used to fully specify the segments on which the function uâ†¦gâ€‹(u,Î¸)u\mapsto g(u,\theta) is piecewise linear and that, once this choice is made, the weights and the bias of the second layer (w(2)w^{(2)} and b(2)b^{(2)}) determine *uniquely* the slope and intercepts on each segment.

More specifically, given the grid a=u1<u2<â€¦<un+1=ba=u\_{1}<u\_{2}<\ldots<u\_{n+1}=b, denote by

|  |  |  |
| --- | --- | --- |
|  | Î”â€‹ui:=ui+1âˆ’ui\Delta u\_{i}:=u\_{i+1}-u\_{i} and mi:=12â€‹(ui+1+ui)m\_{i}:=\frac{1}{2}(u\_{i+1}+u\_{i}), â€ƒ(i=1,â€¦,ni=1,\ldots,n) |  |

the width and the midpoint of each grid segment, respectively. If we set

|  |  |  |
| --- | --- | --- |
|  | wÂ¯i(1)=2/Î”â€‹ui\bar{w}^{(1)}\_{i}=2/\Delta u\_{i}, â€ƒbÂ¯i(1)=âˆ’miâ€‹Î”â€‹ui\bar{b}^{(1)}\_{i}=-m\_{i}\Delta u\_{i}, â€ƒ(i=1,â€¦,ni=1,\ldots,n) |  |

and define Î˜:={wÂ¯(1)}Ã—{bÂ¯(1)}Ã—â„>0nÃ—â„âŠ†Î˜â€‹(n)\Theta:=\{\bar{w}^{(1)}\}\times\{\bar{b}^{(1)}\}\times\mathbb{R}\_{>0}^{n}\times\mathbb{R}\subseteq\Theta(n), then gâ€‹(â‹…,Î¸)g(\cdot,\theta) is piecewise linear exactly on the grid {ui}i=1n+1\{u\_{i}\}\_{i=1}^{n+1}, for any Î¸âˆˆÎ˜\theta\in\Theta. Additionally on each segment [ui,ui+1][u\_{i},u\_{i+1}], the function gâ€‹(â‹…,Î¸)g(\cdot,\theta) has slope

|  |  |  |
| --- | --- | --- |
|  | wi(2)â€‹(wÂ¯i(1)+Î±2â€‹âˆ‘jâ‰ iwÂ¯j(1))w^{(2)}\_{i}\left(\bar{w}^{(1)}\_{i}+\frac{\alpha}{2}\sum\_{j\neq i}\bar{w}^{(1)}\_{j}\right) |  |

and bias

|  |  |  |
| --- | --- | --- |
|  | wi(2)â€‹bÂ¯i(1)+nâ€‹b(2)+(nâˆ’1)â€‹Î±2â€‹wj(2)â€‹bÂ¯j(1)+(Î±2âˆ’1)â€‹(âˆ‘j<iwj(2)âˆ’âˆ‘j>iwj(2)).w^{(2)}\_{i}\bar{b}^{(1)}\_{i}+nb^{(2)}+(n-1)\frac{\alpha}{2}w^{(2)}\_{j}\bar{b}^{(1)}\_{j}+\left(\frac{\alpha}{2}-1\right)\left(\sum\_{j<i}w^{(2)}\_{j}-\sum\_{j>i}w^{(2)}\_{j}\right). |  |

We can therefore exactly match any continuous, strictly increasing, piecewise linear function on the grid {ui}i=1n+1\{u\_{i}\}\_{i=1}^{n+1} by matching the slope and intercept on [u1,u2][u\_{1},u\_{2}], together with the slopes on each of the remaining segments (the intercepts will be automatically matched by continuity). This is a linear system of n+1n+1 equations in n+1n+1 unknowns and it always admits a unique solution (as can be readily checked), which implies that for every xâˆˆRx\in R we can find a Î¸âˆˆÎ˜\theta\in\Theta such that gâ€‹(u,Î¸)=f^â€‹(u,x)g(u,\theta)=\hat{f}(u,x) for all uâˆˆ[a,b]u\in[a,b].
âˆ

###### Lemma A.5.

Let gâˆˆIncrMLPâ€‹(n)g\in\text{IncrMLP}(n). Then Î¸â†¦gâ€‹(u,Î¸)\theta\mapsto g(u,\theta) is locally Lipschitz uniformly in uâˆˆ[0,1]u\in[0,1], i.e. for every compact KâŠ†Î˜â€‹(n)K\subseteq\Theta(n) there exists an L>0L>0 such that:

|  |  |  |
| --- | --- | --- |
|  | |gâ€‹(u,Î¸)âˆ’gâ€‹(u,Î¸â€²)|â‰¤Lâ€‹â€–Î¸âˆ’Î¸^â€–,âˆ€Î¸,Î¸^âˆˆK,âˆ€uâˆˆ[0,1].|g(u,\theta)-g(u,\theta^{\prime})|\leq L\|\theta-\hat{\theta}\|,\quad\forall\theta,\hat{\theta}\in K,\;\forall u\in[0,1]. |  |

###### Proof.

The proof follows by direct computation. We use repeatedly the Cauchy-Schwartz inequality, the fact that the activation Ï\rho is 1-Lipschitz and that â€–uâ€–â‰¤1\|u\|\leq 1:

|  |  |  |  |
| --- | --- | --- | --- |
|  | |gâ€‹(u,Î¸)âˆ’gâ€‹(u,Î¸^)|â‰¤\displaystyle|g(u,\theta)-g(u,\hat{\theta})|\leq | |âŸ¨w(2),ÏâŠ—nâ€‹(uâ€‹w(1)+b(1))âŸ©+b(2)âˆ’âŸ¨w^(2),ÏâŠ—nâ€‹(uâ€‹w^(1)+b^(1))âŸ©âˆ’b^(2)|\displaystyle|\langle w^{(2)},\rho^{\otimes n}(uw^{(1)}+b^{(1)})\rangle+b^{(2)}-\langle\hat{w}^{(2)},\rho^{\otimes n}(u\hat{w}^{(1)}+\hat{b}^{(1)})\rangle-\hat{b}^{(2)}| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤\displaystyle\leq | |âŸ¨w(2),ÏâŠ—nâ€‹(uâ€‹w(1)+b(1))âŸ©âˆ’âŸ¨w^(2),ÏâŠ—nâ€‹(uâ€‹w(1)+b(1))âŸ©|\displaystyle|\langle w^{(2)},\rho^{\otimes n}(uw^{(1)}+b^{(1)})\rangle-\langle\hat{w}^{(2)},\rho^{\otimes n}(uw^{(1)}+b^{(1)})\rangle| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +|âŸ¨w^(2),ÏâŠ—nâ€‹(uâ€‹w(1)+b(1))âŸ©âˆ’âŸ¨w^(2),ÏâŠ—nâ€‹(uâ€‹w^(1)+b^(1))âŸ©|+|b(2)âˆ’b^(2)|\displaystyle+|\langle\hat{w}^{(2)},\rho^{\otimes n}(uw^{(1)}+b^{(1)})\rangle-\langle\hat{w}^{(2)},\rho^{\otimes n}(u\hat{w}^{(1)}+\hat{b}^{(1)})\rangle|+|b^{(2)}-\hat{b}^{(2)}| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | =\displaystyle= | |âŸ¨w(2)âˆ’w^(2),ÏâŠ—nâ€‹(uâ€‹w(1)+b(1))âŸ©|\displaystyle|\langle w^{(2)}-\hat{w}^{(2)},\rho^{\otimes n}(uw^{(1)}+b^{(1)})\rangle| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +|âŸ¨w^(2),ÏâŠ—nâ€‹(uâ€‹w(1)+b(1))âˆ’ÏâŠ—nâ€‹(uâ€‹w^(1)+b^(1))âŸ©|+|b(2)âˆ’b^(2)|\displaystyle+|\langle\hat{w}^{(2)},\rho^{\otimes n}(uw^{(1)}+b^{(1)})-\rho^{\otimes n}(u\hat{w}^{(1)}+\hat{b}^{(1)})\rangle|+|b^{(2)}-\hat{b}^{(2)}| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤\displaystyle\leq | â€–w(2)âˆ’w^(2)â€–â€‹â€–ÏâŠ—nâ€‹(uâ€‹w(1)+b(1))â€–\displaystyle\|w^{(2)}-\hat{w}^{(2)}\|\|\rho^{\otimes n}(uw^{(1)}+b^{(1)})\| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +â€–w^(2)â€–â€‹â€–ÏâŠ—nâ€‹(uâ€‹w(1)+b(1))âˆ’ÏâŠ—nâ€‹(uâ€‹w^(1)+b^(1))â€–+|b(2)âˆ’b^(2)|\displaystyle+\|\hat{w}^{(2)}\|\|\rho^{\otimes n}(uw^{(1)}+b^{(1)})-\rho^{\otimes n}(u\hat{w}^{(1)}+\hat{b}^{(1)})\|+|b^{(2)}-\hat{b}^{(2)}| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤\displaystyle\leq | âˆ¥w(2)âˆ’w^(2)âˆ¥(âˆ¥w(1)âˆ¥+âˆ¥b(1))âˆ¥)+âˆ¥w^(2)âˆ¥(âˆ¥w(1)âˆ’w^(1)âˆ¥+âˆ¥b(1)âˆ’b^(1)âˆ¥+|b(2)âˆ’b^(2)|)\displaystyle\|w^{(2)}-\hat{w}^{(2)}\|(\|w^{(1)}\|+\|b^{(1)})\|)+\|\hat{w}^{(2)}\|(\|w^{(1)}-\hat{w}^{(1)}\|+\|b^{(1)}-\hat{b}^{(1)}\|+|b^{(2)}-\hat{b}^{(2)}|) |  |

Since the parameters are contained in a compact KK, their norms are bounded by a constant, say M>0M>0, so that:

|  |  |  |  |
| --- | --- | --- | --- |
|  | |gâ€‹(u,Î¸)âˆ’gâ€‹(u,Î¸^)|\displaystyle|g(u,\theta)-g(u,\hat{\theta})| | â‰¤2â€‹Mâ€‹(â€–w(2)âˆ’w^(2)â€–+â€–w(1)âˆ’w^(1)â€–+â€–b(1)âˆ’b^(1)â€–+|b(2)âˆ’b^(2)|)\displaystyle\leq 2M(\|w^{(2)}-\hat{w}^{(2)}\|+\|w^{(1)}-\hat{w}^{(1)}\|+\|b^{(1)}-\hat{b}^{(1)}\|+|b^{(2)}-\hat{b}^{(2)}|) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤2â€‹Mâ€‹4â€‹â€–Î¸âˆ’Î¸^â€–\displaystyle\leq 2M\sqrt{4}\|\theta-\hat{\theta}\| |  |

where the last inequality is due to Cauchy-Schwartz (this time applied to the (four-dimensional) vector of parametersâ€™ norms and the four-dimensional unit vector).
âˆ

###### Lemma A.6.

Let (u,xPAâ€‹(k))â†¦Fkâˆ’1â€‹(u|xPAâ€‹(k))(u,x\_{\text{PA}}(k))\mapsto F^{-1}\_{k}(u\>|\>x\_{\text{PA}(k)}) be as in [TheoremÂ 4.4](https://arxiv.org/html/2510.15458v1#S4.Thmtheorem4 "Theorem 4.4 (Universal Approximation Property (UAP) for ğº-causal normalizing flows). â€£ 4 Proposed method: ğº-causal normalizing flows â€£ Robust Optimization in Causal Models and ğº-Causal Normalizing Flows"). Then:

1. i)

   Fkâˆ’1â€‹(u|xPAâ€‹(k))âˆˆL1â€‹(dâ€‹uâŠ—Î¼â€‹(dâ€‹xPAâ€‹(k)))F\_{k}^{-1}(u\>|\>x\_{\text{PA}(k)})\in L^{1}(du\otimes\mu(dx\_{\text{PA}(k)})),
2. ii)

   âˆ‚uFkâˆ’1â€‹(u|xPAâ€‹(k))âˆˆL1â€‹(dâ€‹uâŠ—Î¼â€‹(dâ€‹xPAâ€‹(k)))\partial\_{u}F\_{k}^{-1}(u\>|\>x\_{\text{PA}(k)})\in L^{1}(du\otimes\mu(dx\_{\text{PA}(k)})),
3. iii)

   âˆ‚xjFkâˆ’1â€‹(u|xPAâ€‹(k))âˆˆL1â€‹(dâ€‹uâŠ—Î¼â€‹(dâ€‹xPAâ€‹(k)))\partial\_{x\_{j}}F\_{k}^{-1}(u\>|\>x\_{\text{PA}(k)})\in L^{1}(du\otimes\mu(dx\_{\text{PA}(k)})), for all jâˆˆPAâ€‹(k)j\in\text{PA}(k).

###### Proof.

1. i)

   By direct integration:

   |  |  |  |
   | --- | --- | --- |
   |  | âˆ«â„|PAâ€‹(k)|Î¼(dxPAâ€‹(k))âˆ«[0,1]du|Fkâˆ’1(u|xPAâ€‹(k))|\displaystyle\int\_{\mathbb{R}^{|\text{PA}(k)|}}\mu(dx\_{\text{PA}(k)})\int\_{[0,1]}du|F\_{k}^{-1}(u\>|\>x\_{\text{PA}(k)})| |  |
   |  |  |  |
   | --- | --- | --- |
   |  | =âˆ«â„|PAâ€‹(k)|Î¼â€‹(dâ€‹xPAâ€‹(k))â€‹âˆ«â„Î¼â€‹(dâ€‹xk|xPAâ€‹(k))â€‹|xk|\displaystyle=\int\_{\mathbb{R}^{|\text{PA}(k)|}}\mu(dx\_{\text{PA}(k)})\int\_{\mathbb{R}}\mu(dx\_{k}\>|\>x\_{\text{PA}(k)})|x\_{k}| |  |
   |  |  |  |
   | --- | --- | --- |
   |  | =âˆ«â„Î¼â€‹(dâ€‹xk)â€‹|xk|â‰¤+âˆ\displaystyle=\int\_{\mathbb{R}}\mu(dx\_{k})|x\_{k}|\leq+\infty |  |

   where we have first used the change-of-variable formula (Bogachev, [2007](https://arxiv.org/html/2510.15458v1#bib.bib5), Theorem 3.6.1) with Fkâˆ’1(â‹…|xPAâ€‹(k))#ğ’°[0,1]=Î¼(dxk|xPAâ€‹(k)){F\_{k}^{-1}(\cdot\>|\>x\_{\text{PA}(k)})}\_{\#}\mathcal{U}[0,1]=\mu(dx\_{k}\>|\>x\_{\text{PA}(k)}) (McNeil etÂ al., [2015](https://arxiv.org/html/2510.15458v1#bib.bib16), Proposition A.6) and then used the fact that Î¼\mu has finite first moments.
2. ii)

   uâ†¦Fâˆ’1â€‹(u|x)u\mapsto F^{-1}(u\>|\>x) is increasing on the closed interval [0,1][0,1], therefore by Bogachev ([2007](https://arxiv.org/html/2510.15458v1#bib.bib5), Corollary 5.2.7) it is almost everywhere differentiable and:
   âˆ«\_[0,1] â€”âˆ‚\_u F\_k^-1(u â€… â€” â€…x\_PA(k))â€” du â‰¤F\_k^-1(1 â€… â€” â€…x\_PA(k)) - F\_k^-1(0 â€… â€” â€…x\_PA(k)).
   The right-hand side is just diamâ€‹(suppâ€‹(Î¼â€‹(dâ€‹xk|xPAâ€‹(k))))\text{diam}(\mathrm{supp}(\mu(dx\_{k}\>|\>x\_{\text{PA}(k)}))), which is finite, since Î¼\mu is compactly supported.
3. iii)

   Continuity of uâ†¦Fkâ€‹(u|xPAâ€‹(k))u\mapsto F\_{k}(u\>|\>x\_{\text{PA}(k)}) implies that Fkâ€‹(Fkâˆ’1â€‹(u|xPAâ€‹(k)))=uF\_{k}(F\_{k}^{-1}(u\>|\>x\_{\text{PA}(k)}))=u (McNeil etÂ al., [2015](https://arxiv.org/html/2510.15458v1#bib.bib16), Proposition A.3 (viii)). Differentiating this expression on both sides and using the chain rule yields:

   |  |  |  |  |
   | --- | --- | --- | --- |
   |  | âˆ«[0,1]|âˆ‚xjFkâˆ’1(u|xPAâ€‹(k))|du\displaystyle\int\_{[0,1]}\left|\partial\_{x\_{j}}F\_{k}^{-1}(u\>|\>x\_{\text{PA}(k)})\right|du | =âˆ«[0,1]ğ‘‘uâ€‹|âˆ’âˆ‚xjFkâ€‹(Fkâˆ’1â€‹(u|xPAâ€‹(k))|xPAâ€‹(k))âˆ‚uFkâ€‹(Fkâˆ’1â€‹(u|xPAâ€‹(k))|xPAâ€‹(k))|\displaystyle=\int\_{[0,1]}du\left|-\frac{\partial\_{x\_{j}}F\_{k}(F\_{k}^{-1}(u\>|\>x\_{\text{PA}(k)})\>|\>x\_{\text{PA}(k)})}{\partial\_{u}F\_{k}(F\_{k}^{-1}(u\>|\>x\_{\text{PA}(k)})\>|\>x\_{\text{PA}(k)})}\right| |  |
   |  |  |  |  |
   | --- | --- | --- | --- |
   |  |  | =âˆ«â„dxâ€²|âˆ’âˆ‚xjFk(xâ€²|xPAâ€‹(k))|,\displaystyle=\int\_{\mathbb{R}}dx^{\prime}\left|-\partial\_{x\_{j}}F\_{k}(x^{\prime}\>|\>x\_{\text{PA}(k)})\right|, |  |

   where the second equality follows from the same change-of-variable as in part (i) and by simplifying the conditional density.
   The claim now follows by integrating over â„PAâ€‹(k)\mathbb{R}^{\text{PA}(k)} with respect to Î¼â€‹(dâ€‹xPAâ€‹(k))\mu(dx\_{\text{PA}(k)}) and using the assumption that (xk,xPAâ€‹(k))â†¦Fkâ€‹(xk|xPAâ€‹(k))(x\_{k},x\_{\text{PA}(k)})\mapsto F\_{k}(x\_{k}\>|\>x\_{\text{PA}(k)}) is a C1C^{1} map and therefore admits bounded partial derivatives on compacts.

âˆ

## Appendix B Proofs

### B.1 Proof of [TheoremÂ 3.7](https://arxiv.org/html/2510.15458v1#S3.Thmtheorem7 "Theorem 3.7 (Continuity of ğº-causal optimization problems). â€£ 3 Robust optimization in Structural Causal Models â€£ Robust Optimization in Causal Models and ğº-Causal Normalizing Flows")

###### Proof.

We generalize the proof by Acciaio etÂ al. ([2024](https://arxiv.org/html/2510.15458v1#bib.bib1)) to our GG-causal setting. Given Î¼,Î½âˆˆğ’«Gâ€‹(â„d)\mu,\nu\in\mathcal{P}\_{G}(\mathbb{R}^{d}), let gg be a GG-causal function and let Ï€\pi be the optimal GG-bicausal coupling between Î¼\mu and Î½\nu. Then:

|  |  |  |  |
| --- | --- | --- | --- |
|  | âˆ’ğ”¼Î½â€‹[Qâ€‹(XT,gâ€‹(XVâˆ–T))]\displaystyle-\mathbb{E}^{\nu}\left[Q(X\_{T},g(X\_{V\setminus T}))\right] | =âˆ’âˆ«Qâ€‹(xTâ€²,gâ€‹(xVâˆ–Tâ€²))â€‹Î½â€‹(dâ€‹xâ€²)\displaystyle=-\int Q(x^{\prime}\_{T},g(x^{\prime}\_{V\setminus T}))\nu(dx^{\prime}) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =âˆ’âˆ«Qâ€‹(xTâ€²,gâ€‹(xVâˆ–Tâ€²))â€‹Ï€â€‹(dâ€‹x,dâ€‹xâ€²)\displaystyle=-\int Q(x^{\prime}\_{T},g(x^{\prime}\_{V\setminus T}))\pi(dx,dx^{\prime}) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =âˆ«(Qâ€‹(xT,gâ€‹(xVâˆ–Tâ€²))âˆ’Qâ€‹(xTâ€²,gâ€‹(xVâˆ–Tâ€²)))â€‹Ï€â€‹(dâ€‹x,dâ€‹xâ€²)âˆ’âˆ«Qâ€‹(xT,gâ€‹(xVâˆ–Tâ€²))â€‹Ï€â€‹(dâ€‹x,dâ€‹xâ€²)\displaystyle=\int\left(Q(x\_{T},g(x^{\prime}\_{V\setminus T}))-Q(x^{\prime}\_{T},g(x^{\prime}\_{V\setminus T}))\right)\pi(dx,dx^{\prime})-\int Q(x\_{T},g(x^{\prime}\_{V\setminus T}))\pi(dx,dx^{\prime}) |  |

Since xâ†¦Qâ€‹(x,g)x\mapsto Q(x,g) is uniformly locally LL-Lipschitz, the first integral satisfies:

|  |  |  |  |
| --- | --- | --- | --- |
|  | âˆ«(Qâ€‹(xT,gâ€‹(xVâˆ–Tâ€²))âˆ’Qâ€‹(xTâ€²,gâ€‹(xVâˆ–Tâ€²)))â€‹Ï€â€‹(dâ€‹x,dâ€‹xâ€²)\displaystyle\int\left(Q(x\_{T},g(x^{\prime}\_{V\setminus T}))-Q(x^{\prime}\_{T},g(x^{\prime}\_{V\setminus T}))\right)\pi(dx,dx^{\prime}) | â‰¤Lâ€‹âˆ«â€–xTâˆ’xTâ€²â€–â€‹Ï€â€‹(dâ€‹x,dâ€‹xâ€²)\displaystyle\leq L\int\|x\_{T}-x^{\prime}\_{T}\|\pi(dx,dx^{\prime}) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤Lâ€‹âˆ«â€–xâˆ’xâ€²â€–â€‹Ï€â€‹(dâ€‹x,dâ€‹xâ€²)\displaystyle\leq L\int\|x-x^{\prime}\|\pi(dx,dx^{\prime}) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =Lâ‹…WGâ€‹(Î¼,Î½)\displaystyle=L\cdot W\_{G}(\mu,\nu) |  |

For the second integral, we notice that:

|  |  |  |  |
| --- | --- | --- | --- |
|  | âˆ’âˆ«Qâ€‹(xT,gâ€‹(xVâˆ–Tâ€²))â€‹Ï€â€‹(dâ€‹x,dâ€‹xâ€²)\displaystyle-\int Q(x\_{T},g(x^{\prime}\_{V\setminus T}))\pi(dx,dx^{\prime}) | â‰¤âˆ’âˆ«Qâ€‹(xT,âˆ«gâ€‹(xVâˆ–Tâ€²)â€‹Ï€â€‹(dâ€‹xâ€²|x))â€‹Î¼â€‹(dâ€‹x)\displaystyle\leq-\int Q\bigg(x\_{T},\int g(x^{\prime}\_{V\setminus T})\pi(dx^{\prime}\>|\>x)\bigg)\mu(dx) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =âˆ’âˆ«Qâ€‹(xT,âˆ«gâ€‹(xPAâ€‹(T)â€²)â€‹Ï€â€‹(dâ€‹xâ€²|x)âŸhâ€‹(x))â€‹Î¼â€‹(dâ€‹x)\displaystyle=-\int Q\bigg(x\_{T},\underbrace{\int g(x^{\prime}\_{\text{PA}(T)})\pi(dx^{\prime}\>|\>x)}\_{h(x)}\bigg)\mu(dx) |  |

where we first applied Jensenâ€™s inequality and then the fact that gg is GG-causal. Furthermore, since Ï€\pi is GG-causal, the function hâ€‹(x):=âˆ«gâ€‹(xVâˆ–Tâ€²)â€‹Ï€â€‹(dâ€‹xâ€²|x)h(x):=\int g(x^{\prime}\_{V\setminus T})\pi(dx^{\prime}\>|\>x) actually depends only on xPAâ€‹(T)âˆªPAâ€‹(PAâ€‹(T))x\_{\text{PA}(T)\cup\text{PA}(\text{PA}(T))}. To ease the notation, denote A:=PAâ€‹(T)âˆªPAâ€‹(PAâ€‹(T))A:=\text{PA}(T)\cup\text{PA}(\text{PA}(T)). Then:

|  |  |  |  |
| --- | --- | --- | --- |
|  | âˆ’âˆ«Qâ€‹(xT,hâ€‹(xA))â€‹Î¼â€‹(dâ€‹x)\displaystyle-\int Q\left(x\_{T},h(x\_{A})\right)\mu(dx) | =âˆ’âˆ«Î¼â€‹(dâ€‹xA)â€‹âˆ«Î¼â€‹(dâ€‹xT|xA)â€‹Qâ€‹(xT,hâ€‹(xA))\displaystyle=-\int\mu(dx\_{A})\int\mu(dx\_{T}\>|\>x\_{A})Q(x\_{T},h(x\_{A})) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =âˆ’âˆ«Î¼â€‹(dâ€‹xA)â€‹âˆ«Î¼â€‹(dâ€‹xT|xPAâ€‹(T))â€‹Qâ€‹(xT,hâ€‹(xA))\displaystyle=-\int\mu(dx\_{A})\int\mu(dx\_{T}\>|\>x\_{\text{PA}(T)})Q(x\_{T},h(x\_{A})) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤âˆ’âˆ«Î¼â€‹(dâ€‹xA)â€‹âˆ«Î¼â€‹(dâ€‹xT|xPAâ€‹(T))â€‹Qâ€‹(xT,hâˆ—â€‹(xPAâ€‹(T)))\displaystyle\leq-\int\mu(dx\_{A})\int\mu(dx\_{T}\>|\>x\_{\text{PA}(T)})Q(x\_{T},h^{\*}(x\_{\text{PA}(T)})) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =âˆ’ğ’±â€‹(Î¼)\displaystyle=-\mathcal{V}(\mu) |  |

where in the second equality we have used the fact that XTâŸ‚âŸ‚XA|XPAâ€‹(T)X\_{T}\perp\!\!\!\perp X\_{A}\>|\>X\_{\text{PA}(T)} (see condition (ii) in [DefinitionÂ 2.2](https://arxiv.org/html/2510.15458v1#S2.Thmtheorem2 "Definition 2.2 (G-compatible distribution). â€£ 2.2 ğº-causal Wasserstein distance â€£ 2 Background â€£ Robust Optimization in Causal Models and ğº-Causal Normalizing Flows") or simply notice that XPAâ€‹(T)X\_{\text{PA}(T)} dd-separates XTX\_{T} and XPAâ€‹(PAâ€‹(T))X\_{\text{PA}(\text{PA}(T))}), while the inequality is due to [Eq.Â 1](https://arxiv.org/html/2510.15458v1#S3.E1 "In Definition 3.4 (ğº-causal optimization problem). â€£ 3 Robust optimization in Structural Causal Models â€£ Robust Optimization in Causal Models and ğº-Causal Normalizing Flows").

Putting everything together:

|  |  |  |
| --- | --- | --- |
|  | âˆ’ğ”¼Î½â€‹[Qâ€‹(Y,gâ€‹(X))]â‰¤Lâ‹…WGâ€‹(Î¼,Î½)âˆ’ğ’±â€‹(Î¼)-\mathbb{E}^{\nu}\left[Q(Y,g(X))\right]\leq L\cdot W\_{G}(\mu,\nu)-\mathcal{V}(\mu) |  |

and, since gg is arbitrary, we obtain:

|  |  |  |
| --- | --- | --- |
|  | ğ’±â€‹(Î¼)âˆ’ğ’±â€‹(Î½)â‰¤Lâ‹…WGâ€‹(Î¼,Î½).\mathcal{V}(\mu)-\mathcal{V}(\nu)\leq L\cdot W\_{G}(\mu,\nu). |  |

By symmetry, exchanging Î¼\mu and Î½\nu yields the same inequality for the term ğ’±â€‹(Î½)âˆ’ğ’±â€‹(Î¼)\mathcal{V}(\nu)-\mathcal{V}(\mu), therefore

|  |  |  |
| --- | --- | --- |
|  | |ğ’±â€‹(Î¼)âˆ’ğ’±â€‹(Î½)|â‰¤Lâ‹…WGâ€‹(Î¼,Î½).|\mathcal{V}(\mu)-\mathcal{V}(\nu)|\leq L\cdot W\_{G}(\mu,\nu). |  |

âˆ

### B.2 Proof of [TheoremÂ 4.4](https://arxiv.org/html/2510.15458v1#S4.Thmtheorem4 "Theorem 4.4 (Universal Approximation Property (UAP) for ğº-causal normalizing flows). â€£ 4 Proposed method: ğº-causal normalizing flows â€£ Robust Optimization in Causal Models and ğº-Causal Normalizing Flows")

###### Proof.

We know that Î¼=T#â€‹ğ’©â€‹(0,Id)\mu={T}\_{\#}\mathcal{N}(0,I\_{d}), where T=T(d)âˆ˜â‹¯âˆ˜T(1)T=T^{(d)}\circ\cdots\circ T^{(1)} is the GG-compatible, increasing transformation in the statement of [TheoremÂ 4.3](https://arxiv.org/html/2510.15458v1#S4.Thmtheorem3 "Theorem 4.3. â€£ 4 Proposed method: ğº-causal normalizing flows â€£ Robust Optimization in Causal Models and ğº-Causal Normalizing Flows"). Now, let T^=T^(d)âˆ˜â‹¯âˆ˜T^(1)âˆˆGâ€‹-NFâ€‹(d)\hat{T}=\hat{T}^{(d)}\circ\cdots\circ\hat{T}^{(1)}\in G\text{-}\text{NF}(d) be a G-NFwith flows as in [Eq.Â 2](https://arxiv.org/html/2510.15458v1#S4.E2 "In 4 Proposed method: ğº-causal normalizing flows â€£ Robust Optimization in Causal Models and ğº-Causal Normalizing Flows") and define the GG-bicausal coupling Ï€:=(T,T^)#â€‹ğ’©â€‹(0,1)\pi:={(T,\hat{T})}\_{\#}\mathcal{N}(0,1), then we have that:

|  |  |  |
| --- | --- | --- |
|  | WGâ€‹(Î¼,T^#â€‹Î»)â‰¤âˆ«â„dÃ—â„dâ€–xâˆ’xâ€²â€–â€‹Ï€â€‹(dâ€‹x,dâ€‹xâ€²)=âˆ«[0,1]dâ€–Tâ€‹(u)âˆ’T^â€‹(u)â€–â€‹ğ‘‘u.W\_{G}(\mu,{\hat{T}}\_{\#}\lambda)\leq\int\_{\mathbb{R}^{d}\times\mathbb{R}^{d}}\|x-x^{\prime}\|\pi(dx,dx^{\prime})=\int\_{[0,1]^{d}}\|T(u)-\hat{T}(u)\|du. |  |

We can make the right-hand side smaller than any Îµ>0\varepsilon>0 by using [LemmaÂ A.2](https://arxiv.org/html/2510.15458v1#A1.Thmtheorem2 "Lemma A.2 (Composition lemma). â€£ Appendix A Auxiliary results â€£ Robust Optimization in Causal Models and ğº-Causal Normalizing Flows") (with ğ’³:=[0,1]d\mathcal{X}:=[0,1]^{d}, Î¼(0):=ğ’°â€‹([0,1]d)\mu^{(0)}:=\mathcal{U}([0,1]^{d}) and Î¼(k):=Î¼1:kâŠ—ğ’°â€‹([0,1]dâˆ’k)\mu^{(k)}:=\mu\_{1:k}\otimes\mathcal{U}([0,1]^{d-k}), for k=1,â€¦,dk=1,\ldots,d), provided that we can show that conditions (i) and (ii) therein hold.

Condition (i). Each hypercoupling flow T^(k)\hat{T}^{(k)} differs from the identity only at its kk-th coordinate, which is the output of a shallow MLP (see [Eq.Â 2](https://arxiv.org/html/2510.15458v1#S4.E2 "In 4 Proposed method: ğº-causal normalizing flows â€£ Robust Optimization in Causal Models and ğº-Causal Normalizing Flows")) . But shallow MLPs are Lipschitz functions of their input, therefore each T^(k)\hat{T}^{(k)} is a Lipschitz function.

Condition (ii). We need to show that for every Îµ>0\varepsilon>0, there exists an nâˆˆâ„•n\in\mathbb{N}, a Î¸^â€‹(â‹…)âˆˆMLP\hat{\theta}(\cdot)\in\text{MLP} and a gâ€‹(â‹…,Î¸^â€‹(xPAâ€‹(k)))âˆˆIncrMLPâ€‹(n)g(\cdot,\hat{\theta}(x\_{\text{PA}(k)}))\in\text{IncrMLP}(n) such that

|  |  |  |  |
| --- | --- | --- | --- |
|  | âˆ«[0,1]duâˆ«â„|PAâ€‹(k)|Î¼(dxPAâ€‹(k))|Fkâˆ’1(u|xPAâ€‹(k))âˆ’g(u;Î¸^(xPAâ€‹(k)))|â‰¤Îµ.\int\_{[0,1]}du\int\_{\mathbb{R}^{|\text{PA}(k)|}}\mu(dx\_{\text{PA}(k)})|F^{-1}\_{k}(u\>|\>x\_{\text{PA}(k)})-g(u;\hat{\theta}(x\_{\text{PA}(k)}))|\leq\varepsilon. |  | (7) |

We will prove this bound by splitting the error into three terms and bounding each one separately.

Term 1. First we approximate (u,xPAâ€‹(k))â†¦Fkâˆ’1â€‹(u|xPAâ€‹(k))(u,x\_{\text{PA}}(k))\mapsto F^{-1}\_{k}(u\>|\>x\_{\text{PA}(k)}) with a continuous tensor-product linear spline, f^â€‹(u,xPAâ€‹(k))\hat{f}(u,x\_{\text{PA}(k)}), on the rectangle [0,1]Ã—R[0,1]\times R, where R=âˆj=1|PAâ€‹(k)|[aj,bj]R=\prod\_{j=1}^{|\text{PA}(k)|}[a\_{j},b\_{j}] is a rectangle large enough to contain the compact support of Î¼â€‹(dâ€‹xPAâ€‹(k))\mu(dx\_{\text{PA}(k)}). We choose the approximation grid fine enough to satisfy:

|  |  |  |
| --- | --- | --- |
|  | âˆ«[0,1]duâˆ«â„|PAâ€‹(k)|Î¼(dxPAâ€‹(k))|Fkâˆ’1(u|xPAâ€‹(k))âˆ’f^(u,xPAâ€‹(k))|â‰¤Îµ/2,\int\_{[0,1]}du\int\_{\mathbb{R}^{|\text{PA}(k)|}}\mu(dx\_{\text{PA}(k)})|F^{-1}\_{k}(u\>|\>x\_{\text{PA}(k)})-\hat{f}(u,x\_{\text{PA}(k)})|\leq\varepsilon/2, |  |

and let n+1n+1 be the number of gridpoints in the uu-axis (i.e. the grid on [0,1][0,1] has gridpoints 0=u1<â€¦<un+1=1)0=u\_{1}<\ldots<u\_{n+1}=1).

The validity of this approximation follows from (Schumaker, [2007](https://arxiv.org/html/2510.15458v1#bib.bib23), Theorem 12.7) and requires that (u,x)â†¦Fâˆ’1â€‹(u|x)(u,x)\mapsto F^{-1}(u|x) belong to a suitable tensor Sobolev space (Schumaker, [2007](https://arxiv.org/html/2510.15458v1#bib.bib23), Example 13.5), as we verify in [LemmaÂ A.6](https://arxiv.org/html/2510.15458v1#A1.Thmtheorem6 "Lemma A.6. â€£ Appendix A Auxiliary results â€£ Robust Optimization in Causal Models and ğº-Causal Normalizing Flows").

Term 2. Next, we approximate the univariate functions uâ†¦f^â€‹(u,xPAâ€‹(k))u\mapsto\hat{f}(u,x\_{\text{PA}(k)}), for each xPAâ€‹(k)âˆˆRx\_{\text{PA}(k)}\in R, with neural networks gâ€‹(â‹…;Î¸â€‹(xPAâ€‹(k)))âˆˆIncrMLPâ€‹(n)g(\cdot;\theta(x\_{\text{PA}(k)}))\in\text{IncrMLP}(n), by judiciously choosing the function Î¸:Râ†’Î˜â€‹(n)\theta:R\to\Theta(n).

Since all the functions f^â€‹(â‹…,xPAâ€‹(k))\hat{f}(\cdot,x\_{\text{PA}(k)}) share the same grid on [0,1][0,1], by [LemmaÂ A.4](https://arxiv.org/html/2510.15458v1#A1.Thmtheorem4 "Lemma A.4. â€£ Appendix A Auxiliary results â€£ Robust Optimization in Causal Models and ğº-Causal Normalizing Flows") there exists a parameter subset Î˜âŠ†Î˜â€‹(n)\Theta\subseteq\Theta(n) (which depends only on this grid) such that the set-valued map Î¸~:Râ‡‰Î˜\tilde{\theta}:R\rightrightarrows\Theta, defined as

|  |  |  |
| --- | --- | --- |
|  | Î¸~(xPAâ€‹(k)):=argâ€‹minÎ¸â€²âˆˆÎ˜âˆ¥f^(â‹…,xPAâ€‹(k))âˆ’g(â‹…,Î¸â€²))âˆ¥L1â€‹([0,1]),\tilde{\theta}(x\_{\text{PA}(k)}):=\operatorname\*{arg\,min}\_{\theta^{\prime}\in\Theta}\;\|\hat{f}(\cdot,x\_{\text{PA}(k)})-g(\cdot,\theta^{\prime}))\|\_{L^{1}([0,1])}, |  |

admits a continuous selection Î¸:Râ†’Î˜\theta:R\rightarrow\Theta. We then use this function Î¸\theta to parametrize the neural networks gâ€‹(â‹…,Î¸â€‹(xPAâ€‹(k)))g(\cdot,\theta(x\_{\text{PA}(k)})) and, as implied by [LemmaÂ A.4](https://arxiv.org/html/2510.15458v1#A1.Thmtheorem4 "Lemma A.4. â€£ Appendix A Auxiliary results â€£ Robust Optimization in Causal Models and ğº-Causal Normalizing Flows"), this parametrization is optimal, in the sense that gâ€‹(u,Î¸â€‹(xPAâ€‹(k)))=f^â€‹(u,xPAâ€‹(k))g(u,\theta(x\_{\text{PA}(k)}))=\hat{f}(u,x\_{\text{PA}(k)}) for all uâˆˆ[0,1]u\in[0,1], thus achieving zero approximation zero, i.e.

|  |  |  |
| --- | --- | --- |
|  | âˆ«[0,1]ğ‘‘uâ€‹âˆ«â„|PAâ€‹(k)|Î¼â€‹(dâ€‹xPAâ€‹(k))â€‹|f^â€‹(u,xPAâ€‹(k))âˆ’gâ€‹(u,Î¸â€‹(xPAâ€‹(k)))|=0.\int\_{[0,1]}du\int\_{\mathbb{R}^{|\text{PA}(k)|}}\mu(dx\_{\text{PA}(k)})|\hat{f}(u,x\_{\text{PA}(k)})-g(u,\theta(x\_{\text{PA}(k)}))|=0. |  |

Term 3. Finally, we approximate gâ€‹(u;Î¸â€‹(xPAâ€‹(k)))g(u;\theta(x\_{\text{PA}(k)})) with gâ€‹(u;Î¸^â€‹(xPAâ€‹(k)))g(u;\hat{\theta}(x\_{\text{PA}(k)})), where Î¸^â€‹(â‹…)\hat{\theta}(\cdot) is a suitable MLP.

Since Î¸:Râ†’Î˜\theta:R\to\Theta is a continuous function on a compact, we have that Î¸âˆˆL1â€‹(Î¼)\theta\in L^{1}(\mu), therefore for every Îµâ€²>0\varepsilon^{\prime}>0 there is an MLP777For the theorem to hold we only need the activation function to be non-polynomial and locally essentially bounded (such as ReLU). Î¸^\hat{\theta} such that â€–Î¸âˆ’Î¸^â€–L1â€‹(Î¼)â‰¤Îµâ€²\|\theta-\hat{\theta}\|\_{L^{1}(\mu)}\leq\varepsilon^{\prime} (Leshno etÂ al., [1993](https://arxiv.org/html/2510.15458v1#bib.bib15), Proposition 1).

Therefore:

|  |  |  |
| --- | --- | --- |
|  | âˆ«[0,1]ğ‘‘uâ€‹âˆ«â„|PAâ€‹(k)|Î¼â€‹(dâ€‹xPAâ€‹(k))â€‹|gâ€‹(u;Î¸â€‹(xPAâ€‹(k)))âˆ’gâ€‹(u;Î¸^â€‹(xPAâ€‹(k)))|\displaystyle\int\_{[0,1]}du\int\_{\mathbb{R}^{|\text{PA}(k)|}}\mu(dx\_{\text{PA}(k)})|g(u;\theta(x\_{\text{PA}(k)}))-g(u;\hat{\theta}(x\_{\text{PA}(k)}))| |  |
|  |  |  |
| --- | --- | --- |
|  | â‰¤âˆ«[0,1]duâˆ«â„|PAâ€‹(k)|Î¼(dxPAâ€‹(k))L||Î¸(xPAâ€‹(k))âˆ’Î¸^(xPAâ€‹(k))âˆ¥\displaystyle\leq\int\_{[0,1]}du\int\_{\mathbb{R}^{|\text{PA}(k)|}}\mu(dx\_{\text{PA}(k)})L\>||\theta(x\_{\text{PA}(k)})-\hat{\theta}(x\_{\text{PA}(k)})\| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤Lâ€‹Îµâ€²â‰¤Îµ/2\displaystyle\leq L\varepsilon^{\prime}\leq\varepsilon/2 |  | (choose Îµâ€²=Îµ/2â€‹L\varepsilon^{\prime}=\varepsilon/2L) |

where the first inequality follows from the uniform local Lipschitz property on the compact Î¸â€‹(suppâ€‹(Î¼))âˆªÎ¸^â€‹(suppâ€‹(Î¼))\theta(\mathrm{supp}(\mu))\cup\hat{\theta}(\mathrm{supp}(\mu)) proved in [LemmaÂ A.5](https://arxiv.org/html/2510.15458v1#A1.Thmtheorem5 "Lemma A.5. â€£ Appendix A Auxiliary results â€£ Robust Optimization in Causal Models and ğº-Causal Normalizing Flows").

Summing all three approximation errors together, we obtain the bound in [Eq.Â 7](https://arxiv.org/html/2510.15458v1#A2.E7 "In B.2 Proof of Theorem 4.4 â€£ Appendix B Proofs â€£ Robust Optimization in Causal Models and ğº-Causal Normalizing Flows").
âˆ

### B.3 Proof of [TheoremÂ 4.6](https://arxiv.org/html/2510.15458v1#S4.Thmtheorem6 "Theorem 4.6 (ğ‘Š_ğº training via KL minimization). â€£ 4 Proposed method: ğº-causal normalizing flows â€£ Robust Optimization in Causal Models and ğº-Causal Normalizing Flows")

###### Proof.

First we notice that

|  |  |  |  |
| --- | --- | --- | --- |
|  | WGâ€‹(Î¼,Î½)\displaystyle W\_{G}(\mu,\nu) | =minÏ€âˆˆÎ Gbcâ€‹(Î¼,Î½)â€‹âˆ«â„dÃ—â„dâ€–xâˆ’xâ€²â€–â€‹Ï€â€‹(dâ€‹x,dâ€‹xâ€²)\displaystyle=\min\_{\pi\in\Pi\_{G}^{\text{bc}}(\mu,\nu)}\int\_{\mathbb{R}^{d}\times\mathbb{R}^{d}}\|x-x^{\prime}\|\,\pi(dx,dx^{\prime}) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤minÏ€âˆˆÎ Gbcâ€‹(Î¼,Î½)â€‹âˆ«â„dÃ—â„ddiamâ€‹(K)â‹…ğŸ{xâ‰ xâ€²},Ï€â€‹(dâ€‹x,dâ€‹xâ€²)\displaystyle\leq\min\_{\pi\in\Pi\_{G}^{\text{bc}}(\mu,\nu)}\int\_{\mathbb{R}^{d}\times\mathbb{R}^{d}}\textrm{diam}(K)\cdot\mathbf{1}\_{\{x\neq x^{\prime}\}},\pi(dx,dx^{\prime}) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =:diam(K)â‹…dGâ€‹-â€‹Tâ€‹V(Î¼,Î½)\displaystyle=:\textrm{diam}(K)\cdot d\_{G\text{-}TV}(\mu,\nu) |  |

where in the last equality we have introduced the GG-causal total variation distance, dGâ€‹-â€‹Tâ€‹Vâ€‹(â‹…,â‹…)d\_{G\text{-}TV}(\cdot,\cdot), as a suitable generalization of the total variation distance for GG-bicausal couplings.

The claim then follows by showing that dGâ€‹-â€‹Tâ€‹Vâ€‹(Î¼,Î½)â‰¤(2dâˆ’1)â€‹dTâ€‹Vâ€‹(Î¼,Î½)d\_{G\text{-}TV}(\mu,\nu)\leq(2^{d}-1)d\_{TV}(\mu,\nu) for all sorted DAGs GG by induction on the number of vertices, which is a straightfoward but tedious generalization of Eckstein & Pammer ([2024](https://arxiv.org/html/2510.15458v1#bib.bib11), Lemma 3.5) to our GG-causal setting.

The claim holds trivially if GG has only one vertex (all couplings are GG-bicausal). Suppose now the claim is true for all sorted DAGs on nn vertices. Then for a sorted DAG GG on n+1n+1 vertices, denote by GnG\_{n} its subgraph on vertices {1,â€¦,n}\{1,\ldots,n\}. We start with some definitions. Define:

|  |  |  |
| --- | --- | --- |
|  | Î·â€‹(dâ€‹xn+1|xPAâ€‹(n+1)):=Î¼â€‹(dâ€‹xn+1|xPAâ€‹(n+1))âˆ§Î½â€‹(dâ€‹xn+1|xPAâ€‹(n+1)),\eta(dx\_{n+1}|x\_{\text{PA}(n+1)}):=\mu(dx\_{n+1}|x\_{\text{PA}(n+1)})\wedge\nu(dx\_{n+1}|x\_{\text{PA}(n+1)}), |  |

|  |  |  |
| --- | --- | --- |
|  | Ï€âˆˆÎ Gbcâ€‹(Î¼,Î½)â€‹asâ€‹Ï€:=Ï€nâŠ—Ï€â€‹(dâ€‹xn+1,dâ€‹xn+1â€²|xPAâ€‹(n+1),xPAâ€‹(n+1)â€²),\pi\in\Pi\_{G}^{\text{bc}}(\mu,\nu)\;\text{as}\;\pi:=\pi\_{n}\otimes\pi(dx\_{n+1},dx^{\prime}\_{n+1}\>|\>x\_{\text{PA}(n+1)},x^{\prime}\_{\text{PA}(n+1)}), |  |

where Ï€nâˆˆÎ Gnbcâ€‹(Î¼â€‹(dâ€‹x1:n),Î½â€‹(dâ€‹x1:nâ€²))\pi\_{n}\in\Pi\_{G\_{n}}^{\text{bc}}(\mu(dx\_{1:n}),\nu(dx^{\prime}\_{1:n})), and:

|  |  |  |
| --- | --- | --- |
|  | Ï€â€‹(dâ€‹xn+1,dâ€‹xn+1â€²|xPAâ€‹(n+1),xPAâ€‹(n+1)â€²):={Ïƒâ€‹(dâ€‹xn+1,dâ€‹xn+1â€²|xPAâ€‹(n+1),xPAâ€‹(n+1)â€²)ifÂ xPAâ€‹(n+1)=xPAâ€‹(n+1)â€²Î¼â€‹(dâ€‹xn+1|xPAâ€‹(n+1))âŠ—Î½â€‹(dâ€²â€‹xn+1|xPAâ€‹(n+1)â€²)otherwise\pi(dx\_{n+1},dx^{\prime}\_{n+1}\>|\>x\_{\text{PA}(n+1)},x^{\prime}\_{\text{PA}(n+1)}):=\begin{cases}\sigma(dx\_{n+1},dx^{\prime}\_{n+1}|x\_{\text{PA}(n+1)},x^{\prime}\_{\text{PA}(n+1)})&\text{if $x\_{\text{PA}(n+1)}=x^{\prime}\_{\text{PA}(n+1)}$}\\ \mu(dx\_{n+1}\>|\>x\_{\text{PA}(n+1)})\otimes\nu(d^{\prime}x\_{n+1}\>|\>x^{\prime}\_{\text{PA}(n+1)})&\text{otherwise}\end{cases} |  |

where Ïƒ\sigma is the optimal coupling for the (conditional) total variation distance, i.e.:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ïƒâ€‹(dâ€‹xn+1,dâ€‹xn+1â€²|xPAâ€‹(n+1),xPAâ€‹(n+1)â€²):=\displaystyle\sigma(dx\_{n+1},dx^{\prime}\_{n+1}|x\_{\text{PA}(n+1)},x^{\prime}\_{\text{PA}(n+1)}):= | (id,id)#Î·(dxn+1|xPAâ€‹(n+1))+(Î¼(dxn+1|xPAâ€‹(n+1))\displaystyle(\text{id},\text{id})\_{\#}\eta(dx\_{n+1}|x\_{\text{PA}(n+1)})+(\mu(dx\_{n+1}|x\_{\text{PA}(n+1)}) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | âˆ’Î·(dxn+1|xPAâ€‹(n+1)))âŠ—(Î½(dxn+1|xPAâ€‹(n+1))âˆ’Î·(dxn+1|xPAâ€‹(n+1)))\displaystyle-\eta(dx\_{n+1}|x\_{\text{PA}(n+1)}))\otimes(\nu(dx\_{n+1}\>|\>x\_{\text{PA}(n+1)})-\eta(dx\_{n+1}|x\_{\text{PA}(n+1)})) |  |

Then the following bounds can be established (see Eckstein & Pammer ([2024](https://arxiv.org/html/2510.15458v1#bib.bib11), Lemma 3.5) for step-by-step details):

|  |  |  |  |
| --- | --- | --- | --- |
|  | dGâ€‹-â€‹Tâ€‹Vâ€‹(Î¼,Î½)â‰¤\displaystyle d\_{G\text{-}TV}(\mu,\nu)\leq | âˆ«ğŸ{xâ‰ xâ€²}â€‹Ï€â€‹(dâ€‹x,dâ€‹xâ€²)\displaystyle\int\mathbf{1}\_{\{x\neq x^{\prime}\}}\pi(dx,dx^{\prime}) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | =\displaystyle= | âˆ«ğŸ{x1:nâ‰ x1:nâ€²}â€‹Ï€nâ€‹(dâ€‹x1:n,dâ€‹x1:nâ€²)\displaystyle\int\mathbf{1}\_{\{x\_{1:n}\neq x^{\prime}\_{1:n}\}}\pi\_{n}(dx\_{1:n},dx^{\prime}\_{1:n}) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +âˆ«dTâ€‹Vâ€‹(Î¼â€‹(dâ€‹xn+1|xPAâ€‹(n+1)),Î½â€‹(dâ€‹xn+1|xPAâ€‹(n+1)))â€‹ğŸ{x1:n=x1:nâ€²}â€‹Ï€nâ€‹(dâ€‹x1:n,dâ€‹x1:nâ€²)\displaystyle+\int d\_{TV}(\mu(dx\_{n+1}|x\_{\text{PA}(n+1)}),\nu(dx\_{n+1}|x\_{\text{PA}(n+1)}))\mathbf{1}\_{\{x\_{1:n}=x^{\prime}\_{1:n}\}}\pi\_{n}(dx\_{1:n},dx^{\prime}\_{1:n}) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | =\displaystyle= | âˆ«ğŸ{x1:nâ‰ x1:nâ€²}Ï€n(dx1:n,dx1:nâ€²)+âˆ¥Î·âŠ—(Î¼(dxn+1|xPAâ€‹(n+1))âˆ’Î½(dxn+1|xPAâ€‹(n+1)))âˆ¥Tâ€‹V\displaystyle\int\mathbf{1}\_{\{x\_{1:n}\neq x^{\prime}\_{1:n}\}}\pi\_{n}(dx\_{1:n},dx^{\prime}\_{1:n})+\|\eta\otimes(\mu(dx\_{n+1}|x\_{\text{PA}(n+1)})-\nu(dx\_{n+1}|x\_{\text{PA}(n+1)}))\|\_{TV} |  |

For all Aâˆˆâ„n+1A\in\mathbb{R}^{n+1}, one has:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î·âŠ—(Î¼â€‹(dâ€‹xn+1|xPAâ€‹(n+1))âˆ’Î½â€‹(dâ€‹xn+1|xPAâ€‹(n+1)))â€‹(A)â‰¤\displaystyle\eta\otimes(\mu(dx\_{n+1}|x\_{\text{PA}(n+1)})-\nu(dx\_{n+1}|x\_{\text{PA}(n+1)}))(A)\leq | âˆ¥Î¼(dxn+1|xPAâ€‹(n+1))âˆ’Î½(dxn+1|xPAâ€‹(n+1))âˆ¥Tâ€‹V\displaystyle\|\mu(dx\_{n+1}|x\_{\text{PA}(n+1)})-\nu(dx\_{n+1}|x\_{\text{PA}(n+1)})\|\_{TV} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +âˆ«ğŸ{x1:nâ‰ x1:nâ€²}â€‹Ï€nâ€‹(dâ€‹x1:n,dâ€‹x1:nâ€²)\displaystyle+\int\mathbf{1}\_{\{x\_{1:n}\neq x^{\prime}\_{1:n}\}}\pi\_{n}(dx\_{1:n},dx^{\prime}\_{1:n}) |  |

Putting the two bounds together and minimizing over all GnG\_{n}-bicausal couplings Ï€n\pi\_{n}:

|  |  |  |  |
| --- | --- | --- | --- |
|  | dGâ€‹-â€‹Tâ€‹Vâ€‹(Î¼,Î½)â‰¤\displaystyle d\_{G\text{-}TV}(\mu,\nu)\leq | 2â€‹dGn-TVâ€‹(Î¼1:n,Î½1:n)+dTâ€‹Vâ€‹(Î¼,Î½)\displaystyle 2d\_{\text{$G\_{n}$-TV}}(\mu\_{1:n},\nu\_{1:n})+d\_{TV}(\mu,\nu) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤\displaystyle\leq | (2n+1âˆ’2+1)â€‹dTâ€‹Vâ€‹(Î¼,Î½)\displaystyle(2^{n+1}-2+1)d\_{TV}(\mu,\nu) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | =\displaystyle= | (2n+1âˆ’1)â€‹dTâ€‹Vâ€‹(Î¼,Î½)\displaystyle(2^{n+1}-1)d\_{TV}(\mu,\nu) |  |

where we have used:

|  |  |  |
| --- | --- | --- |
|  | dGn-TVâ€‹(Î¼1:n,Î½1:n)â‰¤(2nâˆ’1)â€‹dTâ€‹Vâ€‹(Î¼1:n,Î½1:n)â‰¤(2nâˆ’1)â€‹dTâ€‹Vâ€‹(Î¼,Î½),d\_{\text{$G\_{n}$-TV}}(\mu\_{1:n},\nu\_{1:n})\leq(2^{n}-1)d\_{TV}(\mu\_{1:n},\nu\_{1:n})\leq(2^{n}-1)d\_{TV}(\mu,\nu), |  |

which follows from the induction hypothesis and the data pre-processing inequality for the total variation distance (Eckstein & Nutz, [2022](https://arxiv.org/html/2510.15458v1#bib.bib10), Lemma 4.1).
âˆ