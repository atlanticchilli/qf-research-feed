---
authors:
- Junyan Ye
- Hoi Ying Wong
- Kyunghyun Park
doc_id: arxiv:2510.10260v1
family_id: arxiv:2510.10260
is_current: true
taxonomy:
  alpha_families: []
  asset_classes: []
  horizons: []
  themes: []
title: 'ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted
  to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from
  the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges
  the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).'
url_abs: http://arxiv.org/abs/2510.10260v1
url_html: https://arxiv.org/html/2510.10260v1
venue: arXiv q-fin
version: 1
year: 2025
---


Junyan Ye
Department of Statistics and Data Science, The Chinese University of Hong Kong, Shatin, N.T., Hong Kong
(, ).
â€ƒâ€ƒ
Hoi Ying Wong33footnotemark: 3
â€ƒâ€ƒ
Kyunghyun Park
Division of Mathematical Sciences,
Nanyang Technological University, Singapore
().

###### Abstract

We propose and analyze a continuous-time robust reinforcement learning framework for optimal stopping problems under ambiguity. In this framework, an agent chooses a stopping rule motivated by two objectives: robust decision-making under ambiguity and learning about the unknown environment.
Here, ambiguity refers to considering multiple probability measuresÂ dominated by a reference measure, reflecting the agentâ€™s awareness that the reference measure representing her learned belief about the environment would be erroneous.
Using the gg-expectation framework, we reformulate an optimal stopping problem under ambiguity as an entropy-regularized optimal control problem under ambiguity, with Bernoulli distributed controls to incorporate exploration into the stopping rules. We then derive the optimal Bernoulli distributed control characterized by backward stochastic differential equations. Moreover, we establish a policy iteration theorem and implement it as a reinforcement learning algorithm. Numerical experiments demonstrate the convergence and robustness of the proposed algorithm across different levels of ambiguity and exploration.

###### keywords:

optimal stopping, ambiguity, robust optimization, gg-expectation, reinforcement learning, policy iteration.

{MSCcodes}

60G40, 60H10, 68T07, 49L20

## 1 Introduction

Optimal stopping is a class of decision problems in which one seeks to choose a time to take a certain action so as to maximize an expected reward. It is applied in various fields, for instance to analyze the optimality of the sequential probability ratio test in statistics (e.g.,â€„[[65](https://arxiv.org/html/2510.10260v1#bib.bib65)]), to study consumption habits in economics (e.g.,â€„[[18](https://arxiv.org/html/2510.10260v1#bib.bib18)]), and notably to derive American option pricing (e.g.,â€„[[55](https://arxiv.org/html/2510.10260v1#bib.bib55)]). A common challenge arising in all these fields is finding the best model to describe the underlying process or probability measure, which is usually unknown. Although significant efforts have been made to propose and analyze general stochastic models with improved estimation techniques, a margin of error in estimation inherently exists.

In response to such model misspecification and estimation errors, recent works, Dai et al.â€„[[15](https://arxiv.org/html/2510.10260v1#bib.bib15)] and Dongâ€„[[17](https://arxiv.org/html/2510.10260v1#bib.bib17)], have cast optimal stopping problems within the continuous time reinforcement learning (RL) framework of Wang et al.â€„[[66](https://arxiv.org/html/2510.10260v1#bib.bib66)] and Wang and Zhouâ€„[[67](https://arxiv.org/html/2510.10260v1#bib.bib67)]. Arguably, the exploratory (or randomized) optimal stopping framework is viewed as model-free, since agents, even without knowledge of the true model or underlying dynamics of the environment, can learn from observed data and determine a stopping rule that yields the best outcome. In this sense, the framework provides a systematic way to balance exploration and exploitation in optimalÂ stopping.

However, the model-free view of the exploratory RL framework has a pitfall: the learning environment reflected in observed data often differs from the actual deployment environment (e.g., due to distributional or domain shifts). Consequently, a stopping rule derived from the learning process may fail in practice. Indeed, Chen and Epstein [[11](https://arxiv.org/html/2510.10260v1#bib.bib11)] explicitly ask: â€œWould ambiguity not disappear eventually as the agent learns about her environment?â€ In response, Epstein and Schneider [[22](https://arxiv.org/html/2510.10260v1#bib.bib22)] and Marinacci [[42](https://arxiv.org/html/2510.10260v1#bib.bib42)] stress that the link between empirical frequencies (i.e., observed data) and asymptotic beliefs (updated through learning) can be weakened by the degree of ambiguity in the agentâ€™s prior beliefs about the environment. This suggests that ambiguity can persist even with extensive learning, limiting the reliability of a purely model-free framework. Such limitations have been recognized in the RL literature, leading to significant developments in robust RL frameworks such as [[9](https://arxiv.org/html/2510.10260v1#bib.bib9), [45](https://arxiv.org/html/2510.10260v1#bib.bib45), [48](https://arxiv.org/html/2510.10260v1#bib.bib48), [59](https://arxiv.org/html/2510.10260v1#bib.bib59), [69](https://arxiv.org/html/2510.10260v1#bib.bib69)].

The aim of this article is to propose and analyze a continuous-time RL framework for optimal stopping under ambiguity. Our framework starts with revisiting the following optimal stopping problem under gg-expectation (Coquet et al.â€„[[12](https://arxiv.org/html/2510.10260v1#bib.bib12)], Pengâ€„[[53](https://arxiv.org/html/2510.10260v1#bib.bib53)]):
Let Tt{\mathcal{}T}\_{t} be the set of all stopping times with values in [t,T][t,T]. Denote by Etgâ€‹[â‹…]{\mathcal{}E}\_{t}^{g}[\cdot] the (conditional) gg-expectation with driver g:Î©Ã—[0,T]Ã—â„dâ†’â„g:\Omega\times[0,T]\times\mathbb{R}^{d}\to\mathbb{R} (satisfying certain regularity and integrability conditions; see DefinitionÂ [2.1](https://arxiv.org/html/2510.10260v1#S2.Thmtheorem1 "Definition 2.1. â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")), which is a filtration-consistent adverse nonlinear expectation whose representing set of probability measures is dominated by a reference measure â„™\mathbb{P} (see Remark [2.2](https://arxiv.org/html/2510.10260v1#S2.Thmtheorem2 "Remark 2.2. â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")). Then, the optimal stopping problem under ambiguity is given by

|  |  |  |  |
| --- | --- | --- | --- |
| (1.1) |  | Vtx:=essâ€‹supÏ„âˆˆTtâ¡Etgâ€‹[âˆ«tÏ„eâˆ’âˆ«tsÎ²uâ€‹ğ‘‘uâ€‹râ€‹(Xsx)â€‹ğ‘‘s+eâˆ’âˆ«tÏ„Î²uâ€‹ğ‘‘uâ€‹Râ€‹(XÏ„x)],\displaystyle V\_{t}^{x}:=\operatorname\*{ess\,sup}\_{\tau\in{\mathcal{}T}\_{t}}{\mathcal{}E}^{g}\_{t}\bigg[\int\_{t}^{\tau}e^{-\int\_{t}^{s}\beta\_{u}du}r(X\_{s}^{x})ds+e^{-\int\_{t}^{\tau}\beta\_{u}du}R(X\_{\tau}^{x})\bigg], |  |

where (Î²t)tâˆˆ[0,T](\beta\_{t})\_{t\in[0,T]} is the discount rate, r:â„dâ†’â„r:\mathbb{R}^{d}\to\mathbb{R} and R:â„dâ†’â„R:\mathbb{R}^{d}\to\mathbb{R} are reward functions, and (Xtx)tâˆˆ[0,T](X\_{t}^{x})\_{t\in[0,T]} is an ItÃ´ semimartingale given by Xtx:=x+âˆ«0tbsoâ€‹ğ‘‘s+âˆ«0tÏƒsoâ€‹ğ‘‘BsX^{x}\_{t}:=x+\int\_{0}^{t}b\_{s}^{o}ds+\int\_{0}^{t}\sigma\_{s}^{o}dB\_{s} on the reference measure â„™\mathbb{P}, where (Bs)sâˆˆ[0,T](B\_{s})\_{s\in[0,T]} is a dd-dimensional Brownian motion on â„™\mathbb{P}, (bso,Ïƒso)sâˆˆ[0,T](b\_{s}^{o},\sigma\_{s}^{o})\_{s\in[0,T]} are baseline parameters, and xâˆˆâ„dx\in\mathbb{R}^{d} is the initial state.

We then combine the penalization method of [[21](https://arxiv.org/html/2510.10260v1#bib.bib21), [39](https://arxiv.org/html/2510.10260v1#bib.bib39), [54](https://arxiv.org/html/2510.10260v1#bib.bib54)] (used to establish the well-posedness of reflected backward stochastic differential equations (BSDEs) characterizing ([1.1](https://arxiv.org/html/2510.10260v1#S1.E1 "Equation 1.1 â€£ 1 Introduction â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175)."))) with the entropy regularization framework of [[66](https://arxiv.org/html/2510.10260v1#bib.bib66), [67](https://arxiv.org/html/2510.10260v1#bib.bib67)] to propose and analyze the following optimal exploratory control problem under ambiguity:

|  |  |  |  |
| --- | --- | --- | --- |
| (1.2) |  | VÂ¯tx;N,Î»:=essâ€‹supÏ€âˆˆÎ â¡Etg[âˆ«tTeâˆ’âˆ«ts(Î²u+Nâ€‹Ï€u)â€‹ğ‘‘u(r(Xsx)+R(Xsx)NÏ€sâˆ’Î»H(Ï€s))+eâˆ’âˆ«tT(Î²u+Nâ€‹Ï€u)â€‹ğ‘‘uR(XTx)],\displaystyle\begin{aligned} \overline{V}\_{t}^{x;N,\lambda}:=\operatorname\*{ess\,sup}\_{\pi\in\Pi}{\mathcal{}E}^{g}\_{t}&[\int\_{t}^{T}e^{-\int\_{t}^{s}(\beta\_{u}+N\pi\_{u})du}\big(r(X\_{s}^{x})+R(X\_{s}^{x})\,N\pi\_{s}-\lambda{\mathcal{}H}(\pi\_{s})\big)\\ &\quad+e^{-\int\_{t}^{T}(\beta\_{u}+N\pi\_{u})du}R(X\_{T}^{x})],\end{aligned} |  |

where Î \Pi is the set of all progressively measurable processes with values in [0,1][0,1], representing Bernoulli-distributed controls randomizing stopping rules (see RemarkÂ [3.2](https://arxiv.org/html/2510.10260v1#S3.Thmtheorem2 "Remark 3.2. â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")), H:[0,1]â†’â„{\mathcal{}H}:[0,1]\to\mathbb{R} denotes the binary differential entropy (see ([3.1](https://arxiv.org/html/2510.10260v1#S3.E1 "Equation 3.1 â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175)."))), Î»>0\lambda>0 represents the level of exploration to learn the unknown environment, and Nâˆˆâ„•N\in\mathbb{N} represents the penalization level (used for approximation of ([1.1](https://arxiv.org/html/2510.10260v1#S1.E1 "Equation 1.1 â€£ 1 Introduction â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175)."))).

In Theorem [3.4](https://arxiv.org/html/2510.10260v1#S3.Thmtheorem4 "Theorem 3.4. â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175)."), we show that if (bo,Ïƒo)(b^{o},\sigma^{o}) are sufficiently integrable (see Assumptionâ€„[2](https://arxiv.org/html/2510.10260v1#S2 "2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")), rr and RR has certain regularity and growth properties, and Î²\beta is uniformly bounded (see Assumption [2](https://arxiv.org/html/2510.10260v1#S2 "2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")), then VÂ¯x;N,Î»\overline{V}^{x;N,\lambda} in ([1.2](https://arxiv.org/html/2510.10260v1#S1.E2 "Equation 1.2 â€£ 1 Introduction â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) can be characterized by a solution of a BSDE. In particular, the optimal Bernoulli-distributed control of ([1.2](https://arxiv.org/html/2510.10260v1#S1.E2 "Equation 1.2 â€£ 1 Introduction â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) is givenÂ by

|  |  |  |  |
| --- | --- | --- | --- |
| (1.3) |  | Ï€tâˆ—,x;N,Î»:=logitâ¡(NÎ»â€‹(Râ€‹(Xtx)âˆ’VÂ¯tx;N,Î»))=[1+eâˆ’NÎ»â€‹(Râ€‹(Xtx)âˆ’VÂ¯tx;N,Î»)]âˆ’1\displaystyle\pi^{\*,x;N,\lambda}\_{t}:=\operatorname{logit}(\frac{N}{\lambda}(R(X\_{t}^{x})-\overline{V}\_{t}^{x;N,\lambda}))=[1+e^{-\frac{N}{\lambda}(R(X\_{t}^{x})-\overline{V}\_{t}^{x;N,\lambda})}]^{-1} |  |

where logitâ¡(x):=(1+expâ¡(âˆ’x))âˆ’1\operatorname{logit}(x):=(1+\exp(-x))^{-1}, xâˆˆâ„x\in\mathbb{R}, denotes the standard logistic function.

It is noteworthy that a similar logistic form as in ([1.3](https://arxiv.org/html/2510.10260v1#S1.E3 "Equation 1.3 â€£ 1 Introduction â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) can also be observed in the non-robust setting in [[15](https://arxiv.org/html/2510.10260v1#bib.bib15)]; however, our value process VÂ¯x;N,Î»\overline{V}^{x;N,\lambda} is established through nonlinear expectation calculations. Moreover, the BSDE techniques of ElÂ Karoui et al.â€„[[21](https://arxiv.org/html/2510.10260v1#bib.bib21)] are instrumental in the verification theorem for our maxmin problems (see Theorem [3.4](https://arxiv.org/html/2510.10260v1#S3.Thmtheorem4 "Theorem 3.4. â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")). Lastly, our BSDE arguments enable a sensitivity analysis of VÂ¯x;N,Î»\overline{V}^{x;N,\lambda} with respect to the level of exploration; see Theoremâ€„[3.5](https://arxiv.org/html/2510.10260v1#S3.Thmtheorem5 "Theorem 3.5. â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).") and Corollaryâ€„[3.6](https://arxiv.org/html/2510.10260v1#S3.Thmtheorem6 "Corollary 3.6. â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).").

Next, under the same assumptions on bo,Ïƒo,r,R,Î²b^{o},\sigma^{o},r,R,\beta, Theorem [4.1](https://arxiv.org/html/2510.10260v1#S4.Thmtheorem1 "Theorem 4.1. â€£ 4 Policy iteration theorem & RL algorithm â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).") establishes aÂ policy iteration result. Specifically, at each step we evaluate the gg-expectation value function under the control Ï€âˆˆÎ \pi\in\Pi from the previous iteration and then update the control in the logistic form driven by this evaluated gg-expectation value (as in ([1.3](https://arxiv.org/html/2510.10260v1#S1.E3 "Equation 1.3 â€£ 1 Introduction â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175)."))). This iterative process ensures that the resulting sequence of value functions and controls converge to the solution of ([1.2](https://arxiv.org/html/2510.10260v1#S1.E2 "Equation 1.2 â€£ 1 Introduction â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) as the number of iterations goes toÂ infinity.

As an application of Theorem [4.1](https://arxiv.org/html/2510.10260v1#S4.Thmtheorem1 "Theorem 4.1. â€£ 4 Policy iteration theorem & RL algorithm â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175)."), under Markovian conditions on bo,Ïƒo,r,R,Î²b^{o},\sigma^{o},r,R,\beta (so that the assumptions made before hold), we devise an RL algorithm (see Algorithmâ€„[1](https://arxiv.org/html/2510.10260v1#alg1 "Algorithm 1 â€£ 4 Policy iteration theorem & RL algorithm â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) in which policy evaluation at each iteration, characterized by a PDE (see Corollary [4.3](https://arxiv.org/html/2510.10260v1#S4.Thmtheorem3 "Corollary 4.3. â€£ 4 Policy iteration theorem & RL algorithm â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")), can be implemented by the deep splitting method of Beck et al.â€„[[5](https://arxiv.org/html/2510.10260v1#bib.bib5)].

Finally, in order to illustrate all our theoretical results, we provide two numerical examples, American put-type and call-type stopping problems (see SectionÂ [5](https://arxiv.org/html/2510.10260v1#S5 "5 Experiments â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")). We are able to observe policy improvement and convergence under several ambiguity degrees. Stability analysis for our exploratory BSDEs solution is also conducted with respect to ambiguity degree Îµ\varepsilon, temperature parameter Î»\lambda and penalty factor NN using put-type stopping problem, while robustness is shown by call-type stopping decision-making under different level of dividend rate misspecification.

### 1.1 Related literature

Sutton and Bartoâ€„[[63](https://arxiv.org/html/2510.10260v1#bib.bib63)] opened up the field of RL, which has since gained significant attention, with successful applications [[29](https://arxiv.org/html/2510.10260v1#bib.bib29), [44](https://arxiv.org/html/2510.10260v1#bib.bib44), [40](https://arxiv.org/html/2510.10260v1#bib.bib40), [60](https://arxiv.org/html/2510.10260v1#bib.bib60), [61](https://arxiv.org/html/2510.10260v1#bib.bib61)]. In continuous-time settings, [[66](https://arxiv.org/html/2510.10260v1#bib.bib66), [67](https://arxiv.org/html/2510.10260v1#bib.bib67)] introduced an RL framework based on relaxed controls, motivating subsequent development of RL schemesÂ [[32](https://arxiv.org/html/2510.10260v1#bib.bib32), [35](https://arxiv.org/html/2510.10260v1#bib.bib35), [36](https://arxiv.org/html/2510.10260v1#bib.bib36), [37](https://arxiv.org/html/2510.10260v1#bib.bib37)], applications and extensionsÂ [[13](https://arxiv.org/html/2510.10260v1#bib.bib13), [14](https://arxiv.org/html/2510.10260v1#bib.bib14), [31](https://arxiv.org/html/2510.10260v1#bib.bib31), [64](https://arxiv.org/html/2510.10260v1#bib.bib64), [68](https://arxiv.org/html/2510.10260v1#bib.bib68)].

Our formulation of exploratory stopping problems under ambiguity aligns with, and can be viewed as, a robust analog of [[15](https://arxiv.org/html/2510.10260v1#bib.bib15), [17](https://arxiv.org/html/2510.10260v1#bib.bib17)], who combine the penalization method for variational inequalities with the exploratory framework of [[66](https://arxiv.org/html/2510.10260v1#bib.bib66), [67](https://arxiv.org/html/2510.10260v1#bib.bib67)] in the PDE setting. Recently, an exploratory stopping-time framework based on a singular control formulation has also been proposed by [[16](https://arxiv.org/html/2510.10260v1#bib.bib16)].

While some proof techniques in our work bear similarities to those in [[15](https://arxiv.org/html/2510.10260v1#bib.bib15), [17](https://arxiv.org/html/2510.10260v1#bib.bib17)], the consideration of ambiguity introduces substantial differences. In particular, due to the ItÃ´ semimartingale setting of XxX^{x} and the nonlinearity induced by the gg-expectation, PDE-based arguments cannot be applied directly. Instead, we establish a robust (i.e., maxâ€“min) verification theorem using BSDE techniques. Building on this, we derive a policy iteration theorem by analyzing a priori estimates for iterative BSDEs. A related recent work of [[26](https://arxiv.org/html/2510.10260v1#bib.bib26)] proposes and analyzes an exploratory optimal stopping framework under discrete stopping times but without ambiguity. Lastly, we refer to [[6](https://arxiv.org/html/2510.10260v1#bib.bib6), [7](https://arxiv.org/html/2510.10260v1#bib.bib7), [57](https://arxiv.org/html/2510.10260v1#bib.bib57)] for machine learning (ML) approaches to optimal stopping.

Moving away from the continuous-time RL (or ML) results to the literature on continuous-time optimal stopping under ambiguity, we refer to [[3](https://arxiv.org/html/2510.10260v1#bib.bib3), [4](https://arxiv.org/html/2510.10260v1#bib.bib4), [47](https://arxiv.org/html/2510.10260v1#bib.bib47), [51](https://arxiv.org/html/2510.10260v1#bib.bib51), [52](https://arxiv.org/html/2510.10260v1#bib.bib52), [58](https://arxiv.org/html/2510.10260v1#bib.bib58)]. More recently, [[43](https://arxiv.org/html/2510.10260v1#bib.bib43)] proposes a framework for optimal stopping that incorporates both ambiguity and learning. Rather than adopting a worst-case approach, as in the above references, the framework employs the smooth ambiguity-aversion model of Klibanoff et al.Â [[38](https://arxiv.org/html/2510.10260v1#bib.bib38)] in combination with Bayesian learning.

### 1.2 Notations and preliminaries

Fix dâˆˆâ„•d\in\mathbb{N}. We endow â„d\mathbb{R}^{d} and â„dÃ—d\mathbb{R}^{d\times d} with the Euclidean inner product âŸ¨â‹…,â‹…âŸ©\langle\cdot,\cdot\rangle and the Frobenius inner product âŸ¨â‹…,â‹…âŸ©F\langle\cdot,\cdot\rangle\_{\operatorname{F}}, respectively. Moreover, we denote by |â‹…||\cdot| the Euclidean norm and denote by âˆ¥â‹…âˆ¥F\|\cdot\|\_{\operatorname{F}} the FrobeniusÂ norm.

Let (Î©,F,â„™)(\Omega,{\mathcal{}F},\mathbb{P}) be a probability space and let B:=(Bt)tâ‰¥0B:=(B\_{t})\_{t\geq 0} be a dd-dimensional standard Brownian motion starting with B0=0B\_{0}=0. Fix T>0T>0 a finite time horizon, and let ğ”½:=(Ft)tâˆˆ[0,T]\mathbb{F}:=({\mathcal{}F}\_{t})\_{t\in[0,T]} be the usual augmentation of the natural filtration generated by BB, i.e., Ft:=Ïƒâ€‹(Bs;sâ‰¤t)âˆ¨N{\mathcal{}F}\_{t}:=\sigma(B\_{s};s\leq t)\vee{\mathcal{}N}, where N{\mathcal{}N} is the set of all â„™\mathbb{P}-null subsets.

For any probability measure â„š\mathbb{Q} on (Î©,F)(\Omega,{\mathcal{}F}), we write ğ”¼â„šâ€‹[â‹…]\mathbb{E}^{\mathbb{Q}}[\cdot] for the expectation underÂ â„š\mathbb{Q} and ğ”¼tâ„š[â‹…]:=ğ”¼â„š[â‹…|Ft]\mathbb{E}^{\mathbb{Q}}\_{t}[\cdot]:=\mathbb{E}^{\mathbb{Q}}[\cdot|{\mathcal{}F}\_{t}] for the conditional expectation under â„š\mathbb{Q} with respect to Ft{\mathcal{}F}\_{t} at time tâ‰¥0t\geq 0. Moreover, we set ğ”¼â€‹[â‹…]:=ğ”¼â„™â€‹[â‹…]\mathbb{E}[\cdot]:=\mathbb{E}^{\mathbb{P}}[\cdot] and ğ”¼tâ€‹[â‹…]:=ğ”¼tâ„™â€‹[â‹…]\mathbb{E}\_{t}[\cdot]:=\mathbb{E}^{\mathbb{P}}\_{t}[\cdot] for tâ‰¥0t\geq 0. For any pâ‰¥1p\geq 1, kâˆˆâ„•k\in\mathbb{N} and tâˆˆ[0,T]t\in[0,T], consider the following sets:

* â€¢

  Lpâ€‹(Ft;â„k)L^{p}({\mathcal{}F}\_{t};\mathbb{R}^{k}) is the set of all â„k\mathbb{R}^{k}-valued, Ft{\mathcal{}F}\_{t}-measurable random variables Î¾\xi such that â€–Î¾â€–Lpp:=ğ”¼â€‹[|Î¾|p]<âˆ\|\xi\|\_{L^{p}}^{p}:=\mathbb{E}[|\xi|^{p}]<\infty;
* â€¢

  ğ•ƒpâ€‹(â„k)\mathbb{L}^{p}(\mathbb{R}^{k}) is the set of all â„k\mathbb{R}^{k}-valued, ğ”½\mathbb{F}-predictable processes Z=(Zt)tâˆˆ[0,T]Z=(Z\_{t})\_{t\in[0,T]} such that â€–Zâ€–ğ•ƒpp:=ğ”¼â€‹[âˆ«0T|Zt|pâ€‹ğ‘‘t]<âˆ\|Z\|^{p}\_{\mathbb{L}^{p}}:=\mathbb{E}[\int\_{0}^{T}|Z\_{t}|^{p}dt]<\infty;
* â€¢

  ğ•Špâ€‹(â„k)\mathbb{S}^{p}(\mathbb{R}^{k}) is the set of all â„k\mathbb{R}^{k}-valued, ğ”½\mathbb{F}-progressively measurable cÃ dlÃ g (i.e., right-continuous with left-limits) processes Y=(Yt)tâˆˆ[0,T]Y=(Y\_{t})\_{t\in[0,T]} such that â€–Yâ€–ğ•Špp:=ğ”¼â€‹[suptâˆˆ[0,T]|Yt|p]<âˆ\|Y\|\_{\mathbb{S}^{p}}^{p}:=\mathbb{E}[\sup\_{t\in[0,T]}|Y\_{t}|^{p}]<\infty;
* â€¢

  Tt{\mathcal{}T}\_{t} is the set of all ğ”½\mathbb{F}-stopping times Ï„\tau with values in [t,T][t,T].

## 2 Optimal stopping under ambiguity

Consider the optimal stopping time choice of an agent facing ambiguity, where the agent is ambiguity-averse and his/her stopping time is determined by observing an ambiguous underlying state process in a continuous-time environment. We model the agentâ€™s preference and the environment by using the gg-expectation Egâ€‹[â‹…]{\mathcal{}E}^{g}[\cdot] (see [[12](https://arxiv.org/html/2510.10260v1#bib.bib12), [53](https://arxiv.org/html/2510.10260v1#bib.bib53)]) defined as follows.

###### Definition 2.1.

Let the driver term g:Î©Ã—[0,T]Ã—â„dâ†’â„g:\Omega\times[0,T]\times\mathbb{R}^{d}\to\mathbb{R} be a mapping such that the following conditions hold:

* (i)

  for zâˆˆâ„dz\in\mathbb{R}^{d}, (gâ€‹(t,z))tâˆˆ[0,T](g(t,z))\_{t\in[0,T]} is ğ”½\mathbb{F}-progressively measurable with â€–gâ€‹(â‹…,z)â€–ğ•ƒ2<âˆ\|g(\cdot,z)\|\_{\mathbb{L}^{2}}<\infty;
* (ii)

  there exists some constant Îº>0\kappa>0 such that for every (Ï‰,t)âˆˆÎ©Ã—[0,T](\omega,t)\in\Omega\times[0,T] and z,zâ€²âˆˆâ„dz,z^{\prime}\in\mathbb{R}^{d} |gâ€‹(Ï‰,t,z)âˆ’gâ€‹(Ï‰,t,zâ€²)|â‰¤Îºâ€‹|zâˆ’zâ€²|;\big|g(\omega,t,z)-g(\omega,t,z^{\prime})\big|\leq\kappa|z-z^{\prime}|;
* (iii)

  for every (Ï‰,t)âˆˆÎ©Ã—[0,T](\omega,t)\in\Omega\times[0,T], gâ€‹(Ï‰,t,â‹…):â„dâ†’â„g(\omega,t,\cdot):\mathbb{R}^{d}\to\mathbb{R} is concave and gâ€‹(Ï‰,t,0)=0g(\omega,t,0)=0.

Then we define Eg:L2â€‹(FT;â„)âˆ‹Î¾â†’Egâ€‹[Î¾]âˆˆâ„{\mathcal{}E}^{g}:L^{2}({\mathcal{}F}\_{T};\mathbb{R})\ni\xi\to{\mathcal{}E}^{g}[\xi]\in\mathbb{R}Â as Egâ€‹[Î¾]:=Y0,{\mathcal{}E}^{g}[\xi]:=Y\_{0},
where (Y,Z)âˆˆğ•Š2â€‹(â„)Ã—ğ•ƒ2â€‹(â„d)(Y,Z)\in\mathbb{S}^{2}(\mathbb{R})\times\mathbb{L}^{2}(\mathbb{R}^{d}) is the unique solution of the following BSDE (see [[49](https://arxiv.org/html/2510.10260v1#bib.bib49), TheoremÂ 3.1]):

|  |  |  |
| --- | --- | --- |
|  | Yt=Î¾+âˆ«tTgâ€‹(s,Zs)â€‹ğ‘‘sâˆ’âˆ«tTZsâ€‹ğ‘‘Bs,\displaystyle Y\_{t}=\xi+\int\_{t}^{T}g(s,Z\_{s})ds-\int\_{t}^{T}Z\_{s}dB\_{s}, |  |

where (Bt)tâˆˆ[0,T](B\_{t})\_{t\in[0,T]} is the fixed dd-dimensional Brownian motion on (Î©,F,â„™)(\Omega,{\mathcal{}F},\mathbb{P}). Moreover, its conditional gg-expectation with respect to Ft{\mathcal{}F}\_{t} is defined by
Etgâ€‹[Î¾]:=Yt{\mathcal{}E}^{g}\_{t}[\xi]:=Y\_{t} for tâˆˆ[0,T]t\in[0,T],
which can be extended into ğ”½\mathbb{F}-stopping times Ï„âˆˆT0\tau\in{\mathcal{}T}\_{0}, i.e., EÏ„gâ€‹[Î¾]:=YÏ„.{\mathcal{}E}^{g}\_{\tau}[\xi]:=Y\_{\tau}.

###### Remark 2.2.

The gg-expectation defined above coincides with a variational representation in the following sense (see [[21](https://arxiv.org/html/2510.10260v1#bib.bib21), Proposition 3.6], [[23](https://arxiv.org/html/2510.10260v1#bib.bib23), Proposition A.1]): Define g^:Î©Ã—[0,T]Ã—â„dâˆ‹(Ï‰,t,z^)â†’g^â€‹(Ï‰,t,z^):=supzâˆˆâ„d(gâ€‹(Ï‰,t,z)âˆ’âŸ¨z,z^âŸ©)âˆˆâ„,\hat{g}:\Omega\times[0,T]\times\mathbb{R}^{d}\ni(\omega,t,\hat{z})\to\hat{g}(\omega,t,\hat{z}):=\sup\_{z\in\mathbb{R}^{d}}\big(g(\omega,t,z)-\langle z,\hat{z}\rangle\big)\in\mathbb{R},
i.e., the convex conjugate function of gâ€‹(Ï‰,t,â‹…)g(\omega,t,\cdot). Denote by Bg{\mathcal{}B}^{g} the set of all ğ”½\mathbb{F} progressively measurable processes Ï‘=(Ï‘t)tâˆˆ[0,T]\vartheta=(\vartheta\_{t})\_{t\in[0,T]} such that â€–g^â€‹(â‹…,Ï‘â‹…)â€–ğ•ƒ2<âˆ\|\hat{g}(\cdot,\vartheta\_{\cdot})\|\_{\mathbb{L}^{2}}<\infty.

For any Ï„âˆˆTt\tau\in{\mathcal{}T}\_{t} and tâˆˆ[0,T]t\in[0,T], the following representation holds:

|  |  |  |
| --- | --- | --- |
|  | Etgâ€‹[Î¾]=essâ€‹infÏ‘âˆˆBgâ¡ğ”¼tâ„™Ï‘â€‹[Î¾+âˆ«tÏ„g^â€‹(s,Ï‘s)â€‹ğ‘‘s]forâ€‹Î¾âˆˆL2â€‹(FÏ„;â„d),\displaystyle{\mathcal{}E}\_{t}^{g}[\xi]=\operatorname\*{ess\,inf}\_{\vartheta\in{\mathcal{}B}^{g}}\mathbb{E}\_{t}^{\mathbb{P}^{\vartheta}}\bigg[\xi+\int\_{t}^{\tau}\hat{g}(s,\vartheta\_{s})ds\bigg]\quad\mbox{for}\;\;\xi\in L^{2}({\mathcal{}F}\_{\tau};\mathbb{R}^{d}), |  |

where â„™Ï‘\mathbb{P}^{\vartheta} is defined on (Î©,FT)(\Omega,{\mathcal{}F}\_{T}) through dâ€‹â„™Ï‘dâ€‹â„™|FT:=expâ¡(âˆ’12â€‹âˆ«0T|Ï‘s|2â€‹ğ‘‘s+âˆ«0TÏ‘sâ€‹ğ‘‘Bs).\frac{d\mathbb{P}^{\vartheta}}{d\mathbb{P}}|\_{{\mathcal{}F}\_{T}}:=\exp(-\frac{1}{2}\int\_{0}^{T}|\vartheta\_{s}|^{2}ds+\int\_{0}^{T}\vartheta\_{s}dB\_{s}).

For (sufficiently integrable) ğ”½\mathbb{F}-predictable processes (bso)sâˆˆ[0,T](b\_{s}^{o})\_{s\in[0,T]} and (Ïƒso)sâˆˆ[0,T](\sigma\_{s}^{o})\_{s\in[0,T]} taking values in â„d\mathbb{R}^{d} and â„dÃ—d\mathbb{R}^{d\times d} respectively, we consider an ItÃ´ (ğ”½,â„™)(\mathbb{F},\mathbb{P})-semimartingale Xx:=(Xtx)tâˆˆ[0,T]X^{x}:=(X^{x}\_{t})\_{t\in[0,T]} givenÂ by

|  |  |  |  |
| --- | --- | --- | --- |
| (2.1) |  | Xtx:=x+âˆ«0tbsoâ€‹ğ‘‘s+âˆ«0tÏƒsoâ€‹ğ‘‘Bs,tâˆˆ[0,T],\displaystyle X\_{t}^{x}:=x+\int\_{0}^{t}b^{o}\_{s}ds+\int\_{0}^{t}\sigma^{o}\_{s}dB\_{s},\quad t\in[0,T], |  |

where xâˆˆâ„dx\in\mathbb{R}^{d} is fixed and does not depend on bob^{o} and Ïƒo\sigma^{o}.

We note that bob^{o} and Ïƒo\sigma^{o} correspond to the baseline parameters (e.g., the estimators) and XxX^{x} corresponds to the reference underlying state process. We assume the certain integrability condition on the baseline parameters. To that end, for any pâ‰¥1p\geq 1, let ğ•ƒpâ€‹(â„d)\mathbb{L}^{p}(\mathbb{R}^{d}) be defined as in Section [1.2](https://arxiv.org/html/2510.10260v1#S1.SS2 "1.2 Notations and preliminaries â€£ 1 Introduction â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).") and let ğ•ƒFpâ€‹(â„dÃ—d)\mathbb{L}\_{\operatorname{F}}^{p}(\mathbb{R}^{d\times d}) be the set of all â„dÃ—d\mathbb{R}^{d\times d}-valued, ğ”½\mathbb{F}-predictable processes H=(Ht)tâˆˆ[0,T]H=(H\_{t})\_{t\in[0,T]} such that â€–Hâ€–ğ•ƒFpp:=ğ”¼â€‹[(âˆ«0Tâ€–Htâ€–F2â€‹ğ‘‘t)p2]<âˆ\|H\|^{p}\_{\mathbb{L}\_{\operatorname{F}}^{p}}:=\mathbb{E}[(\int\_{0}^{T}\|H\_{t}\|\_{\operatorname{F}}^{2}dt)^{\frac{p}{2}}]<\infty.
{as}
boâˆˆğ•ƒpâ€‹(â„d)b^{o}\in\mathbb{L}^{p}(\mathbb{R}^{d}) and Ïƒoâˆˆğ•ƒFpâ€‹(â„dÃ—d)\sigma^{o}\in\mathbb{L}\_{\operatorname{F}}^{p}(\mathbb{R}^{d\times d}) for some pâ‰¥2p\geq 2.

###### Remark 2.3.

Either one of the following conditions is sufficient for Assumption [2](https://arxiv.org/html/2510.10260v1#S2 "2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).") to hold true [[2](https://arxiv.org/html/2510.10260v1#bib.bib2), Lemmaâ€„2.3]:

* (i)

  bob^{o} and Ïƒo\sigma^{o} are uniformly bounded, i.e., there exists some constant Cb,Ïƒ>0C\_{b,\sigma}>0 such that |bto|+â€–Ïƒtoâ€–Fâ‰¤Cb,Ïƒ|b^{o}\_{t}|+\|\sigma\_{t}^{o}\|\_{\operatorname{F}}\leq C\_{b,\sigma} â„™âŠ—dâ€‹t\mathbb{P}\otimes dt-a.e..
* (ii)

  bob^{o} and Ïƒo\sigma^{o} are of the following form: bto=b~oâ€‹(t,Xtx),b\_{t}^{o}=\widetilde{b}^{o}(t,X\_{t}^{x}), Ïƒto=Ïƒ~oâ€‹(t,Xtx)\sigma\_{t}^{o}=\widetilde{\sigma}^{o}(t,X\_{t}^{x}) â„™âŠ—dâ€‹t\mathbb{P}\otimes dt-a.e.,
  where
  b~o:[0,T]Ã—â„dâ†’â„d\widetilde{b}^{o}:[0,T]\times\mathbb{R}^{d}\rightarrow\mathbb{R}^{d} and Ïƒ~o:[0,T]Ã—â„dâ†’â„dÃ—d\widetilde{\sigma}^{o}:[0,T]\times\mathbb{R}^{d}\rightarrow\mathbb{R}^{d\times d} are Borel functions satisfying that |b~o(t,y)âˆ’b~o(t,y^)|+âˆ¥Ïƒ~o(t,y)âˆ’Ïƒ~o(t,y^)âˆ¥Fâ‰¤Cb~,Ïƒ~|yâˆ’y^|\lvert\widetilde{b}^{o}(t,y)-\widetilde{b}^{o}(t,\hat{y})\lvert+\lVert\widetilde{\sigma}^{o}(t,y)-\widetilde{\sigma}^{o}(t,\hat{y})\rVert\_{\operatorname{F}}\leq C\_{\widetilde{b},\widetilde{\sigma}}\lvert y-\hat{y}\rvert and |b~o(t,y)|+âˆ¥Ïƒ~o(t,y)âˆ¥Fâ‰¤Cb~,Ïƒ~(1+|y|)\lvert\widetilde{b}^{o}(t,y)\lvert+\lVert\widetilde{\sigma}^{o}(t,y)\rVert\_{\operatorname{F}}\leq C\_{\widetilde{b},\widetilde{\sigma}}(1+\lvert y\rvert) for every tâˆˆ[0,T]t\in[0,T] and y,y^âˆˆâ„dy,\hat{y}\in\mathbb{R}^{d},
  with some constant Cb~,Ïƒ~>0C\_{\widetilde{b},\widetilde{\sigma}}>0.

###### Remark 2.4.

* (i)

  Under Assumption [2](https://arxiv.org/html/2510.10260v1#S2 "2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175)."), a straightforward application of the Burkholder Davis Gundy (BDG) inequality shows that â€–Xxâ€–ğ•Šp<âˆ\|X^{x}\|\_{\mathbb{S}^{p}}<\infty.
* (ii)

  In fact, both sufficient conditions given in Remark [2.3](https://arxiv.org/html/2510.10260v1#S2.Thmtheorem3 "Remark 2.3. â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).") ensure that AssumptionÂ [2](https://arxiv.org/html/2510.10260v1#S2 "2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).") holds for all pâ‰¥2p\geq 2 (see [[41](https://arxiv.org/html/2510.10260v1#bib.bib41), Theorems 2.3.1 and 2.4.1])

Having completed the descriptions of the gg-expectation and underlying process, we describe the decision-makerâ€™s optimal stopping problem Vx:=(Vtx)tâˆˆ[0,T]V^{x}:=(V\_{t}^{x})\_{t\in[0,T]} under ambiguity: for every tâˆˆ[0,T]t\in[0,T],

|  |  |  |  |
| --- | --- | --- | --- |
| (2.2) |  | Vtx:=essâ€‹supÏ„âˆˆTtâ¡Etgâ€‹[Itx;Ï„];Itx;Ï„:=âˆ«tÏ„eâˆ’âˆ«tsÎ²uâ€‹ğ‘‘uâ€‹râ€‹(Xsx)â€‹ğ‘‘s+eâˆ’âˆ«tÏ„Î²uâ€‹ğ‘‘uâ€‹Râ€‹(XÏ„x),\displaystyle\begin{aligned} &V\_{t}^{x}:=\operatorname\*{ess\,sup}\_{\tau\in{\mathcal{}T}\_{t}}{\mathcal{}E}^{g}\_{t}[\operatorname{I}\_{t}^{x;\tau}];\qquad\operatorname{I}\_{t}^{x;\tau}:=\int\_{t}^{\tau}e^{-\int\_{t}^{s}\beta\_{u}du}r(X\_{s}^{x})ds+e^{-\int\_{t}^{\tau}\beta\_{u}du}R(X\_{\tau}^{x}),\end{aligned} |  |

where both r:â„dâ†’â„r:\mathbb{R}^{d}\to\mathbb{R} and R:â„dâ†’â„R:\mathbb{R}^{d}\to\mathbb{R}
are some Borel functions (representing the intermediate and stopping reward functions), and (Î²u)uâˆˆ[0,T](\beta\_{u})\_{u\in[0,T]} is an ğ”½\mathbb{F}-progressively measurable process taking positive values (representing the subjective discount rate).

{as}

* (i)

  RR is continuous. Moreover, there exists some constant Cr,R>0C\_{r,R}>0 such that for every yâˆˆâ„dy\in\mathbb{R}^{d}, |râ€‹(y)|+|Râ€‹(y)|â‰¤Cr,Râ€‹(1+|y|)|r(y)|+|R(y)|\leq C\_{r,R}(1+|y|).
* (ii)

  There is some CÎ²>0C\_{\beta}>0 such that 0â‰¤Î²tâ€‹(Ï‰)â‰¤CÎ²0\leq\beta\_{t}(\omega)\leq C\_{\beta} for all (Ï‰,t)âˆˆÎ©Ã—[0,T](\omega,t)\in\Omega\times[0,T].

###### Remark 2.5.

Under Assumptions [2](https://arxiv.org/html/2510.10260v1#S2 "2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).") and [2](https://arxiv.org/html/2510.10260v1#S2 "2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175)."), it holds for every tâˆˆ[0,T]t\in[0,T] and Ï„âˆˆTt\tau\in{\mathcal{}T}\_{t} that the integrand Itx;Ï„\operatorname{I}\_{t}^{x;\tau} given in ([2.2](https://arxiv.org/html/2510.10260v1#S2.E2 "Equation 2.2 â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) is in L2â€‹(FÏ„;â„)L^{2}({\mathcal{}F}\_{\tau};\mathbb{R}). Indeed, by the triangle inequality and the positiveness of (Î²u)uâˆˆ[0,T](\beta\_{u})\_{u\in[0,T]}, ğ”¼â€‹[|Itx;Ï„|]â‰¤Cr,Râ€‹(T+1)â€‹â€–Xxâ€–ğ•Š1;\mathbb{E}[|\operatorname{I}\_{t}^{x;\tau}|]\leq C\_{r,R}(T+1)\|X^{x}\|\_{\mathbb{S}^{1}};
see also Assumption [2](https://arxiv.org/html/2510.10260v1#S2 "2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175)."). Moreover, since â€–Xxâ€–ğ•Šp<âˆ\|X^{x}\|\_{\mathbb{S}^{p}}<\infty with the exponent pâ‰¥2p\geq 2 (see RemarkÂ [2.4](https://arxiv.org/html/2510.10260v1#S2.Thmtheorem4 "Remark 2.4. â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")â€„(i)), an application of the Jensenâ€™s inequality with exponent 22 ensures the claim to hold. As a direct consequence, VxV^{x} in ([2.2](https://arxiv.org/html/2510.10260v1#S2.E2 "Equation 2.2 â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) is well-defined.

Let us that the VxV^{x} given in ([2.2](https://arxiv.org/html/2510.10260v1#S2.E2 "Equation 2.2 â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) corresponds to a reflected BSDE with a lower obstacle. To that end, set for every (Ï‰,t,y,z)âˆˆÎ©Ã—[0,T]Ã—â„Ã—â„d(\omega,t,y,z)\in\Omega\times[0,T]\times\mathbb{R}\times\mathbb{R}^{d} by

|  |  |  |  |
| --- | --- | --- | --- |
| (2.3) |  | Ftxâ€‹(Ï‰,y,z):=râ€‹(Xtxâ€‹(Ï‰))âˆ’Î²tâ€‹(Ï‰)â€‹y+gâ€‹(Ï‰,t,z),\displaystyle F\_{t}^{x}(\omega,y,z):=r(X\_{t}^{x}(\omega))-\beta\_{t}(\omega)y+g(\omega,t,z), |  |

where g:Î©Ã—[0,T]Ã—â„dâ†’â„g:\Omega\times[0,T]\times\mathbb{R}^{d}\to\mathbb{R} is defined as in Definition [2.1](https://arxiv.org/html/2510.10260v1#S2.Thmtheorem1 "Definition 2.1. â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175)."), (Xtx)tâˆˆ[0,T](X\_{t}^{x})\_{t\in[0,T]} is given inâ€„([2.1](https://arxiv.org/html/2510.10260v1#S2.E1 "Equation 2.1 â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")), and (Î²t)tâˆˆ[0,T](\beta\_{t})\_{t\in[0,T]} is the discount rate appearing inÂ ([2.2](https://arxiv.org/html/2510.10260v1#S2.E2 "Equation 2.2 â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")).

Denote by (Ytx,Ztx,Ktx)tâˆˆ[0,T](Y\_{t}^{x},Z\_{t}^{x},K^{x}\_{t})\_{t\in[0,T]} a triplet of processes satisfying that

|  |  |  |  |
| --- | --- | --- | --- |
| (2.4) |  | Ytx=Râ€‹(XTx)+âˆ«tTFsxâ€‹(Ysx,Zsx)â€‹ğ‘‘sâˆ’âˆ«tTZsxâ€‹ğ‘‘Bs+KTxâˆ’Ktx,forâ€‹tâˆˆ[0,T],\displaystyle Y\_{t}^{x}=R(X\_{T}^{x})+\int\_{t}^{T}F\_{s}^{x}(Y\_{s}^{x},Z\_{s}^{x})ds-\int\_{t}^{T}Z\_{s}^{x}dB\_{s}+K\_{T}^{x}-K\_{t}^{x},\;\;\mbox{for}\;t\in[0,T], |  |

We then introduce the notion of the reflected BSDE (see [[39](https://arxiv.org/html/2510.10260v1#bib.bib39), Definitionâ€„2.1]). For this, recall the sets ğ•Š2â€‹(â„)\mathbb{S}^{2}(\mathbb{R}) and ğ•ƒ2â€‹(â„d)\mathbb{L}^{2}(\mathbb{R}^{d}) given in Section [1.2](https://arxiv.org/html/2510.10260v1#S1.SS2 "1.2 Notations and preliminaries â€£ 1 Introduction â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).").

###### Definition 2.6.

A triplet (Yx,Zx,Kx)(Y^{x},Z^{x},K^{x}) is said to be a solution
to the reflected BSDE ([2.4](https://arxiv.org/html/2510.10260v1#S2.E4 "Equation 2.4 â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) with the lower obstacle (Râ€‹(Xtx))tâˆˆ[0,T](R(X\_{t}^{x}))\_{t\in[0,T]} if the following conditions hold:

* (i)

  Yxâˆˆğ•Š2â€‹(â„)Y^{x}\in\mathbb{S}^{2}(\mathbb{R}), Zxâˆˆğ•ƒ2â€‹(â„d)Z^{x}\in\mathbb{L}^{2}(\mathbb{R}^{d}) and Kxâˆˆğ•Š2â€‹(â„)K^{x}\in\mathbb{S}^{2}(\mathbb{R}) which is nondecreasing and starts with K0x=0K\_{0}^{x}=0. Moreover, (Yx,Zx,Kx)(Y^{x},Z^{x},K^{x}) satisfies ([2.4](https://arxiv.org/html/2510.10260v1#S2.E4 "Equation 2.4 â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175)."));
* (ii)

  Ytxâ‰¥Râ€‹(Xtx)Y\_{t}^{x}\geq R(X\_{t}^{x}) â„™\mathbb{P}-a.s., for all tâ‰¥0t\geq 0;
* (iii)

  âˆ«0T(Ytâˆ’xâˆ’Râ€‹(Xtâˆ’x))â€‹ğ‘‘Ktx=0\int\_{0}^{T}(Y\_{t-}^{x}-R(X\_{t-}^{x}))dK\_{t}^{x}=0 â„™\mathbb{P}-a.s..

###### Remark 2.7.

Under Assumptions [2](https://arxiv.org/html/2510.10260v1#S2 "2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).") and [2](https://arxiv.org/html/2510.10260v1#S2 "2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175)."), there exists a unique solution (Ytx(Y\_{t}^{x}, Ztx,Ktx)tâˆˆ[0,T]Z\_{t}^{x},K^{x}\_{t})\_{t\in[0,T]} of the reflected BSDE ([2.4](https://arxiv.org/html/2510.10260v1#S2.E4 "Equation 2.4 â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) with the lower obstacle (Râ€‹(Xtx))tâˆˆ[0,T](R(X\_{t}^{x}))\_{t\in[0,T]} (see Definition [2.6](https://arxiv.org/html/2510.10260v1#S2.Thmtheorem6 "Definition 2.6. â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")). Indeed, one can easily show that the parameters of the reflected BSDE satisfy the conditions (i)â€“(iii) given in [[39](https://arxiv.org/html/2510.10260v1#bib.bib39), Section 2], which enables to apply [[39](https://arxiv.org/html/2510.10260v1#bib.bib39), Theorem 3.3] to ensures its existence and uniqueness to hold.

The following proposition establishes that the solution to the reflected BSDE ([2.4](https://arxiv.org/html/2510.10260v1#S2.E4 "Equation 2.4 â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) coincides with the Snell envelope of the optimal stopping problem under ambiguity given in ([2.2](https://arxiv.org/html/2510.10260v1#S2.E2 "Equation 2.2 â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")). This result can be seen as a robust analogue of [[20](https://arxiv.org/html/2510.10260v1#bib.bib20), Propositionâ€„2.3] and [[39](https://arxiv.org/html/2510.10260v1#bib.bib39), Propositionâ€„3.1]. Several properties of (conditional) gg-expectation developed in [[12](https://arxiv.org/html/2510.10260v1#bib.bib12)] are useful in the proof presented in Section [6.1](https://arxiv.org/html/2510.10260v1#S6.SS1 "6.1 Proof of results in Section 2 â€£ 6 Proofs â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).").

###### Proposition 2.8.

Suppose that Assumptions [2](https://arxiv.org/html/2510.10260v1#S2 "2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).") and [2](https://arxiv.org/html/2510.10260v1#S2 "2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).") hold. Let (Vtx)tâˆˆ[0,T](V\_{t}^{x})\_{t\in[0,T]} be given in ([2.2](https://arxiv.org/html/2510.10260v1#S2.E2 "Equation 2.2 â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) (see Remark [2.5](https://arxiv.org/html/2510.10260v1#S2.Thmtheorem5 "Remark 2.5. â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) and let (Ytx)tâˆˆ[0,T](Y\_{t}^{x})\_{t\in[0,T]} be the first component of the unique solution to the reflected BSDE ([2.4](https://arxiv.org/html/2510.10260v1#S2.E4 "Equation 2.4 â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) with the lower obstacle (Râ€‹(Xtx))tâˆˆ[0,T](R(X\_{t}^{x}))\_{t\in[0,T]} (see Remark [2.7](https://arxiv.org/html/2510.10260v1#S2.Thmtheorem7 "Remark 2.7. â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")). Then, Vtx=YtxV\_{t}^{x}=Y\_{t}^{x}, â„™\mathbb{P}-a.s. for all tâˆˆ[0,T]t\in[0,T]. In particular, the stopping time Ï„tâˆ—,xâˆˆTt\tau\_{t}^{\*,x}\in{\mathcal{}T}\_{t}, defined by

|  |  |  |  |
| --- | --- | --- | --- |
| (2.5) |  | Ï„tâˆ—,x:=inf{sâ‰¥t|Ytxâ‰¤Râ€‹(Xtx)}âˆ§T,\displaystyle\tau\_{t}^{\*,x}:=\inf\{s\geq t\,|\,Y\_{t}^{x}\leq R(X\_{t}^{x})\}\wedge T, |  |

is optimal to the robust stopping problem VxV^{x}.

The penalization method is a standard approach for establishing the existence of solutions to reflected BSDEs (see, e.g., [[21](https://arxiv.org/html/2510.10260v1#bib.bib21), [39](https://arxiv.org/html/2510.10260v1#bib.bib39), [54](https://arxiv.org/html/2510.10260v1#bib.bib54)]). We introduce a sequence of penalized BSDEs and remark on the convergence of their solutions to that of the reflected BSDE given ([2.4](https://arxiv.org/html/2510.10260v1#S2.E4 "Equation 2.4 â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")).

To that end, set for every Nâˆˆâ„•N\in\mathbb{N} and (Ï‰,t,y,z)âˆˆÎ©Ã—[0,T]Ã—â„Ã—â„d(\omega,t,y,z)\in\Omega\times[0,T]\times\mathbb{R}\times\mathbb{R}^{d} by

|  |  |  |  |
| --- | --- | --- | --- |
| (2.6) |  | Ftx;Nâ€‹(Ï‰,y,z):=Ftxâ€‹(Ï‰,y,z)+Nâ€‹(Râ€‹(Xtxâ€‹(Ï‰))âˆ’y)+,\displaystyle F\_{t}^{x;N}(\omega,y,z):=F\_{t}^{x}(\omega,y,z)+N(R\big(X\_{t}^{x}(\omega)\big)-y)^{+}, |  |

where FxF^{x} is given in ([2.3](https://arxiv.org/html/2510.10260v1#S2.E3 "Equation 2.3 â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) and (a)+:=maxâ¡{a,0}(a)^{+}:=\max\{a,0\} for aâˆˆâ„a\in\mathbb{R}. Then we denote for every Nâˆˆâ„•N\in\mathbb{N} by (Ytx;N,Ztx;N)tâˆˆ[0,T](Y\_{t}^{x;N},Z\_{t}^{x;N})\_{t\in[0,T]} a couple of processes satisfying that

|  |  |  |  |
| --- | --- | --- | --- |
| (2.7) |  | Ytx;N=Râ€‹(XTx)+âˆ«tTFsx;Nâ€‹(Ysx;N,Zsx;N)â€‹ğ‘‘sâˆ’âˆ«tTZsx;Nâ€‹ğ‘‘Bs,forÂ tâˆˆ[0,T].\displaystyle Y\_{t}^{x;N}=R(X\_{T}^{x})+\int\_{t}^{T}F\_{s}^{x;N}(Y\_{s}^{x;N},Z\_{s}^{x;N})ds-\int\_{t}^{T}Z\_{s}^{x;N}dB\_{s},\;\;\mbox{for $t\in[0,T]$}. |  |

###### Remark 2.9.

Under Assumptions [2](https://arxiv.org/html/2510.10260v1#S2 "2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).") and [2](https://arxiv.org/html/2510.10260v1#S2 "2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175)."), the parameters of the BSDE ([2.7](https://arxiv.org/html/2510.10260v1#S2.E7 "Equation 2.7 â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) satisfy all the conditions given in [[49](https://arxiv.org/html/2510.10260v1#bib.bib49), Section 3]. Hence, we recognize:

* (i)

  For every Nâˆˆâ„•N\in\mathbb{N} there exists a unique solution (Ytx;N,Ztx;N)tâˆˆ[0,T]âˆˆğ•Š2â€‹(â„)Ã—ğ•ƒ2â€‹(â„d)(Y^{x;N}\_{t},Z\_{t}^{x;N})\_{t\in[0,T]}\in\mathbb{S}^{2}(\mathbb{R})\times\mathbb{L}^{2}(\mathbb{R}^{d}) of the BSDE ([2.7](https://arxiv.org/html/2510.10260v1#S2.E7 "Equation 2.7 â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) (see [[49](https://arxiv.org/html/2510.10260v1#bib.bib49), Theorem 3.1]).
* (ii)

  Moreover, if we set Ktx;N:=Nâ€‹âˆ«0t(Râ€‹(Xsx)âˆ’Ysx;N)+â€‹ğ‘‘sK\_{t}^{x;N}:=N\int\_{0}^{t}(R(X\_{s}^{x})-Y\_{s}^{x;N})^{+}ds for tâˆˆ[0,T]t\in[0,T], then it follows from
  [[20](https://arxiv.org/html/2510.10260v1#bib.bib20), Sectionâ€‰6., Eq.â€‰(16)] that there exists some constant C>0C>0 such that for every Nâˆˆâ„•N\in\mathbb{N}, â€–Yx;Nâ€–ğ•Š22+â€–Zx;Nâ€–ğ•ƒ22+â€–KTx;Nâ€–L22â‰¤C.\|Y^{x;N}\|\_{\mathbb{S}^{2}}^{2}+\|Z^{x;N}\|\_{\mathbb{L}^{2}}^{2}+\|K\_{T}^{x;N}\|\_{L^{2}}^{2}\leq C.
* (iii)

  Lastly, we recall that (Ytx,Ztx,Ktx)tâˆˆ[0,T](Y\_{t}^{x},Z\_{t}^{x},K\_{t}^{x})\_{t\in[0,T]} is the unique solution to the reflected gg-BSDE ([2.4](https://arxiv.org/html/2510.10260v1#S2.E4 "Equation 2.4 â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) (see Remark [2.7](https://arxiv.org/html/2510.10260v1#S2.Thmtheorem7 "Remark 2.7. â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")). Then, it follows from [[39](https://arxiv.org/html/2510.10260v1#bib.bib39), Lemmaâ€„3.2 & Theorem 3.3] that111We say Zâˆˆğ•ƒ2â€‹(â„d)Z\in\mathbb{L}^{2}(\mathbb{R}^{d}) is the weak limit of (Zn)nâˆˆâ„•âŠ†ğ•ƒ2â€‹(â„d)(Z^{n})\_{n\in\mathbb{N}}\subseteq\mathbb{L}^{2}(\mathbb{R}^{d}) if for every Ï•âˆˆğ•ƒ2â€‹(â„d)\phi\in\mathbb{L}^{2}(\mathbb{R}^{d}), it holds that âŸ¨Zn,Ï•âŸ©â„™âŠ—dâ€‹tâ†’âŸ¨Z,Ï•âŸ©â„™âŠ—dâ€‹t\langle Z^{n},\phi\rangle\_{\mathbb{P}\otimes dt}\to\langle Z,\phi\rangle\_{\mathbb{P}\otimes dt} as nâ†’âˆn\to\infty, where the inner product is defined by âŸ¨L,MâŸ©â„™âŠ—dâ€‹t:=ğ”¼â€‹[âˆ«0TâŸ¨Lt,MtâŸ©â€‹ğ‘‘t]\langle L,M\rangle\_{\mathbb{P}\otimes dt}:=\mathbb{E}[\int\_{0}^{T}\langle L\_{t},M\_{t}\rangle dt] for L,Mâˆˆğ•ƒ2â€‹(â„d)L,M\in\mathbb{L}^{2}(\mathbb{R}^{d}). Similarly, the weak limit in L2â€‹(Ft;â„d)L^{2}({\mathcal{}F}\_{t};\mathbb{R}^{d}) is defined w.r.t. the inner product âŸ¨Î¾,Î·âŸ©â„™:=ğ”¼â€‹[âŸ¨Î¾,Î·âŸ©]\langle\xi,\eta\rangle\_{\mathbb{P}}:=\mathbb{E}[\langle\xi,\eta\rangle] for Î¾,Î·âˆˆL2â€‹(Ft;â„d)\xi,\eta\in L^{2}({\mathcal{}F}\_{t};\mathbb{R}^{d}). YxY^{x} is the strong limit of (Yx;N)Nâˆˆâ„•(Y^{x;N})\_{N\in\mathbb{N}} in ğ•ƒ2â€‹(â„)\mathbb{L}^{2}(\mathbb{R}) (i.e., as Nâ†’âˆN\to\infty
  â€–Yx;Nâˆ’Yxâ€–ğ•ƒ2â†’0\|Y^{x;N}-Y^{x}\|\_{\mathbb{L}^{2}}\to 0),
  ZxZ^{x} is the weak limit of (Zx;N)Nâˆˆâ„•(Z^{x;N})\_{N\in\mathbb{N}} in ğ•ƒ2â€‹(â„d)\mathbb{L}^{2}(\mathbb{R}^{d}), and for each tâˆˆ[0,T]t\in[0,T] KtxK\_{t}^{x} is the weak limit of Ktx;NK\_{t}^{x;N} in L2â€‹(Ft;â„){L}^{2}({\mathcal{}F}\_{t};\mathbb{R}).

The following proposition shows that for each Nâˆˆâ„•N\in\mathbb{N} the solution to the penalized BSDE ([2.7](https://arxiv.org/html/2510.10260v1#S2.E7 "Equation 2.7 â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) can be represented by a certain optimal stochastic control problem under ambiguity. The corresponding proof is presented in Section [6.1](https://arxiv.org/html/2510.10260v1#S6.SS1 "6.1 Proof of results in Section 2 â€£ 6 Proofs â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).").

###### Proposition 2.10.

Suppose that Assumptions [2](https://arxiv.org/html/2510.10260v1#S2 "2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).") and [2](https://arxiv.org/html/2510.10260v1#S2 "2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).") hold. Let Nâˆˆâ„•N\in\mathbb{N} be given. Denote by Yx;NY^{x;N} the first component of the unique solution to ([2.7](https://arxiv.org/html/2510.10260v1#S2.E7 "Equation 2.7 â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")). Then Yx;NY^{x;N} admits a representation of the robust control optimization problem in the following sense:
Let A{\mathcal{}A} be the set of all ğ”½\mathbb{F}-progressively measurable processes Î±=(Î±t)tâˆˆ[0,T]\alpha=(\alpha\_{t})\_{t\in[0,T]} with values in {0,1}\{0,1\}. Set for every tâˆˆ[0,T]t\in[0,T] and Nâˆˆâ„•N\in\mathbb{N}

|  |  |  |
| --- | --- | --- |
|  | Itx;N,Î±:=âˆ«tTeâˆ’âˆ«ts(Î²u+Nâ€‹Î±u)â€‹ğ‘‘uâ€‹(râ€‹(Xsx)+Râ€‹(Xsx)â€‹Nâ€‹Î±s)â€‹ğ‘‘s+eâˆ’âˆ«tT(Î²u+Nâ€‹Î±u)â€‹ğ‘‘uâ€‹Râ€‹(XTx).\operatorname{I}\_{t}^{x;N,\alpha}:=\int\_{t}^{T}e^{-\int\_{t}^{s}(\beta\_{u}+N\alpha\_{u})du}\big(r(X\_{s}^{x})+R(X\_{s}^{x})\,N\alpha\_{s}\big)ds+e^{-\int\_{t}^{T}(\beta\_{u}+N\alpha\_{u})du}R(X\_{T}^{x}). |  |

Then it holds for every tâˆˆ[0,T]t\in[0,T] that Ytx;N=essâ€‹supÎ±âˆˆAâ¡Etgâ€‹[Itx;N,Î±]=Etgâ€‹[Itx;N,Î±âˆ—,x;N],Y\_{t}^{x;N}=\operatorname\*{ess\,sup}\_{\alpha\in{\mathcal{}A}}{\mathcal{}E}\_{t}^{g}[\operatorname{I}\_{t}^{x;N,\alpha}]={\mathcal{}E}\_{t}^{g}[\operatorname{I}\_{t}^{x;N,\alpha^{\*,x;N}}], â„™\mathbb{P}-a.s.,Â where Î±âˆ—,x;N:=(Î±tâˆ—,x;N)tâˆˆ[0,T]âˆˆA\alpha^{\*,x;N}:=(\alpha^{\*,x;N}\_{t})\_{t\in[0,T]}\in{\mathcal{}A} is the optimizer given by

|  |  |  |  |
| --- | --- | --- | --- |
| (2.8) |  | Î±tâˆ—,x;N:=ğŸ{Râ€‹(Xtx)>Ytx;N}forÂ tâˆˆ[0,T].\displaystyle\alpha^{\*,x;N}\_{t}:={\bf 1}\_{\{R(X\_{t}^{x})>Y\_{t}^{x;N}\}}\quad\mbox{for $t\in[0,T]$}. |  |

## 3 Exploratory framework: approximation of optimal stopping under ambiguity

Based on the results in Section [2](https://arxiv.org/html/2510.10260v1#S2 "2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175)."), we are able to show that
for sufficiently large Nâˆˆâ„•N\in\mathbb{N}, the optimal stopping problem Vx(=Yx)V^{x}(=Y^{x}) under ambiguity in ([2.2](https://arxiv.org/html/2510.10260v1#S2.E2 "Equation 2.2 â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) (see also Proposition [2.8](https://arxiv.org/html/2510.10260v1#S2.Thmtheorem8 "Proposition 2.8. â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) can be approximated by the optimal stochastic control problem Yx;NY^{x;N} under ambiguity (see PropositionÂ [2.10](https://arxiv.org/html/2510.10260v1#S2.Thmtheorem10 "Proposition 2.10. â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")). The proofs of all the results in this section are presented in SectionÂ [6.2](https://arxiv.org/html/2510.10260v1#S6.SS2 "6.2 Proof of results in Section 3 â€£ 6 Proofs â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).").

We introduce an exploratory framework of [[66](https://arxiv.org/html/2510.10260v1#bib.bib66), [67](https://arxiv.org/html/2510.10260v1#bib.bib67)] into Yx;NY^{x;N}. In particular, we aim to study a robust analogue of the optimal exploratory stopping framework in [[15](https://arxiv.org/html/2510.10260v1#bib.bib15)]. To that end, let Î \Pi be the set of all ğ”½\mathbb{F}-progressively measurable processes Ï€=(Ï€t)tâˆˆ[0,T]\pi=(\pi\_{t})\_{t\in[0,T]} taking values in [0,1][0,1], i.e., an exploratory version of the {0,1}\{0,1\}-valued controls set A{\mathcal{}A} appearing in Proposition [2.10](https://arxiv.org/html/2510.10260v1#S2.Thmtheorem10 "Proposition 2.10. â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).").

Then let H:[0,1]âˆ‹aâ†’Hâ€‹(a)âˆˆâ„{\mathcal{}H}:[0,1]\ni a\to{\mathcal{}H}(a)\in\mathbb{R} be the binary differential entropy defined by

|  |  |  |  |
| --- | --- | --- | --- |
| (3.1) |  | Hâ€‹(a):=aâ€‹logâ¡(a)+(1âˆ’a)â€‹logâ¡(1âˆ’a)forÂ aâˆˆ(0,1),\displaystyle{\mathcal{}H}(a):=a\log(a)+(1-a)\log(1-a)\quad\mbox{for $a\in(0,1)$}, |  |

with the convention that Hâ€‹(0):=limaâ†“0Hâ€‹(a)=0{\mathcal{}H}(0):=\lim\_{a\downarrow 0}{\mathcal{}H}(a)=0 and Hâ€‹(1):=limaâ†‘1Hâ€‹(a)=0{\mathcal{}H}(1):=\lim\_{a\uparrow 1}{\mathcal{}H}(a)=0.

Finally, let Î»>0\lambda>0 denote the temperature parameter reflecting the trade-off between exploration and exploitation.

We can then describe the decision-makerâ€™s optimal exploratory control problem VÂ¯x;N,Î»:=(VÂ¯tx;N,Î»)tâˆˆ[0,T]\overline{V}^{x;N,\lambda}:=(\overline{V}\_{t}^{x;N,\lambda})\_{t\in[0,T]} under ambiguity
for any Nâˆˆâ„•N\in\mathbb{N} andÂ Î»>0\lambda>0:

|  |  |  |  |
| --- | --- | --- | --- |
| (3.2) |  | VÂ¯tx;N,Î»:=essâ€‹supÏ€âˆˆÎ â¡Etgâ€‹[JÂ¯tx;N,Î»,Ï€],forÂ tâˆˆ[0,T],\displaystyle\overline{V}\_{t}^{x;N,\lambda}:=\operatorname\*{ess\,sup}\_{\pi\in\Pi}{\mathcal{}E}\_{t}^{g}[\overline{\operatorname{J}}\_{t}^{x;N,\lambda,\pi}],\quad\mbox{for $t\in[0,T]$}, |  |

where for each Ï€âˆˆÎ \pi\in\Pi, the integrand JÂ¯tx;N,Î»,Ï€\overline{\operatorname{J}}\_{t}^{x;N,\lambda,\pi} is given by

|  |  |  |  |
| --- | --- | --- | --- |
|  | JÂ¯tx;N,Î»,Ï€:=\displaystyle\overline{\operatorname{J}}\_{t}^{x;N,\lambda,\pi}:= | âˆ«tTeâˆ’âˆ«ts(Î²u+Nâ€‹Ï€u)â€‹ğ‘‘uâ€‹(râ€‹(Xsx)+Râ€‹(Xsx)â€‹Nâ€‹Ï€sâˆ’Î»â€‹Hâ€‹(Ï€s))\displaystyle\int\_{t}^{T}e^{-\int\_{t}^{s}(\beta\_{u}+N\pi\_{u})du}\big(r(X\_{s}^{x})+R(X\_{s}^{x})\,N\pi\_{s}-\lambda{\mathcal{}H}(\pi\_{s})\big) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +eâˆ’âˆ«tT(Î²u+Nâ€‹Ï€u)â€‹ğ‘‘uâ€‹Râ€‹(XTx),\displaystyle\quad+e^{-\int\_{t}^{T}(\beta\_{u}+N\pi\_{u})du}R(X\_{T}^{x}), |  |

where XxX^{x} is given in ([2.1](https://arxiv.org/html/2510.10260v1#S2.E1 "Equation 2.1 â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) and (Î²t)tâˆˆ[0,T](\beta\_{t})\_{t\in[0,T]} is the discount rate appearing inâ€„([2.2](https://arxiv.org/html/2510.10260v1#S2.E2 "Equation 2.2 â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")).

###### Remark 3.1.

We note that the differential entropy H{\mathcal{}H} given in ([3.1](https://arxiv.org/html/2510.10260v1#S3.E1 "Equation 3.1 â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) is strictly convex and bounded on [0,1][0,1]. Moreover, since all the exploratory control Ï€âˆˆÎ \pi\in\Pi is uniformly bounded byÂ [0,1][0,1], by using the same arguments presented for Remark [2.5](https://arxiv.org/html/2510.10260v1#S2.Thmtheorem5 "Remark 2.5. â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175)."), we have that JÂ¯tx;N,Î»,Ï€âˆˆL2â€‹(FT;â„)\overline{\operatorname{J}}\_{t}^{x;N,\lambda,\pi}\in L^{2}({\mathcal{}F}\_{T};\mathbb{R}) for all Nâˆˆâ„•N\in\mathbb{N}, Î»>0\lambda>0, and Ï€âˆˆÎ \pi\in\Pi. Therefore, VÂ¯x;N,Î»\overline{V}^{x;N,\lambda} given in ([3.2](https://arxiv.org/html/2510.10260v1#S3.E2 "Equation 3.2 â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) is well-defined for all Nâˆˆâ„•N\in\mathbb{N} and Î»>0\lambda>0.

###### Remark 3.2.

Assume that the probability space (Î©,F,â„™)(\Omega,{\mathcal{}F},\mathbb{P}) supports a uniformly distributed random variable UU with values in [0,1][0,1] which is independent of the fixed Brownian motion BB. Then we are able to see that each exploratory control Ï€âˆˆÎ \pi\in\Pi generates a Bernoulli-distributed (randomized) process under drift ambiguity. Indeed, we recall the variational characterization of gg-expectation in RemarkÂ [2.2](https://arxiv.org/html/2510.10260v1#S2.Thmtheorem2 "Remark 2.2. â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).") with the map g^:Î©Ã—[0,T]Ã—â„dâ†’â„\hat{g}:\Omega\times[0,T]\times\mathbb{R}^{d}\to\mathbb{R} and the set Bg{\mathcal{}B}^{g}. Then, for all Nâˆˆâ„•N\in\mathbb{N}, Î»>0\lambda>0, and tâˆˆ[0,T]t\in[0,T], we can rewrite the conditional gg-expectation value Etgâ€‹[JÂ¯tx;N,Î»,Ï€]{\mathcal{}E}\_{t}^{g}[\overline{\operatorname{J}}\_{t}^{x;N,\lambda,\pi}] given inÂ ([3.2](https://arxiv.org/html/2510.10260v1#S3.E2 "Equation 3.2 â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) as the following strong formulation for drift ambiguity under â„™\mathbb{P} (see [[1](https://arxiv.org/html/2510.10260v1#bib.bib1), SectionÂ 5]):

|  |  |  |  |
| --- | --- | --- | --- |
| (3.3) |  | Etgâ€‹[JÂ¯tx;N,Î»,Ï€]=essâ€‹infÏ‘âˆˆBgâ¡ğ”¼tâ€‹[JÂ¯tx;N,Î»,Ï€,Ï‘+âˆ«tTg^â€‹(s,Ï‘s)â€‹ğ‘‘s],\displaystyle{\mathcal{}E}\_{t}^{g}[\overline{\operatorname{J}}\_{t}^{x;N,\lambda,\pi}]=\operatorname\*{ess\,inf}\_{\vartheta\in{\mathcal{}B}^{g}}\mathbb{E}\_{t}[\overline{\operatorname{J}}\_{t}^{x;N,\lambda,\pi,\vartheta}+\int\_{t}^{T}\hat{g}(s,\vartheta\_{s})ds], |  |

where for each Ï€âˆˆÎ \pi\in\Pi and Ï‘âˆˆBg\vartheta\in{\mathcal{}B}^{g}, the term JÂ¯tx;N,Î»,Ï€,Ï‘\overline{\operatorname{J}}\_{t}^{x;N,\lambda,\pi,\vartheta} is given by

|  |  |  |  |
| --- | --- | --- | --- |
|  | JÂ¯tx;N,Î»,Ï€,Ï‘:=\displaystyle\overline{\operatorname{J}}\_{t}^{x;N,\lambda,\pi,\vartheta}:= | âˆ«tTeâˆ’âˆ«ts(Î²u+Nâ€‹Ï€u)â€‹ğ‘‘uâ€‹(râ€‹(Xsx;Ï‘)+Râ€‹(Xsx;Ï‘)â€‹Nâ€‹Ï€sâˆ’Î»â€‹Hâ€‹(Ï€s))â€‹ğ‘‘s\displaystyle\int\_{t}^{T}e^{-\int\_{t}^{s}(\beta\_{u}+N\pi\_{u})du}\big(r(X\_{s}^{x;\vartheta})+R(X\_{s}^{x;\vartheta})\,N\pi\_{s}-\lambda{\mathcal{}H}(\pi\_{s})\big)ds |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +eâˆ’âˆ«tT(Î²u+Nâ€‹Ï€u)â€‹ğ‘‘uâ€‹Râ€‹(XTx;Ï‘),\displaystyle\quad+e^{-\int\_{t}^{T}(\beta\_{u}+N\pi\_{u})du}R(X\_{T}^{x;\vartheta}), |  |

where (Xtx;Ï‘)tâˆˆ[0,T](X^{x;\vartheta}\_{t})\_{t\in[0,T]} is givenÂ by Xtx;Ï‘:=x+âˆ«0t(bso+Ïƒsoâ€‹Ï‘s)â€‹ğ‘‘s+âˆ«0tÏƒsoâ€‹ğ‘‘Bs,X\_{t}^{x;\vartheta}:=x+\int\_{0}^{t}\big(b^{o}\_{s}+\sigma\_{s}^{o}\vartheta\_{s}\big)ds+\int\_{0}^{t}\sigma^{o}\_{s}dB\_{s}, for tâˆˆ[0,T]t\in[0,T],
and (bo,Ïƒo)(b^{o},\sigma^{o}) are the baseline parameters appearing in ([2.1](https://arxiv.org/html/2510.10260v1#S2.E1 "Equation 2.1 â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")).

Then by using the random variable UU and its independence with the filtration ğ”½\mathbb{F} generated by BB, we can apply the Blackwellâ€“Dubins lemma (see [[8](https://arxiv.org/html/2510.10260v1#bib.bib8)]) to ensure that there exists a (randomized) process (Î±~t)tâˆˆ[0,T](\widetilde{\alpha}\_{t})\_{t\in[0,T]} such that for every tâˆˆ[0,T]t\in[0,T], â„™\mathbb{P}-a.s.,

|  |  |  |
| --- | --- | --- |
|  | â„™â€‹(Î±~t=1|Ft)=Ï€t=1âˆ’â„™â€‹(Î±~t=0|Ft),\mathbb{P}(\widetilde{\alpha}\_{t}=1\,|\,{\mathcal{}F}\_{t})=\pi\_{t}=1-\mathbb{P}(\widetilde{\alpha}\_{t}=0\,|\,{\mathcal{}F}\_{t}), |  |

i.e., Î±~t\widetilde{\alpha}\_{t} is a Bernoulli distributed random variable with probability Ï€t\pi\_{t} given Ft{\mathcal{}F}\_{t}.

In order to characterize VÂ¯x;N,Î»\overline{V}^{x;N,\lambda} given in ([3.2](https://arxiv.org/html/2510.10260v1#S3.E2 "Equation 3.2 â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")), we first collect several preliminary results concerning the following auxiliary BSDE formulations: Recall that FxF^{x} is given inÂ ([2.3](https://arxiv.org/html/2510.10260v1#S2.E3 "Equation 2.3 â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")). Set for every Ï€âˆˆÎ \pi\in\Pi and (Ï‰,t,y,z)âˆˆÎ©Ã—[0,T]Ã—â„Ã—â„d(\omega,t,y,z)\in\Omega\times[0,T]\times\mathbb{R}\times\mathbb{R}^{d}

|  |  |  |  |
| --- | --- | --- | --- |
| (3.4) |  | FÂ¯tx;N,Î»,Ï€â€‹(Ï‰,y,z):=Ftxâ€‹(Ï‰,y,z)+Nâ€‹(Râ€‹(Xtxâ€‹(Ï‰))âˆ’y)â€‹Ï€tâ€‹(Ï‰)âˆ’Î»â€‹Hâ€‹(Ï€tâ€‹(Ï‰)).\displaystyle\overline{F}^{x;N,\lambda,\pi}\_{t}(\omega,y,z):=F^{x}\_{t}(\omega,y,z)+N(R\big(X\_{t}^{x}(\omega)\big)-y)\pi\_{t}(\omega)-\lambda{\mathcal{}H}(\pi\_{t}(\omega)). |  |

Then, consider the (controlled) processes (YÂ¯tx;N,Î»,Ï€,ZÂ¯tx;N,Î»,Ï€)tâˆˆ[0,T](\overline{Y}\_{t}^{x;N,\lambda,\pi},\overline{Z}\_{t}^{x;N,\lambda,\pi})\_{t\in[0,T]} satisfying

|  |  |  |  |
| --- | --- | --- | --- |
| (3.5) |  | YÂ¯tx;N,Î»,Ï€=Râ€‹(XTx)+âˆ«tTFÂ¯sx;N,Î»,Ï€â€‹(YÂ¯sx;N,Î»,Ï€,ZÂ¯sx;N,Î»,Ï€)â€‹ğ‘‘sâˆ’âˆ«tTZÂ¯sx;N,Î»,Ï€â€‹ğ‘‘Bs,\displaystyle\overline{Y}\_{t}^{x;N,\lambda,\pi}=R(X\_{T}^{x})+\int\_{t}^{T}\overline{F}\_{s}^{x;N,\lambda,\pi}(\overline{Y}\_{s}^{x;N,\lambda,\pi},\overline{Z}\_{s}^{x;N,\lambda,\pi})ds-\int\_{t}^{T}\overline{Z}\_{s}^{x;N,\lambda,\pi}dB\_{s}, |  |

###### Remark 3.3.

Under Assumptions [2](https://arxiv.org/html/2510.10260v1#S2 "2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).") and [2](https://arxiv.org/html/2510.10260v1#S2 "2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175)."), the following statements hold for all Ï€âˆˆÎ \pi\in\Pi, Nâˆˆâ„•N\in\mathbb{N} and Î»>0\lambda>0:

* (i)

  Since (Ï€t)tâˆˆ[0,T]âˆˆÎ (\pi\_{t})\_{t\in[0,T]}\in\Pi and (Hâ€‹(Ï€t))tâˆˆ[0,T]({\mathcal{}H}(\pi\_{t}))\_{t\in[0,T]} are uniformly bounded (see Remarkâ€„[3.1](https://arxiv.org/html/2510.10260v1#S3.Thmtheorem1 "Remark 3.1. â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")), we are able to see that the parameters of ([3.5](https://arxiv.org/html/2510.10260v1#S3.E5 "Equation 3.5 â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) satisfy all the conditions given in [[49](https://arxiv.org/html/2510.10260v1#bib.bib49), Section 3]. Therefore, there exists a unique solution (YÂ¯tx;N,Î»,Ï€,ZÂ¯tx;N,Î»,Ï€)tâˆˆ[0,T]âˆˆğ•Š2â€‹(â„)Ã—ğ•ƒ2â€‹(â„d)(\overline{Y}\_{t}^{x;N,\lambda,\pi},\overline{Z}\_{t}^{x;N,\lambda,\pi})\_{t\in[0,T]}\in\mathbb{S}^{2}(\mathbb{R})\times\mathbb{L}^{2}(\mathbb{R}^{d}) to ([3.5](https://arxiv.org/html/2510.10260v1#S3.E5 "Equation 3.5 â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")).
* (ii)

  Since YÂ¯tx;N,Î»,Ï€âˆˆL2â€‹(Ft;â„)\overline{Y}\_{t}^{x;N,\lambda,\pi}\in L^{2}({\mathcal{}F}\_{t};\mathbb{R}) and JÂ¯tx;N,Î»,Ï€âˆˆL2â€‹(FT;â„)\overline{\operatorname{J}}\_{t}^{x;N,\lambda,\pi}\in L^{2}({\mathcal{}F}\_{T};\mathbb{R}) (see RemarkÂ [3.1](https://arxiv.org/html/2510.10260v1#S3.Thmtheorem1 "Remark 3.1. â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")), we can use the same arguments presented for Proposition [2.10](https://arxiv.org/html/2510.10260v1#S2.Thmtheorem10 "Proposition 2.10. â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).") to have that

  |  |  |  |  |
  | --- | --- | --- | --- |
  | (3.6) |  | YÂ¯tx;N,Î»,Ï€=Etgâ€‹[JÂ¯tx;N,Î»,Ï€],â„™-a.s. for allÂ tâˆˆ[0,T].\displaystyle\overline{Y}\_{t}^{x;N,\lambda,\pi}={\mathcal{}E}\_{t}^{g}[\overline{\operatorname{J}}\_{t}^{x;N,\lambda,\pi}],\quad\mbox{$\mathbb{P}$-a.s. for all $t\in[0,T]$}. |  |

Moreover, set for every Nâˆˆâ„•N\in\mathbb{N}, Î»>0\lambda>0, and (Ï‰,t,y,z)âˆˆÎ©Ã—[0,T]Ã—â„Ã—â„d(\omega,t,y,z)\in\Omega\times[0,T]\times\mathbb{R}\times\mathbb{R}^{d} by

|  |  |  |  |
| --- | --- | --- | --- |
| (3.7) |  | FÂ¯tx;N,Î»â€‹(Ï‰,y,z):=Ftxâ€‹(Ï‰,y,z)+Gtx;N,Î»â€‹(Ï‰,y),whereâ€‹Gtx;N,Î»â€‹(Ï‰,y):=Nâ€‹(Râ€‹(Xtxâ€‹(Ï‰))âˆ’y)+Î»â€‹logâ¡(eâˆ’NÎ»â€‹{Râ€‹(Xtxâ€‹(Ï‰))âˆ’y}+1).\displaystyle\begin{aligned} &\overline{F}\_{t}^{x;N,\lambda}(\omega,y,z):=F\_{t}^{x}(\omega,y,z)+G\_{t}^{x;N,\lambda}(\omega,y),\\ &\;\;\mbox{where}\;\;G\_{t}^{x;N,\lambda}(\omega,y):=N\Big(R\big(X\_{t}^{x}(\omega)\big)-y\Big)+\lambda\log\Big(e^{-\frac{N}{\lambda}\{R(X\_{t}^{x}(\omega))-y\}}+1\Big).\end{aligned} |  |

Then consider the couple of processes (YÂ¯tx;N,Î»,ZÂ¯tx;N,Î»)tâˆˆ[0,T](\overline{Y}\_{t}^{x;N,\lambda},\overline{Z}\_{t}^{x;N,\lambda})\_{t\in[0,T]} satisfying

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
| (3.8) |  | YÂ¯tx;N,Î»=\displaystyle\overline{Y}\_{t}^{x;N,\lambda}= | Râ€‹(XTx)+âˆ«tTFÂ¯sx;N,Î»â€‹(YÂ¯sx;N,Î»,ZÂ¯sx;N,Î»)â€‹ğ‘‘sâˆ’âˆ«tTZÂ¯sx;N,Î»â€‹ğ‘‘Bs.\displaystyle R(X\_{T}^{x})+\int\_{t}^{T}\overline{F}\_{s}^{x;N,\lambda}(\overline{Y}\_{s}^{x;N,\lambda},\overline{Z}\_{s}^{x;N,\lambda})ds-\int\_{t}^{T}\overline{Z}\_{s}^{x;N,\lambda}dB\_{s}. |  |

In the following theorem, the optimal exploratory control problem VÂ¯x;N,Î»\overline{V}^{x;N,\lambda} under ambiguity and its optimal control are characterized via the auxiliary BSDE given inÂ ([3.8](https://arxiv.org/html/2510.10260v1#S3.E8 "Equation 3.8 â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")).

###### Theorem 3.4.

Suppose that Assumptions [2](https://arxiv.org/html/2510.10260v1#S2 "2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).") and [2](https://arxiv.org/html/2510.10260v1#S2 "2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).") hold. Recall the logistic function logitâ¡(â‹…)\operatorname{logit}(\cdot) in ([1.3](https://arxiv.org/html/2510.10260v1#S1.E3 "Equation 1.3 â€£ 1 Introduction â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")).
The following statements hold for every Nâˆˆâ„•N\in\mathbb{N} and Î»>0\lambda>0.

* (i)

  There exists a unique solution (YÂ¯x;N,Î»,ZÂ¯x;N,Î»)âˆˆğ•Š2â€‹(â„)Ã—ğ•ƒ2â€‹(â„d)(\overline{Y}^{x;N,\lambda},\overline{Z}^{x;N,\lambda})\in\mathbb{S}^{2}(\mathbb{R})\times\mathbb{L}^{2}(\mathbb{R}^{d}) ofÂ ([3.8](https://arxiv.org/html/2510.10260v1#S3.E8 "Equation 3.8 â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")).
* (ii)

  Moreover, recall VÂ¯x;N,Î»\overline{V}^{x;N,\lambda} is given in ([3.2](https://arxiv.org/html/2510.10260v1#S3.E2 "Equation 3.2 â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")). Then it holds for every tâˆˆ[0,T]t\in[0,T] that YÂ¯tx;N,Î»=VÂ¯tx;N,Î»=Etgâ€‹[JÂ¯tx;N,Î»,Ï€âˆ—,x;N,Î»]\overline{Y}\_{t}^{x;N,\lambda}=\overline{V}\_{t}^{x;N,\lambda}={\mathcal{}E}\_{t}^{g}[\overline{\operatorname{J}}\_{t}^{x;N,\lambda,\pi^{\*,x;N,\lambda}}] â„™\mathbb{P}-a.s., where the optimizer Ï€âˆ—,x;N,Î»:=(Ï€tâˆ—,x;N,Î»)tâˆˆ[0,T]âˆˆÎ \pi^{\*,x;N,\lambda}:=(\pi^{\*,x;N,\lambda}\_{t})\_{t\in[0,T]}\in\Pi is given by

  |  |  |  |  |
  | --- | --- | --- | --- |
  | (3.9) |  | Ï€tâˆ—,x;N,Î»:=logitâ¡(NÎ»â€‹(Râ€‹(Xtx)âˆ’YÂ¯tx;N,Î»)),tâˆˆ[0,T].\displaystyle\pi^{\*,x;N,\lambda}\_{t}:=\operatorname{logit}\Big(\frac{N}{\lambda}(R(X\_{t}^{x})-\overline{Y}\_{t}^{x;N,\lambda})\Big),\quad t\in[0,T]. |  |

The following theorem is devoted to showing the comparison and stability results between the exploratory and non-exploratory optimal control problems characterized in Proposition [2.10](https://arxiv.org/html/2510.10260v1#S2.Thmtheorem10 "Proposition 2.10. â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).") and Theorem [3.4](https://arxiv.org/html/2510.10260v1#S3.Thmtheorem4 "Theorem 3.4. â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).").

###### Theorem 3.5.

Suppose that Assumptions [2](https://arxiv.org/html/2510.10260v1#S2 "2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).") and [2](https://arxiv.org/html/2510.10260v1#S2 "2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).") hold. For each Nâˆˆâ„•N\in\mathbb{N} and Î»>0\lambda>0, let (Yx;N,Zx;N)(Y^{x;N},Z^{x;N}) and (YÂ¯x;N,Î»,ZÂ¯x;N,Î»)(\overline{Y}^{x;N,\lambda},\overline{Z}^{x;N,\lambda}) be the unique solution to the BSDEs ([2.7](https://arxiv.org/html/2510.10260v1#S2.E7 "Equation 2.7 â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) and ([3.8](https://arxiv.org/html/2510.10260v1#S3.E8 "Equation 3.8 â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")), respectively. Then it holds that for every Nâˆˆâ„•N\in\mathbb{N} and Î»>0\lambda>0,

|  |  |  |  |
| --- | --- | --- | --- |
| (3.10) |  | Ytx;Nâ‰¤YÂ¯tx;N,Î»,â„™-a.s., for allÂ tâ‰¥0,\displaystyle Y\_{t}^{x;N}\leq\overline{Y}\_{t}^{x;N,\lambda},\quad\mbox{$\mathbb{P}$-a.s., for all $t\geq 0$, } |  |

In particular, there exists some constant C>0C>0 (that does not depend on Nâˆˆâ„•N\in\mathbb{N} and Î»>0\lambda>0 but on T>0T>0) such that for every Nâˆˆâ„•N\in\mathbb{N} and Î»>0\lambda>0,

|  |  |  |  |
| --- | --- | --- | --- |
| (3.11) |  | â€–Yx;Nâˆ’YÂ¯x;N,Î»â€–ğ•Š2+â€–Zx;Nâˆ’ZÂ¯x;N,Î»â€–ğ•ƒ2â‰¤Câ€‹Î»,\displaystyle\|Y^{x;N}-\overline{Y}^{x;N,\lambda}\|\_{\mathbb{S}^{2}}+\|Z^{x;N}-\overline{Z}^{x;N,\lambda}\|\_{\mathbb{L}^{2}}\leq C\lambda, |  |

This implies that for any Nâˆˆâ„•N\in\mathbb{N}, YÂ¯x;N,Î»\overline{Y}^{x;N,\lambda} strongly converges to Yx;NY^{x;N} in ğ•Š2â€‹(â„)\mathbb{S}^{2}(\mathbb{R}),Â asÂ Î»â†“0\lambda\downarrow 0.

As a consequence of Theorem [3.5](https://arxiv.org/html/2510.10260v1#S3.Thmtheorem5 "Theorem 3.5. â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175)."), the following corollary establishes the asymptotic behavior of the optimal exploratory control derived in Theorem [3.4](https://arxiv.org/html/2510.10260v1#S3.Thmtheorem4 "Theorem 3.4. â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).") into the optimal non-exploratory control derived in Proposition [2.10](https://arxiv.org/html/2510.10260v1#S2.Thmtheorem10 "Proposition 2.10. â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).").

###### Corollary 3.6.

Suppose that Assumptions [2](https://arxiv.org/html/2510.10260v1#S2 "2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).") and [2](https://arxiv.org/html/2510.10260v1#S2 "2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).") hold. For each Nâˆˆâ„•N\in\mathbb{N} and Î»>0\lambda>0, let Î±âˆ—,x;NâˆˆA\alpha^{\*,x;N}\in{\mathcal{}A} and Ï€âˆ—,x;N,Î»âˆˆÎ \pi^{\*,x;N,\lambda}\in\Pi be defined as in ([2.8](https://arxiv.org/html/2510.10260v1#S2.E8 "Equation 2.8 â€£ Proposition 2.10. â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) and ([3.9](https://arxiv.org/html/2510.10260v1#S3.E9 "Equation 3.9 â€£ item (ii) â€£ Theorem 3.4. â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")), respectively. Then it holds that for every Nâˆˆâ„•N\in\mathbb{N},

|  |  |  |  |
| --- | --- | --- | --- |
| (3.12) |  | â€–Î±âˆ—,x;Nâˆ’Ï€âˆ—,x;N,Î»â€–ğ•ƒ1â†’0asÂ Î»â†“0,\displaystyle\big\|\alpha^{\*,x;N}-\pi^{\*,x;N,\lambda}\big\|\_{\mathbb{L}^{1}}\to 0\quad\mbox{as $\lambda\downarrow 0$}, |  |

i.e., for any Nâˆˆâ„•N\in\mathbb{N}, Ï€âˆ—,x;N,Î»\pi^{\*,x;N,\lambda} strongly converges to Î±âˆ—,x;N\alpha^{\*,x;N} in the set of all ğ”½\mathbb{F} progressively measurable processes endowed with the norm âˆ¥â‹…âˆ¥ğ•ƒ1\|\cdot\|\_{\mathbb{L}^{1}}, as Î»â†“0\lambda\downarrow 0.

## 4 Policy iteration theorem & RL algorithm

A typical RL approach to finding the optimal strategy is based on policy iteration, where the strategy is successively refined through iterative updates. In this section, we establish the policy iteration theorem based on the verification result in TheoremÂ [3.4](https://arxiv.org/html/2510.10260v1#S3.Thmtheorem4 "Theorem 3.4. â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175)."), and then provide the corresponding reinforcement learning algorithm.

Throughout this section, we fix a sufficiently large Nâˆˆâ„•N\in\mathbb{N} and a small Î»>0\lambda>0 so that YÂ¯x;N,Î»\overline{Y}^{x;N,\lambda} serves as an accurate approximation of YxY^{x} (see Remarkâ€„[2.9](https://arxiv.org/html/2510.10260v1#S2.Thmtheorem9 "Remark 2.9. â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).") and Theorem [3.5](https://arxiv.org/html/2510.10260v1#S3.Thmtheorem5 "Theorem 3.5. â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")). The proofs of all theorems in this section can be found in Section [6.3](https://arxiv.org/html/2510.10260v1#S6.SS3 "6.3 Proof of results in Section 4 â€£ 6 Proofs â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).").

For any Ï€nâˆˆÎ \pi^{n}\in\Pi and nâˆˆâ„•n\in\mathbb{N}, denote by (YÂ¯x;N,Î»,Ï€n,ZÂ¯x;N,Î»,Ï€n)âˆˆğ•Š2â€‹(â„)Ã—ğ•ƒ2â€‹(â„d)(\overline{Y}^{x;N,\lambda,\pi^{n}},\overline{Z}^{x;N,\lambda,\pi^{n}})\in\mathbb{S}^{2}(\mathbb{R})\times\mathbb{L}^{2}(\mathbb{R}^{d}) the unique solution of ([3.5](https://arxiv.org/html/2510.10260v1#S3.E5 "Equation 3.5 â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) under the exploratory control Ï€n\pi^{n} (see Remarkâ€„[3.3](https://arxiv.org/html/2510.10260v1#S3.Thmtheorem3 "Remark 3.3. â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")â€„(i)).
Recall the logistic function logitâ¡(â‹…)\operatorname{logit}(\cdot) in ([1.3](https://arxiv.org/html/2510.10260v1#S1.E3 "Equation 1.3 â€£ 1 Introduction â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")).
Then one can construct Ï€n+1âˆˆÎ \pi^{n+1}\in\Pi as

|  |  |  |  |
| --- | --- | --- | --- |
| (4.1) |  | Ï€tn+1:=logitâ¡(NÎ»â€‹(Râ€‹(Xtx)âˆ’YÂ¯tx;N,Î»,Ï€n)),tâˆˆ[0,T].\displaystyle\pi^{n+1}\_{t}:=\operatorname{logit}(\frac{N}{\lambda}(R(X\_{t}^{x})-\overline{Y}\_{t}^{x;N,\lambda,\pi^{n}})),\quad t\in[0,T]. |  |

###### Theorem 4.1.

Suppose that Assumptions [2](https://arxiv.org/html/2510.10260v1#S2 "2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).") and [2](https://arxiv.org/html/2510.10260v1#S2 "2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).") hold. Let YÂ¯x;N,Î»\overline{Y}^{x;N,\lambda} be the first component of the unique solution ofÂ ([3.8](https://arxiv.org/html/2510.10260v1#S3.E8 "Equation 3.8 â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) (see Theorem [3.4](https://arxiv.org/html/2510.10260v1#S3.Thmtheorem4 "Theorem 3.4. â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")). Let Ï€1âˆˆÎ \pi^{1}\in\Pi be given. Let (YÂ¯x;N,Î»,Ï€1,ZÂ¯x;N,Î»,Ï€1)(\overline{Y}^{x;N,\lambda,\pi^{1}},\overline{Z}^{x;N,\lambda,\pi^{1}}) be the unique solution of ([3.5](https://arxiv.org/html/2510.10260v1#S3.E5 "Equation 3.5 â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) underÂ Ï€1\pi^{1}. For everyÂ nâˆˆâ„•n\in\mathbb{N}, let Ï€n+1\pi^{n+1} be defined iteratively according to ([4.1](https://arxiv.org/html/2510.10260v1#S4.E1 "Equation 4.1 â€£ 4 Policy iteration theorem & RL algorithm â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) and let (YÂ¯x;N,Î»,Ï€n+1,ZÂ¯x;N,Î»,Ï€n+1)(\overline{Y}^{x;N,\lambda,\pi^{n+1}},\overline{Z}^{x;N,\lambda,\pi^{n+1}}) be the unique solution ofÂ ([3.5](https://arxiv.org/html/2510.10260v1#S3.E5 "Equation 3.5 â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) under Ï€n+1\pi^{n+1}. Then the following hold for every nâˆˆâ„•n\in\mathbb{N}:

* (i)

  YÂ¯tx;N,Î»â‰¥YÂ¯tx;N,Î»,Ï€n+1â‰¥YÂ¯tx;N,Î»,Ï€n\overline{Y}\_{t}^{x;N,\lambda}\geq\overline{Y}\_{t}^{x;N,\lambda,\pi^{n+1}}\geq\overline{Y}\_{t}^{x;N,\lambda,\pi^{n}}, â„™\mathbb{P}-a.s., for all tâˆˆ[0,T]t\in[0,T];
* (ii)

  Set Î”â€‹(x;N,Î»,Ï€1):=â€–YÂ¯x;N,Î»âˆ’YÂ¯x;N,Î»,Ï€1â€–ğ•Š22\Delta({x;N,\lambda,\pi^{1}}):=\|\overline{Y}^{x;N,\lambda}-\overline{Y}^{x;N,\lambda,\pi^{1}}\|\_{\mathbb{S}^{2}}^{2}. There exists some constant C>0{C}>0 (that depends on N,T,dN,T,d but not on n,Î»n,\lambda) such that

  |  |  |  |
  | --- | --- | --- |
  |  | â€–YÂ¯x;N,Î»âˆ’YÂ¯x;N,Î»,Ï€n+1â€–ğ•Š22+â€–ZÂ¯x;N,Î»âˆ’ZÂ¯x;N,Î»,Ï€n+1â€–ğ•ƒ22â‰¤Cnn!â€‹Î”â€‹(x;N,Î»,Ï€1),\displaystyle\|\overline{Y}^{x;N,\lambda}-\overline{Y}^{x;N,\lambda,\pi^{n+1}}\|\_{\mathbb{S}^{2}}^{2}+\|\overline{Z}^{x;N,\lambda}-\overline{Z}^{x;N,\lambda,\pi^{n+1}}\|\_{\mathbb{L}^{2}}^{2}\leq\frac{{C}^{n}}{n!}\Delta({x;N,\lambda,\pi^{1}}), |  |
  |  |  |  |
  | --- | --- | --- |
  |  | â€–Ï€n+1âˆ’Ï€âˆ—â€–ğ•Š22â‰¤NÎ»â€‹Cnâˆ’1(nâˆ’1)!â€‹Î”â€‹(x;N,Î»,Ï€1).\displaystyle\|{\pi}^{n+1}-{\pi}^{\*}\|\_{\mathbb{S}^{2}}^{2}\leq\frac{N}{\lambda}\frac{{C}^{n-1}}{(n-1)!}\Delta({x;N,\lambda,\pi^{1}}). |  |

In particular, YÂ¯tx;N,Î»,Ï€nâ†‘YÂ¯tx;N,Î»\overline{Y}^{x;N,\lambda,\pi^{n}}\_{t}\uparrow\overline{Y}^{x;N,\lambda}\_{t} and Ï€tnâ†‘Ï€tâˆ—\pi^{n}\_{t}\uparrow\pi^{\*}\_{t} â„™\mathbb{P}-a.s. for all tâˆˆ[0,T]t\in[0,T] asÂ nâ†’âˆn\to\infty.

Let us mention some Markovian properties of the BSDEs arising in the policy iteration result given in Theorem [4.1](https://arxiv.org/html/2510.10260v1#S4.Thmtheorem1 "Theorem 4.1. â€£ 4 Policy iteration theorem & RL algorithm â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175)."), as well as how these properties can be leveraged to implement the policy iteration algorithm using neural networks.
To that end, in the remainder of this section, we consider the following specification:
{setting}

* (i)

  The map gg given in Definition [2.1](https://arxiv.org/html/2510.10260v1#S2.Thmtheorem1 "Definition 2.1. â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).") is deterministic, i.e., for every Ï‰1,Ï‰2âˆˆÎ©\omega^{1},\omega^{2}\in\Omega, gâ€‹(Ï‰1,â‹…,â‹…)=gâ€‹(Ï‰2,â‹…,â‹…)g(\omega^{1},\cdot,\cdot)=g(\omega^{2},\cdot,\cdot).
* (ii)

  The baseline parameters bob^{o} and Ïƒo\sigma^{o} appearing in ([2.1](https://arxiv.org/html/2510.10260v1#S2.E1 "Equation 2.1 â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) are of the form given in Remark [2.3](https://arxiv.org/html/2510.10260v1#S2.Thmtheorem3 "Remark 2.3. â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")â€„(ii), so that Assumption [2](https://arxiv.org/html/2510.10260v1#S2 "2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).") holds.
* (iii)

  The reward functions RR and rr satisfy all the conditions in AssumptionÂ [2](https://arxiv.org/html/2510.10260v1#S2 "2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")â€„(i). Furthermore, rr is continuous. Lastly, the discount rate process (Î²t)tâˆˆ[0,T](\beta\_{t})\_{t\in[0,T]} is deterministic and bounded by the constant CÎ²>0C\_{\beta}>0 in AssumptionÂ [2](https://arxiv.org/html/2510.10260v1#S2 "2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")â€„(ii).

Denote by Î Ë‡\check{\Pi} the set of all Borel measurable maps
Ï€Ë‡:[0,T]Ã—â„dâˆ‹(t,x~)â†’Ï€Ë‡tâ€‹(x~)âˆˆ[0,1],\check{\pi}:[0,T]\times\mathbb{R}^{d}\ni(t,\tilde{x})\to\check{\pi}\_{t}(\tilde{x})\in[0,1],
so that Ï€Ë‡â€‹(Xx):=(Ï€Ë‡tâ€‹(Xtx))tâˆˆ[0,T]âˆˆÎ \check{\pi}(X^{x}):=(\check{\pi}\_{t}(X\_{t}^{x}))\_{t\in[0,T]}\in\Pi, i.e., Î Ë‡\check{\Pi} is the closed loop policyÂ set.

Under Setting [4](https://arxiv.org/html/2510.10260v1#S4 "4 Policy iteration theorem & RL algorithm â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175)."),
set for every Ï€Ë‡âˆˆÎ Ë‡\check{\pi}\in\check{\Pi} and (t,x~,y,z)âˆˆ[0,T]Ã—â„dÃ—â„Ã—â„d(t,\tilde{x},y,z)\in[0,T]\times\mathbb{R}^{d}\times\mathbb{R}\times\mathbb{R}^{d},

|  |  |  |  |
| --- | --- | --- | --- |
| (4.2) |  | FË‡tN,Î»;Ï€Ë‡â€‹(x~,y,z):=râ€‹(x~)âˆ’Î²tâ€‹y+gâ€‹(t,z)+Nâ€‹(Râ€‹(x~)âˆ’y)â€‹Ï€Ë‡tâ€‹(x~)âˆ’Î»â€‹â„‹â€‹(Ï€Ë‡tâ€‹(x~)),\displaystyle\check{F}\_{t}^{N,\lambda;\check{\pi}}(\tilde{x},y,z):=r(\tilde{x})-\beta\_{t}y+g(t,z)+N(R(\tilde{x})-y)\check{\pi}\_{t}(\tilde{x})-\lambda\mathcal{H}\big(\check{\pi}\_{t}(\tilde{x})\big), |  |

so that (FË‡tN,Î»,Ï€Ë‡â€‹(â‹…,â‹…,â‹…))tâˆˆ[0,T](\check{F}\_{t}^{N,\lambda,\check{\pi}}(\cdot,\cdot,\cdot))\_{t\in[0,T]} is deterministic and FË‡â‹…N,Î»,Ï€Ë‡â€‹(â‹…,â‹…,â‹…)\check{F}\_{\cdot}^{N,\lambda,\check{\pi}}(\cdot,\cdot,\cdot) is Borel measurable.

###### Remark 4.2.

Under SettingÂ [4](https://arxiv.org/html/2510.10260v1#S4 "4 Policy iteration theorem & RL algorithm â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175)."), recall (YÂ¯x;N,Î»,ZÂ¯x;N,Î»)(\overline{Y}^{x;N,\lambda},\overline{Z}^{x;N,\lambda}) satisfying ([3.8](https://arxiv.org/html/2510.10260v1#S3.E8 "Equation 3.8 â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")); see also Theorem [3.4](https://arxiv.org/html/2510.10260v1#S3.Thmtheorem4 "Theorem 3.4. â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")). Then set for every (t,x~,y,z)âˆˆ[0,T]Ã—â„dÃ—â„Ã—â„d(t,\tilde{x},y,z)\in[0,T]\times\mathbb{R}^{d}\times\mathbb{R}\times\mathbb{R}^{d}

|  |  |  |
| --- | --- | --- |
|  | FË‡tN,Î»â€‹(x~,y,z):=râ€‹(x~)âˆ’Î²tâ€‹y+gâ€‹(t,z)+Nâ€‹(Râ€‹(x~)âˆ’y)+Î»â€‹logâ¡(eâˆ’NÎ»â€‹{Râ€‹(x~)âˆ’y}+1).\displaystyle\check{F}\_{t}^{N,\lambda}(\tilde{x},y,z):=r(\tilde{x})-\beta\_{t}y+g(t,z)+N(R(\tilde{x})-y)+\lambda\log(e^{-\frac{N}{\lambda}\{R(\tilde{x})-y\}}+1). |  |

Clearly, FË‡tN,Î»â€‹(Xtx,y,z)=FÂ¯tx;N,Î»â€‹(y,z)\check{F}\_{t}^{N,\lambda}(X\_{t}^{x},y,z)=\overline{F}^{x;N,\lambda}\_{t}(y,z) for (t,x,y,z)âˆˆ[0,T]Ã—â„dÃ—â„Ã—â„d(t,x,y,z)\in[0,T]\times\mathbb{R}^{d}\times\mathbb{R}\times\mathbb{R}^{d}; see ([3.7](https://arxiv.org/html/2510.10260v1#S3.E7 "Equation 3.7 â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")). Moreover, FË‡â‹…N,Î»â€‹(â‹…,â‹…,â‹…)\check{F}\_{\cdot}^{N,\lambda}(\cdot,\cdot,\cdot) and Râ€‹(â‹…)R(\cdot) satisfy the conditions (M1b) and (M1bc\textrm{M1b}^{c}) given in [[19](https://arxiv.org/html/2510.10260v1#bib.bib19)]. Therefore, an application of [[19](https://arxiv.org/html/2510.10260v1#bib.bib19), TheoremÂ 8.12] ensures the existence of a viscosity solution222We refer to [[19](https://arxiv.org/html/2510.10260v1#bib.bib19), Definition 8.11] for the definition of a viscosity solution of ([4.3](https://arxiv.org/html/2510.10260v1#S4.E3 "Equation 4.3 â€£ Remark 4.2. â€£ 4 Policy iteration theorem & RL algorithm â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) with setting the terminal condition Râ†·Î¨R\curvearrowright\Psi and the generator FË‡â‹…N,Î»â†·g\check{F}^{N,\lambda}\_{\cdot}\curvearrowright g therein. vË‡N,Î»\check{v}^{N,\lambda} of the following PDE:,

|  |  |  |  |
| --- | --- | --- | --- |
| (4.3) |  | (âˆ‚tv+â„’â€‹v)â€‹(t,x)+FË‡tN,Î»â€‹(x,vâ€‹(t,x),((Ïƒ~o)âŠ¤â€‹âˆ‡v)â€‹(t,x))=0â€‹(t,x)âˆˆ[0,T)Ã—â„d,\displaystyle(\partial\_{t}v+\mathcal{L}v)(t,x)+\check{F}^{N,\lambda}\_{t}\big(x,v(t,x),((\widetilde{\sigma}^{o})^{\top}\nabla v)(t,x)\big)=0\;\;\;(t,x)\in[0,T)\times\mathbb{R}^{d}, |  |

with vâ€‹(T,â‹…)=Râ€‹(â‹…)v(T,\cdot)=R(\cdot), where the infinitesimal operator â„’\mathcal{L} of XxX^{x} under the measureÂ â„™\mathbb{P} is given by â„’â€‹vâ€‹(t,x):=12â€‹âˆ‘i,j=1d((Ïƒ~o)âŠ¤â€‹Ïƒ~oâ€‹(t,x))i,jâ€‹âˆ‚2vâ€‹(t,x)âˆ‚xiâ€‹âˆ‚xj+âˆ‘i=1db~ioâ€‹(t,x)â€‹âˆ‚vâ€‹(t,x)âˆ‚xi\mathcal{L}v(t,x):=\frac{1}{2}\sum\_{i,j=1}^{d}((\widetilde{\sigma}^{o})^{\top}\widetilde{\sigma}^{o}(t,x))\_{i,j}\frac{\partial^{2}v(t,x)}{\partial x\_{i}\partial x\_{j}}+\sum\_{i=1}^{d}\widetilde{b}^{o}\_{i}(t,x)\frac{\partial v(t,x)}{\partial x\_{i}}. In particular, it holds that YÂ¯tx;N,Î»=vË‡N,Î»â€‹(t,Xtx)\overline{Y}\_{t}^{x;N,\lambda}=\check{v}^{N,\lambda}(t,X\_{t}^{x}), â„™âŠ—dâ€‹t\mathbb{P}\otimes dt-a.e., for all tâˆˆ[0,T]t\in[0,T].

We now have a sequence of closed-loop policies in Î Ë‡\check{\Pi} deriving the policy iteration.

###### Corollary 4.3.

Under SettingÂ [4](https://arxiv.org/html/2510.10260v1#S4 "4 Policy iteration theorem & RL algorithm â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175)."), let Ï€Ë‡1âˆˆÎ Ë‡\check{\pi}^{1}\in\check{\Pi} be given.

* (i)

  There exists two sequences of Borel measurable functions (vN,Î»,n)nâˆˆâ„•(v^{N,\lambda,n})\_{n\in\mathbb{N}} and (wN,Î»,n)nâˆˆâ„•(w^{N,\lambda,n})\_{n\in\mathbb{N}} defined on [0,T]Ã—â„d[0,T]\times\mathbb{R}^{d} (having values in â„\mathbb{R} and â„d\mathbb{R}^{d}, respectively) such that for every nâˆˆâ„•n\in\mathbb{N} and every tâˆˆ[0,T]t\in[0,T], â„™âŠ—dâ€‹t\mathbb{P}\otimes dt-a.e.,

  |  |  |  |
  | --- | --- | --- |
  |  | YÂ¯tx;N,Î»,Ï€Ë‡nâ€‹(Xx)=vN,Î»,nâ€‹(t,Xtx),ZÂ¯sx;N,Î»,Ï€Ë‡nâ€‹(Xx)=((Ïƒ~o)âŠ¤â€‹wN,Î»,n)â€‹(t,Xtx),\displaystyle\overline{Y}\_{t}^{x;N,\lambda,\check{\pi}^{n}(X^{x})}=v^{N,\lambda,n}(t,X\_{t}^{x}),\qquad\overline{Z}\_{s}^{x;N,\lambda,\check{\pi}^{n}(X^{x})}=\big((\widetilde{\sigma}^{o})^{\top}w^{N,\lambda,n}\big)(t,X\_{t}^{x}), |  |

  with Ï€Ë‡nâ€‹(Xx):=(Ï€Ë‡tnâ€‹(Xtx))tâˆˆ[0,T]âˆˆÎ \check{\pi}^{n}(X^{x}):=(\check{\pi}^{n}\_{t}(X\_{t}^{x}))\_{t\in[0,T]}\in\Pi, where for any nâ‰¥2n\geq 2, Ï€Ë‡nâˆˆÎ Ë‡\check{\pi}^{n}\in\check{\Pi} is defined iteratively as for (t,x~)âˆˆ[0,T]Ã—â„d(t,\tilde{x})\in[0,T]\times\mathbb{R}^{d}

  |  |  |  |  |
  | --- | --- | --- | --- |
  | (4.4) |  | Ï€Ë‡tnâ€‹(x~):=logitâ¡(NÎ»â€‹(Râ€‹(x~)âˆ’vN,Î»,nâˆ’1â€‹(t,x~))).\displaystyle\check{\pi}^{n}\_{t}(\tilde{x}):=\operatorname{logit}\Big(\frac{N}{\lambda}\big(R(\tilde{x})-v^{N,\lambda,n-1}(t,\tilde{x})\big)\Big). |  |
* (ii)

  If Ï€Ë‡t1â€‹(â‹…)\check{\pi}\_{t}^{1}(\cdot) is continuous on â„d\mathbb{R}^{d} for any tâˆˆ[0,T]t\in[0,T], one can find a sequence of functions (vN,Î»,n)nâˆˆâ„•(v^{N,\lambda,n})\_{n\in\mathbb{N}} which satisfies all the properties given in (i) and each vN,Î»,nv^{N,\lambda,n}, nâˆˆâ„•n\in\mathbb{N}, is a viscosity solution of the following PDE:

  |  |  |  |
  | --- | --- | --- |
  |  | (âˆ‚tv+â„’â€‹v)â€‹(t,x)+FË‡tN,Î»,Ï€Ë‡nâ€‹(x,vâ€‹(t,x),((Ïƒ~o)âŠ¤â€‹âˆ‡v)â€‹(t,x))=0â€‹(t,x)âˆˆ[0,T)Ã—â„d,(\partial\_{t}v+\mathcal{L}v)(t,x)+\check{F}^{N,\lambda,\check{\pi}^{n}}\_{t}(x,v(t,x),((\widetilde{\sigma}^{o})^{\top}\nabla v)(t,x))=0\;\;\;(t,x)\in[0,T)\times\mathbb{R}^{d}, |  |

  with vâ€‹(T,â‹…)=Râ€‹(â‹…)v(T,\cdot)=R(\cdot), where Ï€Ë‡nâˆˆÎ Ë‡\check{\pi}^{n}\in\check{\Pi} is defined iteratively as in ([4.4](https://arxiv.org/html/2510.10260v1#S4.E4 "Equation 4.4 â€£ item (i) â€£ Corollary 4.3. â€£ 4 Policy iteration theorem & RL algorithm â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")).

The core logic of the policy iteration given in
Theorem [4.1](https://arxiv.org/html/2510.10260v1#S4.Thmtheorem1 "Theorem 4.1. â€£ 4 Policy iteration theorem & RL algorithm â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).") and Corollary [4.3](https://arxiv.org/html/2510.10260v1#S4.Thmtheorem3 "Corollary 4.3. â€£ 4 Policy iteration theorem & RL algorithm â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).") consists of two steps at each iteration. The first is the policy update, given in ([4.1](https://arxiv.org/html/2510.10260v1#S4.E1 "Equation 4.1 â€£ 4 Policy iteration theorem & RL algorithm â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) or ([4.4](https://arxiv.org/html/2510.10260v1#S4.E4 "Equation 4.4 â€£ item (i) â€£ Corollary 4.3. â€£ 4 Policy iteration theorem & RL algorithm â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")). The second is the policy evaluation, which corresponds to derive either the solution (YÂ¯x;N,Î»,Ï€n,ZÂ¯x;N,Î»,Ï€n)(\overline{Y}^{x;N,\lambda,\pi^{n}},\overline{Z}^{x;N,\lambda,\pi^{n}}) of the BSDE ([3.5](https://arxiv.org/html/2510.10260v1#S3.E5 "Equation 3.5 â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) under the updated policy Ï€n\pi^{n}, or equivalently, the solution vN,Î»,nv^{N,\lambda,n} of the PDE under Ï€Ë‡n\check{\pi}^{n} as given in Corollary [4.3](https://arxiv.org/html/2510.10260v1#S4.Thmtheorem3 "Corollary 4.3. â€£ 4 Policy iteration theorem & RL algorithm â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")â€„(ii).

In what follows, we develop an RL scheme, relying on the deep splitting method of Beck et al.Â [[5](https://arxiv.org/html/2510.10260v1#bib.bib5)] and Frey and KÃ¶ckÂ [[25](https://arxiv.org/html/2510.10260v1#bib.bib25)], to implement the policy evaluation step at each iteration. For this purpose, we first introduce some notation, omitting the dependence on (N,Î»)(N,\lambda) (even though the objects still depend on them).

{setting}

Denote by Iâˆˆâ„•I\in\mathbb{N} the number of steps in the time discretization and denote by Î˜âŠ‚â„p\Theta\subset\mathbb{R}^{p} (with some pâˆˆâ„•p\in\mathbb{N}) the parameter spaces for neural networks in.

1. (i)

   Let ti=iâ€‹Î”â€‹tt\_{i}=i\Delta t and Î”â€‹Bi:=Bti+1âˆ’Bti\Delta B\_{i}:=B\_{t\_{i+1}}-B\_{t\_{i}} for i={0,â€¦,Iâˆ’1}i=\{0,\dots,I-1\} with Î”â€‹t:=T/I\Delta t:=T/I. Then the Euler scheme of ([2.1](https://arxiv.org/html/2510.10260v1#S2.E1 "Equation 2.1 â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) under Setting [4](https://arxiv.org/html/2510.10260v1#S4 "4 Policy iteration theorem & RL algorithm â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")â€„(ii) is given by: XË‡0x:=x\check{X}^{x}\_{0}:=x,

   |  |  |  |
   | --- | --- | --- |
   |  | XË‡i+1x:=XË‡ix+b~oâ€‹(ti,XË‡ix)â€‹Î”â€‹t+Ïƒ~oâ€‹(ti,XË‡ix)â€‹Î”â€‹Bi,iâˆˆ{0,â€¦,Iâˆ’1}.\displaystyle\check{X}^{x}\_{i+1}:=\check{X}^{x}\_{i}+\widetilde{b}^{o}(t\_{i},\check{X}^{x}\_{i})\Delta t+\widetilde{\sigma}^{o}(t\_{i},\check{X}^{x}\_{i})\Delta B\_{i},\quad i\in\{0,\ldots,I-1\}. |  |
2. (ii)

   The initial closed-loop policy Ï€Ë‡1\check{\pi}^{1} is given by Ï€Ë‡i1â€‹(â‹…):=logitâ¡(NÎ»â€‹(Râ€‹(â‹…)âˆ’vi0â€‹(â‹…)))\check{\pi}^{1}\_{i}(\cdot):=\operatorname{logit}(\frac{N}{\lambda}(R(\cdot)-v^{0}\_{i}(\cdot))), iâˆˆ{0,â€¦,Iâˆ’1}i\in\{0,\dots,I-1\},
   with some function (at least continuous) vi0:â„dâ†’â„v^{0}\_{i}:\mathbb{R}^{d}\to\mathbb{R}.
3. (iii)

   For each nâˆˆâ„•n\in\mathbb{N} and iâˆˆ{0,â€¦,Iâˆ’1}i\in\{0,\dots,I-1\}, let vinâ€‹(â‹…;Ï‘in):â„dâ†’â„v\_{i}^{n}(\,\cdot\,;\vartheta^{n}\_{i}):\mathbb{R}^{d}\to\mathbb{R}
   be neural realizations of vN,Î»,nâ€‹(ti,â‹…)v^{N,\lambda,n}(t\_{i},\cdot)
   parameterized by Ï‘inâˆˆÎ˜\vartheta^{n}\_{i}\in\Theta (e.g., feed-forward networks (FNNs) with C1C^{1}-regularity or Lipschitz continuous with weakÂ derivative).
4. (vi)

   For each nâˆˆâ„•n\in\mathbb{N}, the time-discretized, n+1n+1-th updated, closed-loop policy Ï€Ë‡n+1â€‹(â‹…;Ï‘in)\check{\pi}^{n+1}(\cdot;\vartheta\_{i}^{n}) (that depends on the parameter Ï‘in\vartheta^{n}\_{i} appearing in (iii)) is given by
   Ï€Ë‡in+1â€‹(â‹…;Ï‘in):=logitâ¡(NÎ»â€‹(Râ€‹(â‹…)âˆ’vinâ€‹(â‹…;Ï‘in)))\check{\pi}^{n+1}\_{i}(\cdot;\vartheta\_{i}^{n}):=\operatorname{logit}(\frac{N}{\lambda}(R(\cdot)-v^{n}\_{i}(\cdot;\vartheta^{n}\_{i}))), iâˆˆ{0,â€¦,Iâˆ’1}.i\in\{0,\dots,I-1\}.
5. (v)

   For each nâˆˆâ„•n\in\mathbb{N}, set for every (x~,y,z)âˆˆâ„dÃ—â„Ã—â„d(\tilde{x},y,z)\in\mathbb{R}^{d}\times\mathbb{R}\times\mathbb{R}^{d},

   |  |  |  |
   | --- | --- | --- |
   |  | FË‡inâ€‹(x~,y,z;Ï‘inâˆ’1):=râ€‹(x~)âˆ’Î²tiâ€‹y+gâ€‹(t,z)+Nâ€‹(Râ€‹(x~)âˆ’y)â€‹Ï€Ë‡inâ€‹(x~;Ï‘inâˆ’1)âˆ’Î»â€‹â„‹â€‹(Ï€Ë‡inâ€‹(x~;Ï‘inâˆ’1)),\displaystyle\begin{aligned} \check{F}\_{i}^{n}(\tilde{x},y,z;\vartheta\_{i}^{n-1})&:=r(\tilde{x})-\beta\_{t\_{i}}y+g(t,z)+N(R(\tilde{x})-y)\check{\pi}\_{i}^{n}(\tilde{x};\vartheta\_{i}^{n-1})\\ &\qquad-\lambda\mathcal{H}\big(\check{\pi}\_{i}^{n}(\tilde{x};\vartheta\_{i}^{n-1})\big),\end{aligned} |  |

   with the convention that Ï€Ë‡1â€‹(â‹…;Ï‘i0)â‰¡Ï€Ë‡i1â€‹(â‹…)\check{\pi}^{1}(\cdot;\vartheta\_{i}^{0})\equiv\check{\pi}\_{i}^{1}(\cdot) for any Ï‘i0âˆˆÎ˜\vartheta\_{i}^{0}\in\Theta (see (ii)) so that FË‡i1â€‹(â‹…,â‹…,â‹…)\check{F}^{1}\_{i}(\cdot,\cdot,\cdot) is not parametrized over Î˜\Theta but depends only on the form Ï€Ë‡i1\check{\pi}\_{i}^{1}.

To apply the deep splitting method, one needs Ïƒ~oâ€‹(ti,XË‡ix)\widetilde{\sigma}^{o}(t\_{i},\check{X}^{x}\_{i}) in the loss function calculation (given in ([4.6](https://arxiv.org/html/2510.10260v1#S4.E6 "Equation 4.6 â€£ 4 Policy iteration theorem & RL algorithm â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175)."))), which is unknown to an RL agent before learning the environment but can be learned from from the realized quadratic covariance of observed data333The mapping â„dÃ—dâˆ‹Aâ†¦A12âˆˆâ„dÃ—d\mathbb{R}^{d\times d}\ni A\mapsto A^{\frac{1}{2}}\in\mathbb{R}^{d\times d} denotes the symmetric positive-definite square root of a positive semidefinite matrix AA.

|  |  |  |
| --- | --- | --- |
|  | Î£â€‹(XË‡i:i+1x):=1Î”â€‹tâ€‹((XË‡i+1xâˆ’XË‡ix)â€‹(XË‡i+1xâˆ’XË‡ix)âŠ¤)12,\Sigma({\check{X}^{x}\_{i:i+1}}):=\frac{1}{\sqrt{\Delta t}}\big((\check{X}^{x}\_{i+1}-\check{X}^{x}\_{i})(\check{X}^{x}\_{i+1}-\check{X}^{x}\_{i})^{\top}\big)^{\frac{1}{2}}, |  |

so that Î£â€‹(XË‡i:i+1x)â€‹Î£â€‹(XË‡i:i+1x)âŠ¤â€‹Î”â€‹tâ†’Ïƒ~oâ€‹(ti,XË‡ix)â€‹Ïƒ~oâ€‹(ti,XË‡ix)âŠ¤â€‹Î”â€‹t\Sigma({\check{X}^{x}\_{i:i+1}})\Sigma({\check{X}^{x}\_{i:i+1}})^{\top}\Delta t\to\widetilde{\sigma}^{o}(t\_{i},\check{X}^{x}\_{i})\widetilde{\sigma}^{o}(t\_{i},\check{X}^{x}\_{i})^{\top}\Delta t as Î”â€‹tâ†“0\Delta t\downarrow 0 in probabilityÂ â„™\mathbb{P}; see e.g., [[34](https://arxiv.org/html/2510.10260v1#bib.bib34), Chapter I, TheoremÂ 4.47] and [[56](https://arxiv.org/html/2510.10260v1#bib.bib56), SectionÂ 6, Theorem 22].

Algorithm 1  Policy iteration algorithm

0:â€‚Batch size Mâˆˆâ„•M\in\mathbb{N}; Number of policy iterations nÂ¯âˆˆâ„•\overline{n}\in\mathbb{N}; Number of epochs â„“Â¯âˆˆâ„•\overline{\ell}\in\mathbb{N} for policy evaluation; Learning rate Î±âˆˆ(0,1)\alpha\in(0,1).

1:â€‚Set the initial closed loop policy Ï€Ë‡i1â€‹(â‹…)\check{\pi}^{1}\_{i}(\cdot), iâˆˆ{0,â€¦,Iâˆ’1}i\in\{0,\ldots,I-1\}, as in Setting [4](https://arxiv.org/html/2510.10260v1#S4 "4 Policy iteration theorem & RL algorithm â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")â€„(ii).

2:â€‚Initialize Ï‘i0,âˆ—âˆˆÎ˜\vartheta\_{i}^{0,\*}\in\Theta, iâˆˆ{0,1,â€¦,I}i\in\{0,1,\dots,I\}.

3:â€‚for n=1,â€¦,nÂ¯n=1,\ldots,\bar{n} do

4:â€ƒâ€‚Initialize Ï‘inâˆˆÎ˜\vartheta^{n}\_{i}\in\Theta, iâˆˆ{0,â€¦,Iâˆ’1}i\in\{0,\ldots,I-1\}, and Ï‘In,âˆ—âˆˆÎ˜\vartheta\_{I}^{n,\*}\in\Theta.

5:â€ƒâ€‚for l=1,â€¦,â„“Â¯l=1,\ldots,\bar{\ell} do

6:â€ƒâ€ƒâ€‚Generate MM trajectories of (XË‡ix)i=0I(\check{X}^{x}\_{i})\_{i=0}^{I}; see Setting [4](https://arxiv.org/html/2510.10260v1#S4 "4 Policy iteration theorem & RL algorithm â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")â€„(i).

7:â€ƒâ€ƒâ€‚for i=Iâˆ’1,â€¦,0i=I-1,\ldots,0 do

8:â€ƒâ€ƒâ€ƒâ€‚Minimize ([4.6](https://arxiv.org/html/2510.10260v1#S4.E6 "Equation 4.6 â€£ 4 Policy iteration theorem & RL algorithm â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) over Ï‘inâˆˆÎ˜\vartheta^{n}\_{i}\in\Theta by using SGD with learning rate Î±\alpha.

9:â€ƒâ€ƒâ€‚end for

10:â€ƒâ€‚end for

11:â€ƒâ€‚Denote by Ï‘in,âˆ—\vartheta^{n,\*}\_{i} the lastly updated parameters at tit\_{i}, iâˆˆ{0,â€¦,Iâˆ’1}i\in\{0,\ldots,I-1\}.

12:â€‚end for

With all this notation set in place, for each iteration nâˆˆâ„•n\in\mathbb{N}, we present the policy evaluation as the following iterative minimization problem: for iâˆˆ{0,â€¦,Iâˆ’1}i\in\{0,\dots,I-1\}

|  |  |  |  |
| --- | --- | --- | --- |
| (4.5) |  | Ï‘in,âˆ—âˆˆargâ€‹minÏ‘inâˆˆÎ˜â¡ğ”nâ€‹(Ï‘in;Ï‘inâˆ’1,âˆ—,Ï‘i+1n,âˆ—),\displaystyle\vartheta^{n,\*}\_{i}\in\operatorname\*{arg\,min}\_{\vartheta^{n}\_{i}\in\Theta}\mathfrak{L}^{n}(\vartheta^{n}\_{i};\vartheta\_{i}^{n-1,\*},\vartheta\_{i+1}^{n,\*}), |  |

where ğ”inâ€‹(â‹…;Ï‘inâˆ’1,âˆ—,Ï‘i+1n,âˆ—):Î˜â†’â„\mathfrak{L}\_{i}^{n}(\cdot;\vartheta\_{i}^{n-1,\*},\vartheta\_{i+1}^{n,\*}):\Theta\to\mathbb{R} is the (parameterized) L2L^{2}-loss function given by

|  |  |  |
| --- | --- | --- |
|  | ğ”n(Ï‘in;Ï‘inâˆ’1,âˆ—,Ï‘i+1n,âˆ—):=ğ”¼[|vi+1n(XË‡i+1x;Ï‘i+1n,âˆ—)âˆ’vin(XË‡ix;Ï‘in)\displaystyle\mathfrak{L}^{n}(\vartheta^{n}\_{i};\vartheta\_{i}^{n-1,\*},\vartheta\_{i+1}^{n,\*}):=\mathbb{E}\Big[\big|v^{n}\_{i+1}(\check{X}^{x}\_{i+1};\vartheta^{n,\*}\_{i+1})-v^{n}\_{i}(\check{X}^{x}\_{i};\vartheta^{n}\_{i}) |  |
|  |  |  |  |
| --- | --- | --- | --- |
| (4.6) |  | +FË‡in(XË‡i+1x,vi+1n(XË‡i+1x;Î¸i+1n,âˆ—),Î£(XË‡i:i+1x)âˆ‡vi+1n(XË‡i+1x;Î¸i+1n,âˆ—);Ï‘inâˆ’1,âˆ—)Î”t|2],\displaystyle\quad\quad+\check{F}^{n}\_{i}\big(\check{X}^{x}\_{i+1},v^{n}\_{i+1}(\check{X}^{x}\_{i+1};\theta^{n,\*}\_{i+1}),\Sigma({\check{X}^{x}\_{i:i+1}})\nabla v^{n}\_{i+1}(\check{X}^{x}\_{i+1};\theta^{n,\*}\_{i+1});\vartheta\_{i}^{n-1,\*}\big)\Delta t\big|^{2}\Big], |  |

with the convention that vInâ€‹(XË‡Ix;Ï‘In,âˆ—):=Râ€‹(XË‡Ix){v}^{n}\_{I}(\check{X}\_{I}^{x};\vartheta\_{I}^{n,\*}):=R(\check{X}\_{I}^{x}) with an arbitraryÂ Ï‘In,âˆ—âˆˆÎ˜\vartheta\_{I}^{n,\*}\in\Theta, and that FË‡i1\check{F}^{1}\_{i} is not parametrized over Î˜\Theta (see Setting [4](https://arxiv.org/html/2510.10260v1#S4 "4 Policy iteration theorem & RL algorithm â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")â€„(v); hence Ï‘i0,âˆ—âˆˆÎ˜\vartheta\_{i}^{0,\*}\in\Theta is also anÂ arbitrary).

We numerically solve the problem given in ([4.5](https://arxiv.org/html/2510.10260v1#S4.E5 "Equation 4.5 â€£ 4 Policy iteration theorem & RL algorithm â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) by using stochastic gradient descent (SGD) algorithms (see, e.g., [[28](https://arxiv.org/html/2510.10260v1#bib.bib28), Section 4.3]). Then we provide a pseudo-code in AlgorithmÂ [1](https://arxiv.org/html/2510.10260v1#alg1 "Algorithm 1 â€£ 4 Policy iteration theorem & RL algorithm â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).") to show how the policy iteration can be implemented.

###### Remark 4.4.

Note that the deep splitting method of [[5](https://arxiv.org/html/2510.10260v1#bib.bib5), [25](https://arxiv.org/html/2510.10260v1#bib.bib25)] is not the only neural realization of our policy evaluation; instead deep BSDEsâ€‰/â€‰PDEs schemes of [[30](https://arxiv.org/html/2510.10260v1#bib.bib30), [33](https://arxiv.org/html/2510.10260v1#bib.bib33), [62](https://arxiv.org/html/2510.10260v1#bib.bib62)] can be an alternative. More recently, several articles, including [[27](https://arxiv.org/html/2510.10260v1#bib.bib27), [46](https://arxiv.org/html/2510.10260v1#bib.bib46)], provide the error analyses for such methods. To obtain a full error-analysis of our policy iteration algorithm, one would need to relax the standard Lipschitz and HÃ¶lder conditions on BSDE generators in the mentioned articles so as to cover the generator
FË‡N,Î»,Ï€Ë‡n\check{F}^{N,\lambda,\check{\pi}^{n}} in ([4.2](https://arxiv.org/html/2510.10260v1#S4.E2 "Equation 4.2 â€£ 4 Policy iteration theorem & RL algorithm â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")), and then incorporate the policy evaluation errors from the neural approximations (under such relaxed conditions) into the convergence rate established in TheoremÂ [4.1](https://arxiv.org/html/2510.10260v1#S4.Thmtheorem1 "Theorem 4.1. â€£ 4 Policy iteration theorem & RL algorithm â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175)."). We defer this direction to a futureÂ work.

## 5 Experiments

In this section,444All computations were performed using PyTorch on a Mac Mini with Apple M4 Pro processor and 64GB RAM. The complete code is available at: <https://github.com/GEOR-TS/Exploratory_Robust_Stopping_RL>.
we analyze some examples to support the applicability of AlgorithmÂ [1](https://arxiv.org/html/2510.10260v1#alg1 "Algorithm 1 â€£ 4 Policy iteration theorem & RL algorithm â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).").
Let us fix gâ€‹(t,z)â‰¡âˆ’Îµâ€‹|z|g(t,z)\equiv-\varepsilon|z| for (t,z)âˆˆ[0,T]Ã—â„d(t,z)\in[0,T]\times\mathbb{R}^{d}, where Îµâ‰¥0\varepsilon\geq 0 represents the degree of ambiguity. By RemarkÂ [2.2](https://arxiv.org/html/2510.10260v1#S2.Thmtheorem2 "Remark 2.2. â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175)."), for any Î¾âˆˆL2â€‹(â„±Ï„;â„d)\xi\in L^{2}(\mathcal{F}\_{\tau};\mathbb{R}^{d}), it holds that â„°tgâ€‹[Î¾]=essâ€‹supÏ‘âˆˆâ„¬Îµâ¡ğ”¼tâ„™Ï‘â€‹[Î¾]\mathcal{E}^{g}\_{t}[\xi]=\operatorname\*{ess\,sup}\_{\vartheta\in\mathcal{B}^{\varepsilon}}\mathbb{E}\_{t}^{\mathbb{P}^{\vartheta}}[\xi], where â„¬Îµ\mathcal{B}^{\varepsilon} includes all ğ”½\mathbb{F}-progressively measurable processes (Ï‘t)tâˆˆ[0,T](\vartheta\_{t})\_{t\in[0,T]} such that |Ï‘t|â‰¤Îµ|\vartheta\_{t}|\leq\varepsilon â„™âŠ—dâ€‹t\mathbb{P}\otimes dt-a.e..

In the training phase, following Setting [4](https://arxiv.org/html/2510.10260v1#S4 "4 Policy iteration theorem & RL algorithm â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")â€„(vi),
we parametrize vN,Î»,nâ€‹(ti,x)v^{N,\lambda,n}(t\_{i},x) by

|  |  |  |
| --- | --- | --- |
|  | vinâ€‹(x;Ï‘in)=Râ€‹(x)+ğ’©â€‹ğ’©1â€‹(x,Râ€‹(x);Ï‘in),xâˆˆâ„d,v^{n}\_{i}(x;\vartheta^{n}\_{i})=R(x)+\mathcal{NN}^{1}(x,R(x);\vartheta^{n}\_{i}),\quad x\in\mathbb{R}^{d}, |  |

where ğ’©â€‹ğ’©1â€‹(â‹…,â‹…;Ï‘in):â„dÃ—â„â†’â„\mathcal{NN}^{1}(\cdot,\cdot;\vartheta^{n}\_{i}):\mathbb{R}^{d}\times\mathbb{R}\to\mathbb{R} denotes an FNN of depth 22, width 20+d20+d, and ReLU\mathrm{ReLU} activation, and Ï‘inâˆˆÎ˜\vartheta^{n}\_{i}\in\Theta denotes the parameters of the FNN. In all experiments, the number of policy iterations, epochs and the training batch size is set to nÂ¯=10\overline{n}=10, â„“Â¯=1000\overline{\ell}=1000 and 2102^{10}, respectively. For numerical stability and training efficiency, we apply batch normalization before the input and at each hidden layer, together with Xavier normal initialization and the ADAM optimizer.
To make dependencies explicit, we denote by (viN,Î»,â‹†;Îµ)i=0I(v^{N,\lambda,\star;\varepsilon}\_{i})\_{i=0}^{I}, obtained after sufficient policy iterations, under penalty factor NN, temperature Î»\lambda, and ambiguity degree Îµ\varepsilon.

We conduct experiments on the American put and call holderâ€™s stopping problems to illustrate the policy improvement, convergence, stability, and robustness of Algorithm [1](https://arxiv.org/html/2510.10260v1#alg1 "Algorithm 1 â€£ 4 Policy iteration theorem & RL algorithm â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).").
The simulation settings are as follows: under Setting [4](https://arxiv.org/html/2510.10260v1#S4 "4 Policy iteration theorem & RL algorithm â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175)."), we let the running reward râ€‹(â‹…)â‰¡0r(\cdot)\equiv 0, the discounting factor Î²tâ‰¡râˆ—\beta\_{t}\equiv r\_{\*}, the volatility Ïƒ~oâ€‹(t,xË‡)=0.4â€‹xË‡\widetilde{\sigma}^{o}(t,\check{x})=0.4\check{x}, the initial price and strike price x=Î“=40x=\Gamma=40, and

* (i)

  (Put)
  T=1T=1, I=50I=50,
  the interest rate râˆ—=0.06r\_{\*}=0.06,
  the payoff Râ€‹(x)=(Î“âˆ’x)+R(x)=(\Gamma-x)^{+}, the drift b~oâ€‹(t,x)=râˆ—â€‹x\widetilde{b}^{o}(t,x)=r\_{\*}x;
* (ii)

  (Call)
  T=0.5T=0.5, I=100I=100,
  the dividend rates in the training simulator Î´train=0.05{\delta}\_{\mathrm{train}}=0.05 and in the testing simulator Î´\delta
  âˆˆ{0,0.05,0.1,0.15,0.2,0.25}\in\{0,0.05,0.1,0.15,0.2,0.25\}, the interest rate râˆ—=0.05r\_{\*}=0.05, the payoff Râ€‹(x)=(xâˆ’Î“)+R(x)=(x-\Gamma)^{+}, the drift b~oâ€‹(t,x)=(râˆ—âˆ’Î´)â€‹x\widetilde{b}^{o}(t,x)=(r\_{\*}-\delta)x.

We first examine the policy improvement and convergence of Algorithm [1](https://arxiv.org/html/2510.10260v1#alg1 "Algorithm 1 â€£ 4 Policy iteration theorem & RL algorithm â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175)."). For the put-type stopping problem, we fix Î»=1\lambda=1 and N=10N=10, and consider several ambiguity degrees Îµâˆˆ{0,0.2,0.4}\varepsilon\in\{0,0.2,0.4\}. The reference values RÎµrefR^{\mathrm{ref}}\_{\varepsilon} for Îµâˆˆ{0,0.2,0.4}\varepsilon\in\{0,0.2,0.4\} are obtained by solving the BSDE ([3.8](https://arxiv.org/html/2510.10260v1#S3.E8 "Equation 3.8 â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) for the corresponding optimal value function using the deep backward scheme of HurÃ© et al.Â [[33](https://arxiv.org/html/2510.10260v1#bib.bib33)], yielding R0ref=5.302R^{\mathrm{ref}}\_{0}=5.302, R0.2ref=4.420R^{\mathrm{ref}}\_{0.2}=4.420, R0.4ref=3.725R^{\mathrm{ref}}\_{0.4}=3.725.
The results illustrating the policy improvement and convergence are shown in FigureÂ [1](https://arxiv.org/html/2510.10260v1#S5.F1 "Figure 1 â€£ 5 Experiments â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175)."), which align well with the theoretical findings in TheoremÂ [4.1](https://arxiv.org/html/2510.10260v1#S4.Thmtheorem1 "Theorem 4.1. â€£ 4 Policy iteration theorem & RL algorithm â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).").

Similarly, for the call-type stopping problem, we again fix Î»=1,N=10\lambda=1,N=10 and consider the same several ambiguity degrees. The reference values RÎµrefR^{\mathrm{ref}}\_{\varepsilon} computed by the deep backward scheme are R0ref=4.378R^{\mathrm{ref}}\_{0}=4.378, R0.2ref=3.677R^{\mathrm{ref}}\_{0.2}=3.677, R0.4ref=3.130R^{\mathrm{ref}}\_{0.4}=3.130. The corresponding policy improvement and convergence results are depicted in FigureÂ [1](https://arxiv.org/html/2510.10260v1#S5.F1 "Figure 1 â€£ 5 Experiments â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).").

![Refer to caption](x1.png)

![Refer to caption](x2.png)

Figure 1: Policy improvement and convergence in Algorithm [1](https://arxiv.org/html/2510.10260v1#alg1 "Algorithm 1 â€£ 4 Policy iteration theorem & RL algorithm â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).") under several ambiguity levels.

To examine the stability of AlgorithmÂ [1](https://arxiv.org/html/2510.10260v1#alg1 "Algorithm 1 â€£ 4 Policy iteration theorem & RL algorithm â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175)."), we vary the penalty, temperature and ambiguity levels as Nâˆˆ{5,10,20}N\in\{5,10,20\}, Î»âˆˆ{0.01,1,5}\lambda\in\{0.01,1,5\}, and Îµâˆˆ{0,0.2,0.4}\varepsilon\in\{0,0.2,0.4\}, and present the corresponding values of v0N,Î»,â‹†;Îµv^{N,\lambda,\star;\varepsilon}\_{0} in TableÂ [1](https://arxiv.org/html/2510.10260v1#S5.T1 "Table 1 â€£ 5 Experiments â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).") (obtained after at-least 10 iterations of the policy improvement; see Figure [1](https://arxiv.org/html/2510.10260v1#S5.F1 "Figure 1 â€£ 5 Experiments â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")).
These results align with the stability analysis w.r.t.Â Î»\lambda given in TheoremÂ [3.5](https://arxiv.org/html/2510.10260v1#S3.Thmtheorem5 "Theorem 3.5. â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).") and the sensitivity analysis of robust optimization problems w.r.t.Â ambiguity level examined in [[2](https://arxiv.org/html/2510.10260v1#bib.bib2), Theorem 2.13], [[10](https://arxiv.org/html/2510.10260v1#bib.bib10), Corollary 5.4].

Table 1: Stability analysis of AlgorithmÂ [1](https://arxiv.org/html/2510.10260v1#alg1 "Algorithm 1 â€£ 4 Policy iteration theorem & RL algorithm â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).") w.r.t.Â the penalty, temperature and ambiguity levels.

| Îµ\varepsilon | v0N,Î»,â‹†;Îµâ€‹(40)v^{N,\lambda,\star;\varepsilon}\_{0}(40) | | | | | | | | |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| N=5N=5 | | | N=10N=10 | | | N=20N=20 | | |
| Î»=0.01\lambda=0.01 | Î»=1\lambda=1 | Î»=5\lambda=5 | Î»=0.01\lambda=0.01 | Î»=1\lambda=1 | Î»=5\lambda=5 | Î»=0.01\lambda=0.01 | Î»=1\lambda=1 | Î»=5\lambda=5 |
| 0 | 5.2225.222 | 5.2785.278 | 6.1136.113 | 5.2335.233 | 5.2795.279 | 5.7885.788 | 5.2395.239 | 5.2965.296 | 5.5705.570 |
| 0.20.2 | 4.3114.311 | 4.4134.413 | 5.2585.258 | 4.4124.412 | 4.4574.457 | 4.9584.958 | 4.4254.425 | 4.4964.496 | 4.7654.765 |
| 0.40.4 | 3.5963.596 | 3.6713.671 | 4.4974.497 | 3.7023.702 | 3.7683.768 | 4.2214.221 | 3.7923.792 | 3.8143.814 | 4.1014.101 |



![Refer to caption](x3.png)

![Refer to caption](x4.png)

Figure 2: Robustness performance under unknown testing environments.

Lastly, we examine the robustness of Algorithm [1](https://arxiv.org/html/2510.10260v1#alg1 "Algorithm 1 â€£ 4 Policy iteration theorem & RL algorithm â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).") in the call-type stopping problem. In particular, to assess the out-of-sample performance under an unknown testing environment, we re-simulate new state trajectories (XË‡ix,Î´)i=0I(\check{X}^{x,\delta}\_{i})\_{i=0}^{I} as in Setting [4](https://arxiv.org/html/2510.10260v1#S4 "4 Policy iteration theorem & RL algorithm â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")â€„(i) under different dividend rates Î´âˆˆ{0,0.05,0.1,0.15,0.2,0.25}\delta\in\{0,0.05,0.1,0.15,0.2,0.25\}, where the number of simulated trajectories is set to 2202^{20}.
We fix N=10N=10 and consider configuration Îµâˆˆ{0,0.1,0.2,0.3}\varepsilon\in\{0,0.1,0.2,0.3\} both for Î»=1\lambda=1 and Î»=5\lambda=5. Using the trained value functions (vi10,Î»,â‹†;Îµâ€‹(â‹…))i=0I(v^{10,\lambda,\star;\varepsilon}\_{i}(\cdot))\_{i=0}^{I}, the stopping policy Ï„Î´Îµ,Î»\tau\_{\delta}^{\varepsilon,\lambda} and corresponding
discounted expected reward RË‡Î´Îµ,Î»\check{R}^{\varepsilon,\lambda}\_{\delta} under such unknown environment are definedÂ by

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ï„Î´Îµ,Î»\displaystyle\tau^{\varepsilon,\lambda}\_{\delta} | :=inf{ti:vi10,Î»,â‹†;Îµâ€‹(XË‡ix,Î´)â‰¤Râ€‹(XË‡ix,Î´),i=0,â€¦,I},\displaystyle:=\inf\big\{t\_{i}:v^{10,\lambda,\star;\varepsilon}\_{i}(\check{X}^{x,\delta}\_{i})\leq R(\check{X}^{x,\delta}\_{i}),\;i=0,\ldots,I\big\}, |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | RË‡Î´Îµ,Î»\displaystyle\check{R}^{\varepsilon,\lambda}\_{\delta} | :=ğ”¼â€‹[eâˆ’râˆ—â€‹Ï„Î´Îµ,Î»â€‹Râ€‹(XË‡ix,Î´)].\displaystyle:=\mathbb{E}\big[e^{-r\_{\*}\tau^{\varepsilon,\lambda}\_{\delta}}R(\check{X}^{x,\delta}\_{i})\big]. |  |

For each Î´\delta, the corresponding American call option price represents the optimal value for the call-type stopping problem, which can be computed using the implicit finite-difference method of Forsyth and VetzalÂ [[24](https://arxiv.org/html/2510.10260v1#bib.bib24)]. We therefore use the option prices computed by this method as reference values RÎ´refR^{\mathrm{ref}}\_{\delta} for each Î´\delta, yielding R0ref=4.954,R^{\mathrm{ref}}\_{0}=4.954, R0.05ref=4.410R^{\mathrm{ref}}\_{0.05}=4.410, R0.1ref=3.990R^{\mathrm{ref}}\_{0.1}=3.990, R0.15ref=3.634R^{\mathrm{ref}}\_{0.15}=3.634, R2ref=3.324R^{\mathrm{ref}}\_{2}=3.324, R0.25ref=3.052R^{\mathrm{ref}}\_{0.25}=3.052. The relative errors are then computed as |RË‡Î´Îµ,Î»âˆ’RÎ´ref|/RÎ´ref{|\check{R}^{\varepsilon,\lambda}\_{\delta}-R^{\mathrm{ref}}\_{\delta}|}/{R^{\mathrm{ref}}\_{\delta}}.

In Figure [2](https://arxiv.org/html/2510.10260v1#S5.F2 "Figure 2 â€£ 5 Experiments â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175)."), when the dividend rate in the testing environment does not deviate significantly from that of the trained environment (near Î´=0.05\delta=0.05), the non-robust value function (i.e., with Îµ=0\varepsilon=0) performs comparably well. However, as the discrepancy between the training and testing environments increases, the benefit of incorporating ambiguity into the framework becomes evident, as reflected by lower relative errors for higher ambiguity levels (e.g., Îµ=0.2,0.3\varepsilon=0.2,0.3).

## 6 Proofs

### 6.1 Proof of results in Section [2](https://arxiv.org/html/2510.10260v1#S2 "2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")

###### Proof 6.1 (Proof of Proposition [2.8](https://arxiv.org/html/2510.10260v1#S2.Thmtheorem8 "Proposition 2.8. â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")).

Step 1. Fix tâˆˆ[0,T]t\in[0,T] and let Ï„âˆˆTt\tau\in{\mathcal{}T}\_{t}. An application of ItÃ´â€™s formula into (eâˆ’âˆ«tsÎ²uâ€‹ğ‘‘uâ€‹Ysx)sâˆˆ[t,T](e^{-\int\_{t}^{s}\beta\_{u}du}Y\_{s}^{x})\_{s\in[t,T]} ensures that

|  |  |  |  |
| --- | --- | --- | --- |
| (6.1) |  | Ytx=eâˆ’âˆ«tÏ„Î²uâ€‹ğ‘‘uâ€‹YÏ„x+âˆ«tÏ„eâˆ’âˆ«tsÎ²uâ€‹ğ‘‘uâ€‹(râ€‹(Xsx)+gâ€‹(s,Zsx))â€‹ğ‘‘sâˆ’âˆ«tÏ„eâˆ’âˆ«tsÎ²uâ€‹ğ‘‘uâ€‹Zsxâ€‹ğ‘‘Bs+âˆ«tÏ„eâˆ’âˆ«tsÎ²uâ€‹ğ‘‘uâ€‹ğ‘‘Ksx.\displaystyle\begin{aligned} Y\_{t}^{x}=&e^{-\int\_{t}^{\tau}\beta\_{u}du}Y\_{\tau}^{x}+\int\_{t}^{\tau}e^{-\int\_{t}^{s}\beta\_{u}du}\big(r(X\_{s}^{x})+g(s,Z\_{s}^{x})\big)ds\\ &-\int\_{t}^{\tau}e^{-\int\_{t}^{s}\beta\_{u}du}Z\_{s}^{x}dB\_{s}+\int\_{t}^{\tau}e^{-\int\_{t}^{s}\beta\_{u}du}dK\_{s}^{x}.\end{aligned} |  |

Since Itx;Ï„âˆˆL2â€‹(FÏ„;â„)\operatorname{I}\_{t}^{x;\tau}\in L^{2}({\mathcal{}F}\_{\tau};\mathbb{R}) (see Remark [2.5](https://arxiv.org/html/2510.10260v1#S2.Thmtheorem5 "Remark 2.5. â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")), dâ€‹Ksxâ‰¥0dK\_{s}^{x}\geq 0 for all sâ‰¥[t,Ï„]s\geq[t,\tau] (as KxK^{x} is nondecreasing) and YÏ„xâ‰¥Râ€‹(XÏ„x)Y\_{\tau}^{x}\geq R(X\_{\tau}^{x}) â„™\mathbb{P}-a.s. (see Definition [2.6](https://arxiv.org/html/2510.10260v1#S2.Thmtheorem6 "Definition 2.6. â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")), it holds that â„™\mathbb{P}-a.s.

|  |  |  |  |
| --- | --- | --- | --- |
|  | Etgâ€‹[Itx;Ï„]\displaystyle{\mathcal{}E}\_{t}^{g}[\operatorname{I}\_{t}^{x;\tau}] | â‰¤Etgâ€‹[Ytxâˆ’âˆ«tÏ„eâˆ’âˆ«tsÎ²uâ€‹ğ‘‘uâ€‹gâ€‹(s,Zsx)â€‹ğ‘‘s+âˆ«tÏ„eâˆ’âˆ«tsÎ²uâ€‹ğ‘‘uâ€‹Zsxâ€‹ğ‘‘Bs]\displaystyle\leq{\mathcal{}E}\_{t}^{g}\bigg[Y\_{t}^{x}-\int\_{t}^{\tau}e^{-\int\_{t}^{s}\beta\_{u}du}g(s,Z\_{s}^{x})ds+\int\_{t}^{\tau}e^{-\int\_{t}^{s}\beta\_{u}du}Z\_{s}^{x}dB\_{s}\bigg] |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
| (6.2) |  |  | =Ytx+Etg[âˆ’âˆ«tÏ„eâˆ’âˆ«tsÎ²uâ€‹ğ‘‘ug(s,Zsx)ds+âˆ«tÏ„eâˆ’âˆ«tsÎ²uâ€‹ğ‘‘uZsxdBs]=:Ytx+IIt,\displaystyle=Y\_{t}^{x}+{\mathcal{}E}^{g}\_{t}\bigg[-\int\_{t}^{\tau}e^{-\int\_{t}^{s}\beta\_{u}du}g(s,Z\_{s}^{x})ds+\int\_{t}^{\tau}e^{-\int\_{t}^{s}\beta\_{u}du}Z\_{s}^{x}dB\_{s}\bigg]=:Y\_{t}^{x}+\operatorname{II}\_{t}, |  |

where the equality holds by the property of Etgâ€‹[â‹…]{\mathcal{}E}\_{t}^{g}[\cdot] given in [[12](https://arxiv.org/html/2510.10260v1#bib.bib12), Lemma 2.1].

Since it holds that âˆ’gâ€‹(s,Zsx)â‰¤|gâ€‹(s,Zsx)|â‰¤Îºâ€‹|Zsx|-g(s,Z\_{s}^{x})\leq|g(s,Z\_{s}^{x})|\leq\kappa|Z\_{s}^{x}| for all sâˆˆ[t,Ï„]s\in[t,\tau] (see DefinitionÂ [2.1](https://arxiv.org/html/2510.10260v1#S2.Thmtheorem1 "Definition 2.1. â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")â€„(ii),â€„(iii)), by the monotonicity of Etgâ€‹[â‹…]{\mathcal{}E}\_{t}^{g}[\cdot] (see [[12](https://arxiv.org/html/2510.10260v1#bib.bib12), Proposition 2.2â€„(iii)]),

|  |  |  |  |
| --- | --- | --- | --- |
| (6.3) |  | IItâ‰¤Etg[Îºâˆ«tÏ„eâˆ’âˆ«tsÎ²uâ€‹ğ‘‘u|Zsx|ds+âˆ«tÏ„eâˆ’âˆ«tsÎ²uâ€‹ğ‘‘uZsxdBs]=:IIIt.\displaystyle\operatorname{II}\_{t}\leq{\mathcal{}E}\_{t}^{g}\bigg[\kappa\int\_{t}^{\tau}e^{-\int\_{t}^{s}\beta\_{u}du}|Z\_{s}^{x}|ds+\int\_{t}^{\tau}e^{-\int\_{t}^{s}\beta\_{u}du}Z\_{s}^{x}dB\_{s}\bigg]=:\operatorname{III}\_{t}. |  |

We note that Eg:L2â€‹(FT;â„)â†’â„{\mathcal{}E}^{g}:L^{2}({\mathcal{}F}\_{T};\mathbb{R})\to\mathbb{R} given in Definition [2.1](https://arxiv.org/html/2510.10260v1#S2.Thmtheorem1 "Definition 2.1. â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).") is an F{\mathcal{}F}-expectation555A nonlinear expectation E:L2â€‹(FT;â„)â†’â„{\mathcal{}E}:L^{2}({\mathcal{}F}\_{T};\mathbb{R})\to\mathbb{R} is called F{\mathcal{}F}-expectation if for each Î¾âˆˆL2â€‹(FT;â„)\xi\in L^{2}({\mathcal{}F}\_{T};\mathbb{R}) and tâˆˆ[0,T]t\in[0,T] there exists a random variable Î·âˆˆL2â€‹(Ft;â„)\eta\in L^{2}({\mathcal{}F}\_{t};\mathbb{R}) such that Eâ€‹[Î¾â€‹ğŸA]=Eâ€‹[Î·â€‹ğŸA]{\mathcal{}E}[\xi{\bf 1}\_{A}]={\mathcal{}E}[\eta{\bf 1}\_{A}] for allÂ AâˆˆFtA\in{\mathcal{}F}\_{t}. Moreover, given Î¼>0\mu>0, we say that an F{\mathcal{}F}-expectation E{\mathcal{}E} is dominated by EÎ¼{\mathcal{}E}^{\mu} if for all Î¾,Î·âˆˆL2â€‹(FT;â„)\xi,\eta\in L^{2}({\mathcal{}F}\_{T};\mathbb{R}) Eâ€‹(Î¾+Î·)âˆ’Eâ€‹(Î¾)â‰¤EÎ¼â€‹[Î·];{\mathcal{}E}(\xi+\eta)-{\mathcal{}E}(\xi)\leq{\mathcal{}E}^{\mu}[\eta]; see [[12](https://arxiv.org/html/2510.10260v1#bib.bib12), Definitions 3.2 and 4.1].. Moreover, by [[12](https://arxiv.org/html/2510.10260v1#bib.bib12), Remark 4.1] it is dominated by a gg-expectation EÎº:L2â€‹(FT;â„)â†’â„{\mathcal{}E}^{\kappa}:L^{2}({\mathcal{}F}\_{T};\mathbb{R})\to\mathbb{R} which is defined by setting that gâ€‹(Ï‰,t,z):=Îºâ€‹|z|g(\omega,t,z):=\kappa|z| for all (Ï‰,t,z)âˆˆÎ©Ã—[0,T]Ã—â„d(\omega,t,z)\in\Omega\times[0,T]\times\mathbb{R}^{d}, where the constant Îº>0\kappa>0 appears in Definition [2.1](https://arxiv.org/html/2510.10260v1#S2.Thmtheorem1 "Definition 2.1. â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")â€„(ii).

Hence, an application of [[12](https://arxiv.org/html/2510.10260v1#bib.bib12), Lemma 4.4] ensures that

|  |  |  |  |
| --- | --- | --- | --- |
| (6.4) |  | IIItâ‰¤EtÎºâ€‹[Îºâ€‹âˆ«tÏ„eâˆ’âˆ«tsÎ²uâ€‹ğ‘‘uâ€‹|Zsx|â€‹ğ‘‘s+âˆ«tÏ„eâˆ’âˆ«tsÎ²uâ€‹ğ‘‘uâ€‹Zsxâ€‹ğ‘‘Bs]=0,\displaystyle\operatorname{III}\_{t}\leq{\mathcal{}E}\_{t}^{\kappa}\bigg[\kappa\int\_{t}^{\tau}e^{-\int\_{t}^{s}\beta\_{u}du}|Z\_{s}^{x}|ds+\int\_{t}^{\tau}e^{-\int\_{t}^{s}\beta\_{u}du}Z\_{s}^{x}dB\_{s}\bigg]=0, |  |

where the equality holds because (eâˆ’âˆ«tsÎ²uâ€‹ğ‘‘uâ€‹Zsx)sâˆˆ[t,T](e^{-\int\_{t}^{s}\beta\_{u}du}Z\_{s}^{x})\_{s\in[t,T]} is ğ”½\mathbb{F}-predictable and satisfies ğ”¼â€‹[âˆ«tT|eâˆ’âˆ«tsÎ²uâ€‹ğ‘‘uâ€‹Zsx|2â€‹ğ‘‘s]<âˆ\mathbb{E}[\int\_{t}^{T}|e^{-\int\_{t}^{s}\beta\_{u}du}Z\_{s}^{x}|^{2}ds]<\infty (noting that Zxâˆˆğ•ƒ2â€‹(â„d)Z^{x}\in\mathbb{L}^{2}(\mathbb{R}^{d}) and Î²tâ‰¥0\beta\_{t}\geq 0 for all tâˆˆ[0,T]t\in[0,T]; see Definition [2.6](https://arxiv.org/html/2510.10260v1#S2.Thmtheorem6 "Definition 2.6. â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).") and Assumption [2](https://arxiv.org/html/2510.10260v1#S2 "2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")â€„(ii)), hence the integrand given in ([6.4](https://arxiv.org/html/2510.10260v1#S6.E4 "Equation 6.4 â€£ Proof 6.1 (Proof of Proposition 2.8). â€£ 6.1 Proof of results in Section 2 â€£ 6 Proofs â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) is EÎº{\mathcal{}E}^{\kappa}-martingale and the corresponding gg-expectation equals zero; see [[12](https://arxiv.org/html/2510.10260v1#bib.bib12), Lemmaâ€„5.5].

Combining ([6.2](https://arxiv.org/html/2510.10260v1#S6.E2 "Equation 6.2 â€£ Proof 6.1 (Proof of Proposition 2.8). â€£ 6.1 Proof of results in Section 2 â€£ 6 Proofs â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")), ([6.3](https://arxiv.org/html/2510.10260v1#S6.E3 "Equation 6.3 â€£ Proof 6.1 (Proof of Proposition 2.8). â€£ 6.1 Proof of results in Section 2 â€£ 6 Proofs â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) and ([6.4](https://arxiv.org/html/2510.10260v1#S6.E4 "Equation 6.4 â€£ Proof 6.1 (Proof of Proposition 2.8). â€£ 6.1 Proof of results in Section 2 â€£ 6 Proofs â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")), we obtain that Etgâ€‹[Itx;Ï„]â‰¤Ytx{\mathcal{}E}\_{t}^{g}[\operatorname{I}\_{t}^{x;\tau}]\leq Y\_{t}^{x} â„™\mathbb{P}-a.s.. Since Ï„âˆˆTt\tau\in{\mathcal{}T}\_{t} is chosen some arbitrary, we have Vtx=essâ€‹supÏ„âˆˆTtâ¡Etgâ€‹[Itx;Ï„]â‰¤Ytx.V\_{t}^{x}=\operatorname\*{ess\,sup}\_{\tau\in{\mathcal{}T}\_{t}}{\mathcal{}E}\_{t}^{g}[\operatorname{I}\_{t}^{x;\tau}]\leq Y\_{t}^{x}.

Step 2. We now claim that Ytxâ‰¤VtxY\_{t}^{x}\leq V\_{t}^{x}. Let Ï„tâˆ—,xâˆˆTt\tau\_{t}^{\*,x}\in{\mathcal{}T}\_{t} be defined as in ([2.5](https://arxiv.org/html/2510.10260v1#S2.E5 "Equation 2.5 â€£ Proposition 2.8. â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")). Since âˆ«0Ï„tâˆ—,x(Ysâˆ’xâˆ’Râ€‹(Xsâˆ’x))â€‹ğ‘‘Ksx=0\int\_{0}^{\tau\_{t}^{\*,x}}(Y\_{s-}^{x}-R(X\_{s-}^{x}))dK\_{s}^{x}=0 â„™\mathbb{P}-a.s. (see Definition [2.6](https://arxiv.org/html/2510.10260v1#S2.Thmtheorem6 "Definition 2.6. â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")â€„(iv)) and Ysâˆ’x>Râ€‹(Xsâˆ’x)Y\_{s-}^{x}>R(X\_{s-}^{x}) for all sâˆˆ(0,Ï„tâˆ—,x)s\in(0,\tau\_{t}^{\*,x}) (by definition of Ï„tâˆ—,x\tau\_{t}^{\*,x}), it holds that

|  |  |  |  |
| --- | --- | --- | --- |
| (6.5) |  | dâ€‹Ksx=0â„™-a.s., for allÂ sâˆˆ(0,Ï„tâˆ—,x).\displaystyle dK\_{s}^{x}=0\quad\mbox{$\mathbb{P}$-a.s., for all $s\in(0,\tau\_{t}^{\*,x})$}. |  |

Applying ItÃ´â€™s formula as given in ([6.1](https://arxiv.org/html/2510.10260v1#S6.E1 "Equation 6.1 â€£ Proof 6.1 (Proof of Proposition 2.8). â€£ 6.1 Proof of results in Section 2 â€£ 6 Proofs â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) and using ([6.5](https://arxiv.org/html/2510.10260v1#S6.E5 "Equation 6.5 â€£ Proof 6.1 (Proof of Proposition 2.8). â€£ 6.1 Proof of results in Section 2 â€£ 6 Proofs â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")), we obtain that â„™\mathbb{P}-a.s.

|  |  |  |  |
| --- | --- | --- | --- |
| (6.6) |  | Ytx=eâˆ’âˆ«tÏ„tâˆ—,xÎ²uâ€‹ğ‘‘uâ€‹YÏ„tâˆ—,xx+âˆ«tÏ„tâˆ—,xeâˆ’âˆ«tsÎ²uâ€‹ğ‘‘uâ€‹(râ€‹(Xsx)+gâ€‹(s,Zsx))â€‹ğ‘‘sâˆ’âˆ«tÏ„tâˆ—,xeâˆ’âˆ«tsÎ²uâ€‹ğ‘‘uâ€‹Zsxâ€‹ğ‘‘Bs.\displaystyle\begin{aligned} Y\_{t}^{x}=&e^{-\int\_{t}^{\tau\_{t}^{\*,x}}\beta\_{u}du}Y\_{\tau\_{t}^{\*,x}}^{x}+\int\_{t}^{\tau\_{t}^{\*,x}}e^{-\int\_{t}^{s}\beta\_{u}du}\Big(r(X\_{s}^{x})+g(s,Z\_{s}^{x})\Big)ds\\ &-\int\_{t}^{\tau\_{t}^{\*,x}}e^{-\int\_{t}^{s}\beta\_{u}du}Z\_{s}^{x}dB\_{s}.\end{aligned} |  |

By putting âˆ«tÏ„tâˆ—,xeâˆ’âˆ«tsÎ²uâ€‹ğ‘‘uâ€‹gâ€‹(s,Zsx)â€‹ğ‘‘sâˆ’âˆ«tÏ„tâˆ—,x(eâˆ’âˆ«tsÎ²uâ€‹ğ‘‘uâ€‹Zsx)âŠ¤â€‹ğ‘‘Bs\int\_{t}^{\tau\_{t}^{\*,x}}e^{-\int\_{t}^{s}\beta\_{u}du}g(s,Z\_{s}^{x})ds-\int\_{t}^{\tau\_{t}^{\*,x}}(e^{-\int\_{t}^{s}\beta\_{u}du}Z\_{s}^{x})^{\top}dB\_{s} into the left-hand side of ([6.6](https://arxiv.org/html/2510.10260v1#S6.E6 "Equation 6.6 â€£ Proof 6.1 (Proof of Proposition 2.8). â€£ 6.1 Proof of results in Section 2 â€£ 6 Proofs â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) and taking the conditional gg-expectation Etgâ€‹[â‹…]{\mathcal{}E}\_{t}^{g}[\cdot], â„™\mathbb{P}-a.s.,

|  |  |  |  |
| --- | --- | --- | --- |
| (6.7) |  | IIItx:=Etgâ€‹[âˆ«tÏ„tâˆ—,xeâˆ’âˆ«tsÎ²uâ€‹ğ‘‘uâ€‹râ€‹(Xsx)â€‹ğ‘‘s+eâˆ’âˆ«tÏ„tâˆ—,xÎ²uâ€‹ğ‘‘uâ€‹YÏ„âˆ—x]=Ytx+Etgâ€‹[âˆ’âˆ«tÏ„tâˆ—,xeâˆ’âˆ«tsÎ²uâ€‹ğ‘‘uâ€‹gâ€‹(s,Zsx)â€‹ğ‘‘s+âˆ«tÏ„tâˆ—,xeâˆ’âˆ«tsÎ²uâ€‹ğ‘‘uâ€‹Zsxâ€‹ğ‘‘Bs]=:Ytx+IVtx,\displaystyle\begin{aligned} \operatorname{III}\_{t}^{x}&:={\mathcal{}E}\_{t}^{g}\bigg[\int\_{t}^{\tau\_{t}^{\*,x}}e^{-\int\_{t}^{s}\beta\_{u}du}r(X\_{s}^{x})ds+e^{-\int\_{t}^{\tau\_{t}^{\*,x}}\beta\_{u}du}Y\_{\tau^{\*}}^{x}\bigg]\\ &\;=Y\_{t}^{x}+{\mathcal{}E}\_{t}^{g}\bigg[-\int\_{t}^{\tau\_{t}^{\*,x}}e^{-\int\_{t}^{s}\beta\_{u}du}g(s,Z\_{s}^{x})ds+\int\_{t}^{\tau\_{t}^{\*,x}}e^{-\int\_{t}^{s}\beta\_{u}du}Z\_{s}^{x}dB\_{s}\bigg]\\ &\;=:Y\_{t}^{x}+\operatorname{IV}\_{t}^{x},\end{aligned} |  |

where we have used the property of Etgâ€‹[â‹…]{\mathcal{}E}\_{t}^{g}[\cdot] given in [[12](https://arxiv.org/html/2510.10260v1#bib.bib12), Lemma 2.1].

Since YÏ„tâˆ—,xxâ‰¤Râ€‹(XÏ„tâˆ—,xx)Y\_{\tau\_{t}^{\*,x}}^{x}\leq R(X\_{\tau\_{t}^{\*,x}}^{x}) on {Ï„tâˆ—,x<T}\{\tau\_{t}^{\*,x}<T\}; YÏ„tâˆ—,xx=Râ€‹(XÏ„tâˆ—,xx)Y\_{\tau\_{t}^{\*,x}}^{x}=R(X\_{\tau\_{t}^{\*,x}}^{x}) on {Ï„tâˆ—,x=T}\{\tau\_{t}^{\*,x}=T\}, we have

|  |  |  |  |
| --- | --- | --- | --- |
| (6.8) |  | IIItxâ‰¤Etgâ€‹[âˆ«tÏ„tâˆ—,xeâˆ’âˆ«tsÎ²uâ€‹ğ‘‘uâ€‹râ€‹(Xsx)â€‹ğ‘‘s+eâˆ’âˆ«tÏ„tâˆ—,xÎ²uâ€‹ğ‘‘uâ€‹Râ€‹(XÏ„tâˆ—,xx)]=Etgâ€‹[Itx;Ï„tâˆ—,x],\displaystyle\begin{aligned} \operatorname{III}\_{t}^{x}&\leq{\mathcal{}E}\_{t}^{g}\bigg[\int\_{t}^{\tau\_{t}^{\*,x}}e^{-\int\_{t}^{s}\beta\_{u}du}r(X\_{s}^{x})ds+e^{-\int\_{t}^{\tau\_{t}^{\*,x}}\beta\_{u}du}R(X\_{\tau\_{t}^{\*,x}}^{x})\bigg]={\mathcal{}E}\_{t}^{g}[\operatorname{I}\_{t}^{x;\tau\_{t}^{\*,x}}],\end{aligned} |  |

where Itx;Ï„tâˆ—,xâˆˆL2â€‹(FÏ„âˆ—;â„)\operatorname{I}\_{t}^{x;\tau\_{t}^{\*,x}}\in L^{2}({\mathcal{}F}\_{\tau^{\*}};\mathbb{R}) is given in ([2.2](https://arxiv.org/html/2510.10260v1#S2.E2 "Equation 2.2 â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) (under the setting Ï„=Ï„tâˆ—,x\tau=\tau\_{t}^{\*,x}) and the last inequality follows from the positiveness of (Î²u)uâˆˆ[0,T](\beta\_{u})\_{u\in[0,T]}.

Let Eâˆ’Îº:L2â€‹(FT;â„)â†’â„{\mathcal{}E}^{-\kappa}:L^{2}({\mathcal{}F}\_{T};\mathbb{R})\to\mathbb{R} be a gg-expectation defined by setting gâ€‹(Ï‰,t,z):=âˆ’Îºâ€‹|z|g(\omega,t,z):=-\kappa|z| for all (Ï‰,t,z)âˆˆÎ©Ã—[0,T]Ã—â„d(\omega,t,z)\in\Omega\times[0,T]\times\mathbb{R}^{d}. Then since it holds that âˆ’gâ€‹(s,Zsx)â‰¥âˆ’|gâ€‹(s,Zsx)|â‰¥âˆ’Îºâ€‹|Zsx|-g(s,Z\_{s}^{x})\geq-|g(s,Z\_{s}^{x})|\geq-\kappa|Z\_{s}^{x}| for all sâˆˆ[t,Ï„tâˆ—,x]s\in[t,\tau\_{t}^{\*,x}] (see DefinitionÂ [2.1](https://arxiv.org/html/2510.10260v1#S2.Thmtheorem1 "Definition 2.1. â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")â€„(ii),â€„(iii)),

|  |  |  |  |
| --- | --- | --- | --- |
| (6.9) |  | IVtxâ‰¥Etgâ€‹[âˆ’Îºâ€‹âˆ«tÏ„tâˆ—,xeâˆ’âˆ«tsÎ²uâ€‹ğ‘‘uâ€‹|Zsx|â€‹ğ‘‘s+âˆ«tÏ„tâˆ—,xeâˆ’âˆ«tsÎ²uâ€‹ğ‘‘uâ€‹Zsxâ€‹ğ‘‘Bs]â‰¥Etâˆ’Îºâ€‹[âˆ’Îºâ€‹âˆ«tÏ„tâˆ—,xeâˆ’âˆ«tsÎ²uâ€‹ğ‘‘uâ€‹|Zsx|â€‹ğ‘‘s+âˆ«tÏ„tâˆ—,xeâˆ’âˆ«tsÎ²uâ€‹ğ‘‘uâ€‹Zsxâ€‹ğ‘‘Bs]=0,\displaystyle\begin{aligned} \operatorname{IV}\_{t}^{x}&\geq{\mathcal{}E}\_{t}^{g}\bigg[-\kappa\int\_{t}^{\tau\_{t}^{\*,x}}e^{-\int\_{t}^{s}\beta\_{u}du}|Z\_{s}^{x}|ds+\int\_{t}^{\tau\_{t}^{\*,x}}e^{-\int\_{t}^{s}\beta\_{u}du}Z\_{s}^{x}dB\_{s}\bigg]\\ &\geq{\mathcal{}E}\_{t}^{-\kappa}\bigg[-\kappa\int\_{t}^{\tau\_{t}^{\*,x}}e^{-\int\_{t}^{s}\beta\_{u}du}|Z\_{s}^{x}|ds+\int\_{t}^{\tau\_{t}^{\*,x}}e^{-\int\_{t}^{s}\beta\_{u}du}Z\_{s}^{x}dB\_{s}\bigg]=0,\end{aligned} |  |

where the first inequality follows from the monotonicity of Etgâ€‹[â‹…]{\mathcal{}E}\_{t}^{g}[\cdot] (see [[12](https://arxiv.org/html/2510.10260v1#bib.bib12), Propositionâ€„2.2â€„(iii)]), the second inequality follows from [[12](https://arxiv.org/html/2510.10260v1#bib.bib12), Lemma 4.4], and the last equality follows from the same arguments presented for the equality given in ([6.4](https://arxiv.org/html/2510.10260v1#S6.E4 "Equation 6.4 â€£ Proof 6.1 (Proof of Proposition 2.8). â€£ 6.1 Proof of results in Section 2 â€£ 6 Proofs â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")).

Combining ([6.7](https://arxiv.org/html/2510.10260v1#S6.E7 "Equation 6.7 â€£ Proof 6.1 (Proof of Proposition 2.8). â€£ 6.1 Proof of results in Section 2 â€£ 6 Proofs â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175)."))â€“([6.9](https://arxiv.org/html/2510.10260v1#S6.E9 "Equation 6.9 â€£ Proof 6.1 (Proof of Proposition 2.8). â€£ 6.1 Proof of results in Section 2 â€£ 6 Proofs â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")), we obtain that Ytxâ‰¤Etgâ€‹[Itx;Ï„tâˆ—,x]Y\_{t}^{x}\leq{\mathcal{}E}\_{t}^{g}[\operatorname{I}\_{t}^{x;\tau\_{t}^{\*,x}}], â„™\mathbb{P}-a.s.. As Ï„tâˆ—,x=inf{sâ‰¥t|Ysxâ‰¤Râ€‹(Xsx)}âˆ§TâˆˆTt\tau\_{t}^{\*,x}=\inf\{s\geq t\,|\,Y\_{s}^{x}\leq R(X\_{s}^{x})\}\wedge T\in{\mathcal{}T}\_{t}, we have Ytxâ‰¤Vtx=essâ€‹supÏ„âˆˆTtâ¡Etgâ€‹[Itx;Ï„],Y\_{t}^{x}\leq V\_{t}^{x}=\operatorname\*{ess\,sup}\_{\tau\in{\mathcal{}T}\_{t}}{\mathcal{}E}\_{t}^{g}[\operatorname{I}\_{t}^{x;\tau}], â„™\mathbb{P}-a.s.,
as claimed. Therefore, Ï„tâˆ—,x\tau\_{t}^{\*,x} given in ([2.5](https://arxiv.org/html/2510.10260v1#S2.E5 "Equation 2.5 â€£ Proposition 2.8. â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) is optimal to ([2.2](https://arxiv.org/html/2510.10260v1#S2.E2 "Equation 2.2 â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")). This completes theÂ proof.

###### Proof 6.2 (Proof of Proposition [2.10](https://arxiv.org/html/2510.10260v1#S2.Thmtheorem10 "Proposition 2.10. â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")).

Step 1. Let Nâˆˆâ„•N\in\mathbb{N} and Î±âˆˆA\alpha\in{\mathcal{}A} be given.
Recalling FxF^{x} given inÂ ([2.3](https://arxiv.org/html/2510.10260v1#S2.E3 "Equation 2.3 â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")), we denote for every (Ï‰,t,y,z)âˆˆÎ©Ã—[0,T]Ã—â„Ã—â„ğ••(\omega,t,y,z)\in\Omega\times[0,T]\times\mathbb{R}\times\mathbb{R^{d}} by

|  |  |  |  |
| --- | --- | --- | --- |
| (6.10) |  | F~tx;N,Î±â€‹(Ï‰,y,z):=Ftxâ€‹(Ï‰,y,z)+Nâ€‹Î±tâ€‹(Ï‰)â€‹(Râ€‹(Xtxâ€‹(Ï‰))âˆ’y).\displaystyle\widetilde{F}\_{t}^{x;N,\alpha}(\omega,y,z):=F\_{t}^{x}(\omega,y,z)+N\alpha\_{t}(\omega)\,\big(R(X\_{t}^{x}(\omega))-y\big). |  |

Then consider the following controlled BSDE: for tâˆˆ[0,T]t\in[0,T]

|  |  |  |  |
| --- | --- | --- | --- |
| (6.11) |  | Y~tx;N,Î±=Râ€‹(XTx)+âˆ«tTF~sx;N,Î±â€‹(Y~sx;N,Î±,Z~sx;N,Î±)â€‹ğ‘‘sâˆ’âˆ«tTZ~sx;N,Î±â€‹ğ‘‘Bs.\displaystyle\widetilde{Y}\_{t}^{x;N,\alpha}=R(X\_{T}^{x})+\int\_{t}^{T}\widetilde{F}^{x;N,\alpha}\_{s}\big(\widetilde{Y}\_{s}^{x;N,\alpha},\widetilde{Z}\_{s}^{x;N,\alpha}\big)ds-\int\_{t}^{T}\widetilde{Z}\_{s}^{x;N,\alpha}dB\_{s}. |  |

Since Î±\alpha is uniformly bounded (noting that it has values only in {0,1}\{0,1\}), one can deduce that the parameters of the BSDE ([6.11](https://arxiv.org/html/2510.10260v1#S6.E11 "Equation 6.11 â€£ Proof 6.2 (Proof of Proposition 2.10). â€£ 6.1 Proof of results in Section 2 â€£ 6 Proofs â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) satisfies all the conditions given in [[49](https://arxiv.org/html/2510.10260v1#bib.bib49), Section 3]. Hence,
there exists a unique solution (Y~tx;N,Î±,Z~tx;N,Î±)tâˆˆ[0,T]âˆˆğ•Š2â€‹(â„)Ã—ğ•ƒ2â€‹(â„d)(\widetilde{Y}\_{t}^{x;N,\alpha},\widetilde{Z}\_{t}^{x;N,\alpha})\_{t\in[0,T]}\in\mathbb{S}^{2}(\mathbb{R})\times\mathbb{L}^{2}(\mathbb{R}^{d}) to the controlled BSDE ([6.11](https://arxiv.org/html/2510.10260v1#S6.E11 "Equation 6.11 â€£ Proof 6.2 (Proof of Proposition 2.10). â€£ 6.1 Proof of results in Section 2 â€£ 6 Proofs â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")).

We now claim that Y~tx;N,Î±=Etgâ€‹[Itx;N,Î±]\widetilde{Y}\_{t}^{x;N,\alpha}={\mathcal{}E}\_{t}^{g}[\operatorname{I}\_{t}^{x;N,\alpha}] for all tâˆˆ[0,T]t\in[0,T]. Indeed, applying ItÃ´â€™s formula into (eâˆ’âˆ«ts(Î²u+Nâ€‹Î±u)â€‹ğ‘‘uâ€‹Y~sx;N,Î±)sâˆˆ[t,T](e^{-\int\_{t}^{s}(\beta\_{u}+N\alpha\_{u})du}\widetilde{Y}\_{s}^{x;N,\alpha})\_{s\in[t,T]} and then taking Etgâ€‹[â‹…]{\mathcal{}E}\_{t}^{g}[\cdot] yield,

|  |  |  |
| --- | --- | --- |
|  | Etgâ€‹[Itx;N,Î±]âˆ’Y~tx;N,Î±\displaystyle{\mathcal{}E}\_{t}^{g}[\operatorname{I}\_{t}^{x;N,\alpha}]-\widetilde{Y}\_{t}^{x;N,\alpha} |  |
|  |  |  |
| --- | --- | --- |
|  | =Etgâ€‹[âˆ’âˆ«tTeâˆ’âˆ«ts(Î²u+Nâ€‹Î±u)â€‹ğ‘‘uâ€‹gâ€‹(s,Z~sx;N,Î±)â€‹ğ‘‘s+âˆ«tTeâˆ’âˆ«ts(Î²u+Nâ€‹Î±u)â€‹ğ‘‘uâ€‹Z~sx;N,Î±â€‹ğ‘‘Bs],\displaystyle\quad={\mathcal{}E}\_{t}^{g}\bigg[-\int\_{t}^{T}e^{-\int\_{t}^{s}(\beta\_{u}+N\alpha\_{u})du}g(s,\widetilde{Z}\_{s}^{x;N,\alpha})ds+\int\_{t}^{T}e^{-\int\_{t}^{s}(\beta\_{u}+N\alpha\_{u})du}\widetilde{Z}\_{s}^{x;N,\alpha}dB\_{s}\bigg], |  |

where we have used the property of Etgâ€‹[â‹…]{\mathcal{}E}\_{t}^{g}[\cdot] given in [[12](https://arxiv.org/html/2510.10260v1#bib.bib12), Lemma 2.1].

Moreover, by using the same arguments presented for the Eg{\mathcal{}E}^{g}-supermartingale property in ([6.2](https://arxiv.org/html/2510.10260v1#S6.E2 "Equation 6.2 â€£ Proof 6.1 (Proof of Proposition 2.8). â€£ 6.1 Proof of results in Section 2 â€£ 6 Proofs â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175)."))â€“([6.4](https://arxiv.org/html/2510.10260v1#S6.E4 "Equation 6.4 â€£ Proof 6.1 (Proof of Proposition 2.8). â€£ 6.1 Proof of results in Section 2 â€£ 6 Proofs â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) and the Eg{\mathcal{}E}^{g}-submartingale property in ([6.7](https://arxiv.org/html/2510.10260v1#S6.E7 "Equation 6.7 â€£ Proof 6.1 (Proof of Proposition 2.8). â€£ 6.1 Proof of results in Section 2 â€£ 6 Proofs â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) and ([6.9](https://arxiv.org/html/2510.10260v1#S6.E9 "Equation 6.9 â€£ Proof 6.1 (Proof of Proposition 2.8). â€£ 6.1 Proof of results in Section 2 â€£ 6 Proofs â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) (see the proof of Proposition [2.8](https://arxiv.org/html/2510.10260v1#S2.Thmtheorem8 "Proposition 2.8. â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) we can deduce that the conditional gg-expectation appearing in the right-hand side of the above equals zero (i.e., the integrand therein is an Eg{\mathcal{}E}^{g}-martingale). Hence the claim holds.

Step 2. It suffices to show that for every tâˆˆ[0,T]t\in[0,T] â„™\mathbb{P}-a.s., Ytx;N=essâ€‹supÎ±âˆˆAâ¡Y~tx;N,Î±.Y\_{t}^{x;N}=\operatorname\*{ess\,sup}\_{\alpha\in{\mathcal{}A}}\widetilde{Y}\_{t}^{x;N,\alpha}.
Indeed, it follows from Step 1 that for every Î±âˆˆA\alpha\in{\mathcal{}A} the parameters of the BSDE ([6.11](https://arxiv.org/html/2510.10260v1#S6.E11 "Equation 6.11 â€£ Proof 6.2 (Proof of Proposition 2.10). â€£ 6.1 Proof of results in Section 2 â€£ 6 Proofs â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) satisfies the conditions given in [[49](https://arxiv.org/html/2510.10260v1#bib.bib49), SectionÂ 3]. Furthermore, the parameters of the BSDE ([2.7](https://arxiv.org/html/2510.10260v1#S2.E7 "Equation 2.7 â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) also satisfies the conditions (see Remark [2.9](https://arxiv.org/html/2510.10260v1#S2.Thmtheorem9 "Remark 2.9. â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")â€„(i)).

We recall that Fx;NF^{x;N} given in ([2.6](https://arxiv.org/html/2510.10260v1#S2.E6 "Equation 2.6 â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) is the generator of ([2.7](https://arxiv.org/html/2510.10260v1#S2.E7 "Equation 2.7 â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) and that for each Î±âˆˆA\alpha\in{\mathcal{}A} F~x;N,Î±\widetilde{F}^{x;N,\alpha} given in ([6.10](https://arxiv.org/html/2510.10260v1#S6.E10 "Equation 6.10 â€£ Proof 6.2 (Proof of Proposition 2.10). â€£ 6.1 Proof of results in Section 2 â€£ 6 Proofs â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) is the generator of ([6.11](https://arxiv.org/html/2510.10260v1#S6.E11 "Equation 6.11 â€£ Proof 6.2 (Proof of Proposition 2.10). â€£ 6.1 Proof of results in Section 2 â€£ 6 Proofs â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")). Then for any Î±âˆˆA\alpha\in{\mathcal{}A}, it holds that for all (Ï‰,t,y,z)âˆˆÎ©Ã—[0,T]Ã—â„Ã—â„d(\omega,t,y,z)\in\Omega\times[0,T]\times\mathbb{R}\times\mathbb{R}^{d}

|  |  |  |
| --- | --- | --- |
|  | Ftx;Nâ€‹(Ï‰,y,z)=Ftxâ€‹(Ï‰,y,z)+Nâ€‹maxaâˆˆ{0,1}â¡{(Râ€‹(Xtxâ€‹(Ï‰))âˆ’y)â€‹a}â‰¥F~tx;N,Î±â€‹(Ï‰,y,z).\displaystyle F\_{t}^{x;N}(\omega,y,z)=F\_{t}^{x}(\omega,y,z)+N\max\_{a\in\{0,1\}}\Big\{\big(R(X\_{t}^{x}(\omega))-y\big)a\Big\}\geq\widetilde{F}\_{t}^{x;N,\alpha}(\omega,y,z). |  |

This ensures that for every tâˆˆ[0,T]t\in[0,T],

|  |  |  |  |
| --- | --- | --- | --- |
| (6.12) |  | Ftx;Nâ€‹(Ytx;N,Ztx;N)â‰¥essâ€‹supÎ±âˆˆAâ¡F~tx;N,Î±â€‹(Ytx;N,Ztx;N).\displaystyle F^{x;N}\_{t}\big(Y\_{t}^{x;N},Z\_{t}^{x;N}\big)\geq\operatorname\*{ess\,sup}\_{\alpha\in{\mathcal{}A}}\widetilde{F}\_{t}^{x;N,\alpha}(Y\_{t}^{x;N},Z\_{t}^{x;N}). |  |

Moreover, let Î±âˆ—,x;N\alpha^{\*,x;N} be defined as in ([2.8](https://arxiv.org/html/2510.10260v1#S2.E8 "Equation 2.8 â€£ Proposition 2.10. â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")). Clearly, it takes values in {0,1}\{0,1\}. Moreover, since Yx;NY^{x;N} is in ğ•Š2â€‹(â„)\mathbb{S}^{2}(\mathbb{R}) (see Remark [2.9](https://arxiv.org/html/2510.10260v1#S2.Thmtheorem9 "Remark 2.9. â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")â€„(i)) and (Râ€‹(Xtx))tâˆˆ[0,T](R(X\_{t}^{x}))\_{t\in[0,T]} are ğ”½\mathbb{F}-progressively measurable (noting that XxX^{x} is ItÃ´ (ğ”½,â„™)(\mathbb{F},\mathbb{P})-semimartingale and RR is continuous), Î±âˆ—,x;N\alpha^{\*,x;N} is ğ”½\mathbb{F}-progressively measurable. Therefore, we have that Î±âˆ—,x;NâˆˆA\alpha^{\*,x;N}\in{\mathcal{}A}.

Moreover, by definition of Î±âˆ—,x;N\alpha^{\*,x;N}, F~tx;N,Î±âˆ—,x;Nâ€‹(Ytx;N,Ztx;N)=Ftx;Nâ€‹(Ytx;N,Ztx;N).\widetilde{F}\_{t}^{x;N,\alpha^{\*,x;N}}(Y\_{t}^{x;N},Z\_{t}^{x;N})=F\_{t}^{x;N}(Y\_{t}^{x;N},Z\_{t}^{x;N}).
This implies that the inequality given in ([6.12](https://arxiv.org/html/2510.10260v1#S6.E12 "Equation 6.12 â€£ Proof 6.2 (Proof of Proposition 2.10). â€£ 6.1 Proof of results in Section 2 â€£ 6 Proofs â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) holds as equality.

Therefore, an application of [[21](https://arxiv.org/html/2510.10260v1#bib.bib21), Proposition 3.1] ensures the claim to hold.

Step 3. Lastly, it follows from [[21](https://arxiv.org/html/2510.10260v1#bib.bib21), Corollary 3.3] that the process Î±âˆ—,x;NâˆˆA\alpha^{\*,x;N}\in{\mathcal{}A} is optimal for the problem given in Step 2., i.e., for all tâˆˆ[0,T]t\in[0,T]
essâ€‹supÎ±âˆˆAâ¡Y~tx;N,Î±=Y~tx;N,Î±âˆ—,x;N.\operatorname\*{ess\,sup}\_{\alpha\in{\mathcal{}A}}\widetilde{Y}\_{t}^{x;N,\alpha}=\widetilde{Y}\_{t}^{x;N,\alpha^{\*,x;N}}.
This completes the proof.

### 6.2 Proof of results in Section [3](https://arxiv.org/html/2510.10260v1#S3 "3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")

###### Proof 6.3 (Proof of Theorem [3.4](https://arxiv.org/html/2510.10260v1#S3.Thmtheorem4 "Theorem 3.4. â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")).

Let Nâˆˆâ„•N\in\mathbb{N} and Î»>0\lambda>0 be given. We prove (i) by showing that the parameters of the BSDE ([3.8](https://arxiv.org/html/2510.10260v1#S3.E8 "Equation 3.8 â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) satisfy all the conditions given in [[49](https://arxiv.org/html/2510.10260v1#bib.bib49), Section 3] to ensure its existence and uniqueness to hold.

As rr is a Borel function and both (Î²t)tâˆˆ[0,T](\beta\_{t})\_{t\in[0,T]} and (gâ€‹(t,z))tâˆˆ[0,T](g(t,z))\_{t\in[0,T]} are ğ”½\mathbb{F}-progressively measurable for all zâˆˆâ„dz\in\mathbb{R}^{d}, (FÂ¯tx;N,Î»â€‹(y,z))tâˆˆ[0,T](\overline{F}\_{t}^{x;N,\lambda}(y,z))\_{t\in[0,T]} given in ([3.7](https://arxiv.org/html/2510.10260v1#S3.E7 "Equation 3.7 â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) is ğ”½\mathbb{F}-progressively measurable for all (y,z)âˆˆâ„Ã—â„d(y,z)\in\mathbb{R}\times\mathbb{R}^{d}. Moreover, since gâ€‹(Ï‰,t,0)=0g(\omega,t,0)=0 for all (Ï‰,t)âˆˆÎ©Ã—[0,T](\omega,t)\in\Omega\times[0,T] (see Definitionâ€„[2.1](https://arxiv.org/html/2510.10260v1#S2.Thmtheorem1 "Definition 2.1. â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")â€„(iii)), by the growth conditions of rr and RR (see Assumptionâ€„[2](https://arxiv.org/html/2510.10260v1#S2 "2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")â€„(i)) and Remark [2.4](https://arxiv.org/html/2510.10260v1#S2.Thmtheorem4 "Remark 2.4. â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")â€„(i), it holds that â€–FÂ¯â‹…x;N,Î»â€‹(0,0)â€–ğ•ƒ2<âˆ\|\overline{F}^{x;N,\lambda}\_{\cdot}(0,0)\|\_{\mathbb{L}^{2}}<\infty and â€–Râ€‹(Xâ‹…x)â€–ğ•ƒ2<âˆ\|R(X\_{\cdot}^{x})\|\_{\mathbb{L}^{2}}<\infty.

By the regularity of gg given in Definition [2.1](https://arxiv.org/html/2510.10260v1#S2.Thmtheorem1 "Definition 2.1. â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")â€„(ii) and the boundedness of (Î²t)tâˆˆ[0,T](\beta\_{t})\_{t\in[0,T]} (see Assumption [2](https://arxiv.org/html/2510.10260v1#S2 "2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")â€„(ii)), for every (Ï‰,t)âˆˆÎ©Ã—[0,T](\omega,t)\in\Omega\times[0,T], y,y^âˆˆâ„y,\hat{y}\in\mathbb{R} and z,z^âˆˆâ„dz,\hat{z}\in\mathbb{R}^{d}

|  |  |  |  |
| --- | --- | --- | --- |
| (6.13) |  | |Ftxâ€‹(Ï‰,y,z)âˆ’Ftxâ€‹(Ï‰,y^,z^)|â‰¤Î²tâ€‹(Ï‰)â€‹|yâˆ’y^|+|gâ€‹(Ï‰,t,z)âˆ’gâ€‹(Ï‰,t,z^)|â‰¤(CÎ²+Îº)â€‹(|yâˆ’y^|+|zâˆ’z^|).\displaystyle\begin{aligned} |F\_{t}^{x}(\omega,y,z)-F\_{t}^{x}(\omega,\hat{y},\hat{z})|&\leq\beta\_{t}(\omega)|y-\hat{y}|+|g(\omega,t,z)-g(\omega,t,\hat{z})|\\ &\leq(C\_{\beta}+\kappa)\big(|y-\hat{y}|+|z-\hat{z}|\big).\end{aligned} |  |

Moreover, since the map

|  |  |  |  |
| --- | --- | --- | --- |
| (6.14) |  | hN,Î»:â„âˆ‹sâ†’hN,Î»â€‹(s):=Î»â€‹logâ¡(expâ¡(âˆ’Nâ€‹Î»âˆ’1â€‹s)+1)âˆˆ(0,+âˆ)\displaystyle h^{N,\lambda}:\mathbb{R}\ni s\to h^{N,\lambda}(s):=\lambda\log(\exp(-N\lambda^{-1}\,s)+1)\in(0,+\infty) |  |

is (strictly) decreasing and Nâ€‹Î»âˆ’1N\lambda^{-1}-Lipschitz continuous, we are able to see that
for every Ï‰âˆˆÎ©\omega\in\Omega, tâˆˆ[0,T]t\in[0,T], and y,y^âˆˆâ„y,\hat{y}\in\mathbb{R}

|  |  |  |  |
| --- | --- | --- | --- |
|  | |Gtx;N,Î»â€‹(Ï‰,y)âˆ’Gtx;N,Î»â€‹(Ï‰,y^)|\displaystyle|G^{x;N,\lambda}\_{t}(\omega,y)-G^{x;N,\lambda}\_{t}(\omega,\hat{y})| | â‰¤Nâ€‹|(Râ€‹(Xtxâ€‹(Ï‰))âˆ’y)âˆ’(Râ€‹(Xtxâ€‹(Ï‰))âˆ’y^)|\displaystyle\leq N\Big|\big(R(X\_{t}^{x}(\omega))-y\big)-\big(R(X\_{t}^{x}(\omega))-\hat{y}\big)\Big| |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
| (6.15) |  |  | +|hN,Î»(R(Xtx(Ï‰))âˆ’y)âˆ’hN,Î»((R(Xtx(Ï‰))âˆ’y^)|\displaystyle\quad+\Big|h^{N,\lambda}\big(R(X\_{t}^{x}(\omega))-y\big)-h^{N,\lambda}\big((R(X\_{t}^{x}(\omega))-\hat{y}\big)\Big| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤2â€‹Nâ€‹|yâˆ’y^|.\displaystyle\leq 2N|y-\hat{y}|. |  |

From ([6.13](https://arxiv.org/html/2510.10260v1#S6.E13 "Equation 6.13 â€£ Proof 6.3 (Proof of Theorem 3.4). â€£ 6.2 Proof of results in Section 3 â€£ 6 Proofs â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) and ([6.15](https://arxiv.org/html/2510.10260v1#S6.E15 "Equation 6.15 â€£ Proof 6.3 (Proof of Theorem 3.4). â€£ 6.2 Proof of results in Section 3 â€£ 6 Proofs â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) and the definition of FÂ¯x;N,Î»\overline{F}^{x;N,\lambda} given in ([3.7](https://arxiv.org/html/2510.10260v1#S3.E7 "Equation 3.7 â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")), it follows that the desired priori estimate of FÂ¯x;N,Î»\overline{F}^{x;N,\lambda} holds. Hence an application of [[49](https://arxiv.org/html/2510.10260v1#bib.bib49), TheoremÂ 3.1] ensures the existence and uniqueness of the solution of ([3.8](https://arxiv.org/html/2510.10260v1#S3.E8 "Equation 3.8 â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")), as claimed.

We now prove (ii). By the representation given in ([3.6](https://arxiv.org/html/2510.10260v1#S3.E6 "Equation 3.6 â€£ item (ii) â€£ Remark 3.3. â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")), it suffices to show that â„™\mathbb{P}-a.s. YÂ¯tx;N,Î»=essâ€‹supÏ€âˆˆÎ â¡YÂ¯tx;N,Î»,Ï€.\overline{Y}\_{t}^{x;N,\lambda}=\operatorname\*{ess\,sup}\_{\pi\in\Pi}\overline{Y}\_{t}^{x;N,\lambda,\pi}.

Since H{\mathcal{}H} is strictly convex on [0,1][0,1] (see Remark [3.1](https://arxiv.org/html/2510.10260v1#S3.Thmtheorem1 "Remark 3.1. â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")), it holds that for every (Ï‰,t,y,z)âˆˆÎ©Ã—[0,T]Ã—â„Ã—â„d(\omega,t,y,z)\in\Omega\times[0,T]\times\mathbb{R}\times\mathbb{R}^{d}

|  |  |  |  |
| --- | --- | --- | --- |
| (6.16) |  | FÂ¯tx;N,Î»â€‹(Ï‰,y,z)=Ftxâ€‹(Ï‰,y,z)+maxaâˆˆ[0,1]â¡{Nâ€‹(Râ€‹(Xtxâ€‹(Ï‰))âˆ’y)â€‹aâˆ’Î»â€‹Hâ€‹(a)},\displaystyle\overline{F}\_{t}^{x;N,\lambda}(\omega,y,z)=F\_{t}^{x}(\omega,y,z)+\max\_{a\in[0,1]}\bigg\{N(R(X\_{t}^{x}(\omega))-y)a-\lambda{\mathcal{}H}(a)\bigg\}, |  |

where the equality holds by the first-order-optimality condition with the corresponding maximizer aâˆ—=(1+eâˆ’Nâ€‹Î»âˆ’1â€‹(Râ€‹(Xtxâ€‹(Ï‰))âˆ’y))âˆ’1âˆˆ[0,1].a^{\*}=(1+e^{-{N}{\lambda}^{-1}(R(X\_{t}^{x}(\omega))-y)})^{-1}\in[0,1].

Then it follows from ([6.16](https://arxiv.org/html/2510.10260v1#S6.E16 "Equation 6.16 â€£ Proof 6.3 (Proof of Theorem 3.4). â€£ 6.2 Proof of results in Section 3 â€£ 6 Proofs â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) that FÂ¯tx;N,Î»â€‹(Ï‰,y,z)â‰¥FÂ¯tx;N,Î»,Ï€â€‹(Ï‰,y,z)\overline{F}\_{t}^{x;N,\lambda}(\omega,y,z)\geq\overline{F}\_{t}^{x;N,\lambda,\pi}(\omega,y,z) for all Ï€âˆˆÎ \pi\in\Pi and (Ï‰,t,y,z)âˆˆÎ©Ã—[0,T]Ã—â„Ã—â„d(\omega,t,y,z)\in\Omega\times[0,T]\times\mathbb{R}\times\mathbb{R}^{d}. This ensures that for every tâˆˆ[0,T]t\in[0,T],

|  |  |  |  |
| --- | --- | --- | --- |
| (6.17) |  | FÂ¯tx;N,Î»â€‹(YÂ¯tx;N,Î»,ZÂ¯tx;N,Î»)â‰¥essâ€‹supÏ€âˆˆAâ¡FÂ¯tx;N,Î»,Ï€â€‹(YÂ¯tx;N,Î»,ZÂ¯tx;N,Î»).\displaystyle\overline{F}\_{t}^{x;N,\lambda}\big(\overline{Y}\_{t}^{x;N,\lambda},\overline{Z}\_{t}^{x;N,\lambda}\big)\geq\operatorname\*{ess\,sup}\_{\pi\in{\mathcal{}A}}\overline{F}\_{t}^{x;N,\lambda,\pi}(\overline{Y}\_{t}^{x;N,\lambda},\overline{Z}\_{t}^{x;N,\lambda}). |  |

Moreover, let Ï€âˆ—,x;N,Î»:=(Ï€tâˆ—,x;N,Î»)tâˆˆ[0,T]\pi^{\*,x;N,\lambda}:=(\pi^{\*,x;N,\lambda}\_{t})\_{t\in[0,T]} be defined as in ([3.9](https://arxiv.org/html/2510.10260v1#S3.E9 "Equation 3.9 â€£ item (ii) â€£ Theorem 3.4. â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")). Clearly, it takes values in [0,1][0,1]. Moreover, since YÂ¯x;N,Î»\overline{Y}^{x;N,\lambda} is in ğ•Š2â€‹(â„)\mathbb{S}^{2}(\mathbb{R}) (see part (i)) and (Râ€‹(Xtx))tâˆˆ[0,T](R(X\_{t}^{x}))\_{t\in[0,T]} are ğ”½\mathbb{F}-progressively measurable (noting that XxX^{x} is ItÃ´ (ğ”½,â„™)(\mathbb{F},\mathbb{P})-semimartingale and RR is continuous), Ï€âˆ—,x;N,Î»\pi^{\*,x;N,\lambda} is ğ”½\mathbb{F}-progressively measurable. Therefore, we have that Ï€tâˆ—,x;N,Î»âˆˆÎ \pi^{\*,x;N,\lambda}\_{t}\in\Pi.

Furthermore, by ([6.16](https://arxiv.org/html/2510.10260v1#S6.E16 "Equation 6.16 â€£ Proof 6.3 (Proof of Theorem 3.4). â€£ 6.2 Proof of results in Section 3 â€£ 6 Proofs â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) and definition of Ï€âˆ—,x;N,Î»\pi^{\*,x;N,\lambda}, it holds that

|  |  |  |
| --- | --- | --- |
|  | FÂ¯tx;N,Î»,Ï€âˆ—,x;N,Î»â€‹(YÂ¯tx;N,Î»,ZÂ¯tx;N,Î»)=FÂ¯tx;N,Î»â€‹(YÂ¯tx;N,Î»,ZÂ¯tx;N,Î»),\overline{F}\_{t}^{x;N,\lambda,\pi^{\*,x;N,\lambda}}(\overline{Y}\_{t}^{x;N,\lambda},\overline{Z}\_{t}^{x;N,\lambda})=\overline{F}\_{t}^{x;N,\lambda}\big(\overline{Y}\_{t}^{x;N,\lambda},\overline{Z}\_{t}^{x;N,\lambda}\big), |  |

which implies that the inequality given in ([6.17](https://arxiv.org/html/2510.10260v1#S6.E17 "Equation 6.17 â€£ Proof 6.3 (Proof of Theorem 3.4). â€£ 6.2 Proof of results in Section 3 â€£ 6 Proofs â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) holds as equality.

Therefore, an application of [[21](https://arxiv.org/html/2510.10260v1#bib.bib21), Proposition 3.1] ensures the claim to hold.

Moreover, a direct application of [[21](https://arxiv.org/html/2510.10260v1#bib.bib21), Corollary 3.3] ensures that Ï€âˆ—,x;N,Î»\pi^{\*,x;N,\lambda} is optimal for VÂ¯x;N,Î»\overline{V}^{x;N,\lambda} given in ([3.2](https://arxiv.org/html/2510.10260v1#S3.E2 "Equation 3.2 â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")). This completes the proof.

###### Proof 6.4 (Proof of Theorem [3.5](https://arxiv.org/html/2510.10260v1#S3.Thmtheorem5 "Theorem 3.5. â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")).

Let Nâˆˆâ„•N\in\mathbb{N} and Î»>0\lambda>0 be given. Recall that FÂ¯x;N,Î»\overline{F}^{x;N,\lambda} and Fx;NF^{x;N}, given in ([3.7](https://arxiv.org/html/2510.10260v1#S3.E7 "Equation 3.7 â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) and ([2.6](https://arxiv.org/html/2510.10260v1#S2.E6 "Equation 2.6 â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")), respectively, are the generators of the BSDEs ([3.8](https://arxiv.org/html/2510.10260v1#S3.E8 "Equation 3.8 â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) and ([2.7](https://arxiv.org/html/2510.10260v1#S2.E7 "Equation 2.7 â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")), respectively. Then set for every (Ï‰,t,y,z)âˆˆÎ©Ã—[0,T]Ã—â„Ã—â„d(\omega,t,y,z)\in\Omega\times[0,T]\times\mathbb{R}\times\mathbb{R}^{d}

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î”â€‹FÂ¯tx;N,Î»â€‹(Ï‰,y,z):=\displaystyle\Delta\overline{F}\_{t}^{x;N,\lambda}(\omega,y,z):= | FÂ¯tx;N,Î»â€‹(Ï‰,y,z)âˆ’Ftx;Nâ€‹(Ï‰,y,z)\displaystyle\overline{F}\_{t}^{x;N,\lambda}(\omega,y,z)-F\_{t}^{x;N}(\omega,y,z) |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
| (6.18) |  | =\displaystyle= | hN,Î»â€‹(Râ€‹(Xtxâ€‹(Ï‰))âˆ’y)+Nâ€‹(Râ€‹(Xtxâ€‹(Ï‰))âˆ’y)â€‹ğŸ{y>Râ€‹(Xtxâ€‹(Ï‰))},\displaystyle h^{N,\lambda}(R(X\_{t}^{x}(\omega))-y\big)+N\big(R(X\_{t}^{x}(\omega))-y){\bf 1}\_{\{y>R(X\_{t}^{x}(\omega))\}}, |  |

where we recall that the map hN,Î»h^{N,\lambda} is given in ([6.14](https://arxiv.org/html/2510.10260v1#S6.E14 "Equation 6.14 â€£ Proof 6.3 (Proof of Theorem 3.4). â€£ 6.2 Proof of results in Section 3 â€£ 6 Proofs â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")).

Since the map hN,Î»h^{N,\lambda} is positive and satisfies that hN,Î»â€‹(s)=âˆ’Nâ€‹s+hN,Î»â€‹(âˆ’s)h^{N,\lambda}(s)=-Ns+h^{N,\lambda}(-s) for all sâˆˆâ„s\in\mathbb{R}, it holds that for every (Ï‰,t,y,z)âˆˆÎ©Ã—[0,T]Ã—â„Ã—â„d(\omega,t,y,z)\in\Omega\times[0,T]\times\mathbb{R}\times\mathbb{R}^{d}

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î”â€‹FÂ¯tx;N,Î»â€‹(Ï‰,t,y,z)\displaystyle\Delta\overline{F}\_{t}^{x;N,\lambda}(\omega,t,y,z) | â‰¥[hN,Î»â€‹(Râ€‹(Xtxâ€‹(Ï‰))âˆ’y)+Nâ€‹(Râ€‹(Xtxâ€‹(Ï‰))âˆ’y)]â€‹ğŸ{y>Râ€‹(Xtxâ€‹(Ï‰))}\displaystyle\geq\bigg[h^{N,\lambda}\big(R(X\_{t}^{x}(\omega))-y\big)+N\big(R(X\_{t}^{x}(\omega))-y\big)\bigg]{\bf 1}\_{\{y>R(X\_{t}^{x}(\omega))\}} |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
| (6.19) |  |  | =hN,Î»â€‹(âˆ’(Râ€‹(Xtxâ€‹(Ï‰))âˆ’y))â€‹ğŸ{y>Râ€‹(Xtxâ€‹(Ï‰))}â‰¥0.\displaystyle=h^{N,\lambda}(-(R(X\_{t}^{x}(\omega))-y)){\bf 1}\_{\{y>R(X\_{t}^{x}(\omega))\}}\geq 0. |  |

Moreover, as the terminal conditions of ([3.8](https://arxiv.org/html/2510.10260v1#S3.E8 "Equation 3.8 â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) and ([2.7](https://arxiv.org/html/2510.10260v1#S2.E7 "Equation 2.7 â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) are coincide, it followsÂ from the comparison principle of BSDEs (see, e.g., [[21](https://arxiv.org/html/2510.10260v1#bib.bib21), Theorem 2.2]) that ([3.10](https://arxiv.org/html/2510.10260v1#S3.E10 "Equation 3.10 â€£ Theorem 3.5. â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) holds.

It remains to show that ([3.11](https://arxiv.org/html/2510.10260v1#S3.E11 "Equation 3.11 â€£ Theorem 3.5. â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) holds. Set for every Nâˆˆâ„•N\in\mathbb{N} and Î»>0\lambda>0,

|  |  |  |  |
| --- | --- | --- | --- |
| (6.20) |  | Î”â€‹Yx;N,Î»:=YÂ¯x;N,Î»âˆ’Yx;N,Î”â€‹Zx;N,Î»:=ZÂ¯x;N,Î»âˆ’Zx;N.\displaystyle\Delta{Y}^{x;N,\lambda}:=\overline{Y}^{x;N,\lambda}-Y^{x;N},\qquad\Delta{Z}^{x;N,\lambda}:=\overline{Z}^{x;N,\lambda}-Z^{x;N}. |  |

Since the parameters of the BSDEs ([3.8](https://arxiv.org/html/2510.10260v1#S3.E8 "Equation 3.8 â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) and ([2.7](https://arxiv.org/html/2510.10260v1#S2.E7 "Equation 2.7 â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) satisfy the conditions given in [[21](https://arxiv.org/html/2510.10260v1#bib.bib21), SectionÂ 5] (with exponent 22) for all Nâˆˆâ„•N\in\mathbb{N} and Î»>0\lambda>0, we are able to apply [[21](https://arxiv.org/html/2510.10260v1#bib.bib21), Proposition 5.1] to have the following a priori estimates:666In [[21](https://arxiv.org/html/2510.10260v1#bib.bib21), SectionÂ 5], the filtration (denoted by (Ft)({\mathcal{}F}\_{t}) therein) is set to be right-continuous andÂ complete (and hence not necessarily the Brownian filtration, as in our case). Nevertheless, we can still apply the stability result given in [[21](https://arxiv.org/html/2510.10260v1#bib.bib21), Proposition 5.1], since the martingales MiM^{i}, i=1,2i=1,2, appearing therein are orthogonal to the Brownian motion. Consequently, the arguments remain valid when the general filtration is replaced with the Brownian one.
for every Nâˆˆâ„•N\in\mathbb{N} and Î»>0\lambda>0

|  |  |  |  |
| --- | --- | --- | --- |
| (6.21) |  | â€–Î”â€‹Yx;N,Î»â€–ğ•Š2+â€–Î”â€‹Zx;N,Î»â€–ğ•ƒ2â‰¤Câ€‹ğ”¼â€‹[âˆ«0T|Î”â€‹FÂ¯tx;N,Î»â€‹(Ytx,N,Ztx;N)|2â€‹ğ‘‘t]12,\displaystyle\|\Delta{Y}^{x;N,\lambda}\|\_{\mathbb{S}^{2}}+\|\Delta{Z}^{x;N,\lambda}\|\_{\mathbb{L}^{2}}\leq C\mathbb{E}\bigg[\int\_{0}^{T}|\Delta\overline{F}\_{t}^{x;N,\lambda}(Y\_{t}^{x,N},Z\_{t}^{x;N})|^{2}dt\bigg]^{\frac{1}{2}}, |  |

with some C>0C>0 (depending on TT but not on NN,Î»\lambda), and Î”FÂ¯x;N,Î»\Delta\overline{F}{}^{x;N,\lambda} given inÂ ([6.18](https://arxiv.org/html/2510.10260v1#S6.E18 "Equation 6.18 â€£ Proof 6.4 (Proof of Theorem 3.5). â€£ 6.2 Proof of results in Section 3 â€£ 6 Proofs â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")).

We note that hN,Î»â€‹(s)=Î»â€‹logâ¡(expâ¡(âˆ’Nâ€‹Î»âˆ’1â€‹s)+1)â‰¤Î»â€‹logâ¡2h^{N,\lambda}(s)=\lambda\log(\exp(-N\lambda^{-1}s)+1)\leq\lambda\log 2 for all sâ‰¥0s\geq 0. On the other hand, a simple calculation ensures for every Nâˆˆâ„•N\in\mathbb{N} and Î»>0\lambda>0 that the map

|  |  |  |
| --- | --- | --- |
|  | hÂ¯N,Î»:[0,âˆ)âˆ‹sâ†’hÂ¯N,Î»â€‹(s):=hN,Î»â€‹(âˆ’s)âˆ’Nâ€‹s=Î»â€‹logâ¡(expâ¡(Nâ€‹Î»âˆ’1â€‹s)+1)âˆ’Nâ€‹s\overline{h}^{N,\lambda}:[0,\infty)\ni s\to\overline{h}^{N,\lambda}(s):=h^{N,\lambda}(-s)-Ns=\lambda\log(\exp({N}{\lambda}^{-1}s)+1)-Ns |  |

is (strictly) decreasing. This implies that hÂ¯N,Î»â€‹(s)â‰¤hÂ¯N,Î»â€‹(0)=Î»â€‹logâ¡2\overline{h}^{N,\lambda}(s)\leq\overline{h}^{N,\lambda}(0)=\lambda\log 2 for all sâ‰¥0s\geq 0.

From these observations and ([6.19](https://arxiv.org/html/2510.10260v1#S6.E19 "Equation 6.19 â€£ Proof 6.4 (Proof of Theorem 3.5). â€£ 6.2 Proof of results in Section 3 â€£ 6 Proofs â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")), we have for every Nâˆˆâ„•N\in\mathbb{N}, Î»>0\lambda>0, and tâˆˆ[0,T]t\in[0,T]

|  |  |  |  |
| --- | --- | --- | --- |
|  | 0â‰¤Î”FÂ¯(Ytx,N,Ztx;N)tx;N,Î»=\displaystyle 0\leq\Delta\overline{F}{}^{x;N,\lambda}\_{t}(Y\_{t}^{x,N},Z\_{t}^{x;N})= | hN,Î»â€‹(âˆ’(Ytx,Nâˆ’Râ€‹(Xtx)))â€‹ğŸ{Ytx,Nâ‰¤Râ€‹(Xtx)}\displaystyle h^{N,\lambda}\Big(-\big(Y\_{t}^{x,N}-R(X\_{t}^{x})\big)\Big){\bf 1}\_{\{Y\_{t}^{x,N}\leq R(X\_{t}^{x})\}} |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
| (6.22) |  |  | +hÂ¯N,Î»â€‹(Ytx,Nâˆ’Râ€‹(Xtx))â€‹ğŸ{Ytx,N>Râ€‹(Xtx)}â‰¤Î»â€‹logâ¡2.\displaystyle+\overline{h}^{N,\lambda}\big(Y\_{t}^{x,N}-R(X\_{t}^{x})\big){\bf 1}\_{\{Y\_{t}^{x,N}>R(X\_{t}^{x})\}}\leq\lambda\log 2. |  |

Combining ([6.22](https://arxiv.org/html/2510.10260v1#S6.E22 "Equation 6.22 â€£ Proof 6.4 (Proof of Theorem 3.5). â€£ 6.2 Proof of results in Section 3 â€£ 6 Proofs â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) with ([6.21](https://arxiv.org/html/2510.10260v1#S6.E21 "Equation 6.21 â€£ Proof 6.4 (Proof of Theorem 3.5). â€£ 6.2 Proof of results in Section 3 â€£ 6 Proofs â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) concludes that for every Nâˆˆâ„•N\in\mathbb{N} and Î»>0\lambda>0 the estimate in ([3.11](https://arxiv.org/html/2510.10260v1#S3.E11 "Equation 3.11 â€£ Theorem 3.5. â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) holds,
as claimed. This completes the proof.

###### Proof 6.5 (Proof of Corollary [3.6](https://arxiv.org/html/2510.10260v1#S3.Thmtheorem6 "Corollary 3.6. â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")).

Set for every Nâˆˆâ„•N\in\mathbb{N} and Î»>0\lambda>0, Dtx;N:=Ytx;Nâˆ’Râ€‹(Xtx)D\_{t}^{x;N}:=Y\_{t}^{x;N}-R(X\_{t}^{x}) and DÂ¯tx;N,Î»:=YÂ¯tx;N,Î»âˆ’Râ€‹(Xtx)\overline{D}\_{t}^{x;N,\lambda}:=\overline{Y}\_{t}^{x;N,\lambda}-R(X\_{t}^{x}), tâˆˆ[0,T]t\in[0,T],
where Yx;NY^{x;N} and YÂ¯x;N,Î»\overline{Y}^{x;N,\lambda} denote the first components of the unique solution to the BSDEs ([2.7](https://arxiv.org/html/2510.10260v1#S2.E7 "Equation 2.7 â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) andÂ ([3.8](https://arxiv.org/html/2510.10260v1#S3.E8 "Equation 3.8 â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")), respectively (see also Remark [2.9](https://arxiv.org/html/2510.10260v1#S2.Thmtheorem9 "Remark 2.9. â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).") and Theorem [3.4](https://arxiv.org/html/2510.10260v1#S3.Thmtheorem4 "Theorem 3.4. â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")â€„(i)).

Then for every Nâˆˆâ„•N\in\mathbb{N} and Î»>0\lambda>0 it holds that for every tâ‰¥0t\geq 0, â„™\mathbb{P}-a.s.,

|  |  |  |  |
| --- | --- | --- | --- |
|  | |Î±tâˆ—,x;Nâˆ’Ï€tâˆ—,x;N,Î»|\displaystyle\big|\alpha\_{t}^{\*,x;N}-\pi^{\*,x;N,\lambda}\_{t}\big| | â‰¤|ğŸ{Dtx;N<0}âˆ’ğŸ{DÂ¯tx;N,Î»<0}|+|ğŸ{DÂ¯tx;N,Î»<0}âˆ’11+eNÎ»â€‹DÂ¯tx;N,Î»|\displaystyle\leq\bigg|{\bf 1}\_{\{D\_{t}^{x;N}<0\}}-{\bf 1}\_{\{\overline{D}\_{t}^{x;N,\lambda}<0\}}\bigg|+\bigg|{\bf 1}\_{\{\overline{D}\_{t}^{x;N,\lambda}<0\}}-\frac{1}{1+e^{\frac{N}{\lambda}\overline{D}\_{t}^{x;N,\lambda}}}\bigg| |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
| (6.23) |  |  | =ğŸ{Dtx;N<0â‰¤DÂ¯tx;N,Î»}+11+eNâ€‹Î»âˆ’1â€‹|DÂ¯tx;N,Î»|,\displaystyle={\bf 1}\_{\{D\_{t}^{x;N}<0\leq\overline{D}\_{t}^{x;N,\lambda}\}}+\frac{1}{1+e^{{N}{\lambda}^{-1}|\overline{D}\_{t}^{x;N,\lambda}|}}, |  |

where the last equality holds as Dtx;Nâ‰¤DÂ¯tx;N,Î»D\_{t}^{x;N}\leq\overline{D}\_{t}^{x;N,\lambda}, â„™\mathbb{P}-a.s., for all tâ‰¥0t\geq 0 (see ([3.10](https://arxiv.org/html/2510.10260v1#S3.E10 "Equation 3.10 â€£ Theorem 3.5. â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175)."))).

By Theorem [3.5](https://arxiv.org/html/2510.10260v1#S3.Thmtheorem5 "Theorem 3.5. â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175)."), for any Nâˆˆâ„•N\in\mathbb{N}
â€–Yx;Nâˆ’YÂ¯x;N,Î»â€–ğ•Š2=â€–Dx;Nâˆ’DÂ¯x;N,Î»â€–ğ•Š2â†’0\|Y^{x;N}-\overline{Y}^{x;N,\lambda}\|\_{\mathbb{S}^{2}}=\|D^{x;N}-\overline{D}^{x;N,\lambda}\|\_{\mathbb{S}^{2}}\to 0 asÂ Î»â†“0\lambda\downarrow 0.
This implies that for any Nâˆˆâ„•N\in\mathbb{N}, |Dtx;Nâˆ’DÂ¯tx;N,Î»|â†’0|D\_{t}^{x;N}-\overline{D}\_{t}^{x;N,\lambda}|\to 0 â„™âŠ—dâ€‹t\mathbb{P}\otimes dt-a.e. as Î»â†“0\lambda\downarrow 0.

Comining this with the a priori estimates given in ([6.23](https://arxiv.org/html/2510.10260v1#S6.E23 "Equation 6.23 â€£ Proof 6.5 (Proof of Corollary 3.6). â€£ 6.2 Proof of results in Section 3 â€£ 6 Proofs â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")), we have for any Nâˆˆâ„•N\in\mathbb{N}

|  |  |  |
| --- | --- | --- |
|  | |Î±tâˆ—,x;Nâˆ’Ï€tâˆ—,x;N,Î»|â†’0â„™âŠ—dâ€‹t-a.e., asÂ Î»â†“0.\displaystyle\big|\alpha\_{t}^{\*,x;N}-\pi^{\*,x;N,\lambda}\_{t}\big|\to 0\quad\mbox{$\mathbb{P}\otimes dt$-a.e., as $\lambda\downarrow 0$.} |  |

Furthermore, since |Î±tâˆ—,x;Nâˆ’Ï€tâˆ—,x;N,Î»|â‰¤2\big|\alpha\_{t}^{\*,x;N}-\pi^{\*,x;N,\lambda}\_{t}\big|\leq 2, â„™âŠ—dâ€‹t\mathbb{P}\otimes dt-a.e., for all Nâˆˆâ„•N\in\mathbb{N} and Î»>0\lambda>0 (noting that (Î±âˆ—,x;N)Nâˆˆâ„•âŠ†A(\alpha^{\*,x;N})\_{N\in\mathbb{N}}\subseteq{\mathcal{}A} and (Ï€âˆ—,x;N,Î»)Nâˆˆâ„•,Î»>0âŠ†Î (\pi^{\*,x;N,\lambda})\_{N\in\mathbb{N},\lambda>0}\subseteq\Pi), the dominated convergence theorem guarantees that the convergence in ([3.12](https://arxiv.org/html/2510.10260v1#S3.E12 "Equation 3.12 â€£ Corollary 3.6. â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) holds for all Nâˆˆâ„•N\in\mathbb{N}.

### 6.3 Proof of results in Section [4](https://arxiv.org/html/2510.10260v1#S4 "4 Policy iteration theorem & RL algorithm â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")

###### Proof 6.6 (Proof of Theorem [4.1](https://arxiv.org/html/2510.10260v1#S4.Thmtheorem1 "Theorem 4.1. â€£ 4 Policy iteration theorem & RL algorithm â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")).

We start by proving (i). Let nâˆˆâ„•n\in\mathbb{N} be given. Since YÂ¯tx;N,Î»â‰¥YÂ¯tx;N,Î»,Ï€\overline{Y}^{x;N,\lambda}\_{t}\geq\overline{Y}^{x;N,\lambda,\pi}\_{t} â„™\mathbb{P}-a.s., for all tâˆˆ[0,T]t\in[0,T] and Ï€âˆˆÎ \pi\in\Pi (see Theorem [3.4](https://arxiv.org/html/2510.10260v1#S3.Thmtheorem4 "Theorem 3.4. â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")â€„(ii)), it suffices to show that YÂ¯tx;N,Î»,Ï€n+1â‰¥YÂ¯tx;N,Î»,Ï€n\overline{Y}\_{t}^{x;N,\lambda,\pi^{n+1}}\geq\overline{Y}\_{t}^{x;N,\lambda,\pi^{n}}, â„™\mathbb{P}-a.s., for all tâˆˆ[0,T]t\in[0,T].

For notational simplicity, let (YÂ¯n,ZÂ¯n):=(YÂ¯x;N,Î»,Ï€n,ZÂ¯x;N,Î»,Ï€n)(\overline{Y}^{n},\overline{Z}^{n}):=(\overline{Y}^{x;N,\lambda,\pi^{n}},\overline{Z}^{x;N,\lambda,\pi^{n}}), (YÂ¯n+1,ZÂ¯n+1):=(YÂ¯x;N,Î»,Ï€n+1,ZÂ¯x;N,Î»,Ï€n+1).(\overline{Y}^{n+1},\overline{Z}^{n+1}):=(\overline{Y}^{x;N,\lambda,\pi^{n+1}},\overline{Z}^{x;N,\lambda,\pi^{n+1}}).
In analogy, let
FÂ¯n:=FÂ¯x;N,Î»,Ï€n\overline{F}^{n}:=\overline{F}^{x;N,\lambda,\pi^{n}}, FÂ¯n+1:=FÂ¯x;N,Î»,Ï€n+1\overline{F}^{n+1}:=\overline{F}^{x;N,\lambda,\pi^{n+1}}.

Then we set for everyÂ tâˆˆ[0,T]t\in[0,T]

|  |  |  |
| --- | --- | --- |
|  | Ï•t:=(FÂ¯tn+1âˆ’FÂ¯tn)â€‹(YÂ¯tn,ZÂ¯tn),Î”â€‹Yt:=YÂ¯tn+1âˆ’YÂ¯tn,Î”â€‹Zt:=(Î”â€‹Zt,1,â€¦,Î”â€‹Zt,d)âŠ¤,\displaystyle\phi\_{t}:=(\overline{F}\_{t}^{n+1}-\overline{F}\_{t}^{n})(\overline{Y}\_{t}^{n},\overline{Z}\_{t}^{n}),\quad\Delta Y\_{t}:=\overline{Y}\_{t}^{{n+1}}-\overline{Y}\_{t}^{{n}},\quad\Delta Z\_{t}:=(\Delta Z\_{t,1},\dots,\Delta Z\_{t,d})^{\top}, |  |

with Î”â€‹Zt,i:=ZÂ¯t,in+1âˆ’ZÂ¯t,in\Delta Z\_{t,i}:=\overline{Z}\_{t,i}^{{n+1}}-\overline{Z}\_{t,i}^{{n}} for i=1,â€¦,d,i=1,\dots,d, where ZÂ¯t,in+1\overline{Z}\_{t,i}^{{n+1}} and ZÂ¯t,in\overline{Z}\_{t,i}^{{n}} denote the ii-th component of ZÂ¯tn+1\overline{Z}\_{t}^{{n+1}} and ZÂ¯tn\overline{Z}\_{t}^{{n}}, respectively.

Moreover, we denote for every tâˆˆ[0,T]t\in[0,T] and i=1,â€¦,di=1,\dots,d,

|  |  |  |
| --- | --- | --- |
|  | nt:=1Î”â€‹Ytâ€‹(FÂ¯tn+1â€‹(YÂ¯tn+1,ZÂ¯tn+1)âˆ’FÂ¯tn+1â€‹(YÂ¯tn,ZÂ¯tn+1))â€‹ğŸ{Î”â€‹Ytâ‰ 0},mt,i:=1Î”â€‹Zt,i(FÂ¯tn+1(YÂ¯tn+1,(ZÂ¯t,1n,â€¦,ZÂ¯t,iâˆ’1n,ZÂ¯t,in+1,â€¦,ZÂ¯t,dn+1)âŠ¤)âˆ’FÂ¯tn+1(YÂ¯tn+1,(ZÂ¯t,1n,â€¦,ZÂ¯t,in,ZÂ¯t,i+1n+1,â€¦,ZÂ¯t,dn+1)âŠ¤))ğŸ{Î”â€‹Zt,iâ‰ 0}.\displaystyle\begin{aligned} n\_{t}:=&\frac{1}{\Delta Y\_{t}}\Big(\overline{F}\_{t}^{{n+1}}(\overline{Y}\_{t}^{n+1},\overline{Z}\_{t}^{n+1})-\overline{F}\_{t}^{{n+1}}(\overline{Y}\_{t}^{{n}},\overline{Z}\_{t}^{{n+1}})\Big){\bf 1}\_{\{\Delta Y\_{t}\neq 0\}},\\ m\_{t,i}:=&\frac{1}{\Delta Z\_{t,i}}\Big(\overline{F}^{{n+1}}\_{t}(\overline{Y}\_{t}^{n+1},(\overline{Z}\_{t,1}^{n},\dots,\overline{Z}\_{t,i-1}^{n},\overline{Z}\_{t,i}^{n+1},\dots,\overline{Z}\_{t,d}^{n+1})^{\top})\\ &\quad\quad\quad-\overline{F}^{{n+1}}\_{t}(\overline{Y}\_{t}^{n+1},(\overline{Z}\_{t,1}^{n},\dots,\overline{Z}\_{t,i}^{n},\overline{Z}\_{t,i+1}^{n+1},\dots,\overline{Z}\_{t,d}^{n+1})^{\top})\Big){\bf 1}\_{\{\Delta Z\_{t,i}\neq 0\}}.\end{aligned} |  |

Clearly, (Î”â€‹Y,Î”â€‹Z)(\Delta Y,\Delta Z) satisfies the following BSDE: for tâˆˆ[0,T]t\in[0,T],

|  |  |  |
| --- | --- | --- |
|  | Î”â€‹Yt=âˆ«tT(nsâ€‹Î”â€‹Ys+msâŠ¤â€‹Î”â€‹Zs+Ï•s)â€‹ğ‘‘sâˆ’âˆ«tTÎ”â€‹Zsâ€‹ğ‘‘Bs.\Delta Y\_{t}=\int\_{t}^{T}\left(n\_{s}\Delta Y\_{s}+m\_{s}^{\top}\Delta Z\_{s}+\phi\_{s}\right)ds-\int\_{t}^{T}\Delta Z\_{s}dB\_{s}. |  |

Moreover, by construction ([4.1](https://arxiv.org/html/2510.10260v1#S4.E1 "Equation 4.1 â€£ 4 Policy iteration theorem & RL algorithm â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")), Ï€tn+1=argmaxaâˆˆ[0,1]â€‹{Nâ€‹(Râ€‹(Xtx)âˆ’YÂ¯tn)â€‹aâˆ’Î»â€‹Hâ€‹(a)}{\pi}\_{t}^{n+1}=\mathrm{argmax}\_{a\in[0,1]}\{N(R(X\_{t}^{x})-\overline{Y}\_{t}^{n})a-\lambda{\mathcal{}H}(a)\}, for all tâˆˆ[0,T].t\in[0,T]. This ensures that Ï•tâ‰¥0\phi\_{t}\geq 0 for all tâˆˆ[0,T]t\in[0,T].

Clearly, it holds that nt=âˆ’(Î²t+Nâ€‹Ï€tn+1)â€‹ğŸ{Î”â€‹Ytâ‰ 0}n\_{t}=-(\beta\_{t}+N\pi\_{t}^{n+1}){\bf 1}\_{\{\Delta Y\_{t}\neq 0\}} for all tâˆˆ[0,T]t\in[0,T]. Moreover, by Assumption [2](https://arxiv.org/html/2510.10260v1#S2 "2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")â€„(ii) and the fact that Ï€n+1âˆˆÎ \pi^{n+1}\in\Pi has values in [0,1][0,1], (nt)tâˆˆ[0,T](n\_{t})\_{t\in[0,T]} is uniformly bounded. Furthermore, by the Lipschitz property of gg (see Definitionâ€„[2.1](https://arxiv.org/html/2510.10260v1#S2.Thmtheorem1 "Definition 2.1. â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")â€„(ii)), for every i=1,â€¦,di=1,\dots,d, (mt,i)tâˆˆ[0,T](m\_{t,i})\_{t\in[0,T]} is uniformly bounded by Îº>0\kappa>0.

Therefore, by letting Î“t:=expâ¡(âˆ«0tmsâ€‹ğ‘‘Bs+âˆ«0t(âˆ’nsâˆ’12â€‹|ms|2)â€‹ğ‘‘s)\Gamma\_{t}:=\exp(\int\_{0}^{t}m\_{s}dB\_{s}+\int\_{0}^{t}(-n\_{s}-\frac{1}{2}|m\_{s}|^{2})ds) for tâˆˆ[0,T]t\in[0,T], applying ItÃ´â€™s formula into (Î“tâ€‹Î”â€‹Yt)tâˆˆ[0,T](\Gamma\_{t}\Delta Y\_{t})\_{t\in[0,T]} and taking the conditional expectation ğ”¼tâ€‹[â‹…]\mathbb{E}\_{t}[\cdot],

|  |  |  |
| --- | --- | --- |
|  | Î”â€‹Yt=Î“tâˆ’1â€‹ğ”¼tâ€‹[âˆ«tTÎ“sâ€‹Ï•sâ€‹ğ‘‘s],â„™-a.s.,for allâ€‹tâˆˆ[0,T].\Delta Y\_{t}=\Gamma\_{t}^{-1}\mathbb{E}\_{t}\bigg[\int\_{t}^{T}\Gamma\_{s}\phi\_{s}ds\bigg],\quad\mbox{$\mathbb{P}$-a.s.,}\quad\mbox{for all}\;\;t\in[0,T]. |  |

Since Ï•â‰¥0\phi\geq 0, we have Î”â€‹Ytâ‰¥0\Delta Y\_{t}\geq 0 â„™\mathbb{P}-a.s., for all tâˆˆ[0,T]t\in[0,T].
Therefore, the partÂ (i) holds.

We now prove (ii). Set for every nâˆˆâ„•n\in\mathbb{N}

|  |  |  |
| --- | --- | --- |
|  | FÂ¯:=FÂ¯x;N,Î»,Î”n+1â€‹FÂ¯:=FÂ¯âˆ’FÂ¯n+1,YÂ¯:=YÂ¯x;N,Î»,Î”nâ€‹YÂ¯t:=YÂ¯tâˆ’YÂ¯tn\overline{F}:=\overline{F}^{{x;N,\lambda}},\quad\Delta^{n+1}\overline{F}:=\overline{F}-\overline{F}^{{n+1}},\quad\overline{Y}:=\overline{Y}^{x;N,\lambda},\quad\Delta^{n}\overline{Y}\_{t}:=\overline{Y}\_{t}-\overline{Y}^{n}\_{t} |  |

In analogy, set ZÂ¯:=ZÂ¯x;N,Î»\overline{Z}:=\overline{Z}^{x;N,\lambda} and Î”nâ€‹ZÂ¯t:=ZÂ¯tâˆ’ZÂ¯n\Delta^{n}\overline{Z}\_{t}:=\overline{Z}\_{t}-\overline{Z}^{n}.

We first note that for any nâˆˆâ„•n\in\mathbb{N}, Ï‰âˆˆÎ©\omega\in\Omega, tâˆˆ[0,T]t\in[0,T], y,y^âˆˆâ„y,\hat{y}\in\mathbb{R} and z,z^âˆˆâ„dz,\hat{z}\in\mathbb{R}^{d}

|  |  |  |  |
| --- | --- | --- | --- |
|  | |FÂ¯tn+1â€‹(Ï‰,y,z)âˆ’FÂ¯tn+1â€‹(Ï‰,y^,z^)|\displaystyle|\overline{F}\_{t}^{{n+1}}(\omega,y,z)-\overline{F}\_{t}^{{n+1}}(\omega,\hat{y},\hat{z})| | â‰¤(Î²tâ€‹(Ï‰)+N)â€‹|yâˆ’y^|+|gâ€‹(Ï‰,t,z)âˆ’gâ€‹(Ï‰,t,z^)|\displaystyle\leq(\beta\_{t}(\omega)+N)|y-\hat{y}|+|g(\omega,t,z)-g(\omega,t,\hat{z})| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤(CÎ²+Îº+N)â€‹(|yâˆ’y^|+|zâˆ’z^|).\displaystyle\leq(C\_{\beta}+\kappa+N)\big(|y-\hat{y}|+|z-\hat{z}|\big). |  |

Set C1:=CÎ²+Îº+N>0C\_{1}:=C\_{\beta}+\kappa+N>0. By the a priori estimate in [[70](https://arxiv.org/html/2510.10260v1#bib.bib70), TheoremÂ 4.2.3], there exists some C2>0C\_{2}>0 (that depends on C1,T,dC\_{1},T,d but not on n,Î»n,\lambda), such that777For any tâˆˆ[0,T]t\in[0,T] and Yâˆˆğ•Š2â€‹(â„)Y\in\mathbb{S}^{2}(\mathbb{R}), denote by â€–Yâ€–ğ•Št22:=ğ”¼â€‹[supsâˆˆ[t,T]|Ys|2]\|Y\|\_{\mathbb{S}^{2}\_{t}}^{2}:=\mathbb{E}[\sup\_{s\in[t,T]}|Y\_{s}|^{2}]. In analogy, for any Zâˆˆğ•ƒ2â€‹(â„d)Z\in\mathbb{L}^{2}(\mathbb{R}^{d}), denote by â€–Zâ€–ğ•ƒt22:=ğ”¼â€‹[âˆ«tT|Zs|2â€‹ğ‘‘s]\|Z\|^{2}\_{\mathbb{L}^{2}\_{t}}:=\mathbb{E}[\int\_{t}^{T}|Z\_{s}|^{2}ds].

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–Î”n+1â€‹YÂ¯â€–ğ•Št22+â€–Î”n+1â€‹ZÂ¯â€–ğ•ƒt22\displaystyle\|\Delta^{n+1}\overline{Y}\|\_{\mathbb{S}^{2}\_{t}}^{2}+\|\Delta^{n+1}\overline{Z}\|\_{\mathbb{L}^{2}\_{t}}^{2} | â‰¤C2â€‹ğ”¼â€‹[âˆ«tT|Î”n+1â€‹FÂ¯sâ€‹(YÂ¯s,ZÂ¯s)|â€‹ğ‘‘s]2\displaystyle\leq C\_{2}\mathbb{E}\bigg[\int\_{t}^{T}\big|\Delta^{n+1}\overline{F}\_{s}(\overline{Y}\_{s},\overline{Z}\_{s})\big|ds\bigg]^{2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤C2â€‹Tâ€‹âˆ«tTğ”¼â€‹[|Î”n+1â€‹FÂ¯sâ€‹(YÂ¯s,ZÂ¯s)|2]â€‹ğ‘‘sfor allÂ tâˆˆ[0,T],\displaystyle\leq C\_{2}T\int\_{t}^{T}\mathbb{E}\Big[\big|\Delta^{n+1}\overline{F}\_{s}(\overline{Y}\_{s},\overline{Z}\_{s})\big|^{2}\Big]ds\quad\mbox{for all $t\in[0,T]$}, |  |

where we have used the Jensenâ€™s inequality with exponent 22 for the last inequality.

Moreover, by setting Lsn:=NÎ»â€‹(Râ€‹(Xsx)âˆ’YÂ¯sn)L^{n}\_{s}:=\frac{N}{\lambda}(R(X^{x}\_{s})-\overline{Y}^{n}\_{s}) and Ls:=NÎ»â€‹(Râ€‹(Xsx)âˆ’YÂ¯s)L\_{s}:=\frac{N}{\lambda}(R(X^{x}\_{s})-\overline{Y}\_{s}) and noting that Ï€sn+1=(1+eâˆ’Lsn)âˆ’1\pi\_{s}^{n+1}=(1+e^{-L\_{s}^{n}})^{-1},
we compute that for all sâˆˆ[t,T]s\in[t,T]

|  |  |  |  |
| --- | --- | --- | --- |
|  | |Î”n+1â€‹FÂ¯sâ€‹(YÂ¯s,ZÂ¯s)|\displaystyle\big|\Delta^{n+1}\overline{F}\_{s}(\overline{Y}\_{s},\overline{Z}\_{s})\big| | =Î»â€‹|(Lsâˆ’Lsn)âˆ’Lsâˆ’Lsn1+eâˆ’Lsn+logâ¡(1+eâˆ’Lsn)âˆ’logâ¡(1+eâˆ’Ls)|\displaystyle=\lambda\bigg|(L\_{s}-L^{n}\_{s})-\frac{L\_{s}-L^{n}\_{s}}{1+e^{-L^{n}\_{s}}}+\log(1+e^{-L^{n}\_{s}})-\log(1+e^{-L\_{s}})\bigg| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤3â€‹Î»â€‹|Lsâˆ’Lsn|=3â€‹Nâ€‹|Î”nâ€‹YÂ¯s|\displaystyle\leq 3\lambda|L\_{s}-L^{n}\_{s}|=3N\big|\Delta^{n}\overline{Y}\_{s}\big| |  |

where we have used the fact that |logâ¡(1+ex)âˆ’logâ¡(1+ey)|â‰¤|xâˆ’y||\log(1+e^{x})-\log(1+e^{y})|\leq|x-y| for all x,yâˆˆâ„x,y\in\mathbb{R}.

By setting C3:=9â€‹C2â€‹Tâ€‹N2>0C\_{3}:=9C\_{2}TN^{2}>0, we have shown that for all tâˆˆ[0,T]t\in[0,T]

|  |  |  |  |
| --- | --- | --- | --- |
| (6.24) |  | â€–Î”n+1â€‹YÂ¯â€–ğ•Št22+â€–Î”n+1â€‹ZÂ¯â€–ğ•ƒt22â‰¤C3â€‹âˆ«tTğ”¼â€‹[|Î”nâ€‹YÂ¯s|2]â€‹ğ‘‘sâ‰¤C3â€‹âˆ«tTâ€–Î”nâ€‹YÂ¯â€–ğ•Šs22â€‹ğ‘‘s.\displaystyle\|\Delta^{n+1}\overline{Y}\|\_{\mathbb{S}^{2}\_{t}}^{2}+\|\Delta^{n+1}\overline{Z}\|\_{\mathbb{L}^{2}\_{t}}^{2}\leq{C}\_{3}\int\_{t}^{T}\mathbb{E}\Big[\big|\Delta^{n}\overline{Y}\_{s}\big|^{2}\Big]ds\leq C\_{3}\int\_{t}^{T}\|\Delta^{n}\overline{Y}\|\_{\mathbb{S}^{2}\_{s}}^{2}ds. |  |

By using the same arguments presented for ([6.24](https://arxiv.org/html/2510.10260v1#S6.E24 "Equation 6.24 â€£ Proof 6.6 (Proof of Theorem 4.1). â€£ 6.3 Proof of results in Section 4 â€£ 6 Proofs â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) iteratively,

|  |  |  |
| --- | --- | --- |
|  | â€–Î”n+1â€‹YÂ¯â€–ğ•Š22+â€–Î”n+1â€‹ZÂ¯â€–ğ•ƒ22â‰¤C3â€‹âˆ«tTâ€–Î”nâ€‹YÂ¯â€–ğ•Štn22â€‹ğ‘‘tn\displaystyle\|\Delta^{n+1}\overline{Y}\|\_{\mathbb{S}^{2}}^{2}+\|\Delta^{n+1}\overline{Z}\|\_{\mathbb{L}^{2}}^{2}\leq C\_{3}\int\_{t}^{T}\|\Delta^{n}\overline{Y}\|\_{\mathbb{S}^{2}\_{t\_{n}}}^{2}dt\_{n} |  |
|  |  |  |
| --- | --- | --- |
|  | â‰¤(C3)2â€‹âˆ«0Tâˆ«tnTâ€–Î”nâˆ’1â€‹YÂ¯â€–ğ•Štnâˆ’122â€‹ğ‘‘tnâˆ’1â€‹ğ‘‘tn\displaystyle\qquad\leq({C\_{3}})^{2}\int\_{0}^{T}\int\_{t\_{n}}^{T}\|\Delta^{n-1}\overline{Y}\|\_{\mathbb{S}^{2}\_{t\_{n-1}}}^{2}dt\_{n-1}\;dt\_{n} |  |
|  |  |  |
| --- | --- | --- |
|  | â‰¤â‹¯\displaystyle\qquad\leq\cdots |  |
|  |  |  |
| --- | --- | --- |
|  | â‰¤(C3)nâ€‹âˆ«0Tâˆ«tnTâ‹¯â€‹âˆ«t2Tâ€–Î”1â€‹YÂ¯â€–ğ•Št122â€‹ğ‘‘t1â€‹â‹¯â€‹ğ‘‘tnâˆ’1â€‹ğ‘‘tn\displaystyle\qquad\leq({C}\_{3})^{n}\int\_{0}^{T}\int\_{t\_{n}}^{T}\cdots\int\_{t\_{2}}^{T}\|\Delta^{1}\overline{Y}\|\_{\mathbb{S}^{2}\_{t\_{1}}}^{2}dt\_{1}\cdots dt\_{n-1}\;dt\_{n} |  |
|  |  |  |
| --- | --- | --- |
|  | â‰¤(C3)nâ€‹â€–Î”1â€‹YÂ¯â€–ğ•Š22â€‹âˆ«0Tâˆ«tnTâ‹¯â€‹âˆ«t2T1â€‹ğ‘‘t1â€‹â‹¯â€‹ğ‘‘tnâˆ’1â€‹ğ‘‘tn=(C3)nâ€‹Tnn!â€‹â€–Î”1â€‹YÂ¯â€–ğ•Š22,\displaystyle\qquad\leq({C}\_{3})^{n}\|\Delta^{1}\overline{Y}\|\_{\mathbb{S}^{2}}^{2}\int\_{0}^{T}\int\_{t\_{n}}^{T}\cdots\int\_{t\_{2}}^{T}1\;dt\_{1}\cdots dt\_{n-1}\;dt\_{n}=({C}\_{3})^{n}\frac{T^{n}}{n!}\|\Delta^{1}\overline{Y}\|\_{\mathbb{S}^{2}}^{2}, |  |

together with the 1-Lipschitz continuity of the logistic function (1+eâˆ’x)âˆ’1(1+e^{-x})^{-1}, we have

|  |  |  |
| --- | --- | --- |
|  | â€–Ï€n+1âˆ’Ï€âˆ—â€–ğ•Š22â‰¤NÎ»â€‹ğ”¼â€‹[suptâˆˆ[0,T]|YÂ¯tx;N,Î»,Ï€nâˆ’YÂ¯tx;N,Î»|2]=NÎ»â€‹â€–Î”nâ€‹YÂ¯â€–ğ•Š2.\displaystyle\|\pi^{n+1}-\pi^{\*}\|^{2}\_{\mathbb{S}^{2}}\leq\frac{N}{\lambda}\mathbb{E}\bigg[\sup\_{t\in[0,T]}|\overline{Y}^{x;N,\lambda,\pi^{n}}\_{t}-\overline{Y}^{x;N,\lambda}\_{t}|^{2}\bigg]=\frac{N}{\lambda}\|\Delta^{n}\overline{Y}\|\_{\mathbb{S}^{2}}. |  |

The monotonicity of Ï€tn+1\pi^{n+1}\_{t} as nâ†‘âˆn\uparrow\infty is obvious from the logistic functional form on YÂ¯x;N,Î»,Ï€n\overline{Y}^{x;N,\lambda,\pi^{n}}, which completes the proof.

Let us consider the following controlled forward-backward SDEs for any Ï€Ë‡âˆˆÎ Ë‡\check{\pi}\in\check{\Pi}: for any (t,x)âˆˆ[0,T]Ã—â„d(t,x)\in[0,T]\times\mathbb{R}^{d} and sâˆˆ[0,T]s\in[0,T],

|  |  |  |  |
| --- | --- | --- | --- |
| (6.25) |  | YË‡st,x;N,Î»,Ï€Ë‡=Râ€‹(XË‡Tt,x)+âˆ«sTFË‡uN,Î»,Ï€Ë‡â€‹(XË‡ut,x,YË‡ut,x;N,Î»,Ï€Ë‡,ZË‡ut,x;N,Î»,Ï€Ë‡)â€‹ğŸ{uâ‰¥t}â€‹ğ‘‘uâˆ’âˆ«sTZË‡ut,x;N,Î»,Ï€Ë‡â€‹ğ‘‘Bu.\displaystyle\begin{aligned} \check{Y}\_{s}^{t,x;N,\lambda,\check{\pi}}&=R(\check{X}\_{T}^{t,x})+\int\_{s}^{T}\check{F}\_{u}^{N,\lambda,\check{\pi}}(\check{X}\_{u}^{t,x},\check{Y}\_{u}^{t,x;N,\lambda,\check{\pi}},\check{Z}\_{u}^{t,x;N,\lambda,\check{\pi}}){\bf 1}\_{\{u\geq t\}}du\\ &\quad-\int\_{s}^{T}\check{Z}\_{u}^{t,x;N,\lambda,\check{\pi}}dB\_{u}.\end{aligned} |  |

where XË‡st,x=x+(âˆ«tsb~oâ€‹(s,XË‡st,x)â€‹ğ‘‘s+Ïƒ~oâ€‹(s,XË‡st,x)â€‹dâ€‹Bs)â€‹ğŸ{sâ‰¥t}\check{X}\_{s}^{t,x}=x+(\int\_{t}^{s}\widetilde{b}^{o}(s,\check{X}\_{s}^{t,x})ds+\widetilde{\sigma}^{o}(s,\check{X}\_{s}^{t,x})dB\_{s}){\bf 1}\_{\{s\geq t\}}.

One can deduce that there exists a unique solution (YË‡t,x;N,Î»,Ï€Ë‡,ZË‡t,x;N,Î»,Ï€Ë‡)(\check{Y}^{t,x;N,\lambda,\check{\pi}},\check{Z}^{t,x;N,\lambda,\check{\pi}}) to ([6.25](https://arxiv.org/html/2510.10260v1#S6.E25 "Equation 6.25 â€£ 6.3 Proof of results in Section 4 â€£ 6 Proofs â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) (see Remark [3.3](https://arxiv.org/html/2510.10260v1#S3.Thmtheorem3 "Remark 3.3. â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")). In particular, since XË‡0,x=Xx\check{X}^{0,x}=X^{x} (see ([2.1](https://arxiv.org/html/2510.10260v1#S2.E1 "Equation 2.1 â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) and RemarkÂ [2.3](https://arxiv.org/html/2510.10260v1#S2.Thmtheorem3 "Remark 2.3. â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")â€„(ii)), (YË‡0,x;N,Î»,Ï€Ë‡,ZË‡0,x;N,Î»,Ï€Ë‡)(\check{Y}^{0,x;N,\lambda,\check{\pi}},\check{Z}^{0,x;N,\lambda,\check{\pi}}) is the unique solution (YÂ¯x;N,Î»,Ï€Ë‡â€‹(Xx),ZÂ¯x;N,Î»,Ï€Ë‡â€‹(Xx))(\overline{Y}^{x;N,\lambda,\check{\pi}(X^{x})},\overline{Z}^{x;N,\lambda,\check{\pi}(X^{x})}) to ([3.5](https://arxiv.org/html/2510.10260v1#S3.E5 "Equation 3.5 â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) under Ï€Ë‡â€‹(Xx)=(Ï€Ë‡tâ€‹(Xtx))tâˆˆ[0,T]âˆˆÎ \check{\pi}(X^{x})=(\check{\pi}\_{t}(X\_{t}^{x}))\_{t\in[0,T]}\in\Pi.

Then we observe the following Markovian representation of ([6.25](https://arxiv.org/html/2510.10260v1#S6.E25 "Equation 6.25 â€£ 6.3 Proof of results in Section 4 â€£ 6 Proofs â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")).

###### Lemma 6.7.

Under SettingÂ [4](https://arxiv.org/html/2510.10260v1#S4 "4 Policy iteration theorem & RL algorithm â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175)."), let Ï€Ë‡âˆˆÎ Ë‡\check{\pi}\in\check{\Pi} be given.

* (i)

  There exist two Borel measurable functions vN,Î»,Ï€Ë‡:[0,T]Ã—â„dâ†’â„v^{N,\lambda,\check{\pi}}:[0,T]\times\mathbb{R}^{d}\to\mathbb{R} and wN,Î»,Ï€Ë‡:[0,T]Ã—â„dâ†’â„dw^{N,\lambda,\check{\pi}}:[0,T]\times\mathbb{R}^{d}\to\mathbb{R}^{d} such that for every tâ‰¤sâ‰¤Tt\leq s\leq T, â„™âŠ—dâ€‹s\mathbb{P}\otimes ds-a.e.,

  |  |  |  |  |
  | --- | --- | --- | --- |
  | (6.26) |  | YË‡st,x;N,Î»,Ï€Ë‡=vN,Î»,Ï€Ë‡â€‹(s,XË‡st,x),ZË‡st,x;N,Î»,Ï€Ë‡=((Ïƒ~o)âŠ¤â€‹wN,Î»,Ï€Ë‡)â€‹(s,XË‡st,x),\displaystyle\check{Y}\_{s}^{t,x;N,\lambda,\check{\pi}}=v^{N,\lambda,\check{\pi}}(s,\check{X}\_{s}^{t,x}),\quad\check{Z}\_{s}^{t,x;N,\lambda,\check{\pi}}=\big((\widetilde{\sigma}^{o})^{\top}w^{N,\lambda,\check{\pi}}\big)(s,\check{X}\_{s}^{t,x}), |  |

  where (YË‡t,x;N,Î»,Ï€Ë‡,ZË‡t,x;N,Î»,Ï€Ë‡)(\check{Y}^{t,x;N,\lambda,\check{\pi}},\check{Z}^{t,x;N,\lambda,\check{\pi}}) is the unique solution of ([6.25](https://arxiv.org/html/2510.10260v1#S6.E25 "Equation 6.25 â€£ 6.3 Proof of results in Section 4 â€£ 6 Proofs â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")).
* (ii)

  Furthermore, if Ï€Ë‡tâ€‹(â‹…)\check{\pi}\_{t}(\cdot) is continuous on â„d\mathbb{R}^{d} for any tâˆˆ[0,T]t\in[0,T],
  one can find a function vN,Î»,Ï€Ë‡:[0,T]Ã—â„dâ†’â„v^{N,\lambda,\check{\pi}}:[0,T]\times\mathbb{R}^{d}\to\mathbb{R} which satisfies the property given in ([6.26](https://arxiv.org/html/2510.10260v1#S6.E26 "Equation 6.26 â€£ item (i) â€£ Lemma 6.7. â€£ 6.3 Proof of results in Section 4 â€£ 6 Proofs â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) and is a viscosity solution of the following PDE:

  |  |  |  |
  | --- | --- | --- |
  |  | (âˆ‚tv+â„’â€‹v)â€‹(t,x)+FË‡tN,Î»,Ï€Ë‡â€‹(x,vâ€‹(t,x),((Ïƒ~o)âŠ¤â€‹âˆ‡v)â€‹(t,x))=0,(t,x)âˆˆ[0,T)Ã—â„d,(\partial\_{t}v+\mathcal{L}v)(t,x)+\check{F}^{N,\lambda,\check{\pi}}\_{t}(x,v(t,x),((\widetilde{\sigma}^{o})^{\top}\nabla v)(t,x))=0,\quad(t,x)\in[0,T)\times\mathbb{R}^{d}, |  |

  with vâ€‹(T,â‹…)=Râ€‹(â‹…)v(T,\cdot)=R(\cdot), where the infinitesimal operator â„’\mathcal{L} is defined as in RemarkÂ [4.2](https://arxiv.org/html/2510.10260v1#S4.Thmtheorem2 "Remark 4.2. â€£ 4 Policy iteration theorem & RL algorithm â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).").
  In particular, vË‡N,Î»,Ï€Ë‡\check{v}^{N,\lambda,\check{\pi}} is locally Lipschitz w.r.t.Â xx and HÃ¶lder continuous w.r.t.Â tt (Hence, it is continuous on [0,T]Ã—â„d[0,T]\times\mathbb{R}^{d}).

###### Proof 6.8.

We start with proving (i). According to [[19](https://arxiv.org/html/2510.10260v1#bib.bib19), Theorem 8.9], it suffices to show that the generator FË‡â‹…N,Î»,Ï€Ë‡â€‹(â‹…,â‹…,â‹…)\check{F}\_{\cdot}^{N,\lambda,\check{\pi}}(\cdot,\cdot,\cdot) given in ([4.2](https://arxiv.org/html/2510.10260v1#S4.E2 "Equation 4.2 â€£ 4 Policy iteration theorem & RL algorithm â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) satisfies the condition (M1b) given in [[19](https://arxiv.org/html/2510.10260v1#bib.bib19)] (noting that XË‡t,x\check{X}^{t,x} given in ([6.25](https://arxiv.org/html/2510.10260v1#S6.E25 "Equation 6.25 â€£ 6.3 Proof of results in Section 4 â€£ 6 Proofs â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) satisfies (M1f) therein; see Remark [2.4](https://arxiv.org/html/2510.10260v1#S2.Thmtheorem4 "Remark 2.4. â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")). Note that Î²t\beta\_{t} and Ï€Ë‡tâ€‹(x)\check{\pi}\_{t}(x) are uniformly bounded (see Setting [4](https://arxiv.org/html/2510.10260v1#S4 "4 Policy iteration theorem & RL algorithm â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")), and gg is uniformly Lipschitz w.r.t.Â zz (see Definition [2.1](https://arxiv.org/html/2510.10260v1#S2.Thmtheorem1 "Definition 2.1. â€£ 2 Optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")). Therefore, FË‡â‹…N,Î»,Ï€Ë‡â€‹(â‹…,â‹…,â‹…)\check{F}\_{\cdot}^{N,\lambda,\check{\pi}}(\cdot,\cdot,\cdot) is uniformly Lipschitz w.r.t.Â (y,z)(y,z) with the corresponding Lipschitz constant depending only on CÎ²,Î»,NC\_{\beta},\lambda,N (not on t,xt,x). Moreover, for all (t,x)âˆˆ[0,T]Ã—â„d(t,x)\in[0,T]\times\mathbb{R}^{d},

|  |  |  |
| --- | --- | --- |
|  | |FË‡tN,Î»,Ï€Ë‡â€‹(x,0,0)|â‰¤|râ€‹(x)|+Nâ€‹|Râ€‹(x)â€‹Ï€Ë‡tâ€‹(x)|+Î»â€‹|Hâ€‹(Ï€Ë‡tâ€‹(x))|.|\check{F}\_{t}^{N,\lambda,\check{\pi}}(x,0,0)|\leq|r(x)|+N|R(x)\check{\pi}\_{t}(x)|+\lambda\big|{\mathcal{}H}\big(\check{\pi}\_{t}(x)\big)\big|. |  |

Note that |Hâ€‹(Ï€Ë‡tâ€‹(â‹…))||{\mathcal{}H}(\check{\pi}\_{t}(\cdot))| is bounded by logâ¡2\log 2 (see Remark [3.1](https://arxiv.org/html/2510.10260v1#S3.Thmtheorem1 "Remark 3.1. â€£ 3 Exploratory framework: approximation of optimal stopping under ambiguity â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")), and râ€‹(â‹…)r(\cdot) and Râ€‹(â‹…)R(\cdot) are linearly growing. Therefore, there exists a constant CC only depends on Cr,R,N,Î»C\_{r,R},N,\lambda (not on (t,x)(t,x)) such that |FË‡N,Î»,Ï€Ë‡â€‹(t,x,0,0)|â‰¤Câ€‹(1+|x|)|\check{F}^{N,\lambda,\check{\pi}}(t,x,0,0)|\leq C(1+|x|) for all (t,x)âˆˆ[0,T]Ã—â„d(t,x)\in[0,T]\times\mathbb{R}^{d}. Thus, (M1b) holds true.

We now prove (ii). As râ€‹(x),Râ€‹(x),Ï€Ë‡tâ€‹(x)r(x),R(x),\check{\pi}\_{t}(x) are continuous w.r.t xx for all tâˆˆ[0,T]t\in[0,T],
the condition (M1bc\mathrm{M1b^{c}}) given in [[19](https://arxiv.org/html/2510.10260v1#bib.bib19)] holds true. Therefore, an application of [[19](https://arxiv.org/html/2510.10260v1#bib.bib19), TheoremÂ 8.12] ensures
that vN,Î»,Ï€Ë‡â€‹(t,x):=YË‡tt,x;N,Î»,Ï€Ë‡v^{N,\lambda,\check{\pi}}(t,x):=\check{Y}\_{t}^{t,x;N,\lambda,\check{\pi}} for (t,x)âˆˆ[0,T]Ã—â„d(t,x)\in[0,T]\times\mathbb{R}^{d}
is a viscosity solution
of the PDE given in the statement (ii); see ([6.25](https://arxiv.org/html/2510.10260v1#S6.E25 "Equation 6.25 â€£ 6.3 Proof of results in Section 4 â€£ 6 Proofs â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")). Moreover, using the flow property of {XË‡st,x;tâ‰¤sâ‰¤T,xâˆˆâ„d}\{\check{X}\_{s}^{t,x};t\leq s\leq T,x\in\mathbb{R}^{d}\} and the uniqueness of the solution of ([6.25](https://arxiv.org/html/2510.10260v1#S6.E25 "Equation 6.25 â€£ 6.3 Proof of results in Section 4 â€£ 6 Proofs â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")), we have for tâ‰¤sâ‰¤Tt\leq s\leq T, â„™âŠ—dâ€‹s\mathbb{P}\otimes ds-a.e., vN,Î»,Ï€Ë‡â€‹(s,XË‡st,x)=YË‡ss,XË‡st,x;N,Î»,Ï€Ë‡=YË‡st,x;N,Î»,Ï€Ë‡,v^{N,\lambda,\check{\pi}}(s,\check{X}\_{s}^{t,x})=\check{Y}\_{s}^{s,\check{X}\_{s}^{t,x};N,\lambda,\check{\pi}}=\check{Y}\_{s}^{t,x;N,\lambda,\check{\pi}}, that is, the property in ([6.26](https://arxiv.org/html/2510.10260v1#S6.E26 "Equation 6.26 â€£ item (i) â€£ Lemma 6.7. â€£ 6.3 Proof of results in Section 4 â€£ 6 Proofs â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")) holds.
Lastly, the regularity of vN,Î»,Ï€Ë‡v^{N,\lambda,\check{\pi}} follows from the argument in the proof of [[19](https://arxiv.org/html/2510.10260v1#bib.bib19), TheoremÂ 8.12], which employs the LpL^{p}-estimation techniques in
the proof of [[50](https://arxiv.org/html/2510.10260v1#bib.bib50), LemmaÂ 2.1 and CorollaryÂ 2.10].

###### Proof 6.9 (Proof of CorollaryÂ [4.3](https://arxiv.org/html/2510.10260v1#S4.Thmtheorem3 "Corollary 4.3. â€£ 4 Policy iteration theorem & RL algorithm â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")).

Part (i) follows immediately from an iterative application of Lemma [6.7](https://arxiv.org/html/2510.10260v1#S6.Thmtheorem7 "Lemma 6.7. â€£ 6.3 Proof of results in Section 4 â€£ 6 Proofs â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")â€„(i). In a similary manner, Part (ii) is obtained by iteratively applying LemmaÂ [6.7](https://arxiv.org/html/2510.10260v1#S6.Thmtheorem7 "Lemma 6.7. â€£ 6.3 Proof of results in Section 4 â€£ 6 Proofs â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175).")â€„(ii). Indeed, as Ï€Ë‡t1â€‹(â‹…)\check{\pi}\_{t}^{1}(\cdot) is continuous, the corresponding function vN,Î»,1v^{N,\lambda,1} satisfies all the properties in Part (i) and is also a viscosity solution of the PDE given in the statement (with the generator FË‡â‹…N,Î»,Ï€Ë‡1)\check{F}\_{\cdot}^{N,\lambda,\check{\pi}^{1}}). In particular, it is continuous on [0,T]Ã—â„d[0,T]\times\mathbb{R}^{d}, the next iteration policy Ï€Ë‡t2â€‹(â‹…)\check{\pi}\_{t}^{2}(\cdot) ,tâˆˆ[0,T]t\in[0,T], (defined as in ([4.4](https://arxiv.org/html/2510.10260v1#S4.E4 "Equation 4.4 â€£ item (i) â€£ Corollary 4.3. â€£ 4 Policy iteration theorem & RL algorithm â€£ ROBUST EXPLORATORY STOPPING UNDER AMBIGUITY IN REINFORCEMENT LEARNING Submitted to the editors October 11, 2025. \fundingH. Y. Wong acknowledges the support from the Research Grants Council of Hong Kong (grant DOI: GRF14308422). K. Park acknowledges the support from the National Research Foundation of Korea (grant DOI: RS-2025-02633175)."))) is also continuous on â„d\mathbb{R}^{d}. The same argument can therefore be applied at each subsequent iteration. This completes the proof.

## References

* [1]

  D.Â Bartl, A.Â Neufeld, and K.Â Park, Numerical method for nonlinear Kolmogorov PDEs via sensitivity analysis, arXiv preprint arXiv:2403.11910, (2024).
* [2]

  D.Â Bartl, A.Â Neufeld, and K.Â Park, Sensitivity of robust optimization problems under drift and volatility uncertainty, Finance Stoch., arXiv:2311.11248, (2025+).
* [3]

  E.Â Bayraktar and S.Â Yao, Optimal stopping for non-linear expectationsâ€”Part I, Stochastic Process. Appl., 121 (2011), pp.Â 185â€“211.
* [4]

  E.Â Bayraktar and S.Â Yao, Optimal stopping for non-linear expectationsâ€”Part II, Stochastic Process. Appl., 121 (2011), pp.Â 212â€“264.
* [5]

  C.Â Beck, S.Â Becker, P.Â Cheridito, A.Â Jentzen, and A.Â Neufeld, Deep splitting method for parabolic PDEs, SIAM J. Sci. Comput., 43 (2021), pp.Â A3135â€“A3154.
* [6]

  S.Â Becker, P.Â Cheridito, and A.Â Jentzen, Deep optimal stopping, J. Mach. Learn. Res., 20 (2019), pp.Â 1â€“25.
* [7]

  S.Â Becker, P.Â Cheridito, A.Â Jentzen, and T.Â Welti, Solving high-dimensional optimal stopping problems using deep learning, Eur. J. Appl. Math., 32 (2021), pp.Â 470â€“514.
* [8]

  D.Â Blackwell and L.Â E. Dubins, An extension of Skorohodâ€™s almost sure representation theorem, Proc. Amer. Math. Soc., 89 (1983), pp.Â 691â€“692.
* [9]

  J.Â Blanchet, M.Â Lu, T.Â Zhang, and H.Â Zhong, Double pessimism is provably efficient for distributionally robust offline reinforcement learning: Generic algorithm and robust partial coverage, Adv. Neural Inf. Process. Syst., 36 (2023), pp.Â 66845â€“66859.
* [10]

  K.Â Chen, K.Â Park, and H.Â Y. Wong, Robust dividend policy: Equivalence of Epstein-Zin and Maenhout preferences, arXiv preprint arXiv:2406.12305, (2024).
* [11]

  Z.Â Chen and L.Â Epstein, Ambiguity, risk, and asset returns in continuous time, Econometrica, 70 (2002), pp.Â 1403â€“1443.
* [12]

  F.Â Coquet, Y.Â Hu, J.Â MÃ©min, and S.Â Peng, Filtration-consistent nonlinear expectations and related gg-expectations, Probab. Theory Relat. Fields, 123 (2002), pp.Â 1â€“27.
* [13]

  M.Â Dai, Y.Â Dong, and Y.Â Jia, Learning equilibrium mean-variance strategy, Math. Finance, 33 (2023), pp.Â 1166â€“1212.
* [14]

  M.Â Dai, Y.Â Dong, Y.Â Jia, and X.Â Zhou, Learning mertonâ€™s strategies in an incomplete market: Recursive entropy regularization and biased gaussian exploration, SSRN Electronic Journal, (2023), <https://doi.org/10.2139/ssrn.4668480>.
* [15]

  M.Â Dai, Y.Â Sun, Z.Â Q. Xu, and X.Â Y. Zhou, Learning to optimally stop diffusion processes, with financial applications, Manag. Sci., (to appear).
* [16]

  J.Â Dianetti, G.Â Ferrari, and R.Â Xu, Exploratory optimal stopping: A singular control formulation, arXiv preprint arXiv:2408.09335, (2024).
* [17]

  Y.Â Dong, Randomized optimal stopping problem in continuous time and reinforcement learning algorithm, SIAM J. Control Optim., 62 (2024), pp.Â 1590â€“1614.
* [18]

  P.Â H. Dybvig, Dusenberryâ€™s ratcheting of consumption: optimal dynamic consumption and investment given intolerance for any decline in standard of living, Rev. Econ. Stud., 62 (1995), pp.Â 287â€“313.
* [19]

  N.Â El Karoui, S.Â HamadÃ¨ne, and A.Â Matoussi, Chapter Eight. BSDEs And Applications, Princeton University Press, Princeton, 2009, pp.Â 267â€“320.
  In: Indifference Pricing: Theory and Applications.
* [20]

  N.Â ElÂ Karoui, C.Â Kapoudjian, E.Â Pardoux, S.Â Peng, and M.-C. Quenez, Reflected solutions of backward SDE, and related obstacle problems for PDEs, Ann. Probab., 25 (1997), pp.Â 702â€“737.
* [21]

  N.Â ElÂ Karoui, S.Â Peng, and M.Â C. Quenez, Backward stochastic differential equations in finance, Math. Finance, 7 (1997), pp.Â 1â€“71.
* [22]

  L.Â G. Epstein and M.Â Schneider, Recursive multiple-priors, J. Econ. Theory, 113 (2003), pp.Â 1â€“31.
* [23]

  G.Â Ferrari, H.Â Li, and F.Â Riedel, Optimal consumption with Hindyâ€“Huangâ€“Kreps preferences under nonlinear expectations, Adv. Appl. Probab., 54 (2022), pp.Â 1222â€“1251.
* [24]

  P.Â A. Forsyth and K.Â R. Vetzal, Quadratic convergence for valuing American options using a penalty method, SIAM J. Sci. Comput., 23 (2002), pp.Â 2095â€“2122.
* [25]

  R.Â Frey and V.Â KÃ¶ck, Convergence analysis of the deep splitting scheme: The case of partial integro-differential equations and the associated forward backward SDEs with jumps, SIAM J. Sci. Comput., 47 (2025), pp.Â A527â€“A552.
* [26]

  N.Â Frikha, L.Â Li, and D.Â Chee, An entropy regularized BSDE approach to Bermudan options and games, arXiv preprint arXiv:2509.18747, (2025).
* [27]

  M.Â Germain, H.Â Pham, and X.Â Warin, Approximation error analysis of some deep backward schemes for nonlinear pdes, SIAM J. Sci. Comput., 44 (2022), pp.Â A28â€“A56.
* [28]

  I.Â Goodfellow, Y.Â Bengio, and A.Â Courville, Deep Learning, MIT Press, 2016.
* [29]

  D.Â Guo, D.Â Yang, H.Â Zhang, etÂ al., Deepseek-r1 incentivizes reasoning in LLMs through reinforcement learning, Nature, 645 (2025), pp.Â 633â€“638.
* [30]

  J.Â Han, A.Â Jentzen, and W.Â E, Solving high-dimensional partial differential equations using deep learning, Proc. Natl. Acad. Sci.,, 115 (2018), pp.Â 8505â€“8510.
* [31]

  X.Â Han, R.Â Wang, and X.Â Y. Zhou, Choquet regularization for continuous-time reinforcement learning, SIAM J. Control Optim., 61 (2023), pp.Â 2777â€“2801.
* [32]

  Y.-J. Huang, Z.Â Wang, and Z.Â Zhou, Convergence of policy iteration for entropy-regularized stochastic control problems, SIAM J. Control Optim., 63 (2025), pp.Â 752â€“777.
* [33]

  C.Â HurÃ©, H.Â Pham, and X.Â Warin, Deep backward schemes for high-dimensional nonlinear PDEs, Math. Comp., 89 (2020), p.Â 1.
* [34]

  J.Â Jacod and A.Â Shiryaev, Limit theorems for stochastic processes, vol.Â 288, Springer Science & Business Media, 2013.
* [35]

  Y.Â Jia and X.Â Y. Zhou, Policy evaluation and temporal-difference learning in continuous time and space: A martingale approach, J. Mach. Learn. Res., 23 (2022), pp.Â 1â€“55.
* [36]

  Y.Â Jia and X.Â Y. Zhou, Policy gradient and actor-critic learning in continuous time and space: Theory and algorithms, J. Mach. Learn. Res., 23 (2022), pp.Â 1â€“50.
* [37]

  Y.Â Jia and X.Â Y. Zhou, q-learning in continuous time, J. Mach. Learn. Res., 24 (2023), pp.Â 1â€“61.
* [38]

  P.Â Klibanoff, M.Â Marinacci, and S.Â Mukerji, A smooth model of decision making under ambiguity, Econometrica, 73 (2005), pp.Â 1849â€“1892.
* [39]

  J.-P. Lepeltier and M.Â Xu, Penalization method for reflected backward stochastic differential equations with one r.c.l.l. barrier, Stat. Probab. Lett., 75 (2005), pp.Â 58â€“66.
* [40]

  S.Â Levine, C.Â Finn, T.Â Darrell, and P.Â Abbeel, End-to-end training of deep visuomotor policies, J. Mach. Learn. Res., 17 (2016), p.Â 1334â€“1373.
* [41]

  X.Â Mao, Stochastic differential equations and applications, Elsevier, 2007.
* [42]

  M.Â Marinacci, Limit laws for non-additive probabilities and their frequentist interpretation, J. Econ. Theory, 84 (1999), pp.Â 145â€“195.
* [43]

  A.Â Mazzon and P.Â Tankov, Optimal stopping and divestment timing under scenario ambiguity and learning, arXiv preprint arXiv:2408.09349, (2024).
* [44]

  V.Â Mnih, K.Â Kavukcuoglu, D.Â Silver, etÂ al., Human-level control through deep reinforcement learning, Nature, 518 (2015), pp.Â 529â€“533.
* [45]

  J.Â Morimoto and K.Â Doya, Robust reinforcement learning, Neural Comput., 17 (2005), pp.Â 335â€“359.
* [46]

  A.Â Neufeld, P.Â Schmocker, and S.Â Wu, Full error analysis of the random deep splitting method for nonlinear parabolic PDEs and PIDEs, arXiv preprint arXiv:2405.05192, (2024).
* [47]

  M.Â Nutz and J.Â Zhang, Optimal stopping under adverse nonlinear expectation and related games, Ann. Appl. Probab., 25 (2015), pp.Â 2503â€“2534.
* [48]

  K.Â Panaganti, Z.Â Xu, D.Â Kalathil, and M.Â Ghavamzadeh, Robust reinforcement learning using offline data, Adv. Neural Inf. Process. Syst., 35 (2022), pp.Â 32211â€“32224.
* [49]

  E.Â Pardoux and S.Â Peng, Adapted solution of a backward stochastic differential equation, Syst. Control Lett., 14 (1990), pp.Â 55â€“61.
* [50]

  E.Â Pardoux and S.Â Peng, Backward stochastic differential equations and quasilinear parabolic partial differential equations, in Stochastic Partial Differential Equations and Their Applications: Proceedings of IFIP WG 7/1 International Conference University of North Carolina at Charlotte, NC June 6â€“8, 1991, Springer, 2005, pp.Â 200â€“217.
* [51]

  K.Â Park, K.Â Chen, and H.Â Y. Wong, Irreversible consumption habit under ambiguity: Singular control and optimal GG-stopping time, Ann. Appl. Probab., 35 (2025), pp.Â 2471â€“2525.
* [52]

  K.Â Park and H.Â Y. Wong, Robust retirement with return ambiguity: Optimal GG-stopping time in dual space, SIAM J. Control Optim., 61 (2023), pp.Â 1009â€“1037.
* [53]

  S.Â Peng, Backward SDE and related gg-expectation, Pitman research notes in mathematics series, (1997), pp.Â 141â€“160.
* [54]

  S.Â Peng and M.Â Xu, The smallest gg-supermartingale and reflected BSDE with single and double L2L^{2} obstacles, Ann. Inst. H. PoincarÃ© Probab. Statist., 41 (2005), pp.Â 605â€“630.
* [55]

  G.Â Peskir and A.Â Shiryaev, Optimal stopping and free-boundary problems, Springer, 2006.
* [56]

  P.Â E. Protter, Stochastic Integration and Differential Equations, Stochastic Modelling and Applied Probability, Springer, Berlin, Heidelberg, 2Â ed., 2005.
* [57]

  A.Â M. Reppen, H.Â M. Soner, and V.Â Tissot-Daguette, Neural optimal stopping boundary, Math. Finance, 35 (2025), pp.Â 441â€“469.
* [58]

  F.Â Riedel, Optimal stopping with multiple priors, Econometrica, 77 (2009), pp.Â 857â€“908.
* [59]

  A.Â Roy, H.Â Xu, and S.Â Pokutta, Reinforcement learning under model mismatch, Adv. Neural Inf. Process. Syst., 30 (2017).
* [60]

  D.Â Silver, A.Â Huang, C.Â Maddison, etÂ al., Mastering the game of Go with deep neural networks and tree search, Nature, 529 (2016), pp.Â 484â€“489.
* [61]

  D.Â Silver, J.Â Schrittwieser, K.Â Simonyan, etÂ al., Mastering the game of Go without human knowledge, Nature, 550 (2017), pp.Â 354â€“359.
* [62]

  J.Â Sirignano and K.Â Spiliopoulos, DGM: A deep learning algorithm for solving partial differential equations, J. Comput. Phys., 375 (2018), pp.Â 1339â€“1364.
* [63]

  R.Â Sutton and A.Â Barto, Reinforcement learning: An introduction, IEEE Trans. Neural Netw., 9 (1998), pp.Â 1054â€“1054.
* [64]

  W.Â Tang, Y.Â P. Zhang, and X.Â Y. Zhou, Exploratory HJB equations and their convergence, SIAM J. Control Optim., 60 (2022), pp.Â 3191â€“3216.
* [65]

  A.Â Wald and J.Â Wolfowitz, Optimum character of the sequential probability ratio test, Ann. Math. Stat., (1948), pp.Â 326â€“339.
* [66]

  H.Â Wang, T.Â Zariphopoulou, and X.Â Y. Zhou, Reinforcement learning in continuous time and space: A stochastic control approach, J. Mach. Learn. Res., 21 (2020), pp.Â 1â€“34.
* [67]

  H.Â Wang and X.Â Y. Zhou, Continuous-time meanâ€“variance portfolio selection: A reinforcement learning framework, Math. Finance, 30 (2020), pp.Â 1273â€“1308.
* [68]

  B.Â Wu and L.Â Li, Reinforcement learning for continuous-time mean-variance portfolio selection in a regime-switching market, J. Econ. Dyn. Control, 158 (2024), p.Â 104787.
* [69]

  H.Â Zhang, H.Â Chen, C.Â Xiao, B.Â Li, M.Â Liu, D.Â Boning, and C.-J. Hsieh, Robust deep reinforcement learning against adversarial perturbations on state observations, Adv. Neural Inf. Process. Syst., 33 (2020), pp.Â 21024â€“21037.
* [70]

  J.Â Zhang, Backward Stochastic Differential Equations, Springer New York, New York, 2017.