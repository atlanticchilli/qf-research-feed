---
authors:
- Yaxuan Kong
- Yoontae Hwang
- Marcus Kaiser
- Chris Vryonides
- Roel Oomen
- Stefan Zohren
doc_id: arxiv:2510.20699v1
family_id: arxiv:2510.20699
is_current: true
taxonomy:
  alpha_families: []
  asset_classes: []
  horizons: []
  themes: []
title: Fusing Narrative Semantics for Financial Volatility Forecasting
url_abs: http://arxiv.org/abs/2510.20699v1
url_html: https://arxiv.org/html/2510.20699v1
venue: arXiv q-fin
version: 1
year: 2025
---


Yaxuan Kong
University of OxfordUnited Kingdom
[yaxuan.kong@eng.ox.ac.uk](mailto:yaxuan.kong@eng.ox.ac.uk)
, 
Yoontae Hwang
Pusan National UniversityRepublic of Korea
[yoontae.hwang@pusan.ac.kr](mailto:yoontae.hwang@pusan.ac.kr)
, 
Marcus Kaiser
Deutsche Bank AGLondonUnited Kingdom
[marcus.kaiser@db.com](mailto:marcus.kaiser@db.com)
, 
Chris Vryonides
Deutsche Bank AGLondonUnited Kingdom
[chris.vryonides@db.com](mailto:chris.vryonides@db.com)
, 
Roel Oomen
Deutsche Bank AGLondonUnited Kingdom
[roel.oomen@db.com](mailto:roel.oomen@db.com)
 and 
Stefan Zohren
University of OxfordUnited Kingdom
[stefan.zohren@eng.ox.ac.uk](mailto:stefan.zohren@eng.ox.ac.uk)

(2025)

###### Abstract.

We introduce M2VN: Multi-Modal Volatility Network, a novel deep learning-based framework for financial volatility forecasting that unifies time series features with unstructured news data. M2VN leverages the representational power of deep neural networks to address two key challenges in this domain: (i) aligning and fusing heterogeneous data modalities, numerical financial data and textual information, and (ii) mitigating look-ahead bias that can undermine the validity of financial models. To achieve this, M2VN combines open-source market features with news embeddings generated by Time Machine GPT, a recently introduced point-in-time LLM, ensuring temporal integrity. An auxiliary alignment loss is introduced to enhance the integration of structured and unstructured data within the deep learning architecture. Extensive experiments demonstrate that M2VN consistently outperforms existing baselines, underscoring its practical value for risk management and financial decision-making in dynamic markets.

Multimodal fusion, Temporal alignment, Look-ahead bias mitigation, Heterogeneous data integration, Representation learning

‚Ä†‚Ä†copyright: acmlicensed‚Ä†‚Ä†journalyear: 2025‚Ä†‚Ä†copyright: cc‚Ä†‚Ä†conference: 6th ACM International Conference on AI in Finance; November 15‚Äì18, 2025; Singapore, Singapore‚Ä†‚Ä†booktitle: 6th ACM International Conference on AI in Finance (ICAIF ‚Äô25), November 15‚Äì18, 2025, Singapore, Singapore‚Ä†‚Ä†doi: 10.1145/3768292.3771256‚Ä†‚Ä†isbn: 979-8-4007-2220-2/2025/11‚Ä†‚Ä†ccs: Computing methodologies¬†Artificial intelligence

## Disclaimer

The opinions expressed in this article are those of the authors alone and do not necessarily represent the views of Deutsche Bank AG. This article is not intended to be comprehensive, nor does it constitute financial or other advice.

## 1. Introduction

Forecasting financial market volatility is a critical task in quantitative finance, with broad applications in risk management, derivative pricing, and portfolio optimization (Andersen et¬†al., [2006](https://arxiv.org/html/2510.20699v1#bib.bib4); Alexander, [2008](https://arxiv.org/html/2510.20699v1#bib.bib2)). Traditionally, volatility forecasting methods have relied primarily on features extracted directly from time series data, such as historical prices, returns, and various statistical measures (Clements et¬†al., [2024](https://arxiv.org/html/2510.20699v1#bib.bib9)). However, recent advances in natural language processing (NLP) and the emergence of large language models (LLMs) have opened new possibilities for incorporating external textual information such as news articles into predictive models (Kong et¬†al., [2024a](https://arxiv.org/html/2510.20699v1#bib.bib23), [b](https://arxiv.org/html/2510.20699v1#bib.bib24)). While text-enhanced time series forecasting (TSF) has been explored in related domains (Liu et¬†al., [2024](https://arxiv.org/html/2510.20699v1#bib.bib26); Jin et¬†al., [2023](https://arxiv.org/html/2510.20699v1#bib.bib21); Gruver et¬†al., [2023](https://arxiv.org/html/2510.20699v1#bib.bib19)), research that specifically leverages LLM-derived textual features for volatility forecasting remains limited.

Most existing studies incorporate textual data through word embeddings and use linear or NLP models for realized volatility (RV) forecasting (Rahimikia et¬†al., [2021](https://arxiv.org/html/2510.20699v1#bib.bib31); Parvini and Assa, [2025](https://arxiv.org/html/2510.20699v1#bib.bib30); Atkins et¬†al., [2018](https://arxiv.org/html/2510.20699v1#bib.bib6)). In contrast, the integration of rich, context-aware information extracted by LLMs with deep learning-based time series models in a unified framework is still largely underexplored. Deep learning models, with their ability to capture complex, nonlinear relationships and to integrate heterogeneous data sources, are particularly well-suited for modeling the intricate dynamics between financial time series and textual information. Nevertheless, progress in this area has been relatively limited to date.

![Refer to caption](x1.png)


Figure 1. Illustration of two challenges: data fusion across modalities and mitigation of look-ahead bias.‚Ä†‚Ä†:

This limited exploration can largely be attributed to several inherent challenges unique to the intersection of financial time series and textual data (as illustrated in Figure 1). Firstly, financial news and time series data represent distinct modalities, making their alignment and integration non-trivial. Specifically, learning how to effectively embed and fuse the intersection of textual information and numerical time series data into a unified embedding space is a significant challenge. Secondly, utilizing news information with LLMs for financial prediction tasks often introduces look-ahead bias, whereby models inadvertently incorporate future knowledge, compromising their applicability and reliability.

To address these challenges, we propose M2VN: Multi-Modal Volatility Network. M2VN innovatively combines traditional time series features - derived from open-sourced end-of-day price information (open, high, low, close, and volume) - with news data, where news content is embedded using Time Machine GPT (TiMaGPT) (Drinkall et¬†al., [2024](https://arxiv.org/html/2510.20699v1#bib.bib16)), a series of point-in-time language models specifically designed to mitigate look-ahead bias. To effectively align these two modalities, we introduce an auxiliary alignment loss function, which significantly enhances the model‚Äôs ability to coherently fuse textual and time series information within a unified framework. Through this design, M2VN can leverage both structured financial data and unstructured textual data to improve volatility forecasting performance. We validate the effectiveness of M2VN through extensive experiments, demonstrating its superior performance compared to existing baselines.

In summary, our paper makes the following key contributions:

1. (1)

   We propose M2VN: Multi-Modal Volatility Network, a novel framework to align and fuse news embeddings with time series data for the task of financial volatility forecasting.
2. (2)

   We assess the incremental value of incorporating rich news articles for RV forecasting, providing quantitative evidence for the benefits of augmenting time series models with contextual textual information.
3. (3)

   To the best of our knowledge, this is the first work to apply TiMaGPT (Drinkall et¬†al., [2024](https://arxiv.org/html/2510.20699v1#bib.bib16)) for mitigating look-ahead bias in financial volatility forecasting with LLMs.

## 2. Related Works

We review related works on financial volatility forecasting and text-enhanced TSF.

### 2.1. Financial Volatility Forecasting

This section first reviews the main models used in volatility forecasting, then discusses recent advances in incorporating exogenous variables to enhance forecast accuracy.

Typical Volatility Forecasting Models.
Traditional volatility forecasting has been dominated GARCH (Engle and Bollerslev, [1986](https://arxiv.org/html/2510.20699v1#bib.bib17); Andersen and Bollerslev, [1998](https://arxiv.org/html/2510.20699v1#bib.bib3)) models, which rely on daily squared returns to capture the dynamics of financial time series. While straightforward, these models are often hampered by the noisiness of daily return-based volatility proxies. The emergence of RV models, built on high-frequency intraday data, significantly improved predictive accuracy, especially with the introduction of the Heterogeneous Auto-Regressive (HAR) model (Corsi, [2009](https://arxiv.org/html/2510.20699v1#bib.bib12); Degiannakis and Filis, [2017](https://arxiv.org/html/2510.20699v1#bib.bib14); Audrino et¬†al., [2020](https://arxiv.org/html/2510.20699v1#bib.bib7)). The HAR framework models current volatility using lagged daily, weekly, and monthly volatility, capturing volatility persistence in a simple linear structure (Clements et¬†al., [2024](https://arxiv.org/html/2510.20699v1#bib.bib9)). Despite the advantages of high-frequency data, its limited accessibility and higher cost have led researchers to explore range-based estimators ‚Äî such as Garman-Klass (Garman and Klass, [1980](https://arxiv.org/html/2510.20699v1#bib.bib18)), Parkinson (Parkinson, [1980](https://arxiv.org/html/2510.20699v1#bib.bib29)), and Rogers-Satchell (Rogers and Satchell, [1991](https://arxiv.org/html/2510.20699v1#bib.bib32)) ‚Äî that utilize daily open, high, low, and close prices. Recent empirical work (Ly√≥csa et¬†al., [2021](https://arxiv.org/html/2510.20699v1#bib.bib27)) demonstrates that, while high-frequency-based models outperform low-frequency ones for very short-term forecasts, the difference in forecast accuracy becomes negligible for longer horizons (e.g., one month), making low-frequency approaches both practical and effective in real-world settings.

Exogenous Variables in Volatility Forecasting.
To further enhance volatility forecasting performance, recent studies have introduced exogenous variables into HAR models, leading to the HAR-X framework (Degiannakis and Filis, [2017](https://arxiv.org/html/2510.20699v1#bib.bib14); Clements et¬†al., [2024](https://arxiv.org/html/2510.20699v1#bib.bib9)). These exogenous predictors typically include implied volatility indices, economic policy and equity market uncertainty indices, geopolitical risk measures, macroeconomic indicators, and asset-specific momentum. By introducing external information that is not captured by historical volatility alone, these additional features ‚Äî many of which are available at daily frequencies ‚Äî have been shown to enhance out-of-sample forecast accuracy (Korkusuz, [2024](https://arxiv.org/html/2510.20699v1#bib.bib25)). Remarkably, several studies find that low-frequency HAR-X models can, in some cases, rival the forecasting performance of traditional high-frequency approaches (Clements et¬†al., [2024](https://arxiv.org/html/2510.20699v1#bib.bib9)). Nevertheless, the literature (Clements and Preve, [2021](https://arxiv.org/html/2510.20699v1#bib.bib10); Christensen et¬†al., [2023](https://arxiv.org/html/2510.20699v1#bib.bib8)) remains primarily focused on structured numerical data or linear models, with relatively few efforts devoted to combining unstructured text sources with these related time series features (Parvini and Assa, [2025](https://arxiv.org/html/2510.20699v1#bib.bib30); Rahimikia et¬†al., [2021](https://arxiv.org/html/2510.20699v1#bib.bib31)). The question of how to effectively align and fuse these disparate modalities remains largely unexplored, motivating the need for systematic research into multi-modal volatility forecasting.

### 2.2. Text-Enhanced Time Series Forecasting

This section reviews recent advances in integrating textual information into TSF, with a focus on applications of LLMs and the challenge of look-ahead bias in financial contexts.

LLMs for TSF. Effectively integrating external textual information has been shown to enhance TSF, for instance by correlating financial news sentiment with market movements (Wan et¬†al., [2021](https://arxiv.org/html/2510.20699v1#bib.bib35)). More recently, research has shown that LLMs can significantly advance this approach. For instance, (Zhou et¬†al., [2023](https://arxiv.org/html/2510.20699v1#bib.bib41); Hwang et¬†al., [2025](https://arxiv.org/html/2510.20699v1#bib.bib20)) demonstrated the versatility of LLMs across a range of time series tasks by employing a GPT-2 backbone capable of capturing temporal dependencies. Similarly, (Gruver et¬†al., [2023](https://arxiv.org/html/2510.20699v1#bib.bib19); Ansari et¬†al., [2024](https://arxiv.org/html/2510.20699v1#bib.bib5)) highlighted the zero-shot forecasting capabilities of pretrained LLMs, showing that appropriate tokenization allows these models to implicitly learn temporal patterns without the need for explicit task-specific training. Furthermore, (Jin et¬†al., [2023](https://arxiv.org/html/2510.20699v1#bib.bib21)) proposed a reprogramming approach that converts time series data into formats more interpretable by LLMs, achieving state-of-the-art forecasting results. Their patch reprogramming method aligns time series patch embeddings with natural language representations, enhancing the model‚Äôs ability to understand and reason about time series data. Collectively, these studies underscore the promise of LLMs in advancing the field of TSF.

Look-Ahead Bias in Financial Forecasting.
Despite their success in general domains, the application of LLM-based methods to financial time series is complicated by the risk of look-ahead bias. Look-ahead bias arises when models inadvertently access future information during training or evaluation, leading to unrealistically optimistic results and compromising the reliability of real-world forecasts (Sarkar and Vafa, [2024](https://arxiv.org/html/2510.20699v1#bib.bib33)). To address this challenge, (Kim et¬†al., [2024](https://arxiv.org/html/2510.20699v1#bib.bib22)) propose using anonymized data to prevent LLMs from exploiting memorized future information; however, this approach may restrict the richness and specificity of textual data available to the model, potentially limiting its forecasting performance. To more robustly mitigate look-ahead bias while preserving the utility of external information, (Drinkall et¬†al., [2024](https://arxiv.org/html/2510.20699v1#bib.bib16)) introduced TiMaGPT, a series of point-in-time LLMs specifically trained to maintain strict temporal integrity. By ensuring that models remain uninformed about future events or language changes during training, TiMaGPT provides a robust foundation for reliable TSF in dynamic financial contexts.

## 3. Methodology

We propose M2VN: Multi-Modal Volatility Network, which is graphically illustrated in Figure 2.

![Refer to caption](x2.png)


Figure 2. The architecture of M2VN, a multi-modal volatility network for financial volatility forecasting.

### 3.1. Problem Formulation

This subsection presents the formal problem formulation for multi-modal volatility forecasting and establishes the mathematical notation employed throughout our methodological exposition. We frame the task as a supervised, sequence-to-one learning problem, where the objective is to predict a future financial volatility measure by leveraging a historical look-back window of heterogeneous data streams. Our temporal domain consists of discrete trading days, indexed by t=1,‚Ä¶,ùíØt=1,\dots,\mathcal{T}. The core task is, at the close of day tt, to predict the realized volatility for a future day t+Ht+H, where H‚â•1H\geq 1 is the prediction horizon. To achieve this, our model is provided with data spanning a look-back window of TT preceding days, covering the interval [t‚àíT+1,t][t-T+1,t]. The input at each time tt is a multi-modal tuple ùêôt=(Xt,Nt,Mt)‚ààùí≥\mathbf{Z}\_{t}=(X\_{t},N\_{t},M\_{t})\in\mathcal{X}, where ùí≥\mathcal{X} is the input space. Each component of this tuple represents a distinct data modality.

First, Xt=(ùê±t‚àíT+1,‚Ä¶,ùê±t)‚àà‚ÑùT√ódxX\_{t}=(\mathbf{x}\_{t-T+1},\dots,\mathbf{x}\_{t})\in\mathbb{R}^{T\times d\_{x}} represents the quantitative market state. Each feature vector ùê±œÑ‚àà‚Ñùdx\mathbf{x}\_{\tau}\in\mathbb{R}^{d\_{x}} is a concatenation of indicators designed to capture different facets of market dynamics, including historical volatility, price trends, and aggregate news information. Its composition is formally expressed as:

|  |  |  |  |
| --- | --- | --- | --- |
| (1) |  | ùê±œÑ=[R‚ÄãVœÑ(d),R‚ÄãVœÑ(w),R‚ÄãVœÑ(m)‚èüRealized Volatility,MœÑ(w),MœÑ(m),MœÑ(q)‚èüMomentum,VœÑ(d)‚èüVolume,VIXœÑ(d)‚èüVIX,CœÑ(d)‚èüNews Count]‚ä§\small\begin{split}\mathbf{x}\_{\tau}=\;[&\underbrace{RV\_{\tau}^{(d)},RV\_{\tau}^{(w)},RV\_{\tau}^{(m)}}\_{\text{Realized Volatility}},\;\underbrace{M\_{\tau}^{(w)},M\_{\tau}^{(m)},M\_{\tau}^{(q)}}\_{\text{Momentum}},\\ &\qquad\underbrace{V\_{\tau}^{(d)}}\_{\text{Volume}},\;\underbrace{\text{VIX}\_{\tau}^{(d)}}\_{\text{VIX}},\;\underbrace{C\_{\tau}^{(d)}}\_{\text{News Count}}]^{\top}\end{split} |  |

Here, R‚ÄãVœÑ(d)RV\_{\tau}^{(d)}, R‚ÄãVœÑ(w)RV\_{\tau}^{(w)}, and R‚ÄãVœÑ(m)RV\_{\tau}^{(m)} denote the daily, weekly (5-day average), and monthly (22-day average) RVs observed up to day œÑ\tau. The terms MœÑ(w)M\_{\tau}^{(w)}, MœÑ(m)M\_{\tau}^{(m)}, and MœÑ(q)M\_{\tau}^{(q)} are weekly, monthly, and quarterly momentum indicators. The trading volume is VœÑ(d)V\_{\tau}^{(d)}. Finally, VIXœÑ(d)\text{VIX}\_{\tau}^{(d)} is the value of the VIX index, and CœÑ(d)C\_{\tau}^{(d)} is the count of relevant news articles for day œÑ\tau.

Second, Nt=(ùêßt‚àíT+1,‚Ä¶,ùêßt)‚àà‚ÑùT√ódnN\_{t}=(\mathbf{n}\_{t-T+1},\dots,\mathbf{n}\_{t})\in\mathbb{R}^{T\times d\_{n}} constitutes the qualitative information state. Each vector ùêßœÑ‚àà‚Ñùdn\mathbf{n}\_{\tau}\in\mathbb{R}^{d\_{n}} is a dense embedding derived from the titles and bodies of news articles published on day œÑ\tau, generated by a pre-trained transformer-based language model. The dimensionality dnd\_{n} corresponds to the output dimension of this language model. Finally, Mt=(ùê¶t‚àíT+1,‚Ä¶,ùê¶t)‚àà‚ÑùT√ódmM\_{t}=(\mathbf{m}\_{t-T+1},\dots,\mathbf{m}\_{t})\in\mathbb{R}^{T\times d\_{m}} contains temporal markers, where each ùê¶œÑ‚àà‚Ñùdm\mathbf{m}\_{\tau}\in\mathbb{R}^{d\_{m}} encodes calendar-related information (e.g., day of the week, month of the year) that captures known market seasonality. The prediction target, or ground truth, is a robust volatility estimator for day t+Ht+H, denoted as yt+H‚ààùí¥‚äÜ‚Ñù‚â•0y\_{t+H}\in\mathcal{Y}\subseteq\mathbb{R}\_{\geq 0}. Following established financial literature, we define this target as an aggregation of multiple volatility estimators to yield a more stable and comprehensive measure. Specifically, let VœÑPV\_{\tau}^{\text{P}}, VœÑGKV\_{\tau}^{\text{GK}}, and VœÑRSV\_{\tau}^{\text{RS}} represent the Parkinson (Parkinson, [1980](https://arxiv.org/html/2510.20699v1#bib.bib29)), Garman-Klass (Garman and Klass, [1980](https://arxiv.org/html/2510.20699v1#bib.bib18)), and Rogers-Satchell (Rogers and Satchell, [1991](https://arxiv.org/html/2510.20699v1#bib.bib32)) volatility estimators for day œÑ\tau, respectively. The target variable is then constructed as yt+H=fagg‚Äã(Vt+HP,Vt+HGK,Vt+HRS)y\_{t+H}=f\_{\text{agg}}(V\_{t+H}^{\text{P}},V\_{t+H}^{\text{GK}},V\_{t+H}^{\text{RS}}), where fagg‚Äã(‚ãÖ)f\_{\text{agg}}(\cdot) is a weighted averaging function (Clements et¬†al., [2024](https://arxiv.org/html/2510.20699v1#bib.bib9)).

Our model is a learnable function Œ¶\Phi with parameters Œ∏\theta, which maps an input instance ùêôt\mathbf{Z}\_{t} from the input space ùí≥\mathcal{X} to a volatility prediction y^t+H‚ààùí¥\hat{y}\_{t+H}\in\mathcal{Y}. The learning process is governed by a dual-objective function, designed to not only ensure predictive accuracy but also to foster a meaningful semantic alignment between the quantitative and qualitative data streams. The primary objective is the prediction loss, which we define using the standard Mean Squared Error (MSE) to penalize deviations between the predicted and actual volatility:

|  |  |  |  |
| --- | --- | --- | --- |
| (2) |  | ‚Ñípred‚Äã(Œ∏)=ùîº(ùêôt,yt+H)‚Äã[(Œ¶‚Äã(ùêôt;Œ∏)‚àíyt+H)2]\mathcal{L}\_{\text{pred}}(\theta)=\mathbb{E}\_{(\mathbf{Z}\_{t},y\_{t+H})}[(\Phi(\mathbf{Z}\_{t};\theta)-y\_{t+H})^{2}] |  |

To enhance the model‚Äôs ability to fuse information, we introduce an auxiliary alignment loss, ‚Ñíalign\mathcal{L}\_{\text{align}}, based on the principles of contrastive learning. This objective encourages the intermediate representations of the market state and the news state for the same timestep within the look-back window to be similar, while being dissimilar to representations from other timesteps (cf.(Liu et¬†al., [2024](https://arxiv.org/html/2510.20699v1#bib.bib26))). Let ùê°œÑquant\mathbf{h}\_{\tau}^{\text{quant}} and ùê°œÑqual\mathbf{h}\_{\tau}^{\text{qual}} be the latent representations for the quantitative and qualitative modalities at timestep œÑ‚àà[t‚àíT+1,t]\tau\in[t-T+1,t], respectively, derived from intermediate layers of Œ¶\Phi. The alignment loss is formulated using an InfoNCE objective, which minimizes the cross-entropy between aligned pairs over a set of negative samples. The final optimization problem is thus to find the parameters Œ∏‚ãÜ\theta^{\star} that minimize a weighted sum of these two objectives:

|  |  |  |  |
| --- | --- | --- | --- |
| (3) |  | Œ∏‚ãÜ=arg‚Å°minŒ∏‚Å°‚Ñí‚Äã(Œ∏)‚Äãwhere‚Äã‚Ñí‚Äã(Œ∏)=‚Ñípred‚Äã(Œ∏)+Œª‚Äã‚Ñíalign‚Äã(Œ∏)\small\theta^{\star}=\arg\min\_{\theta}\mathcal{L}(\theta)\quad\text{where}\quad\mathcal{L}(\theta)=\mathcal{L}\_{\text{pred}}(\theta)+\lambda\mathcal{L}\_{\text{align}}(\theta) |  |

Here, Œª>0\lambda>0 is a scalar hyperparameter that controls the relative importance of the alignment task. This multi-task learning paradigm is crucial for guiding the model to discover the complex, non-linear interactions between market dynamics and the narrative content of financial news.

The notations for the input data (Xt,Nt,MtX\_{t},N\_{t},M\_{t}), the model function (Œ¶\Phi), its parameters (Œ∏\theta), and the dual-objective function (‚Ñípred,‚Ñíalign\mathcal{L}\_{\text{pred}},\mathcal{L}\_{\text{align}}) will be used consistently in the following sections. We will now proceed to detail the specific architectural components of our proposed model, Œ¶\Phi, explaining how its design is tailored to effectively address the learning problem defined herein.

### 3.2. M2VN: Multi-Modal Volatility Network

Forecasting the aggregated RV yt+Hy\_{t+H} from the heterogeneous history ùêôt=(Xt,Nt,Mt)\mathbf{Z}\_{t}=(X\_{t},N\_{t},M\_{t}) requires a representation that is simultaneously faithful to the statistical regularities of price trajectories, sensitive to the semantic content of financial narratives, and aware of calendar-driven seasonality. The proposed architecture therefore processes the three modalities in a tightly coupled pipeline whose transformations preserve temporal coherence and promote cross-modal alignment, ultimately producing a single forecast y^t+H‚ààùí¥\hat{y}\_{t+H}\in\mathcal{Y}. Formally, let Xt‚àà‚ÑùT√ódxX\_{t}\in\mathbb{R}^{T\times d\_{x}}, Nt‚àà‚ÑùT√ódnN\_{t}\in\mathbb{R}^{T\times d\_{n}} and Mt‚àà‚ÑùT√ódmM\_{t}\in\mathbb{R}^{T\times d\_{m}} denote the input tensors for a given prediction time tt. The network is parametrized by Œ∏\theta and is organized into exactly three interacting components that operate sequentially on these inputs.

Multi-Modal Feature Encoder. The first component transforms each modality into a common latent space of width dd while retaining the full temporal resolution TT. Concretely, a learnable map ‚Ñ∞price:‚ÑùT√ódx‚Üí‚ÑùT√ód\mathcal{E}\_{\text{price}}:\mathbb{R}^{T\times d\_{x}}\to\mathbb{R}^{T\times d} embeds quantitative signals, using affine normalization followed by a periodic-spectral operator that decomposes the trajectory into its dominant frequency channels. In parallel, a projection ‚Ñ∞newsLLM:‚ÑùT√ódn‚Üí‚ÑùT√ód\mathcal{E}\_{\text{news}}^{\text{LLM}}:\mathbb{R}^{T\times d\_{n}}\to\mathbb{R}^{T\times d} distills the transformer-derived news embeddings; its weights are shared across dates so that semantic drift, rather than scale variance, drives the dynamics. Finally, a harmonic embedding ‚Ñ∞time:‚ÑùT√ódm‚Üí‚ÑùT√ód\mathcal{E}\_{\text{time}}:\mathbb{R}^{T\times d\_{m}}\to\mathbb{R}^{T\times d} encodes calendar features (Zhou et¬†al., [2021](https://arxiv.org/html/2510.20699v1#bib.bib40); Wu et¬†al., [2021](https://arxiv.org/html/2510.20699v1#bib.bib38)). The encoder outputs three sequences

|  |  |  |  |
| --- | --- | --- | --- |
| (4) |  | ùê°1:Tprice=‚Ñ∞price‚Äã(Xt)‚Äãùê°1:Tnews=‚Ñ∞newsLLM‚Äã(Nt)‚Äãùê°1:Ttime=‚Ñ∞time‚Äã(Mt)\small\mathbf{h}^{\text{price}}\_{1:T}=\mathcal{E}\_{\text{price}}(X\_{t})\quad\mathbf{h}^{\text{news}}\_{1:T}=\mathcal{E}\_{\text{news}}^{\text{LLM}}(N\_{t})\quad\mathbf{h}^{\text{time}}\_{1:T}=\mathcal{E}\_{\text{time}}(M\_{t}) |  |

and each belonging to (‚Ñùd)T(\mathbb{R}^{d})^{T}. For brevity, we denote their joint concatenation as ùê°1:T‚àà‚ÑùT√ó3‚Äãd\mathbf{h}\_{1:T}\in\mathbb{R}^{T\times 3d}. This tensor constitutes the raw material on which temporal reasoning is performed.

Note that employing a modern language model such as GPT-4.1 to embed the news stream NtN\_{t} can spuriously inflate forecasting accuracy, because the model‚Äôs training corpus may already contain future articles or hindsight knowledge. To neutralize this look-ahead bias, we adopt TiMaGPT (Drinkall et¬†al., [2024](https://arxiv.org/html/2510.20699v1#bib.bib16)). For every date œÑ\tau, the embedding ùêßœÑ\mathbf{n}\_{\tau} is generated with TiMaGPT weights whose knowledge cut-off has been rolled back to exactly one year before œÑ\tau. In practice, we advance the cutoff year-by-year during training, freeze the corresponding parameters, and feed the day‚Äôs news content ‚Äî obtained by concatenating all articles published on œÑ\tau ‚Äî through this frozen model to obtain a temporally faithful vector representation.

Latent Dynamics Module. The second component models long-range dependencies and cross-modal interactions over the horizon T+HT+H without resorting to causal truncation (Wu et¬†al., [2023](https://arxiv.org/html/2510.20699v1#bib.bib37)). Let ùê°1:T(0)=ùê°1:T\mathbf{h}^{(0)}\_{1:T}=\mathbf{h}\_{1:T}. A stack of LL identical blocks iteratively refines these embeddings, each block ùíØ‚Ñì:(‚ÑùT√ó3‚Äãd)‚Üí(‚ÑùT√ó3‚Äãd)\mathcal{T}\_{\ell}:(\mathbb{R}^{T\times 3d})\to(\mathbb{R}^{T\times 3d}) being defined as

|  |  |  |  |
| --- | --- | --- | --- |
| (5) |  | ùíØ‚Ñì‚Äã(ùê°)=ùê°+ùí¢‚Ñì‚Äã(ùíû‚Ñì‚Äã(ùê°)),\mathcal{T}\_{\ell}(\mathbf{h})=\mathbf{h}+\mathcal{G}\_{\ell}(\mathcal{C}\_{\ell}(\mathbf{h})), |  |

where ùíû‚Ñì\mathcal{C}\_{\ell} is a spectral‚Äìconvolution operator that exploits the discrete Fourier transform to isolate the kk most energetic periods of the price trajectory before applying an inception-style two-dimensional convolution across the period‚Äìchannel grid.

A key difficulty in volatility forecasting is teasing apart informative, low-frequency rhythms (e.g. weekly or monthly cycles) from high-frequency stochastic noise. To address this, proceeds in two steps (See (Wu et¬†al., [2023](https://arxiv.org/html/2510.20699v1#bib.bib37))):

* ‚Ä¢

  Spectral filtering. Transform the latent trajectory to the Fourier domain and keep the modes with the highest energy, isolating the most structurally significant components.
* ‚Ä¢

  Inception-style 2D convolution. Apply multi-scale kernels to the filtered signal to capture residual cross-temporal interactions.

The choice to select the top energy modes in the backbone is a principled decision, not a heuristic. As proven in Proposition¬†[3.1](https://arxiv.org/html/2510.20699v1#S3.Thmtheorem1 "Proposition 3.1 (Best ùëò-term Fourier approximation). ‚Ä£ 3.2. M2VN: Multi-Modal Volatility Network ‚Ä£ 3. Methodology ‚Ä£ Fusing Narrative Semantics for Financial Volatility Forecasting"), this yields the best k-term approximation that minimizes the Frobenius reconstruction error. This approach is a well-established principle in fields such as Fourier analysis, nonlinear approximation, and compressed sensing, where it has been shown that selecting the k largest transform coefficients provides an optimal approximation (Cohen et¬†al., [2006](https://arxiv.org/html/2510.20699v1#bib.bib11); Davenport et¬†al., [2012](https://arxiv.org/html/2510.20699v1#bib.bib13)). Consequently, this method imposes a strong inductive bias that encourages the network to focus on robust, periodic structures rather than overfitting to incidental noise. This built-in denoising mechanism can materially improve generalization and stability.

It is instructive to provide the following result, the proof of which follows from standard results (Cohen et¬†al., [2006](https://arxiv.org/html/2510.20699v1#bib.bib11); Davenport et¬†al., [2012](https://arxiv.org/html/2510.20699v1#bib.bib13)):

###### Proposition 3.1 (Best kk-term Fourier approximation).

Let ùê°=(h0,‚Ä¶,hT‚àí1)‚àà(‚ÑÇd)T\mathbf{h}=(h\_{0},\dots,h\_{T-1})\in(\mathbb{C}^{d})^{T} and denote by
ùê°^=‚Ñ±‚Äãùê°\widehat{\mathbf{h}}=\mathcal{F}\mathbf{h} its unitary discrete
Fourier transform

|  |  |  |  |
| --- | --- | --- | --- |
| (6) |  | ùê°^œâ=1T‚Äã‚àët=0T‚àí1ht‚Äãe‚àí2‚ÄãœÄ‚Äãi‚Äãœâ‚Äãt/T,œâ=0,‚Ä¶,T‚àí1.\widehat{\mathbf{h}}\_{\omega}=\frac{1}{\sqrt{T}}\sum\_{t=0}^{T-1}h\_{t}\,e^{-2\pi i\omega t/T},\qquad\omega=0,\dots,T-1. |  |

For 1‚â§k‚â§T1\leq k\leq T let
Ik‚äÜ{0,‚Ä¶,T‚àí1}I\_{k}\subseteq\{0,\dots,T-1\} collect the kk indices of largest
energy ‚Äñùê°^œâ‚Äñ2\|\widehat{\mathbf{h}}\_{\omega}\|\_{2} (ties broken arbitrarily) and set

|  |  |  |  |
| --- | --- | --- | --- |
| (7) |  | (ùí´k‚Äãùê°^)œâ={ùê°^œâ,œâ‚ààIk,0,œâ‚àâIk.ùê°k=‚Ñ±‚àí1‚Äã(ùí´k‚Äãùê°^).(\mathcal{P}\_{k}\widehat{\mathbf{h}})\_{\omega}=\begin{cases}\widehat{\mathbf{h}}\_{\omega},&\omega\in I\_{k},\\ 0,&\omega\notin I\_{k}.\end{cases}\qquad\mathbf{h}\_{k}=\mathcal{F}^{-1}(\mathcal{P}\_{k}\widehat{\mathbf{h}}). |  |

Then ùê°k\mathbf{h}\_{k} minimises the approximation error

|  |  |  |  |
| --- | --- | --- | --- |
| (8) |  | ùê°k=arg‚Äãminùê°~‚àà(‚ÑÇd)T‚Å°{‚Äñùê°‚àíùê°~‚ÄñF2‚à£‚Äñ‚Ñ±‚Äãùê°~‚Äñ0‚â§k},\mathbf{h}\_{k}=\operatorname\*{arg\,min}\_{\tilde{\mathbf{h}}\in(\mathbb{C}^{d})^{T}}\{\|\mathbf{h}-\tilde{\mathbf{h}}\|\_{F}^{2}\kern 5.0pt|\kern 5.0pt\|\mathcal{F}\tilde{\mathbf{h}}\|\_{0}\leq k\}, |  |

where ‚à•‚ãÖ‚à•F\|\cdot\|\_{F} is the Frobenius norm and
‚à•‚ãÖ‚à•0\|\cdot\|\_{0} counts non-zero Fourier coefficients.

###### Proof.

Parseval‚Äôs identity for the unitary DFT gives

|  |  |  |  |
| --- | --- | --- | --- |
| (9) |  | ‚Äñùê°‚àíùê°~‚ÄñF2=‚àëœâ=0T‚àí1‚Äñùê°^œâ‚àíùê°~^œâ‚Äñ22(‚àÄùê°~‚àà(‚ÑÇd)T).\|\mathbf{h}-\tilde{\mathbf{h}}\|\_{F}^{2}=\sum\_{\omega=0}^{T-1}\|\widehat{\mathbf{h}}\_{\omega}-\widehat{\tilde{\mathbf{h}}}\_{\omega}\|\_{2}^{2}\qquad(\forall\tilde{\mathbf{h}}\in(\mathbb{C}^{d})^{T}). |  |

If a candidate ùê°~\tilde{\mathbf{h}} has spectral support S={œâ:ùê°~^œâ‚â†0}S=\{\omega:\widehat{\tilde{\mathbf{h}}}\_{\omega}\neq 0\} with |S|‚â§k|S|\leq k, the summand for œâ‚ààS\omega\in S in ([9](https://arxiv.org/html/2510.20699v1#S3.E9 "In 3.2. M2VN: Multi-Modal Volatility Network ‚Ä£ 3. Methodology ‚Ä£ Fusing Narrative Semantics for Financial Volatility Forecasting")) is minimised by ùê°~^œâ=ùê°^œâ\widehat{\tilde{\mathbf{h}}}\_{\omega}=\widehat{\mathbf{h}}\_{\omega}, while the optimal choice for œâ‚àâS\omega\notin S is ùê°~^œâ=0\widehat{\tilde{\mathbf{h}}}\_{\omega}=0. Consequently the minimal achievable error for this support equals

|  |  |  |  |
| --- | --- | --- | --- |
| (10) |  | E‚Äã(S)=‚àëœâ‚àâS‚Äñùê°^œâ‚Äñ22.E(S)=\sum\_{\omega\notin S}\|\widehat{\mathbf{h}}\_{\omega}\|\_{2}^{2}. |  |

Because E‚Äã(S)E(S) decreases when ‚àëœâ‚ààS‚Äñùê°^œâ‚Äñ22\sum\_{\omega\in S}\|\widehat{\mathbf{h}}\_{\omega}\|\_{2}^{2} increases, a support of cardinality kk is optimal only when it captures the kk largest energies ‚Äî namely S=IkS=I\_{k} - yielding the value ‚àëœâ‚àâIk‚Äñùê°^œâ‚Äñ22\sum\_{\omega\notin I\_{k}}\|\widehat{\mathbf{h}}\_{\omega}\|\_{2}^{2}. Reconstructing via the inverse DFT produces the minimiser ùê°k\mathbf{h}\_{k} declared above.
‚àé

And then, the function ùí¢‚Ñì\mathcal{G}\_{\ell} performs gated cross-modal fusion. Given the price representation ùê´1:T\mathbf{r}\_{1:T} and the news representation ùê≠1:T\mathbf{t}\_{1:T} isolated from the input of the block, it computes a gate a1:T=œÉ‚Äã(Wg‚Äã[ùê´1:T;ùê≠1:T])‚àà(0,1)T√ó1a\_{1:T}=\sigma(W\_{g}[\mathbf{r}\_{1:T};\mathbf{t}\_{1:T}])\in(0,1)^{T\times 1} and returns

|  |  |  |  |
| --- | --- | --- | --- |
| (11) |  | ùê≥1:T=a1:T‚äôùê´1:T+(1‚àía1:T)‚äôùê≠1:T,\mathbf{z}\_{1:T}=a\_{1:T}\odot\mathbf{r}\_{1:T}+(1-a\_{1:T})\odot\mathbf{t}\_{1:T}, |  |

together with bilinear and absolute-difference interactions, concatenated and linearly projected back to ‚Ñùd\mathbb{R}^{d}. The entire sequence ùê°1:T(L)\mathbf{h}^{(L)}\_{1:T} emerging from the final block therefore contains price-aware news semantics and news-aware price spectra. To encourage semantic consistency across modalities, we extract aligned embeddings

|  |  |  |  |
| --- | --- | --- | --- |
| (12) |  | ùê´1:T=Pprice‚Äã(ùê°1:T(L)),ùê≠1:T=Pnews‚Äã(ùê°1:T(L)),\mathbf{r}\_{1:T}=P\_{\text{price}}(\mathbf{h}^{(L)}\_{1:T}),\qquad\mathbf{t}\_{1:T}=P\_{\text{news}}(\mathbf{h}^{(L)}\_{1:T}), |  |

where P‚àó:(‚ÑùT√ó3‚Äãd)‚Üí(‚ÑùT√óda)P\_{\ast}:(\mathbb{R}^{T\times 3d})\to(\mathbb{R}^{T\times d\_{a}}) are linear projections into an alignment space of width dad\_{a}. The InfoNCE loss (Oord et¬†al., [2018](https://arxiv.org/html/2510.20699v1#bib.bib28)) introduced in the Preliminary subsection is then realized as

|  |  |  |  |
| --- | --- | --- | --- |
| (13) |  | ‚Ñíalign‚Äã(Œ∏)=ùîº‚Äã[‚àílog‚Å°exp‚Å°(‚ü®ùê´œÑ,ùê≠œÑ‚ü©/œÑ)‚àëœÑ‚Ä≤exp‚Å°(‚ü®ùê´œÑ,ùê≠œÑ‚Ä≤‚ü©/œÑ)],\mathcal{L}\_{\text{align}}(\theta)=\mathbb{E}[-\log\frac{\exp(\langle\mathbf{r}\_{\tau},\mathbf{t}\_{\tau}\rangle/\tau)}{\sum\_{\tau^{\prime}}\exp(\langle\mathbf{r}\_{\tau},\mathbf{t}\_{\tau^{\prime}}\rangle/\tau)}], |  |

where ‚ü®‚ãÖ,‚ãÖ‚ü©\langle\cdot,\cdot\rangle denotes the inner product in ‚Ñùda\mathbb{R}^{d\_{a}} and œÑ>0\tau>0 is a learned temperature parameter.

Volatility Projection Head. The final component converts the fused latent sequence into a point prediction y^t+H\hat{y}\_{t+H}. A temporal projection Wp‚àà‚Ñù(T+H)√óTW\_{p}\in\mathbb{R}^{(T+H)\times T} is applied to the price-channel slice of ùê°1:T(L)\mathbf{h}^{(L)}\_{1:T}, yielding an extended sequence ùê°~1:T+H‚àà‚Ñù(T+H)√ód\tilde{\mathbf{h}}\_{1:T+H}\in\mathbb{R}^{(T+H)\times d} that extrapolates the learned dynamics up to the horizon HH. We retain the last position and pass it through an affine map Wo:‚Ñùd‚Üíùí¥W\_{o}:\mathbb{R}^{d}\to\mathcal{Y}, obtaining

|  |  |  |  |
| --- | --- | --- | --- |
| (14) |  | y^t+H=Wo‚Äã(ùê°~T+H).\hat{y}\_{t+H}=W\_{o}(\tilde{\mathbf{h}}\_{T+H}). |  |

A deterministic output is appropriate because the target yt+Hy\_{t+H} is defined as a smoothed aggregation of three volatility estimators and therefore concentrates most of its uncertainty in the inputs rather than in the measurement.

The network parameters are trained by minimizing the joint objective

|  |  |  |  |
| --- | --- | --- | --- |
| (15) |  | ùí•‚Äã(Œ∏)=ùîº(ùêôt,yt+H)‚Äã[(Œ¶‚Äã(ùêôt;Œ∏)‚àíyt+H)2]+Œª‚Äã‚Ñíalign‚Äã(Œ∏),\mathcal{J}(\theta)=\mathbb{E}\_{(\mathbf{Z}\_{t},y\_{t+H})}[(\Phi(\mathbf{Z}\_{t};\theta)-y\_{t+H})^{2}]+\lambda\,\mathcal{L}\_{\text{align}}(\theta), |  |

where the first term is the mean-squared prediction loss ‚Ñípred\mathcal{L}\_{\text{pred}} defined earlier and Œª>0\lambda>0 controls the relative weight of cross-modal alignment. Stochastic gradient descent with automatic differentiation is employed to solve Œ∏‚ãÜ=arg‚Å°minŒ∏‚Å°ùí•‚Äã(Œ∏)\theta^{\star}=\arg\min\_{\theta}\mathcal{J}(\theta).

| Method | KO | | CMCSA | | COP | | GILD | | MRK | | NKE | | ORCL | |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
|  | QLike | MAPE | QLike | MAPE | QLike | MAPE | QLike | MAPE | QLike | MAPE | QLike | MAPE | QLike | MAPE |
| HAR | 0.0775 | 0.2932 | 0.0937 | 0.3317 | 0.0749 | 0.2982 | 0.0674 | 0.3290 | 0.0804 | 0.3106 | 0.0766 | 0.3179 | 0.0972 | 0.3219 |
| HAR-X (OLS) | 0.0658 | 0.3507 | 0.0746 | 0.3317 | 0.0779 | 0.2889 | 0.0961 | 0.4447 | 0.1060 | 0.3705 | 0.0889 | 0.2946 | 0.1068 | 0.3185 |
| HAR-X (Ridge) | 0.0638 | 0.3484 | 0.0723 | 0.3388 | 0.0746 | 0.2873 | 0.0764 | 0.3942 | 0.0762 | 0.3361 | 0.0713 | 0.2946 | 0.0926 | 0.3082 |
| HAR-X (Lasso) | 0.0627 | 0.3535 | 0.0692 | 0.3216 | 0.0594 | 0.2818 | 0.0716 | 0.4043 | 0.0685 | 0.3475 | 0.0646 | 0.2873 | 0.0809 | 0.3047 |
| Informer | 0.1629 | 0.4028 | 0.1125 | 0.3403 | 0.1399 | 0.3519 | 0.1007 | 0.4035 | 0.1959 | 0.3311 | 0.1453 | 0.3017 | 0.2443 | 0.4125 |
| Autoformer | 0.1095 | 0.3756 | 0.1558 | 0.4036 | 0.1480 | 0.3625 | 0.1089 | 0.3622 | 0.0906 | 0.3420 | 0.1120 | 0.3297 | 0.2119 | 0.4302 |
| DLinear | 0.0845 | 0.3371 | 0.0928 | 0.3355 | 0.0665 | 0.3083 | 0.0688 | 0.2977 | 0.0792 | 0.3174 | 0.0744 | 0.3103 | 0.1006 | 0.3614 |
| TimesNet | 0.0666 | 0.2943 | 0.0833 | 0.3177 | 0.0652 | 0.2839 | 0.0660 | 0.2852 | 0.0760 | 0.3065 | 0.0679 | 0.2905 | 0.0864 | 0.3182 |
| PAttn | 0.0699 | 0.3046 | 0.0805 | 0.3127 | 0.0603 | 0.2888 | 0.0678 | 0.2853 | 0.0699 | 0.3031 | 0.0695 | 0.3038 | 0.0959 | 0.3099 |
| TimeXer | 0.0661 | 0.3040 | 0.0807 | 0.3162 | 0.0589 | 0.3035 | 0.0673 | 0.2760 | 0.0797 | 0.2909 | 0.0671 | 0.2865 | 0.0849 | 0.3255 |
| M2VN (Ours) | 0.0589 | 0.2771 | 0.0769 | 0.3025 | 0.0552 | 0.2722 | 0.0630 | 0.2838 | 0.0679 | 0.3005 | 0.0594 | 0.2747 | 0.0696 | 0.3092 |

Table 1. Out-of-sample forecasting accuracy of ten statistical and deep learning models for daily volatility of seven U.S. stocks. Results are for quasi-likelihood loss (QLike) and mean absolute percentage error (MAPE). For each stock‚Äìmetric pair, the best model is in red, second-best in blue. Values are averages of three runs; standard deviations are sufficiently small and omitted.



| Ablation | KO | | CMCSA | | COP | | GILD | | MRK | | NKE | | ORCL | |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
|  | QLike | MAPE | QLike | MAPE | QLike | MAPE | QLike | MAPE | QLike | MAPE | QLike | MAPE | QLike | MAPE |
| M2VN | 0.0589 | 0.2771 | 0.0769 | 0.3025 | 0.0552 | 0.2722 | 0.0630 | 0.2838 | 0.0679 | 0.3005 | 0.0594 | 0.2747 | 0.0696 | 0.3092 |
| M2VN ‚Äã W/O VœÑ(d)V\_{\tau}^{(d)}‚Äã‚Äã ‚Äã‚Äã ‚Äã‚Äã | 0.0610 | 0.2783 | 0.0812 | 0.3105 | 0.0580 | 0.2822 | 0.0659 | 0.2877 | 0.0701 | 0.3100 | 0.0648 | 0.3039 | 0.0774 | 0.3076 |
| M2VN ‚Äã W/O NœÑN\_{\tau}‚Äã‚Äã ‚Äã‚Äã ‚Äã‚Äã | 0.0641 | 0.2904 | 0.0866 | 0.3184 | 0.0599 | 0.3000 | 0.0632 | 0.2851 | 0.0692 | 0.3169 | 0.0638 | 0.2912 | 0.0780 | 0.3251 |

Table 2. Ablation study on the impact of the daily volume feature (VœÑ(d)V\_{\tau}^{(d)}) in M2VN forecasting, as well as impact of news modality on M2VN volatility forecasting performance. Bold indicates the best result for each stock-metric pair.

## 4. Experiments

Now we present experiment results to thoroughly demonstrate the performance of M2VN on real-world benchmark datasets. The pre-processed dataset, raw news article data, and implementation code are publicly accessible at https://github.com/Yoontae6719/M2VN-Multi-Modal-Learning-Network-for-Volatility-Forecasting

### 4.1. Implementation Details

We present the details of datasets, baseline models, training details and evaluation metrics.

Data Sources. We construct a unified daily panel comprising seven large-capitalization U.S. equities ‚Äî Coca-Cola (KO), Comcast (CMCSA), ConocoPhillips (COP), Gilead Sciences (GILD), Merck & Co. (MRK), Nike (NKE) and Oracle (ORCL). For each ticker, we extract both end-of-day price information (open, high, low, close and volume) and the full text of associated news articles from FNSPID (Dong et¬†al., [2024](https://arxiv.org/html/2510.20699v1#bib.bib15)). The FNSPID dataset (Dong et¬†al., [2024](https://arxiv.org/html/2510.20699v1#bib.bib15)) is an open-source resource that integrates financial news articles with corresponding time series data. All news articles are temporally aligned on a daily basis in a structured format. Owing to the limited availability of news data, above seven tickers were selected according to the volume of associated articles.

Data Processing.
Both news and time series data are partitioned chronologically into a training span from 1 January 2013 to 31 December 2017, a validation span from 1 January 2018 to 31 December 2020, and a strictly held-out test span from 1 January 2021 to 15 December 2023. News documents are temporally aligned so that only articles published on the calendar day before as the corresponding price record enter the model‚Äôs input for that day, thereby precluding any forward-looking leakage.

Baseline models. To evaluate the contribution of M2VN, we compare it against ten strong baselines spanning both classical econometrics and contemporary deep learning. The classical group contains Standard HAR, HAR-X (OLS), HAR-X (Lasso) and HAR-X (Ridge), variants of the Heterogeneous AutoRegressive model widely used in volatility forecasting (Clements et¬†al., [2024](https://arxiv.org/html/2510.20699v1#bib.bib9)). The modern deep learning group encompasses Informer(Zhou et¬†al., [2021](https://arxiv.org/html/2510.20699v1#bib.bib40)), Autoformer(Wu et¬†al., [2021](https://arxiv.org/html/2510.20699v1#bib.bib38)), DLinear(Zeng et¬†al., [2023](https://arxiv.org/html/2510.20699v1#bib.bib39)), TimesNet(Wu et¬†al., [2023](https://arxiv.org/html/2510.20699v1#bib.bib37)), TimeXer(Wang et¬†al., [2024](https://arxiv.org/html/2510.20699v1#bib.bib36)) and the PAttn (Tan et¬†al., [2024](https://arxiv.org/html/2510.20699v1#bib.bib34)) architecture. For all three HAR-X baselines, we first reduce the dimensionality of the daily news embeddings via principal-component analysis (PCA), retaining the top components that explain 95% of variance. This mitigates the curse of dimensionality while keeping the econometric specification parsimonious. All deep learning baselines ingest the raw news embeddings directly.

Training detail. The model is trained using a sequence length of 12 time steps, where the last 2 steps serve as contextual labels, and it forecasts 1 step into the future. All deep learning baselines and our M2VN model are optimized with Adam, an initial learning rate of 3√ó10‚àí43\times 10^{-4}, and cosine decay. Early stopping on the validation set prevents over-fitting. In our models, hyper-parameters are selected by random search, drawing 100 configurations from the following grids: latent dimension d‚àà{12,24,32,64}d\in\{12,24,32,64\}, alignment dimension dŒ±‚àà{32,64,128,256}d\_{\alpha}\in\{32,64,128,256\}, temperature œÑ‚àà{0.01,0.03,0.07}\tau\in\{0.01,0.03,0.07\}, Inception hidden width di‚àà{24,32,64,128,256,512}d\_{i}\in\{24,32,64,128,256,512\} and top-kk signal k‚àà{4,5,6}k\in\{4,5,6\} . Equivalent searches (with architecture-specific ranges) are conducted for every deep baseline to ensure a fair comparison. Each experiment is repeated with three random seeds. we report the mean across runs.

Evaluation metrics. Model performance is quantified with Mean Absolute Percentage Error (MAPE) and the Quasi-Likelihood loss (QLIKE). MAPE delivers an easily interpretable, scale-free percentage error that treats over- and under-predictions symmetrically. QLIKE, rooted in the quasi-log-likelihood of a Gaussian volatility model, places extra weight on under-forecasting during high-volatility episodes. For a forecast y^t\hat{y}\_{t} of the realised volatility yty\_{t} over
nn test points, they are defined as

|  |  |  |  |
| --- | --- | --- | --- |
| (16) |  | MAPE=100%n‚Äã‚àët=1n|yt‚àíy^tyt|,\small\text{MAPE}=\frac{100\%}{n}\sum\_{t=1}^{n}|\frac{y\_{t}-\hat{y}\_{t}}{y\_{t}}|,\quad |  |

|  |  |  |  |
| --- | --- | --- | --- |
| (17) |  | QLIKE=1n‚Äã‚àëi=1n(yiy^i‚àíln‚Å°(yiy^i)‚àí1).\small\text{QLIKE}=\frac{1}{n}\sum\_{i=1}^{n}\left(\frac{y\_{i}}{\hat{y}\_{i}}-\ln\!\left(\frac{y\_{i}}{\hat{y}\_{i}}\right)-1\right). |  |

As our model directly forecasts volatility, we define both yty\_{t} and y^\hat{y} in terms of standard deviation for the QLIKE calculation. While the QLIKE metric is conventionally applied to variance, this consistent application to standard deviation allows it to serve as a robust tool for the internal ranking of model performance. It preserves the essential characteristic of penalizing under-prediction more heavily, which is crucial for our comparative study.

![Refer to caption](x3.png)


Figure 3. Visual comparison of out-of-sample forecast quality for the KO (top) and CMCSA (bottom) datasets over the test period of 2021-2023 . The M2VN model‚Äôs predictions (green) are plotted against the ground truth realized volatility (black), alongside forecasts from the HAR-X (Lasso) (orange) and TimeXer (blue) baselines. The M2VN demonstrates a superior ability to track the fluctuations and sharp peaks of the ground truth data compared to the other models.

### 4.2. M2VN is Useful for Volatility Forecasting?

To answer the central research question of whether our proposed multi-modal framework improves volatility forecasting, we conduct a comprehensive empirical evaluation against a suite of strong classical and deep learning baselines. This section details the out-of-sample performance of M2VN, whose architecture and learning objectives are described in Section [3](https://arxiv.org/html/2510.20699v1#S3 "3. Methodology ‚Ä£ Fusing Narrative Semantics for Financial Volatility Forecasting"), on seven real-world equity datasets.

The quantitative results are presented in Table [1](https://arxiv.org/html/2510.20699v1#S3.T1 "Table 1 ‚Ä£ 3.2. M2VN: Multi-Modal Volatility Network ‚Ä£ 3. Methodology ‚Ä£ Fusing Narrative Semantics for Financial Volatility Forecasting"). Our proposed model, M2VN, demonstrates consistently strong performance, outperforming eight out of ten baselines in the majority of stock-metric settings. Specifically, M2VN achieves either the best (highlighted in red) or second-best (in blue) result in 12 out of 14 evaluation settings (across 7 stocks and 2 metrics). Notably, it secures the top rank in ten cases (71.4%) and places within the top two in the vast majority of settings. While the HAR-X (Lasso) baseline yields some competitive results, obtaining the best performance in two settings (CMCSA‚ÄìQLike and ORCL‚ÄìMAPE), M2VN either matches or exceeds Lasso‚Äôs accuracy in most other stock‚Äìmetric combinations, highlighting its overall superiority. Relative to TimesNet (Wu et¬†al., [2023](https://arxiv.org/html/2510.20699v1#bib.bib37)), an ablation of our backbone that omits the cross-modal alignment loss ‚Ñíalign\mathcal{L}\_{\text{align}}, mean QLike and MAPE fall by 11.7 % and 3.6 %, respectively.

The superior performance of M2VN can be attributed to several key architectural innovations. First, the explicit fusion of quantitative market data and qualitative news narratives, enforced by the auxiliary contrastive alignment loss (‚Ñíalign\mathcal{L}\_{\text{align}}), enables the model to learn a richer, more robust representation of the underlying market state than models relying on price data alone. Second, the latent dynamics module, which leverages spectral filtering to isolate the dominant periodic components of the volatility signal (cf. Proposition [3.1](https://arxiv.org/html/2510.20699v1#S3.Thmtheorem1 "Proposition 3.1 (Best ùëò-term Fourier approximation). ‚Ä£ 3.2. M2VN: Multi-Modal Volatility Network ‚Ä£ 3. Methodology ‚Ä£ Fusing Narrative Semantics for Financial Volatility Forecasting")), effectively denoised the input sequences. This structural inductive bias may encourages the model to focus on persistent, predictable patterns rather than overfitting to stochastic noise, a common pitfall in financial TSF. These empirical results suggest that the architectural choices embodied in M2VN are suitable for the complex task of volatility forecasting.

### 4.3. Is Volume a Proxy for News Information?

To investigate the intricate relationship between market activity and narrative information, we examine whether trading volume can serve as a proxy for news. According to the efficient market hypothesis, news is rapidly incorporated into prices, with trading volume often reflecting the arrival and processing of such information. This raises the question of whether the information contained in news is subsumed by the more readily available quantitative volume data.

To test this hypothesis, we conduct a targeted ablation study on our M2VN model. Specifically, we compare the performance of the full model ‚Äî which leverages both daily trading volume (VœÑ(d)V\_{\tau}^{(d)}) from the quantitative market state (XtX\_{t}) and news embeddings (NtN\_{t}) - against an identical model with the volume feature ablated. In this alternative configuration, the model relies on news embeddings and other market variables to capture information flow. If trading volume were merely a proxy for news, removing it from a model already equipped with rich news representations should yield only marginal changes in performance.

The results of this experiment are summarized in Table [2](https://arxiv.org/html/2510.20699v1#S3.T2 "Table 2 ‚Ä£ 3.2. M2VN: Multi-Modal Volatility Network ‚Ä£ 3. Methodology ‚Ä£ Fusing Narrative Semantics for Financial Volatility Forecasting"). Excluding trading volume results in a statistically significant and consistent reduction in forecasting accuracy across all seven equities. The complete model (with volume, standard M2VN) outperforms its ablated counterpart (without volume, M2VN W/O VœÑ(d)V\_{\tau}^{(d)}) on 27 out of 28 stock-metric pairs. On average, incorporating volume reduces MAPE by 2.86¬±\pm3.06% and QLike by 5.64¬±\pm2.40% across all stocks.

These findings suggest that trading volume is not merely a proxy for news but rather a source of complementary information. The substantial improvement in predictive accuracy achieved by including volume ‚Äî even alongside rich textual news features ‚Äî indicates that volume captures unique aspects of market dynamics. These may capture investor disagreement, liquidity, or the intensity of market attention, which are not fully reflected in the semantic content of news alone. Therefore, to develop high-fidelity financial forecasting models, it is crucial to regard both news narratives and trading volume as essential, non-redundant sources of information.

### 4.4. Is News Embedding Predictive?

To ascertain the direct contribution of news embeddings to the predictive capability of the M2VN model, an ablation study was conducted by comparing the model‚Äôs performance with and without the news modality (NtN\_{t}). Table [2](https://arxiv.org/html/2510.20699v1#S3.T2 "Table 2 ‚Ä£ 3.2. M2VN: Multi-Modal Volatility Network ‚Ä£ 3. Methodology ‚Ä£ Fusing Narrative Semantics for Financial Volatility Forecasting") presents the out-of-sample forecasting accuracy for all seven U.S. stocks across two key metrics: Quasi-Likelihood Loss (QLike) and Mean Absolute Percentage Error (MAPE). The results unequivocally demonstrate that the inclusion of news embeddings consistently enhances forecasting performance.

Across all evaluated stocks and both metrics, the M2VN model with news embeddings (standard M2VN) significantly outperforms its counterpart without news embeddings (W/O NtN\_{t}). For instance, in the case of Coca-Cola (KO), the QLike decreased from 0.0641 to 0.0589 and MAPE from 0.2904 to 0.2771 when news was incorporated. Similar improvements are observed for Comcast (CMCSA), where QLike dropped from 0.0866 to 0.0769 and MAPE from 0.3184 to 0.3025. Notably, for stocks like ConocoPhillips (COP) and Oracle (ORCL), the performance gaps are particularly pronounced, with substantial reductions in both QLike and MAPE, indicating that news content provides highly meaningful signals for these equities. For COP, QLike improved from 0.0599 to 0.0552 and MAPE from 0.3000 to 0.2722. Similarly, for ORCL, QLike went from 0.0780 to 0.0696 and MAPE from 0.3251 to 0.3092.

While the benefit of news integration is universal, the magnitude of improvement varies across stocks. For example, Gilead Sciences (GILD) shows a comparatively smaller performance gain (0.0632 vs. 0.0630 for QLike, and 0.2851 vs. 0.2838 for MAPE), suggesting that its volatility dynamics might be less influenced by publicly available news narratives, or that its price movements are predominantly driven by other factors captured within the quantitative market state. Conversely, stocks exhibiting larger gains, such as COP and ORCL, may have volatility profiles more responsive to specific news events or broader market sentiment conveyed through news. This differentiation could arise from factors like sector-specific news impact, company-specific event frequency, or the general information efficiency of the stock. News embeddings likely contribute by providing timely, qualitative insights into market-moving events or shifts in sentiment that are not immediately captured by historical price data alone. Their effectiveness in certain cases, and their less pronounced impact in others, underscore the interconnected relationship between qualitative information and asset price dynamics. Overall, the consistent outperformance of the model incorporating news embeddings provides strong evidence that this modality contributes meaningful predictive signals for volatility forecasting within the M2VN multi-modal architecture.

### 4.5. Showcasing Forecast Quality

To supplement the aggregate metrics in Table¬†[1](https://arxiv.org/html/2510.20699v1#S3.T1 "Table 1 ‚Ä£ 3.2. M2VN: Multi-Modal Volatility Network ‚Ä£ 3. Methodology ‚Ä£ Fusing Narrative Semantics for Financial Volatility Forecasting"), this section qualitatively compares M2VN‚Äôs out-of-sample forecasts against ground truth and two strong baselines ‚Äî HAR-X (Lasso) and TimeXer ‚Äî on the CMCSA and KO datasets (2021‚Äì2023), as shown in Figure¬†[3](https://arxiv.org/html/2510.20699v1#S4.F3 "Figure 3 ‚Ä£ 4.1. Implementation Details ‚Ä£ 4. Experiments ‚Ä£ Fusing Narrative Semantics for Financial Volatility Forecasting"). Notably, while all models struggle to predict the exact magnitude of extreme, single-day volatility shocks (e.g., the CMCSA spike in early 2022), M2VN more effectively captures these peaks compared to the baselines. In contrast, HAR-X (Lasso) and TimeXer tend to produce overly smoothed forecasts that underestimate sharp spikes, with HAR-X (Lasso) particularly weak in tracking the post-event decay. This results in persistent over-prediction during calmer periods, as seen with HAR-X (Lasso) in KO dataset from August 2022 to January 2023 and April to June 2023. Although M2VN may underestimate the absolute height of extreme movements, it reliably identifies the direction and timing of sharp upward shifts, while the baselines often produce muted or delayed responses. This qualitative improvement is significant, as forecasting rare volatility shocks is a well-known challenge in financial modeling due to their underrepresentation in training data, and M2VN‚Äôs ability to anticipate such events marks a notable advance over existing methods.

## 5. Conclusion

This study introduced M2VN, a novel deep learning framework that effectively unifies structured financial time series with unstructured news narratives for volatility forecasting. By leveraging temporally faithful news embeddings from TiMaGPT and cross-modal alignment through a contrastive loss, M2VN tackles key challenges of data modality fusion and look-ahead bias. Experiments on large-cap U.S. equities show that M2VN consistently outperforms both classical econometric models and deep learning baselines, achieving higher accuracy in both MAPE and QLike metrics. Ablation studies further confirm that both trading volume and news embeddings provide unique, complementary predictive signals. These findings highlight the value of cross-modal integration for improved volatility modeling and financial risk management.

###### Acknowledgements.

The authors would like to thank the reviewers for their insightful and detailed comments. Y.K. gratefully acknowledges financial support from Deutsche Bank AG.

## References

* (1)
* Alexander (2008)

  Carol Alexander. 2008.
  *Market risk analysis, practical financial econometrics*. Vol.¬†2.
  John Wiley & Sons.
* Andersen and Bollerslev (1998)

  Torben¬†G Andersen and Tim Bollerslev. 1998.
  Answering the skeptics: Yes, standard volatility models do provide accurate forecasts.
  *International economic review* (1998), 885‚Äì905.
* Andersen et¬†al. (2006)

  Torben¬†G Andersen, Tim Bollerslev, Peter¬†F Christoffersen, and Francis¬†X Diebold. 2006.
  Volatility and correlation forecasting.
  *Handbook of economic forecasting* 1 (2006), 777‚Äì878.
* Ansari et¬†al. (2024)

  Abdul¬†Fatir Ansari, Lorenzo Stella, Caner Turkmen, Xiyuan Zhang, Pedro Mercado, Huibin Shen, Oleksandr Shchur, Syama¬†Sundar Rangapuram, Sebastian¬†Pineda Arango, Shubham Kapoor, et¬†al. 2024.
  Chronos: Learning the language of time series.
  *arXiv preprint arXiv:2403.07815* (2024).
* Atkins et¬†al. (2018)

  Adam Atkins, Mahesan Niranjan, and Enrico Gerding. 2018.
  Financial news predicts stock market volatility better than close price.
  *The Journal of Finance and Data Science* 4, 2 (2018), 120‚Äì137.
* Audrino et¬†al. (2020)

  Francesco Audrino, Fabio Sigrist, and Daniele Ballinari. 2020.
  The impact of sentiment and attention measures on stock market volatility.
  *International Journal of Forecasting* 36, 2 (2020), 334‚Äì357.
* Christensen et¬†al. (2023)

  Kim Christensen, Mathias Siggaard, and Bezirgen Veliyev. 2023.
  A machine learning approach to volatility forecasting.
  *Journal of Financial Econometrics* 21, 5 (2023), 1680‚Äì1727.
* Clements et¬†al. (2024)

  Adam Clements, Daniel Preve, and Clarence Tee. 2024.
  Harvesting the HAR-X Volatility Model.
  *Available at SSRN 4733597* (2024).
* Clements and Preve (2021)

  Adam Clements and Daniel¬†PA Preve. 2021.
  A practical guide to harnessing the har volatility model.
  *Journal of Banking & Finance* 133 (2021), 106285.
* Cohen et¬†al. (2006)

  Albert Cohen, Wolfgang Dahmen, and Ronald DeVore. 2006.
  *Compressed Sensing and Best *k*-Term Approximation*.
  Technical Report IGPM Report 260. RWTH Aachen University.

  <https://www.igpm.rwth-aachen.de/Download/reports/pdf/IGPM260.pdf>
  Technical report, later published in J. Amer. Math. Soc. 22(1):211‚Äì231, 2009.
* Corsi (2009)

  Fulvio Corsi. 2009.
  A simple approximate long-memory model of realized volatility.
  *Journal of Financial Econometrics* 7, 2 (2009), 174‚Äì196.
* Davenport et¬†al. (2012)

  Mark¬†A Davenport, Marco¬†F Duarte, Yonina¬†C Eldar, and Gitta Kutyniok. 2012.
  *Introduction to compressed sensing*.
  Cambridge University Press. 1‚Äì64 pages.
* Degiannakis and Filis (2017)

  Stavros Degiannakis and George Filis. 2017.
  Forecasting oil price realized volatility using information channels from other asset classes.
  *Journal of International Money and Finance* 76 (2017), 28‚Äì49.
* Dong et¬†al. (2024)

  Zihan Dong, Xinyu Fan, and Zhiyuan Peng. 2024.
  Fnspid: A comprehensive financial news dataset in time series. In *Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*. 4918‚Äì4927.
* Drinkall et¬†al. (2024)

  Felix Drinkall, Eghbal Rahimikia, Janet¬†B Pierrehumbert, and Stefan Zohren. 2024.
  Time machine GPT.
  *arXiv preprint arXiv:2404.18543* (2024).
* Engle and Bollerslev (1986)

  Robert¬†F Engle and Tim Bollerslev. 1986.
  Modelling the persistence of conditional variances.
  *Econometric reviews* 5, 1 (1986), 1‚Äì50.
* Garman and Klass (1980)

  Mark¬†B Garman and Michael¬†J Klass. 1980.
  On the estimation of security price volatilities from historical data.
  *Journal of business* (1980), 67‚Äì78.
* Gruver et¬†al. (2023)

  Nate Gruver, Marc Finzi, Shikai Qiu, and Andrew¬†G Wilson. 2023.
  Large language models are zero-shot time series forecasters.
  *Advances in Neural Information Processing Systems* 36 (2023), 19622‚Äì19635.
* Hwang et¬†al. (2025)

  Yoontae Hwang, Yaxuan Kong, Stefan Zohren, and Yongjae Lee. 2025.
  Decision-informed Neural Networks with Large Language Model Integration for Portfolio Optimization.
  *arXiv preprint arXiv:2502.00828* (2025).
* Jin et¬†al. (2023)

  Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James¬†Y Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, et¬†al. 2023.
  Time-llm: Time series forecasting by reprogramming large language models.
  *arXiv preprint arXiv:2310.01728* (2023).
* Kim et¬†al. (2024)

  Alex Kim, Maximilian Muhn, and Valeri¬†V Nikolaev. 2024.
  Financial Statement Analysis with Large Language Models.
  *Chicago Booth Research Paper Forthcoming, Fama-Miller Working Paper* (2024).
* Kong et¬†al. (2024a)

  Yaxuan Kong, Yuqi Nie, Xiaowen Dong, John¬†M Mulvey, H¬†Vincent Poor, Qingsong Wen, and Stefan Zohren. 2024a.
  Large Language Models for Financial and Investment Management: Applications and Benchmarks.
  *Journal of Portfolio Management* 51, 2 (2024).
* Kong et¬†al. (2024b)

  Yaxuan Kong, Yuqi Nie, Xiaowen Dong, John¬†M Mulvey, H¬†Vincent Poor, Qingsong Wen, and Stefan Zohren. 2024b.
  Large Language Models for Financial and Investment Management: Models, Opportunities, and Challenges.
  *Journal of Portfolio Management* 51, 2 (2024).
* Korkusuz (2024)

  Burak Korkusuz. 2024.
  Beyond the S&P 500: examining the role of external volatilities in market forecasting.
  *Review of Economic Design* (2024), 1‚Äì28.
* Liu et¬†al. (2024)

  Haoxin Liu, Shangqing Xu, Zhiyuan Zhao, Lingkai Kong, Harshavardhan Prabhakar¬†Kamarthi, Aditya Sasanur, Megha Sharma, Jiaming Cui, Qingsong Wen, Chao Zhang, et¬†al. 2024.
  Time-mmd: Multi-domain multimodal dataset for time series analysis.
  *Advances in Neural Information Processing Systems* 37 (2024), 77888‚Äì77933.
* Ly√≥csa et¬†al. (2021)

  ≈†tefan Ly√≥csa, Peter Moln√°r, and Tom√°≈° V·ª≥rost. 2021.
  Stock market volatility forecasting: Do we need high-frequency data?
  *International Journal of Forecasting* 37, 3 (2021), 1092‚Äì1110.
* Oord et¬†al. (2018)

  Aaron van¬†den Oord, Yazhe Li, and Oriol Vinyals. 2018.
  Representation learning with contrastive predictive coding.
  *arXiv preprint arXiv:1807.03748* (2018).
* Parkinson (1980)

  Michael Parkinson. 1980.
  The extreme value method for estimating the variance of the rate of return.
  *Journal of business* (1980), 61‚Äì65.
* Parvini and Assa (2025)

  Navid Parvini and Hirbod Assa. 2025.
  Textual Regression for Realized Volatility: A Model for Long-Term Forecasting.
  *Available at SSRN* (2025).
* Rahimikia et¬†al. (2021)

  Eghbal Rahimikia, Stefan Zohren, and Ser-Huang Poon. 2021.
  Realised volatility forecasting: Machine learning via financial word embedding.
  *arXiv preprint arXiv:2108.00480* (2021).
* Rogers and Satchell (1991)

  L¬†Christopher¬†G Rogers and Stephen¬†E Satchell. 1991.
  Estimating variance from high, low and closing prices.
  *The Annals of Applied Probability* (1991), 504‚Äì512.
* Sarkar and Vafa (2024)

  Suproteem¬†K Sarkar and Keyon Vafa. 2024.
  Lookahead bias in pretrained language models.
  *Available at SSRN* (2024).
* Tan et¬†al. (2024)

  Mingtian Tan, Mike Merrill, Vinayak Gupta, Tim Althoff, and Tom Hartvigsen. 2024.
  Are language models actually useful for time series forecasting?
  *Advances in Neural Information Processing Systems* 37 (2024), 60162‚Äì60191.
* Wan et¬†al. (2021)

  Xingchen Wan, Jie Yang, Slavi Marinov, Jan-Peter Calliess, Stefan Zohren, and Xiaowen Dong. 2021.
  Sentiment correlation in financial news networks and associated market movements.
  *Scientific reports* 11, 1 (2021), 3062.
* Wang et¬†al. (2024)

  Yuxuan Wang, Haixu Wu, Jiaxiang Dong, Yong Liu, Yunzhong Qiu, Haoran Zhang, Jianmin Wang, and Mingsheng Long. 2024.
  Timexer: Empowering transformers for time series forecasting with exogenous variables.
  *Advances in Neural Information Processing Systems* (2024).
* Wu et¬†al. (2023)

  Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. 2023.
  TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis. In *International Conference on Learning Representations*.
* Wu et¬†al. (2021)

  Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. 2021.
  Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting.
  *Advances in neural information processing systems* 34 (2021), 22419‚Äì22430.
* Zeng et¬†al. (2023)

  Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. 2023.
  Are transformers effective for time series forecasting?. In *Proceedings of the AAAI conference on artificial intelligence*, Vol.¬†37. 11121‚Äì11128.
* Zhou et¬†al. (2021)

  Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. 2021.
  Informer: Beyond efficient transformer for long sequence time-series forecasting. In *Proceedings of the AAAI conference on artificial intelligence*, Vol.¬†35. 11106‚Äì11115.
* Zhou et¬†al. (2023)

  Tian Zhou, Peisong Niu, Liang Sun, Rong Jin, et¬†al. 2023.
  One fits all: Power general time series analysis by pretrained lm.
  *Advances in neural information processing systems* 36 (2023), 43322‚Äì43355.