---
authors:
- Yuchao Dong
- Harry Zheng
doc_id: arxiv:2510.24128v1
family_id: arxiv:2510.24128
is_current: true
taxonomy:
  alpha_families: []
  asset_classes: []
  horizons: []
  themes: []
title: 'Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization
  Method'
url_abs: http://arxiv.org/abs/2510.24128v1
url_html: https://arxiv.org/html/2510.24128v1
venue: arXiv q-fin
version: 1
year: 2025
---


Yuchao Dong
and Harry Zheng
 School of Mathematical Sciences, Key Laboratory of Intelligent Computing and Applications (Ministry of Education), Tongji University, Shanghai 200092, China. The work of the second author was funded by National Natural Science Foundation of China
(No.12471425) Department of Mathematics, Imperial College, London SW7 2BZ, UK (Email: h.zheng@imperial.ac.uk). The work was supported by the Engineering and Physical Sciences Research Council of
UK (Grant No. EP/V008331/1).

###### Abstract

This paper studies the time-inconsistent MV optimal stopping problem via a game-theoretic approach to find equilibrium strategies. To overcome the mathematical intractability of direct equilibrium analysis, we propose a vanishing regularization method: first, we introduce an entropy-based regularization term to the MV objective, modeling mixed-strategy stopping times using the intensity of a Cox process. For this regularized problem, we derive a coupled extended Hamilton-Jacobi-Bellman (HJB) equation system, prove a verification theorem linking its solutions to equilibrium intensities, and establish the existence of classical solutions for small time horizons via a contraction mapping argument. By letting the regularization term tend to zero, we formally recover a system of parabolic variational inequalities that characterizes equilibrium stopping times for the original MV problem. This system includes an additional key quadratic termâ€“a distinction from classical optimal stopping, where stopping conditions depend only on comparing the value function to the instantaneous reward.

Keywords: Mean-variance problems, Time-inconsistency, Cox process, Equilibrium stopping time, Extended HJB equation, Vanishing Regularization Method

AMS MSC2010: 60G40; 60J70; 91A10; 91A25; 91G80; 91B02; 91B51.

## 1 Introduction

Given a diffusion process XX, the classical optimal stopping problem is to determine a stopping time Ï„\tau that maximizes

|  |  |  |
| --- | --- | --- |
|  | ð”¼â€‹[fâ€‹(XÏ„)].\mathbb{E}\left[f(X\_{\tau})\right]. |  |

Optimal stopping has many applications, for example, financial decision-making (e.g., timing for asset sales) and statistical inference (e.g., stopping rules for hypothesis testing [[17](https://arxiv.org/html/2510.24128v1#bib.bib17)]). However, in financial contexts, there is often an additional imperative to mitigate decision-related risk. In line with the mean-variance analysis of [[14](https://arxiv.org/html/2510.24128v1#bib.bib14)], we identify the return with the expectation and the risk with the variance and aim to select a stopping time that maximizes

|  |  |  |  |
| --- | --- | --- | --- |
|  | ð”¼â€‹[fâ€‹(XÏ„)]âˆ’Î³2â€‹Varâ€‹[fâ€‹(XÏ„)],\mathbb{E}\left[f(X\_{\tau})\right]-\frac{\gamma}{2}\text{Var}\left[f(X\_{\tau})\right], |  | (1) |

where Î³â‰¥0\gamma\geq 0 denotes the risk aversion coefficient. Problem ([1](https://arxiv.org/html/2510.24128v1#S1.E1 "In 1 Introduction â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")) is referred as a mean-variance (MV) stopping problem in the literature.

Similar to the classical dynamic MV problem, the optimal stopping rule typically depends on the initial state xx, which means that it does not generally satisfy Bellmanâ€™s principle of optimality. In the literature this is known as time-inconsistency. Time-inconsistent problems are typically studied using two approaches. One is to formulate the problem for a fixed initial state and allow the corresponding optimal stopping rule to depend on that initial state, called the pre-commitment approach. The other is to look for a stopping rule that remains optimal at every period when re-evaluated from that periodâ€™s perspective, called the game-theoretic approach.

Strotz [[16](https://arxiv.org/html/2510.24128v1#bib.bib16)] is the first to explore the game-theoretic approach to time-inconsistent problems in dynamic utility maximization with non-exponential discounting. Bjork et al. [[4](https://arxiv.org/html/2510.24128v1#bib.bib4)] give a comprehensive treatment of time-inconsistent Markovian models and characterize the equilibrium by a solution to a generalized HJB equation, called the extended HJB system. Time inconsistent control problems have attracted considerable research interest in recent years with many applications. For example, Bjork et al. [[5](https://arxiv.org/html/2510.24128v1#bib.bib5)] solve a MV problem with state dependent risk aversion. He and Liang [[11](https://arxiv.org/html/2510.24128v1#bib.bib11)] study a defined contribution insurance problem in a MV framework. Dai et al. [[8](https://arxiv.org/html/2510.24128v1#bib.bib8)] solve a MV problem with reinforcement learning method.
All aforementioned papers have fixed finite horizon.

The literature on the game-theoretic approach to time-inconsistent stopping problems is in the early developing stage. Christensen and LindensjÃ¶ [[6](https://arxiv.org/html/2510.24128v1#bib.bib6)] study an equilibrium stopping problem with initial state dependent reward. Bayraktar et al. [[2](https://arxiv.org/html/2510.24128v1#bib.bib2)] consider three equilibrium concepts proposed in the literature for time-inconsistent stopping problems with non-exponential discount.
There is little research for MV stopping problems. The only ones the authors are aware of are Peskir and Shiryaev [[15](https://arxiv.org/html/2510.24128v1#bib.bib15)] on the so-called dynamic optimal stopping time, which is similar to the game theoretic approach and Christensen and LindensjÃ¶ [[7](https://arxiv.org/html/2510.24128v1#bib.bib7)] on a subgame perfect Nash equilibrium for stopping problems.

In this paper, we study the equilibrium strategy and relate it to the extended HJB equation, which means that we need to formulate the problem as a game and look for equilibrium. It is a fundamental result in game theory that equilibrium generally exists for mixed strategies rather than pure strategies in a broad class of games111For example, the rock - paper - scissors game is a classic example in game theory, and it has no pure strategy Nash equilibrium but has a mixed strategy Nash equilibrium in which the player choose each action with equal probability 1/31/3. . Hence, we focus on mixed strategy stopping times by allowing the agents to choose the intensity function of a Cox process as a randomization device for the stopping decision, whereas [[7](https://arxiv.org/html/2510.24128v1#bib.bib7), [15](https://arxiv.org/html/2510.24128v1#bib.bib15)] characterize the equilibrium and provide other necessary and sufficient equilibrium conditions, but do not derive the extended HJB equation. While their results coincide with ours for geometric Brownian motion case, our derivation is motivated by the vanishing regularity approach, a key distinction from prior research.

We now describe the key methodology for solving the MV stopping problem. We first add a regularization term, weighted by a constant Î»\lambda into the target functional (see ([2](https://arxiv.org/html/2510.24128v1#S2.E2 "In 2 Mean Variance Stopping and its Relaxed Problem â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method"))), to form a regularized problem with control variate being the intensity (as opposed to the stopping time), which makes the definition of the equilibrium straightforward. We then derive the associated extended HJB equation (see ([3](https://arxiv.org/html/2510.24128v1#S3.E3 "In Theorem 3.1. â€£ 3 Extended HJB Equation for Regularized Problem â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method"))), prove a verification theorem (see Theorem [3.1](https://arxiv.org/html/2510.24128v1#S3.Thmtheorem1 "Theorem 3.1. â€£ 3 Extended HJB Equation for Regularized Problem â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")) and establish the existence of its solution under certain technical assumptions (see Theorem [3.2](https://arxiv.org/html/2510.24128v1#S3.Thmtheorem2 "Theorem 3.2. â€£ 3 Extended HJB Equation for Regularized Problem â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")). Finally, we let Î»\lambda tend to zero (i.e., vanishing regularization) to formally obtain a system of parabolic variational inequalities (see ([7](https://arxiv.org/html/2510.24128v1#S4.E7 "In 4 Extended HJB Equation for Original Problem â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method"))) that characterizes the equilibrium stopping time for the original MV stopping problem. To the best knowledge of the authors, this is the first time such a system of equations is reported in the literature. Furthermore, we find that the stopping condition is not determined solely by comparing the value function with the instantaneous reward; instead, an additional quadratic term also plays a role in the formulation, which is in sharp contrast to the standard optimal stopping problem.

Finally, we aim to emphasize the motivation underlying our method. In [[9](https://arxiv.org/html/2510.24128v1#bib.bib9)], the authors investigated an entropy-regularized optimal stopping problem and demonstrated that the corresponding optimal value function is associated with a penalized form for the variational inequality. Notably, this penalized equation also converges to the original variational inequality as the regularization parameter tends to zero. For MV optimal stopping problems, we adopt an analogous approach: we first introduce and analyze a regularized version of the problem, then subsequently let the regularization parameter vanish to recover results for the original MV stopping problem. Such ideas are pervasive in mathematical research. When addressing a computationally or theoretically challenging problem, researchers often first consider a perturbed or regularized counterpartâ€”whose solution is more tractable to deriveâ€”and then take an appropriate limit to revert to the original problem. It is precisely this core logic that leads us to name our approach Vanishing Regularization Method.

The rest of the paper is organized as follows: In Section 2 we formulate the MV stopping problem and its relaxed problem together with the definition of equilibrium. In Section 3 we derive the extended HJB equation for the relaxed problem and prove a verification theorem (Theorem [3.1](https://arxiv.org/html/2510.24128v1#S3.Thmtheorem1 "Theorem 3.1. â€£ 3 Extended HJB Equation for Regularized Problem â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")) and an existence result (Theorem [3.2](https://arxiv.org/html/2510.24128v1#S3.Thmtheorem2 "Theorem 3.2. â€£ 3 Extended HJB Equation for Regularized Problem â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")). In Section 4 we let Î»\lambda tend to zero to formally get the variation system for the original MV stopping problem and show that it characterizes the equilibrium stopping time (Theorems [4.1](https://arxiv.org/html/2510.24128v1#S4.Thmtheorem1 "Theorem 4.1. â€£ 4 Extended HJB Equation for Original Problem â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method") and [4.2](https://arxiv.org/html/2510.24128v1#S4.Thmtheorem2 "Theorem 4.2. â€£ 4 Extended HJB Equation for Original Problem â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")). In Section 5 we give some further discussions on our results, including infinite horizon case, discrete time approximation, and general time-inconsistent problems. Section 6 concludes the paper. Appendix contains the proofs of a local time approximation relation ([9](https://arxiv.org/html/2510.24128v1#S4.E9 "In 4 Extended HJB Equation for Original Problem â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")) and a technical lemma (Lemma [B.1](https://arxiv.org/html/2510.24128v1#A2.Thmlemma1 "Lemma B.1. â€£ Appendix B A Key Lemma â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")) that is needed in the proof of Theorem [3.2](https://arxiv.org/html/2510.24128v1#S3.Thmtheorem2 "Theorem 3.2. â€£ 3 Extended HJB Equation for Regularized Problem â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method").

## 2 Mean Variance Stopping and its Relaxed Problem

In this section, we introduce the basic framework of the problem. Especially, we give the formulation for the relaxed MV stopping problem and the definition of related equilibrium strategy.
Let (Î©,â„±,P)(\Omega,\mathcal{F},P) be a probability space, on which a standard dd-dimensional Brownian motion WW is defined222For simplicity, we consider the case that the dimension of the Brownian motion is same to that of the state process. It can be extended to other cases without any major modification as long as Assumption [2.1](https://arxiv.org/html/2510.24128v1#S2.Thmassumption1 "Assumption 2.1. â€£ 2 Mean Variance Stopping and its Relaxed Problem â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method") holds.. The â„d\mathbb{R}^{d}-valued state process XX satisfies

|  |  |  |
| --- | --- | --- |
|  | dâ€‹Xt=bâ€‹(t,Xt)â€‹dâ€‹t+Ïƒâ€‹(t,Xt)â€‹dâ€‹Wt.dX\_{t}=b(t,X\_{t})dt+\sigma(t,X\_{t})dW\_{t}. |  |

Denote by ð”½:={â„±t}t\mathbb{F}:=\{\mathcal{F}\_{t}\}\_{t} the natural filtration generated by WW, augmented by all PP-null sets. The set ð’¯t,T\mathcal{T}\_{t,T} is defined as the totality of all ð”½\mathbb{F}-stopping time taking values in [t,T][t,T]. For any time tâˆˆ[0,T]t\in[0,T], the MV stopping problem is to choose Ï„âˆˆð’¯t,T\tau\in\mathcal{T}\_{t,T} such that the following functional

|  |  |  |
| --- | --- | --- |
|  | Jâ€‹(t,x;Ï„):=ð”¼t,xâ€‹[fâ€‹(XÏ„)]âˆ’Î³2â€‹Vart,xâ€‹[fâ€‹(XÏ„)]J(t,x;\tau):=\mathbb{E}\_{t,x}\left[f(X\_{\tau})\right]-\frac{\gamma}{2}\text{Var}\_{t,x}\left[f(X\_{\tau})\right] |  |

is maximized, where ð”¼t,xâ€‹[â‹…]\mathbb{E}\_{t,x}[\cdot] and Vart,xâ€‹[â‹…]\text{Var}\_{t,x}[\cdot] denote the conditional expectation and conditional variance conditioning on Xt=xX\_{t}=x, respectively.

In this paper, we shall adopt the following assumptions on the coefficients.

###### Assumption 2.1.

The coefficients b,Ïƒb,\sigma and ff are Lipschitz continuous with linear growth in xx, uniformly in tt, i.e., there exists a constant CC such that, for any tâˆˆ[0,T]t\in[0,T] and x,yâˆˆâ„dx,y\in\mathbb{R}^{d},

|  |  |  |
| --- | --- | --- |
|  | |bâ€‹(t,x)|,|Ïƒâ€‹(t,x)|,|fâ€‹(x)|â‰¤Câ€‹(1+|x|),|b(t,x)|,|\sigma(t,x)|,|f(x)|\leq C(1+|x|), |  |

and

|  |  |  |
| --- | --- | --- |
|  | |bâ€‹(t,x)âˆ’bâ€‹(t,y)|,|Ïƒâ€‹(t,x)âˆ’Ïƒâ€‹(t,y)|,|fâ€‹(x)âˆ’fâ€‹(y)|â‰¤Câ€‹|xâˆ’y|.|b(t,x)-b(t,y)|,|\sigma(t,x)-\sigma(t,y)|,|f(x)-f(y)|\leq C|x-y|. |  |

Moreover, Ïƒâ€‹ÏƒT\sigma\sigma^{T} is uniformly non-degenerate, i.e., there exists a constant cc such that Ïƒâ€‹ÏƒTâ€‹(t,x)â‰¥câ€‹I\sigma\sigma^{T}(t,x)\geq cI for any tâˆˆ[0,T]t\in[0,T] and xâˆˆâ„dx\in\mathbb{R}^{d}.

For any function ff defined on [0,T]Ã—â„d[0,T]\times\mathbb{R}^{d}, we use the notations âˆ‚tf\partial\_{t}f and âˆ‚xf\partial\_{x}f to represent the derivatives with respect to tt and xx and âˆ‚xâ€‹xf\partial\_{xx}f its Hessian. In this paper, we use CC to represent a constant that could depend on the coefficients but may be different from line to line.

It is well-known that, for MV problem, the dynamic programming principle fails. Thus, people focus on two kinds of strategies. One is called pre-committed strategy, which is a fixed plan chosen at the initial time and enforced irrevocably across all future periods, regardless of new information or changing market conditions. On the other hand, an equilibrium strategy or time-consistent strategy is a plan that remains optimal at every period when re-evaluated from that periodâ€™s perspective, which aligns with the concept of a subgame perfect equilibrium in dynamic games, where strategies are optimal in all â€subgamesâ€ (i.e., at all points in time).

In this paper, we study the equilibrium strategy, especially the extended HJB equation related to it. This means that we need to formulate the problem as a game and look for equilibrium. It is indeed a fundamental result in game theory that equilibrium generally exists for mixed strategies rather than pure strategies in a broad class of games. Hence, it is better to consider some notion of mixed strategy, namely, at each time, the players fix a probability of stopping and decide whether or not to stop according to this probability. Such mixed strategy is also called randomized stopping time in some literature, see, for example, Bayer et al. [[1](https://arxiv.org/html/2510.24128v1#bib.bib1)] and Dong [[9](https://arxiv.org/html/2510.24128v1#bib.bib9)]. Here, we model it as a doubly stochastic Poisson process. More precisely, let Î˜\Theta be a random variable, which is exponentially distributed with unit intensity and independent of Brownian motion. Given a non-negative ð”½\mathbb{F}-adapted process {Ï€s}tâ‰¤sâ‰¤T\{\pi\_{s}\}\_{t\leq s\leq T}, a random time Ï„\tau is defined as

|  |  |  |
| --- | --- | --- |
|  | Ï„:=inf{sâˆˆ[t,T]:âˆ«tsÏ€uâ€‹ð‘‘uâ‰¥Î˜}âˆ§T,\tau:=\inf\left\{s\in[t,T]:\int\_{t}^{s}\pi\_{u}du\geq\Theta\right\}{\wedge}T, |  |

where we adopt the convention that the infimum of an empty set is infinity. It represents the time that the player chooses to stop. Literally speaking, it means that, conditioning on having not stopped before, the probability that the player stops between tt and t+dâ€‹tt+dt is Ï€tâ€‹dâ€‹t\pi\_{t}dt. Under this formulation, instead of choosing a stopping time, the player chooses the intensity process Ï€\pi to optimize the MV objective function.

From the results of doubly stochastic Poisson processes (see Jeanblanc et al.Â [[12](https://arxiv.org/html/2510.24128v1#bib.bib12)] for details), one can compute that, for any function Ï†\varphi,

|  |  |  |
| --- | --- | --- |
|  | ð”¼â€‹[Ï†â€‹(XÏ„)|â„±T]=âˆ«tTÏ†â€‹(Xs)â€‹Ï€sâ€‹eâˆ’âˆ«tsÏ€uâ€‹ð‘‘uâ€‹ð‘‘s+Ï†â€‹(XT)â€‹eâˆ’âˆ«tTÏ€uâ€‹ð‘‘u.\mathbb{E}\left[\varphi(X\_{\tau})|\mathcal{F}\_{T}\right]=\int\_{t}^{T}\varphi(X\_{s})\pi\_{s}e^{-\int\_{t}^{s}\pi\_{u}du}ds+\varphi(X\_{T})e^{-\int\_{t}^{T}\pi\_{u}du}. |  |

Then, we have that

|  |  |  |
| --- | --- | --- |
|  | ð”¼t,xâ€‹[Ï†â€‹(XÏ„)]=ð”¼t,xâ€‹[ð”¼â€‹[Ï†â€‹(XÏ„)|â„±T]]=ð”¼t,xâ€‹[âˆ«tTÏ†â€‹(Xs)â€‹Ï€sâ€‹eâˆ’âˆ«tsÏ€uâ€‹ð‘‘uâ€‹ð‘‘s+Ï†â€‹(XT)â€‹eâˆ’âˆ«tTÏ€uâ€‹ð‘‘u].\mathbb{E}\_{t,x}[\varphi(X\_{\tau})]=\mathbb{E}\_{t,x}[\mathbb{E}[\varphi(X\_{\tau})|\mathcal{F}\_{T}]]=\mathbb{E}\_{t,x}\left[\int\_{t}^{T}\varphi(X\_{s})\pi\_{s}e^{-\int\_{t}^{s}\pi\_{u}du}ds+\varphi(X\_{T})e^{-\int\_{t}^{T}\pi\_{u}du}\right]. |  |

Thus, the MV criteria can be rewritten as

|  |  |  |
| --- | --- | --- |
|  | ð”¼t,xâ€‹[fâ€‹(XÏ„)]âˆ’Î³2â€‹Vart,xâ€‹[fâ€‹(XÏ„)]=ð”¼t,xâ€‹[âˆ«tT(fâˆ’Î³2â€‹f2)â€‹(Xs)â€‹Ï€sâ€‹eâˆ’âˆ«tsÏ€uâ€‹ð‘‘uâ€‹ð‘‘s+(fâˆ’Î³2â€‹f2)â€‹(XT)â€‹eâˆ’âˆ«tTÏ€uâ€‹ð‘‘u]+Î³2â€‹(ð”¼t,xâ€‹[âˆ«tTfâ€‹(Xs)â€‹Ï€sâ€‹eâˆ’âˆ«tsÏ€uâ€‹ð‘‘uâ€‹ð‘‘s+fâ€‹(XT)â€‹eâˆ’âˆ«tTÏ€uâ€‹ð‘‘u])2.\begin{split}&\mathbb{E}\_{t,x}\left[f(X\_{\tau})\right]-\frac{\gamma}{2}\text{Var}\_{t,x}\left[f(X\_{\tau})\right]\\ =&\mathbb{E}\_{t,x}\left[\int\_{t}^{T}(f-\frac{\gamma}{2}f^{2})(X\_{s})\pi\_{s}e^{-\int\_{t}^{s}\pi\_{u}du}ds+(f-\frac{\gamma}{2}f^{2})(X\_{T})e^{-\int\_{t}^{T}\pi\_{u}du}\right]\\ &+\frac{\gamma}{2}\left(\mathbb{E}\_{t,x}\left[\int\_{t}^{T}f(X\_{s})\pi\_{s}e^{-\int\_{t}^{s}\pi\_{u}du}ds+f(X\_{T})e^{-\int\_{t}^{T}\pi\_{u}du}\right]\right)^{2}.\end{split} |  |

However, we consider a regularized MV reward, which is defined as the following

|  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- |
|  | JÎ»â€‹(t,x;Ï€)\displaystyle J^{\lambda}(t,x;\pi) | :=\displaystyle:= | ð”¼t,xâ€‹[âˆ«tT{(fâˆ’Î³2â€‹f2)â€‹(Xs)â€‹Ï€s+Î»â€‹Hâ€‹(Ï€s)}â€‹eâˆ’âˆ«tsÏ€uâ€‹ð‘‘uâ€‹ð‘‘s+(fâˆ’Î³2â€‹f2)â€‹(XT)â€‹eâˆ’âˆ«tTÏ€uâ€‹ð‘‘u]\displaystyle\mathbb{E}\_{t,x}\left[\int\_{t}^{T}\left\{(f-\frac{\gamma}{2}f^{2})(X\_{s})\pi\_{s}+\lambda H(\pi\_{s})\right\}e^{-\int\_{t}^{s}\pi\_{u}du}ds+(f-\frac{\gamma}{2}f^{2})(X\_{T})e^{-\int\_{t}^{T}\pi\_{u}du}\right] |  | (2) |
|  |  |  | +Î³2â€‹(ð”¼t,xâ€‹[âˆ«tTfâ€‹(Xs)â€‹Ï€sâ€‹eâˆ’âˆ«tsÏ€uâ€‹ð‘‘uâ€‹ð‘‘s+fâ€‹(XT)â€‹eâˆ’âˆ«tTÏ€uâ€‹ð‘‘u])2,\displaystyle{}+\frac{\gamma}{2}\left(\mathbb{E}\_{t,x}\left[\int\_{t}^{T}f(X\_{s})\pi\_{s}e^{-\int\_{t}^{s}\pi\_{u}du}ds+f(X\_{T})e^{-\int\_{t}^{T}\pi\_{u}du}\right]\right)^{2}, |  |

where the function HH is given by Hâ€‹(Ï€)=Ï€âˆ’Ï€â€‹logâ¡Ï€H(\pi)=\pi-\pi\log\pi and Î»\lambda is a positive constant.
The regularization term prevents the intensity taking values 0 and âˆž\infty, which refers to continue and stop deterministically. This provides mathematically tractability for the problem, which one can derive the related HJB equations. The motivation comes from [[9](https://arxiv.org/html/2510.24128v1#bib.bib9)] in the study of reinforcement learning method for optimal stopping problem, where HH is referred as unnormalized entropy to encourage randomness in the strategy.

We will let Î»\lambda tend to zero to go back to the original problem. Dong [[9](https://arxiv.org/html/2510.24128v1#bib.bib9)] proves that the HJB equations converge for optimal stopping problem. To our best knowledge, it is still an open problem for MV stopping problem, but, formally, we will see that the extended HJB equation converge to some system that gives an equilibrium stopping time for MV problem.

Since we focus on extended HJB equation, we shall restrict to the Markovian strategies, i.e., Ï€s=Ï€â€‹(s,Xs)\pi\_{s}=\pi(s,X\_{s}) for some deterministic function Ï€\pi. Similar to [[3](https://arxiv.org/html/2510.24128v1#bib.bib3)], one can define an equilibrium in the following sense.

###### Definition 2.1.

A strategy Ï€âˆ—\pi^{\*} is called an equilibrium strategy, if for any Îµ,v>0\varepsilon,v>0 and tâˆˆ[0,T)t\in[0,T), define the perturbed policy Ï€Îµ,v\pi^{\varepsilon,v} as

|  |  |  |
| --- | --- | --- |
|  | Ï€Îµ,v(s,x)={v,Â ifÂ tâ‰¤sâ‰¤t+Îµ;Ï€âˆ—â€‹(s,x)â€‹Â ifÂ s>t+Îµ,\pi^{\varepsilon,v}(s,x)=\left\{\begin{split}&v,\text{ if $t\leq s\leq t+\varepsilon$};\\ &\pi^{\*}(s,x)\text{ if $s>t+\varepsilon$},\end{split}\right. |  |

and it holds that

|  |  |  |
| --- | --- | --- |
|  | lim infÎµâ†’0JÎ»â€‹(t,x;Ï€âˆ—)âˆ’JÎ»â€‹(t,x;Ï€Îµ,v)Îµâ‰¥0,a.s.,\liminf\_{\varepsilon\rightarrow 0}\frac{J^{\lambda}(t,x;\pi^{\*})-J^{\lambda}(t,x;\pi^{\varepsilon,v})}{\varepsilon}\geq 0,a.s., |  |

for any initial state xx.

## 3 Extended HJB Equation for Regularized Problem

In this section, we derive the extended HJB equation for the regularized MV problem. To this end, we first define an operator â„’\mathcal{L} as, for any smooth function Ï†\varphi,

|  |  |  |
| --- | --- | --- |
|  | (â„’â€‹Ï†)â€‹(t,x):=12â€‹tr(Ïƒâ€‹ÏƒTâ€‹(t,x)â€‹âˆ‚xâ€‹xÏ†â€‹(t,x))+bâ€‹(t,x)â€‹âˆ‚xÏ†â€‹(t,x).(\mathcal{L}\varphi)(t,x):=\frac{1}{2}\mathop{\text{tr}}(\sigma\sigma^{T}(t,x)\partial\_{xx}\varphi(t,x))+b(t,x)\partial\_{x}\varphi(t,x). |  |

We have the following verification theorem.

###### Theorem 3.1.

For a Markovian strategy Ï€âˆ—=Ï€âˆ—â€‹(t,x)\pi^{\*}=\pi^{\*}(t,x), let (VÎ»,gÎ»)(V^{\lambda},g^{\lambda}) be a classical solution of the following parabolic system

|  |  |  |  |
| --- | --- | --- | --- |
|  | {âˆ‚tVÎ»+â„’â€‹VÎ»+Î»â€‹expâ¡(âˆ’VÎ»+Î³2â€‹(fâˆ’gÎ»)2âˆ’fÎ»)âˆ’Î³â€‹|Ïƒâ€‹âˆ‚xgÎ»|2=0,VÎ»â€‹(T,x)=fâ€‹(x),âˆ‚tgÎ»+â„’â€‹gÎ»âˆ’expâ¡(âˆ’VÎ»+Î³2â€‹(fâˆ’gÎ»)2âˆ’fÎ»)â€‹(gÎ»âˆ’f)=0,gÎ»â€‹(T,x)=fâ€‹(x).\left\{\begin{split}&\partial\_{t}V^{\lambda}+\mathcal{L}V^{\lambda}+\lambda\exp(-\frac{V^{\lambda}+\frac{\gamma}{2}(f-g^{\lambda})^{2}-f}{\lambda})-\gamma|\sigma\partial\_{x}g^{\lambda}|^{2}=0,\ V^{\lambda}(T,x)=f(x),\\ &\partial\_{t}g^{\lambda}+\mathcal{L}g^{\lambda}-\exp(-\frac{V^{\lambda}+\frac{\gamma}{2}(f-g^{\lambda})^{2}-f}{\lambda})(g^{\lambda}-f)=0,\ g^{\lambda}(T,x)=f(x).\end{split}\right. |  | (3) |

Assume that VÎ»V^{\lambda}, gÎ»g^{\lambda}, their derivatives (up to first order in tt and second order in xx) and Ï€\pi are all continuous with polynomial growth in xx, uniformly in tt. Then Ï€âˆ—\pi^{\*} is an equilibrium strategy if and only if

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ï€âˆ—â€‹(t,x)=expâ¡(âˆ’VÎ»+Î³2â€‹(fâˆ’gÎ»)2âˆ’fÎ»).\pi^{\*}(t,x)=\exp(-\frac{V^{\lambda}+\frac{\gamma}{2}(f-g^{\lambda})^{2}-f}{\lambda}). |  | (4) |

###### Proof.

The proof consists of two steps:

1. 1.

   We start by showing that gÎ»â€‹(t,x)=ð”¼t,xâ€‹[fâ€‹(XÏ„)]g^{\lambda}(t,x)=\mathbb{E}\_{t,x}[f(X\_{\tau})] and VÎ»â€‹(t,x)=JÎ»â€‹(t,x;Ï€âˆ—)V^{\lambda}(t,x)=J^{\lambda}(t,x;\pi^{\*});
2. 2.

   In the second step, we prove that Ï€âˆ—\pi^{\*} is an equilibrium if and only if ([4](https://arxiv.org/html/2510.24128v1#S3.E4 "In Theorem 3.1. â€£ 3 Extended HJB Equation for Regularized Problem â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")) holds.

For the first step, applying ItÃ´ formula to gÎ»â€‹(s,Xs)â€‹eâˆ’âˆ«tsÏ€âˆ—â€‹(u,Xu)â€‹ð‘‘ug^{\lambda}(s,X\_{s})e^{-\int\_{t}^{s}\pi^{\*}(u,X\_{u})du}, we have that

|  |  |  |
| --- | --- | --- |
|  | dâ€‹(gÎ»â€‹(s,Xs)â€‹eâˆ’âˆ«tsÏ€âˆ—â€‹(u,Xu)â€‹ð‘‘u)=eâˆ’âˆ«tsÏ€âˆ—â€‹(u,Xu)â€‹ð‘‘uâ€‹((âˆ‚t+â„’)â€‹gÎ»â€‹(s,Xs)âˆ’Ï€âˆ—â€‹(u,Xu)â€‹gÎ»â€‹(u,Xu))â€‹dâ€‹s+Ïƒâ€‹âˆ‚xgÎ»â€‹(s,Xs)â€‹eâˆ’âˆ«tsÏ€âˆ—â€‹(u,Xu)â€‹ð‘‘uâ€‹dâ€‹Ws.\begin{split}d\left(g^{\lambda}(s,X\_{s})e^{-\int\_{t}^{s}\pi^{\*}(u,X\_{u})du}\right)=&e^{-\int\_{t}^{s}\pi^{\*}(u,X\_{u})du}\left((\partial\_{t}+\mathcal{L})g^{\lambda}(s,X\_{s})-\pi^{\*}(u,X\_{u})g^{\lambda}(u,X\_{u})\right)ds\\ &+\sigma\partial\_{x}g^{\lambda}(s,X\_{s})e^{-\int\_{t}^{s}\pi^{\*}(u,X\_{u})du}dW\_{s}.\end{split} |  |

From the growth assumption on the derivative of gÎ»g^{\lambda}, it holds that the stochastic integral is a martingale. Thus, taking conditional expectation, one can verify that

|  |  |  |
| --- | --- | --- |
|  | gÎ»â€‹(t,x)=ð”¼t,xâ€‹[âˆ«tTfâ€‹(Xs)â€‹Ï€âˆ—â€‹(s,Xs)â€‹eâˆ’âˆ«tsÏ€âˆ—â€‹(u,Xu)â€‹ð‘‘uâ€‹ð‘‘s+fâ€‹(XT)â€‹eâˆ’âˆ«tTÏ€âˆ—â€‹(u,Xu)â€‹ð‘‘u],g^{\lambda}(t,x)=\mathbb{E}\_{t,x}\left[\int\_{t}^{T}f(X\_{s})\pi^{\*}(s,X\_{s})e^{-\int\_{t}^{s}\pi^{\*}(u,X\_{u})du}ds+f(X\_{T})e^{-\int\_{t}^{T}\pi^{\*}(u,X\_{u})du}\right], |  |

Then, define the function hÎ»h^{\lambda} as hÎ»=VÎ»âˆ’Î³2â€‹(gÎ»)2h^{\lambda}=V^{\lambda}-\frac{\gamma}{2}(g^{\lambda})^{2}. It is easy to see that hÎ»h^{\lambda} solves

|  |  |  |
| --- | --- | --- |
|  | (âˆ‚t+â„’)â€‹hÎ»â€‹(t,x)+Ï€âˆ—â€‹(t,x)â€‹(fâˆ’Î³2â€‹f2âˆ’hÎ»)â€‹(t,x)+Î»â€‹Hâ€‹(Ï€âˆ—)=0,hÎ»â€‹(T,x)=fâ€‹(x)âˆ’Î³2â€‹f2â€‹(x).(\partial\_{t}+\mathcal{L})h^{\lambda}(t,x)+\pi^{\*}(t,x)(f-\frac{\gamma}{2}f^{2}-h^{\lambda})(t,x)+\lambda H(\pi^{\*})=0,\ h^{\lambda}(T,x)=f(x)-\frac{\gamma}{2}f^{2}(x). |  |

Similarly, one can verify that

|  |  |  |
| --- | --- | --- |
|  | hÎ»â€‹(t,x)=ð”¼t,xâ€‹[âˆ«tT{(fâˆ’Î³2â€‹f2)â€‹(Xs)â€‹Ï€âˆ—â€‹(s,Xs)+Î»â€‹Hâ€‹(Ï€s)}â€‹eâˆ’âˆ«tsÏ€âˆ—â€‹(u,Xu)â€‹ð‘‘uâ€‹ð‘‘s+fâ€‹(XT)â€‹eâˆ’âˆ«tTÏ€âˆ—â€‹(u,Xu)â€‹ð‘‘u].h^{\lambda}(t,x)=\mathbb{E}\_{t,x}\left[\int\_{t}^{T}\left\{(f-\frac{\gamma}{2}f^{2})(X\_{s})\pi^{\*}(s,X\_{s})+\lambda H(\pi\_{s})\right\}e^{-\int\_{t}^{s}\pi^{\*}(u,X\_{u})du}ds+f(X\_{T})e^{-\int\_{t}^{T}\pi^{\*}(u,X\_{u})du}\right]. |  |

This implies that JÎ»â€‹(t,x;Ï€âˆ—)=hÎ»â€‹(t,x)+Î³2â€‹gÎ»â€‹(t,x)=VÎ»â€‹(t,x)J^{\lambda}(t,x;\pi^{\*})=h^{\lambda}(t,x)+\frac{\gamma}{2}g^{\lambda}(t,x)=V^{\lambda}(t,x).

Next, we prove the second step. For any tt, Îµ\varepsilon and vv, consider the perturbed strategy Ï€Îµ,v\pi^{\varepsilon,v}. Since Ï€Îµ,v\pi^{\varepsilon,v} coincides with Ï€âˆ—\pi^{\*} after time t+Îµt+\varepsilon, it holds that

|  |  |  |
| --- | --- | --- |
|  | JÎ»â€‹(t,x;Ï€Îµ,v)=ð”¼t,xâ€‹[âˆ«tt+Îµ{(fâˆ’Î³2â€‹f2)â€‹(Xs)â€‹v+Î»â€‹Hâ€‹(v)}â€‹eâˆ’vâ€‹(sâˆ’t)â€‹ð‘‘s+hÎ»â€‹(t+Îµ,Xt+Îµ)â€‹eâˆ’vâ€‹Îµ]+Î³2â€‹(ð”¼t,xâ€‹[âˆ«tt+Îµfâ€‹(Xs)â€‹vâ€‹eâˆ’vâ€‹(sâˆ’t)â€‹ð‘‘s+gÎ»â€‹(t+Îµ,Xt+Îµ)â€‹eâˆ’vâ€‹Îµ])2.\begin{split}J^{\lambda}(t,x;\pi^{\varepsilon,v})=&\mathbb{E}\_{t,x}\left[\int\_{t}^{t+\varepsilon}\left\{(f-\frac{\gamma}{2}f^{2})(X\_{s})v+\lambda H(v)\right\}e^{-v(s-t)}ds+h^{\lambda}(t+\varepsilon,X\_{t+\varepsilon})e^{-v\varepsilon}\right]\\ &+\frac{\gamma}{2}\left(\mathbb{E}\_{t,x}\left[\int\_{t}^{t+\varepsilon}f(X\_{s})ve^{-v(s-t)}ds+g^{\lambda}(t+\varepsilon,X\_{t+\varepsilon})e^{-v\varepsilon}\right]\right)^{2}.\end{split} |  |

Applying ItÃ´ formula to gÎ»â€‹(s,Xs)â€‹eâˆ’vâ€‹(sâˆ’t)g^{\lambda}(s,X\_{s})e^{-v(s-t)} and taking conditional expectation, we get that

|  |  |  |
| --- | --- | --- |
|  | ð”¼t,xâ€‹[gÎ»â€‹(t+Îµ,Xt+Îµ)â€‹eâˆ’vâ€‹Îµ]=gÎ»â€‹(t,x)+ð”¼t,xâ€‹[âˆ«tt+Îµ(âˆ‚t+â„’)â€‹gÎ»â€‹(s,Xs)â€‹eâˆ’vâ€‹(sâˆ’t)âˆ’vâ€‹gÎ»â€‹(s,Xs)â€‹eâˆ’vâ€‹(sâˆ’t)â€‹dâ€‹s].\begin{split}\mathbb{E}\_{t,x}[g^{\lambda}(t+\varepsilon,X\_{t+\varepsilon})e^{-v\varepsilon}]=g^{\lambda}(t,x)+\mathbb{E}\_{t,x}\left[\int\_{t}^{t+\varepsilon}(\partial\_{t}+\mathcal{L})g^{\lambda}(s,X\_{s})e^{-v(s-t)}-vg^{\lambda}(s,X\_{s})e^{-v(s-t)}ds\right].\end{split} |  |

Then, it holds that

|  |  |  |
| --- | --- | --- |
|  | ð”¼t,xâ€‹[âˆ«tt+Îµfâ€‹(Xs)â€‹vâ€‹eâˆ’vâ€‹(sâˆ’t)â€‹ð‘‘s+gÎ»â€‹(t+Îµ,Xt+Îµ)â€‹eâˆ’vâ€‹Îµ]=gÎ»â€‹(t,x)+ð”¼t,xâ€‹[âˆ«tt+Îµ(âˆ‚t+â„’)â€‹gÎ»â€‹(s,Xs)â€‹eâˆ’vâ€‹(sâˆ’t)+vâ€‹(fâˆ’gÎ»)â€‹(s,Xs)â€‹eâˆ’vâ€‹(sâˆ’t)â€‹dâ€‹s]=gÎ»â€‹(t,x)+((âˆ‚t+â„’)â€‹gÎ»â€‹(t,x)+vâ€‹(fâˆ’gÎ»)â€‹(t,x))â€‹Îµ+oâ€‹(Îµ),\begin{split}&\mathbb{E}\_{t,x}\left[\int\_{t}^{t+\varepsilon}f(X\_{s})ve^{-v(s-t)}ds+g^{\lambda}(t+\varepsilon,X\_{t+\varepsilon})e^{-v\varepsilon}\right]\\ =&g^{\lambda}(t,x)+\mathbb{E}\_{t,x}\left[\int\_{t}^{t+\varepsilon}(\partial\_{t}+\mathcal{L})g^{\lambda}(s,X\_{s})e^{-v(s-t)}+v(f-g^{\lambda})(s,X\_{s})e^{-v(s-t)}ds\right]\\ =&g^{\lambda}(t,x)+\bigg((\partial\_{t}+\mathcal{L})g^{\lambda}(t,x)+v(f-g^{\lambda})(t,x)\bigg)\varepsilon+o(\varepsilon),\end{split} |  |

where the last equality is due to the fact that the related functions are continuous and XX also has continuous trajectories. Thus, we get that

|  |  |  |
| --- | --- | --- |
|  | (ð”¼t,xâ€‹[âˆ«tt+Îµfâ€‹(Xs)â€‹vâ€‹eâˆ’vâ€‹(sâˆ’t)â€‹ð‘‘s+gÎ»â€‹(t+Îµ,Xt+Îµ)â€‹eâˆ’vâ€‹Îµ])2=(gÎ»)2â€‹(t,x)+2â€‹gÎ»â€‹(t,x)â€‹((âˆ‚t+â„’)â€‹gÎ»â€‹(t,x)+vâ€‹(fâˆ’gÎ»)â€‹(t,x))â€‹Îµ+oâ€‹(Îµ).\begin{split}&\left(\mathbb{E}\_{t,x}\left[\int\_{t}^{t+\varepsilon}f(X\_{s})ve^{-v(s-t)}ds+g^{\lambda}(t+\varepsilon,X\_{t+\varepsilon})e^{-v\varepsilon}\right]\right)^{2}\\ =&(g^{\lambda})^{2}(t,x)+2g^{\lambda}(t,x)\bigg((\partial\_{t}+\mathcal{L})g^{\lambda}(t,x)+v(f-g^{\lambda})(t,x)\bigg)\varepsilon+o(\varepsilon).\end{split} |  |

With a similar argument, one also have that

|  |  |  |
| --- | --- | --- |
|  | ð”¼t,xâ€‹[âˆ«tt+Îµ{(fâˆ’Î³2â€‹f2)â€‹(Xs)â€‹v+Î»â€‹Hâ€‹(v)}â€‹eâˆ’vâ€‹(sâˆ’t)â€‹ð‘‘s+hÎ»â€‹(t+Îµ,Xt+Îµ)â€‹eâˆ’vâ€‹Îµ]=hÎ»â€‹(t,x)+((âˆ‚t+â„’)â€‹hÎ»â€‹(t,x)+vâ€‹(fâˆ’Î³2â€‹f2âˆ’hÎ»)â€‹(t,x)+Î»â€‹Hâ€‹(v))â€‹Îµ+oâ€‹(Îµ).\begin{split}&\mathbb{E}\_{t,x}\left[\int\_{t}^{t+\varepsilon}\left\{(f-\frac{\gamma}{2}f^{2})(X\_{s})v+\lambda H(v)\right\}e^{-v(s-t)}ds+h^{\lambda}(t+\varepsilon,X\_{t+\varepsilon})e^{-v\varepsilon}\right]\\ =&h^{\lambda}(t,x)+\bigg((\partial\_{t}+\mathcal{L})h^{\lambda}(t,x)+v(f-\frac{\gamma}{2}f^{2}-h^{\lambda})(t,x)+\lambda H(v)\bigg)\varepsilon+o(\varepsilon).\end{split} |  |

Combining these estimations together, we get that

|  |  |  |
| --- | --- | --- |
|  | JÎ»â€‹(t,x;Ï€Îµ,v)=hÎ»(t,x)+Î³2(gÎ»)2(t,x)+((âˆ‚t+â„’)hÎ»(t,x)+Î³gÎ»(t,x)(âˆ‚t+â„’)gÎ»(t,x)+v(fâˆ’Î³2f2âˆ’hÎ»+Î³gÎ»fâˆ’Î³(gÎ»)2)(t,x)+Î»H(v))Îµ+o(Îµ).\begin{split}J^{\lambda}(t,x;\pi^{\varepsilon,v})=&h^{\lambda}(t,x)+\frac{\gamma}{2}(g^{\lambda})^{2}(t,x)+\bigg((\partial\_{t}+\mathcal{L})h^{\lambda}(t,x)+\gamma g^{\lambda}(t,x)(\partial\_{t}+\mathcal{L})g^{\lambda}(t,x)\\ &+v(f-\frac{\gamma}{2}f^{2}-h^{\lambda}+\gamma g^{\lambda}f-\gamma(g^{\lambda})^{2})(t,x)+\lambda H(v)\bigg)\varepsilon+o(\varepsilon).\end{split} |  |

From the equations satisfied by gÎ»g^{\lambda} and hÎ»h^{\lambda}, also noting that JÎ»â€‹(t,x;Ï€âˆ—)=hÎ»â€‹(t,x)+Î³2â€‹(gÎ»)2â€‹(t,x)J^{\lambda}(t,x;\pi^{\*})=h^{\lambda}(t,x)+\frac{\gamma}{2}(g^{\lambda})^{2}(t,x), we derive that

|  |  |  |
| --- | --- | --- |
|  | limÎµâ†’0JÎ»â€‹(t,x;Ï€âˆ—)âˆ’JÎ»â€‹(t,x;Ï€Îµ,v)Îµ=(vâˆ’Ï€âˆ—â€‹(t,x))â€‹(fâˆ’Î³2â€‹f2âˆ’hÎ»+Î³â€‹gÎ»â€‹fâˆ’Î³â€‹(gÎ»)2)â€‹(t,x)+Î»â€‹(Hâ€‹(v)âˆ’Hâ€‹(Ï€âˆ—â€‹(t,x))).\begin{split}&\mathop{\text{lim}}\_{\varepsilon\rightarrow 0}\frac{J^{\lambda}(t,x;\pi^{\*})-J^{\lambda}(t,x;\pi^{\varepsilon,v})}{\varepsilon}\\ =&(v-\pi^{\*}(t,x))(f-\frac{\gamma}{2}f^{2}-h^{\lambda}+\gamma g^{\lambda}f-\gamma(g^{\lambda})^{2})(t,x)+\lambda(H(v)-H(\pi^{\*}(t,x))).\end{split} |  |

Hence, Ï€âˆ—\pi^{\*} is an equilibrium strategy if and only if

|  |  |  |
| --- | --- | --- |
|  | Ï€âˆ—â€‹(t,x)âˆˆargmaxvâ€‹vâ€‹(fâˆ’Î³2â€‹f2âˆ’hÎ»âˆ’Î³â€‹(gÎ»)2+Î³â€‹gÎ»â€‹f)â€‹(t,x)+Î»â€‹Hâ€‹(v),\pi^{\*}(t,x)\in\text{argmax}\_{v}v(f-\frac{\gamma}{2}f^{2}-h^{\lambda}-\gamma(g^{\lambda})^{2}+\gamma g^{\lambda}f)(t,x)+\lambda H(v), |  |

which implies that Ï€âˆ—\pi^{\*} satisfies the optimality condition ([4](https://arxiv.org/html/2510.24128v1#S3.E4 "In Theorem 3.1. â€£ 3 Extended HJB Equation for Regularized Problem â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")).
âˆŽ

To our best knowledge, it is still a hard open problem to prove existence and/or uniqueness for solutions of an extended HJB system with a general assumption. In linear-quadratic mean-variance problem, one can reduce it to an ODE system and obtain a solution, see [[5](https://arxiv.org/html/2510.24128v1#bib.bib5)].
For the solvability of ([3](https://arxiv.org/html/2510.24128v1#S3.E3 "In Theorem 3.1. â€£ 3 Extended HJB Equation for Regularized Problem â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")), we give an existence result with additional technical assumptions on the coefficients and a small time interval.

###### Theorem 3.2.

In addition to Assumption [2.1](https://arxiv.org/html/2510.24128v1#S2.Thmassumption1 "Assumption 2.1. â€£ 2 Mean Variance Stopping and its Relaxed Problem â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method"), we further assume that the coefficients bb,Ïƒ\sigma and ff are uniformly bounded. Then, for a sufficiently small TT, ([3](https://arxiv.org/html/2510.24128v1#S3.E3 "In Theorem 3.1. â€£ 3 Extended HJB Equation for Regularized Problem â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")) admits a classical solution (VÎ»,gÎ»)(V^{\lambda},g^{\lambda}).

###### Proof.

The solution is to be considered as a fixed point of a contraction mapping. For that purpose,
denote by ð•‚\mathbb{K} the Banach space Câ€‹([0,T],C1â€‹(â„d))C([0,T],C^{1}(\mathbb{R}^{d})) equipped with the norm â€–lâ€–ð•‚=supt,x|lâ€‹(t,x)|+supt,x|âˆ‚xlâ€‹(t,x)|\|l\|\_{\mathbb{K}}=\sup\_{t,x}|l(t,x)|+\sup\_{t,x}|\partial\_{x}l(t,x)|. For any lâˆˆð•‚l\in\mathbb{K}, define a mapping FF as k=Fâ€‹(l)k=F(l) is the solution of the following system

|  |  |  |  |
| --- | --- | --- | --- |
|  | {âˆ‚tv+â„’â€‹v+Î»â€‹expâ¡(âˆ’v+Î³2â€‹(fâˆ’l)2âˆ’fÎ»)âˆ’Î³â€‹|Ïƒâ€‹âˆ‚xl|2=0,vâ€‹(T,x)=f,âˆ‚tk+â„’â€‹kâˆ’expâ¡(âˆ’v+Î³2â€‹(fâˆ’l)2âˆ’fÎ»)â€‹(kâˆ’f)=0,kâ€‹(T,x)=f.\left\{\begin{split}&\partial\_{t}v+\mathcal{L}v+\lambda\exp(-\frac{v+\frac{\gamma}{2}(f-l)^{2}-f}{\lambda})-\gamma|\sigma\partial\_{x}l|^{2}=0,v(T,x)=f,\\ &\partial\_{t}k+\mathcal{L}k-\exp(-\frac{v+\frac{\gamma}{2}(f-l)^{2}-f}{\lambda})(k-f)=0,k(T,x)=f.\end{split}\right. |  | (5) |

Let Bmâ€‹(0)B\_{m}(0) be the ball in ð•‚\mathbb{K} centered at 0 with radius m=â€–fâ€–âˆž+â€–âˆ‚xfâ€–âˆž+1m=\|f\|\_{\infty}+\|\partial\_{x}f\|\_{\infty}+1. We are to show that, for a sufficiently small TT, FF is a contraction from Bmâ€‹(0)B\_{m}(0) into itself and, thus, admits a fixed point, which would be the solution of ([3](https://arxiv.org/html/2510.24128v1#S3.E3 "In Theorem 3.1. â€£ 3 Extended HJB Equation for Regularized Problem â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")).

The proof consists of several steps:

Step 11. Well-posedness of first equation. Choose any N>0N>0, let Î¾N\xi\_{N} be a smooth cutoff function such that Î¾Nâ€‹(x)=x\xi\_{N}(x)=x, for xâ‰¤Nx\leq N; and Î¾Nâ€‹(x)=N+1\xi\_{N}(x)=N+1, for xâ‰¥N+1x\geq N+1. Consider the following equation

|  |  |  |
| --- | --- | --- |
|  | âˆ‚tv+â„’â€‹v+Î»â€‹expâ¡(Î¾Nâ€‹(âˆ’v+Î³2â€‹(fâˆ’l)2âˆ’fÎ»))âˆ’Î³â€‹|Ïƒâ€‹âˆ‚xl|2=0,vâ€‹(T,x)=f.\partial\_{t}v+\mathcal{L}v+\lambda\exp(\xi\_{N}(-\frac{v+\frac{\gamma}{2}(f-l)^{2}-f}{\lambda}))-\gamma|\sigma\partial\_{x}l|^{2}=0,v(T,x)=f. |  |

Noting that the third term is a bounded Lipschitz function of vv, it admits a solution vv. Lemma [B.1](https://arxiv.org/html/2510.24128v1#A2.Thmlemma1 "Lemma B.1. â€£ Appendix B A Key Lemma â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method") yields that

|  |  |  |
| --- | --- | --- |
|  | âˆ’Câ€‹(â€–fâ€–âˆž+Î»â€‹expâ¡(N+1Î»))â‰¤vâ‰¤Câ€‹(Tâ€‹m2+â€–fâ€–âˆž).-C(\|f\|\_{\infty}+\lambda\exp(\frac{N+1}{\lambda}))\leq v\leq C(Tm^{2}+\|f\|\_{\infty}). |  |

Let us give a refined lower bound estimation independent of NN, which implies that vv solves the first equation in ([5](https://arxiv.org/html/2510.24128v1#S3.E5 "In 3 Extended HJB Equation for Regularized Problem â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")) when NN is sufficiently large.
Denote by Ïˆâ€‹(x)=1+|x|2\psi(x)=\sqrt{1+|x|^{2}}. One can compute that

|  |  |  |
| --- | --- | --- |
|  | Dxâ€‹Ïˆ=x1+|x|2â€‹Â andÂ â€‹Dx2â€‹Ïˆ=11+|x|2â€‹Iâˆ’1(1+|x|2)12â€‹xâŠ—x,D\_{x}\psi=\frac{x}{\sqrt{1+|x|^{2}}}\text{ and }D^{2}\_{x}\psi=\frac{1}{\sqrt{1+|x|^{2}}}I-\frac{1}{\left(1+|x|^{2}\right)^{\frac{1}{2}}}x\otimes x, |  |

and, thus, â„’â€‹Ïˆâ‰¤C\mathcal{L}\psi\leq C. Since vv is a bounded function, it holds that, for any Îµ>0\varepsilon>0, v+Îµâ€‹Ïˆv+\varepsilon\psi attains a minimum at some point (tâˆ—,xâˆ—)(t^{\*},x^{\*}). If tâˆ—=Tt^{\*}=T, then the terminal condition implies that

|  |  |  |
| --- | --- | --- |
|  | vâ‰¥âˆ’â€–fâ€–âˆžâˆ’Îµâ€‹Ïˆ.v\geq-\|f\|\_{\infty}-\varepsilon\psi. |  |

If tâˆ—<Tt^{\*}<T, it holds that, at (tâˆ—,xâˆ—)(t^{\*},x^{\*}),

|  |  |  |
| --- | --- | --- |
|  | Dx2â€‹vâ‰¥âˆ’Îµâ€‹Dx2â€‹Ïˆ,Dxâ€‹v=âˆ’Îµâ€‹Dxâ€‹Ïˆ,Â andÂ â€‹âˆ‚tvâ‰¥0.D^{2}\_{x}v\geq-\varepsilon D^{2}\_{x}\psi,D\_{x}v=-\varepsilon D\_{x}\psi,\text{ and }\partial\_{t}v\geq 0. |  |

This yields that (âˆ‚t+â„’)â€‹vâ€‹(tâˆ—,xâˆ—)â‰¥âˆ’Îµâ€‹â„’â€‹Ïˆâ‰¥âˆ’Câ€‹Îµ(\partial\_{t}+\mathcal{L})v(t^{\*},x^{\*})\geq-\varepsilon\mathcal{L}\psi\geq-C\varepsilon. On the other hand, we have

|  |  |  |
| --- | --- | --- |
|  | (âˆ‚t+â„’)â€‹v=Î³â€‹|Ïƒâ€‹Dxâ€‹l|2âˆ’Î»â€‹expâ¡(Î¾Nâ€‹(âˆ’v+Î³2â€‹(fâˆ’l)2âˆ’fÎ»)).(\partial\_{t}+\mathcal{L})v=\gamma|\sigma D\_{x}l|^{2}-\lambda\exp(\xi\_{N}(-\frac{v+\frac{\gamma}{2}(f-l)^{2}-f}{\lambda})). |  |

Combining these two inequalities, we get that

|  |  |  |
| --- | --- | --- |
|  | Î»â€‹expâ¡(Î¾Nâ€‹(âˆ’v+Î³2â€‹(fâˆ’l)2âˆ’fÎ»))â‰¤Î³â€‹|Ïƒâ€‹Dxâ€‹l|2+Câ€‹Îµ,\lambda\exp(\xi\_{N}(-\frac{v+\frac{\gamma}{2}(f-l)^{2}-f}{\lambda}))\leq\gamma|\sigma D\_{x}l|^{2}+C\varepsilon, |  |

or, equivalently,

|  |  |  |
| --- | --- | --- |
|  | Î¾Nâ€‹(fâˆ’vâˆ’Î³2â€‹(fâˆ’l)2Î»)â‰¤logâ¡Î³â€‹|Ïƒâ€‹âˆ‚xl|2+Câ€‹ÎµÎ».\xi\_{N}(\frac{f-v-\frac{\gamma}{2}(f-l)^{2}}{\lambda})\ \leq\log\frac{\gamma|\sigma\partial\_{x}l|^{2}+C\varepsilon}{\lambda}. |  |

This implies that, at (râˆ—,xâˆ—)(r^{\*},x^{\*}),

|  |  |  |
| --- | --- | --- |
|  | vâ‰¥fâˆ’Î»â€‹logâ¡Î³â€‹|Ïƒâ€‹âˆ‚xl|2+Câ€‹ÎµÎ».v\geq f-\lambda\log\frac{\gamma|\sigma\partial\_{x}l|^{2}+C\varepsilon}{\lambda}. |  |

Thus, we have a lower bound estimation

|  |  |  |
| --- | --- | --- |
|  | vâ‰¥âˆ’â€–fâ€–âˆž+Î»â€‹logâ¡Î³â€‹Câ€‹m2+Câ€‹ÎµÎ»âˆ’Îµâ€‹Ïˆ.v\geq-\|f\|\_{\infty}+\lambda\log\frac{\gamma Cm^{2}+C\varepsilon}{\lambda}-\varepsilon\psi. |  |

Letting Îµ\varepsilon go to 0, we finally get that

|  |  |  |
| --- | --- | --- |
|  | vâ‰¥âˆ’â€–fâ€–âˆž+Î»â€‹logâ¡Î³â€‹Câ€‹m2Î».v\geq-\|f\|\_{\infty}+\lambda\log\frac{\gamma Cm^{2}}{\lambda}. |  |

Step 2. Bound and derivative estimations for kk. Note that the second estimation of ([5](https://arxiv.org/html/2510.24128v1#S3.E5 "In 3 Extended HJB Equation for Regularized Problem â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")) is just a linear equation of kk. The well-posedness is straight-forward and one obtains the following bound estimation from Lemma [B.1](https://arxiv.org/html/2510.24128v1#A2.Thmlemma1 "Lemma B.1. â€£ Appendix B A Key Lemma â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")

|  |  |  |
| --- | --- | --- |
|  | â€–kâ€–âˆžâ‰¤â€–fâ€–âˆž.\|k\|\_{\infty}\leq\|f\|\_{\infty}. |  |

To give estimation of the derivative, note that, from lower bound estimation for VV in the first step, it holds that

|  |  |  |
| --- | --- | --- |
|  | expâ¡(âˆ’v+Î³2â€‹(fâˆ’l)2âˆ’fÎ»)â‰¤(Î³â€‹Câ€‹m2Î»)CÎ»â€‹(1+â€–fâ€–âˆž2+m2).\exp(-\frac{v+\frac{\gamma}{2}(f-l)^{2}-f}{\lambda})\leq(\frac{\gamma Cm^{2}}{\lambda})^{\frac{C}{\lambda}(1+\|f\|^{2}\_{\infty}+m^{2})}. |  |

Hence, using Lemma [B.1](https://arxiv.org/html/2510.24128v1#A2.Thmlemma1 "Lemma B.1. â€£ Appendix B A Key Lemma â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method") again, we have that

|  |  |  |
| --- | --- | --- |
|  | â€–âˆ‚xkâ€–âˆžâ‰¤(1+T)â€‹â€–âˆ‚xfâ€–âˆž+Câ€‹(T+T)â€‹(Î³â€‹Câ€‹m2Î»)CÎ»â€‹(1+â€–fâ€–âˆž2+m2).\|\partial\_{x}k\|\_{\infty}\leq(1+\sqrt{T})\|\partial\_{x}f\|\_{\infty}+C(\sqrt{T}+T)(\frac{\gamma Cm^{2}}{\lambda})^{\frac{C}{\lambda}(1+\|f\|^{2}\_{\infty}+m^{2})}. |  |

Then, for a sufficiently small TT, FF is mapping from Bmâ€‹(0)B\_{m}(0) into itself.

Step 3. Contraction of the mapping FF.
For any hiâˆˆBmâ€‹(0)h\_{i}\in B\_{m}(0) with i=1,2i=1,2, let (vi,ki)(v\_{i},k\_{i}) be the related solutions of ([5](https://arxiv.org/html/2510.24128v1#S3.E5 "In 3 Extended HJB Equation for Regularized Problem â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")). Denote by Î´â€‹v=v1âˆ’v2\delta v=v\_{1}-v\_{2}, Î´â€‹k=k1âˆ’k2\delta k=k\_{1}-k\_{2}, and Î´â€‹l=l1âˆ’l2\delta l=l\_{1}-l\_{2}. Then, we that (Î´â€‹v,Î´â€‹k)(\delta v,\delta k) satisfy

|  |  |  |  |
| --- | --- | --- | --- |
|  | {(âˆ‚t+â„’)â€‹Î´â€‹vâˆ’Î´â€‹eâ€‹(Î´â€‹vâˆ’Î³2â€‹(2â€‹fâˆ’l1âˆ’l2)â€‹Î´â€‹l)âˆ’Î³â€‹(|Ïƒâ€‹âˆ‚xl1|2âˆ’|Ïƒâ€‹âˆ‚xl2|2)=0,Î´â€‹vâ€‹(T,x)=0,(âˆ‚t+â„’)â€‹Î´â€‹kâˆ’expâ¡(âˆ’v1+Î³2â€‹(fâˆ’l1)2âˆ’fÎ»)â€‹Î´â€‹k+(k2âˆ’f)â€‹Î´â€‹eâ€‹Î´â€‹vâˆ’Î³2â€‹(2â€‹fâˆ’l1âˆ’l2)â€‹Î´â€‹lÎ»=0,Î´â€‹kâ€‹(T,x)=0.\left\{\begin{split}&(\partial\_{t}+\mathcal{L})\delta v-\delta e(\delta v-\frac{\gamma}{2}(2f-l\_{1}-l\_{2})\delta l)-\gamma(|\sigma\partial\_{x}l\_{1}|^{2}-|\sigma\partial\_{x}l\_{2}|^{2})=0,\delta v(T,x)=0,\\ &(\partial\_{t}+\mathcal{L})\delta k-\exp(-\frac{v\_{1}+\frac{\gamma}{2}(f-l\_{1})^{2}-f}{\lambda})\delta k+(k\_{2}-f)\delta e\frac{\delta v-\frac{\gamma}{2}(2f-l\_{1}-l\_{2})\delta l}{\lambda}=0,\delta k(T,x)=0.\end{split}\right. |  | (6) |

with

|  |  |  |
| --- | --- | --- |
|  | Î´â€‹e=âˆ«01expâ¡(âˆ’v1+Î³2â€‹(fâˆ’l1)2âˆ’f+sâ€‹(Î´â€‹vâˆ’Î³2â€‹(2â€‹fâˆ’l1âˆ’l2)â€‹Î´â€‹l)Î»)â€‹ð‘‘s.\delta e=\int\_{0}^{1}\exp(-\frac{v\_{1}+\frac{\gamma}{2}(f-l\_{1})^{2}-f+s(\delta v-\frac{\gamma}{2}(2f-l\_{1}-l\_{2})\delta l)}{\lambda})ds. |  |

From the estimation in previous step, we know that the related functions vi,kiv\_{i},k\_{i} and hih\_{i} are uniformly bounded. Thus, Î´â€‹e\delta e is a bounded function. Moreover, one gets that

|  |  |  |
| --- | --- | --- |
|  | â€–Î´â€‹eâ€‹Î³2â€‹(2â€‹fâˆ’l1âˆ’l2)â€‹Î´â€‹lâ€–âˆžâ‰¤Î³2â€‹â€–Î´â€‹eâ€‹(2â€‹fâˆ’l1âˆ’l2)â€–âˆžâ€‹â€–Î´â€‹lâ€–âˆžâ‰¤Câ€‹â€–Î´â€‹lâ€–âˆž,\|\delta e\frac{\gamma}{2}(2f-l\_{1}-l\_{2})\delta l\|\_{\infty}\leq\frac{\gamma}{2}\|\delta e(2f-l\_{1}-l\_{2})\|\_{\infty}\|\delta l\|\_{\infty}\leq C\|\delta l\|\_{\infty}, |  |

and

|  |  |  |
| --- | --- | --- |
|  | â€–|Ïƒâ€‹Dxâ€‹l1|2âˆ’|Ïƒâ€‹Dxâ€‹l2|2â€–âˆžâ‰¤Câ€‹â€–Ïƒâ€‹Dxâ€‹l1+Ïƒâ€‹Dxâ€‹l2â€–âˆžâ€‹â€–Ïƒâ€‹Dxâ€‹l1âˆ’Ïƒâ€‹Dxâ€‹l2â€–âˆžâ‰¤Câ€‹â€–Ïƒâ€‹Dxâ€‹l1âˆ’Ïƒâ€‹Dxâ€‹l2â€–âˆž.\||\sigma D\_{x}l\_{1}|^{2}-|\sigma D\_{x}l\_{2}|^{2}\|\_{\infty}\leq C\|\sigma D\_{x}l\_{1}+\sigma D\_{x}l\_{2}\|\_{\infty}\|\sigma D\_{x}l\_{1}-\sigma D\_{x}l\_{2}\|\_{\infty}\leq C\|\sigma D\_{x}l\_{1}-\sigma D\_{x}l\_{2}\|\_{\infty}. |  |

Then, using Lemma [B.1](https://arxiv.org/html/2510.24128v1#A2.Thmlemma1 "Lemma B.1. â€£ Appendix B A Key Lemma â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method") again, we have that there exists a constant CC depending on the coefficients, mm and Î»\lambda such that

|  |  |  |
| --- | --- | --- |
|  | âˆ¥Î´vâˆ¥âˆžâ‰¤CT(âˆ¥Î´lâˆ¥âˆž+âˆ¥âˆ‚xÎ´lâˆ¥âˆž.\|\delta v\|\_{\infty}\leq CT(\|\delta l\|\_{\infty}+\|\partial\_{x}\delta l\|\_{\infty}. |  |

Furthermore, with a similar argument,

|  |  |  |
| --- | --- | --- |
|  | â€–Î´â€‹kâ€–âˆžâ‰¤Câ€‹Tâ€‹â€–(k2âˆ’f)â€‹Î´â€‹eâ€‹Î´â€‹vâˆ’Î³2â€‹(2â€‹fâˆ’l1âˆ’l2)â€‹Î´â€‹lÎ»â€–âˆžâ‰¤Câ€‹Tâ€‹(â€–Î´â€‹vâ€–âˆž+â€–Î´â€‹lâ€–âˆž).,\|\delta k\|\_{\infty}\leq CT\|(k\_{2}-f)\delta e\frac{\delta v-\frac{\gamma}{2}(2f-l\_{1}-l\_{2})\delta l}{\lambda}\|\_{\infty}\leq CT(\|\delta v\|\_{\infty}+\|\delta l\|\_{\infty})., |  |

and

|  |  |  |
| --- | --- | --- |
|  | â€–âˆ‚xÎ´â€‹kâ€–âˆžâ‰¤Câ€‹(T+T)â€‹(â€–(k2âˆ’f)â€‹Î´â€‹eâ€‹Î´â€‹vâˆ’Î³2â€‹(2â€‹fâˆ’l1âˆ’l2)â€‹Î´â€‹lÎ»â€–âˆž+â€–expâ¡(âˆ’v1+Î³2â€‹(fâˆ’l1)2âˆ’fÎ»)â€‹Î´â€‹kâ€–âˆž)â‰¤Câ€‹(T+T)â€‹(â€–Î´â€‹vâ€–âˆž+â€–Î´â€‹lâ€–âˆž+â€–Î´â€‹kâ€–âˆž).\begin{split}\|\partial\_{x}\delta k\|\_{\infty}\leq&C(\sqrt{T}+T)(\|(k\_{2}-f)\delta e\frac{\delta v-\frac{\gamma}{2}(2f-l\_{1}-l\_{2})\delta l}{\lambda}\|\_{\infty}+\|\exp(-\frac{v\_{1}+\frac{\gamma}{2}(f-l\_{1})^{2}-f}{\lambda})\delta k\|\_{\infty})\\ \leq&C(\sqrt{T}+T)(\|\delta v\|\_{\infty}+\|\delta l\|\_{\infty}+\|\delta k\|\_{\infty}).\end{split} |  |

Combining these estimations, we see that FF is a contraction for a sufficiently small TT.
âˆŽ

## 4 Extended HJB Equation for Original Problem

For the optimal stopping problem, i.e., Î³=0\gamma=0, [[9](https://arxiv.org/html/2510.24128v1#bib.bib9)] proves that the first equation in ([3](https://arxiv.org/html/2510.24128v1#S3.E3 "In Theorem 3.1. â€£ 3 Extended HJB Equation for Regularized Problem â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")) converges to the variational inequality satisfied by the value function. For MV stopping problem, we would like follow the same procedure. Unfortunately, at present, we can only formally deduce the limiting equation. Below is a brief introduction. Assume that
(VÎ»,gÎ»)(V^{\lambda},g^{\lambda}) converge to some functions (V,g)(V,g) when Î»\lambda goes to 0 Then, it is natural to conjecture that

|  |  |  |
| --- | --- | --- |
|  | âˆ‚tV+â„’â€‹Vâˆ’Î³â€‹|Ïƒâ€‹âˆ‚xg|2=limÎ»â†’0âˆ‚tVÎ»+â„’â€‹VÎ»âˆ’Î³â€‹|Ïƒâ€‹âˆ‚xgÎ»|2â‰¤0.\partial\_{t}V+\mathcal{L}V-\gamma\left|\sigma\partial\_{x}g\right|^{2}=\lim\_{\lambda\rightarrow 0}\partial\_{t}V^{\lambda}+\mathcal{L}V^{\lambda}-\gamma\left|\sigma\partial\_{x}g^{\lambda}\right|^{2}\leq 0. |  |

If âˆ‚tVÎ»+â„’â€‹VÎ»\partial\_{t}V^{\lambda}+\mathcal{L}V^{\lambda} and âˆ‚xgÎ»\partial\_{x}g^{\lambda} are bounded uniformly in Î»\lambda, then so is Î»â€‹expâ¡(âˆ’VÎ»+Î³2â€‹(fâˆ’gÎ»)2âˆ’fÎ»)\lambda\exp(-\frac{V^{\lambda}+\frac{\gamma}{2}(f-g^{\lambda})^{2}-f}{\lambda}). This suggests that V+Î³2â€‹(fâˆ’g)2â‰¥fV+\frac{\gamma}{2}(f-g)^{2}\geq f. Moreover, if V+Î³2â€‹(fâˆ’g)2>fV+\frac{\gamma}{2}(f-g)^{2}>f, one has expâ¡(âˆ’VÎ»+Î³2â€‹(fâˆ’gÎ»)2âˆ’fÎ»)\exp(-\frac{V^{\lambda}+\frac{\gamma}{2}(f-g^{\lambda})^{2}-f}{\lambda}) converges to 0, which implies âˆ‚tV+â„’â€‹Vâˆ’Î³â€‹|Ïƒâ€‹âˆ‚xg|2=0\partial\_{t}V+\mathcal{L}V-\gamma\left|\sigma\partial\_{x}g\right|^{2}=0, and âˆ‚tg+â„’â€‹g=0\partial\_{t}g+\mathcal{L}g=0. From ([3](https://arxiv.org/html/2510.24128v1#S3.E3 "In Theorem 3.1. â€£ 3 Extended HJB Equation for Regularized Problem â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")), it is not clear what is satisfied by gg on the set {V+Î³2â€‹(fâˆ’g)2=f}\{V+\frac{\gamma}{2}(f-g)^{2}=f\}. But, from the probabilistic representation of gÎ»g^{\lambda}, one also guess that gâ€‹(t,x)=ð”¼t,xâ€‹[fâ€‹(XÏ„)]g(t,x)=\mathbb{E}\_{t,x}[f(X\_{\tau})] for a stopping time Ï„\tau. If Ï„\tau is the hitting time of the set {V+Î³2â€‹(fâˆ’g)2=f}\{V+\frac{\gamma}{2}(f-g)^{2}=f\}, Then, gg should equal to ff on that set. In summary, ([3](https://arxiv.org/html/2510.24128v1#S3.E3 "In Theorem 3.1. â€£ 3 Extended HJB Equation for Regularized Problem â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")) formally converges to the following system

|  |  |  |  |
| --- | --- | --- | --- |
|  | {minâ¡{âˆ’(âˆ‚tV+â„’â€‹Vâˆ’Î³â€‹|Ïƒâ€‹âˆ‚xg|2),V+Î³2â€‹(fâˆ’g)2âˆ’f}=0,Vâ€‹(T,x)=f,âˆ‚tg+â„’â€‹g=0,Â onÂ {V+Î³2â€‹(fâˆ’g)2>f},g=fâ€‹Â onÂ {V+Î³2â€‹(fâˆ’g)2=f},gâ€‹(T,x)=fâ€‹(x).\left\{\begin{split}&\min\left\{-\left(\partial\_{t}V+\mathcal{L}V-\gamma\left|\sigma\partial\_{x}g\right|^{2}\right),V+\frac{\gamma}{2}(f-g)^{2}-f\right\}=0,V(T,x)=f,\\ &\partial\_{t}g+\mathcal{L}g=0,\text{ on $\{V+\frac{\gamma}{2}(f-g)^{2}>f\}$},\\ &g=f\text{ on $\{V+\frac{\gamma}{2}(f-g)^{2}=f\}$},g(T,x)=f(x).\end{split}\right. |  | (7) |

Now, let us assume that the above system admits a pair of solution (V,g)(V,g). The means that (V,g)(V,g) is a pair of continuous functions, second-order continuous differentiable in the region {V+Î³2â€‹(fâˆ’g)2>f}\{V+\frac{\gamma}{2}(f-g)^{2}>f\} and satisfies ([7](https://arxiv.org/html/2510.24128v1#S4.E7 "In 4 Extended HJB Equation for Original Problem â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")). Define the set

|  |  |  |
| --- | --- | --- |
|  | ð’ž={(t,x)|V+Î³2â€‹(fâˆ’g)2>f}.\mathcal{C}=\{(t,x)|V+\frac{\gamma}{2}(f-g)^{2}>f\}. |  |

Construct the stopping time Ï„ð’ž\tau\_{\mathcal{C}} as

|  |  |  |
| --- | --- | --- |
|  | Ï„ð’ž=inf{sâ‰¥t|(s,Xs)âˆ‰ð’ž}.\tau\_{\mathcal{C}}=\inf\{s\geq t|(s,X\_{s})\notin\mathcal{C}\}. |  |

One can verify that

|  |  |  |
| --- | --- | --- |
|  | Vâ€‹(t,x)=ð”¼t,xâ€‹[fâ€‹(XÏ„ð’ž)]âˆ’Î³2â€‹Vart,xâ€‹[fâ€‹(XÏ„ð’ž)],Â andÂ â€‹gâ€‹(t,x)=ð”¼t,xâ€‹[fâ€‹(XÏ„ð’ž)].V(t,x)=\mathbb{E}\_{t,x}\left[f(X\_{\tau\_{\mathcal{C}}})\right]-\frac{\gamma}{2}\text{Var}\_{t,x}\left[f(X\_{\tau\_{\mathcal{C}}})\right],\text{ and }g(t,x)=\mathbb{E}\_{t,x}\left[f(X\_{\tau\_{\mathcal{C}}})\right]. |  |

Moreover, we also define

|  |  |  |
| --- | --- | --- |
|  | hâ€‹(t,x):=Vâ€‹(t,x)âˆ’Î³2â€‹g2â€‹(t,x)=ð”¼t,xâ€‹[(fâˆ’Î³2â€‹f2)â€‹(XÏ„ð’ž)].h(t,x):=V(t,x)-\frac{\gamma}{2}g^{2}(t,x)=\mathbb{E}\_{t,x}\left[(f-\frac{\gamma}{2}f^{2})(X\_{\tau\_{\mathcal{C}}})\right]. |  |

Next, we prove that these functions characterize a stopping policy that is an equilibrium in some sense. Before that, let us first introduce some definitions. For any (t,x)âˆˆâ„Ã—â„n(t,x)\in\mathbb{R}\times\mathbb{R}^{n} and r>0r>0, the parabolic cylinder Qâ€‹(t,x;r)Q(t,x;r) is defined as333The definition of parabolic cylinders is different from that in text books of parabolic PDEs, see [[13](https://arxiv.org/html/2510.24128v1#bib.bib13)]. The reason is that we consider PDEs with terminal conditions instead of initial conditions.

|  |  |  |
| --- | --- | --- |
|  | Qâ€‹(t,x;r):={(s,y)âˆˆâ„Ã—â„n|maxâ¡{|xâˆ’y|,(sâˆ’t)12}â‰¤r,sâ‰¥t}.Q(t,x;r):=\{(s,y)\in\mathbb{R}\times\mathbb{R}^{n}|\max\{|x-y|,(s-t)^{\frac{1}{2}}\}\leq r,s\geq t\}. |  |

For any set Î©\Omega, the parabolic boundary ð’«â€‹Î©\mathcal{P}\Omega is defined as the set of all points (t,x)âˆˆÎ©Â¯(t,x)\in\bar{\Omega} such that for any Îµ>0\varepsilon>0, Qâ€‹(t,x;Îµ)Q(t,x;\varepsilon) contains points not in Î©\Omega. Finally, note that, since ð’ž\mathcal{C} is a open set, for any (t,x)âˆˆð’ž(t,x)\in\mathcal{C}, there exists Îµ>0\varepsilon>0 such that Qâ€‹(t,x;Îµ)âˆˆð’žQ(t,x;\varepsilon)\in\mathcal{C}.

The notion of equilibrium strategy is similar to that used in [[7](https://arxiv.org/html/2510.24128v1#bib.bib7)].
For any vâ‰¥0v\geq 0, let NvN^{v} be a Poisson point process independent of the Brownian motion WW. Its first jump time after tt is denoted as Ï„v\tau^{v}, i.e.,

|  |  |  |
| --- | --- | --- |
|  | Ï„v=inf{sâ‰¥t|Nsâˆ’Nt=1}.\tau^{v}=\inf\left\{s\geq t|N\_{s}-N\_{t}=1\right\}. |  |

For any Îµ\varepsilon, define two stopping time

|  |  |  |
| --- | --- | --- |
|  | Ï„Îµ=inf{sâ‰¥t||Xsâˆ’Xt|â‰¥Îµ}âˆ§(t+Îµ)âˆ§T,\tau^{\varepsilon}=\inf\left\{s\geq t||X\_{s}-X\_{t}|\geq\varepsilon\right\}\wedge(t+\varepsilon)\wedge T, |  |

and

|  |  |  |
| --- | --- | --- |
|  | Ï„~ð’ž=inf{sâ‰¥Ï„Îµ|(s,Xs)âˆ‰ð’ž}.\tilde{\tau}\_{\mathcal{C}}=\inf\left\{s\geq\tau^{\varepsilon}|(s,X\_{s})\notin\mathcal{C}\right\}. |  |

The perturbation Ï„ð’žÎµ,v\tau^{\varepsilon,v}\_{\mathcal{C}} of Ï„ð’ž\tau\_{\mathcal{C}} is defined as,

|  |  |  |
| --- | --- | --- |
|  | Ï„ð’žÎµ,v=1{Ï„vâ‰¤Ï„Îµ}â€‹Ï„v+1{Ï„v>Ï„Îµ}â€‹Ï„~ð’ž.\tau\_{\mathcal{C}}^{\varepsilon,v}=1\_{\{\tau^{v}\leq\tau^{\varepsilon}\}}\tau^{v}+1\_{\{\tau^{v}>\tau^{\varepsilon}\}}\tilde{\tau}\_{\mathcal{C}}. |  |

###### Theorem 4.1.

Assume that there exists a solution (V,g)(V,g) of ([7](https://arxiv.org/html/2510.24128v1#S4.E7 "In 4 Extended HJB Equation for Original Problem â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")) such that the functions and their derivatives are polynomial-growth w.r.t. xx uniformly in tt. For any (t,x)âˆˆð’žâˆª(ð’žc/ð’«â€‹ð’žc)(t,x)\in\mathcal{C}\cup(\mathcal{C}^{c}/\mathcal{P}\mathcal{C}^{c}), it holds that

|  |  |  |
| --- | --- | --- |
|  | lim infÎµâ†’0Jâ€‹(t,x;Ï„ð’ž)âˆ’Jâ€‹(t,x;Ï„ð’žÎµ,v)ð”¼t,xâ€‹[Ï„Îµâˆ’t]â‰¥0.\liminf\_{\varepsilon\to 0}\frac{J(t,x;\tau\_{\mathcal{C}})-J(t,x;\tau\_{\mathcal{C}}^{\varepsilon,v})}{\mathbb{E}\_{t,x}[\tau^{\varepsilon}-t]}\geq 0. |  |

###### Proof.

From the definition of Ï„ð’žÎµ,v\tau\_{\mathcal{C}}^{\varepsilon,v}, it holds that

|  |  |  |
| --- | --- | --- |
|  | ð”¼t,xâ€‹[fâ€‹(XÏ„ð’žÎµ,v)]=ð”¼t,xâ€‹[âˆ«tÏ„Îµfâ€‹(Xs)â€‹vâ€‹eâˆ’vâ€‹(sâˆ’t)â€‹ð‘‘s+fâ€‹(XÏ„~ð’ž)â€‹eâˆ’vâ€‹(Ï„Îµâˆ’t)]=ð”¼t,xâ€‹[âˆ«tÏ„Îµfâ€‹(Xs)â€‹vâ€‹eâˆ’vâ€‹(sâˆ’t)â€‹ð‘‘s+gâ€‹(Ï„Îµ,XÏ„Îµ)â€‹eâˆ’vâ€‹(Ï„Îµâˆ’t)].\begin{split}&\mathbb{E}\_{t,x}\left[f(X\_{\tau\_{\mathcal{C}}^{\varepsilon,v}})\right]\\ =&\mathbb{E}\_{t,x}\left[\int\_{t}^{\tau^{\varepsilon}}f(X\_{s})ve^{-v(s-t)}ds+f(X\_{\tilde{\tau}\_{\mathcal{C}}})e^{-v(\tau^{\varepsilon}-t)}\right]\\ =&\mathbb{E}\_{t,x}\left[\int\_{t}^{\tau^{\varepsilon}}f(X\_{s})ve^{-v(s-t)}ds+g(\tau^{\varepsilon},X\_{\tau^{\varepsilon}})e^{-v(\tau^{\varepsilon}-t)}\right].\end{split} |  |

Note that, for sufficiently small Îµ\varepsilon, XsX\_{s} stays in ð’ž\mathcal{C} or ð’žc\mathcal{C}^{c} for sâˆˆ[t,Ï„Îµ]s\in[t,\tau^{\varepsilon}] as we assume that (t,x)âˆˆð’žâˆª(ð’žc/ð’«â€‹ð’žc)(t,x)\in\mathcal{C}\cup(\mathcal{C}^{c}/\mathcal{P}\mathcal{C}^{c}). Thus, one can apply ItÃ´ formula to get that

|  |  |  |
| --- | --- | --- |
|  | ð”¼t,xâ€‹[gâ€‹(Ï„Îµ,XÏ„Îµ)â€‹eâˆ’vâ€‹Ï„Îµ]=gâ€‹(t,x)+ð”¼â€‹[âˆ«tÏ„Îµ(âˆ‚t+â„’)â€‹gâ€‹(s,Xs)â€‹eâˆ’vâ€‹(sâˆ’t)âˆ’vâ€‹gâ€‹(s,Xs)â€‹eâˆ’vâ€‹(sâˆ’t)â€‹dâ€‹s].\begin{split}&\mathbb{E}\_{t,x}\left[g(\tau^{\varepsilon},X\_{\tau^{\varepsilon}})e^{-v\tau^{\varepsilon}}\right]\\ =&g(t,x)+\mathbb{E}\left[\int\_{t}^{\tau^{\varepsilon}}(\partial\_{t}+\mathcal{L})g(s,X\_{s})e^{-v(s-t)}-vg(s,X\_{s})e^{-v(s-t)}ds\right].\end{split} |  |

Then, we have

|  |  |  |
| --- | --- | --- |
|  | ð”¼t,xâ€‹[fâ€‹(XÏ„ð’žÎµ,v)]=gâ€‹(t,x)+((âˆ‚t+â„’)â€‹gâ€‹(t,x)+vâ€‹(fâˆ’g)â€‹(t,x))â€‹ð”¼â€‹[Ï„Îµâˆ’t]+oâ€‹(ð”¼â€‹[Ï„Îµâˆ’t]),\begin{split}&\mathbb{E}\_{t,x}\left[f(X\_{\tau^{\varepsilon,v}\_{\mathcal{C}}})\right]\\ =&g(t,x)+\left((\partial\_{t}+\mathcal{L})g(t,x)+v(f-g)(t,x)\right)\mathbb{E}\left[\tau^{\varepsilon}-t\right]+o(\mathbb{E}\left[\tau^{\varepsilon}-t\right]),\end{split} |  |

which implies that

|  |  |  |
| --- | --- | --- |
|  | (ð”¼t,xâ€‹[fâ€‹(XÏ„ð’žÎµ,v)])2=g2â€‹(t,x)+2â€‹gâ€‹(t,x)â€‹((âˆ‚t+â„’)â€‹gâ€‹(t,x)+vâ€‹(fâˆ’g)â€‹(t,x))â€‹ð”¼â€‹[Ï„Îµâˆ’t]+oâ€‹(ð”¼â€‹[Ï„Îµâˆ’t]).\begin{split}&\left(\mathbb{E}\_{t,x}\left[f(X\_{\tau^{\varepsilon,v}\_{\mathcal{C}}})\right]\right)^{2}\\ =&g^{2}(t,x)+2g(t,x)\left((\partial\_{t}+\mathcal{L})g(t,x)+v(f-g)(t,x)\right)\mathbb{E}\left[\tau^{\varepsilon}-t\right]+o(\mathbb{E}\left[\tau^{\varepsilon}-t\right]).\end{split} |  |

Similarly, it holds that

|  |  |  |
| --- | --- | --- |
|  | ð”¼t,xâ€‹[(fâˆ’Î³2â€‹f2)â€‹(XÏ„ð’žÎµ,v)]=hâ€‹(t,x)+((âˆ‚t+â„’)â€‹(h)+vâ€‹(fâˆ’Î³2â€‹f2âˆ’h))â€‹ð”¼â€‹[Ï„Îµâˆ’t]+oâ€‹(ð”¼â€‹[Ï„Îµâˆ’t]).\begin{split}&\mathbb{E}\_{t,x}\left[(f-\frac{\gamma}{2}f^{2})(X\_{\tau^{\varepsilon,v}\_{\mathcal{C}}})\right]\\ =&h(t,x)+\left((\partial\_{t}+\mathcal{L})(h)+v(f-\frac{\gamma}{2}f^{2}-h)\right)\mathbb{E}\left[\tau^{\varepsilon}-t\right]+o(\mathbb{E}\left[\tau^{\varepsilon}-t\right]).\end{split} |  |

Hence, recalling that h=Vâˆ’Î³2â€‹g2h=V-\frac{\gamma}{2}g^{2},

|  |  |  |
| --- | --- | --- |
|  | Jâ€‹(t,x;Ï„ð’žÎµ,v)âˆ’Jâ€‹(t,x;Ï„ð’ž)=((âˆ‚t+â„’)â€‹h+Î³â€‹gâ€‹(âˆ‚t+â„’)â€‹g+vâ€‹(fâˆ’(V+Î³2â€‹(fâˆ’g)2)))â€‹ð”¼â€‹[Ï„Îµâˆ’t]+oâ€‹(ð”¼â€‹[Ï„Îµâˆ’t])=((âˆ‚t+â„’)â€‹Vâˆ’Î³â€‹|Ïƒâ€‹âˆ‚xg|2+vâ€‹(fâˆ’(V+Î³2â€‹(fâˆ’g)2)))â€‹ð”¼â€‹[Ï„Îµâˆ’t]+oâ€‹(ð”¼â€‹[Ï„Îµâˆ’t]).\begin{split}&J(t,x;\tau\_{\mathcal{C}}^{\varepsilon,v})-J(t,x;\tau\_{\mathcal{C}})\\ =&((\partial\_{t}+\mathcal{L})h+\gamma g(\partial\_{t}+\mathcal{L})g+v(f-(V+\frac{\gamma}{2}(f-g)^{2})))\mathbb{E}[\tau^{\varepsilon}-t]+o(\mathbb{E}[\tau^{\varepsilon}-t])\\ =&((\partial\_{t}+\mathcal{L})V-\gamma|\sigma\partial\_{x}g|^{2}+v(f-(V+\frac{\gamma}{2}(f-g)^{2})))\mathbb{E}[\tau^{\varepsilon}-t]+o(\mathbb{E}[\tau^{\varepsilon}-t]).\end{split} |  |

The first equation in ([7](https://arxiv.org/html/2510.24128v1#S4.E7 "In 4 Extended HJB Equation for Original Problem â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")) implies the desired result.
âˆŽ

For the case (t,x)âˆˆð’«â€‹ð’žc\{T}Ã—â„(t,x)\in\mathcal{P}\mathcal{C}^{c}\backslash\{T\}\times\mathbb{R}, it turns out to be a very subtle problem. Thus, we focus on one dimensional case, i.e., xâˆˆâ„x\in\mathbb{R} and assume that the free boundary ð’«â€‹ð’žc\{T}Ã—â„\mathcal{P}\mathcal{C}^{c}\backslash\{T\}\times\mathbb{R} is locally Lipschitz continuous with respect to time tt. More precisely, there exists a small ball Qâ€‹(t,x;r)Q(t,x;r) and a Lipschitz continuous curve cc and r>0r>0 such that ð’žâ€‹â‹‚Qâ€‹(t,x;r)={(s,y)|tâ‰¤sâ‰¤t+r12,|yâˆ’x|â‰¤r,yâ‰¥câ€‹(s)}\mathcal{C}\bigcap Q(t,x;r)=\{(s,y)|t\leq s\leq t+r^{\frac{1}{2}},|y-x|\leq r,y\geq c(s)\}.

###### Theorem 4.2.

In addition to the assumption in Theorem [4.1](https://arxiv.org/html/2510.24128v1#S4.Thmtheorem1 "Theorem 4.1. â€£ 4 Extended HJB Equation for Original Problem â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method") and locally Lipschitz assumption on the free boundary, we further assume
that VV is C1C^{1} in Qâ€‹(t,x;r)Q(t,x;r). If it holds that, for (t,x)âˆˆð’«â€‹ð’žc\{T}Ã—â„(t,x)\in\mathcal{P}\mathcal{C}^{c}\backslash\{T\}\times\mathbb{R},

|  |  |  |  |
| --- | --- | --- | --- |
|  | (âˆ‚t+â„’)â€‹Vâ€‹(t,x+)+(âˆ‚t+â„’)â€‹Vâ€‹(t,xâˆ’)â‰¤Î³â€‹Ïƒ2â€‹(t,x)â€‹(âˆ‚xgâ€‹(t,x+)+âˆ‚xgâ€‹(t,xâˆ’)2)2,(\partial\_{t}+\mathcal{L})V(t,x+)+(\partial\_{t}+\mathcal{L})V(t,x-)\leq\gamma\sigma^{2}(t,x)\left(\frac{\partial\_{x}g(t,x+)+\partial\_{x}g(t,x-)}{2}\right)^{2}, |  | (8) |

then we have

|  |  |  |
| --- | --- | --- |
|  | lim infÎµâ†’0Jâ€‹(t,x;Ï„ð’ž)âˆ’Jâ€‹(t,x;Ï„ð’žÎµ,v)ð”¼t,xâ€‹[Ï„Îµâˆ’t]â‰¥0.\liminf\_{\varepsilon\rightarrow 0}\frac{J(t,x;\tau\_{\mathcal{C}})-J(t,x;\tau\_{\mathcal{C}}^{\varepsilon,v})}{\mathbb{E}\_{t,x}[\tau^{\varepsilon}-t]}\geq 0. |  |

###### Proof.

From ItÃ´-Tanaka formula (see [[15](https://arxiv.org/html/2510.24128v1#bib.bib15)]), one can get that

|  |  |  |
| --- | --- | --- |
|  | g(Ï„Îµ,XÏ„Îµ)eâˆ’(Ï„Îµâˆ’t)=g(t,x)+âˆ«tÏ„Îµ12((âˆ‚t+â„’)g(s,Xs+)eâˆ’vâ€‹(sâˆ’t)âˆ’vg(s,Xs+)eâˆ’vâ€‹(sâˆ’t)+(âˆ‚t+â„’)g(s,Xsâˆ’)eâˆ’vâ€‹(sâˆ’t)âˆ’vg(s,Xsâˆ’)eâˆ’vâ€‹(sâˆ’t))1{Xsâ‰ câ€‹(s)}ds+âˆ«tÏ„Îµ12â€‹Ïƒâ€‹(âˆ‚xgâ€‹(s,Xs+)+âˆ‚xgâ€‹(s,Xsâˆ’))â€‹eâˆ’vâ€‹(tâˆ’s)â€‹ð‘‘Ws+12â€‹âˆ«tÏ„Îµ(âˆ‚xgâ€‹(s,Xs+)âˆ’âˆ‚xgâ€‹(s,Xsâˆ’))â€‹eâˆ’vâ€‹(sâˆ’t)â€‹1{Xs=câ€‹(s)}â€‹ð‘‘lsc,\begin{split}&g(\tau^{\varepsilon},X\_{\tau^{\varepsilon}})e^{-(\tau^{\varepsilon}-t)}=g(t,x)+\int\_{t}^{\tau^{\varepsilon}}\frac{1}{2}\bigg((\partial\_{t}+\mathcal{L})g(s,X\_{s}+)e^{-v(s-t)}-vg(s,X\_{s}+)e^{-v(s-t)}\\ &+(\partial\_{t}+\mathcal{L})g(s,X\_{s}-)e^{-v(s-t)}-vg(s,X\_{s}-)e^{-v(s-t)}\bigg)1\_{\{X\_{s}\neq c(s)\}}ds\\ &+\int\_{t}^{\tau^{\varepsilon}}\frac{1}{2}\sigma(\partial\_{x}g(s,X\_{s}+)+\partial\_{x}g(s,X\_{s}-))e^{-v(t-s)}dW\_{s}\\ &+\frac{1}{2}\int\_{t}^{\tau^{\varepsilon}}(\partial\_{x}g(s,X\_{s}+)-\partial\_{x}g(s,X\_{s}-))e^{-v(s-t)}1\_{\{X\_{s}=c(s)\}}dl^{c}\_{s},\end{split} |  |

where lscl^{c}\_{s} is the local time of XX at the curve cc. Then, we have that

|  |  |  |
| --- | --- | --- |
|  | ð”¼t,x[âˆ«tÏ„Îµ12((âˆ‚t+â„’)g(s,Xs+)eâˆ’vâ€‹(sâˆ’t)âˆ’vg(s,Xs+)eâˆ’vâ€‹(sâˆ’t)+(âˆ‚t+â„’)g(s,Xsâˆ’)eâˆ’vâ€‹(sâˆ’t)âˆ’vg(s,Xsâˆ’)eâˆ’vâ€‹(sâˆ’t))1{Xsâ‰ câ€‹(s)}ds]=12(((âˆ‚t+â„’)g(t,x+)+(âˆ‚t+â„’)g(t,xâˆ’)âˆ’2vg(t,x))ð”¼[Ï„Îµâˆ’t]+o(ð”¼[Ï„Îµâˆ’t]).\begin{split}&\mathbb{E}\_{t,x}\bigg[\int\_{t}^{\tau^{\varepsilon}}\frac{1}{2}\bigg((\partial\_{t}+\mathcal{L})g(s,X\_{s}+)e^{-v(s-t)}-vg(s,X\_{s}+)e^{-v(s-t)}\\ &+(\partial\_{t}+\mathcal{L})g(s,X\_{s}-)e^{-v(s-t)}-vg(s,X\_{s}-)e^{-v(s-t)}\bigg)1\_{\{X\_{s}\neq c(s)\}}ds\bigg]\\ =&\frac{1}{2}\left(((\partial\_{t}+\mathcal{L})g(t,x+)+(\partial\_{t}+\mathcal{L})g(t,x-)-2vg(t,x)\right)\mathbb{E}[\tau^{\varepsilon}-t]+o(\mathbb{E}[\tau^{\varepsilon}-t]).\end{split} |  |

It is proved in Appendix [A](https://arxiv.org/html/2510.24128v1#A1 "Appendix A Proof of (9) â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method") that

|  |  |  |  |
| --- | --- | --- | --- |
|  | (ð”¼t,xâ€‹[lÏ„Îµcâˆ’ltc])2=Ïƒ2â€‹(t,x)â€‹ð”¼t,xâ€‹[Ï„Îµâˆ’t]+oâ€‹(ð”¼t,xâ€‹[Ï„Îµâˆ’t]),\left(\mathbb{E}\_{t,x}[l^{c}\_{\tau^{\varepsilon}}-l\_{t}^{c}]\right)^{2}=\sigma^{2}(t,x)\mathbb{E}\_{t,x}[\tau^{\varepsilon}-t]+o(\mathbb{E}\_{t,x}[\tau^{\varepsilon}-t]), |  | (9) |

which further implies that

|  |  |  |  |
| --- | --- | --- | --- |
|  | (ð”¼t,xâ€‹[âˆ«tÏ„Îµ(âˆ‚xgâ€‹(s,Xs+)âˆ’âˆ‚xgâ€‹(s,Xsâˆ’))â€‹eâˆ’vâ€‹(sâˆ’t)â€‹1{Xs=câ€‹(s)}â€‹ð‘‘lsc])2=(âˆ‚xgâ€‹(t,x+)âˆ’âˆ‚xgâ€‹(t,xâˆ’))2â€‹Ïƒ2â€‹(t,x)â€‹ð”¼t,xâ€‹[Ï„Îµâˆ’t]+oâ€‹(ð”¼t,xâ€‹[Ï„Îµâˆ’t]).\begin{split}&\left(\mathbb{E}\_{t,x}[\int\_{t}^{\tau^{\varepsilon}}(\partial\_{x}g(s,X\_{s}+)-\partial\_{x}g(s,X\_{s}-))e^{-v(s-t)}1\_{\{X\_{s}=c(s)\}}dl^{c}\_{s}]\right)^{2}\\ =&\left(\partial\_{x}g(t,x+)-\partial\_{x}g(t,x-)\right)^{2}\sigma^{2}(t,x)\mathbb{E}\_{t,x}[\tau^{\varepsilon}-t]+o(\mathbb{E}\_{t,x}[\tau^{\varepsilon}-t]).\end{split} |  | (10) |

Hence,

|  |  |  |
| --- | --- | --- |
|  | (ð”¼t,x[eâˆ’vâ€‹Ï„Îµg(Ï„Îµ,XÏ„Îµ)])2=g2(t,x)+(((âˆ‚t+â„’)g(t,x+)+(âˆ‚t+â„’)g(t,xâˆ’))g(t,x)âˆ’2vg12(t,x)+(âˆ‚xgâ€‹(t,x+)âˆ’âˆ‚xgâ€‹(t,xâˆ’)2)2Ïƒ2(t,x))ð”¼t,x[Ï„Îµâˆ’t]+gâ€‹(t,x)â€‹ð”¼t,xâ€‹[âˆ«tÏ„Îµ(âˆ‚xgâ€‹(s,Xs+)âˆ’âˆ‚xgâ€‹(s,Xsâˆ’))â€‹eâˆ’vâ€‹(sâˆ’t)â€‹1{Xs=câ€‹(s)}â€‹ð‘‘lsc]+oâ€‹(ð”¼t,xâ€‹[Ï„Îµâˆ’t]).\begin{split}&(\mathbb{E}\_{t,x}[e^{-v\tau^{\varepsilon}}g(\tau^{\varepsilon},X\_{\tau^{\varepsilon}})])^{2}=g^{2}(t,x)+\bigg(((\partial\_{t}+\mathcal{L})g(t,x+)+(\partial\_{t}+\mathcal{L})g(t,x-))g(t,x)-2vg^{2}\_{1}(t,x)\\ &+\left(\frac{\partial\_{x}g(t,x+)-\partial\_{x}g(t,x-)}{2}\right)^{2}\sigma^{2}(t,x)\bigg)\mathbb{E}\_{t,x}[\tau^{\varepsilon}-t]\\ &+g(t,x)\mathbb{E}\_{t,x}[\int\_{t}^{\tau^{\varepsilon}}(\partial\_{x}g(s,X\_{s}+)-\partial\_{x}g(s,X\_{s}-))e^{-v(s-t)}1\_{\{X\_{s}=c(s)\}}dl^{c}\_{s}]+o(\mathbb{E}\_{t,x}[\tau^{\varepsilon}-t]).\end{split} |  |

Similarly,

|  |  |  |
| --- | --- | --- |
|  | ð”¼t,xâ€‹[eâˆ’vâ€‹Ï„Îµâ€‹hâ€‹(Ï„Îµ,XÏ„Îµ)]=hâ€‹(t,x)+12â€‹((âˆ‚t+â„’)â€‹hâ€‹(t,x+)+(âˆ‚t+â„’)â€‹hâ€‹(t,xâˆ’)âˆ’vâ€‹hâ€‹(t,x))â€‹ð”¼t,xâ€‹[Ï„Îµâˆ’t]+12â€‹ð”¼t,xâ€‹[âˆ«tÏ„Îµ(âˆ‚xhâ€‹(s,Xs+)âˆ’âˆ‚xhâ€‹(s,Xsâˆ’))â€‹eâˆ’vâ€‹(sâˆ’t)â€‹1{Xs=câ€‹(s)}â€‹ð‘‘lsc].\begin{split}&\mathbb{E}\_{t,x}[e^{-v\tau^{\varepsilon}}h(\tau^{\varepsilon},X\_{\tau^{\varepsilon}})]\\ =&h(t,x)+\frac{1}{2}\left((\partial\_{t}+\mathcal{L})h(t,x+)+(\partial\_{t}+\mathcal{L})h(t,x-)-vh(t,x)\right)\mathbb{E}\_{t,x}[\tau^{\varepsilon}-t]\\ &+\frac{1}{2}\mathbb{E}\_{t,x}[\int\_{t}^{\tau^{\varepsilon}}(\partial\_{x}h(s,X\_{s}+)-\partial\_{x}h(s,X\_{s}-))e^{-v(s-t)}1\_{\{X\_{s}=c(s)\}}dl^{c}\_{s}].\end{split} |  |

Since V=h+Î³2â€‹g2V=h+\frac{\gamma}{2}g^{2} is C1C^{1} across cc, it holds that

|  |  |  |
| --- | --- | --- |
|  | ð”¼t,x[âˆ«tÏ„Îµ(âˆ‚xh(s,Xs+)âˆ’âˆ‚xh(s,Xsâˆ’))eâˆ’vâ€‹(sâˆ’t)1{Xs=câ€‹(s)}dlsc+Î³â€‹gâ€‹(t,x)â€‹ð”¼t,xâ€‹[âˆ«tÏ„Îµ(âˆ‚xgâ€‹(s,Xs+)âˆ’âˆ‚xgâ€‹(s,Xsâˆ’))â€‹eâˆ’vâ€‹(sâˆ’t)â€‹1{Xs=câ€‹(s)}â€‹ð‘‘lsc]=oâ€‹(ð”¼t,xâ€‹[Ï„Îµâˆ’t]).\begin{split}&\mathbb{E}\_{t,x}[\int\_{t}^{\tau^{\varepsilon}}(\partial\_{x}h(s,X\_{s}+)-\partial\_{x}h(s,X\_{s}-))e^{-v(s-t)}1\_{\{X\_{s}=c(s)\}}dl^{c}\_{s}\\ &+\gamma g(t,x)\mathbb{E}\_{t,x}[\int\_{t}^{\tau^{\varepsilon}}(\partial\_{x}g(s,X\_{s}+)-\partial\_{x}g(s,X\_{s}-))e^{-v(s-t)}1\_{\{X\_{s}=c(s)\}}dl^{c}\_{s}]=o(\mathbb{E}\_{t,x}[\tau^{\varepsilon}-t]).\end{split} |  |

Then,

|  |  |  |
| --- | --- | --- |
|  | Jâ€‹(t,x;Ï„ð’žÎµ,v)âˆ’Jâ€‹(t,x;Ï„ð’ž)ð”¼t,xâ€‹[Ï„Îµâˆ’t]=12â€‹((âˆ‚t+â„’)â€‹(Vâˆ’Î³2â€‹g2)â€‹(t,x+)+Î³â€‹gâ€‹(âˆ‚t+â„’)â€‹gâ€‹(t,x+))+12â€‹((âˆ‚t+â„’)â€‹(Vâˆ’Î³2â€‹g2)â€‹(t,xâˆ’)+Î³â€‹gâ€‹(âˆ‚t+â„’)â€‹gâ€‹(t,xâˆ’))+Î³2â€‹(âˆ‚xgâ€‹(t,x+)âˆ’âˆ‚xgâ€‹(t,xâˆ’)2)2â€‹Ïƒ2â€‹(t,x)+oâ€‹(1)=12((âˆ‚t+â„’)V(t,x+)âˆ’Î³|Ïƒâˆ‚xg|2(t,x+)+(âˆ‚t+â„’)V(t,xâˆ’)âˆ’Î³|Ïƒâˆ‚xg|2(t,xâˆ’))+Î³2â€‹(âˆ‚xgâ€‹(t,x+)âˆ’âˆ‚xgâ€‹(t,xâˆ’)2)2â€‹Ïƒ2â€‹(t,x)+oâ€‹(1),\begin{split}&\frac{J(t,x;\tau\_{\mathcal{C}}^{\varepsilon,v})-J(t,x;\tau\_{\mathcal{C}})}{\mathbb{E}\_{t,x}[\tau^{\varepsilon}-t]}\\ =&\frac{1}{2}\left((\partial\_{t}+\mathcal{L})(V-\frac{\gamma}{2}g^{2})(t,x+)+\gamma g(\partial\_{t}+\mathcal{L})g(t,x+)\right)\\ &+\frac{1}{2}\left((\partial\_{t}+\mathcal{L})(V-\frac{\gamma}{2}g^{2})(t,x-)+\gamma g(\partial\_{t}+\mathcal{L})g(t,x-)\right)\\ &+\frac{\gamma}{2}\left(\frac{\partial\_{x}g(t,x+)-\partial\_{x}g(t,x-)}{2}\right)^{2}\sigma^{2}(t,x)+o(1)\\ =&\frac{1}{2}\bigg((\partial\_{t}+\mathcal{L})V(t,x+)-\gamma|\sigma\partial\_{x}g|^{2}(t,x+)\\ &+(\partial\_{t}+\mathcal{L})V(t,x-)-\gamma|\sigma\partial\_{x}g|^{2}(t,x-)\bigg)\\ &+\frac{\gamma}{2}\left(\frac{\partial\_{x}g(t,x+)-\partial\_{x}g(t,x-)}{2}\right)^{2}\sigma^{2}(t,x)+o(1),\\ \end{split} |  |

which gives the desired result according the assumption of the theorem.
âˆŽ

###### Remark 4.1.

It is a well-known result in the theory of optimal stopping that the value function is C1C^{1} across the free boundary. However, one can not expect that gg is also C1C^{1}, see the example given in Subsection [5.1](https://arxiv.org/html/2510.24128v1#S5.SS1 "5.1 Infinite Horizon Case â€£ 5 Further Discussions â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method"). Note that a similar condition is also given in [[7](https://arxiv.org/html/2510.24128v1#bib.bib7)].

## 5 Further Discussions

### 5.1 Infinite Horizon Case

In this subsection, we consider an infinite horizon example that one can give an explicit solution. Let XX be a geometric Brownian motion

|  |  |  |
| --- | --- | --- |
|  | dâ€‹Xt=Î¼â€‹Xtâ€‹dâ€‹t+Ïƒâ€‹Xtâ€‹dâ€‹Wt.dX\_{t}=\mu X\_{t}dt+\sigma X\_{t}dW\_{t}. |  |

Consider the MV stopping problem

|  |  |  |
| --- | --- | --- |
|  | Jâ€‹(x;Ï„)=ð”¼xâ€‹[XÏ„]âˆ’Î³2â€‹Varxâ€‹[XÏ„].J(x;\tau)=\mathbb{E}\_{x}\left[X\_{\tau}\right]-\frac{\gamma}{2}\text{Var}\_{x}\left[X\_{\tau}\right]. |  |

For this infinite horizon case, one can have an elliptic system similar to ([7](https://arxiv.org/html/2510.24128v1#S4.E7 "In 4 Extended HJB Equation for Original Problem â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method"))

|  |  |  |  |
| --- | --- | --- | --- |
|  | {minâ¡{âˆ’(â„’â€‹vâˆ’Î³2â€‹|Ïƒâ€‹xâ€‹âˆ‚xg|2),v+Î³2â€‹(fâˆ’g)2âˆ’f}=0,â„’â€‹g=0,Â onÂ {v+Î³2â€‹(fâˆ’g)2>f},g=fâ€‹Â onÂ {v+Î³2â€‹(fâˆ’g)2=f},\left\{\begin{split}&\min\left\{-\left(\mathcal{L}v-\frac{\gamma}{2}\left|\sigma x\partial\_{x}g\right|^{2}\right),v+\frac{\gamma}{2}(f-g)^{2}-f\right\}=0,\\ &\mathcal{L}g=0,\text{ on $\{v+\frac{\gamma}{2}(f-g)^{2}>f\}$},\\ &g=f\text{ on $\{v+\frac{\gamma}{2}(f-g)^{2}=f\}$},\end{split}\right. |  | (11) |

with â„’=12â€‹Ïƒ2â€‹x2â€‹âˆ‚2âˆ‚x2+Î¼â€‹xâ€‹âˆ‚âˆ‚x\mathcal{L}=\frac{1}{2}\sigma^{2}x^{2}\frac{\partial^{2}}{\partial x^{2}}+\mu x\frac{\partial}{\partial x}. Denote by Ï=2â€‹Î¼Ïƒ2\rho=\frac{2\mu}{\sigma^{2}} and assume that Ïâˆˆ(0,1/2)\rho\in(0,1/2). Let b=2â€‹ÏÎ³â€‹(1âˆ’Ï)b=\frac{2\rho}{\gamma(1-\rho)}. Let us check that

|  |  |  |
| --- | --- | --- |
|  | V(x)={(1âˆ’Î³2â€‹b)â€‹bÏâ€‹x1âˆ’Ï+Î³2â€‹b2â€‹Ïâ€‹x2âˆ’2â€‹Ï,Â forÂ x<b,x,Â forÂ xâ‰¥b,V(x)=\left\{\begin{split}&(1-\frac{\gamma}{2}b)b^{\rho}x^{1-\rho}+\frac{\gamma}{2}b^{2\rho}x^{2-2\rho},\text{ for $x<b$},\\ &x,\text{ for $x\geq b$},\end{split}\right. |  |

and

|  |  |  |
| --- | --- | --- |
|  | g(x)={bÏâ€‹x1âˆ’Ï,Â forÂ x<b,x,Â forÂ xâ‰¥b,g(x)=\left\{\begin{split}&b^{\rho}x^{1-\rho},\text{ for $x<b$},\\ &x,\text{ for $x\geq b$},\end{split}\right. |  |

satisfy ([11](https://arxiv.org/html/2510.24128v1#S5.E11 "In 5.1 Infinite Horizon Case â€£ 5 Further Discussions â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")). For x<bx<b, it holds that

|  |  |  |
| --- | --- | --- |
|  | âˆ‚xV=(1âˆ’Ï)â€‹(1âˆ’Î³2â€‹b)â€‹bÏâ€‹xâˆ’Ï+Î³â€‹(1âˆ’Ï)â€‹b2â€‹Ïâ€‹x1âˆ’2â€‹Ï,\partial\_{x}V=(1-\rho)(1-\frac{\gamma}{2}b)b^{\rho}x^{-\rho}+\gamma(1-\rho)b^{2\rho}x^{1-2\rho}, |  |

|  |  |  |
| --- | --- | --- |
|  | âˆ‚xâ€‹xV=âˆ’Ïâ€‹(1âˆ’Ï)â€‹(1âˆ’Î³2â€‹b)â€‹bÏâ€‹xâˆ’Ïâˆ’1+Î³â€‹(1âˆ’Ï)â€‹(1âˆ’2â€‹Ï)â€‹b2â€‹Ïâ€‹xâˆ’2â€‹Ï,\partial\_{xx}V=-\rho(1-\rho)(1-\frac{\gamma}{2}b)b^{\rho}x^{-\rho-1}+\gamma(1-\rho)(1-2\rho)b^{2\rho}x^{-2\rho}, |  |

and âˆ‚xg=(1âˆ’Ïâ€‹bÏâ€‹xâˆ’Ï)\partial\_{x}g=(1-\rho b^{\rho}x^{-\rho}). Thus, it is easy to check that, for x<bx<b, â„’â€‹Vâˆ’Î³2â€‹|Ïƒâ€‹xâ€‹âˆ‚xg|2=0\mathcal{L}V-\frac{\gamma}{2}\left|\sigma x\partial\_{x}g\right|^{2}=0 and â„’â€‹g=0\mathcal{L}g=0. When xâ‰¥bx\geq b, it is easy to see that âˆ‚xV=âˆ‚xg=1\partial\_{x}V=\partial\_{x}g=1 and âˆ‚xâ€‹xV=0\partial\_{xx}V=0. Hence,

|  |  |  |
| --- | --- | --- |
|  | â„’â€‹Vâˆ’Î³2â€‹|Ïƒâ€‹xâ€‹âˆ‚xg|2=Î¼â€‹xâˆ’Î³2â€‹Ïƒ2â€‹x2=Î³2â€‹Ïƒ2â€‹xâ€‹(ÏÎ³âˆ’x)â‰¤0.\mathcal{L}V-\frac{\gamma}{2}\left|\sigma x\partial\_{x}g\right|^{2}=\mu x-\frac{\gamma}{2}\sigma^{2}x^{2}=\frac{\gamma}{2}\sigma^{2}x(\frac{\rho}{\gamma}-x)\leq 0. |  |

Now let us verify that V+Î³2â€‹(xâˆ’g)2>xV+\frac{\gamma}{2}(x-g)^{2}>x for x<bx<b. Direct computation yields that

|  |  |  |
| --- | --- | --- |
|  | V+Î³2â€‹(xâˆ’g)2=(1âˆ’Î³2â€‹b)â€‹bÏâ€‹x1âˆ’Ï+Î³2â€‹b2â€‹Ïâ€‹x2âˆ’2â€‹Ï+Î³2â€‹(xâˆ’bÏâ€‹x1âˆ’Ï)2=Î³2â€‹x2+Î³â€‹b2â€‹Ïâ€‹x2âˆ’2â€‹Ïâˆ’Î³â€‹bÏâ€‹x2âˆ’Ï+(1âˆ’Î³2â€‹b)â€‹bÏâ€‹x1âˆ’Ï=xâ€‹(Î³2â€‹x+Î³â€‹b2â€‹Ïâ€‹x1âˆ’2â€‹Ïâˆ’Î³â€‹bÏâ€‹x1âˆ’Ï+(1âˆ’Î³2â€‹b)â€‹bÏâ€‹xâˆ’Ï).\begin{split}&V+\frac{\gamma}{2}(x-g)^{2}\\ =&(1-\frac{\gamma}{2}b)b^{\rho}x^{1-\rho}+\frac{\gamma}{2}b^{2\rho}x^{2-2\rho}+\frac{\gamma}{2}(x-b^{\rho}x^{1-\rho})^{2}\\ =&\frac{\gamma}{2}x^{2}+\gamma b^{2\rho}x^{2-2\rho}-\gamma b^{\rho}x^{2-\rho}+(1-\frac{\gamma}{2}b)b^{\rho}x^{1-\rho}\\ =&x\left(\frac{\gamma}{2}x+\gamma b^{2\rho}x^{1-2\rho}-\gamma b^{\rho}x^{1-\rho}+(1-\frac{\gamma}{2}b)b^{\rho}x^{-\rho}\right).\end{split} |  |

Thus, we have to show that Î³2â€‹x+Î³â€‹b2â€‹Ïâ€‹x1âˆ’2â€‹Ïâˆ’Î³â€‹bÏâ€‹x1âˆ’Ï+(1âˆ’Î³2â€‹b)â€‹bÏâ€‹xâˆ’Ï>1\frac{\gamma}{2}x+\gamma b^{2\rho}x^{1-2\rho}-\gamma b^{\rho}x^{1-\rho}+(1-\frac{\gamma}{2}b)b^{\rho}x^{-\rho}>1 for x<bx<b. For that purpose, define a function Îºâ€‹(z)=Î³2â€‹bâ€‹z2â€‹Ïâˆ’1+(1âˆ’Î³2â€‹b)â€‹zÏ\kappa(z)=\frac{\gamma}{2}bz^{2\rho-1}+(1-\frac{\gamma}{2}b)z^{\rho}. Let us find its infimum on [1,âˆž)[1,\infty). Taking derivative with respect to zz, we see that Îºâ€²â€‹(z)=zÏâˆ’1â€‹((2â€‹Ïâˆ’1)â€‹Î³2â€‹bâ€‹zÏâˆ’1+(1âˆ’Î³2â€‹b)â€‹Ï)\kappa^{\prime}(z)=z^{\rho-1}((2\rho-1)\frac{\gamma}{2}bz^{\rho-1}+(1-\frac{\gamma}{2}b)\rho). Since Ïâ‰¤12\rho\leq\frac{1}{2} and zâ‰¥1z\geq 1, it holds that

|  |  |  |
| --- | --- | --- |
|  | (2â€‹Ïâˆ’1)â€‹Î³2â€‹bâ€‹zÏâˆ’1+(1âˆ’Î³2â€‹b)â€‹Ïâ‰¥(2â€‹Ïâˆ’1)â€‹Î³2â€‹b+(1âˆ’Î³2â€‹b)â€‹Ï=0,(2\rho-1)\frac{\gamma}{2}bz^{\rho-1}+(1-\frac{\gamma}{2}b)\rho\geq(2\rho-1)\frac{\gamma}{2}b+(1-\frac{\gamma}{2}b)\rho=0, |  |

where we use the fact that b=2â€‹ÏÎ³â€‹(1âˆ’Ï)b=\frac{2\rho}{\gamma(1-\rho)}.
This implies that Îº\kappa is strictly increasing on [1,âˆž)[1,\infty) and taking minimum at z=1z=1, which equals to 11. Then, for x<bx<b,

|  |  |  |
| --- | --- | --- |
|  | Î³2â€‹x+Î³â€‹b2â€‹Ïâ€‹x1âˆ’2â€‹Ïâˆ’Î³â€‹bÏâ€‹x1âˆ’Ï+(1âˆ’Î³2â€‹b)â€‹bÏâ€‹xâˆ’Ï=Î³2â€‹xâ€‹(1âˆ’bÏâ€‹xâˆ’Ï)2+Î³2â€‹b2â€‹Ïâ€‹x1âˆ’2â€‹Ï+(1âˆ’Î³2â€‹b)â€‹bÏâ€‹xâˆ’Ïâ‰¥Îºâ€‹(bâ€‹xâˆ’1)>1.\begin{split}&\frac{\gamma}{2}x+\gamma b^{2\rho}x^{1-2\rho}-\gamma b^{\rho}x^{1-\rho}+(1-\frac{\gamma}{2}b)b^{\rho}x^{-\rho}\\ =&\frac{\gamma}{2}x(1-b^{\rho}x^{-\rho})^{2}+\frac{\gamma}{2}b^{2\rho}x^{1-2\rho}+(1-\frac{\gamma}{2}b)b^{\rho}x^{-\rho}\\ \geq&\kappa(bx^{-1})>1.\end{split} |  |

Thus, (V,g)(V,g) is a solution of ([11](https://arxiv.org/html/2510.24128v1#S5.E11 "In 5.1 Infinite Horizon Case â€£ 5 Further Discussions â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")). On x=bx=b, we can check that

|  |  |  |
| --- | --- | --- |
|  | â„’â€‹Vâ€‹(bâˆ’)=Î³2â€‹Ïƒ2â€‹b2â€‹(1âˆ’Ï)2,â„’â€‹Vâ€‹(b+)=Î¼â€‹b,âˆ‚xgâ€‹(bâˆ’)=1âˆ’Ï,Â andÂ â€‹âˆ‚xgâ€‹(b+)=1.\mathcal{L}V(b-)=\frac{\gamma}{2}\sigma^{2}b^{2}(1-\rho)^{2},\mathcal{L}V(b+)=\mu b,\partial\_{x}g(b-)=1-\rho,\text{ and }\partial\_{x}g(b+)=1. |  |

Then,

|  |  |  |
| --- | --- | --- |
|  | â„’â€‹Vâ€‹(bâˆ’)+â„’â€‹Vâ€‹(b+)=Î³2â€‹Ïƒ2â€‹b2â€‹(1âˆ’Ï)2+Î¼â€‹b=Ïƒ2â€‹bâ€‹(Î³2â€‹bâ€‹(1âˆ’Ï)2+Ï)=Ïƒ2â€‹bâ€‹Ïâ€‹(2âˆ’Ï),\mathcal{L}V(b-)+\mathcal{L}V(b+)=\frac{\gamma}{2}\sigma^{2}b^{2}(1-\rho)^{2}+\mu b=\sigma^{2}b(\frac{\gamma}{2}b(1-\rho)^{2}+\rho)=\sigma^{2}b\rho(2-\rho), |  |

and

|  |  |  |
| --- | --- | --- |
|  | Î³â€‹Ïƒ2â€‹b2â€‹(1+1âˆ’Ï2)2=Ïƒ2â€‹bâ€‹(2âˆ’Ï)â€‹(2âˆ’Ï)â€‹Î³â€‹b4=Ïƒ2â€‹bâ€‹(2âˆ’Ï)â€‹Ïâ€‹2âˆ’Ï2âˆ’2â€‹Ï>Ïƒ2â€‹bâ€‹(2âˆ’Ï)â€‹Ï.\gamma\sigma^{2}b^{2}\left(\frac{1+1-\rho}{2}\right)^{2}=\sigma^{2}b(2-\rho)\frac{(2-\rho)\gamma b}{4}=\sigma^{2}b(2-\rho)\rho\frac{2-\rho}{2-2\rho}>\sigma^{2}b(2-\rho)\rho. |  |

Thus, the stopping time is an equilibrium.

Moreover, one can also compute that Vâ€²â€‹(b+)=Vâ€²â€‹(bâˆ’)V^{\prime}(b+)=V^{\prime}(b-), g1â€²â€‹(bâˆ’)=1âˆ’Ïg^{\prime}\_{1}(b-)=1-\rho and g1â€²â€‹(b+)=1g^{\prime}\_{1}(b+)=1. This suggests that one can expect VV to be C1C^{1}, but gg not C1C^{1}, so one can not assume that gg is C1C^{1} across the free boundary.

### 5.2 Discrete Time Approximation

For many time-inconsistent problems, the equilibrium solution in continuous time is regarded as the limit of its counterpart in discrete-time models. This is the logical structure of the derivation for extended HJB equation, see [[4](https://arxiv.org/html/2510.24128v1#bib.bib4)]. For that reason, we would like to check that whether one can get the same equation by considering the discrete time MV stopping problem.

Let Î”â€‹t=TN\Delta t=\frac{T}{N} and tk=kâ€‹Î”â€‹tt\_{k}=k\Delta t for k=0,1,â€¦,Nk=0,1,...,N. We assume that one can only stop at these time points. Recursively define a sequence of stopping times {Ï„i}i=0,1,â€¦,N\{\tau\_{i}\}\_{i=0,1,...,N} as the follows. Set Ï„N=tN\tau\_{N}=t\_{N}, Uâ€‹(N,x)=fâ€‹(x)U(N,x)=f(x), and Vâ€‹(N,x)=fâ€‹(x)V(N,x)=f(x). For i=Nâˆ’1,Nâˆ’2,â€¦,0i=N-1,N-2,...,0, define

|  |  |  |
| --- | --- | --- |
|  | Uâ€‹(i,x):=ð”¼ti,xâ€‹[fâ€‹(XÏ„i+1)]âˆ’Î³2â€‹Varti,xâ€‹[fâ€‹(XÏ„i+1)],U(i,x):=\mathbb{E}\_{t\_{i},x}[f(X\_{\tau\_{i+1}})]-\frac{\gamma}{2}\text{Var}\_{t\_{i},x}[f(X\_{\tau\_{i+1}})], |  |

|  |  |  |
| --- | --- | --- |
|  | Ï„i:={ti,Â ifÂ fâ€‹(Xti)â‰¥Uiâ€‹(Xti),Ï„i+1,ifÂ fâ€‹(Xti)<Uiâ€‹(Xti),\tau\_{i}:=\left\{\begin{matrix}t\_{i},\text{ if $f(X\_{t\_{i}})\geq U\_{i}(X\_{t\_{i}})$,}\\ \tau\_{i+1},\text{if $f(X\_{t\_{i}})<U\_{i}(X\_{t\_{i}})$,}\end{matrix}\right. |  |

and

|  |  |  |
| --- | --- | --- |
|  | Vâ€‹(i,x):=ð”¼ti,xâ€‹[fâ€‹(XÏ„i)]âˆ’Î³2â€‹Varti,xâ€‹[fâ€‹(XÏ„i)].V(i,x):=\mathbb{E}\_{t\_{i},x}[f(X\_{\tau\_{i}})]-\frac{\gamma}{2}\text{Var}\_{t\_{i},x}[f(X\_{\tau\_{i}})]. |  |

Moreover, we also define gâ€‹(i,x):=ð”¼ti,xâ€‹[fâ€‹(XÏ„i)]g(i,x):=\mathbb{E}\_{t\_{i},x}[f(X\_{\tau\_{i}})]. The motivation of these definitions are the following. We view the MV stopping problem from a game-theoretic perspective as a non-cooperative game. We have one player at each time point tnt\_{n}, who can only choose the stopping decision at tnt\_{n}. The stopping time Ï„i\tau\_{i} represent the time the process being stopped conditioned on its has not been stopped before tit\_{i}. Player nn has two options: to stop immediately or to continue. The reward of the first option is fâ€‹(Xti)f(X\_{t\_{i}}). Choosing to continue, the process is stopped at time Ï„i+1\tau\_{i+1} and the expected reward is given by Uâ€‹(i,Xti)U(i,X\_{t\_{i}}). Thus, the strategy of player nn is to decide to stop when fâ€‹(Xti)â‰¥Uâ€‹(i,Xti)f(X\_{t\_{i}})\geq U(i,X\_{t\_{i}}).

Now let us check the equation satisfied by ViV\_{i}. It is easy to see that Vâ€‹(i,x)â‰¥fâ€‹(x)V(i,x)\geq f(x). If Vâ€‹(i,x)>fâ€‹(x)V(i,x)>f(x), it implies that Ï„i=Ï„i+1\tau\_{i}=\tau\_{i+1}. In this case, it holds that

|  |  |  |
| --- | --- | --- |
|  | Vâ€‹(i,x)=Uâ€‹(i,x)=ð”¼ti,xâ€‹[Vâ€‹(i+1,Xti+1)]âˆ’Î³2â€‹Varti,xâ€‹[gâ€‹(i+1,Xti+1)].V(i,x)=U(i,x)=\mathbb{E}\_{t\_{i},x}[V(i+1,X\_{t\_{i+1}})]-\frac{\gamma}{2}\text{Var}\_{t\_{i},x}[g(i+1,X\_{t\_{i+1}})]. |  |

Combining two situations, we have

|  |  |  |
| --- | --- | --- |
|  | minâ¡{Vâ€‹(i,x)âˆ’(ð”¼ti,xâ€‹[Vâ€‹(i+1,Xti+1)]âˆ’Î³2â€‹Varti,xâ€‹[gâ€‹(i+1,Xti+1)]),Vâ€‹(i,x)âˆ’fâ€‹(x)}=0.\min\left\{V(i,x)-\left(\mathbb{E}\_{t\_{i},x}[V(i+1,X\_{t\_{i+1}})]-\frac{\gamma}{2}\text{Var}\_{t\_{i},x}[g(i+1,X\_{t\_{i+1}})]\right),V(i,x)-f(x)\right\}=0. |  |

Now we let Î”â€‹t\Delta t go to zero. Formally, one see that

|  |  |  |
| --- | --- | --- |
|  | ð”¼ti,xâ€‹[Vâ€‹(i+1,Xti+1)]âˆ’Vâ€‹(i,x)=(âˆ‚tV+â„’â€‹V)â€‹(ti,x)â€‹Î”â€‹t+oâ€‹(Î”â€‹t),\mathbb{E}\_{t\_{i},x}[V(i+1,X\_{t\_{i+1}})]-V(i,x)=(\partial\_{t}V+\mathcal{L}V)(t\_{i},x)\Delta t+o(\Delta t), |  |

and

|  |  |  |
| --- | --- | --- |
|  | Varti,xâ€‹[gâ€‹(i+1,Xti+1)]=ð”¼ti,xâ€‹[g2â€‹(i+1,Xti+1)]âˆ’(ð”¼ti,xâ€‹[gâ€‹(i+1,Xti+1)])2=g2â€‹(i,x)+(âˆ‚t+â„’)â€‹g2â€‹Î”â€‹tâˆ’(gâ€‹(i,x)+(âˆ‚t+â„’)â€‹gâ€‹Î”â€‹t)2â€‹oâ€‹(Î”â€‹t)=(âˆ‚t+â„’)â€‹g2â€‹Î”â€‹tâˆ’gâ€‹(âˆ‚t+â„’)â€‹gâ€‹Î”â€‹t+oâ€‹(Î”â€‹t)=|Ïƒâ€‹âˆ‚xg|2â€‹Î”â€‹t+oâ€‹(Î”â€‹t).\begin{split}&\text{Var}\_{t\_{i},x}[g(i+1,X\_{t\_{i+1}})]=\mathbb{E}\_{t\_{i},x}[g^{2}(i+1,X\_{t\_{i+1}})]-\left(\mathbb{E}\_{t\_{i},x}[g(i+1,X\_{t\_{i+1}})]\right)^{2}\\ =&g^{2}(i,x)+(\partial\_{t}+\mathcal{L})g^{2}\Delta t-\left(g(i,x)+(\partial\_{t}+\mathcal{L})g\Delta t\right)^{2}o(\Delta t)\\ =&(\partial\_{t}+\mathcal{L})g^{2}\Delta t-g(\partial\_{t}+\mathcal{L})g\Delta t+o(\Delta t)\\ =&|\sigma\partial\_{x}g|^{2}\Delta t+o(\Delta t).\end{split} |  |

Hence, we get the following system

|  |  |  |  |
| --- | --- | --- | --- |
|  | {minâ¡{âˆ’(âˆ‚tV+â„’â€‹Vâˆ’Î³â€‹|Ïƒâ€‹âˆ‚xg|2),Vâˆ’f}=0,Vâ€‹(T,x)=f,âˆ‚tg+â„’â€‹g=0,Â onÂ {V>f},g=fâ€‹Â onÂ {V=f},gâ€‹(T,x)=fâ€‹(x).\left\{\begin{split}&\min\left\{-\left(\partial\_{t}V+\mathcal{L}V-\gamma\left|\sigma\partial\_{x}g\right|^{2}\right),V-f\right\}=0,V(T,x)=f,\\ &\partial\_{t}g+\mathcal{L}g=0,\text{ on $\{V>f\}$},\\ &g=f\text{ on $\{V=f\}$},g(T,x)=f(x).\end{split}\right. |  | (12) |

Note that this equation is different from ([7](https://arxiv.org/html/2510.24128v1#S4.E7 "In 4 Extended HJB Equation for Original Problem â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")) with the condition V+Î³2â€‹(fâˆ’g)2â‰¥fV+\frac{\gamma}{2}(f-g)^{2}\geq f replaced by Vâ‰¥fV\geq f. Clearly Vâ‰¥fV\geq f implies V+Î³2â€‹(fâˆ’g)2â‰¥fV+\frac{\gamma}{2}(f-g)^{2}\geq f. Moreover, on {V=f}\{V=f\}, we also have g=fg=f and hence, V+Î³2â€‹(fâˆ’g)2=fV+\frac{\gamma}{2}(f-g)^{2}=f. Thus, the solution of the above system also satisfies
([7](https://arxiv.org/html/2510.24128v1#S4.E7 "In 4 Extended HJB Equation for Original Problem â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")). In this sense, it seems that ([7](https://arxiv.org/html/2510.24128v1#S4.E7 "In 4 Extended HJB Equation for Original Problem â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")) is a more general equation. However, it remains unclear whether there exists a solution that satisfies ([7](https://arxiv.org/html/2510.24128v1#S4.E7 "In 4 Extended HJB Equation for Original Problem â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")), but do not satisfy ([12](https://arxiv.org/html/2510.24128v1#S5.E12 "In 5.2 Discrete Time Approximation â€£ 5 Further Discussions â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")).

### 5.3 General Time-inconsistent Problems

To explain the additional quadratic term in the condition V+Î³2â€‹(fâˆ’g)2â‰¥fV+\frac{\gamma}{2}(f-g)^{2}\geq f of ([7](https://arxiv.org/html/2510.24128v1#S4.E7 "In 4 Extended HJB Equation for Original Problem â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")), let us consider a more general time-inconsistent problem in which the player choose stopping time to maximize the following functional

|  |  |  |
| --- | --- | --- |
|  | ð”¼t,xâ€‹[fâ€‹(XÏ„)]+Gâ€‹(ð”¼t,xâ€‹[kâ€‹(XÏ„)]).\mathbb{E}\_{t,x}[f(X\_{\tau})]+G(\mathbb{E}\_{t,x}[k(X\_{\tau})]). |  |

We just give some formal arguments for illustration. First, we consider the regularized problem. For an equilibrium Ï€âˆ—\pi^{\*}, define gâ€‹(t,x)=ð”¼t,xâ€‹[kâ€‹(XÏ„)]g(t,x)=\mathbb{E}\_{t,x}[k(X\_{\tau})] and hâ€‹(t,x)=ð”¼t,xâ€‹[fâ€‹(XÏ„)+âˆ«0Ï„Î»â€‹Hâ€‹(Ï€sâˆ—)â€‹ð‘‘s]h(t,x)=\mathbb{E}\_{t,x}[f(X\_{\tau})+\int\_{0}^{\tau}\lambda H(\pi^{\*}\_{s})ds]. Then, it holds that (g,h)(g,h) solves

|  |  |  |
| --- | --- | --- |
|  | {(âˆ‚t+â„’)â€‹h+Î»â€‹Hâ€‹(Ï€âˆ—)+Ï€âˆ—â€‹(fâˆ’h)=0,g2â€‹(T)=f,(âˆ‚t+â„’)â€‹g+Ï€âˆ—â€‹(kâˆ’g)=0,gâ€‹(T)=k.\left\{\begin{split}&(\partial\_{t}+\mathcal{L})h+\lambda H(\pi^{\*})+\pi^{\*}(f-h)=0,g\_{2}(T)=f,\\ &(\partial\_{t}+\mathcal{L})g+\pi^{\*}(k-g)=0,g(T)=k.\end{split}\right. |  |

For a purterbed strategy Ï€Îµ,v\pi^{\varepsilon,v}, one can get that

|  |  |  |
| --- | --- | --- |
|  | ð”¼t,xâ€‹[kâ€‹(XÏ€Îµ,v)]=gâ€‹(t,x)+((âˆ‚t+â„’)â€‹g+vâ€‹(kâˆ’g))â€‹Îµ+oâ€‹(Îµ),\mathbb{E}\_{t,x}[k(X\_{\pi^{\varepsilon,v}})]=g(t,x)+\left((\partial\_{t}+\mathcal{L})g+v(k-g)\right)\varepsilon+o(\varepsilon), |  |

and

|  |  |  |
| --- | --- | --- |
|  | ð”¼t,xâ€‹[fâ€‹(XÏ€Îµ,v)+âˆ«0Ï„Îµ,vÎ»â€‹Hâ€‹(Ï€sÎµ,v)â€‹ð‘‘s]=hâ€‹(t,x)+((âˆ‚t+â„’)â€‹h+vâ€‹(fâˆ’h)+Î»â€‹Hâ€‹(v))â€‹Îµ+oâ€‹(Îµ).\begin{split}&\mathbb{E}\_{t,x}[f(X\_{\pi^{\varepsilon,v}})+\int\_{0}^{\tau^{\varepsilon,v}}\lambda H(\pi^{\varepsilon,v}\_{s})ds]\\ =&h(t,x)+\left((\partial\_{t}+\mathcal{L})h+v(f-h)+\lambda H(v)\right)\varepsilon+o(\varepsilon).\end{split} |  |

Moreover, it holds that

|  |  |  |
| --- | --- | --- |
|  | Gâ€‹(ð”¼t,xâ€‹[kâ€‹(XÏ€Îµ,v)])=Gâ€‹(g)+Gâ€²â€‹(g)â€‹((âˆ‚t+â„’)â€‹g+vâ€‹(kâˆ’g))â€‹Îµ+oâ€‹(Îµ).G(\mathbb{E}\_{t,x}[k(X\_{\pi^{\varepsilon,v}})])=G(g)+G^{\prime}(g)\left((\partial\_{t}+\mathcal{L})g+v(k-g)\right)\varepsilon+o(\varepsilon). |  |

Then, one can show that Ï€âˆ—\pi^{\*} is an equilibrium if and only if

|  |  |  |
| --- | --- | --- |
|  | Ï€âˆ—=expâ¡(âˆ’h+Gâ€²â€‹(g)â€‹gâˆ’fâˆ’Gâ€²â€‹(g)â€‹kÎ»).\pi^{\*}=\exp(-\frac{h+G^{\prime}(g)g-f-G^{\prime}(g)k}{\lambda}). |  |

Note that the value function V=h+Gâ€‹(g)V=h+G(g) and satisfies

|  |  |  |
| --- | --- | --- |
|  | (âˆ‚t+â„’)â€‹V+Î»â€‹expâ¡(âˆ’h+Gâ€²â€‹(g)â€‹gâˆ’fâˆ’Gâ€²â€‹(g)â€‹kÎ»)+â„‹Gâ€‹(g)=0,(\partial\_{t}+\mathcal{L})V+\lambda\exp(-\frac{h+G^{\prime}(g)g-f-G^{\prime}(g)k}{\lambda})+\mathcal{H}\_{G}(g)=0, |  |

where the operator â„‹G\mathcal{H}\_{G} is defined as

|  |  |  |
| --- | --- | --- |
|  | â„‹Gâ€‹(Ï†)=Gâ€²â€‹(Ï†)â€‹â„’â€‹Ï†âˆ’â„’â€‹Gâ€‹(Ï†).\mathcal{H}\_{G}(\varphi)=G^{\prime}(\varphi)\mathcal{L}\varphi-\mathcal{L}G(\varphi). |  |

We also see that

|  |  |  |
| --- | --- | --- |
|  | h+Gâ€²â€‹(g)â€‹gâˆ’fâˆ’Gâ€²â€‹(g)â€‹k=Vâˆ’(f+Gâ€‹(k))+Gâ€‹(k)âˆ’Gâ€‹(g)âˆ’Gâ€²â€‹(g)â€‹(kâˆ’g).h+G^{\prime}(g)g-f-G^{\prime}(g)k=V-(f+G(k))+G(k)-G(g)-G^{\prime}(g)(k-g). |  |

Denote by Î”Gâ€‹(k,g)=Gâ€‹(k)âˆ’Gâ€‹(g)âˆ’Gâ€²â€‹(g)â€‹(kâˆ’g)\Delta\_{G}(k,g)=G(k)-G(g)-G^{\prime}(g)(k-g). Then, when letting Î»\lambda go to zero, VV should converge to the following variational inequality

|  |  |  |
| --- | --- | --- |
|  | minâ¡{âˆ’((âˆ‚t+â„’)â€‹V+HGâ€‹(g)),Vâˆ’(f+Gâ€‹(k))+Î”Gâ€‹(k,g)}=0.\min\left\{-\left((\partial\_{t}+\mathcal{L})V+H\_{G}(g)\right),V-(f+G(k))+\Delta\_{G}(k,g)\right\}=0. |  |

Note that the condition is written as V+Î”Gâ€‹(k,g)â‰¥(f+Gâ€‹(g))V+\Delta\_{G}(k,g)\geq(f+G(g)). For MV case, i.e., Gâ€‹(k)=Î³2â€‹k2G(k)=\frac{\gamma}{2}k^{2}, Î”Gâ€‹(k,g)=Î³2â€‹(kâˆ’g)2\Delta\_{G}(k,g)=\frac{\gamma}{2}(k-g)^{2}, which is the same condition in ([7](https://arxiv.org/html/2510.24128v1#S4.E7 "In 4 Extended HJB Equation for Original Problem â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")). Moreover, define the set

|  |  |  |
| --- | --- | --- |
|  | ð’ž={(t,x)|V+Î”Gâ€‹(k,g)>f+Gâ€‹(k)},\mathcal{C}=\{(t,x)|V+\Delta\_{G}(k,g)>f+G(k)\}, |  |

and the stopping time Ï„ð’ž\tau\_{\mathcal{C}} as

|  |  |  |
| --- | --- | --- |
|  | Ï„ð’ž=inf{s>t|(s,Xs)âˆ‰ð’ž}.\tau\_{\mathcal{C}}=\inf\{s>t|(s,X\_{s})\notin\mathcal{C}\}. |  |

Then, one can formally verify that Ï„ð’ž\tau\_{\mathcal{C}} in the same sense as that in the statement of Theorem [4.1](https://arxiv.org/html/2510.24128v1#S4.Thmtheorem1 "Theorem 4.1. â€£ 4 Extended HJB Equation for Original Problem â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method").

From the previous discussion, we see that the appearance of the additional term Î”Gâ€‹(k,g)\Delta\_{G}(k,g) is due to two factors. One is that GG is a non-linear function, which makes the problem time-inconsistent. The other is that, when equilibrium is under consideration, the perturbation lies within the family of relaxed strategies rather than pure strategies. This coincides with the fact in the game theory that pure strategy equilibrium is different from mixed strategy equilibrium.

## 6 Conclusions

This paper systematically investigates the MV optimal stopping problemâ€“a time-inconsistent stochastic optimization problem-by developing a vanishing regularization method and deriving the corresponding extended HJB equation. More precisely, to tackle the mathematical intractability of direct equilibrium analysis, we introduce a regularized problem, which enables rigorous derivation of the equilibrium strategy and the associated extended HJB equation. Then, letting Î»â†’0\lambda\rightarrow 0, i.e. vanishing regularization, we formally recover a system of parabolic variational inequalities ([7](https://arxiv.org/html/2510.24128v1#S4.E7 "In 4 Extended HJB Equation for Original Problem â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")) for the original MV problem. This system characterizes equilibrium stopping times and includes a key quadratic term Î³2â€‹(fâˆ’g)2\frac{\gamma}{2}(f-g)^{2}â€“a distinction from classical optimal stopping, where stopping conditions depend only on comparing the value function to the instantaneous reward. By extending the analysis to general time-inconsistent problems, we demonstrate that the additional term in our stopping condition arises from the non-linearity of the objective (responsible for time inconsistency) and the use of mixed strategies.

This work provides a rigorous mathematical foundation for MV stopping problems, with potential applications in financial decision-making (e.g., asset sale timing, portfolio liquidation) and statistical inference (e.g., risk-aware hypothesis testing stopping rules). However, there are still some open problems left for future research, including
proving rigorous convergence of the regularized extended HJB system to the limiting variational inequality and developing numerical algorithms to compute equilibrium stopping rules for practical applications.

## References

* [1]

  C.Â Bayer, D.Â Belomestny, P.Â Hager, P.Â Pigato, and J.Â Schoenmakers, Randomized optimal stopping algorithms and their convergence analysis, SIAM
  Journal on Financial Mathematics, 12 (2021), pp.Â 1201â€“1225.
* [2]

  E.Â Bayraktar, Z.Â Wang, and Z.Â Zhou, Equilibria of time-inconsistent
  stopping for one-dimensional diffusion processes, Mathematical Finance, 33
  (2023), pp.Â 797â€“841.
* [3]

  T.Â BjÃ¶rk, M.Â Khapko, and A.Â Murgoci, On time-inconsistent
  stochastic control in continuous time, Finance and Stochastics, 21 (2017),
  pp.Â 331â€“360.
* [4]

  T.Â BjÃ¶rk, M.Â Khapko, A.Â Murgoci, etÂ al., Time-inconsistent
  control theory with finance applications, vol.Â 732, Springer, 2021.
* [5]

  T.Â BjÃ¶rk, A.Â Murgoci, and X.Â Y. Zhou, Meanâ€“variance portfolio
  optimization with state-dependent risk aversion, Mathematical Finance: An
  International Journal of Mathematics, Statistics and Financial Economics, 24
  (2014), pp.Â 1â€“24.
* [6]

  S.Â Christensen and K.Â LindensjÃ¶, On finding equilibrium stopping
  times for time-inconsistent markovian problems, SIAM Journal on Control and
  Optimization, 56 (2018), pp.Â 4228â€“4255.
* [7]

  S.Â Christensen and K.Â LindensjÃ¶, On time-inconsistent stopping
  problems and mixed strategy stopping times, Stochastic Processes and their
  Applications, 130 (2020), pp.Â 2886â€“2917.
* [8]

  M.Â Dai, Y.Â Dong, and Y.Â Jia, Learning equilibrium mean-variance
  strategy, Mathematical Finance, 33 (2023), pp.Â 1166â€“1212.
* [9]

  Y.Â Dong, Randomized optimal stopping problem in continuous time and
  reinforcement learning algorithm, SIAM Journal on Control and Optimization,
  62 (2024), pp.Â 1590â€“1614.
* [10]

  K.Â D. Elworthy and X.-M. Li, Formulae for the derivatives of heat
  semigroups, Journal of Functional Analysis, 125 (1994), pp.Â 252â€“286.
* [11]

  L.Â He and Z.Â Liang, Optimal investment strategy for the dc plan with
  the return of premiums clauses in a meanâ€“variance framework, Insurance:
  Mathematics and Economics, 53 (2013), pp.Â 643â€“649.
* [12]

  M.Â Jeanblanc, M.Â Yor, and M.Â Chesney, Mathematical methods for
  financial markets, Springer Science & Business Media, 2009.
* [13]

  G.Â M. Lieberman, Second order parabolic differential equations,
  World scientific, 1996.
* [14]

  H.Â M. Markowitz, Portfolio selection: efficient diversification of
  investments, Yale university press, 2008.
* [15]

  G.Â Peskir and A.Â Shiryaev, Optimal stopping and free-boundary
  problems, Springer, 2006.
* [16]

  R.Â H. Strotz, Myopia and inconsistency in dynamic utility
  maximization, The review of economic studies, 23 (1955), pp.Â 165â€“180.
* [17]

  A.Â Tartakovsky, I.Â Nikiforov, and M.Â Basseville, Sequential
  analysis: Hypothesis testing and changepoint detection, CRC press, 2014.

## Appendix A Proof of ([9](https://arxiv.org/html/2510.24128v1#S4.E9 "In 4 Extended HJB Equation for Original Problem â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method"))

Define another stopping time Ï„~Îµ\tilde{\tau}^{\varepsilon} as

|  |  |  |
| --- | --- | --- |
|  | Ï„~Îµ=inf{sâ‰¥t||Xsâˆ’Xt|â‰¥Îµ}.\tilde{\tau}^{\varepsilon}=\inf\{s\geq t||X\_{s}-X\_{t}|\geq\varepsilon\}. |  |

Note that Ï„Îµ\tau^{\varepsilon} coincides to Ï„~Îµ\tilde{\tau}^{\varepsilon} on the set {Ï„~Îµ<t+Îµ}\{\tilde{\tau}^{\varepsilon}<t+\varepsilon\}. Then, we claim that

|  |  |  |  |
| --- | --- | --- | --- |
|  | limÎµâ†’0Îµ2ð”¼t,xâ€‹[Ï„~Îµâˆ’t]=Ïƒ2â€‹(t,x).\lim\_{\varepsilon\rightarrow 0}\frac{\varepsilon^{2}}{\mathbb{E}\_{t,x}[\tilde{\tau}^{\varepsilon}-t]}=\sigma^{2}(t,x). |  | (13) |

To prove the claim, we follow the same argument as that in the proof of Lemma 5.5 in [[7](https://arxiv.org/html/2510.24128v1#bib.bib7)]. For any a>1Ïƒ2â€‹(t,x)a>\frac{1}{\sigma^{2}(t,x)}, define a function FF as Fâ€‹(t,y)=aâ€‹(yâˆ’x)2âˆ’tF(t,y)=a(y-x)^{2}-t. It is easy to see that

|  |  |  |
| --- | --- | --- |
|  | (âˆ‚t+â„’)â€‹Fâ€‹(s,y)=2â€‹aâ€‹Î¼â€‹(s,y)â€‹(yâˆ’x)+aâ€‹Ïƒ2â€‹(s,y)âˆ’1,(\partial\_{t}+\mathcal{L})F(s,y)=2a\mu(s,y)(y-x)+a\sigma^{2}(s,y)-1, |  |

which is greater than 0 for {(s,y)||yâˆ’x|â‰¤Îµ,sâˆ’tâ‰¤Îµ}\{(s,y)||y-x|\leq\varepsilon,s-t\leq\varepsilon\} with a sufficiently small Îµ\varepsilon. Thus, applying ItÃ´ formula to Fâ€‹(s,Xs)F(s,X\_{s}) and taking conditional expectation, it holds that

|  |  |  |
| --- | --- | --- |
|  | ð”¼t,xâ€‹[Ï„~Îµâˆ’t]â‰¤aâ€‹ð”¼t,xâ€‹[|XÏ„~Îµâˆ’x|2]=aâ€‹Îµ2.\mathbb{E}\_{t,x}[\tilde{\tau}^{\varepsilon}-t]\leq a\mathbb{E}\_{t,x}[|X\_{\tilde{\tau}^{\varepsilon}}-x|^{2}]=a\varepsilon^{2}. |  |

Similarly, one also has

|  |  |  |
| --- | --- | --- |
|  | ð”¼t,xâ€‹[Ï„~Îµâˆ’t]â‰¥aâ€‹Îµ2,\mathbb{E}\_{t,x}[\tilde{\tau}^{\varepsilon}-t]\geq a\varepsilon^{2}, |  |

for a<1Ïƒ2â€‹(t,x)a<\frac{1}{\sigma^{2}(t,x)}. Combining these two estimations give the claim ([13](https://arxiv.org/html/2510.24128v1#A1.E13 "In Appendix A Proof of (9) â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")).
Using Chebyshevâ€™s inequality, we also have that

|  |  |  |  |
| --- | --- | --- | --- |
|  | Pâ€‹(Ï„~Îµ>t+Îµ)â‰¤ð”¼t,xâ€‹[Ï„~Îµâˆ’t]Îµ=Oâ€‹(Îµ).P(\tilde{\tau}^{\varepsilon}>t+\varepsilon)\leq\frac{\mathbb{E}\_{t,x}[\tilde{\tau}^{\varepsilon}-t]}{\varepsilon}=O(\varepsilon). |  | (14) |

The argument for ([13](https://arxiv.org/html/2510.24128v1#A1.E13 "In Appendix A Proof of (9) â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")) also yields that

|  |  |  |
| --- | --- | --- |
|  | limhâ†’0ð”¼t,xâ€‹[|XÏ„Îµâˆ’x|2]ð”¼t,xâ€‹[Ï„Îµâˆ’t]=Ïƒ2â€‹(t,x).\lim\_{h\rightarrow 0}\frac{\mathbb{E}\_{t,x}[|X\_{\tau^{\varepsilon}}-x|^{2}]}{\mathbb{E}\_{t,x}[\tau^{\varepsilon}-t]}=\sigma^{2}(t,x). |  |

Note that, using ([14](https://arxiv.org/html/2510.24128v1#A1.E14 "In Appendix A Proof of (9) â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")),

|  |  |  |
| --- | --- | --- |
|  | Îµ2â‰¥ð”¼t,xâ€‹[|XÏ„Îµâˆ’x|2]â‰¥Îµ2â€‹Pâ€‹(Ï„Îµ<t+Îµ)=Îµ2âˆ’Îµ2â€‹Pâ€‹(Ï„Îµ=t+Îµ)=Îµ2+oâ€‹(Îµ2).\varepsilon^{2}\geq\mathbb{E}\_{t,x}[|X\_{\tau^{\varepsilon}}-x|^{2}]\geq\varepsilon^{2}P(\tau^{\varepsilon}<t+\varepsilon)=\varepsilon^{2}-\varepsilon^{2}P(\tau^{\varepsilon}=t+\varepsilon)=\varepsilon^{2}+o(\varepsilon^{2}). |  |

Hence, we get that

|  |  |  |  |
| --- | --- | --- | --- |
|  | ð”¼t,xâ€‹[Ï„Îµâˆ’t]=1Ïƒ2â€‹(t,x)â€‹Îµ2+oâ€‹(Îµ).\mathbb{E}\_{t,x}[\tau^{\varepsilon}-t]=\frac{1}{\sigma^{2}(t,x)}\varepsilon^{2}+o(\varepsilon). |  | (15) |

Let kâ€‹(t,y):=|câ€‹(t)âˆ’y|k(t,y):=|c(t)-y|. From ItÃ´-Tanaka formula, one has that

|  |  |  |
| --- | --- | --- |
|  | kâ€‹(Ï„Îµ,XÏ„Îµ)=âˆ«tÏ„Îµ12â€‹((âˆ‚t+â„’)â€‹kâ€‹(s,Xs+)+(âˆ‚t+â„’)â€‹kâ€‹(s,Xsâˆ’))â€‹ð‘‘s+âˆ«tÏ„Îµ12â€‹Ïƒâ€‹(âˆ‚ykâ€‹(s,Xs+)+âˆ‚ykâ€‹(s,Xsâˆ’))â€‹ð‘‘Ws+âˆ«tÏ„Îµ12â€‹(âˆ‚ykâ€‹(s,Xs+)âˆ’âˆ‚ykâ€‹(s,Xsâˆ’))â€‹1{Xs=câ€‹(s)}â€‹ð‘‘lsc.\begin{split}k(\tau^{\varepsilon},X\_{\tau^{\varepsilon}})=&\int\_{t}^{\tau^{\varepsilon}}\frac{1}{2}((\partial\_{t}+\mathcal{L})k(s,X\_{s}+)+(\partial\_{t}+\mathcal{L})k(s,X\_{s}-))ds\\ &+\int\_{t}^{\tau^{\varepsilon}}\frac{1}{2}\sigma(\partial\_{y}k(s,X\_{s}+)+\partial\_{y}k(s,X\_{s}-))dW\_{s}\\ &+\int\_{t}^{\tau^{\varepsilon}}\frac{1}{2}(\partial\_{y}k(s,X\_{s}+)-\partial\_{y}k(s,X\_{s}-))1\_{\{X\_{s}=c(s)\}}dl\_{s}^{c}.\end{split} |  |

Note that, for Xs=câ€‹(s)X\_{s}=c(s), âˆ‚ykâ€‹(s,Xs+)=1\partial\_{y}k(s,X\_{s}+)=1 and âˆ‚ykâ€‹(s,Xs+)=âˆ’1\partial\_{y}k(s,X\_{s}+)=-1. Thus,

|  |  |  |  |
| --- | --- | --- | --- |
|  | (ð”¼t,xâ€‹[kâ€‹(Ï„Îµ,XÏ„Îµ)])2=(ð”¼t,xâ€‹[lÏ„Îµcâˆ’ltc])2+oâ€‹(ð”¼t,xâ€‹[Ï„Îµâˆ’t]).(\mathbb{E}\_{t,x}[k(\tau^{\varepsilon},X\_{\tau^{\varepsilon}})])^{2}=(\mathbb{E}\_{t,x}[l^{c}\_{\tau^{\varepsilon}}-l^{c}\_{t}])^{2}+o(\mathbb{E}\_{t,x}[\tau^{\varepsilon}-t]). |  | (16) |

When tt is fixed, for sufficienly small Îµ\varepsilon,

|  |  |  |
| --- | --- | --- |
|  | ð”¼t,xâ€‹[kâ€‹(Ï„Îµ,XÏ„Îµ)]=ð”¼t,xâ€‹[|x+Îµâˆ’câ€‹(Ï„Îµ)|â€‹1{XÏ„Îµ=x+Îµ}]+ð”¼t,xâ€‹[|xâˆ’Îµâˆ’câ€‹(Ï„Îµ)|â€‹1{XÏ„Îµ=xâˆ’Îµ}]+ð”¼t,xâ€‹[|XÏ„Îµâˆ’câ€‹(Ï„Îµ)|â€‹1{Ï„Îµ=t+Îµ}].\begin{split}\mathbb{E}\_{t,x}[k(\tau^{\varepsilon},X\_{\tau^{\varepsilon}})]=&\mathbb{E}\_{t,x}[|x+\varepsilon-c(\tau^{\varepsilon})|1\_{\{X\_{\tau^{\varepsilon}=x+\varepsilon}\}}]+\mathbb{E}\_{t,x}[|x-\varepsilon-c(\tau^{\varepsilon})|1\_{\{X\_{\tau^{\varepsilon}=x-\varepsilon}\}}]\\ &+\mathbb{E}\_{t,x}[|X\_{\tau^{\varepsilon}}-c(\tau^{\varepsilon})|1\_{\{\tau^{\varepsilon}=t+\varepsilon\}}].\end{split} |  |

We estimate the right hand side term by term. Estimations for the first two term
are similar. It holds that

|  |  |  |
| --- | --- | --- |
|  | ð”¼t,xâ€‹[|xÂ±Îµâˆ’câ€‹(Ï„Îµ)|â€‹1{XÏ„Îµ=xÂ±Îµ}]â‰¤ð”¼t,xâ€‹[(|xÂ±Îµâˆ’câ€‹(t)|+|câ€‹(t)âˆ’câ€‹(Ï„Îµ)|)â€‹1{XÏ„Îµ=xÂ±Îµ}]â‰¤pÂ±â€‹Îµ+Câ€‹ð”¼t,xâ€‹[Ï„Îµâˆ’t]=pÂ±+Oâ€‹(Îµ2),\begin{split}\mathbb{E}\_{t,x}[|x\pm\varepsilon-c(\tau^{\varepsilon})|1\_{\{X\_{\tau^{\varepsilon}=x\pm\varepsilon}\}}]&\leq\mathbb{E}\_{t,x}[\left(|x\pm\varepsilon-c(t)|+|c(t)-c(\tau^{\varepsilon})|\right)1\_{\{X\_{\tau^{\varepsilon}=x\pm\varepsilon}\}}]\\ &\leq p\_{\pm}\varepsilon+C\mathbb{E}\_{t,x}[\tau^{\varepsilon}-t]=p\_{\pm}+O(\varepsilon^{2}),\end{split} |  |

where pÂ±=Pâ€‹(XÏ„Îµ=xÂ±Îµ)p\_{\pm}=P(X\_{\tau^{\varepsilon}}=x\pm\varepsilon) and the last inequality is due to the assumption that câ€‹(â‹…)c(\cdot) is Lipschitz continuous. For the last term,

|  |  |  |
| --- | --- | --- |
|  | ð”¼t,xâ€‹[|XÏ„Îµâˆ’câ€‹(Ï„Îµ)|â€‹1{Ï„Îµ=t+Îµ}]=ð”¼t,xâ€‹[|XÏ„Îµâˆ’câ€‹(t)+câ€‹(t)âˆ’câ€‹(Ï„Îµ)|â€‹1{Ï„Îµ=t+Îµ}]â‰¤Câ€‹Îµâ€‹Pâ€‹(Ï„Îµ=t+Îµ)=Câ€‹Îµâ€‹Pâ€‹(Ï„~Îµ>t+Îµ)=Oâ€‹(Îµ2).\begin{split}\mathbb{E}\_{t,x}[|X\_{\tau^{\varepsilon}}-c(\tau^{\varepsilon})|1\_{\{\tau^{\varepsilon}=t+\varepsilon\}}]=&\mathbb{E}\_{t,x}[|X\_{\tau^{\varepsilon}}-c(t)+c(t)-c(\tau^{\varepsilon})|1\_{\{\tau^{\varepsilon}=t+\varepsilon\}}]\\ \leq&C\varepsilon P(\tau^{\varepsilon}=t+\varepsilon)=C\varepsilon P(\tilde{\tau}^{\varepsilon}>t+\varepsilon)=O(\varepsilon^{2}).\end{split} |  |

Combining these estimations, we get that

|  |  |  |  |
| --- | --- | --- | --- |
|  | ð”¼â€‹[kâ€‹(Ï„Îµ,XÏ„Îµ)]=(pâˆ’+p+)â€‹Îµ+Oâ€‹(Îµ2)=Îµ+Oâ€‹(Îµ2),\mathbb{E}[k(\tau^{\varepsilon},X\_{\tau^{\varepsilon}})]=(p\_{-}+p\_{+})\varepsilon+O(\varepsilon^{2})=\varepsilon+O(\varepsilon^{2}), |  | (17) |

where we use ([14](https://arxiv.org/html/2510.24128v1#A1.E14 "In Appendix A Proof of (9) â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")) again to get the last equality.
Hence, ([15](https://arxiv.org/html/2510.24128v1#A1.E15 "In Appendix A Proof of (9) â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")),([16](https://arxiv.org/html/2510.24128v1#A1.E16 "In Appendix A Proof of (9) â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")) and ([17](https://arxiv.org/html/2510.24128v1#A1.E17 "In Appendix A Proof of (9) â€£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")) yield that

|  |  |  |
| --- | --- | --- |
|  | limÎµâ†’0(ð”¼t,xâ€‹[lÏ„Îµcâˆ’ltc])2ð”¼t,xâ€‹[Ï„Îµâˆ’t]=Ïƒ2â€‹(t,x).\lim\_{\varepsilon\rightarrow 0}\frac{(\mathbb{E}\_{t,x}[l^{c}\_{\tau^{\varepsilon}}-l^{c}\_{t}])^{2}}{\mathbb{E}\_{t,x}[\tau^{\varepsilon}-t]}=\sigma^{2}(t,x). |  |

## Appendix B A Key Lemma

###### Lemma B.1.

Let Ï†\varphi be the solution of

|  |  |  |
| --- | --- | --- |
|  | (âˆ‚t+â„’)â€‹Ï†+câ€‹Ï†=g,Ï†â€‹(T,x)=f,(\partial\_{t}+\mathcal{L})\varphi+c\varphi=g,\varphi(T,x)=f, |  |

with c,gc,g being bounded continuous functions and ff also bounded with bounded derivatives. Then

1. 1.

   There exists a constant CC depending on the coefficients such that

   |  |  |  |
   | --- | --- | --- |
   |  | âˆ’Câ€‹(â€–fâˆ’â€–âˆž+Tâ€‹â€–gâˆ’â€–âˆž)â‰¤Ï†â‰¤Câ€‹(â€–f+â€–âˆž+Tâ€‹â€–g+â€–âˆž),-C(\|f^{-}\|\_{\infty}+T\|g^{-}\|\_{\infty})\leq\varphi\leq C(\|f^{+}\|\_{\infty}+T\|g^{+}\|\_{\infty}), |  |

   where fÂ±f^{\pm} represents the positive and negative part of ff respectively. If câ‰¤0c\leq 0, the constant CC can be chosen to be 11.
2. 2.

   Assuming that câ‰¡0c\equiv 0, we have

   |  |  |  |
   | --- | --- | --- |
   |  | â€–âˆ‚xÏ†â€–âˆžâ‰¤(1+Câ€‹T)â€‹â€–âˆ‚xfâ€–âˆž+Câ€‹(T+T)â€‹â€–gâ€–âˆž.\|\partial\_{x}\varphi\|\_{\infty}\leq(1+C\sqrt{T})\|\partial\_{x}f\|\_{\infty}+C(\sqrt{T}+T)\|g\|\_{\infty}. |  |

###### Proof.

From Feymann-Kac representation, it holds that

|  |  |  |
| --- | --- | --- |
|  | Ï†â€‹(t,x)=ð”¼t,xâ€‹[eâˆ«tTcâ€‹(u,Xu)â€‹ð‘‘uâ€‹fâ€‹(XT)+âˆ«tTeâˆ«tscâ€‹(u,Xu)â€‹ð‘‘uâ€‹gâ€‹(s,Xs)â€‹ð‘‘s].\varphi(t,x)=\mathbb{E}\_{t,x}\left[e^{\int\_{t}^{T}c(u,X\_{u})du}f(X\_{T})+\int\_{t}^{T}e^{\int\_{t}^{s}c(u,X\_{u})du}g(s,X\_{s})ds\right]. |  |

Then, one can easily get the first estimation from the assumptions on the coefficients.

To prove the second estimation, define two processes âˆ‡X\nabla X and NN as

|  |  |  |
| --- | --- | --- |
|  | dâ€‹âˆ‡X=âˆ‚xbâ€‹(Xt)â€‹âˆ‡Xtâ€‹dâ€‹t+âˆ‚xÏƒâ€‹(Xt)â€‹âˆ‡Xtâ€‹dâ€‹Wt,âˆ‡Xt=I,d\nabla X=\partial\_{x}b(X\_{t})\nabla X\_{t}dt+\partial\_{x}\sigma(X\_{t})\nabla X\_{t}dW\_{t},\nabla X\_{t}=I, |  |

and

|  |  |  |
| --- | --- | --- |
|  | Ns=1sâˆ’tâ€‹âˆ«ts<Ïƒâˆ’1â€‹(Xu)â€‹âˆ‡Xu,dâ€‹Wu>.N\_{s}=\frac{1}{s-t}\int\_{t}^{s}<\sigma^{-1}(X\_{u})\nabla X\_{u},dW\_{u}>. |  |

One can verify that

|  |  |  |
| --- | --- | --- |
|  | ð”¼t,xâ€‹[suptâ‰¤sâ‰¤T|âˆ‡Xsâˆ’I|2]â‰¤Câ€‹Tâ€‹, andÂ â€‹ð”¼t,xâ€‹[|Ns|2]â‰¤Câ€‹(1sâˆ’t+1).\mathbb{E}\_{t,x}[\sup\_{t\leq s\leq T}|\nabla X\_{s}-I|^{2}]\leq CT\text{, and }\mathbb{E}\_{t,x}[|N\_{s}|^{2}]\leq C(\frac{1}{s-t}+1). |  |

Then, using Bismut-Elworthy-Li formula [[10](https://arxiv.org/html/2510.24128v1#bib.bib10)], we get that, for any function Î¾\xi

|  |  |  |
| --- | --- | --- |
|  | âˆ‚xð”¼t,xâ€‹[Î¾â€‹(Xs)]=ð”¼t,xâ€‹[âˆ‚xÎ¾â€‹(Xs)â€‹âˆ‡Xs]=ð”¼t,xâ€‹[Î¾â€‹(Xs)â€‹Ns].\partial\_{x}\mathbb{E}\_{t,x}[\xi(X\_{s})]=\mathbb{E}\_{t,x}[\partial\_{x}\xi(X\_{s})\nabla X\_{s}]=\mathbb{E}\_{t,x}[\xi(X\_{s})N\_{s}]. |  |

Thus,

|  |  |  |
| --- | --- | --- |
|  | âˆ‚xÏ†â€‹(t,x)=ð”¼t,xâ€‹[âˆ‚xfâ€‹(XT)â€‹âˆ‡XT]+âˆ«tTð”¼t,xâ€‹[gâ€‹(s,Xs)â€‹Ns]â€‹ð‘‘s.\partial\_{x}\varphi(t,x)=\mathbb{E}\_{t,x}[\partial\_{x}f(X\_{T})\nabla X\_{T}]+\int\_{t}^{T}\mathbb{E}\_{t,x}[g(s,X\_{s})N\_{s}]ds. |  |

Clearly, we have

|  |  |  |
| --- | --- | --- |
|  | |ð”¼t,xâ€‹[âˆ‚xfâ€‹(XT)â€‹âˆ‡XT]|â‰¤(Câ€‹T+1)â€‹â€–âˆ‚xfâ€–âˆž,|\mathbb{E}\_{t,x}[\partial\_{x}f(X\_{T})\nabla X\_{T}]|\leq(C\sqrt{T}+1)\|\partial\_{x}f\|\_{\infty}, |  |

and

|  |  |  |
| --- | --- | --- |
|  | âˆ«tTð”¼t,xâ€‹[gâ€‹(s,Xs)â€‹Ns]â€‹ð‘‘sâ‰¤â€–gâ€–âˆžâ€‹âˆ«tT(ð”¼t,xâ€‹[|Ns|2])12â€‹ð‘‘sâ‰¤Câ€‹â€–gâ€–âˆžâ€‹âˆ«tT1(sâˆ’t)12+1â€‹dâ€‹s=Câ€‹(Tâˆ’t+(Tâˆ’t))â€‹â€–gâ€–âˆž.\begin{split}&\int\_{t}^{T}\mathbb{E}\_{t,x}[g(s,X\_{s})N\_{s}]ds\leq\|g\|\_{\infty}\int\_{t}^{T}(\mathbb{E}\_{t,x}[|N\_{s}|^{2}])^{\frac{1}{2}}ds\\ \leq&C\|g\|\_{\infty}\int\_{t}^{T}\frac{1}{(s-t)^{\frac{1}{2}}}+1ds=C(\sqrt{T-t}+(T-t))\|g\|\_{\infty}.\end{split} |  |

This gives the second estimation.
âˆŽ