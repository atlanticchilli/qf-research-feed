---
authors:
- Yuchao Dong
- Harry Zheng
doc_id: arxiv:2510.24128v1
family_id: arxiv:2510.24128
is_current: true
taxonomy:
  alpha_families: []
  asset_classes: []
  horizons: []
  themes: []
title: 'Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization
  Method'
url_abs: http://arxiv.org/abs/2510.24128v1
url_html: https://arxiv.org/html/2510.24128v1
venue: arXiv q-fin
version: 1
year: 2025
---


Yuchao Dong
and Harry Zheng
 School of Mathematical Sciences, Key Laboratory of Intelligent Computing and Applications (Ministry of Education), Tongji University, Shanghai 200092, China. The work of the second author was funded by National Natural Science Foundation of China
(No.12471425) Department of Mathematics, Imperial College, London SW7 2BZ, UK (Email: h.zheng@imperial.ac.uk). The work was supported by the Engineering and Physical Sciences Research Council of
UK (Grant No. EP/V008331/1).

###### Abstract

This paper studies the time-inconsistent MV optimal stopping problem via a game-theoretic approach to find equilibrium strategies. To overcome the mathematical intractability of direct equilibrium analysis, we propose a vanishing regularization method: first, we introduce an entropy-based regularization term to the MV objective, modeling mixed-strategy stopping times using the intensity of a Cox process. For this regularized problem, we derive a coupled extended Hamilton-Jacobi-Bellman (HJB) equation system, prove a verification theorem linking its solutions to equilibrium intensities, and establish the existence of classical solutions for small time horizons via a contraction mapping argument. By letting the regularization term tend to zero, we formally recover a system of parabolic variational inequalities that characterizes equilibrium stopping times for the original MV problem. This system includes an additional key quadratic term‚Äìa distinction from classical optimal stopping, where stopping conditions depend only on comparing the value function to the instantaneous reward.

Keywords: Mean-variance problems, Time-inconsistency, Cox process, Equilibrium stopping time, Extended HJB equation, Vanishing Regularization Method

AMS MSC2010: 60G40; 60J70; 91A10; 91A25; 91G80; 91B02; 91B51.

## 1 Introduction

Given a diffusion process XX, the classical optimal stopping problem is to determine a stopping time œÑ\tau that maximizes

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Äã[f‚Äã(XœÑ)].\mathbb{E}\left[f(X\_{\tau})\right]. |  |

Optimal stopping has many applications, for example, financial decision-making (e.g., timing for asset sales) and statistical inference (e.g., stopping rules for hypothesis testing [[17](https://arxiv.org/html/2510.24128v1#bib.bib17)]). However, in financial contexts, there is often an additional imperative to mitigate decision-related risk. In line with the mean-variance analysis of [[14](https://arxiv.org/html/2510.24128v1#bib.bib14)], we identify the return with the expectation and the risk with the variance and aim to select a stopping time that maximizes

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùîº‚Äã[f‚Äã(XœÑ)]‚àíŒ≥2‚ÄãVar‚Äã[f‚Äã(XœÑ)],\mathbb{E}\left[f(X\_{\tau})\right]-\frac{\gamma}{2}\text{Var}\left[f(X\_{\tau})\right], |  | (1) |

where Œ≥‚â•0\gamma\geq 0 denotes the risk aversion coefficient. Problem ([1](https://arxiv.org/html/2510.24128v1#S1.E1 "In 1 Introduction ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")) is referred as a mean-variance (MV) stopping problem in the literature.

Similar to the classical dynamic MV problem, the optimal stopping rule typically depends on the initial state xx, which means that it does not generally satisfy Bellman‚Äôs principle of optimality. In the literature this is known as time-inconsistency. Time-inconsistent problems are typically studied using two approaches. One is to formulate the problem for a fixed initial state and allow the corresponding optimal stopping rule to depend on that initial state, called the pre-commitment approach. The other is to look for a stopping rule that remains optimal at every period when re-evaluated from that period‚Äôs perspective, called the game-theoretic approach.

Strotz [[16](https://arxiv.org/html/2510.24128v1#bib.bib16)] is the first to explore the game-theoretic approach to time-inconsistent problems in dynamic utility maximization with non-exponential discounting. Bjork et al. [[4](https://arxiv.org/html/2510.24128v1#bib.bib4)] give a comprehensive treatment of time-inconsistent Markovian models and characterize the equilibrium by a solution to a generalized HJB equation, called the extended HJB system. Time inconsistent control problems have attracted considerable research interest in recent years with many applications. For example, Bjork et al. [[5](https://arxiv.org/html/2510.24128v1#bib.bib5)] solve a MV problem with state dependent risk aversion. He and Liang [[11](https://arxiv.org/html/2510.24128v1#bib.bib11)] study a defined contribution insurance problem in a MV framework. Dai et al. [[8](https://arxiv.org/html/2510.24128v1#bib.bib8)] solve a MV problem with reinforcement learning method.
All aforementioned papers have fixed finite horizon.

The literature on the game-theoretic approach to time-inconsistent stopping problems is in the early developing stage. Christensen and Lindensj√∂ [[6](https://arxiv.org/html/2510.24128v1#bib.bib6)] study an equilibrium stopping problem with initial state dependent reward. Bayraktar et al. [[2](https://arxiv.org/html/2510.24128v1#bib.bib2)] consider three equilibrium concepts proposed in the literature for time-inconsistent stopping problems with non-exponential discount.
There is little research for MV stopping problems. The only ones the authors are aware of are Peskir and Shiryaev [[15](https://arxiv.org/html/2510.24128v1#bib.bib15)] on the so-called dynamic optimal stopping time, which is similar to the game theoretic approach and Christensen and Lindensj√∂ [[7](https://arxiv.org/html/2510.24128v1#bib.bib7)] on a subgame perfect Nash equilibrium for stopping problems.

In this paper, we study the equilibrium strategy and relate it to the extended HJB equation, which means that we need to formulate the problem as a game and look for equilibrium. It is a fundamental result in game theory that equilibrium generally exists for mixed strategies rather than pure strategies in a broad class of games111For example, the rock - paper - scissors game is a classic example in game theory, and it has no pure strategy Nash equilibrium but has a mixed strategy Nash equilibrium in which the player choose each action with equal probability 1/31/3. . Hence, we focus on mixed strategy stopping times by allowing the agents to choose the intensity function of a Cox process as a randomization device for the stopping decision, whereas [[7](https://arxiv.org/html/2510.24128v1#bib.bib7), [15](https://arxiv.org/html/2510.24128v1#bib.bib15)] characterize the equilibrium and provide other necessary and sufficient equilibrium conditions, but do not derive the extended HJB equation. While their results coincide with ours for geometric Brownian motion case, our derivation is motivated by the vanishing regularity approach, a key distinction from prior research.

We now describe the key methodology for solving the MV stopping problem. We first add a regularization term, weighted by a constant Œª\lambda into the target functional (see ([2](https://arxiv.org/html/2510.24128v1#S2.E2 "In 2 Mean Variance Stopping and its Relaxed Problem ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method"))), to form a regularized problem with control variate being the intensity (as opposed to the stopping time), which makes the definition of the equilibrium straightforward. We then derive the associated extended HJB equation (see ([3](https://arxiv.org/html/2510.24128v1#S3.E3 "In Theorem 3.1. ‚Ä£ 3 Extended HJB Equation for Regularized Problem ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method"))), prove a verification theorem (see Theorem [3.1](https://arxiv.org/html/2510.24128v1#S3.Thmtheorem1 "Theorem 3.1. ‚Ä£ 3 Extended HJB Equation for Regularized Problem ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")) and establish the existence of its solution under certain technical assumptions (see Theorem [3.2](https://arxiv.org/html/2510.24128v1#S3.Thmtheorem2 "Theorem 3.2. ‚Ä£ 3 Extended HJB Equation for Regularized Problem ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")). Finally, we let Œª\lambda tend to zero (i.e., vanishing regularization) to formally obtain a system of parabolic variational inequalities (see ([7](https://arxiv.org/html/2510.24128v1#S4.E7 "In 4 Extended HJB Equation for Original Problem ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method"))) that characterizes the equilibrium stopping time for the original MV stopping problem. To the best knowledge of the authors, this is the first time such a system of equations is reported in the literature. Furthermore, we find that the stopping condition is not determined solely by comparing the value function with the instantaneous reward; instead, an additional quadratic term also plays a role in the formulation, which is in sharp contrast to the standard optimal stopping problem.

Finally, we aim to emphasize the motivation underlying our method. In [[9](https://arxiv.org/html/2510.24128v1#bib.bib9)], the authors investigated an entropy-regularized optimal stopping problem and demonstrated that the corresponding optimal value function is associated with a penalized form for the variational inequality. Notably, this penalized equation also converges to the original variational inequality as the regularization parameter tends to zero. For MV optimal stopping problems, we adopt an analogous approach: we first introduce and analyze a regularized version of the problem, then subsequently let the regularization parameter vanish to recover results for the original MV stopping problem. Such ideas are pervasive in mathematical research. When addressing a computationally or theoretically challenging problem, researchers often first consider a perturbed or regularized counterpart‚Äîwhose solution is more tractable to derive‚Äîand then take an appropriate limit to revert to the original problem. It is precisely this core logic that leads us to name our approach Vanishing Regularization Method.

The rest of the paper is organized as follows: In Section 2 we formulate the MV stopping problem and its relaxed problem together with the definition of equilibrium. In Section 3 we derive the extended HJB equation for the relaxed problem and prove a verification theorem (Theorem [3.1](https://arxiv.org/html/2510.24128v1#S3.Thmtheorem1 "Theorem 3.1. ‚Ä£ 3 Extended HJB Equation for Regularized Problem ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")) and an existence result (Theorem [3.2](https://arxiv.org/html/2510.24128v1#S3.Thmtheorem2 "Theorem 3.2. ‚Ä£ 3 Extended HJB Equation for Regularized Problem ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")). In Section 4 we let Œª\lambda tend to zero to formally get the variation system for the original MV stopping problem and show that it characterizes the equilibrium stopping time (Theorems [4.1](https://arxiv.org/html/2510.24128v1#S4.Thmtheorem1 "Theorem 4.1. ‚Ä£ 4 Extended HJB Equation for Original Problem ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method") and [4.2](https://arxiv.org/html/2510.24128v1#S4.Thmtheorem2 "Theorem 4.2. ‚Ä£ 4 Extended HJB Equation for Original Problem ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")). In Section 5 we give some further discussions on our results, including infinite horizon case, discrete time approximation, and general time-inconsistent problems. Section 6 concludes the paper. Appendix contains the proofs of a local time approximation relation ([9](https://arxiv.org/html/2510.24128v1#S4.E9 "In 4 Extended HJB Equation for Original Problem ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")) and a technical lemma (Lemma [B.1](https://arxiv.org/html/2510.24128v1#A2.Thmlemma1 "Lemma B.1. ‚Ä£ Appendix B A Key Lemma ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")) that is needed in the proof of Theorem [3.2](https://arxiv.org/html/2510.24128v1#S3.Thmtheorem2 "Theorem 3.2. ‚Ä£ 3 Extended HJB Equation for Regularized Problem ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method").

## 2 Mean Variance Stopping and its Relaxed Problem

In this section, we introduce the basic framework of the problem. Especially, we give the formulation for the relaxed MV stopping problem and the definition of related equilibrium strategy.
Let (Œ©,‚Ñ±,P)(\Omega,\mathcal{F},P) be a probability space, on which a standard dd-dimensional Brownian motion WW is defined222For simplicity, we consider the case that the dimension of the Brownian motion is same to that of the state process. It can be extended to other cases without any major modification as long as Assumption [2.1](https://arxiv.org/html/2510.24128v1#S2.Thmassumption1 "Assumption 2.1. ‚Ä£ 2 Mean Variance Stopping and its Relaxed Problem ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method") holds.. The ‚Ñùd\mathbb{R}^{d}-valued state process XX satisfies

|  |  |  |
| --- | --- | --- |
|  | d‚ÄãXt=b‚Äã(t,Xt)‚Äãd‚Äãt+œÉ‚Äã(t,Xt)‚Äãd‚ÄãWt.dX\_{t}=b(t,X\_{t})dt+\sigma(t,X\_{t})dW\_{t}. |  |

Denote by ùîΩ:={‚Ñ±t}t\mathbb{F}:=\{\mathcal{F}\_{t}\}\_{t} the natural filtration generated by WW, augmented by all PP-null sets. The set ùíØt,T\mathcal{T}\_{t,T} is defined as the totality of all ùîΩ\mathbb{F}-stopping time taking values in [t,T][t,T]. For any time t‚àà[0,T]t\in[0,T], the MV stopping problem is to choose œÑ‚ààùíØt,T\tau\in\mathcal{T}\_{t,T} such that the following functional

|  |  |  |
| --- | --- | --- |
|  | J‚Äã(t,x;œÑ):=ùîºt,x‚Äã[f‚Äã(XœÑ)]‚àíŒ≥2‚ÄãVart,x‚Äã[f‚Äã(XœÑ)]J(t,x;\tau):=\mathbb{E}\_{t,x}\left[f(X\_{\tau})\right]-\frac{\gamma}{2}\text{Var}\_{t,x}\left[f(X\_{\tau})\right] |  |

is maximized, where ùîºt,x‚Äã[‚ãÖ]\mathbb{E}\_{t,x}[\cdot] and Vart,x‚Äã[‚ãÖ]\text{Var}\_{t,x}[\cdot] denote the conditional expectation and conditional variance conditioning on Xt=xX\_{t}=x, respectively.

In this paper, we shall adopt the following assumptions on the coefficients.

###### Assumption 2.1.

The coefficients b,œÉb,\sigma and ff are Lipschitz continuous with linear growth in xx, uniformly in tt, i.e., there exists a constant CC such that, for any t‚àà[0,T]t\in[0,T] and x,y‚àà‚Ñùdx,y\in\mathbb{R}^{d},

|  |  |  |
| --- | --- | --- |
|  | |b‚Äã(t,x)|,|œÉ‚Äã(t,x)|,|f‚Äã(x)|‚â§C‚Äã(1+|x|),|b(t,x)|,|\sigma(t,x)|,|f(x)|\leq C(1+|x|), |  |

and

|  |  |  |
| --- | --- | --- |
|  | |b‚Äã(t,x)‚àíb‚Äã(t,y)|,|œÉ‚Äã(t,x)‚àíœÉ‚Äã(t,y)|,|f‚Äã(x)‚àíf‚Äã(y)|‚â§C‚Äã|x‚àíy|.|b(t,x)-b(t,y)|,|\sigma(t,x)-\sigma(t,y)|,|f(x)-f(y)|\leq C|x-y|. |  |

Moreover, œÉ‚ÄãœÉT\sigma\sigma^{T} is uniformly non-degenerate, i.e., there exists a constant cc such that œÉ‚ÄãœÉT‚Äã(t,x)‚â•c‚ÄãI\sigma\sigma^{T}(t,x)\geq cI for any t‚àà[0,T]t\in[0,T] and x‚àà‚Ñùdx\in\mathbb{R}^{d}.

For any function ff defined on [0,T]√ó‚Ñùd[0,T]\times\mathbb{R}^{d}, we use the notations ‚àÇtf\partial\_{t}f and ‚àÇxf\partial\_{x}f to represent the derivatives with respect to tt and xx and ‚àÇx‚Äãxf\partial\_{xx}f its Hessian. In this paper, we use CC to represent a constant that could depend on the coefficients but may be different from line to line.

It is well-known that, for MV problem, the dynamic programming principle fails. Thus, people focus on two kinds of strategies. One is called pre-committed strategy, which is a fixed plan chosen at the initial time and enforced irrevocably across all future periods, regardless of new information or changing market conditions. On the other hand, an equilibrium strategy or time-consistent strategy is a plan that remains optimal at every period when re-evaluated from that period‚Äôs perspective, which aligns with the concept of a subgame perfect equilibrium in dynamic games, where strategies are optimal in all ‚Äùsubgames‚Äù (i.e., at all points in time).

In this paper, we study the equilibrium strategy, especially the extended HJB equation related to it. This means that we need to formulate the problem as a game and look for equilibrium. It is indeed a fundamental result in game theory that equilibrium generally exists for mixed strategies rather than pure strategies in a broad class of games. Hence, it is better to consider some notion of mixed strategy, namely, at each time, the players fix a probability of stopping and decide whether or not to stop according to this probability. Such mixed strategy is also called randomized stopping time in some literature, see, for example, Bayer et al. [[1](https://arxiv.org/html/2510.24128v1#bib.bib1)] and Dong [[9](https://arxiv.org/html/2510.24128v1#bib.bib9)]. Here, we model it as a doubly stochastic Poisson process. More precisely, let Œò\Theta be a random variable, which is exponentially distributed with unit intensity and independent of Brownian motion. Given a non-negative ùîΩ\mathbb{F}-adapted process {œÄs}t‚â§s‚â§T\{\pi\_{s}\}\_{t\leq s\leq T}, a random time œÑ\tau is defined as

|  |  |  |
| --- | --- | --- |
|  | œÑ:=inf{s‚àà[t,T]:‚à´tsœÄu‚Äãùëëu‚â•Œò}‚àßT,\tau:=\inf\left\{s\in[t,T]:\int\_{t}^{s}\pi\_{u}du\geq\Theta\right\}{\wedge}T, |  |

where we adopt the convention that the infimum of an empty set is infinity. It represents the time that the player chooses to stop. Literally speaking, it means that, conditioning on having not stopped before, the probability that the player stops between tt and t+d‚Äãtt+dt is œÄt‚Äãd‚Äãt\pi\_{t}dt. Under this formulation, instead of choosing a stopping time, the player chooses the intensity process œÄ\pi to optimize the MV objective function.

From the results of doubly stochastic Poisson processes (see Jeanblanc et al.¬†[[12](https://arxiv.org/html/2510.24128v1#bib.bib12)] for details), one can compute that, for any function œÜ\varphi,

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Äã[œÜ‚Äã(XœÑ)|‚Ñ±T]=‚à´tTœÜ‚Äã(Xs)‚ÄãœÄs‚Äãe‚àí‚à´tsœÄu‚Äãùëëu‚Äãùëës+œÜ‚Äã(XT)‚Äãe‚àí‚à´tTœÄu‚Äãùëëu.\mathbb{E}\left[\varphi(X\_{\tau})|\mathcal{F}\_{T}\right]=\int\_{t}^{T}\varphi(X\_{s})\pi\_{s}e^{-\int\_{t}^{s}\pi\_{u}du}ds+\varphi(X\_{T})e^{-\int\_{t}^{T}\pi\_{u}du}. |  |

Then, we have that

|  |  |  |
| --- | --- | --- |
|  | ùîºt,x‚Äã[œÜ‚Äã(XœÑ)]=ùîºt,x‚Äã[ùîº‚Äã[œÜ‚Äã(XœÑ)|‚Ñ±T]]=ùîºt,x‚Äã[‚à´tTœÜ‚Äã(Xs)‚ÄãœÄs‚Äãe‚àí‚à´tsœÄu‚Äãùëëu‚Äãùëës+œÜ‚Äã(XT)‚Äãe‚àí‚à´tTœÄu‚Äãùëëu].\mathbb{E}\_{t,x}[\varphi(X\_{\tau})]=\mathbb{E}\_{t,x}[\mathbb{E}[\varphi(X\_{\tau})|\mathcal{F}\_{T}]]=\mathbb{E}\_{t,x}\left[\int\_{t}^{T}\varphi(X\_{s})\pi\_{s}e^{-\int\_{t}^{s}\pi\_{u}du}ds+\varphi(X\_{T})e^{-\int\_{t}^{T}\pi\_{u}du}\right]. |  |

Thus, the MV criteria can be rewritten as

|  |  |  |
| --- | --- | --- |
|  | ùîºt,x‚Äã[f‚Äã(XœÑ)]‚àíŒ≥2‚ÄãVart,x‚Äã[f‚Äã(XœÑ)]=ùîºt,x‚Äã[‚à´tT(f‚àíŒ≥2‚Äãf2)‚Äã(Xs)‚ÄãœÄs‚Äãe‚àí‚à´tsœÄu‚Äãùëëu‚Äãùëës+(f‚àíŒ≥2‚Äãf2)‚Äã(XT)‚Äãe‚àí‚à´tTœÄu‚Äãùëëu]+Œ≥2‚Äã(ùîºt,x‚Äã[‚à´tTf‚Äã(Xs)‚ÄãœÄs‚Äãe‚àí‚à´tsœÄu‚Äãùëëu‚Äãùëës+f‚Äã(XT)‚Äãe‚àí‚à´tTœÄu‚Äãùëëu])2.\begin{split}&\mathbb{E}\_{t,x}\left[f(X\_{\tau})\right]-\frac{\gamma}{2}\text{Var}\_{t,x}\left[f(X\_{\tau})\right]\\ =&\mathbb{E}\_{t,x}\left[\int\_{t}^{T}(f-\frac{\gamma}{2}f^{2})(X\_{s})\pi\_{s}e^{-\int\_{t}^{s}\pi\_{u}du}ds+(f-\frac{\gamma}{2}f^{2})(X\_{T})e^{-\int\_{t}^{T}\pi\_{u}du}\right]\\ &+\frac{\gamma}{2}\left(\mathbb{E}\_{t,x}\left[\int\_{t}^{T}f(X\_{s})\pi\_{s}e^{-\int\_{t}^{s}\pi\_{u}du}ds+f(X\_{T})e^{-\int\_{t}^{T}\pi\_{u}du}\right]\right)^{2}.\end{split} |  |

However, we consider a regularized MV reward, which is defined as the following

|  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- |
|  | JŒª‚Äã(t,x;œÄ)\displaystyle J^{\lambda}(t,x;\pi) | :=\displaystyle:= | ùîºt,x‚Äã[‚à´tT{(f‚àíŒ≥2‚Äãf2)‚Äã(Xs)‚ÄãœÄs+Œª‚ÄãH‚Äã(œÄs)}‚Äãe‚àí‚à´tsœÄu‚Äãùëëu‚Äãùëës+(f‚àíŒ≥2‚Äãf2)‚Äã(XT)‚Äãe‚àí‚à´tTœÄu‚Äãùëëu]\displaystyle\mathbb{E}\_{t,x}\left[\int\_{t}^{T}\left\{(f-\frac{\gamma}{2}f^{2})(X\_{s})\pi\_{s}+\lambda H(\pi\_{s})\right\}e^{-\int\_{t}^{s}\pi\_{u}du}ds+(f-\frac{\gamma}{2}f^{2})(X\_{T})e^{-\int\_{t}^{T}\pi\_{u}du}\right] |  | (2) |
|  |  |  | +Œ≥2‚Äã(ùîºt,x‚Äã[‚à´tTf‚Äã(Xs)‚ÄãœÄs‚Äãe‚àí‚à´tsœÄu‚Äãùëëu‚Äãùëës+f‚Äã(XT)‚Äãe‚àí‚à´tTœÄu‚Äãùëëu])2,\displaystyle{}+\frac{\gamma}{2}\left(\mathbb{E}\_{t,x}\left[\int\_{t}^{T}f(X\_{s})\pi\_{s}e^{-\int\_{t}^{s}\pi\_{u}du}ds+f(X\_{T})e^{-\int\_{t}^{T}\pi\_{u}du}\right]\right)^{2}, |  |

where the function HH is given by H‚Äã(œÄ)=œÄ‚àíœÄ‚Äãlog‚Å°œÄH(\pi)=\pi-\pi\log\pi and Œª\lambda is a positive constant.
The regularization term prevents the intensity taking values 0 and ‚àû\infty, which refers to continue and stop deterministically. This provides mathematically tractability for the problem, which one can derive the related HJB equations. The motivation comes from [[9](https://arxiv.org/html/2510.24128v1#bib.bib9)] in the study of reinforcement learning method for optimal stopping problem, where HH is referred as unnormalized entropy to encourage randomness in the strategy.

We will let Œª\lambda tend to zero to go back to the original problem. Dong [[9](https://arxiv.org/html/2510.24128v1#bib.bib9)] proves that the HJB equations converge for optimal stopping problem. To our best knowledge, it is still an open problem for MV stopping problem, but, formally, we will see that the extended HJB equation converge to some system that gives an equilibrium stopping time for MV problem.

Since we focus on extended HJB equation, we shall restrict to the Markovian strategies, i.e., œÄs=œÄ‚Äã(s,Xs)\pi\_{s}=\pi(s,X\_{s}) for some deterministic function œÄ\pi. Similar to [[3](https://arxiv.org/html/2510.24128v1#bib.bib3)], one can define an equilibrium in the following sense.

###### Definition 2.1.

A strategy œÄ‚àó\pi^{\*} is called an equilibrium strategy, if for any Œµ,v>0\varepsilon,v>0 and t‚àà[0,T)t\in[0,T), define the perturbed policy œÄŒµ,v\pi^{\varepsilon,v} as

|  |  |  |
| --- | --- | --- |
|  | œÄŒµ,v(s,x)={v,¬†if¬†t‚â§s‚â§t+Œµ;œÄ‚àó‚Äã(s,x)‚Äã¬†if¬†s>t+Œµ,\pi^{\varepsilon,v}(s,x)=\left\{\begin{split}&v,\text{ if $t\leq s\leq t+\varepsilon$};\\ &\pi^{\*}(s,x)\text{ if $s>t+\varepsilon$},\end{split}\right. |  |

and it holds that

|  |  |  |
| --- | --- | --- |
|  | lim infŒµ‚Üí0JŒª‚Äã(t,x;œÄ‚àó)‚àíJŒª‚Äã(t,x;œÄŒµ,v)Œµ‚â•0,a.s.,\liminf\_{\varepsilon\rightarrow 0}\frac{J^{\lambda}(t,x;\pi^{\*})-J^{\lambda}(t,x;\pi^{\varepsilon,v})}{\varepsilon}\geq 0,a.s., |  |

for any initial state xx.

## 3 Extended HJB Equation for Regularized Problem

In this section, we derive the extended HJB equation for the regularized MV problem. To this end, we first define an operator ‚Ñí\mathcal{L} as, for any smooth function œÜ\varphi,

|  |  |  |
| --- | --- | --- |
|  | (‚Ñí‚ÄãœÜ)‚Äã(t,x):=12‚Äãtr(œÉ‚ÄãœÉT‚Äã(t,x)‚Äã‚àÇx‚ÄãxœÜ‚Äã(t,x))+b‚Äã(t,x)‚Äã‚àÇxœÜ‚Äã(t,x).(\mathcal{L}\varphi)(t,x):=\frac{1}{2}\mathop{\text{tr}}(\sigma\sigma^{T}(t,x)\partial\_{xx}\varphi(t,x))+b(t,x)\partial\_{x}\varphi(t,x). |  |

We have the following verification theorem.

###### Theorem 3.1.

For a Markovian strategy œÄ‚àó=œÄ‚àó‚Äã(t,x)\pi^{\*}=\pi^{\*}(t,x), let (VŒª,gŒª)(V^{\lambda},g^{\lambda}) be a classical solution of the following parabolic system

|  |  |  |  |
| --- | --- | --- | --- |
|  | {‚àÇtVŒª+‚Ñí‚ÄãVŒª+Œª‚Äãexp‚Å°(‚àíVŒª+Œ≥2‚Äã(f‚àígŒª)2‚àífŒª)‚àíŒ≥‚Äã|œÉ‚Äã‚àÇxgŒª|2=0,VŒª‚Äã(T,x)=f‚Äã(x),‚àÇtgŒª+‚Ñí‚ÄãgŒª‚àíexp‚Å°(‚àíVŒª+Œ≥2‚Äã(f‚àígŒª)2‚àífŒª)‚Äã(gŒª‚àíf)=0,gŒª‚Äã(T,x)=f‚Äã(x).\left\{\begin{split}&\partial\_{t}V^{\lambda}+\mathcal{L}V^{\lambda}+\lambda\exp(-\frac{V^{\lambda}+\frac{\gamma}{2}(f-g^{\lambda})^{2}-f}{\lambda})-\gamma|\sigma\partial\_{x}g^{\lambda}|^{2}=0,\ V^{\lambda}(T,x)=f(x),\\ &\partial\_{t}g^{\lambda}+\mathcal{L}g^{\lambda}-\exp(-\frac{V^{\lambda}+\frac{\gamma}{2}(f-g^{\lambda})^{2}-f}{\lambda})(g^{\lambda}-f)=0,\ g^{\lambda}(T,x)=f(x).\end{split}\right. |  | (3) |

Assume that VŒªV^{\lambda}, gŒªg^{\lambda}, their derivatives (up to first order in tt and second order in xx) and œÄ\pi are all continuous with polynomial growth in xx, uniformly in tt. Then œÄ‚àó\pi^{\*} is an equilibrium strategy if and only if

|  |  |  |  |
| --- | --- | --- | --- |
|  | œÄ‚àó‚Äã(t,x)=exp‚Å°(‚àíVŒª+Œ≥2‚Äã(f‚àígŒª)2‚àífŒª).\pi^{\*}(t,x)=\exp(-\frac{V^{\lambda}+\frac{\gamma}{2}(f-g^{\lambda})^{2}-f}{\lambda}). |  | (4) |

###### Proof.

The proof consists of two steps:

1. 1.

   We start by showing that gŒª‚Äã(t,x)=ùîºt,x‚Äã[f‚Äã(XœÑ)]g^{\lambda}(t,x)=\mathbb{E}\_{t,x}[f(X\_{\tau})] and VŒª‚Äã(t,x)=JŒª‚Äã(t,x;œÄ‚àó)V^{\lambda}(t,x)=J^{\lambda}(t,x;\pi^{\*});
2. 2.

   In the second step, we prove that œÄ‚àó\pi^{\*} is an equilibrium if and only if ([4](https://arxiv.org/html/2510.24128v1#S3.E4 "In Theorem 3.1. ‚Ä£ 3 Extended HJB Equation for Regularized Problem ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")) holds.

For the first step, applying It√¥ formula to gŒª‚Äã(s,Xs)‚Äãe‚àí‚à´tsœÄ‚àó‚Äã(u,Xu)‚Äãùëëug^{\lambda}(s,X\_{s})e^{-\int\_{t}^{s}\pi^{\*}(u,X\_{u})du}, we have that

|  |  |  |
| --- | --- | --- |
|  | d‚Äã(gŒª‚Äã(s,Xs)‚Äãe‚àí‚à´tsœÄ‚àó‚Äã(u,Xu)‚Äãùëëu)=e‚àí‚à´tsœÄ‚àó‚Äã(u,Xu)‚Äãùëëu‚Äã((‚àÇt+‚Ñí)‚ÄãgŒª‚Äã(s,Xs)‚àíœÄ‚àó‚Äã(u,Xu)‚ÄãgŒª‚Äã(u,Xu))‚Äãd‚Äãs+œÉ‚Äã‚àÇxgŒª‚Äã(s,Xs)‚Äãe‚àí‚à´tsœÄ‚àó‚Äã(u,Xu)‚Äãùëëu‚Äãd‚ÄãWs.\begin{split}d\left(g^{\lambda}(s,X\_{s})e^{-\int\_{t}^{s}\pi^{\*}(u,X\_{u})du}\right)=&e^{-\int\_{t}^{s}\pi^{\*}(u,X\_{u})du}\left((\partial\_{t}+\mathcal{L})g^{\lambda}(s,X\_{s})-\pi^{\*}(u,X\_{u})g^{\lambda}(u,X\_{u})\right)ds\\ &+\sigma\partial\_{x}g^{\lambda}(s,X\_{s})e^{-\int\_{t}^{s}\pi^{\*}(u,X\_{u})du}dW\_{s}.\end{split} |  |

From the growth assumption on the derivative of gŒªg^{\lambda}, it holds that the stochastic integral is a martingale. Thus, taking conditional expectation, one can verify that

|  |  |  |
| --- | --- | --- |
|  | gŒª‚Äã(t,x)=ùîºt,x‚Äã[‚à´tTf‚Äã(Xs)‚ÄãœÄ‚àó‚Äã(s,Xs)‚Äãe‚àí‚à´tsœÄ‚àó‚Äã(u,Xu)‚Äãùëëu‚Äãùëës+f‚Äã(XT)‚Äãe‚àí‚à´tTœÄ‚àó‚Äã(u,Xu)‚Äãùëëu],g^{\lambda}(t,x)=\mathbb{E}\_{t,x}\left[\int\_{t}^{T}f(X\_{s})\pi^{\*}(s,X\_{s})e^{-\int\_{t}^{s}\pi^{\*}(u,X\_{u})du}ds+f(X\_{T})e^{-\int\_{t}^{T}\pi^{\*}(u,X\_{u})du}\right], |  |

Then, define the function hŒªh^{\lambda} as hŒª=VŒª‚àíŒ≥2‚Äã(gŒª)2h^{\lambda}=V^{\lambda}-\frac{\gamma}{2}(g^{\lambda})^{2}. It is easy to see that hŒªh^{\lambda} solves

|  |  |  |
| --- | --- | --- |
|  | (‚àÇt+‚Ñí)‚ÄãhŒª‚Äã(t,x)+œÄ‚àó‚Äã(t,x)‚Äã(f‚àíŒ≥2‚Äãf2‚àíhŒª)‚Äã(t,x)+Œª‚ÄãH‚Äã(œÄ‚àó)=0,hŒª‚Äã(T,x)=f‚Äã(x)‚àíŒ≥2‚Äãf2‚Äã(x).(\partial\_{t}+\mathcal{L})h^{\lambda}(t,x)+\pi^{\*}(t,x)(f-\frac{\gamma}{2}f^{2}-h^{\lambda})(t,x)+\lambda H(\pi^{\*})=0,\ h^{\lambda}(T,x)=f(x)-\frac{\gamma}{2}f^{2}(x). |  |

Similarly, one can verify that

|  |  |  |
| --- | --- | --- |
|  | hŒª‚Äã(t,x)=ùîºt,x‚Äã[‚à´tT{(f‚àíŒ≥2‚Äãf2)‚Äã(Xs)‚ÄãœÄ‚àó‚Äã(s,Xs)+Œª‚ÄãH‚Äã(œÄs)}‚Äãe‚àí‚à´tsœÄ‚àó‚Äã(u,Xu)‚Äãùëëu‚Äãùëës+f‚Äã(XT)‚Äãe‚àí‚à´tTœÄ‚àó‚Äã(u,Xu)‚Äãùëëu].h^{\lambda}(t,x)=\mathbb{E}\_{t,x}\left[\int\_{t}^{T}\left\{(f-\frac{\gamma}{2}f^{2})(X\_{s})\pi^{\*}(s,X\_{s})+\lambda H(\pi\_{s})\right\}e^{-\int\_{t}^{s}\pi^{\*}(u,X\_{u})du}ds+f(X\_{T})e^{-\int\_{t}^{T}\pi^{\*}(u,X\_{u})du}\right]. |  |

This implies that JŒª‚Äã(t,x;œÄ‚àó)=hŒª‚Äã(t,x)+Œ≥2‚ÄãgŒª‚Äã(t,x)=VŒª‚Äã(t,x)J^{\lambda}(t,x;\pi^{\*})=h^{\lambda}(t,x)+\frac{\gamma}{2}g^{\lambda}(t,x)=V^{\lambda}(t,x).

Next, we prove the second step. For any tt, Œµ\varepsilon and vv, consider the perturbed strategy œÄŒµ,v\pi^{\varepsilon,v}. Since œÄŒµ,v\pi^{\varepsilon,v} coincides with œÄ‚àó\pi^{\*} after time t+Œµt+\varepsilon, it holds that

|  |  |  |
| --- | --- | --- |
|  | JŒª‚Äã(t,x;œÄŒµ,v)=ùîºt,x‚Äã[‚à´tt+Œµ{(f‚àíŒ≥2‚Äãf2)‚Äã(Xs)‚Äãv+Œª‚ÄãH‚Äã(v)}‚Äãe‚àív‚Äã(s‚àít)‚Äãùëës+hŒª‚Äã(t+Œµ,Xt+Œµ)‚Äãe‚àív‚ÄãŒµ]+Œ≥2‚Äã(ùîºt,x‚Äã[‚à´tt+Œµf‚Äã(Xs)‚Äãv‚Äãe‚àív‚Äã(s‚àít)‚Äãùëës+gŒª‚Äã(t+Œµ,Xt+Œµ)‚Äãe‚àív‚ÄãŒµ])2.\begin{split}J^{\lambda}(t,x;\pi^{\varepsilon,v})=&\mathbb{E}\_{t,x}\left[\int\_{t}^{t+\varepsilon}\left\{(f-\frac{\gamma}{2}f^{2})(X\_{s})v+\lambda H(v)\right\}e^{-v(s-t)}ds+h^{\lambda}(t+\varepsilon,X\_{t+\varepsilon})e^{-v\varepsilon}\right]\\ &+\frac{\gamma}{2}\left(\mathbb{E}\_{t,x}\left[\int\_{t}^{t+\varepsilon}f(X\_{s})ve^{-v(s-t)}ds+g^{\lambda}(t+\varepsilon,X\_{t+\varepsilon})e^{-v\varepsilon}\right]\right)^{2}.\end{split} |  |

Applying It√¥ formula to gŒª‚Äã(s,Xs)‚Äãe‚àív‚Äã(s‚àít)g^{\lambda}(s,X\_{s})e^{-v(s-t)} and taking conditional expectation, we get that

|  |  |  |
| --- | --- | --- |
|  | ùîºt,x‚Äã[gŒª‚Äã(t+Œµ,Xt+Œµ)‚Äãe‚àív‚ÄãŒµ]=gŒª‚Äã(t,x)+ùîºt,x‚Äã[‚à´tt+Œµ(‚àÇt+‚Ñí)‚ÄãgŒª‚Äã(s,Xs)‚Äãe‚àív‚Äã(s‚àít)‚àív‚ÄãgŒª‚Äã(s,Xs)‚Äãe‚àív‚Äã(s‚àít)‚Äãd‚Äãs].\begin{split}\mathbb{E}\_{t,x}[g^{\lambda}(t+\varepsilon,X\_{t+\varepsilon})e^{-v\varepsilon}]=g^{\lambda}(t,x)+\mathbb{E}\_{t,x}\left[\int\_{t}^{t+\varepsilon}(\partial\_{t}+\mathcal{L})g^{\lambda}(s,X\_{s})e^{-v(s-t)}-vg^{\lambda}(s,X\_{s})e^{-v(s-t)}ds\right].\end{split} |  |

Then, it holds that

|  |  |  |
| --- | --- | --- |
|  | ùîºt,x‚Äã[‚à´tt+Œµf‚Äã(Xs)‚Äãv‚Äãe‚àív‚Äã(s‚àít)‚Äãùëës+gŒª‚Äã(t+Œµ,Xt+Œµ)‚Äãe‚àív‚ÄãŒµ]=gŒª‚Äã(t,x)+ùîºt,x‚Äã[‚à´tt+Œµ(‚àÇt+‚Ñí)‚ÄãgŒª‚Äã(s,Xs)‚Äãe‚àív‚Äã(s‚àít)+v‚Äã(f‚àígŒª)‚Äã(s,Xs)‚Äãe‚àív‚Äã(s‚àít)‚Äãd‚Äãs]=gŒª‚Äã(t,x)+((‚àÇt+‚Ñí)‚ÄãgŒª‚Äã(t,x)+v‚Äã(f‚àígŒª)‚Äã(t,x))‚ÄãŒµ+o‚Äã(Œµ),\begin{split}&\mathbb{E}\_{t,x}\left[\int\_{t}^{t+\varepsilon}f(X\_{s})ve^{-v(s-t)}ds+g^{\lambda}(t+\varepsilon,X\_{t+\varepsilon})e^{-v\varepsilon}\right]\\ =&g^{\lambda}(t,x)+\mathbb{E}\_{t,x}\left[\int\_{t}^{t+\varepsilon}(\partial\_{t}+\mathcal{L})g^{\lambda}(s,X\_{s})e^{-v(s-t)}+v(f-g^{\lambda})(s,X\_{s})e^{-v(s-t)}ds\right]\\ =&g^{\lambda}(t,x)+\bigg((\partial\_{t}+\mathcal{L})g^{\lambda}(t,x)+v(f-g^{\lambda})(t,x)\bigg)\varepsilon+o(\varepsilon),\end{split} |  |

where the last equality is due to the fact that the related functions are continuous and XX also has continuous trajectories. Thus, we get that

|  |  |  |
| --- | --- | --- |
|  | (ùîºt,x‚Äã[‚à´tt+Œµf‚Äã(Xs)‚Äãv‚Äãe‚àív‚Äã(s‚àít)‚Äãùëës+gŒª‚Äã(t+Œµ,Xt+Œµ)‚Äãe‚àív‚ÄãŒµ])2=(gŒª)2‚Äã(t,x)+2‚ÄãgŒª‚Äã(t,x)‚Äã((‚àÇt+‚Ñí)‚ÄãgŒª‚Äã(t,x)+v‚Äã(f‚àígŒª)‚Äã(t,x))‚ÄãŒµ+o‚Äã(Œµ).\begin{split}&\left(\mathbb{E}\_{t,x}\left[\int\_{t}^{t+\varepsilon}f(X\_{s})ve^{-v(s-t)}ds+g^{\lambda}(t+\varepsilon,X\_{t+\varepsilon})e^{-v\varepsilon}\right]\right)^{2}\\ =&(g^{\lambda})^{2}(t,x)+2g^{\lambda}(t,x)\bigg((\partial\_{t}+\mathcal{L})g^{\lambda}(t,x)+v(f-g^{\lambda})(t,x)\bigg)\varepsilon+o(\varepsilon).\end{split} |  |

With a similar argument, one also have that

|  |  |  |
| --- | --- | --- |
|  | ùîºt,x‚Äã[‚à´tt+Œµ{(f‚àíŒ≥2‚Äãf2)‚Äã(Xs)‚Äãv+Œª‚ÄãH‚Äã(v)}‚Äãe‚àív‚Äã(s‚àít)‚Äãùëës+hŒª‚Äã(t+Œµ,Xt+Œµ)‚Äãe‚àív‚ÄãŒµ]=hŒª‚Äã(t,x)+((‚àÇt+‚Ñí)‚ÄãhŒª‚Äã(t,x)+v‚Äã(f‚àíŒ≥2‚Äãf2‚àíhŒª)‚Äã(t,x)+Œª‚ÄãH‚Äã(v))‚ÄãŒµ+o‚Äã(Œµ).\begin{split}&\mathbb{E}\_{t,x}\left[\int\_{t}^{t+\varepsilon}\left\{(f-\frac{\gamma}{2}f^{2})(X\_{s})v+\lambda H(v)\right\}e^{-v(s-t)}ds+h^{\lambda}(t+\varepsilon,X\_{t+\varepsilon})e^{-v\varepsilon}\right]\\ =&h^{\lambda}(t,x)+\bigg((\partial\_{t}+\mathcal{L})h^{\lambda}(t,x)+v(f-\frac{\gamma}{2}f^{2}-h^{\lambda})(t,x)+\lambda H(v)\bigg)\varepsilon+o(\varepsilon).\end{split} |  |

Combining these estimations together, we get that

|  |  |  |
| --- | --- | --- |
|  | JŒª‚Äã(t,x;œÄŒµ,v)=hŒª(t,x)+Œ≥2(gŒª)2(t,x)+((‚àÇt+‚Ñí)hŒª(t,x)+Œ≥gŒª(t,x)(‚àÇt+‚Ñí)gŒª(t,x)+v(f‚àíŒ≥2f2‚àíhŒª+Œ≥gŒªf‚àíŒ≥(gŒª)2)(t,x)+ŒªH(v))Œµ+o(Œµ).\begin{split}J^{\lambda}(t,x;\pi^{\varepsilon,v})=&h^{\lambda}(t,x)+\frac{\gamma}{2}(g^{\lambda})^{2}(t,x)+\bigg((\partial\_{t}+\mathcal{L})h^{\lambda}(t,x)+\gamma g^{\lambda}(t,x)(\partial\_{t}+\mathcal{L})g^{\lambda}(t,x)\\ &+v(f-\frac{\gamma}{2}f^{2}-h^{\lambda}+\gamma g^{\lambda}f-\gamma(g^{\lambda})^{2})(t,x)+\lambda H(v)\bigg)\varepsilon+o(\varepsilon).\end{split} |  |

From the equations satisfied by gŒªg^{\lambda} and hŒªh^{\lambda}, also noting that JŒª‚Äã(t,x;œÄ‚àó)=hŒª‚Äã(t,x)+Œ≥2‚Äã(gŒª)2‚Äã(t,x)J^{\lambda}(t,x;\pi^{\*})=h^{\lambda}(t,x)+\frac{\gamma}{2}(g^{\lambda})^{2}(t,x), we derive that

|  |  |  |
| --- | --- | --- |
|  | limŒµ‚Üí0JŒª‚Äã(t,x;œÄ‚àó)‚àíJŒª‚Äã(t,x;œÄŒµ,v)Œµ=(v‚àíœÄ‚àó‚Äã(t,x))‚Äã(f‚àíŒ≥2‚Äãf2‚àíhŒª+Œ≥‚ÄãgŒª‚Äãf‚àíŒ≥‚Äã(gŒª)2)‚Äã(t,x)+Œª‚Äã(H‚Äã(v)‚àíH‚Äã(œÄ‚àó‚Äã(t,x))).\begin{split}&\mathop{\text{lim}}\_{\varepsilon\rightarrow 0}\frac{J^{\lambda}(t,x;\pi^{\*})-J^{\lambda}(t,x;\pi^{\varepsilon,v})}{\varepsilon}\\ =&(v-\pi^{\*}(t,x))(f-\frac{\gamma}{2}f^{2}-h^{\lambda}+\gamma g^{\lambda}f-\gamma(g^{\lambda})^{2})(t,x)+\lambda(H(v)-H(\pi^{\*}(t,x))).\end{split} |  |

Hence, œÄ‚àó\pi^{\*} is an equilibrium strategy if and only if

|  |  |  |
| --- | --- | --- |
|  | œÄ‚àó‚Äã(t,x)‚ààargmaxv‚Äãv‚Äã(f‚àíŒ≥2‚Äãf2‚àíhŒª‚àíŒ≥‚Äã(gŒª)2+Œ≥‚ÄãgŒª‚Äãf)‚Äã(t,x)+Œª‚ÄãH‚Äã(v),\pi^{\*}(t,x)\in\text{argmax}\_{v}v(f-\frac{\gamma}{2}f^{2}-h^{\lambda}-\gamma(g^{\lambda})^{2}+\gamma g^{\lambda}f)(t,x)+\lambda H(v), |  |

which implies that œÄ‚àó\pi^{\*} satisfies the optimality condition ([4](https://arxiv.org/html/2510.24128v1#S3.E4 "In Theorem 3.1. ‚Ä£ 3 Extended HJB Equation for Regularized Problem ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")).
‚àé

To our best knowledge, it is still a hard open problem to prove existence and/or uniqueness for solutions of an extended HJB system with a general assumption. In linear-quadratic mean-variance problem, one can reduce it to an ODE system and obtain a solution, see [[5](https://arxiv.org/html/2510.24128v1#bib.bib5)].
For the solvability of ([3](https://arxiv.org/html/2510.24128v1#S3.E3 "In Theorem 3.1. ‚Ä£ 3 Extended HJB Equation for Regularized Problem ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")), we give an existence result with additional technical assumptions on the coefficients and a small time interval.

###### Theorem 3.2.

In addition to Assumption [2.1](https://arxiv.org/html/2510.24128v1#S2.Thmassumption1 "Assumption 2.1. ‚Ä£ 2 Mean Variance Stopping and its Relaxed Problem ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method"), we further assume that the coefficients bb,œÉ\sigma and ff are uniformly bounded. Then, for a sufficiently small TT, ([3](https://arxiv.org/html/2510.24128v1#S3.E3 "In Theorem 3.1. ‚Ä£ 3 Extended HJB Equation for Regularized Problem ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")) admits a classical solution (VŒª,gŒª)(V^{\lambda},g^{\lambda}).

###### Proof.

The solution is to be considered as a fixed point of a contraction mapping. For that purpose,
denote by ùïÇ\mathbb{K} the Banach space C‚Äã([0,T],C1‚Äã(‚Ñùd))C([0,T],C^{1}(\mathbb{R}^{d})) equipped with the norm ‚Äñl‚ÄñùïÇ=supt,x|l‚Äã(t,x)|+supt,x|‚àÇxl‚Äã(t,x)|\|l\|\_{\mathbb{K}}=\sup\_{t,x}|l(t,x)|+\sup\_{t,x}|\partial\_{x}l(t,x)|. For any l‚ààùïÇl\in\mathbb{K}, define a mapping FF as k=F‚Äã(l)k=F(l) is the solution of the following system

|  |  |  |  |
| --- | --- | --- | --- |
|  | {‚àÇtv+‚Ñí‚Äãv+Œª‚Äãexp‚Å°(‚àív+Œ≥2‚Äã(f‚àíl)2‚àífŒª)‚àíŒ≥‚Äã|œÉ‚Äã‚àÇxl|2=0,v‚Äã(T,x)=f,‚àÇtk+‚Ñí‚Äãk‚àíexp‚Å°(‚àív+Œ≥2‚Äã(f‚àíl)2‚àífŒª)‚Äã(k‚àíf)=0,k‚Äã(T,x)=f.\left\{\begin{split}&\partial\_{t}v+\mathcal{L}v+\lambda\exp(-\frac{v+\frac{\gamma}{2}(f-l)^{2}-f}{\lambda})-\gamma|\sigma\partial\_{x}l|^{2}=0,v(T,x)=f,\\ &\partial\_{t}k+\mathcal{L}k-\exp(-\frac{v+\frac{\gamma}{2}(f-l)^{2}-f}{\lambda})(k-f)=0,k(T,x)=f.\end{split}\right. |  | (5) |

Let Bm‚Äã(0)B\_{m}(0) be the ball in ùïÇ\mathbb{K} centered at 0 with radius m=‚Äñf‚Äñ‚àû+‚Äñ‚àÇxf‚Äñ‚àû+1m=\|f\|\_{\infty}+\|\partial\_{x}f\|\_{\infty}+1. We are to show that, for a sufficiently small TT, FF is a contraction from Bm‚Äã(0)B\_{m}(0) into itself and, thus, admits a fixed point, which would be the solution of ([3](https://arxiv.org/html/2510.24128v1#S3.E3 "In Theorem 3.1. ‚Ä£ 3 Extended HJB Equation for Regularized Problem ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")).

The proof consists of several steps:

Step 11. Well-posedness of first equation. Choose any N>0N>0, let ŒæN\xi\_{N} be a smooth cutoff function such that ŒæN‚Äã(x)=x\xi\_{N}(x)=x, for x‚â§Nx\leq N; and ŒæN‚Äã(x)=N+1\xi\_{N}(x)=N+1, for x‚â•N+1x\geq N+1. Consider the following equation

|  |  |  |
| --- | --- | --- |
|  | ‚àÇtv+‚Ñí‚Äãv+Œª‚Äãexp‚Å°(ŒæN‚Äã(‚àív+Œ≥2‚Äã(f‚àíl)2‚àífŒª))‚àíŒ≥‚Äã|œÉ‚Äã‚àÇxl|2=0,v‚Äã(T,x)=f.\partial\_{t}v+\mathcal{L}v+\lambda\exp(\xi\_{N}(-\frac{v+\frac{\gamma}{2}(f-l)^{2}-f}{\lambda}))-\gamma|\sigma\partial\_{x}l|^{2}=0,v(T,x)=f. |  |

Noting that the third term is a bounded Lipschitz function of vv, it admits a solution vv. Lemma [B.1](https://arxiv.org/html/2510.24128v1#A2.Thmlemma1 "Lemma B.1. ‚Ä£ Appendix B A Key Lemma ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method") yields that

|  |  |  |
| --- | --- | --- |
|  | ‚àíC‚Äã(‚Äñf‚Äñ‚àû+Œª‚Äãexp‚Å°(N+1Œª))‚â§v‚â§C‚Äã(T‚Äãm2+‚Äñf‚Äñ‚àû).-C(\|f\|\_{\infty}+\lambda\exp(\frac{N+1}{\lambda}))\leq v\leq C(Tm^{2}+\|f\|\_{\infty}). |  |

Let us give a refined lower bound estimation independent of NN, which implies that vv solves the first equation in ([5](https://arxiv.org/html/2510.24128v1#S3.E5 "In 3 Extended HJB Equation for Regularized Problem ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")) when NN is sufficiently large.
Denote by œà‚Äã(x)=1+|x|2\psi(x)=\sqrt{1+|x|^{2}}. One can compute that

|  |  |  |
| --- | --- | --- |
|  | Dx‚Äãœà=x1+|x|2‚Äã¬†and¬†‚ÄãDx2‚Äãœà=11+|x|2‚ÄãI‚àí1(1+|x|2)12‚Äãx‚äóx,D\_{x}\psi=\frac{x}{\sqrt{1+|x|^{2}}}\text{ and }D^{2}\_{x}\psi=\frac{1}{\sqrt{1+|x|^{2}}}I-\frac{1}{\left(1+|x|^{2}\right)^{\frac{1}{2}}}x\otimes x, |  |

and, thus, ‚Ñí‚Äãœà‚â§C\mathcal{L}\psi\leq C. Since vv is a bounded function, it holds that, for any Œµ>0\varepsilon>0, v+Œµ‚Äãœàv+\varepsilon\psi attains a minimum at some point (t‚àó,x‚àó)(t^{\*},x^{\*}). If t‚àó=Tt^{\*}=T, then the terminal condition implies that

|  |  |  |
| --- | --- | --- |
|  | v‚â•‚àí‚Äñf‚Äñ‚àû‚àíŒµ‚Äãœà.v\geq-\|f\|\_{\infty}-\varepsilon\psi. |  |

If t‚àó<Tt^{\*}<T, it holds that, at (t‚àó,x‚àó)(t^{\*},x^{\*}),

|  |  |  |
| --- | --- | --- |
|  | Dx2‚Äãv‚â•‚àíŒµ‚ÄãDx2‚Äãœà,Dx‚Äãv=‚àíŒµ‚ÄãDx‚Äãœà,¬†and¬†‚Äã‚àÇtv‚â•0.D^{2}\_{x}v\geq-\varepsilon D^{2}\_{x}\psi,D\_{x}v=-\varepsilon D\_{x}\psi,\text{ and }\partial\_{t}v\geq 0. |  |

This yields that (‚àÇt+‚Ñí)‚Äãv‚Äã(t‚àó,x‚àó)‚â•‚àíŒµ‚Äã‚Ñí‚Äãœà‚â•‚àíC‚ÄãŒµ(\partial\_{t}+\mathcal{L})v(t^{\*},x^{\*})\geq-\varepsilon\mathcal{L}\psi\geq-C\varepsilon. On the other hand, we have

|  |  |  |
| --- | --- | --- |
|  | (‚àÇt+‚Ñí)‚Äãv=Œ≥‚Äã|œÉ‚ÄãDx‚Äãl|2‚àíŒª‚Äãexp‚Å°(ŒæN‚Äã(‚àív+Œ≥2‚Äã(f‚àíl)2‚àífŒª)).(\partial\_{t}+\mathcal{L})v=\gamma|\sigma D\_{x}l|^{2}-\lambda\exp(\xi\_{N}(-\frac{v+\frac{\gamma}{2}(f-l)^{2}-f}{\lambda})). |  |

Combining these two inequalities, we get that

|  |  |  |
| --- | --- | --- |
|  | Œª‚Äãexp‚Å°(ŒæN‚Äã(‚àív+Œ≥2‚Äã(f‚àíl)2‚àífŒª))‚â§Œ≥‚Äã|œÉ‚ÄãDx‚Äãl|2+C‚ÄãŒµ,\lambda\exp(\xi\_{N}(-\frac{v+\frac{\gamma}{2}(f-l)^{2}-f}{\lambda}))\leq\gamma|\sigma D\_{x}l|^{2}+C\varepsilon, |  |

or, equivalently,

|  |  |  |
| --- | --- | --- |
|  | ŒæN‚Äã(f‚àív‚àíŒ≥2‚Äã(f‚àíl)2Œª)‚â§log‚Å°Œ≥‚Äã|œÉ‚Äã‚àÇxl|2+C‚ÄãŒµŒª.\xi\_{N}(\frac{f-v-\frac{\gamma}{2}(f-l)^{2}}{\lambda})\ \leq\log\frac{\gamma|\sigma\partial\_{x}l|^{2}+C\varepsilon}{\lambda}. |  |

This implies that, at (r‚àó,x‚àó)(r^{\*},x^{\*}),

|  |  |  |
| --- | --- | --- |
|  | v‚â•f‚àíŒª‚Äãlog‚Å°Œ≥‚Äã|œÉ‚Äã‚àÇxl|2+C‚ÄãŒµŒª.v\geq f-\lambda\log\frac{\gamma|\sigma\partial\_{x}l|^{2}+C\varepsilon}{\lambda}. |  |

Thus, we have a lower bound estimation

|  |  |  |
| --- | --- | --- |
|  | v‚â•‚àí‚Äñf‚Äñ‚àû+Œª‚Äãlog‚Å°Œ≥‚ÄãC‚Äãm2+C‚ÄãŒµŒª‚àíŒµ‚Äãœà.v\geq-\|f\|\_{\infty}+\lambda\log\frac{\gamma Cm^{2}+C\varepsilon}{\lambda}-\varepsilon\psi. |  |

Letting Œµ\varepsilon go to 0, we finally get that

|  |  |  |
| --- | --- | --- |
|  | v‚â•‚àí‚Äñf‚Äñ‚àû+Œª‚Äãlog‚Å°Œ≥‚ÄãC‚Äãm2Œª.v\geq-\|f\|\_{\infty}+\lambda\log\frac{\gamma Cm^{2}}{\lambda}. |  |

Step 2. Bound and derivative estimations for kk. Note that the second estimation of ([5](https://arxiv.org/html/2510.24128v1#S3.E5 "In 3 Extended HJB Equation for Regularized Problem ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")) is just a linear equation of kk. The well-posedness is straight-forward and one obtains the following bound estimation from Lemma [B.1](https://arxiv.org/html/2510.24128v1#A2.Thmlemma1 "Lemma B.1. ‚Ä£ Appendix B A Key Lemma ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")

|  |  |  |
| --- | --- | --- |
|  | ‚Äñk‚Äñ‚àû‚â§‚Äñf‚Äñ‚àû.\|k\|\_{\infty}\leq\|f\|\_{\infty}. |  |

To give estimation of the derivative, note that, from lower bound estimation for VV in the first step, it holds that

|  |  |  |
| --- | --- | --- |
|  | exp‚Å°(‚àív+Œ≥2‚Äã(f‚àíl)2‚àífŒª)‚â§(Œ≥‚ÄãC‚Äãm2Œª)CŒª‚Äã(1+‚Äñf‚Äñ‚àû2+m2).\exp(-\frac{v+\frac{\gamma}{2}(f-l)^{2}-f}{\lambda})\leq(\frac{\gamma Cm^{2}}{\lambda})^{\frac{C}{\lambda}(1+\|f\|^{2}\_{\infty}+m^{2})}. |  |

Hence, using Lemma [B.1](https://arxiv.org/html/2510.24128v1#A2.Thmlemma1 "Lemma B.1. ‚Ä£ Appendix B A Key Lemma ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method") again, we have that

|  |  |  |
| --- | --- | --- |
|  | ‚Äñ‚àÇxk‚Äñ‚àû‚â§(1+T)‚Äã‚Äñ‚àÇxf‚Äñ‚àû+C‚Äã(T+T)‚Äã(Œ≥‚ÄãC‚Äãm2Œª)CŒª‚Äã(1+‚Äñf‚Äñ‚àû2+m2).\|\partial\_{x}k\|\_{\infty}\leq(1+\sqrt{T})\|\partial\_{x}f\|\_{\infty}+C(\sqrt{T}+T)(\frac{\gamma Cm^{2}}{\lambda})^{\frac{C}{\lambda}(1+\|f\|^{2}\_{\infty}+m^{2})}. |  |

Then, for a sufficiently small TT, FF is mapping from Bm‚Äã(0)B\_{m}(0) into itself.

Step 3. Contraction of the mapping FF.
For any hi‚ààBm‚Äã(0)h\_{i}\in B\_{m}(0) with i=1,2i=1,2, let (vi,ki)(v\_{i},k\_{i}) be the related solutions of ([5](https://arxiv.org/html/2510.24128v1#S3.E5 "In 3 Extended HJB Equation for Regularized Problem ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")). Denote by Œ¥‚Äãv=v1‚àív2\delta v=v\_{1}-v\_{2}, Œ¥‚Äãk=k1‚àík2\delta k=k\_{1}-k\_{2}, and Œ¥‚Äãl=l1‚àíl2\delta l=l\_{1}-l\_{2}. Then, we that (Œ¥‚Äãv,Œ¥‚Äãk)(\delta v,\delta k) satisfy

|  |  |  |  |
| --- | --- | --- | --- |
|  | {(‚àÇt+‚Ñí)‚ÄãŒ¥‚Äãv‚àíŒ¥‚Äãe‚Äã(Œ¥‚Äãv‚àíŒ≥2‚Äã(2‚Äãf‚àíl1‚àíl2)‚ÄãŒ¥‚Äãl)‚àíŒ≥‚Äã(|œÉ‚Äã‚àÇxl1|2‚àí|œÉ‚Äã‚àÇxl2|2)=0,Œ¥‚Äãv‚Äã(T,x)=0,(‚àÇt+‚Ñí)‚ÄãŒ¥‚Äãk‚àíexp‚Å°(‚àív1+Œ≥2‚Äã(f‚àíl1)2‚àífŒª)‚ÄãŒ¥‚Äãk+(k2‚àíf)‚ÄãŒ¥‚Äãe‚ÄãŒ¥‚Äãv‚àíŒ≥2‚Äã(2‚Äãf‚àíl1‚àíl2)‚ÄãŒ¥‚ÄãlŒª=0,Œ¥‚Äãk‚Äã(T,x)=0.\left\{\begin{split}&(\partial\_{t}+\mathcal{L})\delta v-\delta e(\delta v-\frac{\gamma}{2}(2f-l\_{1}-l\_{2})\delta l)-\gamma(|\sigma\partial\_{x}l\_{1}|^{2}-|\sigma\partial\_{x}l\_{2}|^{2})=0,\delta v(T,x)=0,\\ &(\partial\_{t}+\mathcal{L})\delta k-\exp(-\frac{v\_{1}+\frac{\gamma}{2}(f-l\_{1})^{2}-f}{\lambda})\delta k+(k\_{2}-f)\delta e\frac{\delta v-\frac{\gamma}{2}(2f-l\_{1}-l\_{2})\delta l}{\lambda}=0,\delta k(T,x)=0.\end{split}\right. |  | (6) |

with

|  |  |  |
| --- | --- | --- |
|  | Œ¥‚Äãe=‚à´01exp‚Å°(‚àív1+Œ≥2‚Äã(f‚àíl1)2‚àíf+s‚Äã(Œ¥‚Äãv‚àíŒ≥2‚Äã(2‚Äãf‚àíl1‚àíl2)‚ÄãŒ¥‚Äãl)Œª)‚Äãùëës.\delta e=\int\_{0}^{1}\exp(-\frac{v\_{1}+\frac{\gamma}{2}(f-l\_{1})^{2}-f+s(\delta v-\frac{\gamma}{2}(2f-l\_{1}-l\_{2})\delta l)}{\lambda})ds. |  |

From the estimation in previous step, we know that the related functions vi,kiv\_{i},k\_{i} and hih\_{i} are uniformly bounded. Thus, Œ¥‚Äãe\delta e is a bounded function. Moreover, one gets that

|  |  |  |
| --- | --- | --- |
|  | ‚ÄñŒ¥‚Äãe‚ÄãŒ≥2‚Äã(2‚Äãf‚àíl1‚àíl2)‚ÄãŒ¥‚Äãl‚Äñ‚àû‚â§Œ≥2‚Äã‚ÄñŒ¥‚Äãe‚Äã(2‚Äãf‚àíl1‚àíl2)‚Äñ‚àû‚Äã‚ÄñŒ¥‚Äãl‚Äñ‚àû‚â§C‚Äã‚ÄñŒ¥‚Äãl‚Äñ‚àû,\|\delta e\frac{\gamma}{2}(2f-l\_{1}-l\_{2})\delta l\|\_{\infty}\leq\frac{\gamma}{2}\|\delta e(2f-l\_{1}-l\_{2})\|\_{\infty}\|\delta l\|\_{\infty}\leq C\|\delta l\|\_{\infty}, |  |

and

|  |  |  |
| --- | --- | --- |
|  | ‚Äñ|œÉ‚ÄãDx‚Äãl1|2‚àí|œÉ‚ÄãDx‚Äãl2|2‚Äñ‚àû‚â§C‚Äã‚ÄñœÉ‚ÄãDx‚Äãl1+œÉ‚ÄãDx‚Äãl2‚Äñ‚àû‚Äã‚ÄñœÉ‚ÄãDx‚Äãl1‚àíœÉ‚ÄãDx‚Äãl2‚Äñ‚àû‚â§C‚Äã‚ÄñœÉ‚ÄãDx‚Äãl1‚àíœÉ‚ÄãDx‚Äãl2‚Äñ‚àû.\||\sigma D\_{x}l\_{1}|^{2}-|\sigma D\_{x}l\_{2}|^{2}\|\_{\infty}\leq C\|\sigma D\_{x}l\_{1}+\sigma D\_{x}l\_{2}\|\_{\infty}\|\sigma D\_{x}l\_{1}-\sigma D\_{x}l\_{2}\|\_{\infty}\leq C\|\sigma D\_{x}l\_{1}-\sigma D\_{x}l\_{2}\|\_{\infty}. |  |

Then, using Lemma [B.1](https://arxiv.org/html/2510.24128v1#A2.Thmlemma1 "Lemma B.1. ‚Ä£ Appendix B A Key Lemma ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method") again, we have that there exists a constant CC depending on the coefficients, mm and Œª\lambda such that

|  |  |  |
| --- | --- | --- |
|  | ‚à•Œ¥v‚à•‚àû‚â§CT(‚à•Œ¥l‚à•‚àû+‚à•‚àÇxŒ¥l‚à•‚àû.\|\delta v\|\_{\infty}\leq CT(\|\delta l\|\_{\infty}+\|\partial\_{x}\delta l\|\_{\infty}. |  |

Furthermore, with a similar argument,

|  |  |  |
| --- | --- | --- |
|  | ‚ÄñŒ¥‚Äãk‚Äñ‚àû‚â§C‚ÄãT‚Äã‚Äñ(k2‚àíf)‚ÄãŒ¥‚Äãe‚ÄãŒ¥‚Äãv‚àíŒ≥2‚Äã(2‚Äãf‚àíl1‚àíl2)‚ÄãŒ¥‚ÄãlŒª‚Äñ‚àû‚â§C‚ÄãT‚Äã(‚ÄñŒ¥‚Äãv‚Äñ‚àû+‚ÄñŒ¥‚Äãl‚Äñ‚àû).,\|\delta k\|\_{\infty}\leq CT\|(k\_{2}-f)\delta e\frac{\delta v-\frac{\gamma}{2}(2f-l\_{1}-l\_{2})\delta l}{\lambda}\|\_{\infty}\leq CT(\|\delta v\|\_{\infty}+\|\delta l\|\_{\infty})., |  |

and

|  |  |  |
| --- | --- | --- |
|  | ‚Äñ‚àÇxŒ¥‚Äãk‚Äñ‚àû‚â§C‚Äã(T+T)‚Äã(‚Äñ(k2‚àíf)‚ÄãŒ¥‚Äãe‚ÄãŒ¥‚Äãv‚àíŒ≥2‚Äã(2‚Äãf‚àíl1‚àíl2)‚ÄãŒ¥‚ÄãlŒª‚Äñ‚àû+‚Äñexp‚Å°(‚àív1+Œ≥2‚Äã(f‚àíl1)2‚àífŒª)‚ÄãŒ¥‚Äãk‚Äñ‚àû)‚â§C‚Äã(T+T)‚Äã(‚ÄñŒ¥‚Äãv‚Äñ‚àû+‚ÄñŒ¥‚Äãl‚Äñ‚àû+‚ÄñŒ¥‚Äãk‚Äñ‚àû).\begin{split}\|\partial\_{x}\delta k\|\_{\infty}\leq&C(\sqrt{T}+T)(\|(k\_{2}-f)\delta e\frac{\delta v-\frac{\gamma}{2}(2f-l\_{1}-l\_{2})\delta l}{\lambda}\|\_{\infty}+\|\exp(-\frac{v\_{1}+\frac{\gamma}{2}(f-l\_{1})^{2}-f}{\lambda})\delta k\|\_{\infty})\\ \leq&C(\sqrt{T}+T)(\|\delta v\|\_{\infty}+\|\delta l\|\_{\infty}+\|\delta k\|\_{\infty}).\end{split} |  |

Combining these estimations, we see that FF is a contraction for a sufficiently small TT.
‚àé

## 4 Extended HJB Equation for Original Problem

For the optimal stopping problem, i.e., Œ≥=0\gamma=0, [[9](https://arxiv.org/html/2510.24128v1#bib.bib9)] proves that the first equation in ([3](https://arxiv.org/html/2510.24128v1#S3.E3 "In Theorem 3.1. ‚Ä£ 3 Extended HJB Equation for Regularized Problem ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")) converges to the variational inequality satisfied by the value function. For MV stopping problem, we would like follow the same procedure. Unfortunately, at present, we can only formally deduce the limiting equation. Below is a brief introduction. Assume that
(VŒª,gŒª)(V^{\lambda},g^{\lambda}) converge to some functions (V,g)(V,g) when Œª\lambda goes to 0 Then, it is natural to conjecture that

|  |  |  |
| --- | --- | --- |
|  | ‚àÇtV+‚Ñí‚ÄãV‚àíŒ≥‚Äã|œÉ‚Äã‚àÇxg|2=limŒª‚Üí0‚àÇtVŒª+‚Ñí‚ÄãVŒª‚àíŒ≥‚Äã|œÉ‚Äã‚àÇxgŒª|2‚â§0.\partial\_{t}V+\mathcal{L}V-\gamma\left|\sigma\partial\_{x}g\right|^{2}=\lim\_{\lambda\rightarrow 0}\partial\_{t}V^{\lambda}+\mathcal{L}V^{\lambda}-\gamma\left|\sigma\partial\_{x}g^{\lambda}\right|^{2}\leq 0. |  |

If ‚àÇtVŒª+‚Ñí‚ÄãVŒª\partial\_{t}V^{\lambda}+\mathcal{L}V^{\lambda} and ‚àÇxgŒª\partial\_{x}g^{\lambda} are bounded uniformly in Œª\lambda, then so is Œª‚Äãexp‚Å°(‚àíVŒª+Œ≥2‚Äã(f‚àígŒª)2‚àífŒª)\lambda\exp(-\frac{V^{\lambda}+\frac{\gamma}{2}(f-g^{\lambda})^{2}-f}{\lambda}). This suggests that V+Œ≥2‚Äã(f‚àíg)2‚â•fV+\frac{\gamma}{2}(f-g)^{2}\geq f. Moreover, if V+Œ≥2‚Äã(f‚àíg)2>fV+\frac{\gamma}{2}(f-g)^{2}>f, one has exp‚Å°(‚àíVŒª+Œ≥2‚Äã(f‚àígŒª)2‚àífŒª)\exp(-\frac{V^{\lambda}+\frac{\gamma}{2}(f-g^{\lambda})^{2}-f}{\lambda}) converges to 0, which implies ‚àÇtV+‚Ñí‚ÄãV‚àíŒ≥‚Äã|œÉ‚Äã‚àÇxg|2=0\partial\_{t}V+\mathcal{L}V-\gamma\left|\sigma\partial\_{x}g\right|^{2}=0, and ‚àÇtg+‚Ñí‚Äãg=0\partial\_{t}g+\mathcal{L}g=0. From ([3](https://arxiv.org/html/2510.24128v1#S3.E3 "In Theorem 3.1. ‚Ä£ 3 Extended HJB Equation for Regularized Problem ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")), it is not clear what is satisfied by gg on the set {V+Œ≥2‚Äã(f‚àíg)2=f}\{V+\frac{\gamma}{2}(f-g)^{2}=f\}. But, from the probabilistic representation of gŒªg^{\lambda}, one also guess that g‚Äã(t,x)=ùîºt,x‚Äã[f‚Äã(XœÑ)]g(t,x)=\mathbb{E}\_{t,x}[f(X\_{\tau})] for a stopping time œÑ\tau. If œÑ\tau is the hitting time of the set {V+Œ≥2‚Äã(f‚àíg)2=f}\{V+\frac{\gamma}{2}(f-g)^{2}=f\}, Then, gg should equal to ff on that set. In summary, ([3](https://arxiv.org/html/2510.24128v1#S3.E3 "In Theorem 3.1. ‚Ä£ 3 Extended HJB Equation for Regularized Problem ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")) formally converges to the following system

|  |  |  |  |
| --- | --- | --- | --- |
|  | {min‚Å°{‚àí(‚àÇtV+‚Ñí‚ÄãV‚àíŒ≥‚Äã|œÉ‚Äã‚àÇxg|2),V+Œ≥2‚Äã(f‚àíg)2‚àíf}=0,V‚Äã(T,x)=f,‚àÇtg+‚Ñí‚Äãg=0,¬†on¬†{V+Œ≥2‚Äã(f‚àíg)2>f},g=f‚Äã¬†on¬†{V+Œ≥2‚Äã(f‚àíg)2=f},g‚Äã(T,x)=f‚Äã(x).\left\{\begin{split}&\min\left\{-\left(\partial\_{t}V+\mathcal{L}V-\gamma\left|\sigma\partial\_{x}g\right|^{2}\right),V+\frac{\gamma}{2}(f-g)^{2}-f\right\}=0,V(T,x)=f,\\ &\partial\_{t}g+\mathcal{L}g=0,\text{ on $\{V+\frac{\gamma}{2}(f-g)^{2}>f\}$},\\ &g=f\text{ on $\{V+\frac{\gamma}{2}(f-g)^{2}=f\}$},g(T,x)=f(x).\end{split}\right. |  | (7) |

Now, let us assume that the above system admits a pair of solution (V,g)(V,g). The means that (V,g)(V,g) is a pair of continuous functions, second-order continuous differentiable in the region {V+Œ≥2‚Äã(f‚àíg)2>f}\{V+\frac{\gamma}{2}(f-g)^{2}>f\} and satisfies ([7](https://arxiv.org/html/2510.24128v1#S4.E7 "In 4 Extended HJB Equation for Original Problem ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")). Define the set

|  |  |  |
| --- | --- | --- |
|  | ùíû={(t,x)|V+Œ≥2‚Äã(f‚àíg)2>f}.\mathcal{C}=\{(t,x)|V+\frac{\gamma}{2}(f-g)^{2}>f\}. |  |

Construct the stopping time œÑùíû\tau\_{\mathcal{C}} as

|  |  |  |
| --- | --- | --- |
|  | œÑùíû=inf{s‚â•t|(s,Xs)‚àâùíû}.\tau\_{\mathcal{C}}=\inf\{s\geq t|(s,X\_{s})\notin\mathcal{C}\}. |  |

One can verify that

|  |  |  |
| --- | --- | --- |
|  | V‚Äã(t,x)=ùîºt,x‚Äã[f‚Äã(XœÑùíû)]‚àíŒ≥2‚ÄãVart,x‚Äã[f‚Äã(XœÑùíû)],¬†and¬†‚Äãg‚Äã(t,x)=ùîºt,x‚Äã[f‚Äã(XœÑùíû)].V(t,x)=\mathbb{E}\_{t,x}\left[f(X\_{\tau\_{\mathcal{C}}})\right]-\frac{\gamma}{2}\text{Var}\_{t,x}\left[f(X\_{\tau\_{\mathcal{C}}})\right],\text{ and }g(t,x)=\mathbb{E}\_{t,x}\left[f(X\_{\tau\_{\mathcal{C}}})\right]. |  |

Moreover, we also define

|  |  |  |
| --- | --- | --- |
|  | h‚Äã(t,x):=V‚Äã(t,x)‚àíŒ≥2‚Äãg2‚Äã(t,x)=ùîºt,x‚Äã[(f‚àíŒ≥2‚Äãf2)‚Äã(XœÑùíû)].h(t,x):=V(t,x)-\frac{\gamma}{2}g^{2}(t,x)=\mathbb{E}\_{t,x}\left[(f-\frac{\gamma}{2}f^{2})(X\_{\tau\_{\mathcal{C}}})\right]. |  |

Next, we prove that these functions characterize a stopping policy that is an equilibrium in some sense. Before that, let us first introduce some definitions. For any (t,x)‚àà‚Ñù√ó‚Ñùn(t,x)\in\mathbb{R}\times\mathbb{R}^{n} and r>0r>0, the parabolic cylinder Q‚Äã(t,x;r)Q(t,x;r) is defined as333The definition of parabolic cylinders is different from that in text books of parabolic PDEs, see [[13](https://arxiv.org/html/2510.24128v1#bib.bib13)]. The reason is that we consider PDEs with terminal conditions instead of initial conditions.

|  |  |  |
| --- | --- | --- |
|  | Q‚Äã(t,x;r):={(s,y)‚àà‚Ñù√ó‚Ñùn|max‚Å°{|x‚àíy|,(s‚àít)12}‚â§r,s‚â•t}.Q(t,x;r):=\{(s,y)\in\mathbb{R}\times\mathbb{R}^{n}|\max\{|x-y|,(s-t)^{\frac{1}{2}}\}\leq r,s\geq t\}. |  |

For any set Œ©\Omega, the parabolic boundary ùí´‚ÄãŒ©\mathcal{P}\Omega is defined as the set of all points (t,x)‚ààŒ©¬Ø(t,x)\in\bar{\Omega} such that for any Œµ>0\varepsilon>0, Q‚Äã(t,x;Œµ)Q(t,x;\varepsilon) contains points not in Œ©\Omega. Finally, note that, since ùíû\mathcal{C} is a open set, for any (t,x)‚ààùíû(t,x)\in\mathcal{C}, there exists Œµ>0\varepsilon>0 such that Q‚Äã(t,x;Œµ)‚ààùíûQ(t,x;\varepsilon)\in\mathcal{C}.

The notion of equilibrium strategy is similar to that used in [[7](https://arxiv.org/html/2510.24128v1#bib.bib7)].
For any v‚â•0v\geq 0, let NvN^{v} be a Poisson point process independent of the Brownian motion WW. Its first jump time after tt is denoted as œÑv\tau^{v}, i.e.,

|  |  |  |
| --- | --- | --- |
|  | œÑv=inf{s‚â•t|Ns‚àíNt=1}.\tau^{v}=\inf\left\{s\geq t|N\_{s}-N\_{t}=1\right\}. |  |

For any Œµ\varepsilon, define two stopping time

|  |  |  |
| --- | --- | --- |
|  | œÑŒµ=inf{s‚â•t||Xs‚àíXt|‚â•Œµ}‚àß(t+Œµ)‚àßT,\tau^{\varepsilon}=\inf\left\{s\geq t||X\_{s}-X\_{t}|\geq\varepsilon\right\}\wedge(t+\varepsilon)\wedge T, |  |

and

|  |  |  |
| --- | --- | --- |
|  | œÑ~ùíû=inf{s‚â•œÑŒµ|(s,Xs)‚àâùíû}.\tilde{\tau}\_{\mathcal{C}}=\inf\left\{s\geq\tau^{\varepsilon}|(s,X\_{s})\notin\mathcal{C}\right\}. |  |

The perturbation œÑùíûŒµ,v\tau^{\varepsilon,v}\_{\mathcal{C}} of œÑùíû\tau\_{\mathcal{C}} is defined as,

|  |  |  |
| --- | --- | --- |
|  | œÑùíûŒµ,v=1{œÑv‚â§œÑŒµ}‚ÄãœÑv+1{œÑv>œÑŒµ}‚ÄãœÑ~ùíû.\tau\_{\mathcal{C}}^{\varepsilon,v}=1\_{\{\tau^{v}\leq\tau^{\varepsilon}\}}\tau^{v}+1\_{\{\tau^{v}>\tau^{\varepsilon}\}}\tilde{\tau}\_{\mathcal{C}}. |  |

###### Theorem 4.1.

Assume that there exists a solution (V,g)(V,g) of ([7](https://arxiv.org/html/2510.24128v1#S4.E7 "In 4 Extended HJB Equation for Original Problem ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")) such that the functions and their derivatives are polynomial-growth w.r.t. xx uniformly in tt. For any (t,x)‚ààùíû‚à™(ùíûc/ùí´‚Äãùíûc)(t,x)\in\mathcal{C}\cup(\mathcal{C}^{c}/\mathcal{P}\mathcal{C}^{c}), it holds that

|  |  |  |
| --- | --- | --- |
|  | lim infŒµ‚Üí0J‚Äã(t,x;œÑùíû)‚àíJ‚Äã(t,x;œÑùíûŒµ,v)ùîºt,x‚Äã[œÑŒµ‚àít]‚â•0.\liminf\_{\varepsilon\to 0}\frac{J(t,x;\tau\_{\mathcal{C}})-J(t,x;\tau\_{\mathcal{C}}^{\varepsilon,v})}{\mathbb{E}\_{t,x}[\tau^{\varepsilon}-t]}\geq 0. |  |

###### Proof.

From the definition of œÑùíûŒµ,v\tau\_{\mathcal{C}}^{\varepsilon,v}, it holds that

|  |  |  |
| --- | --- | --- |
|  | ùîºt,x‚Äã[f‚Äã(XœÑùíûŒµ,v)]=ùîºt,x‚Äã[‚à´tœÑŒµf‚Äã(Xs)‚Äãv‚Äãe‚àív‚Äã(s‚àít)‚Äãùëës+f‚Äã(XœÑ~ùíû)‚Äãe‚àív‚Äã(œÑŒµ‚àít)]=ùîºt,x‚Äã[‚à´tœÑŒµf‚Äã(Xs)‚Äãv‚Äãe‚àív‚Äã(s‚àít)‚Äãùëës+g‚Äã(œÑŒµ,XœÑŒµ)‚Äãe‚àív‚Äã(œÑŒµ‚àít)].\begin{split}&\mathbb{E}\_{t,x}\left[f(X\_{\tau\_{\mathcal{C}}^{\varepsilon,v}})\right]\\ =&\mathbb{E}\_{t,x}\left[\int\_{t}^{\tau^{\varepsilon}}f(X\_{s})ve^{-v(s-t)}ds+f(X\_{\tilde{\tau}\_{\mathcal{C}}})e^{-v(\tau^{\varepsilon}-t)}\right]\\ =&\mathbb{E}\_{t,x}\left[\int\_{t}^{\tau^{\varepsilon}}f(X\_{s})ve^{-v(s-t)}ds+g(\tau^{\varepsilon},X\_{\tau^{\varepsilon}})e^{-v(\tau^{\varepsilon}-t)}\right].\end{split} |  |

Note that, for sufficiently small Œµ\varepsilon, XsX\_{s} stays in ùíû\mathcal{C} or ùíûc\mathcal{C}^{c} for s‚àà[t,œÑŒµ]s\in[t,\tau^{\varepsilon}] as we assume that (t,x)‚ààùíû‚à™(ùíûc/ùí´‚Äãùíûc)(t,x)\in\mathcal{C}\cup(\mathcal{C}^{c}/\mathcal{P}\mathcal{C}^{c}). Thus, one can apply It√¥ formula to get that

|  |  |  |
| --- | --- | --- |
|  | ùîºt,x‚Äã[g‚Äã(œÑŒµ,XœÑŒµ)‚Äãe‚àív‚ÄãœÑŒµ]=g‚Äã(t,x)+ùîº‚Äã[‚à´tœÑŒµ(‚àÇt+‚Ñí)‚Äãg‚Äã(s,Xs)‚Äãe‚àív‚Äã(s‚àít)‚àív‚Äãg‚Äã(s,Xs)‚Äãe‚àív‚Äã(s‚àít)‚Äãd‚Äãs].\begin{split}&\mathbb{E}\_{t,x}\left[g(\tau^{\varepsilon},X\_{\tau^{\varepsilon}})e^{-v\tau^{\varepsilon}}\right]\\ =&g(t,x)+\mathbb{E}\left[\int\_{t}^{\tau^{\varepsilon}}(\partial\_{t}+\mathcal{L})g(s,X\_{s})e^{-v(s-t)}-vg(s,X\_{s})e^{-v(s-t)}ds\right].\end{split} |  |

Then, we have

|  |  |  |
| --- | --- | --- |
|  | ùîºt,x‚Äã[f‚Äã(XœÑùíûŒµ,v)]=g‚Äã(t,x)+((‚àÇt+‚Ñí)‚Äãg‚Äã(t,x)+v‚Äã(f‚àíg)‚Äã(t,x))‚Äãùîº‚Äã[œÑŒµ‚àít]+o‚Äã(ùîº‚Äã[œÑŒµ‚àít]),\begin{split}&\mathbb{E}\_{t,x}\left[f(X\_{\tau^{\varepsilon,v}\_{\mathcal{C}}})\right]\\ =&g(t,x)+\left((\partial\_{t}+\mathcal{L})g(t,x)+v(f-g)(t,x)\right)\mathbb{E}\left[\tau^{\varepsilon}-t\right]+o(\mathbb{E}\left[\tau^{\varepsilon}-t\right]),\end{split} |  |

which implies that

|  |  |  |
| --- | --- | --- |
|  | (ùîºt,x‚Äã[f‚Äã(XœÑùíûŒµ,v)])2=g2‚Äã(t,x)+2‚Äãg‚Äã(t,x)‚Äã((‚àÇt+‚Ñí)‚Äãg‚Äã(t,x)+v‚Äã(f‚àíg)‚Äã(t,x))‚Äãùîº‚Äã[œÑŒµ‚àít]+o‚Äã(ùîº‚Äã[œÑŒµ‚àít]).\begin{split}&\left(\mathbb{E}\_{t,x}\left[f(X\_{\tau^{\varepsilon,v}\_{\mathcal{C}}})\right]\right)^{2}\\ =&g^{2}(t,x)+2g(t,x)\left((\partial\_{t}+\mathcal{L})g(t,x)+v(f-g)(t,x)\right)\mathbb{E}\left[\tau^{\varepsilon}-t\right]+o(\mathbb{E}\left[\tau^{\varepsilon}-t\right]).\end{split} |  |

Similarly, it holds that

|  |  |  |
| --- | --- | --- |
|  | ùîºt,x‚Äã[(f‚àíŒ≥2‚Äãf2)‚Äã(XœÑùíûŒµ,v)]=h‚Äã(t,x)+((‚àÇt+‚Ñí)‚Äã(h)+v‚Äã(f‚àíŒ≥2‚Äãf2‚àíh))‚Äãùîº‚Äã[œÑŒµ‚àít]+o‚Äã(ùîº‚Äã[œÑŒµ‚àít]).\begin{split}&\mathbb{E}\_{t,x}\left[(f-\frac{\gamma}{2}f^{2})(X\_{\tau^{\varepsilon,v}\_{\mathcal{C}}})\right]\\ =&h(t,x)+\left((\partial\_{t}+\mathcal{L})(h)+v(f-\frac{\gamma}{2}f^{2}-h)\right)\mathbb{E}\left[\tau^{\varepsilon}-t\right]+o(\mathbb{E}\left[\tau^{\varepsilon}-t\right]).\end{split} |  |

Hence, recalling that h=V‚àíŒ≥2‚Äãg2h=V-\frac{\gamma}{2}g^{2},

|  |  |  |
| --- | --- | --- |
|  | J‚Äã(t,x;œÑùíûŒµ,v)‚àíJ‚Äã(t,x;œÑùíû)=((‚àÇt+‚Ñí)‚Äãh+Œ≥‚Äãg‚Äã(‚àÇt+‚Ñí)‚Äãg+v‚Äã(f‚àí(V+Œ≥2‚Äã(f‚àíg)2)))‚Äãùîº‚Äã[œÑŒµ‚àít]+o‚Äã(ùîº‚Äã[œÑŒµ‚àít])=((‚àÇt+‚Ñí)‚ÄãV‚àíŒ≥‚Äã|œÉ‚Äã‚àÇxg|2+v‚Äã(f‚àí(V+Œ≥2‚Äã(f‚àíg)2)))‚Äãùîº‚Äã[œÑŒµ‚àít]+o‚Äã(ùîº‚Äã[œÑŒµ‚àít]).\begin{split}&J(t,x;\tau\_{\mathcal{C}}^{\varepsilon,v})-J(t,x;\tau\_{\mathcal{C}})\\ =&((\partial\_{t}+\mathcal{L})h+\gamma g(\partial\_{t}+\mathcal{L})g+v(f-(V+\frac{\gamma}{2}(f-g)^{2})))\mathbb{E}[\tau^{\varepsilon}-t]+o(\mathbb{E}[\tau^{\varepsilon}-t])\\ =&((\partial\_{t}+\mathcal{L})V-\gamma|\sigma\partial\_{x}g|^{2}+v(f-(V+\frac{\gamma}{2}(f-g)^{2})))\mathbb{E}[\tau^{\varepsilon}-t]+o(\mathbb{E}[\tau^{\varepsilon}-t]).\end{split} |  |

The first equation in ([7](https://arxiv.org/html/2510.24128v1#S4.E7 "In 4 Extended HJB Equation for Original Problem ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")) implies the desired result.
‚àé

For the case (t,x)‚ààùí´‚Äãùíûc\{T}√ó‚Ñù(t,x)\in\mathcal{P}\mathcal{C}^{c}\backslash\{T\}\times\mathbb{R}, it turns out to be a very subtle problem. Thus, we focus on one dimensional case, i.e., x‚àà‚Ñùx\in\mathbb{R} and assume that the free boundary ùí´‚Äãùíûc\{T}√ó‚Ñù\mathcal{P}\mathcal{C}^{c}\backslash\{T\}\times\mathbb{R} is locally Lipschitz continuous with respect to time tt. More precisely, there exists a small ball Q‚Äã(t,x;r)Q(t,x;r) and a Lipschitz continuous curve cc and r>0r>0 such that ùíû‚Äã‚ãÇQ‚Äã(t,x;r)={(s,y)|t‚â§s‚â§t+r12,|y‚àíx|‚â§r,y‚â•c‚Äã(s)}\mathcal{C}\bigcap Q(t,x;r)=\{(s,y)|t\leq s\leq t+r^{\frac{1}{2}},|y-x|\leq r,y\geq c(s)\}.

###### Theorem 4.2.

In addition to the assumption in Theorem [4.1](https://arxiv.org/html/2510.24128v1#S4.Thmtheorem1 "Theorem 4.1. ‚Ä£ 4 Extended HJB Equation for Original Problem ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method") and locally Lipschitz assumption on the free boundary, we further assume
that VV is C1C^{1} in Q‚Äã(t,x;r)Q(t,x;r). If it holds that, for (t,x)‚ààùí´‚Äãùíûc\{T}√ó‚Ñù(t,x)\in\mathcal{P}\mathcal{C}^{c}\backslash\{T\}\times\mathbb{R},

|  |  |  |  |
| --- | --- | --- | --- |
|  | (‚àÇt+‚Ñí)‚ÄãV‚Äã(t,x+)+(‚àÇt+‚Ñí)‚ÄãV‚Äã(t,x‚àí)‚â§Œ≥‚ÄãœÉ2‚Äã(t,x)‚Äã(‚àÇxg‚Äã(t,x+)+‚àÇxg‚Äã(t,x‚àí)2)2,(\partial\_{t}+\mathcal{L})V(t,x+)+(\partial\_{t}+\mathcal{L})V(t,x-)\leq\gamma\sigma^{2}(t,x)\left(\frac{\partial\_{x}g(t,x+)+\partial\_{x}g(t,x-)}{2}\right)^{2}, |  | (8) |

then we have

|  |  |  |
| --- | --- | --- |
|  | lim infŒµ‚Üí0J‚Äã(t,x;œÑùíû)‚àíJ‚Äã(t,x;œÑùíûŒµ,v)ùîºt,x‚Äã[œÑŒµ‚àít]‚â•0.\liminf\_{\varepsilon\rightarrow 0}\frac{J(t,x;\tau\_{\mathcal{C}})-J(t,x;\tau\_{\mathcal{C}}^{\varepsilon,v})}{\mathbb{E}\_{t,x}[\tau^{\varepsilon}-t]}\geq 0. |  |

###### Proof.

From It√¥-Tanaka formula (see [[15](https://arxiv.org/html/2510.24128v1#bib.bib15)]), one can get that

|  |  |  |
| --- | --- | --- |
|  | g(œÑŒµ,XœÑŒµ)e‚àí(œÑŒµ‚àít)=g(t,x)+‚à´tœÑŒµ12((‚àÇt+‚Ñí)g(s,Xs+)e‚àív‚Äã(s‚àít)‚àívg(s,Xs+)e‚àív‚Äã(s‚àít)+(‚àÇt+‚Ñí)g(s,Xs‚àí)e‚àív‚Äã(s‚àít)‚àívg(s,Xs‚àí)e‚àív‚Äã(s‚àít))1{Xs‚â†c‚Äã(s)}ds+‚à´tœÑŒµ12‚ÄãœÉ‚Äã(‚àÇxg‚Äã(s,Xs+)+‚àÇxg‚Äã(s,Xs‚àí))‚Äãe‚àív‚Äã(t‚àís)‚ÄãùëëWs+12‚Äã‚à´tœÑŒµ(‚àÇxg‚Äã(s,Xs+)‚àí‚àÇxg‚Äã(s,Xs‚àí))‚Äãe‚àív‚Äã(s‚àít)‚Äã1{Xs=c‚Äã(s)}‚Äãùëëlsc,\begin{split}&g(\tau^{\varepsilon},X\_{\tau^{\varepsilon}})e^{-(\tau^{\varepsilon}-t)}=g(t,x)+\int\_{t}^{\tau^{\varepsilon}}\frac{1}{2}\bigg((\partial\_{t}+\mathcal{L})g(s,X\_{s}+)e^{-v(s-t)}-vg(s,X\_{s}+)e^{-v(s-t)}\\ &+(\partial\_{t}+\mathcal{L})g(s,X\_{s}-)e^{-v(s-t)}-vg(s,X\_{s}-)e^{-v(s-t)}\bigg)1\_{\{X\_{s}\neq c(s)\}}ds\\ &+\int\_{t}^{\tau^{\varepsilon}}\frac{1}{2}\sigma(\partial\_{x}g(s,X\_{s}+)+\partial\_{x}g(s,X\_{s}-))e^{-v(t-s)}dW\_{s}\\ &+\frac{1}{2}\int\_{t}^{\tau^{\varepsilon}}(\partial\_{x}g(s,X\_{s}+)-\partial\_{x}g(s,X\_{s}-))e^{-v(s-t)}1\_{\{X\_{s}=c(s)\}}dl^{c}\_{s},\end{split} |  |

where lscl^{c}\_{s} is the local time of XX at the curve cc. Then, we have that

|  |  |  |
| --- | --- | --- |
|  | ùîºt,x[‚à´tœÑŒµ12((‚àÇt+‚Ñí)g(s,Xs+)e‚àív‚Äã(s‚àít)‚àívg(s,Xs+)e‚àív‚Äã(s‚àít)+(‚àÇt+‚Ñí)g(s,Xs‚àí)e‚àív‚Äã(s‚àít)‚àívg(s,Xs‚àí)e‚àív‚Äã(s‚àít))1{Xs‚â†c‚Äã(s)}ds]=12(((‚àÇt+‚Ñí)g(t,x+)+(‚àÇt+‚Ñí)g(t,x‚àí)‚àí2vg(t,x))ùîº[œÑŒµ‚àít]+o(ùîº[œÑŒµ‚àít]).\begin{split}&\mathbb{E}\_{t,x}\bigg[\int\_{t}^{\tau^{\varepsilon}}\frac{1}{2}\bigg((\partial\_{t}+\mathcal{L})g(s,X\_{s}+)e^{-v(s-t)}-vg(s,X\_{s}+)e^{-v(s-t)}\\ &+(\partial\_{t}+\mathcal{L})g(s,X\_{s}-)e^{-v(s-t)}-vg(s,X\_{s}-)e^{-v(s-t)}\bigg)1\_{\{X\_{s}\neq c(s)\}}ds\bigg]\\ =&\frac{1}{2}\left(((\partial\_{t}+\mathcal{L})g(t,x+)+(\partial\_{t}+\mathcal{L})g(t,x-)-2vg(t,x)\right)\mathbb{E}[\tau^{\varepsilon}-t]+o(\mathbb{E}[\tau^{\varepsilon}-t]).\end{split} |  |

It is proved in Appendix [A](https://arxiv.org/html/2510.24128v1#A1 "Appendix A Proof of (9) ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method") that

|  |  |  |  |
| --- | --- | --- | --- |
|  | (ùîºt,x‚Äã[lœÑŒµc‚àíltc])2=œÉ2‚Äã(t,x)‚Äãùîºt,x‚Äã[œÑŒµ‚àít]+o‚Äã(ùîºt,x‚Äã[œÑŒµ‚àít]),\left(\mathbb{E}\_{t,x}[l^{c}\_{\tau^{\varepsilon}}-l\_{t}^{c}]\right)^{2}=\sigma^{2}(t,x)\mathbb{E}\_{t,x}[\tau^{\varepsilon}-t]+o(\mathbb{E}\_{t,x}[\tau^{\varepsilon}-t]), |  | (9) |

which further implies that

|  |  |  |  |
| --- | --- | --- | --- |
|  | (ùîºt,x‚Äã[‚à´tœÑŒµ(‚àÇxg‚Äã(s,Xs+)‚àí‚àÇxg‚Äã(s,Xs‚àí))‚Äãe‚àív‚Äã(s‚àít)‚Äã1{Xs=c‚Äã(s)}‚Äãùëëlsc])2=(‚àÇxg‚Äã(t,x+)‚àí‚àÇxg‚Äã(t,x‚àí))2‚ÄãœÉ2‚Äã(t,x)‚Äãùîºt,x‚Äã[œÑŒµ‚àít]+o‚Äã(ùîºt,x‚Äã[œÑŒµ‚àít]).\begin{split}&\left(\mathbb{E}\_{t,x}[\int\_{t}^{\tau^{\varepsilon}}(\partial\_{x}g(s,X\_{s}+)-\partial\_{x}g(s,X\_{s}-))e^{-v(s-t)}1\_{\{X\_{s}=c(s)\}}dl^{c}\_{s}]\right)^{2}\\ =&\left(\partial\_{x}g(t,x+)-\partial\_{x}g(t,x-)\right)^{2}\sigma^{2}(t,x)\mathbb{E}\_{t,x}[\tau^{\varepsilon}-t]+o(\mathbb{E}\_{t,x}[\tau^{\varepsilon}-t]).\end{split} |  | (10) |

Hence,

|  |  |  |
| --- | --- | --- |
|  | (ùîºt,x[e‚àív‚ÄãœÑŒµg(œÑŒµ,XœÑŒµ)])2=g2(t,x)+(((‚àÇt+‚Ñí)g(t,x+)+(‚àÇt+‚Ñí)g(t,x‚àí))g(t,x)‚àí2vg12(t,x)+(‚àÇxg‚Äã(t,x+)‚àí‚àÇxg‚Äã(t,x‚àí)2)2œÉ2(t,x))ùîºt,x[œÑŒµ‚àít]+g‚Äã(t,x)‚Äãùîºt,x‚Äã[‚à´tœÑŒµ(‚àÇxg‚Äã(s,Xs+)‚àí‚àÇxg‚Äã(s,Xs‚àí))‚Äãe‚àív‚Äã(s‚àít)‚Äã1{Xs=c‚Äã(s)}‚Äãùëëlsc]+o‚Äã(ùîºt,x‚Äã[œÑŒµ‚àít]).\begin{split}&(\mathbb{E}\_{t,x}[e^{-v\tau^{\varepsilon}}g(\tau^{\varepsilon},X\_{\tau^{\varepsilon}})])^{2}=g^{2}(t,x)+\bigg(((\partial\_{t}+\mathcal{L})g(t,x+)+(\partial\_{t}+\mathcal{L})g(t,x-))g(t,x)-2vg^{2}\_{1}(t,x)\\ &+\left(\frac{\partial\_{x}g(t,x+)-\partial\_{x}g(t,x-)}{2}\right)^{2}\sigma^{2}(t,x)\bigg)\mathbb{E}\_{t,x}[\tau^{\varepsilon}-t]\\ &+g(t,x)\mathbb{E}\_{t,x}[\int\_{t}^{\tau^{\varepsilon}}(\partial\_{x}g(s,X\_{s}+)-\partial\_{x}g(s,X\_{s}-))e^{-v(s-t)}1\_{\{X\_{s}=c(s)\}}dl^{c}\_{s}]+o(\mathbb{E}\_{t,x}[\tau^{\varepsilon}-t]).\end{split} |  |

Similarly,

|  |  |  |
| --- | --- | --- |
|  | ùîºt,x‚Äã[e‚àív‚ÄãœÑŒµ‚Äãh‚Äã(œÑŒµ,XœÑŒµ)]=h‚Äã(t,x)+12‚Äã((‚àÇt+‚Ñí)‚Äãh‚Äã(t,x+)+(‚àÇt+‚Ñí)‚Äãh‚Äã(t,x‚àí)‚àív‚Äãh‚Äã(t,x))‚Äãùîºt,x‚Äã[œÑŒµ‚àít]+12‚Äãùîºt,x‚Äã[‚à´tœÑŒµ(‚àÇxh‚Äã(s,Xs+)‚àí‚àÇxh‚Äã(s,Xs‚àí))‚Äãe‚àív‚Äã(s‚àít)‚Äã1{Xs=c‚Äã(s)}‚Äãùëëlsc].\begin{split}&\mathbb{E}\_{t,x}[e^{-v\tau^{\varepsilon}}h(\tau^{\varepsilon},X\_{\tau^{\varepsilon}})]\\ =&h(t,x)+\frac{1}{2}\left((\partial\_{t}+\mathcal{L})h(t,x+)+(\partial\_{t}+\mathcal{L})h(t,x-)-vh(t,x)\right)\mathbb{E}\_{t,x}[\tau^{\varepsilon}-t]\\ &+\frac{1}{2}\mathbb{E}\_{t,x}[\int\_{t}^{\tau^{\varepsilon}}(\partial\_{x}h(s,X\_{s}+)-\partial\_{x}h(s,X\_{s}-))e^{-v(s-t)}1\_{\{X\_{s}=c(s)\}}dl^{c}\_{s}].\end{split} |  |

Since V=h+Œ≥2‚Äãg2V=h+\frac{\gamma}{2}g^{2} is C1C^{1} across cc, it holds that

|  |  |  |
| --- | --- | --- |
|  | ùîºt,x[‚à´tœÑŒµ(‚àÇxh(s,Xs+)‚àí‚àÇxh(s,Xs‚àí))e‚àív‚Äã(s‚àít)1{Xs=c‚Äã(s)}dlsc+Œ≥‚Äãg‚Äã(t,x)‚Äãùîºt,x‚Äã[‚à´tœÑŒµ(‚àÇxg‚Äã(s,Xs+)‚àí‚àÇxg‚Äã(s,Xs‚àí))‚Äãe‚àív‚Äã(s‚àít)‚Äã1{Xs=c‚Äã(s)}‚Äãùëëlsc]=o‚Äã(ùîºt,x‚Äã[œÑŒµ‚àít]).\begin{split}&\mathbb{E}\_{t,x}[\int\_{t}^{\tau^{\varepsilon}}(\partial\_{x}h(s,X\_{s}+)-\partial\_{x}h(s,X\_{s}-))e^{-v(s-t)}1\_{\{X\_{s}=c(s)\}}dl^{c}\_{s}\\ &+\gamma g(t,x)\mathbb{E}\_{t,x}[\int\_{t}^{\tau^{\varepsilon}}(\partial\_{x}g(s,X\_{s}+)-\partial\_{x}g(s,X\_{s}-))e^{-v(s-t)}1\_{\{X\_{s}=c(s)\}}dl^{c}\_{s}]=o(\mathbb{E}\_{t,x}[\tau^{\varepsilon}-t]).\end{split} |  |

Then,

|  |  |  |
| --- | --- | --- |
|  | J‚Äã(t,x;œÑùíûŒµ,v)‚àíJ‚Äã(t,x;œÑùíû)ùîºt,x‚Äã[œÑŒµ‚àít]=12‚Äã((‚àÇt+‚Ñí)‚Äã(V‚àíŒ≥2‚Äãg2)‚Äã(t,x+)+Œ≥‚Äãg‚Äã(‚àÇt+‚Ñí)‚Äãg‚Äã(t,x+))+12‚Äã((‚àÇt+‚Ñí)‚Äã(V‚àíŒ≥2‚Äãg2)‚Äã(t,x‚àí)+Œ≥‚Äãg‚Äã(‚àÇt+‚Ñí)‚Äãg‚Äã(t,x‚àí))+Œ≥2‚Äã(‚àÇxg‚Äã(t,x+)‚àí‚àÇxg‚Äã(t,x‚àí)2)2‚ÄãœÉ2‚Äã(t,x)+o‚Äã(1)=12((‚àÇt+‚Ñí)V(t,x+)‚àíŒ≥|œÉ‚àÇxg|2(t,x+)+(‚àÇt+‚Ñí)V(t,x‚àí)‚àíŒ≥|œÉ‚àÇxg|2(t,x‚àí))+Œ≥2‚Äã(‚àÇxg‚Äã(t,x+)‚àí‚àÇxg‚Äã(t,x‚àí)2)2‚ÄãœÉ2‚Äã(t,x)+o‚Äã(1),\begin{split}&\frac{J(t,x;\tau\_{\mathcal{C}}^{\varepsilon,v})-J(t,x;\tau\_{\mathcal{C}})}{\mathbb{E}\_{t,x}[\tau^{\varepsilon}-t]}\\ =&\frac{1}{2}\left((\partial\_{t}+\mathcal{L})(V-\frac{\gamma}{2}g^{2})(t,x+)+\gamma g(\partial\_{t}+\mathcal{L})g(t,x+)\right)\\ &+\frac{1}{2}\left((\partial\_{t}+\mathcal{L})(V-\frac{\gamma}{2}g^{2})(t,x-)+\gamma g(\partial\_{t}+\mathcal{L})g(t,x-)\right)\\ &+\frac{\gamma}{2}\left(\frac{\partial\_{x}g(t,x+)-\partial\_{x}g(t,x-)}{2}\right)^{2}\sigma^{2}(t,x)+o(1)\\ =&\frac{1}{2}\bigg((\partial\_{t}+\mathcal{L})V(t,x+)-\gamma|\sigma\partial\_{x}g|^{2}(t,x+)\\ &+(\partial\_{t}+\mathcal{L})V(t,x-)-\gamma|\sigma\partial\_{x}g|^{2}(t,x-)\bigg)\\ &+\frac{\gamma}{2}\left(\frac{\partial\_{x}g(t,x+)-\partial\_{x}g(t,x-)}{2}\right)^{2}\sigma^{2}(t,x)+o(1),\\ \end{split} |  |

which gives the desired result according the assumption of the theorem.
‚àé

###### Remark 4.1.

It is a well-known result in the theory of optimal stopping that the value function is C1C^{1} across the free boundary. However, one can not expect that gg is also C1C^{1}, see the example given in Subsection [5.1](https://arxiv.org/html/2510.24128v1#S5.SS1 "5.1 Infinite Horizon Case ‚Ä£ 5 Further Discussions ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method"). Note that a similar condition is also given in [[7](https://arxiv.org/html/2510.24128v1#bib.bib7)].

## 5 Further Discussions

### 5.1 Infinite Horizon Case

In this subsection, we consider an infinite horizon example that one can give an explicit solution. Let XX be a geometric Brownian motion

|  |  |  |
| --- | --- | --- |
|  | d‚ÄãXt=Œº‚ÄãXt‚Äãd‚Äãt+œÉ‚ÄãXt‚Äãd‚ÄãWt.dX\_{t}=\mu X\_{t}dt+\sigma X\_{t}dW\_{t}. |  |

Consider the MV stopping problem

|  |  |  |
| --- | --- | --- |
|  | J‚Äã(x;œÑ)=ùîºx‚Äã[XœÑ]‚àíŒ≥2‚ÄãVarx‚Äã[XœÑ].J(x;\tau)=\mathbb{E}\_{x}\left[X\_{\tau}\right]-\frac{\gamma}{2}\text{Var}\_{x}\left[X\_{\tau}\right]. |  |

For this infinite horizon case, one can have an elliptic system similar to ([7](https://arxiv.org/html/2510.24128v1#S4.E7 "In 4 Extended HJB Equation for Original Problem ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method"))

|  |  |  |  |
| --- | --- | --- | --- |
|  | {min‚Å°{‚àí(‚Ñí‚Äãv‚àíŒ≥2‚Äã|œÉ‚Äãx‚Äã‚àÇxg|2),v+Œ≥2‚Äã(f‚àíg)2‚àíf}=0,‚Ñí‚Äãg=0,¬†on¬†{v+Œ≥2‚Äã(f‚àíg)2>f},g=f‚Äã¬†on¬†{v+Œ≥2‚Äã(f‚àíg)2=f},\left\{\begin{split}&\min\left\{-\left(\mathcal{L}v-\frac{\gamma}{2}\left|\sigma x\partial\_{x}g\right|^{2}\right),v+\frac{\gamma}{2}(f-g)^{2}-f\right\}=0,\\ &\mathcal{L}g=0,\text{ on $\{v+\frac{\gamma}{2}(f-g)^{2}>f\}$},\\ &g=f\text{ on $\{v+\frac{\gamma}{2}(f-g)^{2}=f\}$},\end{split}\right. |  | (11) |

with ‚Ñí=12‚ÄãœÉ2‚Äãx2‚Äã‚àÇ2‚àÇx2+Œº‚Äãx‚Äã‚àÇ‚àÇx\mathcal{L}=\frac{1}{2}\sigma^{2}x^{2}\frac{\partial^{2}}{\partial x^{2}}+\mu x\frac{\partial}{\partial x}. Denote by œÅ=2‚ÄãŒºœÉ2\rho=\frac{2\mu}{\sigma^{2}} and assume that œÅ‚àà(0,1/2)\rho\in(0,1/2). Let b=2‚ÄãœÅŒ≥‚Äã(1‚àíœÅ)b=\frac{2\rho}{\gamma(1-\rho)}. Let us check that

|  |  |  |
| --- | --- | --- |
|  | V(x)={(1‚àíŒ≥2‚Äãb)‚ÄãbœÅ‚Äãx1‚àíœÅ+Œ≥2‚Äãb2‚ÄãœÅ‚Äãx2‚àí2‚ÄãœÅ,¬†for¬†x<b,x,¬†for¬†x‚â•b,V(x)=\left\{\begin{split}&(1-\frac{\gamma}{2}b)b^{\rho}x^{1-\rho}+\frac{\gamma}{2}b^{2\rho}x^{2-2\rho},\text{ for $x<b$},\\ &x,\text{ for $x\geq b$},\end{split}\right. |  |

and

|  |  |  |
| --- | --- | --- |
|  | g(x)={bœÅ‚Äãx1‚àíœÅ,¬†for¬†x<b,x,¬†for¬†x‚â•b,g(x)=\left\{\begin{split}&b^{\rho}x^{1-\rho},\text{ for $x<b$},\\ &x,\text{ for $x\geq b$},\end{split}\right. |  |

satisfy ([11](https://arxiv.org/html/2510.24128v1#S5.E11 "In 5.1 Infinite Horizon Case ‚Ä£ 5 Further Discussions ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")). For x<bx<b, it holds that

|  |  |  |
| --- | --- | --- |
|  | ‚àÇxV=(1‚àíœÅ)‚Äã(1‚àíŒ≥2‚Äãb)‚ÄãbœÅ‚Äãx‚àíœÅ+Œ≥‚Äã(1‚àíœÅ)‚Äãb2‚ÄãœÅ‚Äãx1‚àí2‚ÄãœÅ,\partial\_{x}V=(1-\rho)(1-\frac{\gamma}{2}b)b^{\rho}x^{-\rho}+\gamma(1-\rho)b^{2\rho}x^{1-2\rho}, |  |

|  |  |  |
| --- | --- | --- |
|  | ‚àÇx‚ÄãxV=‚àíœÅ‚Äã(1‚àíœÅ)‚Äã(1‚àíŒ≥2‚Äãb)‚ÄãbœÅ‚Äãx‚àíœÅ‚àí1+Œ≥‚Äã(1‚àíœÅ)‚Äã(1‚àí2‚ÄãœÅ)‚Äãb2‚ÄãœÅ‚Äãx‚àí2‚ÄãœÅ,\partial\_{xx}V=-\rho(1-\rho)(1-\frac{\gamma}{2}b)b^{\rho}x^{-\rho-1}+\gamma(1-\rho)(1-2\rho)b^{2\rho}x^{-2\rho}, |  |

and ‚àÇxg=(1‚àíœÅ‚ÄãbœÅ‚Äãx‚àíœÅ)\partial\_{x}g=(1-\rho b^{\rho}x^{-\rho}). Thus, it is easy to check that, for x<bx<b, ‚Ñí‚ÄãV‚àíŒ≥2‚Äã|œÉ‚Äãx‚Äã‚àÇxg|2=0\mathcal{L}V-\frac{\gamma}{2}\left|\sigma x\partial\_{x}g\right|^{2}=0 and ‚Ñí‚Äãg=0\mathcal{L}g=0. When x‚â•bx\geq b, it is easy to see that ‚àÇxV=‚àÇxg=1\partial\_{x}V=\partial\_{x}g=1 and ‚àÇx‚ÄãxV=0\partial\_{xx}V=0. Hence,

|  |  |  |
| --- | --- | --- |
|  | ‚Ñí‚ÄãV‚àíŒ≥2‚Äã|œÉ‚Äãx‚Äã‚àÇxg|2=Œº‚Äãx‚àíŒ≥2‚ÄãœÉ2‚Äãx2=Œ≥2‚ÄãœÉ2‚Äãx‚Äã(œÅŒ≥‚àíx)‚â§0.\mathcal{L}V-\frac{\gamma}{2}\left|\sigma x\partial\_{x}g\right|^{2}=\mu x-\frac{\gamma}{2}\sigma^{2}x^{2}=\frac{\gamma}{2}\sigma^{2}x(\frac{\rho}{\gamma}-x)\leq 0. |  |

Now let us verify that V+Œ≥2‚Äã(x‚àíg)2>xV+\frac{\gamma}{2}(x-g)^{2}>x for x<bx<b. Direct computation yields that

|  |  |  |
| --- | --- | --- |
|  | V+Œ≥2‚Äã(x‚àíg)2=(1‚àíŒ≥2‚Äãb)‚ÄãbœÅ‚Äãx1‚àíœÅ+Œ≥2‚Äãb2‚ÄãœÅ‚Äãx2‚àí2‚ÄãœÅ+Œ≥2‚Äã(x‚àíbœÅ‚Äãx1‚àíœÅ)2=Œ≥2‚Äãx2+Œ≥‚Äãb2‚ÄãœÅ‚Äãx2‚àí2‚ÄãœÅ‚àíŒ≥‚ÄãbœÅ‚Äãx2‚àíœÅ+(1‚àíŒ≥2‚Äãb)‚ÄãbœÅ‚Äãx1‚àíœÅ=x‚Äã(Œ≥2‚Äãx+Œ≥‚Äãb2‚ÄãœÅ‚Äãx1‚àí2‚ÄãœÅ‚àíŒ≥‚ÄãbœÅ‚Äãx1‚àíœÅ+(1‚àíŒ≥2‚Äãb)‚ÄãbœÅ‚Äãx‚àíœÅ).\begin{split}&V+\frac{\gamma}{2}(x-g)^{2}\\ =&(1-\frac{\gamma}{2}b)b^{\rho}x^{1-\rho}+\frac{\gamma}{2}b^{2\rho}x^{2-2\rho}+\frac{\gamma}{2}(x-b^{\rho}x^{1-\rho})^{2}\\ =&\frac{\gamma}{2}x^{2}+\gamma b^{2\rho}x^{2-2\rho}-\gamma b^{\rho}x^{2-\rho}+(1-\frac{\gamma}{2}b)b^{\rho}x^{1-\rho}\\ =&x\left(\frac{\gamma}{2}x+\gamma b^{2\rho}x^{1-2\rho}-\gamma b^{\rho}x^{1-\rho}+(1-\frac{\gamma}{2}b)b^{\rho}x^{-\rho}\right).\end{split} |  |

Thus, we have to show that Œ≥2‚Äãx+Œ≥‚Äãb2‚ÄãœÅ‚Äãx1‚àí2‚ÄãœÅ‚àíŒ≥‚ÄãbœÅ‚Äãx1‚àíœÅ+(1‚àíŒ≥2‚Äãb)‚ÄãbœÅ‚Äãx‚àíœÅ>1\frac{\gamma}{2}x+\gamma b^{2\rho}x^{1-2\rho}-\gamma b^{\rho}x^{1-\rho}+(1-\frac{\gamma}{2}b)b^{\rho}x^{-\rho}>1 for x<bx<b. For that purpose, define a function Œ∫‚Äã(z)=Œ≥2‚Äãb‚Äãz2‚ÄãœÅ‚àí1+(1‚àíŒ≥2‚Äãb)‚ÄãzœÅ\kappa(z)=\frac{\gamma}{2}bz^{2\rho-1}+(1-\frac{\gamma}{2}b)z^{\rho}. Let us find its infimum on [1,‚àû)[1,\infty). Taking derivative with respect to zz, we see that Œ∫‚Ä≤‚Äã(z)=zœÅ‚àí1‚Äã((2‚ÄãœÅ‚àí1)‚ÄãŒ≥2‚Äãb‚ÄãzœÅ‚àí1+(1‚àíŒ≥2‚Äãb)‚ÄãœÅ)\kappa^{\prime}(z)=z^{\rho-1}((2\rho-1)\frac{\gamma}{2}bz^{\rho-1}+(1-\frac{\gamma}{2}b)\rho). Since œÅ‚â§12\rho\leq\frac{1}{2} and z‚â•1z\geq 1, it holds that

|  |  |  |
| --- | --- | --- |
|  | (2‚ÄãœÅ‚àí1)‚ÄãŒ≥2‚Äãb‚ÄãzœÅ‚àí1+(1‚àíŒ≥2‚Äãb)‚ÄãœÅ‚â•(2‚ÄãœÅ‚àí1)‚ÄãŒ≥2‚Äãb+(1‚àíŒ≥2‚Äãb)‚ÄãœÅ=0,(2\rho-1)\frac{\gamma}{2}bz^{\rho-1}+(1-\frac{\gamma}{2}b)\rho\geq(2\rho-1)\frac{\gamma}{2}b+(1-\frac{\gamma}{2}b)\rho=0, |  |

where we use the fact that b=2‚ÄãœÅŒ≥‚Äã(1‚àíœÅ)b=\frac{2\rho}{\gamma(1-\rho)}.
This implies that Œ∫\kappa is strictly increasing on [1,‚àû)[1,\infty) and taking minimum at z=1z=1, which equals to 11. Then, for x<bx<b,

|  |  |  |
| --- | --- | --- |
|  | Œ≥2‚Äãx+Œ≥‚Äãb2‚ÄãœÅ‚Äãx1‚àí2‚ÄãœÅ‚àíŒ≥‚ÄãbœÅ‚Äãx1‚àíœÅ+(1‚àíŒ≥2‚Äãb)‚ÄãbœÅ‚Äãx‚àíœÅ=Œ≥2‚Äãx‚Äã(1‚àíbœÅ‚Äãx‚àíœÅ)2+Œ≥2‚Äãb2‚ÄãœÅ‚Äãx1‚àí2‚ÄãœÅ+(1‚àíŒ≥2‚Äãb)‚ÄãbœÅ‚Äãx‚àíœÅ‚â•Œ∫‚Äã(b‚Äãx‚àí1)>1.\begin{split}&\frac{\gamma}{2}x+\gamma b^{2\rho}x^{1-2\rho}-\gamma b^{\rho}x^{1-\rho}+(1-\frac{\gamma}{2}b)b^{\rho}x^{-\rho}\\ =&\frac{\gamma}{2}x(1-b^{\rho}x^{-\rho})^{2}+\frac{\gamma}{2}b^{2\rho}x^{1-2\rho}+(1-\frac{\gamma}{2}b)b^{\rho}x^{-\rho}\\ \geq&\kappa(bx^{-1})>1.\end{split} |  |

Thus, (V,g)(V,g) is a solution of ([11](https://arxiv.org/html/2510.24128v1#S5.E11 "In 5.1 Infinite Horizon Case ‚Ä£ 5 Further Discussions ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")). On x=bx=b, we can check that

|  |  |  |
| --- | --- | --- |
|  | ‚Ñí‚ÄãV‚Äã(b‚àí)=Œ≥2‚ÄãœÉ2‚Äãb2‚Äã(1‚àíœÅ)2,‚Ñí‚ÄãV‚Äã(b+)=Œº‚Äãb,‚àÇxg‚Äã(b‚àí)=1‚àíœÅ,¬†and¬†‚Äã‚àÇxg‚Äã(b+)=1.\mathcal{L}V(b-)=\frac{\gamma}{2}\sigma^{2}b^{2}(1-\rho)^{2},\mathcal{L}V(b+)=\mu b,\partial\_{x}g(b-)=1-\rho,\text{ and }\partial\_{x}g(b+)=1. |  |

Then,

|  |  |  |
| --- | --- | --- |
|  | ‚Ñí‚ÄãV‚Äã(b‚àí)+‚Ñí‚ÄãV‚Äã(b+)=Œ≥2‚ÄãœÉ2‚Äãb2‚Äã(1‚àíœÅ)2+Œº‚Äãb=œÉ2‚Äãb‚Äã(Œ≥2‚Äãb‚Äã(1‚àíœÅ)2+œÅ)=œÉ2‚Äãb‚ÄãœÅ‚Äã(2‚àíœÅ),\mathcal{L}V(b-)+\mathcal{L}V(b+)=\frac{\gamma}{2}\sigma^{2}b^{2}(1-\rho)^{2}+\mu b=\sigma^{2}b(\frac{\gamma}{2}b(1-\rho)^{2}+\rho)=\sigma^{2}b\rho(2-\rho), |  |

and

|  |  |  |
| --- | --- | --- |
|  | Œ≥‚ÄãœÉ2‚Äãb2‚Äã(1+1‚àíœÅ2)2=œÉ2‚Äãb‚Äã(2‚àíœÅ)‚Äã(2‚àíœÅ)‚ÄãŒ≥‚Äãb4=œÉ2‚Äãb‚Äã(2‚àíœÅ)‚ÄãœÅ‚Äã2‚àíœÅ2‚àí2‚ÄãœÅ>œÉ2‚Äãb‚Äã(2‚àíœÅ)‚ÄãœÅ.\gamma\sigma^{2}b^{2}\left(\frac{1+1-\rho}{2}\right)^{2}=\sigma^{2}b(2-\rho)\frac{(2-\rho)\gamma b}{4}=\sigma^{2}b(2-\rho)\rho\frac{2-\rho}{2-2\rho}>\sigma^{2}b(2-\rho)\rho. |  |

Thus, the stopping time is an equilibrium.

Moreover, one can also compute that V‚Ä≤‚Äã(b+)=V‚Ä≤‚Äã(b‚àí)V^{\prime}(b+)=V^{\prime}(b-), g1‚Ä≤‚Äã(b‚àí)=1‚àíœÅg^{\prime}\_{1}(b-)=1-\rho and g1‚Ä≤‚Äã(b+)=1g^{\prime}\_{1}(b+)=1. This suggests that one can expect VV to be C1C^{1}, but gg not C1C^{1}, so one can not assume that gg is C1C^{1} across the free boundary.

### 5.2 Discrete Time Approximation

For many time-inconsistent problems, the equilibrium solution in continuous time is regarded as the limit of its counterpart in discrete-time models. This is the logical structure of the derivation for extended HJB equation, see [[4](https://arxiv.org/html/2510.24128v1#bib.bib4)]. For that reason, we would like to check that whether one can get the same equation by considering the discrete time MV stopping problem.

Let Œî‚Äãt=TN\Delta t=\frac{T}{N} and tk=k‚ÄãŒî‚Äãtt\_{k}=k\Delta t for k=0,1,‚Ä¶,Nk=0,1,...,N. We assume that one can only stop at these time points. Recursively define a sequence of stopping times {œÑi}i=0,1,‚Ä¶,N\{\tau\_{i}\}\_{i=0,1,...,N} as the follows. Set œÑN=tN\tau\_{N}=t\_{N}, U‚Äã(N,x)=f‚Äã(x)U(N,x)=f(x), and V‚Äã(N,x)=f‚Äã(x)V(N,x)=f(x). For i=N‚àí1,N‚àí2,‚Ä¶,0i=N-1,N-2,...,0, define

|  |  |  |
| --- | --- | --- |
|  | U‚Äã(i,x):=ùîºti,x‚Äã[f‚Äã(XœÑi+1)]‚àíŒ≥2‚ÄãVarti,x‚Äã[f‚Äã(XœÑi+1)],U(i,x):=\mathbb{E}\_{t\_{i},x}[f(X\_{\tau\_{i+1}})]-\frac{\gamma}{2}\text{Var}\_{t\_{i},x}[f(X\_{\tau\_{i+1}})], |  |

|  |  |  |
| --- | --- | --- |
|  | œÑi:={ti,¬†if¬†f‚Äã(Xti)‚â•Ui‚Äã(Xti),œÑi+1,if¬†f‚Äã(Xti)<Ui‚Äã(Xti),\tau\_{i}:=\left\{\begin{matrix}t\_{i},\text{ if $f(X\_{t\_{i}})\geq U\_{i}(X\_{t\_{i}})$,}\\ \tau\_{i+1},\text{if $f(X\_{t\_{i}})<U\_{i}(X\_{t\_{i}})$,}\end{matrix}\right. |  |

and

|  |  |  |
| --- | --- | --- |
|  | V‚Äã(i,x):=ùîºti,x‚Äã[f‚Äã(XœÑi)]‚àíŒ≥2‚ÄãVarti,x‚Äã[f‚Äã(XœÑi)].V(i,x):=\mathbb{E}\_{t\_{i},x}[f(X\_{\tau\_{i}})]-\frac{\gamma}{2}\text{Var}\_{t\_{i},x}[f(X\_{\tau\_{i}})]. |  |

Moreover, we also define g‚Äã(i,x):=ùîºti,x‚Äã[f‚Äã(XœÑi)]g(i,x):=\mathbb{E}\_{t\_{i},x}[f(X\_{\tau\_{i}})]. The motivation of these definitions are the following. We view the MV stopping problem from a game-theoretic perspective as a non-cooperative game. We have one player at each time point tnt\_{n}, who can only choose the stopping decision at tnt\_{n}. The stopping time œÑi\tau\_{i} represent the time the process being stopped conditioned on its has not been stopped before tit\_{i}. Player nn has two options: to stop immediately or to continue. The reward of the first option is f‚Äã(Xti)f(X\_{t\_{i}}). Choosing to continue, the process is stopped at time œÑi+1\tau\_{i+1} and the expected reward is given by U‚Äã(i,Xti)U(i,X\_{t\_{i}}). Thus, the strategy of player nn is to decide to stop when f‚Äã(Xti)‚â•U‚Äã(i,Xti)f(X\_{t\_{i}})\geq U(i,X\_{t\_{i}}).

Now let us check the equation satisfied by ViV\_{i}. It is easy to see that V‚Äã(i,x)‚â•f‚Äã(x)V(i,x)\geq f(x). If V‚Äã(i,x)>f‚Äã(x)V(i,x)>f(x), it implies that œÑi=œÑi+1\tau\_{i}=\tau\_{i+1}. In this case, it holds that

|  |  |  |
| --- | --- | --- |
|  | V‚Äã(i,x)=U‚Äã(i,x)=ùîºti,x‚Äã[V‚Äã(i+1,Xti+1)]‚àíŒ≥2‚ÄãVarti,x‚Äã[g‚Äã(i+1,Xti+1)].V(i,x)=U(i,x)=\mathbb{E}\_{t\_{i},x}[V(i+1,X\_{t\_{i+1}})]-\frac{\gamma}{2}\text{Var}\_{t\_{i},x}[g(i+1,X\_{t\_{i+1}})]. |  |

Combining two situations, we have

|  |  |  |
| --- | --- | --- |
|  | min‚Å°{V‚Äã(i,x)‚àí(ùîºti,x‚Äã[V‚Äã(i+1,Xti+1)]‚àíŒ≥2‚ÄãVarti,x‚Äã[g‚Äã(i+1,Xti+1)]),V‚Äã(i,x)‚àíf‚Äã(x)}=0.\min\left\{V(i,x)-\left(\mathbb{E}\_{t\_{i},x}[V(i+1,X\_{t\_{i+1}})]-\frac{\gamma}{2}\text{Var}\_{t\_{i},x}[g(i+1,X\_{t\_{i+1}})]\right),V(i,x)-f(x)\right\}=0. |  |

Now we let Œî‚Äãt\Delta t go to zero. Formally, one see that

|  |  |  |
| --- | --- | --- |
|  | ùîºti,x‚Äã[V‚Äã(i+1,Xti+1)]‚àíV‚Äã(i,x)=(‚àÇtV+‚Ñí‚ÄãV)‚Äã(ti,x)‚ÄãŒî‚Äãt+o‚Äã(Œî‚Äãt),\mathbb{E}\_{t\_{i},x}[V(i+1,X\_{t\_{i+1}})]-V(i,x)=(\partial\_{t}V+\mathcal{L}V)(t\_{i},x)\Delta t+o(\Delta t), |  |

and

|  |  |  |
| --- | --- | --- |
|  | Varti,x‚Äã[g‚Äã(i+1,Xti+1)]=ùîºti,x‚Äã[g2‚Äã(i+1,Xti+1)]‚àí(ùîºti,x‚Äã[g‚Äã(i+1,Xti+1)])2=g2‚Äã(i,x)+(‚àÇt+‚Ñí)‚Äãg2‚ÄãŒî‚Äãt‚àí(g‚Äã(i,x)+(‚àÇt+‚Ñí)‚Äãg‚ÄãŒî‚Äãt)2‚Äão‚Äã(Œî‚Äãt)=(‚àÇt+‚Ñí)‚Äãg2‚ÄãŒî‚Äãt‚àíg‚Äã(‚àÇt+‚Ñí)‚Äãg‚ÄãŒî‚Äãt+o‚Äã(Œî‚Äãt)=|œÉ‚Äã‚àÇxg|2‚ÄãŒî‚Äãt+o‚Äã(Œî‚Äãt).\begin{split}&\text{Var}\_{t\_{i},x}[g(i+1,X\_{t\_{i+1}})]=\mathbb{E}\_{t\_{i},x}[g^{2}(i+1,X\_{t\_{i+1}})]-\left(\mathbb{E}\_{t\_{i},x}[g(i+1,X\_{t\_{i+1}})]\right)^{2}\\ =&g^{2}(i,x)+(\partial\_{t}+\mathcal{L})g^{2}\Delta t-\left(g(i,x)+(\partial\_{t}+\mathcal{L})g\Delta t\right)^{2}o(\Delta t)\\ =&(\partial\_{t}+\mathcal{L})g^{2}\Delta t-g(\partial\_{t}+\mathcal{L})g\Delta t+o(\Delta t)\\ =&|\sigma\partial\_{x}g|^{2}\Delta t+o(\Delta t).\end{split} |  |

Hence, we get the following system

|  |  |  |  |
| --- | --- | --- | --- |
|  | {min‚Å°{‚àí(‚àÇtV+‚Ñí‚ÄãV‚àíŒ≥‚Äã|œÉ‚Äã‚àÇxg|2),V‚àíf}=0,V‚Äã(T,x)=f,‚àÇtg+‚Ñí‚Äãg=0,¬†on¬†{V>f},g=f‚Äã¬†on¬†{V=f},g‚Äã(T,x)=f‚Äã(x).\left\{\begin{split}&\min\left\{-\left(\partial\_{t}V+\mathcal{L}V-\gamma\left|\sigma\partial\_{x}g\right|^{2}\right),V-f\right\}=0,V(T,x)=f,\\ &\partial\_{t}g+\mathcal{L}g=0,\text{ on $\{V>f\}$},\\ &g=f\text{ on $\{V=f\}$},g(T,x)=f(x).\end{split}\right. |  | (12) |

Note that this equation is different from ([7](https://arxiv.org/html/2510.24128v1#S4.E7 "In 4 Extended HJB Equation for Original Problem ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")) with the condition V+Œ≥2‚Äã(f‚àíg)2‚â•fV+\frac{\gamma}{2}(f-g)^{2}\geq f replaced by V‚â•fV\geq f. Clearly V‚â•fV\geq f implies V+Œ≥2‚Äã(f‚àíg)2‚â•fV+\frac{\gamma}{2}(f-g)^{2}\geq f. Moreover, on {V=f}\{V=f\}, we also have g=fg=f and hence, V+Œ≥2‚Äã(f‚àíg)2=fV+\frac{\gamma}{2}(f-g)^{2}=f. Thus, the solution of the above system also satisfies
([7](https://arxiv.org/html/2510.24128v1#S4.E7 "In 4 Extended HJB Equation for Original Problem ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")). In this sense, it seems that ([7](https://arxiv.org/html/2510.24128v1#S4.E7 "In 4 Extended HJB Equation for Original Problem ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")) is a more general equation. However, it remains unclear whether there exists a solution that satisfies ([7](https://arxiv.org/html/2510.24128v1#S4.E7 "In 4 Extended HJB Equation for Original Problem ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")), but do not satisfy ([12](https://arxiv.org/html/2510.24128v1#S5.E12 "In 5.2 Discrete Time Approximation ‚Ä£ 5 Further Discussions ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")).

### 5.3 General Time-inconsistent Problems

To explain the additional quadratic term in the condition V+Œ≥2‚Äã(f‚àíg)2‚â•fV+\frac{\gamma}{2}(f-g)^{2}\geq f of ([7](https://arxiv.org/html/2510.24128v1#S4.E7 "In 4 Extended HJB Equation for Original Problem ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")), let us consider a more general time-inconsistent problem in which the player choose stopping time to maximize the following functional

|  |  |  |
| --- | --- | --- |
|  | ùîºt,x‚Äã[f‚Äã(XœÑ)]+G‚Äã(ùîºt,x‚Äã[k‚Äã(XœÑ)]).\mathbb{E}\_{t,x}[f(X\_{\tau})]+G(\mathbb{E}\_{t,x}[k(X\_{\tau})]). |  |

We just give some formal arguments for illustration. First, we consider the regularized problem. For an equilibrium œÄ‚àó\pi^{\*}, define g‚Äã(t,x)=ùîºt,x‚Äã[k‚Äã(XœÑ)]g(t,x)=\mathbb{E}\_{t,x}[k(X\_{\tau})] and h‚Äã(t,x)=ùîºt,x‚Äã[f‚Äã(XœÑ)+‚à´0œÑŒª‚ÄãH‚Äã(œÄs‚àó)‚Äãùëës]h(t,x)=\mathbb{E}\_{t,x}[f(X\_{\tau})+\int\_{0}^{\tau}\lambda H(\pi^{\*}\_{s})ds]. Then, it holds that (g,h)(g,h) solves

|  |  |  |
| --- | --- | --- |
|  | {(‚àÇt+‚Ñí)‚Äãh+Œª‚ÄãH‚Äã(œÄ‚àó)+œÄ‚àó‚Äã(f‚àíh)=0,g2‚Äã(T)=f,(‚àÇt+‚Ñí)‚Äãg+œÄ‚àó‚Äã(k‚àíg)=0,g‚Äã(T)=k.\left\{\begin{split}&(\partial\_{t}+\mathcal{L})h+\lambda H(\pi^{\*})+\pi^{\*}(f-h)=0,g\_{2}(T)=f,\\ &(\partial\_{t}+\mathcal{L})g+\pi^{\*}(k-g)=0,g(T)=k.\end{split}\right. |  |

For a purterbed strategy œÄŒµ,v\pi^{\varepsilon,v}, one can get that

|  |  |  |
| --- | --- | --- |
|  | ùîºt,x‚Äã[k‚Äã(XœÄŒµ,v)]=g‚Äã(t,x)+((‚àÇt+‚Ñí)‚Äãg+v‚Äã(k‚àíg))‚ÄãŒµ+o‚Äã(Œµ),\mathbb{E}\_{t,x}[k(X\_{\pi^{\varepsilon,v}})]=g(t,x)+\left((\partial\_{t}+\mathcal{L})g+v(k-g)\right)\varepsilon+o(\varepsilon), |  |

and

|  |  |  |
| --- | --- | --- |
|  | ùîºt,x‚Äã[f‚Äã(XœÄŒµ,v)+‚à´0œÑŒµ,vŒª‚ÄãH‚Äã(œÄsŒµ,v)‚Äãùëës]=h‚Äã(t,x)+((‚àÇt+‚Ñí)‚Äãh+v‚Äã(f‚àíh)+Œª‚ÄãH‚Äã(v))‚ÄãŒµ+o‚Äã(Œµ).\begin{split}&\mathbb{E}\_{t,x}[f(X\_{\pi^{\varepsilon,v}})+\int\_{0}^{\tau^{\varepsilon,v}}\lambda H(\pi^{\varepsilon,v}\_{s})ds]\\ =&h(t,x)+\left((\partial\_{t}+\mathcal{L})h+v(f-h)+\lambda H(v)\right)\varepsilon+o(\varepsilon).\end{split} |  |

Moreover, it holds that

|  |  |  |
| --- | --- | --- |
|  | G‚Äã(ùîºt,x‚Äã[k‚Äã(XœÄŒµ,v)])=G‚Äã(g)+G‚Ä≤‚Äã(g)‚Äã((‚àÇt+‚Ñí)‚Äãg+v‚Äã(k‚àíg))‚ÄãŒµ+o‚Äã(Œµ).G(\mathbb{E}\_{t,x}[k(X\_{\pi^{\varepsilon,v}})])=G(g)+G^{\prime}(g)\left((\partial\_{t}+\mathcal{L})g+v(k-g)\right)\varepsilon+o(\varepsilon). |  |

Then, one can show that œÄ‚àó\pi^{\*} is an equilibrium if and only if

|  |  |  |
| --- | --- | --- |
|  | œÄ‚àó=exp‚Å°(‚àíh+G‚Ä≤‚Äã(g)‚Äãg‚àíf‚àíG‚Ä≤‚Äã(g)‚ÄãkŒª).\pi^{\*}=\exp(-\frac{h+G^{\prime}(g)g-f-G^{\prime}(g)k}{\lambda}). |  |

Note that the value function V=h+G‚Äã(g)V=h+G(g) and satisfies

|  |  |  |
| --- | --- | --- |
|  | (‚àÇt+‚Ñí)‚ÄãV+Œª‚Äãexp‚Å°(‚àíh+G‚Ä≤‚Äã(g)‚Äãg‚àíf‚àíG‚Ä≤‚Äã(g)‚ÄãkŒª)+‚ÑãG‚Äã(g)=0,(\partial\_{t}+\mathcal{L})V+\lambda\exp(-\frac{h+G^{\prime}(g)g-f-G^{\prime}(g)k}{\lambda})+\mathcal{H}\_{G}(g)=0, |  |

where the operator ‚ÑãG\mathcal{H}\_{G} is defined as

|  |  |  |
| --- | --- | --- |
|  | ‚ÑãG‚Äã(œÜ)=G‚Ä≤‚Äã(œÜ)‚Äã‚Ñí‚ÄãœÜ‚àí‚Ñí‚ÄãG‚Äã(œÜ).\mathcal{H}\_{G}(\varphi)=G^{\prime}(\varphi)\mathcal{L}\varphi-\mathcal{L}G(\varphi). |  |

We also see that

|  |  |  |
| --- | --- | --- |
|  | h+G‚Ä≤‚Äã(g)‚Äãg‚àíf‚àíG‚Ä≤‚Äã(g)‚Äãk=V‚àí(f+G‚Äã(k))+G‚Äã(k)‚àíG‚Äã(g)‚àíG‚Ä≤‚Äã(g)‚Äã(k‚àíg).h+G^{\prime}(g)g-f-G^{\prime}(g)k=V-(f+G(k))+G(k)-G(g)-G^{\prime}(g)(k-g). |  |

Denote by ŒîG‚Äã(k,g)=G‚Äã(k)‚àíG‚Äã(g)‚àíG‚Ä≤‚Äã(g)‚Äã(k‚àíg)\Delta\_{G}(k,g)=G(k)-G(g)-G^{\prime}(g)(k-g). Then, when letting Œª\lambda go to zero, VV should converge to the following variational inequality

|  |  |  |
| --- | --- | --- |
|  | min‚Å°{‚àí((‚àÇt+‚Ñí)‚ÄãV+HG‚Äã(g)),V‚àí(f+G‚Äã(k))+ŒîG‚Äã(k,g)}=0.\min\left\{-\left((\partial\_{t}+\mathcal{L})V+H\_{G}(g)\right),V-(f+G(k))+\Delta\_{G}(k,g)\right\}=0. |  |

Note that the condition is written as V+ŒîG‚Äã(k,g)‚â•(f+G‚Äã(g))V+\Delta\_{G}(k,g)\geq(f+G(g)). For MV case, i.e., G‚Äã(k)=Œ≥2‚Äãk2G(k)=\frac{\gamma}{2}k^{2}, ŒîG‚Äã(k,g)=Œ≥2‚Äã(k‚àíg)2\Delta\_{G}(k,g)=\frac{\gamma}{2}(k-g)^{2}, which is the same condition in ([7](https://arxiv.org/html/2510.24128v1#S4.E7 "In 4 Extended HJB Equation for Original Problem ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")). Moreover, define the set

|  |  |  |
| --- | --- | --- |
|  | ùíû={(t,x)|V+ŒîG‚Äã(k,g)>f+G‚Äã(k)},\mathcal{C}=\{(t,x)|V+\Delta\_{G}(k,g)>f+G(k)\}, |  |

and the stopping time œÑùíû\tau\_{\mathcal{C}} as

|  |  |  |
| --- | --- | --- |
|  | œÑùíû=inf{s>t|(s,Xs)‚àâùíû}.\tau\_{\mathcal{C}}=\inf\{s>t|(s,X\_{s})\notin\mathcal{C}\}. |  |

Then, one can formally verify that œÑùíû\tau\_{\mathcal{C}} in the same sense as that in the statement of Theorem [4.1](https://arxiv.org/html/2510.24128v1#S4.Thmtheorem1 "Theorem 4.1. ‚Ä£ 4 Extended HJB Equation for Original Problem ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method").

From the previous discussion, we see that the appearance of the additional term ŒîG‚Äã(k,g)\Delta\_{G}(k,g) is due to two factors. One is that GG is a non-linear function, which makes the problem time-inconsistent. The other is that, when equilibrium is under consideration, the perturbation lies within the family of relaxed strategies rather than pure strategies. This coincides with the fact in the game theory that pure strategy equilibrium is different from mixed strategy equilibrium.

## 6 Conclusions

This paper systematically investigates the MV optimal stopping problem‚Äìa time-inconsistent stochastic optimization problem-by developing a vanishing regularization method and deriving the corresponding extended HJB equation. More precisely, to tackle the mathematical intractability of direct equilibrium analysis, we introduce a regularized problem, which enables rigorous derivation of the equilibrium strategy and the associated extended HJB equation. Then, letting Œª‚Üí0\lambda\rightarrow 0, i.e. vanishing regularization, we formally recover a system of parabolic variational inequalities ([7](https://arxiv.org/html/2510.24128v1#S4.E7 "In 4 Extended HJB Equation for Original Problem ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")) for the original MV problem. This system characterizes equilibrium stopping times and includes a key quadratic term Œ≥2‚Äã(f‚àíg)2\frac{\gamma}{2}(f-g)^{2}‚Äìa distinction from classical optimal stopping, where stopping conditions depend only on comparing the value function to the instantaneous reward. By extending the analysis to general time-inconsistent problems, we demonstrate that the additional term in our stopping condition arises from the non-linearity of the objective (responsible for time inconsistency) and the use of mixed strategies.

This work provides a rigorous mathematical foundation for MV stopping problems, with potential applications in financial decision-making (e.g., asset sale timing, portfolio liquidation) and statistical inference (e.g., risk-aware hypothesis testing stopping rules). However, there are still some open problems left for future research, including
proving rigorous convergence of the regularized extended HJB system to the limiting variational inequality and developing numerical algorithms to compute equilibrium stopping rules for practical applications.

## References

* [1]

  C.¬†Bayer, D.¬†Belomestny, P.¬†Hager, P.¬†Pigato, and J.¬†Schoenmakers, Randomized optimal stopping algorithms and their convergence analysis, SIAM
  Journal on Financial Mathematics, 12 (2021), pp.¬†1201‚Äì1225.
* [2]

  E.¬†Bayraktar, Z.¬†Wang, and Z.¬†Zhou, Equilibria of time-inconsistent
  stopping for one-dimensional diffusion processes, Mathematical Finance, 33
  (2023), pp.¬†797‚Äì841.
* [3]

  T.¬†Bj√∂rk, M.¬†Khapko, and A.¬†Murgoci, On time-inconsistent
  stochastic control in continuous time, Finance and Stochastics, 21 (2017),
  pp.¬†331‚Äì360.
* [4]

  T.¬†Bj√∂rk, M.¬†Khapko, A.¬†Murgoci, et¬†al., Time-inconsistent
  control theory with finance applications, vol.¬†732, Springer, 2021.
* [5]

  T.¬†Bj√∂rk, A.¬†Murgoci, and X.¬†Y. Zhou, Mean‚Äìvariance portfolio
  optimization with state-dependent risk aversion, Mathematical Finance: An
  International Journal of Mathematics, Statistics and Financial Economics, 24
  (2014), pp.¬†1‚Äì24.
* [6]

  S.¬†Christensen and K.¬†Lindensj√∂, On finding equilibrium stopping
  times for time-inconsistent markovian problems, SIAM Journal on Control and
  Optimization, 56 (2018), pp.¬†4228‚Äì4255.
* [7]

  S.¬†Christensen and K.¬†Lindensj√∂, On time-inconsistent stopping
  problems and mixed strategy stopping times, Stochastic Processes and their
  Applications, 130 (2020), pp.¬†2886‚Äì2917.
* [8]

  M.¬†Dai, Y.¬†Dong, and Y.¬†Jia, Learning equilibrium mean-variance
  strategy, Mathematical Finance, 33 (2023), pp.¬†1166‚Äì1212.
* [9]

  Y.¬†Dong, Randomized optimal stopping problem in continuous time and
  reinforcement learning algorithm, SIAM Journal on Control and Optimization,
  62 (2024), pp.¬†1590‚Äì1614.
* [10]

  K.¬†D. Elworthy and X.-M. Li, Formulae for the derivatives of heat
  semigroups, Journal of Functional Analysis, 125 (1994), pp.¬†252‚Äì286.
* [11]

  L.¬†He and Z.¬†Liang, Optimal investment strategy for the dc plan with
  the return of premiums clauses in a mean‚Äìvariance framework, Insurance:
  Mathematics and Economics, 53 (2013), pp.¬†643‚Äì649.
* [12]

  M.¬†Jeanblanc, M.¬†Yor, and M.¬†Chesney, Mathematical methods for
  financial markets, Springer Science & Business Media, 2009.
* [13]

  G.¬†M. Lieberman, Second order parabolic differential equations,
  World scientific, 1996.
* [14]

  H.¬†M. Markowitz, Portfolio selection: efficient diversification of
  investments, Yale university press, 2008.
* [15]

  G.¬†Peskir and A.¬†Shiryaev, Optimal stopping and free-boundary
  problems, Springer, 2006.
* [16]

  R.¬†H. Strotz, Myopia and inconsistency in dynamic utility
  maximization, The review of economic studies, 23 (1955), pp.¬†165‚Äì180.
* [17]

  A.¬†Tartakovsky, I.¬†Nikiforov, and M.¬†Basseville, Sequential
  analysis: Hypothesis testing and changepoint detection, CRC press, 2014.

## Appendix A Proof of ([9](https://arxiv.org/html/2510.24128v1#S4.E9 "In 4 Extended HJB Equation for Original Problem ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method"))

Define another stopping time œÑ~Œµ\tilde{\tau}^{\varepsilon} as

|  |  |  |
| --- | --- | --- |
|  | œÑ~Œµ=inf{s‚â•t||Xs‚àíXt|‚â•Œµ}.\tilde{\tau}^{\varepsilon}=\inf\{s\geq t||X\_{s}-X\_{t}|\geq\varepsilon\}. |  |

Note that œÑŒµ\tau^{\varepsilon} coincides to œÑ~Œµ\tilde{\tau}^{\varepsilon} on the set {œÑ~Œµ<t+Œµ}\{\tilde{\tau}^{\varepsilon}<t+\varepsilon\}. Then, we claim that

|  |  |  |  |
| --- | --- | --- | --- |
|  | limŒµ‚Üí0Œµ2ùîºt,x‚Äã[œÑ~Œµ‚àít]=œÉ2‚Äã(t,x).\lim\_{\varepsilon\rightarrow 0}\frac{\varepsilon^{2}}{\mathbb{E}\_{t,x}[\tilde{\tau}^{\varepsilon}-t]}=\sigma^{2}(t,x). |  | (13) |

To prove the claim, we follow the same argument as that in the proof of Lemma 5.5 in [[7](https://arxiv.org/html/2510.24128v1#bib.bib7)]. For any a>1œÉ2‚Äã(t,x)a>\frac{1}{\sigma^{2}(t,x)}, define a function FF as F‚Äã(t,y)=a‚Äã(y‚àíx)2‚àítF(t,y)=a(y-x)^{2}-t. It is easy to see that

|  |  |  |
| --- | --- | --- |
|  | (‚àÇt+‚Ñí)‚ÄãF‚Äã(s,y)=2‚Äãa‚ÄãŒº‚Äã(s,y)‚Äã(y‚àíx)+a‚ÄãœÉ2‚Äã(s,y)‚àí1,(\partial\_{t}+\mathcal{L})F(s,y)=2a\mu(s,y)(y-x)+a\sigma^{2}(s,y)-1, |  |

which is greater than 0 for {(s,y)||y‚àíx|‚â§Œµ,s‚àít‚â§Œµ}\{(s,y)||y-x|\leq\varepsilon,s-t\leq\varepsilon\} with a sufficiently small Œµ\varepsilon. Thus, applying It√¥ formula to F‚Äã(s,Xs)F(s,X\_{s}) and taking conditional expectation, it holds that

|  |  |  |
| --- | --- | --- |
|  | ùîºt,x‚Äã[œÑ~Œµ‚àít]‚â§a‚Äãùîºt,x‚Äã[|XœÑ~Œµ‚àíx|2]=a‚ÄãŒµ2.\mathbb{E}\_{t,x}[\tilde{\tau}^{\varepsilon}-t]\leq a\mathbb{E}\_{t,x}[|X\_{\tilde{\tau}^{\varepsilon}}-x|^{2}]=a\varepsilon^{2}. |  |

Similarly, one also has

|  |  |  |
| --- | --- | --- |
|  | ùîºt,x‚Äã[œÑ~Œµ‚àít]‚â•a‚ÄãŒµ2,\mathbb{E}\_{t,x}[\tilde{\tau}^{\varepsilon}-t]\geq a\varepsilon^{2}, |  |

for a<1œÉ2‚Äã(t,x)a<\frac{1}{\sigma^{2}(t,x)}. Combining these two estimations give the claim ([13](https://arxiv.org/html/2510.24128v1#A1.E13 "In Appendix A Proof of (9) ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")).
Using Chebyshev‚Äôs inequality, we also have that

|  |  |  |  |
| --- | --- | --- | --- |
|  | P‚Äã(œÑ~Œµ>t+Œµ)‚â§ùîºt,x‚Äã[œÑ~Œµ‚àít]Œµ=O‚Äã(Œµ).P(\tilde{\tau}^{\varepsilon}>t+\varepsilon)\leq\frac{\mathbb{E}\_{t,x}[\tilde{\tau}^{\varepsilon}-t]}{\varepsilon}=O(\varepsilon). |  | (14) |

The argument for ([13](https://arxiv.org/html/2510.24128v1#A1.E13 "In Appendix A Proof of (9) ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")) also yields that

|  |  |  |
| --- | --- | --- |
|  | limh‚Üí0ùîºt,x‚Äã[|XœÑŒµ‚àíx|2]ùîºt,x‚Äã[œÑŒµ‚àít]=œÉ2‚Äã(t,x).\lim\_{h\rightarrow 0}\frac{\mathbb{E}\_{t,x}[|X\_{\tau^{\varepsilon}}-x|^{2}]}{\mathbb{E}\_{t,x}[\tau^{\varepsilon}-t]}=\sigma^{2}(t,x). |  |

Note that, using ([14](https://arxiv.org/html/2510.24128v1#A1.E14 "In Appendix A Proof of (9) ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")),

|  |  |  |
| --- | --- | --- |
|  | Œµ2‚â•ùîºt,x‚Äã[|XœÑŒµ‚àíx|2]‚â•Œµ2‚ÄãP‚Äã(œÑŒµ<t+Œµ)=Œµ2‚àíŒµ2‚ÄãP‚Äã(œÑŒµ=t+Œµ)=Œµ2+o‚Äã(Œµ2).\varepsilon^{2}\geq\mathbb{E}\_{t,x}[|X\_{\tau^{\varepsilon}}-x|^{2}]\geq\varepsilon^{2}P(\tau^{\varepsilon}<t+\varepsilon)=\varepsilon^{2}-\varepsilon^{2}P(\tau^{\varepsilon}=t+\varepsilon)=\varepsilon^{2}+o(\varepsilon^{2}). |  |

Hence, we get that

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùîºt,x‚Äã[œÑŒµ‚àít]=1œÉ2‚Äã(t,x)‚ÄãŒµ2+o‚Äã(Œµ).\mathbb{E}\_{t,x}[\tau^{\varepsilon}-t]=\frac{1}{\sigma^{2}(t,x)}\varepsilon^{2}+o(\varepsilon). |  | (15) |

Let k‚Äã(t,y):=|c‚Äã(t)‚àíy|k(t,y):=|c(t)-y|. From It√¥-Tanaka formula, one has that

|  |  |  |
| --- | --- | --- |
|  | k‚Äã(œÑŒµ,XœÑŒµ)=‚à´tœÑŒµ12‚Äã((‚àÇt+‚Ñí)‚Äãk‚Äã(s,Xs+)+(‚àÇt+‚Ñí)‚Äãk‚Äã(s,Xs‚àí))‚Äãùëës+‚à´tœÑŒµ12‚ÄãœÉ‚Äã(‚àÇyk‚Äã(s,Xs+)+‚àÇyk‚Äã(s,Xs‚àí))‚ÄãùëëWs+‚à´tœÑŒµ12‚Äã(‚àÇyk‚Äã(s,Xs+)‚àí‚àÇyk‚Äã(s,Xs‚àí))‚Äã1{Xs=c‚Äã(s)}‚Äãùëëlsc.\begin{split}k(\tau^{\varepsilon},X\_{\tau^{\varepsilon}})=&\int\_{t}^{\tau^{\varepsilon}}\frac{1}{2}((\partial\_{t}+\mathcal{L})k(s,X\_{s}+)+(\partial\_{t}+\mathcal{L})k(s,X\_{s}-))ds\\ &+\int\_{t}^{\tau^{\varepsilon}}\frac{1}{2}\sigma(\partial\_{y}k(s,X\_{s}+)+\partial\_{y}k(s,X\_{s}-))dW\_{s}\\ &+\int\_{t}^{\tau^{\varepsilon}}\frac{1}{2}(\partial\_{y}k(s,X\_{s}+)-\partial\_{y}k(s,X\_{s}-))1\_{\{X\_{s}=c(s)\}}dl\_{s}^{c}.\end{split} |  |

Note that, for Xs=c‚Äã(s)X\_{s}=c(s), ‚àÇyk‚Äã(s,Xs+)=1\partial\_{y}k(s,X\_{s}+)=1 and ‚àÇyk‚Äã(s,Xs+)=‚àí1\partial\_{y}k(s,X\_{s}+)=-1. Thus,

|  |  |  |  |
| --- | --- | --- | --- |
|  | (ùîºt,x‚Äã[k‚Äã(œÑŒµ,XœÑŒµ)])2=(ùîºt,x‚Äã[lœÑŒµc‚àíltc])2+o‚Äã(ùîºt,x‚Äã[œÑŒµ‚àít]).(\mathbb{E}\_{t,x}[k(\tau^{\varepsilon},X\_{\tau^{\varepsilon}})])^{2}=(\mathbb{E}\_{t,x}[l^{c}\_{\tau^{\varepsilon}}-l^{c}\_{t}])^{2}+o(\mathbb{E}\_{t,x}[\tau^{\varepsilon}-t]). |  | (16) |

When tt is fixed, for sufficienly small Œµ\varepsilon,

|  |  |  |
| --- | --- | --- |
|  | ùîºt,x‚Äã[k‚Äã(œÑŒµ,XœÑŒµ)]=ùîºt,x‚Äã[|x+Œµ‚àíc‚Äã(œÑŒµ)|‚Äã1{XœÑŒµ=x+Œµ}]+ùîºt,x‚Äã[|x‚àíŒµ‚àíc‚Äã(œÑŒµ)|‚Äã1{XœÑŒµ=x‚àíŒµ}]+ùîºt,x‚Äã[|XœÑŒµ‚àíc‚Äã(œÑŒµ)|‚Äã1{œÑŒµ=t+Œµ}].\begin{split}\mathbb{E}\_{t,x}[k(\tau^{\varepsilon},X\_{\tau^{\varepsilon}})]=&\mathbb{E}\_{t,x}[|x+\varepsilon-c(\tau^{\varepsilon})|1\_{\{X\_{\tau^{\varepsilon}=x+\varepsilon}\}}]+\mathbb{E}\_{t,x}[|x-\varepsilon-c(\tau^{\varepsilon})|1\_{\{X\_{\tau^{\varepsilon}=x-\varepsilon}\}}]\\ &+\mathbb{E}\_{t,x}[|X\_{\tau^{\varepsilon}}-c(\tau^{\varepsilon})|1\_{\{\tau^{\varepsilon}=t+\varepsilon\}}].\end{split} |  |

We estimate the right hand side term by term. Estimations for the first two term
are similar. It holds that

|  |  |  |
| --- | --- | --- |
|  | ùîºt,x‚Äã[|x¬±Œµ‚àíc‚Äã(œÑŒµ)|‚Äã1{XœÑŒµ=x¬±Œµ}]‚â§ùîºt,x‚Äã[(|x¬±Œµ‚àíc‚Äã(t)|+|c‚Äã(t)‚àíc‚Äã(œÑŒµ)|)‚Äã1{XœÑŒµ=x¬±Œµ}]‚â§p¬±‚ÄãŒµ+C‚Äãùîºt,x‚Äã[œÑŒµ‚àít]=p¬±+O‚Äã(Œµ2),\begin{split}\mathbb{E}\_{t,x}[|x\pm\varepsilon-c(\tau^{\varepsilon})|1\_{\{X\_{\tau^{\varepsilon}=x\pm\varepsilon}\}}]&\leq\mathbb{E}\_{t,x}[\left(|x\pm\varepsilon-c(t)|+|c(t)-c(\tau^{\varepsilon})|\right)1\_{\{X\_{\tau^{\varepsilon}=x\pm\varepsilon}\}}]\\ &\leq p\_{\pm}\varepsilon+C\mathbb{E}\_{t,x}[\tau^{\varepsilon}-t]=p\_{\pm}+O(\varepsilon^{2}),\end{split} |  |

where p¬±=P‚Äã(XœÑŒµ=x¬±Œµ)p\_{\pm}=P(X\_{\tau^{\varepsilon}}=x\pm\varepsilon) and the last inequality is due to the assumption that c‚Äã(‚ãÖ)c(\cdot) is Lipschitz continuous. For the last term,

|  |  |  |
| --- | --- | --- |
|  | ùîºt,x‚Äã[|XœÑŒµ‚àíc‚Äã(œÑŒµ)|‚Äã1{œÑŒµ=t+Œµ}]=ùîºt,x‚Äã[|XœÑŒµ‚àíc‚Äã(t)+c‚Äã(t)‚àíc‚Äã(œÑŒµ)|‚Äã1{œÑŒµ=t+Œµ}]‚â§C‚ÄãŒµ‚ÄãP‚Äã(œÑŒµ=t+Œµ)=C‚ÄãŒµ‚ÄãP‚Äã(œÑ~Œµ>t+Œµ)=O‚Äã(Œµ2).\begin{split}\mathbb{E}\_{t,x}[|X\_{\tau^{\varepsilon}}-c(\tau^{\varepsilon})|1\_{\{\tau^{\varepsilon}=t+\varepsilon\}}]=&\mathbb{E}\_{t,x}[|X\_{\tau^{\varepsilon}}-c(t)+c(t)-c(\tau^{\varepsilon})|1\_{\{\tau^{\varepsilon}=t+\varepsilon\}}]\\ \leq&C\varepsilon P(\tau^{\varepsilon}=t+\varepsilon)=C\varepsilon P(\tilde{\tau}^{\varepsilon}>t+\varepsilon)=O(\varepsilon^{2}).\end{split} |  |

Combining these estimations, we get that

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùîº‚Äã[k‚Äã(œÑŒµ,XœÑŒµ)]=(p‚àí+p+)‚ÄãŒµ+O‚Äã(Œµ2)=Œµ+O‚Äã(Œµ2),\mathbb{E}[k(\tau^{\varepsilon},X\_{\tau^{\varepsilon}})]=(p\_{-}+p\_{+})\varepsilon+O(\varepsilon^{2})=\varepsilon+O(\varepsilon^{2}), |  | (17) |

where we use ([14](https://arxiv.org/html/2510.24128v1#A1.E14 "In Appendix A Proof of (9) ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")) again to get the last equality.
Hence, ([15](https://arxiv.org/html/2510.24128v1#A1.E15 "In Appendix A Proof of (9) ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")),([16](https://arxiv.org/html/2510.24128v1#A1.E16 "In Appendix A Proof of (9) ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")) and ([17](https://arxiv.org/html/2510.24128v1#A1.E17 "In Appendix A Proof of (9) ‚Ä£ Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method")) yield that

|  |  |  |
| --- | --- | --- |
|  | limŒµ‚Üí0(ùîºt,x‚Äã[lœÑŒµc‚àíltc])2ùîºt,x‚Äã[œÑŒµ‚àít]=œÉ2‚Äã(t,x).\lim\_{\varepsilon\rightarrow 0}\frac{(\mathbb{E}\_{t,x}[l^{c}\_{\tau^{\varepsilon}}-l^{c}\_{t}])^{2}}{\mathbb{E}\_{t,x}[\tau^{\varepsilon}-t]}=\sigma^{2}(t,x). |  |

## Appendix B A Key Lemma

###### Lemma B.1.

Let œÜ\varphi be the solution of

|  |  |  |
| --- | --- | --- |
|  | (‚àÇt+‚Ñí)‚ÄãœÜ+c‚ÄãœÜ=g,œÜ‚Äã(T,x)=f,(\partial\_{t}+\mathcal{L})\varphi+c\varphi=g,\varphi(T,x)=f, |  |

with c,gc,g being bounded continuous functions and ff also bounded with bounded derivatives. Then

1. 1.

   There exists a constant CC depending on the coefficients such that

   |  |  |  |
   | --- | --- | --- |
   |  | ‚àíC‚Äã(‚Äñf‚àí‚Äñ‚àû+T‚Äã‚Äñg‚àí‚Äñ‚àû)‚â§œÜ‚â§C‚Äã(‚Äñf+‚Äñ‚àû+T‚Äã‚Äñg+‚Äñ‚àû),-C(\|f^{-}\|\_{\infty}+T\|g^{-}\|\_{\infty})\leq\varphi\leq C(\|f^{+}\|\_{\infty}+T\|g^{+}\|\_{\infty}), |  |

   where f¬±f^{\pm} represents the positive and negative part of ff respectively. If c‚â§0c\leq 0, the constant CC can be chosen to be 11.
2. 2.

   Assuming that c‚â°0c\equiv 0, we have

   |  |  |  |
   | --- | --- | --- |
   |  | ‚Äñ‚àÇxœÜ‚Äñ‚àû‚â§(1+C‚ÄãT)‚Äã‚Äñ‚àÇxf‚Äñ‚àû+C‚Äã(T+T)‚Äã‚Äñg‚Äñ‚àû.\|\partial\_{x}\varphi\|\_{\infty}\leq(1+C\sqrt{T})\|\partial\_{x}f\|\_{\infty}+C(\sqrt{T}+T)\|g\|\_{\infty}. |  |

###### Proof.

From Feymann-Kac representation, it holds that

|  |  |  |
| --- | --- | --- |
|  | œÜ‚Äã(t,x)=ùîºt,x‚Äã[e‚à´tTc‚Äã(u,Xu)‚Äãùëëu‚Äãf‚Äã(XT)+‚à´tTe‚à´tsc‚Äã(u,Xu)‚Äãùëëu‚Äãg‚Äã(s,Xs)‚Äãùëës].\varphi(t,x)=\mathbb{E}\_{t,x}\left[e^{\int\_{t}^{T}c(u,X\_{u})du}f(X\_{T})+\int\_{t}^{T}e^{\int\_{t}^{s}c(u,X\_{u})du}g(s,X\_{s})ds\right]. |  |

Then, one can easily get the first estimation from the assumptions on the coefficients.

To prove the second estimation, define two processes ‚àáX\nabla X and NN as

|  |  |  |
| --- | --- | --- |
|  | d‚Äã‚àáX=‚àÇxb‚Äã(Xt)‚Äã‚àáXt‚Äãd‚Äãt+‚àÇxœÉ‚Äã(Xt)‚Äã‚àáXt‚Äãd‚ÄãWt,‚àáXt=I,d\nabla X=\partial\_{x}b(X\_{t})\nabla X\_{t}dt+\partial\_{x}\sigma(X\_{t})\nabla X\_{t}dW\_{t},\nabla X\_{t}=I, |  |

and

|  |  |  |
| --- | --- | --- |
|  | Ns=1s‚àít‚Äã‚à´ts<œÉ‚àí1‚Äã(Xu)‚Äã‚àáXu,d‚ÄãWu>.N\_{s}=\frac{1}{s-t}\int\_{t}^{s}<\sigma^{-1}(X\_{u})\nabla X\_{u},dW\_{u}>. |  |

One can verify that

|  |  |  |
| --- | --- | --- |
|  | ùîºt,x‚Äã[supt‚â§s‚â§T|‚àáXs‚àíI|2]‚â§C‚ÄãT‚Äã, and¬†‚Äãùîºt,x‚Äã[|Ns|2]‚â§C‚Äã(1s‚àít+1).\mathbb{E}\_{t,x}[\sup\_{t\leq s\leq T}|\nabla X\_{s}-I|^{2}]\leq CT\text{, and }\mathbb{E}\_{t,x}[|N\_{s}|^{2}]\leq C(\frac{1}{s-t}+1). |  |

Then, using Bismut-Elworthy-Li formula [[10](https://arxiv.org/html/2510.24128v1#bib.bib10)], we get that, for any function Œæ\xi

|  |  |  |
| --- | --- | --- |
|  | ‚àÇxùîºt,x‚Äã[Œæ‚Äã(Xs)]=ùîºt,x‚Äã[‚àÇxŒæ‚Äã(Xs)‚Äã‚àáXs]=ùîºt,x‚Äã[Œæ‚Äã(Xs)‚ÄãNs].\partial\_{x}\mathbb{E}\_{t,x}[\xi(X\_{s})]=\mathbb{E}\_{t,x}[\partial\_{x}\xi(X\_{s})\nabla X\_{s}]=\mathbb{E}\_{t,x}[\xi(X\_{s})N\_{s}]. |  |

Thus,

|  |  |  |
| --- | --- | --- |
|  | ‚àÇxœÜ‚Äã(t,x)=ùîºt,x‚Äã[‚àÇxf‚Äã(XT)‚Äã‚àáXT]+‚à´tTùîºt,x‚Äã[g‚Äã(s,Xs)‚ÄãNs]‚Äãùëës.\partial\_{x}\varphi(t,x)=\mathbb{E}\_{t,x}[\partial\_{x}f(X\_{T})\nabla X\_{T}]+\int\_{t}^{T}\mathbb{E}\_{t,x}[g(s,X\_{s})N\_{s}]ds. |  |

Clearly, we have

|  |  |  |
| --- | --- | --- |
|  | |ùîºt,x‚Äã[‚àÇxf‚Äã(XT)‚Äã‚àáXT]|‚â§(C‚ÄãT+1)‚Äã‚Äñ‚àÇxf‚Äñ‚àû,|\mathbb{E}\_{t,x}[\partial\_{x}f(X\_{T})\nabla X\_{T}]|\leq(C\sqrt{T}+1)\|\partial\_{x}f\|\_{\infty}, |  |

and

|  |  |  |
| --- | --- | --- |
|  | ‚à´tTùîºt,x‚Äã[g‚Äã(s,Xs)‚ÄãNs]‚Äãùëës‚â§‚Äñg‚Äñ‚àû‚Äã‚à´tT(ùîºt,x‚Äã[|Ns|2])12‚Äãùëës‚â§C‚Äã‚Äñg‚Äñ‚àû‚Äã‚à´tT1(s‚àít)12+1‚Äãd‚Äãs=C‚Äã(T‚àít+(T‚àít))‚Äã‚Äñg‚Äñ‚àû.\begin{split}&\int\_{t}^{T}\mathbb{E}\_{t,x}[g(s,X\_{s})N\_{s}]ds\leq\|g\|\_{\infty}\int\_{t}^{T}(\mathbb{E}\_{t,x}[|N\_{s}|^{2}])^{\frac{1}{2}}ds\\ \leq&C\|g\|\_{\infty}\int\_{t}^{T}\frac{1}{(s-t)^{\frac{1}{2}}}+1ds=C(\sqrt{T-t}+(T-t))\|g\|\_{\infty}.\end{split} |  |

This gives the second estimation.
‚àé