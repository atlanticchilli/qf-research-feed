---
authors:
- Jin Ma
- Ying Tan
- Renyuan Xu
doc_id: arxiv:2510.11829v1
family_id: arxiv:2510.11829
is_current: true
taxonomy:
  alpha_families: []
  asset_classes: []
  horizons: []
  themes: []
title: 'SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence
  analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama
  Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s
  60th birthday'
url_abs: http://arxiv.org/abs/2510.11829v1
url_html: https://arxiv.org/html/2510.11829v1
venue: arXiv q-fin
version: 1
year: 2025
---


Jin Ma, Â Ying Tan, Â and
Renyuan Xu
Department of Mathematics, University of Southern California, Los Angeles, CA, 90089.
Email: jinma@usc.edu. This author is supported in part by US NSF grants DMS#2205972 and #2510403.
Department of Statistics and Applied Probability, University of California, Santa Barbara, CA 93106. Email: yingtan@ucsb.edu.Department of Management of Science & Engineering, Stanford University, Stanford, CA 94305. Email: renyuanxu@stanford.edu. This author is supported in part by the NSF CAREER Award DMS-2524465.

(October 13, 2025)

###### Abstract

Generative AI can be framed as the problem of learning a model that maps simple reference measures into complex data distributions, and it has recently found a strong connection to the classical theory of the SchrÃ¶dinger bridge problems (SBPs)
due partly to their common nature of interpolating between prescribed marginals via entropy-regularized stochastic dynamics. However, the classical SBP enforces hard terminal constraints, which often leads to instability in practical implementations, especially in high-dimensional or data-scarce regimes.
To address this challenge, we follow the idea of the so-called soft-constrained SchrÃ¶dinger bridge problem(SCSBP), in which the terminal constraint is replaced by a general penalty function. This relaxation leads to a more flexible stochastic control formulation of McKeanâ€“Vlasov type.

We establish the existence of optimal solutions for all penalty levels and prove that, as the penalty grows, both the controls and value functions converge to those of the classical SBP at a linear rate. Our analysis builds on Doobâ€™s hh-transform representations, the stability results of SchrÃ¶dinger potentials, Î“\Gamma-convergence, and a novel fixed-point argument that couples an optimization problem over the space of measures with an auxiliary entropic optimal transport problem. These results not only provide the first quantitative convergence guarantees for soft-constrained bridges but also shed light on how penalty regularization enables robust generative modeling, fine-tuning, and transfer learning.

Keywords. SchrÃ¶dinger bridge, soft-constrained SchrÃ¶dinger bridge, entropic optimal transport stability, SchrÃ¶dinger potentials, generative AI, hh-transform, converse of ScheffÃ©â€™s Theorem, Î“\Gamma-convergence, Schauderâ€™s fixed-point.

2020 AMS Mathematics subject classification: 60H10, 60J60, 49J21, 68T01, 93E20.

## 1 Introduction

Generative modeling provides a powerful framework for synthesizing data that preserves the statistical structure of real-world samples while introducing controlled variability. Among the most prominent approaches, diffusion models â€” such as those introduced by [[55](https://arxiv.org/html/2510.11829v1#bib.bib55), [57](https://arxiv.org/html/2510.11829v1#bib.bib57), [58](https://arxiv.org/html/2510.11829v1#bib.bib58), [36](https://arxiv.org/html/2510.11829v1#bib.bib36)] â€” have achieved remarkable success, underpinning state-of-the-art systems like DALLÂ·E 2 and 3 [[51](https://arxiv.org/html/2510.11829v1#bib.bib51), [6](https://arxiv.org/html/2510.11829v1#bib.bib6)], Stable Diffusion [[53](https://arxiv.org/html/2510.11829v1#bib.bib53)], and Sora [[47](https://arxiv.org/html/2510.11829v1#bib.bib47)]. These models learn to reverse a diffusion process that gradually adds noise to data, enabling the generation of realistic samples from pure noise. Such a structure, namely, transforming a noise distribution into a data distribution, closely mirrors the SchrÃ¶dinger bridge problem (or dynamic optimal transport), which has recently gained renewed attention as a principled framework for generative modeling due to its structural parallels with diffusion models and its ability to interpolate between distributions in a statistically grounded manner.

The SchrÃ¶dinger bridge problem (SBP for short), originally proposed as an entropy-regularized variant of optimal transport, seeks the most likely evolution of a process, subject to a reference diffusion process, that matches prescribed marginal distributions Î¼ini,Î¼tar{\mu\_{\rm ini}},{\mu\_{\rm tar}} at two endpoints. Under suitable regularity conditions, the optimally controlled process remains a diffusion but with an additional drift term added to the reference process. This result has been established through various approaches and levels of generality, with seminal contributions by [[26](https://arxiv.org/html/2510.11829v1#bib.bib26), [7](https://arxiv.org/html/2510.11829v1#bib.bib7), [37](https://arxiv.org/html/2510.11829v1#bib.bib37), [25](https://arxiv.org/html/2510.11829v1#bib.bib25), [18](https://arxiv.org/html/2510.11829v1#bib.bib18)].

The recent generative modeling literature has seen a surge in the use of SchrÃ¶dinger bridges. In these applications, one typically starts or chooses some distribution Î¼ini{\mu\_{\rm ini}} that is easy to sample from, and tries to "learn" an unknown distribution Î¼tar{\mu\_{\rm tar}} of a given data set. By numerically approximating the solution to the SchrÃ¶dinger bridge problem, one can generate unlimited samples (i.e., synthetic data points that resemble the original data set). One such algorithm is presented by De Bortoli et al. [[20](https://arxiv.org/html/2510.11829v1#bib.bib20)] and Vargas et al.Â [[64](https://arxiv.org/html/2510.11829v1#bib.bib64)], who approximate the iterative proportional fitting procedure (Deming-Stephan [[21](https://arxiv.org/html/2510.11829v1#bib.bib21)]), using score matching with neural networks and maximum likelihood, respectively.
Concurrently, Wang et al. [[66](https://arxiv.org/html/2510.11829v1#bib.bib66)] proposed a two-stage method with an auxiliary bridge handling possible non-smooth Î¼tar{\mu\_{\rm tar}}. Some more recent developments include
[[13](https://arxiv.org/html/2510.11829v1#bib.bib13), [56](https://arxiv.org/html/2510.11829v1#bib.bib56), [49](https://arxiv.org/html/2510.11829v1#bib.bib49), [52](https://arxiv.org/html/2510.11829v1#bib.bib52), [67](https://arxiv.org/html/2510.11829v1#bib.bib67), [32](https://arxiv.org/html/2510.11829v1#bib.bib32), [63](https://arxiv.org/html/2510.11829v1#bib.bib63), [54](https://arxiv.org/html/2510.11829v1#bib.bib54), [62](https://arxiv.org/html/2510.11829v1#bib.bib62), [2](https://arxiv.org/html/2510.11829v1#bib.bib2)] as well as developing optimal transport techniques for generative AI tasks [[44](https://arxiv.org/html/2510.11829v1#bib.bib44), [5](https://arxiv.org/html/2510.11829v1#bib.bib5), [42](https://arxiv.org/html/2510.11829v1#bib.bib42), [68](https://arxiv.org/html/2510.11829v1#bib.bib68), [1](https://arxiv.org/html/2510.11829v1#bib.bib1)].

However, the classical SBP imposes hard terminal constraints on the marginal distributions, which can result in computational difficulties, instability in high-dimensional settings, and limited adaptability when aligning with empirical data in generative tasks. In practice, most numerical schemes for solving the SBP rely on iterative procedures that alternately relax the initial and terminal constraints. These algorithms can exhibit instability, particularly when the two constraints differ significantly, and their convergence guarantees for general target distributions remain an open problem in the literature.

In this paper, in light of Garg et al. [[29](https://arxiv.org/html/2510.11829v1#bib.bib29)], we study a soft-constrained SchrÃ¶dinger bridge problem (SCSBP).
Mathematically, we consider a (smooth) penalty function G:ğ’«2â€‹(â„d)â†¦â„+G:\mathscr{P}\_{2}(\mathbb{R}^{d})\mapsto\mathbb{R}\_{+}, satisfying Gâ€‹(Î¼;Î¼tar)=0G(\mu;{\mu\_{\rm tar}})=0,
where ğ’«2â€‹(â„d)\mathscr{P}\_{2}(\mathbb{R}^{d}) denotes the 2-Wasserstein space on â„d\mathbb{R}^{d}, and Î¼tarâˆˆğ’«2â€‹(â„d){\mu\_{\rm tar}}\in\mathscr{P}\_{2}(\mathbb{R}^{d}) is some given â€œtargetâ€ distribution. For each kâˆˆâ„•k\in\mathbb{N} and a given initial distribution Î¼iniâˆˆğ’«2â€‹(â„d){\mu\_{\rm ini}}\in\mathscr{P}\_{2}(\mathbb{R}^{d}), we consider the following stochastic control problem with dynamics:

|  |  |  |
| --- | --- | --- |
|  | dâ€‹XtÎ±=(bâ€‹(t,XtÎ±)+Ïƒâ€‹(t)â€‹Î±t)â€‹dâ€‹t+Ïƒâ€‹(t)â€‹dâ€‹Wt,â„™âˆ˜(X0Î±)âˆ’1=Î¼ini,dX^{\alpha}\_{t}=\big(b(t,X^{\alpha}\_{t})+\sigma(t)\alpha\_{t}\big)dt+\sigma(t)dW\_{t},\quad\mathbb{P}\circ(X\_{0}^{\alpha})^{-1}={\mu\_{\rm ini}}, |  |

and cost functional

|  |  |  |
| --- | --- | --- |
|  | Jkâ€‹(Î±)=ğ”¼â€‹[12â€‹âˆ«0T|Î±s|2â€‹ğ‘‘s+kâ€‹Gâ€‹(â„™XTÎ±)],J^{k}(\alpha)=\mathbb{E}\left[\frac{1}{2}\int\_{0}^{T}|\alpha\_{s}|^{2}ds+kG(\mathbb{P}\_{X^{\alpha}\_{T}})\right], |  |

where â„™XTÎ±\mathbb{P}\_{X^{\alpha}\_{T}} is the law of XTÎ±X^{\alpha}\_{T} and the control Î±\alpha is chosen from a square-integrable progressively measurable admissible control set ğ’œ{\cal A}. The goal is to find Vk:=infÎ±âˆˆğ’œJkâ€‹(Î±)V^{k}:=\inf\_{\alpha\in{\cal A}}J^{k}(\alpha) and optimal control Î±k\alpha^{k}, for each kâˆˆâ„•k\in\mathbb{N}, and study the limiting behavior of {Î±k}\{\alpha^{k}\} and {Vk}\{V^{k}\}.
Clearly, the dependence of the terminal cost on â„™XTÎ±\mathbb{P}\_{X^{\alpha}\_{T}} renders this relaxed formulation a non-standard stochastic control problem, leading to a McKean-Vlasov type of stochastic control. In contrast to Garg et al. [[29](https://arxiv.org/html/2510.11829v1#bib.bib29)], which focuses on the case where the penalty
GG is given by the KL divergence and
Î¼ini\mu\_{\rm ini} is a delta measure, we investigate the problem under more general cost functions and initial distributions.

Compared to existing methods using SBP under hard constraints, there are several advantages to considering SCSBP. First, when the KL divergence between Î¼tar{\mu\_{\rm tar}} and â„™XTÎ±\mathbb{P}\_{X^{\alpha}\_{T}} is infinite, the SchrÃ¶dinger bridge does not admit a solution, whereas SCSBP always does Garg et al. [[29](https://arxiv.org/html/2510.11829v1#bib.bib29)]. More importantly, the penalty parameter kk acts as a regularization factor, preventing the algorithm from overfitting to Î¼tar\mu\_{\rm tar}, which is crucial for certain generative modeling tasks with limited data [[30](https://arxiv.org/html/2510.11829v1#bib.bib30), [45](https://arxiv.org/html/2510.11829v1#bib.bib45)]. In addition, SCSBP provides a more general framework in generative AI, with applications beyond pre-training in data generation, and can be applied to fine-tuning and transfer learning (see Examples [2.6](https://arxiv.org/html/2510.11829v1#S2.ThmTheorem6 "Example 2.6 (Fine-tuning under a reward signal). â€£ Applications in Generative AI. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") and [2.7](https://arxiv.org/html/2510.11829v1#S2.ThmTheorem7 "Example 2.7 (Transfer learning). â€£ Applications in Generative AI. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")).

### 1.1 Outline of the Main Results and Contributions

The soft-constrained SchrÃ¶dinger bridge problem (SCSBP) studied in this paper replaces the terminal distribution constraint by a general penalty function, which leads to a McKeanâ€“Vlasov type stochastic control problem. The main results include the existence of the optimal solution to the SCSBP at each penalty level kk, and the convergence of the solutions to the SCSBP to that of the SBP, in terms
of both the optimal policy and the corresponding value function, as kâ†’âˆk\to\infty.

More precisely, we begin with the special case where the initial measure is a delta measure. In this setting, we derive the explicit form of the optimal control policy for SCSBP via Doobâ€™s hh-transform (PropositionÂ [3.6](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem6 "Proposition 3.6. â€£ 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) and more importantly, we establish a linear convergence rate for the optimal control (TheoremÂ [4.1](https://arxiv.org/html/2510.11829v1#S4.ThmTheorem1 "Theorem 4.1. â€£ 4.1 The Convergence of Optimal Policies â€£ 4 Convergence Results under Delta Initial Distribution â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")). To the best of our knowledge, such a rate of convergence is novel in the literature. Moreover, by applying the so-called early stopping, we are able to obtain the linear convergence results for the corresponding value functions (PropositionÂ [4.4](https://arxiv.org/html/2510.11829v1#S4.ThmTheorem4 "Proposition 4.4. â€£ 4.2 The Convergence of the Value Function â€£ 4 Convergence Results under Delta Initial Distribution â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) as well as the Wasserstein distance between the target distribution and the output distribution of the SCSBP (PropositionÂ [4.5](https://arxiv.org/html/2510.11829v1#S4.ThmTheorem5 "Proposition 4.5. â€£ 4.2 The Convergence of the Value Function â€£ 4 Convergence Results under Delta Initial Distribution â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")).

The similar results in the case of a general initial distribution is much more involved. Among other things, we establish and/or extend some recently observed stability results of the solutions to the SBP, as the foundation for a fixed-point argument. A key element in our argument is the continuous dependence (or stability) of a mapping that is well known in the (static) optimal transport literature. Specifically, for any (Î¼ini,Î¼tar)âˆˆğ’«2â€‹(â„d)Ã—ğ’«2â€‹(â„d)({\mu\_{\rm ini}},{\mu\_{\rm tar}})\in\mathscr{P}\_{2}(\mathbb{R}^{d})\times\mathscr{P}\_{2}(\mathbb{R}^{d}), it is known (cf. e.g., [[7](https://arxiv.org/html/2510.11829v1#bib.bib7)]) that there exists a unique pair of Ïƒ\sigma-finite measures (Î½0,Î½):=ğ’¯â€‹(Î¼ini,Î¼tar)(\nu\_{0},\nu):={\cal T}({\mu\_{\rm ini}},{\mu\_{\rm tar}}) such that the measure Ï€âˆˆğ’«2â€‹(â„dÃ—â„d)\pi\in\mathscr{P}\_{2}(\mathbb{R}^{d}\times\mathbb{R}^{d}) defined by

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ï€â€‹(E)=âˆ«Epâ€‹(T,y;0,x)â€‹Î½0â€‹(dâ€‹x)â€‹Î½â€‹(dâ€‹y),Eâˆˆâ„¬â€‹(â„dÃ—â„d)\displaystyle\pi(E)=\int\_{E}p(T,y;0,x)\nu\_{0}(dx)\nu(dy),\qquad E\in\mathscr{B}(\mathbb{R}^{d}\times\mathbb{R}^{d}) |  | (1.1) |

has the marginals Î¼ini{\mu\_{\rm ini}} and Î¼\mu, where pâ€‹(â‹…,â‹…;â‹…,â‹…)p(\cdot,\cdot;\cdot,\cdot) is the transition density of a given diffusion process. If we fix Î¼ini{\mu\_{\rm ini}}, and denote ÏÎ¼\rho^{\mu} to be the density of the measure Î½=ğ’¯â€‹(Î¼)\nu={\cal T}(\mu), Î¼âˆˆğ’«2â€‹(â„d)\mu\in\mathscr{P}\_{2}(\mathbb{R}^{d}), then it turns out that the solution to the SCSBP is the fixed-point of the mapping Î“:ğ’«2â€‹(â„d)â†¦ğ’«2â€‹(â„d)\Gamma:\mathscr{P}\_{2}(\mathbb{R}^{d})\mapsto\mathscr{P}\_{2}(\mathbb{R}^{d}), where

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î“â€‹(Î¼):=argâ€‹minÎ½âˆˆğ’«2â€‹(â„d)â¡{kâ€‹Gâ€‹(Î½)+ğ”¼Xâˆ¼Î½â€‹[logâ¡ÏÎ¼â€‹(X)]}.\displaystyle\Gamma(\mu):={\operatorname\*{arg\,min}}\_{\nu\in\mathscr{P}\_{2}(\mathbb{R}^{d})}\left\{kG(\nu)+\mathbb{E}\_{X\sim\nu}[\log\rho^{\mu}(X)]\right\}. |  | (1.2) |

The successful application of Schauderâ€™s fixed-point theorem on the Wasserstein space relies on several key elements, in particular a crucial continuous dependence result of the mapping Î¼â†¦ÏÎ¼\mu\mapsto\rho^{\mu}, for which we introduce an auxiliary entropic optimal transport problem, and identify its solution to the measure Ï€\pi in ([1.1](https://arxiv.org/html/2510.11829v1#S1.E1 "In 1.1 Outline of the Main Results and Contributions â€£ 1 Introduction â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")).
By utilizing some important stability results of the corresponding SchrÃ¶dinger potentials (Proposition [5.8](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem8 "Proposition 5.8. â€£ 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")), together with some arguments in the spirit of the converse of ScheffÃ©â€™s theorem (i.e., the weak convergence in Prohorov metric vs. the convergence in densities) as well as the so-called Î“\Gamma-convergence of the minimizers of the optimization problem ([1.2](https://arxiv.org/html/2510.11829v1#S1.E2 "In 1.1 Outline of the Main Results and Contributions â€£ 1 Introduction â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")), we are able to verify the required properties so the mapping Î“\Gamma has a fixed-point (TheoremÂ [6.1](https://arxiv.org/html/2510.11829v1#S6.ThmTheorem1 "Theorem 6.1. â€£ 6 Existence of optimal control and convergence for general ğœ‡áµ¢â‚™áµ¢ â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")).
As a direct consequence, we establish that the optimal control of the SCSBP converges linearly with respect to the penalty parameter kk (TheoremÂ [4.1](https://arxiv.org/html/2510.11829v1#S4.ThmTheorem1 "Theorem 4.1. â€£ 4.1 The Convergence of Optimal Policies â€£ 4 Convergence Results under Delta Initial Distribution â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")). We believe that such a fixed-point perspective is novel in the literature, as it not only offers a constructive framework for characterizing solutions in the general case but also yields insights into how the penalty parameter affects the convergence rate.

### 1.2 Closely Related Literature

Our general formulation is largely inspired by Garg et al. [[29](https://arxiv.org/html/2510.11829v1#bib.bib29)], which investigates the SCSBP with the KL divergence as the penalty function GG. Within that framework, the authors established the asymptotic convergence of the optimal policies as the penalty parameter kk tends to infinity, under the assumption that the initial measure is a delta measure. However, the use of KL divergence presents practical difficulties: if the model distribution Î¼\mu assigns zero probability to any region where the data distribution Î¼tar\mu\_{\rm tar} has positive mass, then KLâ€‹(Î¼tarâˆ¥Î¼)=+âˆ\mathrm{KL}(\mu\_{\rm tar}\|\mu)=+\infty, rendering the divergence ill-posed under support mismatch [[15](https://arxiv.org/html/2510.11829v1#bib.bib15), [38](https://arxiv.org/html/2510.11829v1#bib.bib38)]. Moreover, a delta initial measure is rarely employed in generative tasks, as it lacks the diversity and randomness required for effective training. Finally, no convergence rate is quantified therein.

On a technical level, our formulation is closely related to HernÃ¡ndez-Tangpi [[35](https://arxiv.org/html/2510.11829v1#bib.bib35)], in which
the authors use a probabilistic approach to recast a mean-field SchrÃ¶dinger bridge into a stochastic optimization problem with McKean-Vlasov dynamics, and connect the optimal control to a solution to a forward backward SDE of McKean-Vlasov type (MKV FBSDE). However, given the generality of the drift, diffusion, and running cost functions, the associated MKV FBSDE is derived without a discussion of uniqueness. In fact, it is not completely obvious that a McKean-Vlasov-type SBP can be converted to a McKean-Vlasov stochastic control problem via the usual Girsanov theorem argument, as we show in Remark [2.1](https://arxiv.org/html/2510.11829v1#S2.ThmTheorem1 "Remark 2.1 (Subtlety in formulating the McKean-Vlasov version of the problem). â€£ SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") below. Moreover, the conditions imposed on the penalty function
GG are abstract and can be difficult to verify in common examples. By contrast, our framework leverages the PDE formulation and Doobâ€™s hh-transform representation, requiring only mild growth conditions on
GG and control of density gaps (see AssumptionÂ ([3.2](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem2 "Assumption 3.2. â€£ 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday"))). Several concrete examples of admissible
GG are provided in Example [3.4](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem4 "Example 3.4. â€£ 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") and [3.5](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem5 "Example 3.5. â€£ 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday").

The rest of the paper is organized as follows. Section [2](https://arxiv.org/html/2510.11829v1#S2 "2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") introduces the necessary concepts and notations. In particular, we present the connection between the underlying SBP and its stochastic control formulation, and introduce the notion of the SCSBP together with some potential applications. Section [3](https://arxiv.org/html/2510.11829v1#S3 "3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") is devoted to the existence of optimal policies for the SCSBP at each penalty parameter kk, while Section [4.1](https://arxiv.org/html/2510.11829v1#S4.SS1 "4.1 The Convergence of Optimal Policies â€£ 4 Convergence Results under Delta Initial Distribution â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") establishes that the penalized optimal policies converge to those of the original SBP as kâ†’âˆk\to\infty, with a linear rate of convergence. In addition, we prove convergence of the corresponding value functions and quantify the distance between the terminal distribution and the target distribution in terms of the Wasserstein distance. Sections [5](https://arxiv.org/html/2510.11829v1#S5 "5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") and [6](https://arxiv.org/html/2510.11829v1#S6 "6 Existence of optimal control and convergence for general ğœ‡áµ¢â‚™áµ¢ â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") are devoted to the case with a general initial distribution. A crucial stability result is established in Section [5](https://arxiv.org/html/2510.11829v1#S5 "5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday"), via the stability of SchrÃ¶dinger potentials of an auxiliary entropy optimal transport problem, and in Section [6](https://arxiv.org/html/2510.11829v1#S6 "6 Existence of optimal control and convergence for general ğœ‡áµ¢â‚™áµ¢ â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") we complete the fixed-point argument.

## 2 Preliminary

Throughout this paper, we consider a generic Euclidean space ğ•\mathbb{X}, and regardless of its dimension, we denote (â‹…,â‹…)(\cdot,\cdot) and |â‹…||\cdot| be its inner product and norm, respectively. We denote â„‚â€‹([0,T];ğ•)\mathbb{C}([0,T];\mathbb{X}) to be the space of ğ•\mathbb{X}-valued continuous functions defined on [0,T][0,T] with the usual sup-norm, and in particular, we denote â„‚Td:=â„‚â€‹([0,T];â„d)\mathbb{C}^{d}\_{T}:=\mathbb{C}([0,T];\mathbb{R}^{d}), and let â„¬â€‹(â„‚Td)\mathscr{B}(\mathbb{C}\_{T}^{d}) be its topological Borel field.
We shall consider the following
canonical probabilistic space: (Î©,â„±,â„™)(\Omega,{\cal F},\mathbb{P}), where (Î©,â„±):=(â„‚Td,â„¬â€‹(â„‚Td))(\Omega,{\cal F}):=(\mathbb{C}^{d}\_{T},\mathscr{B}(\mathbb{C}^{d}\_{T})) and â„™âˆˆğ’«â€‹(Î©)\mathbb{P}\in\mathscr{P}(\Omega), the space of all probability measures defined on (Î©,â„±)(\Omega,{\cal F}). Finally, we let â„™0âˆˆğ’«â€‹(Î©)\mathbb{P}^{0}\in\mathscr{P}(\Omega) be
the Wiener measure on (Î©,â„±)(\Omega,{\cal F}); Wtâ€‹(Ï‰):=Ï‰â€‹(t)W\_{t}(\omega):=\omega(t), Ï‰âˆˆÎ©\omega\in\Omega, the canonical process; and ğ”½0:={â„±t0}tâˆˆ[0,T]\mathbb{F}^{0}:=\{{\cal F}^{0}\_{t}\}\_{t\in[0,T]}, where â„±t0:=â„¬t(â„‚Td):=Ïƒ{Ï‰(â‹…âˆ§t):Ï‰âˆˆâ„‚Td}{\cal F}^{0}\_{t}:=\mathscr{B}\_{t}(\mathbb{C}\_{T}^{d}):=\sigma\{\omega(\cdot\wedge t):\omega\in\mathbb{C}^{d}\_{T}\}, tâˆˆ[0,T]t\in[0,T]. We assume that ğ”½0\mathbb{F}^{0} has the usual â„™0\mathbb{P}^{0}-augmentation so that it satisfies the usual hypotheses (cf. e.g., [[50](https://arxiv.org/html/2510.11829v1#bib.bib50)]), and for pâ‰¥1p\geq 1, we denote ğ•ƒğ”½0pâ€‹([0,T];ğ•)\mathbb{L}^{p}\_{\mathbb{F}^{0}}([0,T];\mathbb{X}) to be all ğ•\mathbb{X}-valued, pp-integrable, ğ”½0\mathbb{F}^{0}-adapted processes. Finally, we denote â„³â€‹(â„d)\mathscr{M}(\mathbb{R}^{d}) to be all Ïƒ\sigma-finite measures on â„d\mathbb{R}^{d} and ğ’«pâ€‹(â„d)\mathscr{P}\_{p}(\mathbb{R}^{d}) to be all probability measures with finite pp-th moment on â„d\mathbb{R}^{d} equipped with pp-Wasserstein metric, denoted by Wpâ€‹(â‹…,â‹…)W\_{p}(\cdot,\cdot).

We recall that a classical SchrÃ¶dinger Bridge Problem (SBP) amounts to finding, for
â„™âˆˆğ’«â€‹(Î©)\mathbb{P}\in\mathscr{P}(\Omega),

|  |  |  |  |
| --- | --- | --- | --- |
|  | Vâ€‹(â„™):=infâ„šâˆˆğ’«â€²DKLâ€‹(â„šâˆ¥â„™),\displaystyle V(\mathbb{P}):=\inf\_{\mathbb{Q}\in\mathscr{P}^{\prime}}D\_{\rm KL}(\mathbb{Q}\|\mathbb{P}), |  | (2.1) |

where ğ’«â€²âŠ‚ğ’«â€‹(Î©)\mathscr{P}^{\prime}\subset\mathscr{P}(\Omega) is a given admissible set, and DKL(â‹…âˆ¥â‹…)D\_{\rm KL}(\,\cdot\,\|\,\cdot\,) is the so-called Kullback-Leibler Divergence or the Relative Entropy (cf. [[39](https://arxiv.org/html/2510.11829v1#bib.bib39)]), defined by

|  |  |  |
| --- | --- | --- |
|  | DKL(â„šâˆ¥â„™):={ğ”¼â„šâ€‹[logâ¡(dâ€‹â„šdâ€‹â„™)],ifÂ â€‹dâ€‹â„šâ‰ªdâ€‹â„™;âˆ,other wise.\displaystyle D\_{\rm KL}(\mathbb{Q}\|\mathbb{P}):=\begin{cases}\mathbb{E}^{\mathbb{Q}}\big[\log\big(\frac{d\mathbb{Q}}{d\mathbb{P}}\big)\big],&{\text{if }}d\mathbb{Q}\ll d\mathbb{P};\\ \infty,&\text{other wise}.\end{cases} |  |

We remark here that the KL-divergence DKL(â‹…âˆ¥â‹…)D\_{\rm KL}(\,\cdot\,\|\,\cdot\,) can be easily extended to any Ïƒ\sigma-finite measures222In this case DKLâ€‹(â„šâˆ¥â„™):=âˆ«logâ¡(â„šâ€‹(dâ€‹x)â„™â€‹(dâ€‹x))â€‹â„šâ€‹(dâ€‹x)D\_{\rm KL}(\mathbb{Q}\|\mathbb{P}):=\int\log\big(\frac{\mathbb{Q}(dx)}{\mathbb{P}(dx)}\big)\mathbb{Q}(dx), if dâ€‹â„šâ‰ªdâ€‹â„™d\mathbb{Q}\ll d\mathbb{P}.. In what follows, we shall focus on the case when
â„š=â„™âˆ˜Xâˆ’1\mathbb{Q}=\mathbb{P}\,\circ\,X^{-1} for some â„d\mathbb{R}^{d}-valued continuous process XX defined on the canonical space with some â„™âˆˆğ’«â€‹(Î©)\mathbb{P}\in\mathscr{P}(\Omega), such that â„š0:=â„™âˆ˜X0âˆ’1=Î¼ini\mathbb{Q}\_{0}\negthinspace:=\negthinspace\mathbb{P}\circ{X\_{0}}^{-1}={\mu\_{\rm ini}}, â„šT:=â„™âˆ˜XTâˆ’1=Î¼tar\mathbb{Q}\_{T}\negthinspace:=\negthinspace\mathbb{P}\circ{X\_{T}}^{-1}\negthinspace=\negthinspace{\mu\_{\rm tar}}\negthinspace, and denote ğ’«â€²:=ğ’«â€‹(Î¼ini,Î¼tar)âŠ†ğ’«â€‹(Î©)\mathscr{P}^{\prime}:=\mathscr{P}({\mu\_{\rm ini}},{\mu\_{\rm tar}})\subseteq\mathscr{P}(\Omega) be all such â€œpath measuresâ€.

We note that if â„™=â„™0\mathbb{P}=\mathbb{P}^{0} is Wiener measure and â„š\mathbb{Q} is equivalent to â„™0\mathbb{P}^{0}, then by the Cameron-Martin-Girsanov theorem, there exists Î±âˆˆğ•ƒğ”½02â€‹([0,T];â„dÃ—d)\alpha\in\mathbb{L}^{2}\_{\mathbb{F}^{0}}([0,T];\mathbb{R}^{d\times d}), such that

|  |  |  |
| --- | --- | --- |
|  | dâ€‹â„šdâ€‹â„™0|â„±t:=â„°tâ€‹(Î±)=expâ¡{âˆ«0tÎ±sâ€‹ğ‘‘Wsâˆ’12â€‹âˆ«0t|Î±s|2â€‹ğ‘‘s},tâˆˆ[0,T],\frac{d\mathbb{Q}}{d\mathbb{P}^{0}}\Big|\_{{\cal F}\_{t}}:=\mathscr{E}\_{t}(\alpha)=\exp\Big\{\int\_{0}^{t}\alpha\_{s}dW\_{s}-\frac{1}{2}\int\_{0}^{t}|\alpha\_{s}|^{2}ds\Big\},\quad t\in[0,T], |  |

is a â„™0\mathbb{P}^{0}-martingale, and W~t:=Wtâˆ’âˆ«0tÎ±sâ€‹ğ‘‘s\widetilde{W}\_{t}:=W\_{t}-\int\_{0}^{t}\alpha\_{s}ds, tâˆˆ[0,T]t\in[0,T], is a â„š\mathbb{Q}-Brownian motion. It can then be easily calculated that

|  |  |  |  |
| --- | --- | --- | --- |
|  | DKLâ€‹(â„šâˆ¥â„™0)=12â€‹ğ”¼â„šâ€‹[âˆ«0T|Î±t|2â€‹ğ‘‘t].\displaystyle D\_{\rm KL}(\mathbb{Q}\|\mathbb{P}^{0})=\frac{1}{2}\mathbb{E}^{\mathbb{Q}}\Big[\int\_{0}^{T}|\alpha\_{t}|^{2}dt\Big]. |  | (2.2) |

#### SchrÃ¶dinger Bridge and Related Control Problem.

In light of ([2.2](https://arxiv.org/html/2510.11829v1#S2.E2 "In 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")), one can easily associate a SBP to a stochastic control problem (see, e.g., [[14](https://arxiv.org/html/2510.11829v1#bib.bib14), Â§4.4], [[17](https://arxiv.org/html/2510.11829v1#bib.bib17), Â§1] and [[40](https://arxiv.org/html/2510.11829v1#bib.bib40), p36]). Consider, for example, a standard SDE
on (Î©,â„±,â„™0)(\Omega,{\cal F},\mathbb{P}^{0}):

|  |  |  |  |
| --- | --- | --- | --- |
|  | dâ€‹Xt=bâ€‹(t,Xt)â€‹dâ€‹t+dâ€‹Wt,X0âˆ¼Î¼ini.\displaystyle dX\_{t}=b(t,X\_{t})dt+dW\_{t},\quad X\_{0}\sim{\mu\_{\rm ini}}. |  | (2.3) |

Denote â„™=â„™X:=â„™0âˆ˜Xâˆ’1âˆˆğ’«â€‹(Î©)\mathbb{P}=\mathbb{P}\_{X}:=\mathbb{P}^{0}\circ X^{-1}\in\mathscr{P}(\Omega).
Then we consider the following SBP:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Vâ€‹(Î¼ini,Î¼tar):=infâ„šâˆˆğ’«â€‹(Î¼ini,Î¼tar)DKLâ€‹(â„šâˆ¥â„™).V({\mu\_{\rm ini}},{\mu\_{\rm tar}}):=\inf\_{\mathbb{Q}\in\mathscr{P}({\mu\_{\rm ini}},{\mu\_{\rm tar}})}D\_{\rm KL}(\mathbb{Q}\|\mathbb{P}). |  | (2.4) |

Similar to ([2.2](https://arxiv.org/html/2510.11829v1#S2.E2 "In 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")), we can recast ([2.4](https://arxiv.org/html/2510.11829v1#S2.E4 "In SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) as the following stochastic control problem:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Vâ€‹(Î¼ini,Î¼tar)=infÎ±âˆˆğ’œJâ€‹(Î±)=infÎ±âˆˆğ’œğ”¼â„šâ€‹[12â€‹âˆ«0T|Î±s|2â€‹ğ‘‘s],\displaystyle V({\mu\_{\rm ini}},{\mu\_{\rm tar}})=\inf\_{\alpha\in\mathcal{A}}J(\alpha)=\inf\_{\alpha\in\mathcal{A}}\mathbb{E}^{\mathbb{Q}}\Big[\frac{1}{2}\int\_{0}^{T}|\alpha\_{s}|^{2}ds\Big], |  | (2.5) |

where â„šâˆˆğ’«â€‹(Î©)\mathbb{Q}\in\mathscr{P}(\Omega) is such that dâ€‹â„šdâ€‹â„™=â„°â€‹(Î±)\frac{d\mathbb{Q}}{d\mathbb{P}}=\mathscr{E}(\alpha) for some Î±âˆˆğ’œâŠ†ğ•ƒğ”½02â€‹([0,T])\alpha\in\mathcal{A}\subseteq\mathbb{L}^{2}\_{\mathbb{F}^{0}}([0,T]), under which the underlying controlled dynamics takes the form:

|  |  |  |  |
| --- | --- | --- | --- |
|  | dâ€‹XtÎ±=[bâ€‹(t,XtÎ±)+Î±t]â€‹dâ€‹t+dâ€‹W~t,â„šâˆ˜(X0Î±)âˆ’1=Î¼ini,â„šâˆ˜(XTÎ±)âˆ’1=Î¼tar,\displaystyle dX^{\alpha}\_{t}=[b(t,X^{\alpha}\_{t})+\alpha\_{t}]dt+d\widetilde{W}\_{t},\quad\mathbb{Q}\circ(X\_{0}^{\alpha})^{-1}={\mu\_{\rm ini}},\quad\mathbb{Q}\circ(X\_{T}^{\alpha})^{-1}={\mu\_{\rm tar}}, |  | (2.6) |

where W~\widetilde{W} is a â„š\mathbb{Q}-Brownian motion.

###### Remark 2.1 (Subtlety in formulating the McKean-Vlasov version of the problem).

It is rather tempting to apply the idea above to the so-called McKean-Vlasov SBP (MVSBP). Consider, for example,
the following McKean-Vlasov SDE on (Î©,â„±,â„™0)(\Omega,{\cal F},\mathbb{P}^{0}):

|  |  |  |
| --- | --- | --- |
|  | dâ€‹Xt=bâ€‹(t,Xt,â„™t)â€‹dâ€‹t+dâ€‹Wt,X0âˆ¼Î¼ini.\displaystyle dX\_{t}=b(t,X\_{t},\mathbb{P}\_{t})dt+dW\_{t},\quad X\_{0}\sim{\mu\_{\rm ini}}. |  |

where, again, we denote â„™=â„™0âˆ˜Xâˆ’1âˆˆğ’«2â€‹(Î©)\mathbb{P}=\mathbb{P}^{0}\circ X^{-1}\in\mathscr{P}\_{2}(\Omega), and â„™t:=â„™0âˆ˜Xtâˆ’1âˆˆğ’«2â€‹(â„d)\mathbb{P}\_{t}:=\mathbb{P}^{0}\circ X^{-1}\_{t}\in\mathscr{P}\_{2}(\mathbb{R}^{d}), tâˆˆ[0,T]t\in[0,T].
Similar to ([2.4](https://arxiv.org/html/2510.11829v1#S2.E4 "In SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) we can define an SBP, and let us
refer to it as an MVSBP. Again, by ([2.2](https://arxiv.org/html/2510.11829v1#S2.E2 "In 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")), we can recast such MVSBP as the following (weak form) stochastic control problem:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Vâ€‹(Î¼ini,Î¼tar):=infÎ±ğ”¼â„šâ€‹[âˆ«0T12â€‹|Î±t|2â€‹dâ¡t],\displaystyle V({\mu\_{\rm ini}},{\mu\_{\rm tar}}):=\inf\_{\alpha}\mathbb{E}^{\mathbb{Q}}\Big[\int\_{0}^{T}\frac{1}{2}|\alpha\_{t}|^{2}\operatorname{{\rm d}}t\Big], |  | (2.7) |

where â„šâˆˆğ’«â€‹(Î©)\mathbb{Q}\in\mathscr{P}(\Omega) is such that the underlying controlled process X=XÎ±X=X^{\alpha} satisfies, under â„š\mathbb{Q}:

|  |  |  |  |
| --- | --- | --- | --- |
|  | dâ€‹Xt=[Î±t+bâ€‹(t,Xt,â„™t)]â€‹dâ€‹t+dâ€‹W~t,X0âˆ¼Î¼ini,XTâˆ¼Î¼tar,\displaystyle dX\_{t}=[\alpha\_{t}+b(t,X\_{t},\mathbb{P}\_{t})]dt+d\widetilde{W}\_{t},\quad X\_{0}\sim{\mu\_{\rm ini}},\quad X\_{T}\sim{\mu\_{\rm tar}}, |  | (2.8) |

where W~\widetilde{W} is a â„š\mathbb{Q}-Brownian motion, and we can assume that Î±âˆˆğ•ƒğ”½02â€‹([0,T])\alpha\in\mathbb{L}^{2}\_{\mathbb{F}^{0}}([0,T]). However, by looking at ([2.8](https://arxiv.org/html/2510.11829v1#S2.E8 "In Remark 2.1 (Subtlety in formulating the McKean-Vlasov version of the problem). â€£ SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) more closely we see that the problem ([2.7](https://arxiv.org/html/2510.11829v1#S2.E7 "In Remark 2.1 (Subtlety in formulating the McKean-Vlasov version of the problem). â€£ SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) and ([2.8](https://arxiv.org/html/2510.11829v1#S2.E8 "In Remark 2.1 (Subtlety in formulating the McKean-Vlasov version of the problem). â€£ SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) do not form a McKean-Vlasov control problem, since â„™tâ‰ â„šâˆ˜(Xt)âˆ’1\mathbb{P}\_{t}\neq\mathbb{Q}\circ(X\_{t})^{-1}(!). Therefore, an MVSBP should be formulated more carefully so as to connect to an McKean-Vlasov stochastic control problem.
âˆ

Ideally, the optimal solution to the SchrÃ¶dinger bridge problem ([2.5](https://arxiv.org/html/2510.11829v1#S2.E5 "In SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday"))-([2.6](https://arxiv.org/html/2510.11829v1#S2.E6 "In SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) provides a transport map from the initial distribution Î¼ini{\mu\_{\rm ini}} to the target distribution Î¼tar{\mu\_{\rm tar}}. This transport is interpolated by a diffusion process that most closely resembles the canonical Brownian motion in the space of path measures. However, designing training algorithms to (approximately) learn the optimal solution to ([2.5](https://arxiv.org/html/2510.11829v1#S2.E5 "In SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday"))â€“([2.6](https://arxiv.org/html/2510.11829v1#S2.E6 "In SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) typically involves an iterative scheme that alternately relaxes the initial and terminal constraints [[20](https://arxiv.org/html/2510.11829v1#bib.bib20), [64](https://arxiv.org/html/2510.11829v1#bib.bib64), [66](https://arxiv.org/html/2510.11829v1#bib.bib66), [54](https://arxiv.org/html/2510.11829v1#bib.bib54)], and whose convergence rate and computational complexity in high-dimensional settings remain unclear.

While there could be techniques in stochastic control theory to deal with such a constraint, we shall follow the idea of [[35](https://arxiv.org/html/2510.11829v1#bib.bib35)], by approximating the original control problem by a family of unconstrained
McKean-Vlasov stochastic control problems with terminal penalties. More precisely, we shall allow the law of XTÎ±X^{\alpha}\_{T} in ([2.6](https://arxiv.org/html/2510.11829v1#S2.E6 "In SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) to be different from Î¼tar{\mu\_{\rm tar}}, but add a corresponding penalty function to it in the cost functional Jâ€‹(â‹…)J(\cdot).

To this end, let us still denote ğ’œâŠ†ğ’«â€‹(Î©){\cal A}\subseteq\mathscr{P}(\Omega) to be the set of all â„šâˆˆğ’«â€‹(Î©)\mathbb{Q}\in\mathscr{P}(\Omega) such that

(i) dâ€‹â„šdâ€‹â„™|â„±T=â„°â€‹(Î±)=expâ¡{âˆ«0TÎ±sâ€‹ğ‘‘Wsâˆ’12â€‹âˆ«0T|Î±s|2â€‹ğ‘‘s}\frac{d\mathbb{Q}}{d\mathbb{P}}\Big|\_{{\cal F}\_{T}}=\mathscr{E}(\alpha)=\exp\big\{\int\_{0}^{T}\alpha\_{s}dW\_{s}-\frac{1}{2}\int\_{0}^{T}|\alpha\_{s}|^{2}ds\big\}, Î±âˆˆğ•ƒğ”½02â€‹([0,T];â„dÃ—d)\alpha\in\mathbb{L}^{2}\_{\mathbb{F}^{0}}([0,T];\mathbb{R}^{d\times d});

(ii) Under â„š\mathbb{Q}, the underlying state process XX follows the dynamics:

|  |  |  |  |
| --- | --- | --- | --- |
|  | dâ€‹XtÎ±=[bâ€‹(t,XtÎ±)+Ïƒâ€‹(t)â€‹Î±t]â€‹dâ€‹t+Ïƒâ€‹(t)â€‹dâ€‹W~t,â„šâˆ˜(X0Î±)âˆ’1=Î¼ini,\displaystyle dX^{\alpha}\_{t}=[b(t,X^{\alpha}\_{t})+\sigma(t)\alpha\_{t}]dt+\sigma(t)d\widetilde{W}\_{t},\quad\mathbb{Q}\circ(X\_{0}^{\alpha})^{-1}={\mu\_{\rm ini}}, |  | (2.9) |

where W~\widetilde{W} is a â„š\mathbb{Q}-Brownian motion.

Throughout this paper, we shall make the following Standing Assumption on the coefficients bb and Ïƒ\sigma.

###### Assumption 2.2.

The coefficients b:[0,T]Ã—â„dâ†¦â„db:[0,T]\times\mathbb{R}^{d}\mapsto\mathbb{R}^{d} and Ïƒ:[0,T]â†¦â„dÃ—d\sigma:[0,T]\mapsto\mathbb{R}^{d\times d} are given deterministic continuous functions, such that there exists L>0L>0, it holds that

|  |  |  |
| --- | --- | --- |
|  | |bâ€‹(t,x)âˆ’bâ€‹(t,y)|â‰¤Lâ€‹|xâˆ’y|,tâˆˆ[0,T],x,yâˆˆâ„d.|b(t,x)-b(t,y)|\leq L|x-y|,\qquad t\in[0,T],~x,y\in\mathbb{R}^{d}. |  |

Clearly, under Assumption [2.2](https://arxiv.org/html/2510.11829v1#S2.ThmTheorem2 "Assumption 2.2. â€£ SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday"), the SDE ([2.9](https://arxiv.org/html/2510.11829v1#S2.E9 "In SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) has a unique strong solution XÎ±X^{\alpha} on (Î©,â„±,â„™0)(\Omega,{\cal F},\mathbb{P}^{0}) for any given Î±âˆˆğ•ƒğ”½01â€‹([0,T];â„dÃ—d)\alpha\in\mathbb{L}^{1}\_{\mathbb{F}^{0}}([0,T];\mathbb{R}^{d\times d}) (see [[50](https://arxiv.org/html/2510.11829v1#bib.bib50), [69](https://arxiv.org/html/2510.11829v1#bib.bib69)]). We shall often identify â„šâˆˆğ’œ\mathbb{Q}\in{\cal A} with its associated process Î±\alpha, and denote â„šâˆ¼Î±\mathbb{Q}\sim\alpha and Î±âˆˆğ’œ\alpha\in{\cal A} when the context is clear. The key element of the soft-constrained SchrÃ¶dinger Bridge Problem is the following penalty function.

###### Definition 2.3.

A smooth function Gâ€‹(â‹…)=Gâ€‹(â‹…;Î¼tar):ğ’«2â€‹(â„d)â†’[0,âˆ)G(\cdot)=G(\cdot;{\mu\_{\rm tar}}):\mathscr{P}\_{2}(\mathbb{R}^{d})\to[0,\infty) is called a smooth penalty function associated to Î¼tarâˆˆğ’«2â€‹(â„d){\mu\_{\rm tar}}\in\mathscr{P}\_{2}(\mathbb{R}^{d}) if:
Gâ€‹(Î¼;Î¼tar)=0G(\mu;{\mu\_{\rm tar}})=0 if and only if Î¼=Î¼tar\mu={\mu\_{\rm tar}}.
âˆ

Now let us introduce the following family of McKean-Vlasov-type stochastic control problems:

###### Problem 2.4 (Soft-constrained SchrÃ¶dinger Bridge Problem (SCSBP)).

For kâˆˆâ„•k\in\mathbb{N}, find Î±kâˆˆğ’œ\alpha^{k}\in{\cal A} such that Vk:=Jkâ€‹(Î±^k)=infÎ±âˆˆğ’œJkâ€‹(Î±)V^{k}:=J^{k}(\widehat{\alpha}^{k})=\inf\_{\alpha\in{\cal A}}J^{k}(\alpha), where

|  |  |  |  |
| --- | --- | --- | --- |
|  | Jkâ€‹(Î±)=ğ”¼â„šâ€‹[12â€‹âˆ«0T|Î±s|2â€‹ğ‘‘s+kâ€‹Gâ€‹(â„šXTÎ±)],\displaystyle J^{k}(\alpha)=\mathbb{E}^{\mathbb{Q}}\left[\frac{1}{2}\int\_{0}^{T}|\alpha\_{s}|^{2}ds+kG(\mathbb{Q}\_{X^{\alpha}\_{T}})\right], |  | (2.10) |

and Gâ€‹(â‹…)=Gâ€‹(â‹…;Î¼tar)G(\cdot)=G(\cdot;{\mu\_{\rm tar}}) is the given penalty function satisfying Definition [2.3](https://arxiv.org/html/2510.11829v1#S2.ThmTheorem3 "Definition 2.3. â€£ SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") and â„šâˆ¼Î±\mathbb{Q}\sim\alpha.
âˆ

#### Applications in Generative AI.

We remark that the SCSBP Problem [2.4](https://arxiv.org/html/2510.11829v1#S2.ThmTheorem4 "Problem 2.4 (Soft-constrained SchrÃ¶dinger Bridge Problem (SCSBP)). â€£ SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")
offers a general framework that can be applied to address multiple problems in generative AI. We briefly mention a few motivational examples.

###### Example 2.5 (Data generation).

The goal of generative AI is to train a data generation procedure using a finite number of iid. data samples {x1,â‹¯,xN}\{x\_{1},\cdots,x\_{N}\} under a (unknown) target distribution Î¼tar{\mu\_{\rm tar}}, in order to simulate unlimited number of data samples whose underlying distribution is close to Î¼tar{\mu\_{\rm tar}} [[58](https://arxiv.org/html/2510.11829v1#bib.bib58), [36](https://arxiv.org/html/2510.11829v1#bib.bib36), [33](https://arxiv.org/html/2510.11829v1#bib.bib33)].

To cast this problem into our framework, we can take, for example, Î¼ini=ğ’©â€‹(0,I){\mu\_{\rm ini}}=\mathcal{N}(0,I) and Î¼tar=pdata{\mu\_{\rm tar}}=p\_{\rm data} in the theoretical framework (or Î¼tar=1Nâ€‹âˆ‘i=1NÎ´xi{\mu\_{\rm tar}}=\frac{1}{N}\sum\_{i=1}^{N}\delta\_{x\_{i}} in the practical implementation). Then the optimal control Î±âˆ—\alpha^{\*} of SCSBP leads to a controlled process (XtÎ±âˆ—)0â‰¤tâ‰¤T(X\_{t}^{\alpha^{\*}})\_{0\leq t\leq T} that simulates the data output XTÎ±âˆ—X\_{T}^{\alpha^{\*}}. Our key results (see Theorem [4.1](https://arxiv.org/html/2510.11829v1#S4.ThmTheorem1 "Theorem 4.1. â€£ 4.1 The Convergence of Optimal Policies â€£ 4 Convergence Results under Delta Initial Distribution â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") and Theorem [6.4](https://arxiv.org/html/2510.11829v1#S6.ThmTheorem4 "Theorem 6.4. â€£ 6 Existence of optimal control and convergence for general ğœ‡áµ¢â‚™áµ¢ â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") below) show that the terminal measure â„šXTÎ±âˆ—\mathbb{Q}\_{X\_{T}^{\alpha^{\*}}} is close to Î¼tar{\mu\_{\rm tar}}, when kk is sufficiently large.
âˆ

###### Example 2.6 (Fine-tuning under a reward signal).

Fine-tuning a diffusion model means taking a pre-trained model and training it further on a smaller, task-specific dataset so it learns to generate outputs more suited to that new context [[60](https://arxiv.org/html/2510.11829v1#bib.bib60), [70](https://arxiv.org/html/2510.11829v1#bib.bib70), [62](https://arxiv.org/html/2510.11829v1#bib.bib62), [34](https://arxiv.org/html/2510.11829v1#bib.bib34)]. For example, a diffusion model trained on general images can be fine-tuned to generate a specific style (evaluated via a reward function). This process updates the modelâ€™s parameters just enough to adapt to the new data, without starting training from scratch.

In terms of our framework, we can consider ([2.3](https://arxiv.org/html/2510.11829v1#S2.E3 "In SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) as a pre-trained model with the drift bâ€‹(t,x):=sÎ¸^â€‹(t,x)b(t,x):=s\_{\widehat{\theta}}(t,x) being a well-trained score function, and Î¸^\widehat{\theta} is the trained parameter. Note that, as the result of pre-training, the output measure â„šXT\mathbb{Q}\_{X\_{T}} is sufficiently close to some data distribution Î¼tar{\mu\_{\rm tar}}. We then introduce a fine-tuning procedure through a reference measure prefp\_{\rm ref} with density expâ¡(râ€‹(x))âˆ«â„dexpâ¡(râ€‹(x))â€‹ğ‘‘x\frac{\exp(r(x))}{\int\_{\mathbb{R}^{d}}\exp(r(x))dx}, where r:â„dâ†’â„r:\mathbb{R}^{d}\rightarrow\mathbb{R} is a given reward function satisfying âˆ«â„dexpâ¡(râ€‹(x))â€‹ğ‘‘x<âˆ\int\_{\mathbb{R}^{d}}\exp(r(x))dx<\infty. Now replacing Î¼tar=pref{\mu\_{\rm tar}}=p\_{\rm ref}, the optimal control Î±âˆ—\alpha^{\*} of the corresponding SCSBP can then serve as the fine-tuning score function; and consequently, the new drift term bâ€‹(t,XtÎ±â£âˆ—)+Ïƒâ€‹(t)â€‹Î±âˆ—b(t,X^{\alpha\*}\_{t})+\sigma(t)\alpha^{\*} acts as a combined score function.

Clearly, in this application the penalty parameter kk should not be chosen too large; otherwise, the effect of the preference function may dominate the fidelity to the original data distribution. With an appropriately selected kk, the resulting measure â„šXTÎ±âˆ—\mathbb{Q}\_{X^{\alpha^{\*}}\_{T}} not only reflects pdatap\_{\rm data} but also integrates the reward function rr. In contrast, we remark that the classic SBP ([2.1](https://arxiv.org/html/2510.11829v1#S2.E1 "In 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) is not capable of handling this application as it has a pre-fixed target distribution.
âˆ

###### Example 2.7 (Transfer learning).

Transfer learning is a machine learning approach where knowledge gained from a â€œsource task" is reused to improve learning in a related but different â€œtarget task" [[9](https://arxiv.org/html/2510.11829v1#bib.bib9), [61](https://arxiv.org/html/2510.11829v1#bib.bib61), [48](https://arxiv.org/html/2510.11829v1#bib.bib48)]. In what follows we shall consider transfer learning in the context of data generation.

Let us consider a source task (Ysou)(Y\_{\rm sou}), characterized by a distribution psoup\_{\rm sou}, and a target task (Ytar)(Y\_{\rm tar}) with distribution ptarp\_{\rm tar}. Typically, psoup\_{\rm sou} and ptarp\_{\rm tar} are assumed to be close under a certain divergence or distance function Gâ€‹(ptar;psou)G(p\_{\rm tar};p\_{\rm sou}) (assuming Gâ‰¥0G\geq 0), such as the Wasserstein distance [[48](https://arxiv.org/html/2510.11829v1#bib.bib48)].

To fit the transfer learning into our framework, we can take Î¼ini=psou{\mu\_{\rm ini}}=p\_{\rm sou}, Î¼tar=ptar{\mu\_{\rm tar}}=p\_{\rm tar}, and set bâ‰¡0b\equiv 0 for simplicity. In this case, if we choose Î±=0\alpha=0, and X0âˆ¼psouX\_{0}\sim p\_{\rm sou}, then XT0=X0+WTâˆ¼psouâˆ—ğ’©â€‹(0,Tâ€‹ğ•€d)X^{0}\_{T}=X\_{0}+W\_{T}\sim p\_{\rm sou}\*\mathcal{N}(0,T\mathbb{I}\_{d}), where ğ’©â€‹(0,Tâ€‹ğ•€d)=â„™0âˆ˜WTâˆ’1\mathcal{N}(0,T{\mathbb{I}\_{d}})=\mathbb{P}^{0}\circ W\_{T}^{-1} and ğ•€d\mathbb{I}\_{d} denotes the dÃ—dd\times d identity matrix. Thus, denoting the optimal control by Î±^\widehat{\alpha} and noting that Î±â‰¡0\alpha\equiv 0 is sub-optimal, we must have

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â„šâ€‹[12â€‹âˆ«0T|Î±^s|2â€‹ğ‘‘s]â‰¤ğ”¼â„šâ€‹[12â€‹âˆ«0T|Î±^s|2â€‹ğ‘‘s+kâ€‹Gâ€‹(â„šXTÎ±^)]â‰¤kâ€‹Gâ€‹(psouâˆ—ğ’©â€‹(0,Tâ€‹ğ•€d);ptar).\displaystyle\mathbb{E}^{\mathbb{Q}}\left[\frac{1}{2}\int\_{0}^{T}|\widehat{\alpha}\_{s}|^{2}ds\right]\leq\mathbb{E}^{\mathbb{Q}}\left[\frac{1}{2}\int\_{0}^{T}|\widehat{\alpha}\_{s}|^{2}ds+kG(\mathbb{Q}\_{X^{\widehat{\alpha}}\_{T}})\right]\leq kG(p\_{\rm sou}\*\mathcal{N}(0,T{\mathbb{I}\_{d}});p\_{\rm tar}). |  |

This implies that the optimal control Î±^\widehat{\alpha} has a small L2L^{2}-norm, indicating only minor adjustments are required during samplingâ€”provided kk is not too large.
âˆ

## 3 Existence of Optimal Policies for SCSBPâ€™s

In this section we study the stochastic control problem ([2.9](https://arxiv.org/html/2510.11829v1#S2.E9 "In SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday"))-([2.10](https://arxiv.org/html/2510.11829v1#S2.E10 "In Problem 2.4 (Soft-constrained SchrÃ¶dinger Bridge Problem (SCSBP)). â€£ SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) and the associated soft-constrained SBP. In particular, we shall prove that the optimal control for each kâˆˆâ„•k\in\mathbb{N} exists and in next section we will show that these optimal policies will converge to the solution of the original SBP, with a linear rate of convergence. We shall assume that the target distribution for the SCSBP has a
density ftarâˆˆğ•ƒ1â€‹(â„d)f\_{\rm tar}\in\mathbb{L}^{1}(\mathbb{R}^{d}). Also, we shall assume
Ïƒâ€‹(â‹…)=Id\sigma(\cdot)=I\_{d}, that is, in what follows we assume that the underlying diffusion takes the form:

|  |  |  |  |
| --- | --- | --- | --- |
|  | dâ€‹Xt=bâ€‹(t,Xt)â€‹dâ€‹t+dâ€‹Wt,X0âˆ¼Î¼ini,tâˆˆ[0,T],\displaystyle dX\_{t}=b(t,X\_{t})dt+dW\_{t},\quad X\_{0}\sim{\mu\_{\rm ini}},\quad t\in[0,T], |  | (3.1) |

where WW is the canonical Brownian motion under â„™0\mathbb{P}^{0}. Let pâ€‹(â‹…,â‹…;â‹…,â‹…)p(\cdot,\cdot;\cdot,\cdot) denote the transition density of the solution XX, so that â„™0â€‹{Xsâˆˆdâ€‹z|Xt=x}=pâ€‹(s,z;t,x)â€‹dâ€‹z\mathbb{P}^{0}\{X\_{s}\in dz|X\_{t}=x\}=p(s,z;t,x)dz, 0â‰¤t<sâ‰¤T0\leq t<s\leq T, z,xâˆˆâ„dz,x\in\mathbb{R}^{d}. Then, it is well known that pâ€‹(â‹…,â‹…;â‹…,â‹…)p(\cdot,\cdot;\cdot,\cdot) is the fundamental solution to Kolmogorov backward (parabolic) PDE, and under mild conditions (see, e.g., [[3](https://arxiv.org/html/2510.11829v1#bib.bib3)]), there exist c1c\_{1}, c2c\_{2}, Î»\lambda, Î›>0\Lambda>0, it holds that

|  |  |  |  |
| --- | --- | --- | --- |
|  | c1â€‹(sâˆ’t)âˆ’d2â€‹eâˆ’Î»â€‹|zâˆ’x|2sâˆ’t<pâ€‹(s,z;t,x)<c2â€‹(sâˆ’t)âˆ’d2â€‹eâˆ’Î›â€‹|zâˆ’x|24â€‹(sâˆ’t).c\_{1}(s-t)^{-\frac{d}{2}}e^{-\frac{\lambda|z-x|^{2}}{s-t}}<p(s,z;t,x)<c\_{2}(s-t)^{-\frac{d}{2}}e^{-\frac{\Lambda|z-x|^{2}}{4(s-t)}}. |  | (3.2) |

Keeping the original SBP ([2.4](https://arxiv.org/html/2510.11829v1#S2.E4 "In SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) and ([2.5](https://arxiv.org/html/2510.11829v1#S2.E5 "In SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) associated with ([3.1](https://arxiv.org/html/2510.11829v1#S3.E1 "In 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) in mind, let us now recall the Problem [2.4](https://arxiv.org/html/2510.11829v1#S2.ThmTheorem4 "Problem 2.4 (Soft-constrained SchrÃ¶dinger Bridge Problem (SCSBP)). â€£ SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") and the cost functional Jkâ€‹(Î±)J^{k}(\alpha) defined by ([2.10](https://arxiv.org/html/2510.11829v1#S2.E10 "In Problem 2.4 (Soft-constrained SchrÃ¶dinger Bridge Problem (SCSBP)). â€£ SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")). For notational simplicity in what follows we shall identify â„šâˆˆğ’œ\mathbb{Q}\in{\cal A} by â„š=â„™XÎ±=â„™0âˆ˜(XÎ±)âˆ’1\mathbb{Q}=\mathbb{P}\_{X^{\alpha}}=\mathbb{P}^{0}\circ(X^{\alpha})^{-1}. Clearly, we have â„™X0=â„™X\mathbb{P}\_{X^{0}}=\mathbb{P}\_{X}, where XX solves ([3.1](https://arxiv.org/html/2510.11829v1#S3.E1 "In 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")). Furthermore, we shall denote ğ”¼â€‹[â‹…]=ğ”¼â„™0â€‹[â‹…]\mathbb{E}[\cdot]=\mathbb{E}^{\mathbb{P}^{0}}[\cdot] when context is clear, and for each kâˆˆâ„•k\in\mathbb{N} we can easily check that

|  |  |  |  |
| --- | --- | --- | --- |
|  | Jkâ€‹(Î±)=ğ”¼â€‹[12â€‹âˆ«0T|Î±s|2â€‹ğ‘‘s+kâ€‹Gâ€‹(â„™XTÎ±)]=DKLâ€‹(â„™XÎ±âˆ¥â„™X)+kâ€‹Gâ€‹(â„™XTÎ±).\displaystyle J^{k}(\alpha)=\mathbb{E}\Big[\frac{1}{2}\int\_{0}^{T}|\alpha\_{s}|^{2}ds+kG(\mathbb{P}\_{X^{\alpha}\_{T}})\Big]=D\_{\rm KL}(\mathbb{P}\_{X^{\alpha}}\|\mathbb{P}\_{X})+kG(\mathbb{P}\_{X^{\alpha}\_{T}}). |  | (3.3) |

Now let us define, for each kâˆˆâ„•k\in\mathbb{N}, a mapping Dkâ€‹(â‹…):ğ’«2â€‹(â„d)â†’â„D\_{k}(\cdot):\mathscr{P}\_{2}(\mathbb{R}^{d})\to\mathbb{R} by

|  |  |  |
| --- | --- | --- |
|  | Dkâ€‹(Î¼)=DKLâ€‹(Î¼âˆ¥â„™XT)+kâ€‹Gâ€‹(Î¼),D\_{k}(\mu)=D\_{\rm KL}(\mu\|\mathbb{P}\_{X\_{T}})+kG(\mu), |  |

and note that DKLâ€‹(â„™XÎ±âˆ¥â„™X)â‰¥DKLâ€‹(â„™XTÎ±âˆ¥â„™XT)D\_{\rm KL}(\mathbb{P}\_{X^{\alpha}}\|\mathbb{P}\_{X})\geq D\_{\rm KL}(\mathbb{P}\_{X^{\alpha}\_{T}}\|\mathbb{P}\_{X\_{T}}),
we deduce from ([3.3](https://arxiv.org/html/2510.11829v1#S3.E3 "In 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) that

|  |  |  |  |
| --- | --- | --- | --- |
|  | Jkâ€‹(Î±)â‰¥DKLâ€‹(â„™XTÎ±âˆ¥â„™XT)+kâ€‹Gâ€‹(â„™XTÎ±)=Dkâ€‹(â„™XTÎ±).\displaystyle J^{k}(\alpha)\geq D\_{\rm KL}(\mathbb{P}\_{X^{\alpha}\_{T}}\|\mathbb{P}\_{X\_{T}})+kG(\mathbb{P}\_{X^{\alpha}\_{T}})=D\_{k}(\mathbb{P}\_{X^{\alpha}\_{T}}). |  | (3.4) |

If Î±^\widehat{\alpha} is the optimal control corresponding to the original SBP, that is, â„™XTÎ±^=Î¼tar\mathbb{P}\_{X^{\widehat{\alpha}}\_{T}}={\mu\_{\rm tar}}, then by definition of the penalty function Gâ€‹(â‹…)G(\cdot), we should have Gâ€‹(Î¼tar)=0G({\mu\_{\rm tar}})=0, and therefore,

|  |  |  |
| --- | --- | --- |
|  | Dkâ€‹(â„™XTÎ±^)=Dkâ€‹(Î¼tar)=DKLâ€‹(Î¼tarâˆ¥â„™XT)+kâ€‹Gâ€‹(Î¼tar)=DKLâ€‹(Î¼tarâˆ¥â„™XT).\displaystyle D\_{k}(\mathbb{P}\_{X^{\widehat{\alpha}}\_{T}})=D\_{k}({\mu\_{\rm tar}})=D\_{\rm KL}({\mu\_{\rm tar}}\|\mathbb{P}\_{X\_{T}})+kG({\mu\_{\rm tar}})=D\_{\rm KL}({\mu\_{\rm tar}}\|\mathbb{P}\_{X\_{T}}). |  |

Throughout the rest of this section, we shall focus on the special case: Î¼ini=Î´x0{\mu\_{\rm ini}}=\delta\_{x\_{0}} for some x0âˆˆâ„dx\_{0}\in\mathbb{R}^{d}. The case with general initial distribution Î¼ini{\mu\_{\rm ini}} will be studied in Sections [5](https://arxiv.org/html/2510.11829v1#S5 "5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") and [6](https://arxiv.org/html/2510.11829v1#S6 "6 Existence of optimal control and convergence for general ğœ‡áµ¢â‚™áµ¢ â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday"). We begin with the following well-known
result from [[18](https://arxiv.org/html/2510.11829v1#bib.bib18)], which will play an important role in our discussion.

###### Lemma 3.1 ( [[18](https://arxiv.org/html/2510.11829v1#bib.bib18), Theorem 3.1]).

Let XX be a weak solution to ([3.1](https://arxiv.org/html/2510.11829v1#S3.E1 "In 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) with X0=x0âˆˆâ„dX\_{0}=x\_{0}\in\mathbb{R}^{d} (i.e., Î¼ini=Î´x0{\mu\_{\rm ini}}=\delta\_{x\_{0}}). Assume that DKLâ€‹(Î¼tarâˆ¥â„™XT)<âˆD\_{\rm KL}({\mu\_{\rm tar}}\|\mathbb{P}\_{X\_{T}})<\infty. Then, the optimal solution to the SBP ([2.4](https://arxiv.org/html/2510.11829v1#S2.E4 "In SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday"))-([2.5](https://arxiv.org/html/2510.11829v1#S2.E5 "In SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) is given by Î±^t=âˆ‡logâ¡hâ€‹(t,XtÎ±^)\widehat{\alpha}\_{t}=\nabla\log h(t,X^{\widehat{\alpha}}\_{t}), where

|  |  |  |  |
| --- | --- | --- | --- |
|  | hâ€‹(t,x)=âˆ«â„dpâ€‹(T,z;t,x)â€‹ftarâ€‹(z)pâ€‹(T,z;0,x0)â€‹ğ‘‘z:=ğ”¼â€‹[ftarâ€‹(XT)pâ€‹(T,XT;0,x0)|Xt=x],\displaystyle h(t,x)=\int\_{\mathbb{R}^{d}}p(T,z;t,x)\frac{f\_{\rm tar}(z)}{p(T,z;0,x\_{0})}dz:=\mathbb{E}\Big[\frac{f\_{\rm tar}(X\_{T})}{p(T,X\_{T};0,x\_{0})}\Big|X\_{t}=x\Big], |  | (3.5) |

for (t,x)âˆˆ[0,T]Ã—â„d(t,x)\in[0,T]\times\mathbb{R}^{d}.
âˆ

Next, we make the following assumptions on the penalty function GG:

###### Assumption 3.2.

(i) There exists some small constant Îµ>0\varepsilon>0 such that

|  |  |  |  |
| --- | --- | --- | --- |
|  | Gâ€‹(Î¼)â†’+âˆ,asâ€‹â€–Î¼â€–2+Îµâ†’+âˆ.G(\mu)\rightarrow+\infty,\quad\mathrm{as}\,\,\|\mu\|^{2+\varepsilon}\rightarrow+\infty. |  | (3.6) |

where â€–Î¼â€–p:=âˆ«â„d|x|pâ€‹Î¼â€‹(dâ€‹x)\|\mu\|^{p}:=\int\_{\mathbb{R}^{d}}|x|^{p}\mu(dx) for any p>0p>0.

(ii) There exist C,Î»>0C,\lambda>0, and a function Ï•:â„dâ†’(0,1]\phi:\mathbb{R}^{d}\to(0,1] satisfying Ï•â€‹(x)â€‹eÎ»â€‹|xâˆ’x0|2â‰¤C\phi(x)e^{\lambda|x-x\_{0}|^{2}}\leq C, such that for any Î¼âˆˆğ’«2â€‹(â„d)\mu\in\mathscr{P}\_{2}(\mathbb{R}^{d}) with density function fÎ¼f\_{\mu}, it holds that

|  |  |  |  |
| --- | --- | --- | --- |
|  | |fÎ¼â€‹(x)âˆ’ftarâ€‹(x)|â‰¤Câ€‹Ï•â€‹(x)â€‹Gâ€‹(Î¼),xâˆˆâ„d.\displaystyle|f\_{\mu}(x)-f\_{\rm tar}(x)|\leq C\phi(x)G(\mu),\quad x\in\mathbb{R}^{d}. |  | (3.7) |

###### Remark 3.3.

(i) The function Gâ€‹(Î¼)G(\mu) on the right hand side of ([3.7](https://arxiv.org/html/2510.11829v1#S3.E7 "In Assumption 3.2. â€£ 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) should read |Gâ€‹(Î¼)âˆ’Gâ€‹(Î¼tar)||G(\mu)-G({\mu\_{\rm tar}})|, as Gâ€‹(Î¼tar)=0G({\mu\_{\rm tar}})=0, which essentially states that if Î¼\mu is close to Î¼tar{\mu\_{\rm tar}} in terms of GG, then fÎ¼f\_{\mu} is close to ftarf\_{\rm tar} in ğ•ƒ1\mathbb{L}^{1}.

(ii) A slightly stronger consequence of ([3.7](https://arxiv.org/html/2510.11829v1#S3.E7 "In Assumption 3.2. â€£ 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) is the following. Recall the (generalized) Kantorovich and Rubinstein dual representation (cf. e.g., [[23](https://arxiv.org/html/2510.11829v1#bib.bib23)]): denoting Lip(1)(1) to be all Lipschitz functions Ï†:â„dâ†’â„\varphi:\mathbb{R}^{d}\to\mathbb{R} with Lipschitz constant Lipâ‰¤Ï†1{}\_{\varphi}\leq 1 (hence |Ï†â€‹(x)|â‰¤Câ€‹(1+|x|)|\varphi(x)|\leq C(1+|x|)), then it holds that

|  |  |  |
| --- | --- | --- |
|  | W1â€‹(Î¼,Î¼tar)=supÏ†âˆˆLâ€‹iâ€‹pâ€‹(1){âˆ«â„dÏ†â€‹(x)â€‹(fÎ¼â€‹(x)âˆ’ftarâ€‹(x))â€‹ğ‘‘x}â‰¤Câ€‹Gâ€‹(Î¼)â€‹âˆ«â„d(1+|x|)â€‹Ï•â€‹(x)â€‹ğ‘‘x.\displaystyle W\_{1}(\mu,{\mu\_{\rm tar}})=\sup\_{\varphi\in Lip(1)}\left\{\int\_{\mathbb{R}^{d}}\varphi(x)(f\_{\mu}(x)-f\_{\rm tar}(x))dx\right\}\leq CG(\mu)\int\_{\mathbb{R}^{d}}(1+|x|)\phi(x)dx. |  |

This suggests that Gâ€‹(Î¼)âˆ¼0G(\mu)\sim 0 implies that Î¼\mu is close to Î¼tar{\mu\_{\rm tar}} in the sense of W1W\_{1}.
âˆ

Before we proceed, let us give two examples that justify Assumption [3.2](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem2 "Assumption 3.2. â€£ 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday").

###### Example 3.4.

We consider the class of Î¼\mu and Î¼tar{\mu\_{\rm tar}} such that

|  |  |  |
| --- | --- | --- |
|  | Gâ€‹(Î¼):=âˆ«â„|x|pâ€‹|fÎ¼â€‹(x)âˆ’ftarâ€‹(x)|â€‹ğ‘‘x,G(\mu):=\int\_{\mathbb{R}}|x|^{p}|f\_{\mu}(x)-f\_{\rm tar}(x)|dx, |  |

is well-defined for a given pp.
Clearly, Definition [2.3](https://arxiv.org/html/2510.11829v1#S2.ThmTheorem3 "Definition 2.3. â€£ SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")-(i) and Assumption [3.2](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem2 "Assumption 3.2. â€£ 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")-(i) are satisfied when p>2p>2. Assumption [3.2](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem2 "Assumption 3.2. â€£ 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")-(i) holds when

|  |  |  |
| --- | --- | --- |
|  | Ï•â€‹(x)â‰¤|fÎ¼â€‹(x)âˆ’ftarâ€‹(x)|câ€‹âˆ«â„|x|pâ€‹|fÎ¼â€‹(x)âˆ’ftarâ€‹(x)|â€‹ğ‘‘x,\displaystyle\phi(x)\leq\frac{|f\_{\mu}(x)-f\_{\rm tar}(x)|}{c\int\_{\mathbb{R}}|x|^{p}|f\_{\mu}(x)-f\_{\rm tar}(x)|dx}, |  |

for all Î¼\mu in the collection one may consider.
âˆ

Another natural example of GG satisfying Assumption [3.2](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem2 "Assumption 3.2. â€£ 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") would be the Wasserstein distance or the KL divergence, augmented with a small â€œguardrailâ€ term that enforces the uniform (weighted) pointwise control in ([3.7](https://arxiv.org/html/2510.11829v1#S3.E7 "In Assumption 3.2. â€£ 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")). This guardrail can be taken as a weighted LâˆL\_{\infty} norm, a HÃ¶lder â„‚Î±\mathbb{C}^{\alpha} seminorm, or an RKHS norm (e.g., with kernel kâ€‹(x,y)=Ï•â€‹(x)â€‹Ï•â€‹(y)â€‹Îºâ€‹(xâˆ’y)k(x,y)=\phi(x)\phi(y)\kappa(x-y)). In next example we illustrate such a choice with the Wasserstein distance plus an LâˆL\_{\infty} guardrail.

###### Example 3.5.

Consider the case that Î¼tarâˆˆğ’«pâ€‹(â„d){\mu\_{\rm tar}}\in\mathscr{P}\_{p}(\mathbb{R}^{d}) with p>2p>2.
We define, for c>0c>0 and Ï•â€‹(x)=expâ¡(âˆ’Î»â€‹|xâˆ’x0|2)\phi(x)=\exp(-\lambda|x-x\_{0}|^{2}) with some x0âˆˆâ„dx\_{0}\in\mathbb{R}^{d},

|  |  |  |
| --- | --- | --- |
|  | Gâ€‹(Î¼):=W2â€‹(Î¼,Î¼tar)+câ€‹â€–fÎ¼âˆ’ftarÏ•â€–Lâˆ.\displaystyle G(\mu):=W\_{2}(\mu,{\mu\_{\rm tar}})+c\Big\|\frac{f\_{\mu}-f\_{\rm tar}}{\phi}\Big\|\_{L^{\infty}}. |  |

Then, it is easy to check that

|  |  |  |
| --- | --- | --- |
|  | |fÎ¼â€‹(x)âˆ’ftarâ€‹(x)|â‰¤â€–fÎ¼âˆ’ftarÏ•â€–Lâˆâ€‹Ï•â€‹(x)â‰¤1câ€‹Ï•â€‹(x)â€‹Gâ€‹(Î¼).\displaystyle|f\_{\mu}(x)-f\_{\rm tar}(x)|\leq\Big\|\frac{f\_{\mu}-f\_{\rm tar}}{\phi}\Big\|\_{L^{\infty}}\phi(x)\leq\frac{1}{c}\phi(x)G(\mu). |  |

Thus ([3.7](https://arxiv.org/html/2510.11829v1#S3.E7 "In Assumption 3.2. â€£ 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) holds and Ï•â€‹(x)â€‹eÎ»â€‹|xâˆ’x0|2â‰¤C\phi(x)e^{\lambda|x-x\_{0}|^{2}}\leq C holds with C=maxâ¡{1,1c}C=\max\{1,\frac{1}{c}\}.

Let {Î¼n}nâ‰¥1âŠ‚ğ’«2â€‹(â„d)\{\mu\_{n}\}\_{n\geq 1}\subset\mathscr{P}\_{2}(\mathbb{R}^{d}) with â€–Î¼nâ€–2+Îµâ†’âˆ\|\mu\_{n}\|^{2+\varepsilon}\rightarrow\infty. We claim that Gâ€‹(Î¼n)G(\mu\_{n}) must be unbounded. Indeed, suppose not. Then there exists M,Mâ€²>0M,M^{\prime}>0 such that
W22â€‹(Î¼n,Î¼tar)â‰¤MW\_{2}^{2}(\mu\_{n},{\mu\_{\rm tar}})\leq M and â€–Ï•âˆ’1â€‹(fÎ¼nâˆ’ftar)â€–Lâˆ<Mâ€²\|\phi^{-1}(f\_{\mu\_{n}}-f\_{\rm tar})\|\_{L^{\infty}}<M^{\prime} for all nâˆˆâ„•+n\in\mathbb{N}\_{+}. Hence fÎ¼nâ€‹(x)â‰¤ftarâ€‹(x)+Mâ€²â€‹Ï•â€‹(x)f\_{\mu\_{n}}(x)\leq f\_{\rm tar}(x)+M^{\prime}\phi(x), xâˆˆâ„dx\in\mathbb{R}^{d}.
Integrating against |x|2+Îµ|x|^{2+\varepsilon} and using the facts that Î¼tarâˆˆğ’«2+Îµâ€‹(â„d){\mu\_{\rm tar}}\in\mathscr{P}\_{2+\varepsilon}(\mathbb{R}^{d}) with Îµ=pâˆ’2>0\varepsilon=p-2>0 and âˆ«|x|2+Îµâ€‹Ï•â€‹(x)â€‹ğ‘‘x<âˆ\int|x|^{2+\varepsilon}\phi(x)dx<\infty, we have

|  |  |  |
| --- | --- | --- |
|  | â€–Î¼nâ€–2+Îµâ‰¤â€–Î¼tarâ€–2+Îµ+Mâ€²â€‹âˆ«|x|2+Îµâ€‹Ï•â€‹(x)â€‹ğ‘‘x<âˆ,nâˆˆâ„•.\displaystyle\|\mu\_{n}\|^{2+\varepsilon}\leq\|{\mu\_{\rm tar}}\|^{2+\varepsilon}+M^{\prime}\int|x|^{2+\varepsilon}\phi(x)dx<\infty,\qquad n\in\mathbb{N}. |  |

This contradicts the fact that â€–Î¼nâ€–2+Îµâ†’âˆ\|\mu\_{n}\|^{2+\varepsilon}\rightarrow\infty, proving the claim. Hence
([3.6](https://arxiv.org/html/2510.11829v1#S3.E6 "In Assumption 3.2. â€£ 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) holds.
âˆ

We are now ready to investigate the existence of optimal control of Problem [2.4](https://arxiv.org/html/2510.11829v1#S2.ThmTheorem4 "Problem 2.4 (Soft-constrained SchrÃ¶dinger Bridge Problem (SCSBP)). â€£ SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") for each kâˆˆâ„•k\in\mathbb{N}, which would be essential for our approximation scheme. Recall that in the rest of the section we assume that Î¼ini=Î´x0{\mu\_{\rm ini}}=\delta\_{x\_{0}} for some x0âˆˆâ„dx\_{0}\in\mathbb{R}^{d}.
To begin with, we first claim that for each kâˆˆâ„•k\in\mathbb{N}, there exists Î¼^kâˆˆğ’«2â€‹(â„d)\widehat{\mu}\_{k}\in\mathscr{P}\_{2}(\mathbb{R}^{d}) such that the static optimization problem on the measure space has a solution

|  |  |  |  |
| --- | --- | --- | --- |
|  | Dkâ€‹(Î¼^k)=infÎ¼âˆˆğ’«2â€‹(â„d)Dkâ€‹(Î¼).\displaystyle D\_{k}(\widehat{\mu}\_{k})=\inf\_{\mu\in\mathscr{P}\_{2}(\mathbb{R}^{d})}D\_{k}(\mu). |  | (3.8) |

Indeed, let XX be the solution to uncontrolled SDE ([2.6](https://arxiv.org/html/2510.11829v1#S2.E6 "In SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")), and Î¼tar{\mu\_{\rm tar}} be given such that DKLâ€‹(Î¼tarâˆ¥â„™XT)<âˆD\_{\rm KL}({\mu\_{\rm tar}}\|\mathbb{P}\_{X\_{T}})<\infty. Since Gâ€‹(Î¼tar)=0G({\mu\_{\rm tar}})=0, we have

|  |  |  |
| --- | --- | --- |
|  | m:=Dkâ€‹(Î¼tar)=DKLâ€‹(Î¼tarâˆ¥â„™XT)+kâ€‹Gâ€‹(Î¼tar)=DKLâ€‹(Î¼tarâˆ¥â„™XT)<âˆ.\displaystyle m:=D\_{k}({\mu\_{\rm tar}})=D\_{\rm KL}({\mu\_{\rm tar}}\|\mathbb{P}\_{X\_{T}})+kG({\mu\_{\rm tar}})=D\_{\rm KL}({\mu\_{\rm tar}}\|\mathbb{P}\_{X\_{T}})<\infty. |  |

Next, let us define, for fixed kâˆˆâ„•k\in\mathbb{N}, a set

|  |  |  |
| --- | --- | --- |
|  | ğ’®k:={Î¼âˆˆğ’«2â€‹(â„d):Dkâ€‹(Î¼)â‰¤m}.\displaystyle\mathcal{S}\_{k}:=\Big\{\mu\in\mathscr{P}\_{2}(\mathbb{R}^{d}):D\_{k}(\mu)\leq m\Big\}. |  |

Clearly, ğ’®kâ‰ âˆ…\mathcal{S}\_{k}\neq\emptyset since Î¼tarâˆˆğ’®k{\mu\_{\rm tar}}\in\mathcal{S}\_{k}, and by ([3.6](https://arxiv.org/html/2510.11829v1#S3.E6 "In Assumption 3.2. â€£ 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")), there exists Mk>0M\_{k}>0 such that
â€–Î¼â€–2+Îµâ‰¤Mk\|\mu\|^{2+\varepsilon}\leq M\_{k}, for all Î¼âˆˆğ’®k\mu\in\mathcal{S}\_{k}. Thus ğ’®k{\cal S}\_{k} is uniformly integrable in ğ•ƒ2\mathbb{L}^{2}. Now let {Î¼k(i)}i=1âˆâŠ‚ğ’«2â€‹(â„d)\{\mu\_{k}^{(i)}\}\_{i=1}^{\infty}\subset\mathscr{P}\_{2}(\mathbb{R}^{d}) be a minimizing sequence, namely,

|  |  |  |
| --- | --- | --- |
|  | limiâ†’âˆDkâ€‹(Î¼k(i))=infÎ¼âˆˆğ’«2â€‹(â„d)Dkâ€‹(Î¼).\displaystyle\lim\_{i\rightarrow\infty}D\_{k}(\mu\_{k}^{(i)})=\inf\_{\mu\in\mathscr{P}\_{2}(\mathbb{R}^{d})}D\_{k}(\mu). |  |

Since infÎ¼âˆˆğ’«2â€‹(â„d)Dkâ€‹(Î¼)â‰¤Dkâ€‹(Î¼tar)=m\inf\_{\mu\in\mathscr{P}\_{2}(\mathbb{R}^{d})}D\_{k}(\mu)\leq D\_{k}({\mu\_{\rm tar}})=m, we may assume without loss of generality that

{Î¼k(i)}i=1âˆâŠ‚ğ’®k\{\mu\_{k}^{(i)}\}\_{i=1}^{\infty}\subset\mathcal{S}\_{k}. Since ğ’®k\mathcal{S}\_{k} is uniformly integrable and is tight
in ğ’«2â€‹(â„d)\mathscr{P}\_{2}(\mathbb{R}^{d}), there exists subsequence {Î¼k(il)}l=1âˆ\{\mu\_{k}^{(i\_{l})}\}\_{l=1}^{\infty} such that
Î¼^k:=limlâ†’âˆÎ¼k(il)âˆˆğ’«2â€‹(â„d)\widehat{\mu}\_{k}:=\lim\_{l\rightarrow\infty}\mu\_{k}^{(i\_{l})}\in\mathscr{P}\_{2}(\mathbb{R}^{d})333This follows from the result on
Wasserstein distance vs. weak convergence (see, e.g., [[65](https://arxiv.org/html/2510.11829v1#bib.bib65), Theorem 7.12]), which states that Wpâ€‹(Î¼k,Î¼)â†’0W\_{p}(\mu\_{k},\mu)\to 0 if and only if Î¼kâ†’Î¼\mu\_{k}\to\mu weakly, and limRâ†’âˆlimÂ¯kâ†’âˆâˆ«{dâ€‹(x,x0)â‰¥R}dâ€‹(x,x0)pâ€‹Î¼kâ€‹(dâ€‹x)=0\lim\_{R\to\infty}\mathop{\overline{\rm lim}}\_{k\to\infty}\int\_{\{d(x,x\_{0})\geq R\}}d(x,x\_{0})^{p}\mu\_{k}(dx)=0..
Since the mapping Î¼â†¦Dkâ€‹(Î¼)\mu\mapsto D\_{k}(\mu) is continuous, we have

|  |  |  |
| --- | --- | --- |
|  | Dkâ€‹(Î¼^k)=DKLâ€‹(Î¼^kâˆ¥â„™XT)+kâ€‹Gâ€‹(Î¼^k)=infÎ¼âˆˆğ’«2â€‹(â„d)Dkâ€‹(Î¼),\displaystyle D\_{k}(\widehat{\mu}\_{k})=D\_{\rm KL}(\widehat{\mu}\_{k}\|\mathbb{P}\_{X\_{T}})+kG(\widehat{\mu}\_{k})=\inf\_{\mu\in\mathscr{P}\_{2}(\mathbb{R}^{d})}D\_{k}(\mu), |  |

proving the claim. Furthermore, if we denote the density of â„™XT\mathbb{P}\_{X\_{T}} by fXTf\_{X\_{T}}, and note that DKLâ€‹(Î¼^k;â„™XT)â‰¤Dkâ€‹(Î¼^k)â‰¤m<âˆD\_{\rm KL}(\widehat{\mu}\_{k};\mathbb{P}\_{X\_{T}})\leq D\_{k}(\widehat{\mu}\_{k})\leq m<\infty, we know that dâ€‹Î¼^kdâ€‹â„™XT\frac{d\widehat{\mu}\_{k}}{d\mathbb{P}\_{X\_{T}}} exists and

|  |  |  |
| --- | --- | --- |
|  | dâ€‹Î¼^kdâ€‹x(x)=dâ€‹Î¼^kdâ€‹â„™XTâ‹…fXT(x)=:fk(x).\frac{d\widehat{\mu}\_{k}}{dx}(x)=\frac{d\widehat{\mu}\_{k}}{d\mathbb{P}\_{X\_{T}}}\cdot f\_{X\_{T}}(x)=:f\_{k}(x). |  |

Keeping the above discussion in mind, we are now ready to prove the following theorem.

###### Proposition 3.6.

Assume that Assumption [3.2](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem2 "Assumption 3.2. â€£ 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") is in force, and that Î¼ini=Î´x0{\mu\_{\rm ini}}=\delta\_{x\_{0}}, x0âˆˆâ„dx\_{0}\in\mathbb{R}^{d}.
Then, for each kâˆˆâ„•k\in\mathbb{N}, the optimal control for Problem [2.4](https://arxiv.org/html/2510.11829v1#S2.ThmTheorem4 "Problem 2.4 (Soft-constrained SchrÃ¶dinger Bridge Problem (SCSBP)). â€£ SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday"), denoted by Î±^k\widehat{\alpha}^{k}, exists. Furthermore, Î±^k\widehat{\alpha}^{k} has the following explicit feedback form: Î±^tk:=âˆ‡logâ¡hkâ€‹(t,XtÎ±^k)\widehat{\alpha}^{k}\_{t}:=\nabla\log h^{k}(t,X^{\widehat{\alpha}^{k}}\_{t}), where

|  |  |  |  |
| --- | --- | --- | --- |
|  | hkâ€‹(t,x)=âˆ«â„dfkâ€‹(z)pâ€‹(T,z;0,x0)â€‹pâ€‹(T,z;t,x)â€‹ğ‘‘z=ğ”¼â€‹[fkâ€‹(XT)pâ€‹(T,XT;0,x0)|Xt=x].\displaystyle h^{k}(t,x)=\int\_{\mathbb{R}^{d}}\frac{f\_{k}(z)}{p(T,z;0,x\_{0})}p(T,z;t,x)dz=\mathbb{E}\Big[\frac{f\_{k}(X\_{T})}{p(T,X\_{T};0,x\_{0})}\Big|X\_{t}=x\Big]. |  | (3.9) |

###### Proof.

Let kâˆˆâ„•k\in\mathbb{N} be fixed, and let Î¼^k\widehat{\mu}\_{k} be the minimizer of Dkâ€‹(â‹…)D\_{k}(\cdot) defined by ([3.8](https://arxiv.org/html/2510.11829v1#S3.E8 "In 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")). Then, by ([3.4](https://arxiv.org/html/2510.11829v1#S3.E4 "In 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")), for
any Î±âˆˆğ•ƒğ”½02â€‹([0,T])\alpha\in\mathbb{L}^{2}\_{\mathbb{F}^{0}}([0,T]), we have

|  |  |  |
| --- | --- | --- |
|  | Jkâ€‹(Î±)â‰¥Dkâ€‹(â„™XTÎ±)â‰¥Dkâ€‹(Î¼^k),\displaystyle J^{k}(\alpha)\geq D\_{k}(\mathbb{P}\_{X^{\alpha}\_{T}})\geq D\_{k}(\widehat{\mu}\_{k}), |  |

Therefore, in order to find the optimal control for Problem [2.4](https://arxiv.org/html/2510.11829v1#S2.ThmTheorem4 "Problem 2.4 (Soft-constrained SchrÃ¶dinger Bridge Problem (SCSBP)). â€£ SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") , it suffices to find Î±^k\widehat{\alpha}^{k} such that (i) XTÎ±^kâˆ¼Î¼^k\ X^{\widehat{\alpha}^{k}}\_{T}\sim\widehat{\mu}\_{k}; and (ii) Jkâ€‹(Î±^k)=Dkâ€‹(â„™XTÎ±^k)J^{k}(\widehat{\alpha}^{k})=D\_{k}(\mathbb{P}\_{X^{\widehat{\alpha}^{k}}\_{T}}).

To this end, we first recall that Î¼^k\widehat{\mu}\_{k} is the minimizer of the function Dkâ€‹(â‹…)D\_{k}(\cdot) with density fkf\_{k}. Next, we apply Lemma [3.1](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem1 "Lemma 3.1 ( [18, Theorem 3.1]). â€£ 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") with Î¼tar{\mu\_{\rm tar}} being replaced by Î¼^k\widehat{\mu}\_{k} to get the optimal control Î±^kâˆˆğ’«â€‹(Î¼ini,Î¼^k)\widehat{\alpha}^{k}\in\mathscr{P}({\mu\_{\rm ini}},\widehat{\mu}\_{k}) for the original SBP ([2.5](https://arxiv.org/html/2510.11829v1#S2.E5 "In SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday"))-([2.6](https://arxiv.org/html/2510.11829v1#S2.E6 "In SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")), which satisfies Î±^tk=âˆ‡logâ¡hkâ€‹(t,XtÎ±^k)\widehat{\alpha}^{k}\_{t}=\nabla\log h^{k}(t,X^{\widehat{\alpha}^{k}}\_{t}), where hkh^{k} is defined by ([3.9](https://arxiv.org/html/2510.11829v1#S3.E9 "In Proposition 3.6. â€£ 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")), and XTÎ±^kâˆ¼Î¼^k\ X^{\widehat{\alpha}^{k}}\_{T}\sim\widehat{\mu}\_{k}. Now, note that for this SBP we have

|  |  |  |
| --- | --- | --- |
|  | Vâ€‹(Î¼ini,Î¼^k)=12â€‹ğ”¼â€‹[âˆ«0T|Î±^sk|2â€‹ğ‘‘s]=DKLâ€‹(Î¼^kâˆ¥â„™XT),V({\mu\_{\rm ini}},\widehat{\mu}\_{k})=\frac{1}{2}\mathbb{E}\Big[\int\_{0}^{T}|\widehat{\alpha}^{k}\_{s}|^{2}ds\Big]=D\_{\rm KL}(\widehat{\mu}\_{k}\|\mathbb{P}\_{X\_{T}}), |  |

we conclude that

|  |  |  |
| --- | --- | --- |
|  | Jkâ€‹(Î±^k)=DKLâ€‹(Î¼^kâˆ¥â„™XT)+kâ€‹Gâ€‹(Î¼^k)=Dkâ€‹(Î¼^k)=Dkâ€‹(â„™XTÎ±^k).J^{k}(\widehat{\alpha}^{k})=D\_{\rm KL}(\widehat{\mu}\_{k}\|\mathbb{P}\_{X\_{T}})+kG(\widehat{\mu}\_{k})=D\_{k}(\widehat{\mu}\_{k})=D\_{k}(\mathbb{P}\_{X^{\widehat{\alpha}^{k}}\_{T}}). |  |

In other words, Î±^k\widehat{\alpha}^{k} is indeed the optimal control for the Problem [2.4](https://arxiv.org/html/2510.11829v1#S2.ThmTheorem4 "Problem 2.4 (Soft-constrained SchrÃ¶dinger Bridge Problem (SCSBP)). â€£ SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday"), proving the proposition.
âˆ

## 4 Convergence Results under Delta Initial Distribution

We make the following two observations. First, if we denote gâ€‹(x):=ftarâ€‹(x)pâ€‹(T,x;0,x0)g(x):=\frac{f\_{\rm tar}(x)}{p(T,x;0,x\_{0})}, then by ([3.5](https://arxiv.org/html/2510.11829v1#S3.E5 "In Lemma 3.1 ( [18, Theorem 3.1]). â€£ 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) we can write hâ€‹(t,x)=ğ”¼t,xâ€‹[gâ€‹(XT)]:=ğ”¼â€‹[gâ€‹(XT)|Xt=x]h(t,x)=\mathbb{E}\_{t,x}[g(X\_{T})]:=\mathbb{E}[g(X\_{T})|X\_{t}=x], where XX is the solution to ([3.1](https://arxiv.org/html/2510.11829v1#S3.E1 "In 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) with X0=x0X\_{0}=x\_{0}.
By Feynman-Kac formula, we see that hh satisfy the PDE:

|  |  |  |  |
| --- | --- | --- | --- |
|  | {âˆ‚thâ€‹(t,x)+â„’tâ€‹hâ€‹(t,x)=0;hâ€‹(T,x)=gâ€‹(x)=ftarâ€‹(x)pâ€‹(T,x;0,x0),\displaystyle\begin{cases}\partial\_{t}h(t,x)+\mathscr{L}\_{t}h(t,x)=0;\\ h(T,x)=g(x)=\frac{f\_{\rm tar}(x)}{p(T,x;0,x\_{0})},\end{cases} |  | (4.1) |

where the infinitesimal generator â„’t\mathscr{L}\_{t} is defined by â„’t:=bâ€‹(t,x)â‹…âˆ‡+12â€‹Î”\mathscr{L}\_{t}:=b(t,x)\cdot\nabla+\frac{1}{2}\Delta. Similarly, we define gkâ€‹(x):=fkâ€‹(x)pâ€‹(T,x;0,x0)g\_{k}(x):=\frac{f\_{k}(x)}{p(T,x;0,x\_{0})}, then the function
hkâ€‹(t,x)h^{k}(t,x) can also be represented as the solution of the PDE:

|  |  |  |  |
| --- | --- | --- | --- |
|  | {âˆ‚thkâ€‹(t,x)+â„’tâ€‹hkâ€‹(t,x)=0;hkâ€‹(T,x)=gkâ€‹(x)=fkâ€‹(x)pâ€‹(T,x;0,x0).\displaystyle\begin{cases}\partial\_{t}h^{k}(t,x)+\mathscr{L}\_{t}h^{k}(t,x)=0;\\ h^{k}(T,x)=g\_{k}(x)=\frac{f\_{k}(x)}{p(T,x;0,x\_{0})}.\end{cases} |  | (4.2) |

Recall that Î¼^kâˆˆğ’®k\widehat{\mu}\_{k}\in{\cal S}\_{k}, we have kâ€‹Gâ€‹(Î¼^k)â‰¤Dkâ€‹(Î¼^k)â‰¤mkG(\widehat{\mu}\_{k})\leq D\_{k}(\widehat{\mu}\_{k})\leq m, or

|  |  |  |  |
| --- | --- | --- | --- |
|  | Gâ€‹(Î¼^k)â‰¤m/k.G(\widehat{\mu}\_{k})\leq m/k. |  | (4.3) |

Then Assumption [3.2](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem2 "Assumption 3.2. â€£ 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")-(ii) amounts to saying that |fkâ€‹(x)âˆ’ftarâ€‹(x)|â‰¤Câ€²k|f\_{k}(x)-f\_{\rm tar}(x)|\leq\frac{C^{\prime}}{k} with constant Câ€²=Câ€‹mâ€‹MC^{\prime}=CmM.
In other words, for all xâˆˆâ„dx\in\mathbb{R}^{d}, as kâ†’âˆk\to\infty we have

|  |  |  |
| --- | --- | --- |
|  | {|gâ€‹(x)âˆ’gkâ€‹(x)|=|hâ€‹(T,x)âˆ’hkâ€‹(T,x)|=1pâ€‹(T,x;0,x0)â€‹|ftarâ€‹(x)âˆ’fkâ€‹(x)|â†’0|hâ€‹(t,x)âˆ’hkâ€‹(t,x)|â‰¤âˆ«â„dpâ€‹(T,z;t,x)pâ€‹(T,z;0,x0)â€‹|ftarâ€‹(z)âˆ’fkâ€‹(z)|â€‹ğ‘‘zâ†’0,\displaystyle\left\{\begin{array}[]{lll}\displaystyle|g(x)-g\_{k}(x)|=|h(T,x)-h^{k}(T,x)|=\frac{1}{p(T,x;0,x\_{0})}|f\_{\rm tar}(x)-f\_{k}(x)|\to 0\vskip 6.0pt plus 2.0pt minus 2.0pt\\ \displaystyle|h(t,x)-h^{k}(t,x)|\leq\int\_{\mathbb{R}^{d}}\frac{p(T,z;t,x)}{p(T,z;0,x\_{0})}|f\_{\rm tar}(z)-f\_{k}(z)|dz\to 0,\end{array}\right. |  |

We shall use these facts to study the convergence of the optimal policies in the next subsection.

### 4.1 The Convergence of Optimal Policies

We shall now argue that the optimal controls for Problem [2.4](https://arxiv.org/html/2510.11829v1#S2.ThmTheorem4 "Problem 2.4 (Soft-constrained SchrÃ¶dinger Bridge Problem (SCSBP)). â€£ SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday"), {Î±^kâ€‹(â‹…,â‹…)}\{\widehat{\alpha}^{k}(\cdot,\cdot)\}, given by Proposition [3.6](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem6 "Proposition 3.6. â€£ 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday"), actually converges to the solution of the original SBP Î±^â€‹(â‹…,â‹…)\widehat{\alpha}(\cdot,\cdot) given by Lemma [3.1](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem1 "Lemma 3.1 ( [18, Theorem 3.1]). â€£ 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday"), and also establish its rate of convergence. More precisely, we have the following theorem.

###### Theorem 4.1.

Assume that the Assumption [3.2](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem2 "Assumption 3.2. â€£ 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") is in force, and that Î¼ini=Î´x0{\mu\_{\rm ini}}=\delta\_{x\_{0}} for some x0âˆˆâ„dx\_{0}\in\mathbb{R}^{d}.
Furthermore, assume that there exists constants C,Î´>0C,\delta>0, such that Î´â‰¤gâ€‹(x),gkâ€‹(x)â‰¤C\delta\leq g(x),g\_{k}(x)\leq C, xâˆˆâ„dx\in\mathbb{R}^{d}, kâˆˆâ„•k\in\mathbb{N}. Let Î±^â€‹(t,x)\widehat{\alpha}(t,x) and Î±^kâ€‹(t,x)\widehat{\alpha}^{k}(t,x), (t,x)âˆˆ[0,T]Ã—â„d(t,x)\in[0,T]\times\mathbb{R}^{d} be the optimal controls given in Lemma [3.1](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem1 "Lemma 3.1 ( [18, Theorem 3.1]). â€£ 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") and Proposition [3.6](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem6 "Proposition 3.6. â€£ 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday"), respectively. Then, it holds that

|  |  |  |
| --- | --- | --- |
|  | âˆ«0T|Î±^kâ€‹(t,x)âˆ’Î±^â€‹(t,x)|â€‹ğ‘‘tâ‰¤Ck,xâˆˆâ„d,\int\_{0}^{T}|\widehat{\alpha}^{k}(t,x)-\widehat{\alpha}(t,x)|dt\leq\frac{C}{k},\qquad x\in\mathbb{R}^{d}, |  |

where C>0C>0 is some constant independent of kk.

###### Remark 4.2.

We note that the assumption Î´â‰¤gâ€‹(x)=ftarâ€‹(x)Pâ€‹(T,x;0,x0)â‰¤C\delta\leq g(x)=\frac{f\_{\rm tar}(x)}{P(T,x;0,x\_{0})}\leq C (resp. Î´â‰¤gkâ€‹(x)â‰¤C\delta\leq g\_{k}(x)\leq C) amounts to saying that ftarâ€‹(x)f\_{\rm tar}(x) (resp. fkâ€‹(x)f\_{k}(x)) âˆPâ€‹(T,x;0,x0)\propto\,P(T,x;0,x\_{0}) as xâ†’âˆx\to\infty, which is not particularly a stringent condition in light of the general estimate ([3.2](https://arxiv.org/html/2510.11829v1#S3.E2 "In 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")), and the arbitrariness of the sample data selection for the data generation procedure.
âˆ

###### Proof.

First, by definition Î±^â€‹(t,x)=âˆ‡logâ¡hâ€‹(t,x)\widehat{\alpha}(t,x)=\nabla\log h(t,x) and Î±^kâ€‹(t,x)=âˆ‡logâ¡hkâ€‹(t,x)\widehat{\alpha}^{k}(t,x)=\nabla\log h^{k}(t,x), where hkh^{k} and hh are the solution to ([4.1](https://arxiv.org/html/2510.11829v1#S4.E1 "In 4 Convergence Results under Delta Initial Distribution â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) and ([4.2](https://arxiv.org/html/2510.11829v1#S4.E2 "In 4 Convergence Results under Delta Initial Distribution â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")), respectively, and âˆ‡=âˆ‚x\nabla=\partial\_{x}. We can easily deduce that

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | |Î±^kâ€‹(t,x)âˆ’Î±^â€‹(t,x)|\displaystyle|\widehat{\alpha}^{k}(t,x)-\widehat{\alpha}(t,x)| | =\displaystyle= | |âˆ‡logâ¡hkâ€‹(t,x)âˆ’âˆ‡logâ¡hâ€‹(t,x)|=|âˆ‡hkâ€‹(t,x)hkâ€‹(t,x)âˆ’âˆ‡hâ€‹(t,x)hâ€‹(t,x)|\displaystyle|\nabla\log h^{k}(t,x)-\nabla\log h(t,x)|=\left|\frac{\nabla h^{k}(t,x)}{h^{k}(t,x)}-\frac{\nabla h(t,x)}{h(t,x)}\right| |  |
|  |  | =\displaystyle= | |âˆ‡hkâ€‹(t,x)â€‹hâ€‹(t,x)âˆ’âˆ‡hâ€‹(t,x)â€‹hâ€‹(t,x)+âˆ‡hâ€‹(t,x)â€‹hâ€‹(t,x)âˆ’âˆ‡hâ€‹(t,x)â€‹hkâ€‹(t,x)hkâ€‹(t,x)â€‹hâ€‹(t,x)|\displaystyle\left|\frac{\nabla h^{k}(t,x)h(t,x)-\nabla h(t,x)h(t,x)+\nabla h(t,x)h(t,x)-\nabla h(t,x)h^{k}(t,x)}{h^{k}(t,x)h(t,x)}\right| |  |
|  |  | â‰¤\displaystyle\leq | |âˆ‡hkâ€‹(t,x)âˆ’âˆ‡hâ€‹(t,x)hkâ€‹(t,x)|+|âˆ‡hâ€‹(t,x)|â€‹|hâ€‹(t,x)âˆ’hkâ€‹(t,x)hkâ€‹(t,x)â€‹hâ€‹(t,x)|:=I1+I2.\displaystyle\left|\frac{\nabla h^{k}(t,x)-\nabla h(t,x)}{h^{k}(t,x)}\right|+|\nabla h(t,x)|\left|\frac{h(t,x)-h^{k}(t,x)}{h^{k}(t,x)h(t,x)}\right|:=I\_{1}+I\_{2}. |  |

We now estimate I1I\_{1} and I2I\_{2}, respectively. To this end we first apply the well-known Bismut-Elworthy-Li formula [[8](https://arxiv.org/html/2510.11829v1#bib.bib8), [24](https://arxiv.org/html/2510.11829v1#bib.bib24)] (see also the representation formula in [[27](https://arxiv.org/html/2510.11829v1#bib.bib27), [43](https://arxiv.org/html/2510.11829v1#bib.bib43)]) to get

|  |  |  |  |
| --- | --- | --- | --- |
|  | {âˆ‡hâ€‹(t,x)=âˆ‚xğ”¼t,xâ€‹[gâ€‹(XT)]=ğ”¼t,xâ€‹[gâ€‹(XT)â€‹NT],âˆ‡hkâ€‹(t,x)=âˆ‚xğ”¼t,xâ€‹[gkâ€‹(XT)]=ğ”¼t,xâ€‹[gkâ€‹(XT)â€‹NT],\displaystyle\left\{\begin{array}[]{lll}\nabla h(t,x)=\partial\_{x}\mathbb{E}\_{t,x}[g(X\_{T})]=\mathbb{E}\_{t,x}\big[g(X\_{T})N\_{T}\big],\vskip 6.0pt plus 2.0pt minus 2.0pt\\ \nabla h^{k}(t,x)=\partial\_{x}\mathbb{E}\_{t,x}[g\_{k}(X\_{T})]=\mathbb{E}\_{t,x}\big[g\_{k}(X\_{T})N\_{T}\big],\end{array}\right. |  | (4.8) |

where,

|  |  |  |
| --- | --- | --- |
|  | Ns=Nst,x:=1sâˆ’tâ€‹âˆ«ts(âˆ‡Xrt,x)âŠ¤â€‹ğ‘‘Wr,sâˆˆ[t,T],\displaystyle N\_{s}=N^{t,x}\_{s}:=\frac{1}{s-t}\int\_{t}^{s}(\nabla X^{t,x}\_{r})^{\top}dW\_{r},\qquad s\in[t,T], |  |

and âˆ‡X=âˆ‡Xt,x\nabla X=\nabla X^{t,x} is a â„dÃ—d\mathbb{R}^{d\times d}-valued variational process satisfying the (random) ODE:

|  |  |  |
| --- | --- | --- |
|  | âˆ‚xjXsi=Î´iâ€‹j+âˆ«tsâˆ‘â„“=1dâˆ‚xâ„“biâ€‹(r,Xr)â€‹âˆ‚xjXrâ„“â€‹dâ€‹r,1â‰¤i,jâ‰¤d,sâˆˆ[t,T].\displaystyle\partial\_{x^{j}}X^{i}\_{s}=\delta\_{ij}+\int\_{t}^{s}\sum\_{\ell=1}^{d}\partial\_{x^{\ell}}b^{i}(r,X\_{r})\partial\_{x^{j}}X^{\ell}\_{r}dr,\qquad 1\leq i,j\leq d,\quad s\in[t,T]. |  |

Furthermore,
one can easily check that

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[|âˆ‡Xst,x|2]â‰¤Câ€‹eCâ€‹(sâˆ’t),ğ”¼â€‹[|Nst,x|2]â‰¤Csâˆ’tâ€‹eCâ€‹(sâˆ’t),0â‰¤tâ‰¤sâ‰¤T.\displaystyle\mathbb{E}\big[|\nabla X^{t,x}\_{s}|^{2}\big]\leq Ce^{C(s-t)},\qquad\mathbb{E}\big[|N^{t,x}\_{s}|^{2}\big]\leq{C\over s-t}e^{C(s-t)},\qquad 0\leq t\leq s\leq T. |  |

Therefore, denoting C>0C>0 to be a generic constant that is allowed to vary from line to line, and applying Assumption [3.7](https://arxiv.org/html/2510.11829v1#S3.E7 "In Assumption 3.2. â€£ 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") and estimate ([3.2](https://arxiv.org/html/2510.11829v1#S3.E2 "In 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) we have

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | |âˆ‡hkâ€‹(t,x)âˆ’âˆ‡hâ€‹(t,x)|â‰¤ğ”¼â€‹[|gâ€‹(XTt,x)âˆ’gkâ€‹(XTt,x)|â€‹|NTt,x|]=ğ”¼â€‹[|fkâ€‹(XTt,x)âˆ’ftarâ€‹(XTt,x)pâ€‹(T,XTt,x;0,x0)|â€‹|NTt,x|]â‰¤(ğ”¼â€‹[|NTt,x|2])12â€‹[ğ”¼â€‹|fkâ€‹(XTt,x)âˆ’ftarâ€‹(XTt,x)pâ€‹(T,XTt,x;0,x0)|2]12â‰¤Câ€‹eCâ€‹(Tâˆ’t)Tâˆ’tâ€‹Gâ€‹(Î¼^k)â€‹[ğ”¼â€‹|Ï•â€‹(XTt,x)pâ€‹(T,XTt,x;0,x0)|2]12â‰¤Ckâ€‹Tâˆ’t.\displaystyle\begin{split}|\nabla h^{k}(t,x)-\nabla h(t,x)|&\leq\mathbb{E}\big[|g(X^{t,x}\_{T})-g\_{k}(X^{t,x}\_{T})||N^{t,x}\_{T}|\big]=\mathbb{E}\Big[\Big|\frac{f\_{k}(X^{t,x}\_{T})-f\_{\rm tar}(X^{t,x}\_{T})}{p(T,X^{t,x}\_{T};0,x\_{0})}\Big||N^{t,x}\_{T}|\Big]\\ &\leq\Big(\mathbb{E}[|N^{t,x}\_{T}|^{2}]\Big)^{\frac{1}{2}}\Big[\mathbb{E}\Big|\frac{f\_{k}(X^{t,x}\_{T})-f\_{\rm tar}(X^{t,x}\_{T})}{p(T,X^{t,x}\_{T};0,x\_{0})}\Big|^{2}\Big]^{\frac{1}{2}}\\ &\leq\frac{Ce^{C(T-t)}}{\sqrt{T-t}}G(\widehat{\mu}\_{k})\Big[\mathbb{E}\Big|\frac{\phi(X^{t,x}\_{T})}{p(T,X^{t,x}\_{T};0,x\_{0})}\Big|^{2}\Big]^{\frac{1}{2}}\leq\frac{C}{k\sqrt{T-t}}.\end{split} | |  | (4.9) |

Next, we note that by assumption Î´â‰¤gâ€‹(x),gkâ€‹(x)â‰¤C\delta\leq g(x),g\_{k}(x)\leq C for all xâˆˆâ„dx\in\mathbb{R}^{d} and kâˆˆâ„•k\in\mathbb{N}, by the weak maximum principle we conclude that as the solutions to the PDEs ([4.1](https://arxiv.org/html/2510.11829v1#S4.E1 "In 4 Convergence Results under Delta Initial Distribution â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) and ([4.2](https://arxiv.org/html/2510.11829v1#S4.E2 "In 4 Convergence Results under Delta Initial Distribution â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")), respectively, it holds that Î´â‰¤hâ€‹(t,x),hkâ€‹(t,x)â‰¤C\delta\leq h(t,x),h^{k}(t,x)\leq C, for all (t,x)âˆˆâ„dÃ—[0,T)(t,x)\in\mathbb{R}^{d}\times[0,T).
Consequently, we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | I1â‰¤CÎ´â€‹kâ€‹Tâˆ’tâ‰¤Ckâ€‹Tâˆ’t.I\_{1}\leq\frac{C}{\delta k\sqrt{T-t}}\leq\frac{C}{k\sqrt{T-t}}. |  | (4.10) |

Similarly, we can argue that |âˆ‡hâ€‹(t,x)|â‰¤CTâˆ’t|\nabla h(t,x)|\leq\frac{C}{\sqrt{T-t}}, and that

|  |  |  |  |
| --- | --- | --- | --- |
|  | |hkâ€‹(t,x)âˆ’hâ€‹(t,x)|â‰¤ğ”¼â€‹[|fkâ€‹(XTt,x)âˆ’ftarâ€‹(XTt,x)pâ€‹(T,XTt,x;0,x0)|]â‰¤Câ€‹Gâ€‹(Î¼^k)â€‹ğ”¼â€‹[|Ï•â€‹(XTt,x)pâ€‹(T,XTt,x;0,x0)|]â‰¤Ck,\displaystyle|h^{k}(t,x)-h(t,x)|\leq\mathbb{E}\Big[\Big|\frac{f\_{k}(X^{t,x}\_{T})-f\_{\rm tar}(X^{t,x}\_{T})}{p(T,X^{t,x}\_{T};0,x\_{0})}\Big|\Big]\leq CG(\widehat{\mu}\_{k})\mathbb{E}\Big[\Big|\frac{\phi(X^{t,x}\_{T})}{p(T,X^{t,x}\_{T};0,x\_{0})}\Big|\Big]\leq\frac{C}{k}, |  | (4.11) |

where the last inequality is due to ([4.3](https://arxiv.org/html/2510.11829v1#S4.E3 "In 4 Convergence Results under Delta Initial Distribution â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) and hence I2â‰¤Ckâ€‹Tâˆ’tI\_{2}\leq\frac{C}{k\sqrt{T-t}}. This, together with ([4.10](https://arxiv.org/html/2510.11829v1#S4.E10 "In 4.1 The Convergence of Optimal Policies â€£ 4 Convergence Results under Delta Initial Distribution â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) and ([4.1](https://arxiv.org/html/2510.11829v1#S4.Ex5 "4.1 The Convergence of Optimal Policies â€£ 4 Convergence Results under Delta Initial Distribution â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")), we obtain

|  |  |  |  |
| --- | --- | --- | --- |
|  | |Î±^kâ€‹(t,x)âˆ’Î±^â€‹(t,x)|â‰¤Ckâ€‹Tâˆ’t\displaystyle|\widehat{\alpha}^{k}(t,x)-\widehat{\alpha}(t,x)|\leq\frac{C}{k\sqrt{T-t}} |  | (4.12) |

and hence convergence result :

|  |  |  |
| --- | --- | --- |
|  | âˆ«0T|Î±^kâ€‹(t,x)âˆ’Î±^â€‹(t,x)|â€‹ğ‘‘tâ‰¤âˆ«0TCkâ€‹Tâˆ’tâ€‹ğ‘‘tâ‰¤Câ€‹Tk,\displaystyle\int\_{0}^{T}|\widehat{\alpha}^{k}(t,x)-\widehat{\alpha}(t,x)|dt\leq\int\_{0}^{T}\frac{C}{k\sqrt{T-t}}dt\leq\frac{C\sqrt{T}}{k}, |  |

proving the theorem.
âˆ

###### Remark 4.3.

A particular example is when we take the penalty function Gâ€‹(Î¼)=DKLâ€‹(Î¼âˆ¥Î¼tar)G(\mu)=D\_{\rm KL}(\mu\|{\mu\_{\rm tar}}). In this case, it is known
(see, e.g., [[29](https://arxiv.org/html/2510.11829v1#bib.bib29), Theorem 2]) that the optimal control for ([2.9](https://arxiv.org/html/2510.11829v1#S2.E9 "In SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday"))-([2.10](https://arxiv.org/html/2510.11829v1#S2.E10 "In Problem 2.4 (Soft-constrained SchrÃ¶dinger Bridge Problem (SCSBP)). â€£ SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) is given by
Î±^tk=âˆ‡logâ¡hkâ€‹(XtÎ±^k,t)\widehat{\alpha}\_{t}^{k}=\nabla\log h^{k}(X\_{t}^{\widehat{\alpha}^{k}},t), where

|  |  |  |
| --- | --- | --- |
|  | hk(t,x)=dkâˆ’1âˆ«p(T,z,;t,x)(ftarâ€‹(z)pâ€‹(T,z;0,x0))kk+1dz,\displaystyle h^{k}(t,x)=d\_{k}^{-1}\int p(T,z,;t,x)\Big(\frac{f\_{\rm tar}(z)}{p(T,z;0,x\_{0})}\Big)^{\frac{k}{k+1}}dz, |  |

with dk=âˆ«ftarâ€‹(x)k1+kâ€‹pâ€‹(x,T|x0,0)11+kâ€‹ğ‘‘xd\_{k}=\int f\_{\rm tar}(x)^{\frac{k}{1+k}}p(x,T\,|\,x\_{0},0)^{\frac{1}{1+k}}dx. In addition, Jkâ€‹(Î±^)=âˆ’(1+k)â€‹logâ¡(Ck)J^{k}(\widehat{\alpha})=-(1+k)\,\log(C\_{k}).
Consequently, Assumption [3.2](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem2 "Assumption 3.2. â€£ 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")-(ii) can be reduced to that ğ”¼â€‹[|ftarâ€‹(XT)pâ€‹(T,XT;0,x0)|2]\mathbb{E}\Big[\big|\frac{f\_{\rm tar}(X\_{T})}{p(T,X\_{T};0,x\_{0})}\big|^{2}\Big] is bounded (see Assumption [5.4](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem4 "Assumption 5.4. â€£ 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") for similar conditions); and the linear rate of convergence can be proved with the same arguments.
âˆ

### 4.2 The Convergence of the Value Function

Having worked out the convergence analysis for the optimal controls, it is natural to extend the results to the convergence of value functions. However, the singularity at the terminal time TT in ([4.12](https://arxiv.org/html/2510.11829v1#S4.E12 "In 4.1 The Convergence of Optimal Policies â€£ 4 Convergence Results under Delta Initial Distribution â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) requires some technical care. It turns out that the popular notion of early stopping in diffusion models as well as the flow-based method literature [[4](https://arxiv.org/html/2510.11829v1#bib.bib4), [41](https://arxiv.org/html/2510.11829v1#bib.bib41), [33](https://arxiv.org/html/2510.11829v1#bib.bib33)] is exactly the remedy to this issue.

To be more precise, for any Îµ>0\varepsilon>0, we introduce the following Îµ\varepsilon-value function.

|  |  |  |
| --- | --- | --- |
|  | JÎµâ€‹(Î±):=ğ”¼â€‹[âˆ«0Tâˆ’Îµ12â€‹|Î±|2â€‹ğ‘‘t].\displaystyle J\_{\varepsilon}(\alpha):=\mathbb{E}\left[\int\_{0}^{T-\varepsilon}\frac{1}{2}|\alpha|^{2}dt\right]. |  |

There are many practical reasons, mainly for computational purposes, to invoke the notion of early stopping, as elaborated in
[[4](https://arxiv.org/html/2510.11829v1#bib.bib4), [41](https://arxiv.org/html/2510.11829v1#bib.bib41), [33](https://arxiv.org/html/2510.11829v1#bib.bib33)]. But on the other hand, it is clear that the Îµ\varepsilon-value function effectively excludes the singularity at the terminal time TT. This leads to the following straightforward result.

###### Proposition 4.4.

Assume that all the assumptions of Theorem [4.1](https://arxiv.org/html/2510.11829v1#S4.ThmTheorem1 "Theorem 4.1. â€£ 4.1 The Convergence of Optimal Policies â€£ 4 Convergence Results under Delta Initial Distribution â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") are in force. Then, for any Îµ>0\varepsilon>0, there exists a generic constant C:=Câ€‹(Îµ)=ğ’ªâ€‹(1Îµ)>0C:=C(\varepsilon)=\mathcal{O}(\frac{1}{\sqrt{\varepsilon}})>0, independent of kk, such that

|  |  |  |  |
| --- | --- | --- | --- |
|  | |JÎµâ€‹(Î±^k)âˆ’JÎµâ€‹(Î±^)|â‰¤Ck,kâˆˆâ„•.\displaystyle|J\_{\varepsilon}(\widehat{\alpha}^{k})-J\_{\varepsilon}(\widehat{\alpha})|\leq\frac{C}{k},\qquad k\in\mathbb{N}. |  | (4.13) |

where Î±^k\widehat{\alpha}^{k} and Î±^\widehat{\alpha} are the optimal controls in Theorem [4.1](https://arxiv.org/html/2510.11829v1#S4.ThmTheorem1 "Theorem 4.1. â€£ 4.1 The Convergence of Optimal Policies â€£ 4 Convergence Results under Delta Initial Distribution â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday"), respectively.

###### Proof.

The proof is straightforward. For any kâˆˆâ„•k\in\mathbb{N}, let Î±^k\widehat{\alpha}^{k} and Î±^\widehat{\alpha} be the optimal controls in Theorem [4.1](https://arxiv.org/html/2510.11829v1#S4.ThmTheorem1 "Theorem 4.1. â€£ 4.1 The Convergence of Optimal Policies â€£ 4 Convergence Results under Delta Initial Distribution â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday"), respectively. Then, for any Îµ>0\varepsilon>0, applying ([4.12](https://arxiv.org/html/2510.11829v1#S4.E12 "In 4.1 The Convergence of Optimal Policies â€£ 4 Convergence Results under Delta Initial Distribution â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) we have

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | |JÎµâ€‹(Î±^k)âˆ’JÎµâ€‹(Î±^)|\displaystyle|J\_{\varepsilon}(\widehat{\alpha}^{k})-J\_{\varepsilon}(\widehat{\alpha})|\negthinspace | â‰¤\displaystyle\negthinspace\leq\negthinspace | ğ”¼â€‹[12â€‹âˆ«0Tâˆ’Îµ||Î±^sk|2âˆ’|Î±^s|2|â€‹ğ‘‘s]â‰¤ğ”¼â€‹[12â€‹âˆ«0Tâˆ’Îµ|Î±^skâˆ’Î±^s|â€‹(|Î±^sk|+|Î±^s|)â€‹ğ‘‘s]\displaystyle\negthinspace\mathbb{E}\Big[\frac{1}{2}\int\_{0}^{T-\varepsilon}\big||\widehat{\alpha}^{k}\_{s}|^{2}-|\widehat{\alpha}\_{s}|^{2}\big|ds\Big]\leq\mathbb{E}\Big[\frac{1}{2}\int\_{0}^{T-\varepsilon}\big|\widehat{\alpha}^{k}\_{s}-\widehat{\alpha}\_{s}\big|\big(|\widehat{\alpha}^{k}\_{s}|+|\widehat{\alpha}\_{s}|\big)ds\Big] |  |
|  |  | â‰¤\displaystyle\negthinspace\leq\negthinspace | ckâ€‹ğ”¼â€‹[12â€‹âˆ«0Tâˆ’Îµ1Tâˆ’sâ€‹(|Î±^sk|+|Î±^s|)â€‹ğ‘‘s]â‰¤ckâ€‹Îµâ€‹ğ”¼â€‹[12â€‹âˆ«0T(|Î±^sk|+|Î±^s|)â€‹ğ‘‘s],\displaystyle\negthinspace\frac{c}{k}\mathbb{E}\Big[\frac{1}{2}\int\_{0}^{T-\varepsilon}\frac{1}{\sqrt{T-s}}\big(|\widehat{\alpha}^{k}\_{s}|+|\widehat{\alpha}\_{s}|\big)ds\Big]\leq\frac{c}{k\sqrt{\varepsilon}}\mathbb{E}\Big[\frac{1}{2}\int\_{0}^{T}\big(|\widehat{\alpha}^{k}\_{s}|+|\widehat{\alpha}\_{s}|\big)\,ds\Big], |  |

where
the last inequality is due to the fact that 1Tâˆ’sâ‰¤1Îµ\frac{1}{\sqrt{T-s}}\leq\frac{1}{\sqrt{\varepsilon}} for sâˆˆ[0,Tâˆ’Îµ]s\in[0,T-\varepsilon]. To further bound ([4.2](https://arxiv.org/html/2510.11829v1#S4.Ex12 "4.2 The Convergence of the Value Function â€£ 4 Convergence Results under Delta Initial Distribution â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")), we recall that the definitions of Jâ€‹(â‹…)J(\cdot) ([2.5](https://arxiv.org/html/2510.11829v1#S2.E5 "In SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) and Jkâ€‹(â‹…)J^{k}(\cdot) ([2.10](https://arxiv.org/html/2510.11829v1#S2.E10 "In Problem 2.4 (Soft-constrained SchrÃ¶dinger Bridge Problem (SCSBP)). â€£ SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")), kâˆˆâ„•k\in\mathbb{N}, and define

|  |  |  |
| --- | --- | --- |
|  | Vâˆ—=Jâ€‹(Î±^)=infÎ±âˆˆğ’œJâ€‹(Î±);Vk,âˆ—=Jkâ€‹(Î±^k)=infÎ±âˆˆğ’œJkâ€‹(Î±).\displaystyle V^{\*}=J(\widehat{\alpha})=\inf\_{\alpha\in\mathcal{A}}J(\alpha);\qquad V^{k,\*}=J^{k}(\widehat{\alpha}^{k})=\inf\_{\alpha\in\mathcal{A}}J^{k}(\alpha). |  |

We should note that XÎ±^X^{\widehat{\alpha}} follows the constrained dynamics ([2.6](https://arxiv.org/html/2510.11829v1#S2.E6 "In SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")), whereas XÎ±^kX^{\widehat{\alpha}^{k}} follows the soft-constrained dynamics ([2.9](https://arxiv.org/html/2510.11829v1#S2.E9 "In SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")). Clearly, by definition ([2.10](https://arxiv.org/html/2510.11829v1#S2.E10 "In Problem 2.4 (Soft-constrained SchrÃ¶dinger Bridge Problem (SCSBP)). â€£ SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) we have

|  |  |  |
| --- | --- | --- |
|  | supkâ‰¥1Jkâ€‹(Î±)={ğ”¼â€‹[âˆ«0T12â€‹|Î±t|2â€‹dâ¡t]Â ifÂ â€‹â„™XTÎ±=Î¼tarâˆotherwise.\displaystyle\sup\_{k\geq 1}J^{k}(\alpha)=\begin{cases}\displaystyle\mathbb{E}\Big[\int\_{0}^{T}\frac{1}{2}|\alpha\_{t}|^{2}\operatorname{{\rm d}}t\Big]&\textrm{ if }\,\,\mathbb{P}\_{X\_{T}^{\alpha}}={\mu\_{\rm tar}}\\ \infty&\textrm{otherwise}.\end{cases} |  |

Thus, since Î±^\widehat{\alpha} satisfies the constraint dynamics ([2.6](https://arxiv.org/html/2510.11829v1#S2.E6 "In SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")), we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | Vâˆ—=Jâ€‹(Î±^)=infÎ±âˆˆğ’œsupkâ‰¥1Jkâ€‹(Î±)â‰¥infÎ±âˆˆğ’œJkâ€‹(Î±)=Jkâ€‹(Î±^k)=Vk,âˆ—,kâˆˆâ„•.\displaystyle V^{\*}=J(\widehat{\alpha})=\inf\_{\alpha\in\mathcal{A}}\sup\_{k\geq 1}J^{k}(\alpha)\geq\inf\_{\alpha\in\mathcal{A}}J^{k}(\alpha)=J^{k}(\widehat{\alpha}^{k})=V^{k,\*},\qquad k\in\mathbb{N}. |  | (4.15) |

Consequently, we have, for each kâˆˆâ„•k\in\mathbb{N}, a simple application of Cauchyâ€“Schwarz inequality and the fact ([4.15](https://arxiv.org/html/2510.11829v1#S4.E15 "In 4.2 The Convergence of the Value Function â€£ 4 Convergence Results under Delta Initial Distribution â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) yields

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â€‹[12â€‹âˆ«0T(|Î±^sk|+|Î±^s|)â€‹ğ‘‘s]â‰¤T2â€‹(ğ”¼â€‹[âˆ«0T|Î±^sk|2â€‹ğ‘‘s])1/2+(ğ”¼â€‹[âˆ«0T|Î±^s|2â€‹ğ‘‘s])1/2â‰¤Tâ€‹Vâˆ—.\displaystyle\mathbb{E}\Big[\frac{1}{2}\int\_{0}^{T}\big(|\widehat{\alpha}^{k}\_{s}|+|\widehat{\alpha}\_{s}|\big)ds\Big]\leq\frac{{\sqrt{T}}}{2}\Big(\mathbb{E}\Big[\int\_{0}^{T}|\widehat{\alpha}^{k}\_{s}|^{2}ds\Big]\Big)^{1/2}+\Big(\mathbb{E}\Big[\int\_{0}^{T}|\widehat{\alpha}\_{s}|^{2}ds\Big]\Big)^{1/2}\leq\sqrt{T}\sqrt{V^{\*}}. |  | (4.16) |

Combining ([4.2](https://arxiv.org/html/2510.11829v1#S4.Ex12 "4.2 The Convergence of the Value Function â€£ 4 Convergence Results under Delta Initial Distribution â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) and ([4.16](https://arxiv.org/html/2510.11829v1#S4.E16 "In 4.2 The Convergence of the Value Function â€£ 4 Convergence Results under Delta Initial Distribution â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")), we obtain ([4.13](https://arxiv.org/html/2510.11829v1#S4.E13 "In Proposition 4.4. â€£ 4.2 The Convergence of the Value Function â€£ 4 Convergence Results under Delta Initial Distribution â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")).
âˆ

Besides the convergence of the value functions, another important convergence, that relies crucially on the convergence of the optimal controls, is the convergence of the terminal law â„™XTÎ±^k\mathbb{P}\_{X^{\widehat{\alpha}^{k}}\_{T}} (with respect to the target distribution Î¼tar{\mu\_{\rm tar}}), measured, for instance, in the Wasserstein distance. Again, to avoid the technicalities that the singularity at terminal time TT might cause, we shall focus on the early stopped state XTâˆ’ÎµÎ±^kX^{\widehat{\alpha}^{k}}\_{T-\varepsilon}, which is a commonly used criterion in statistical estimation results for generative diffusion models (see, e.g., [[28](https://arxiv.org/html/2510.11829v1#bib.bib28), [33](https://arxiv.org/html/2510.11829v1#bib.bib33), [12](https://arxiv.org/html/2510.11829v1#bib.bib12)]). More precisely, we have the following result.

###### Proposition 4.5.

Let all assumptions in Theorem [4.1](https://arxiv.org/html/2510.11829v1#S4.ThmTheorem1 "Theorem 4.1. â€£ 4.1 The Convergence of Optimal Policies â€£ 4 Convergence Results under Delta Initial Distribution â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") be in force. Assume further that the optimal policy Î±^\widehat{\alpha} of the original SBP is Lipschitz in xx: there exists Îº>0\kappa>0, such that

|  |  |  |  |
| --- | --- | --- | --- |
|  | |Î±^â€‹(t,x)âˆ’Î±^â€‹(t,y)|â‰¤Îºâ€‹|xâˆ’y|,tâˆˆ[0,T].\displaystyle|\widehat{\alpha}(t,x)-\widehat{\alpha}(t,y)|\leq\kappa|x-y|,\qquad t\in[0,T]. |  | (4.17) |

Then there exists a constant C>0C>0, depending on the Lipschitz constants LL in Assumption [2.2](https://arxiv.org/html/2510.11829v1#S2.ThmTheorem2 "Assumption 2.2. â€£ SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") and Îº\kappa in ([4.17](https://arxiv.org/html/2510.11829v1#S4.E17 "In Proposition 4.5. â€£ 4.2 The Convergence of the Value Function â€£ 4 Convergence Results under Delta Initial Distribution â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")), but independent of kâˆˆâ„•k\in\mathbb{N}, such that for any Îµ>0\varepsilon>0, it holds that

|  |  |  |  |
| --- | --- | --- | --- |
|  | W2â€‹(â„™XTâˆ’ÎµÎ±^k,Î¼tar)â‰¤Câ€‹lnâ¡Tâˆ’lnâ¡Îµk+Câ€‹Îµ.\displaystyle{W}\_{2}(\mathbb{P}\_{X^{\widehat{\alpha}^{k}}\_{T-\varepsilon}},{\mu\_{\rm tar}})\leq\frac{C\sqrt{\ln T-\ln\varepsilon}}{k}+C\varepsilon. |  | (4.18) |

In particular, if we choose Îµ=1k\varepsilon=\frac{1}{k}, then it holds that

|  |  |  |  |
| --- | --- | --- | --- |
|  | W2(â„™XTâˆ’ÎµÎ±^k,Î¼tar)â‰¤Ck(lnâ¡k+lnâ¡T)+1)=ğ’ª(lnâ¡kk).\displaystyle{W}\_{2}(\mathbb{P}\_{X^{\widehat{\alpha}^{k}}\_{T-\varepsilon}},{\mu\_{\rm tar}})\leq\frac{C}{k}\big(\sqrt{\ln k}+\sqrt{\ln T})+1\big)=\mathcal{O}\Big(\frac{\sqrt{\ln k}}{k}\Big). |  | (4.19) |

###### Remark 4.6.

(i) The linear (i.e., âˆ¼1k\sim\frac{1}{k}) "closeness" between the law of the optimal state and Î¼tar{\mu\_{\rm tar}} has appeared several times so far. For example, ([4.3](https://arxiv.org/html/2510.11829v1#S4.E3 "In 4 Convergence Results under Delta Initial Distribution â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) implies that Gâ€‹(â„™XTÎ±^k)=Gâ€‹(â„™XTÎ±^k;Î¼tar)â‰¤ckG(\mathbb{P}\_{X^{\widehat{\alpha}^{k}}\_{T}})=G(\mathbb{P}\_{X^{\widehat{\alpha}^{k}}\_{T}};{\mu\_{\rm tar}})\leq\frac{c}{k}, and by
Remark [3.3](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem3 "Remark 3.3. â€£ 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")-(ii), this implies that W1â€‹(â„™XTÎ±^k,Î¼tar)âˆ¼1kW\_{1}(\mathbb{P}\_{X^{\widehat{\alpha}^{k}}\_{T}},{\mu\_{\rm tar}})\sim\frac{1}{k}. The result in ([4.19](https://arxiv.org/html/2510.11829v1#S4.E19 "In Proposition 4.5. â€£ 4.2 The Convergence of the Value Function â€£ 4 Convergence Results under Delta Initial Distribution â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) is in the same spirit, by under the stronger W2{W}\_{2}-distance, but compensated by an early stopping.

(ii) The Lipschitz condition ([4.17](https://arxiv.org/html/2510.11829v1#S4.E17 "In Proposition 4.5. â€£ 4.2 The Convergence of the Value Function â€£ 4 Convergence Results under Delta Initial Distribution â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) for the optimal control Î±^\widehat{\alpha} is not unusual in the diffusion model literature (see, e.g., [[60](https://arxiv.org/html/2510.11829v1#bib.bib60), [11](https://arxiv.org/html/2510.11829v1#bib.bib11), [12](https://arxiv.org/html/2510.11829v1#bib.bib12)]). In fact, this can be argued via regularity of the solution to the PDE ([4.1](https://arxiv.org/html/2510.11829v1#S4.E1 "In 4 Convergence Results under Delta Initial Distribution â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) combined with the speed of decay of the density ftarf\_{\rm tar}, which can be assumed and analyzed rigorously (see Assumption [5.4](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem4 "Assumption 5.4. â€£ 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") below). We therefore consider such an assumption non-stringent.
âˆ

[Proof of Proposition [4.5](https://arxiv.org/html/2510.11829v1#S4.ThmTheorem5 "Proposition 4.5. â€£ 4.2 The Convergence of the Value Function â€£ 4 Convergence Results under Delta Initial Distribution â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday").]
First note that XÎ±^X^{\widehat{\alpha}} and XÎ±^kX^{\widehat{\alpha}^{k}} satisfy the following SDEs, respectively:

|  |  |  |  |
| --- | --- | --- | --- |
|  | {dâ€‹XtÎ±^=[bâ€‹(t,XtÎ±^)+Î±^tâ€‹(XtÎ±^)]â€‹dâ€‹t+dâ€‹WtX0Î±^=x0;dâ€‹XtÎ±^k=[bâ€‹(t,XtÎ±^k)+Î±^tkâ€‹(XtÎ±^k)]â€‹dâ€‹t+dâ€‹Wt,X0Î±^=x0.\displaystyle\begin{cases}dX^{\widehat{\alpha}}\_{t}=[b(t,X^{\widehat{\alpha}}\_{t})+\widehat{\alpha}\_{t}(X^{\widehat{\alpha}}\_{t})]dt+dW\_{t}\qquad\quad&X^{\widehat{\alpha}}\_{0}=x\_{0};\\ dX^{\widehat{\alpha}^{k}}\_{t}=[b(t,X^{\widehat{\alpha}^{k}}\_{t})+\widehat{\alpha}\_{t}^{k}(X^{\widehat{\alpha}^{k}}\_{t})]dt+dW\_{t},&X^{\widehat{\alpha}}\_{0}=x\_{0}.\end{cases} |  | (4.20) |

Let us now denote Î±^tâ€‹(x)=Î±^â€‹(t,x)\widehat{\alpha}\_{t}(x)=\widehat{\alpha}(t,x), Î±^tkâ€‹(x)=Î±^kâ€‹(t,x)\widehat{\alpha}^{k}\_{t}(x)=\widehat{\alpha}^{k}(t,x), and define

|  |  |  |
| --- | --- | --- |
|  | bÎ±^â€‹(t,x)=bâ€‹(t,x)+Î±^tâ€‹(x),Î”â€‹Î±^tkâ€‹(x)=Î±^tkâ€‹(x)âˆ’Î±^tâ€‹(x),(t,x)âˆˆ[0,T]Ã—â„d.b^{\widehat{\alpha}}(t,x)=b(t,x)+\widehat{\alpha}\_{t}(x),\quad\Delta\widehat{\alpha}^{k}\_{t}(x)=\widehat{\alpha}^{k}\_{t}(x)-\widehat{\alpha}\_{t}(x),\qquad(t,x)\in[0,T]\times\mathbb{R}^{d}. |  |

Then we see that SDE ([4.20](https://arxiv.org/html/2510.11829v1#S4.E20 "In 4.2 The Convergence of the Value Function â€£ 4 Convergence Results under Delta Initial Distribution â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) can be written as

|  |  |  |
| --- | --- | --- |
|  | {dâ€‹XtÎ±^=bÎ±^â€‹(t,XtÎ±^)â€‹dâ€‹t+dâ€‹WtX0Î±^=x0;dâ€‹XtÎ±^k=[bÎ±^â€‹(t,XtÎ±^k)+Î”â€‹Î±^tkâ€‹(XtÎ±^k)]â€‹dâ€‹t+dâ€‹Wt,X0Î±^=x0.\displaystyle\begin{cases}dX^{\widehat{\alpha}}\_{t}=b^{\widehat{\alpha}}(t,X^{\widehat{\alpha}}\_{t})dt+dW\_{t}\qquad\quad&X^{\widehat{\alpha}}\_{0}=x\_{0};\\ dX^{\widehat{\alpha}^{k}}\_{t}=[b^{\widehat{\alpha}}(t,X^{\widehat{\alpha}^{k}}\_{t})+\Delta\widehat{\alpha}\_{t}^{k}(X^{\widehat{\alpha}^{k}}\_{t})]dt+dW\_{t},&X^{\widehat{\alpha}}\_{0}=x\_{0}.\end{cases} |  |

That is,

|  |  |  |
| --- | --- | --- |
|  | XtÎ±^âˆ’XtÎ±^k=âˆ«0t[bÎ±^â€‹(s,XsÎ±^)âˆ’bÎ±^â€‹(s,XsÎ±^k)+Î”â€‹Î±^skâ€‹(XsÎ±^k)]â€‹ğ‘‘s,tâˆˆ[0,T]\displaystyle X^{\widehat{\alpha}}\_{t}-X^{\widehat{\alpha}^{k}}\_{t}=\int\_{0}^{t}[b^{\widehat{\alpha}}(s,X^{\widehat{\alpha}}\_{s})-b^{\widehat{\alpha}}(s,X^{\widehat{\alpha}^{k}}\_{s})+\Delta\widehat{\alpha}\_{s}^{k}(X^{\widehat{\alpha}^{k}}\_{s})]ds,\qquad t\in[0,T] |  |

Note that by Assumption [2.2](https://arxiv.org/html/2510.11829v1#S2.ThmTheorem2 "Assumption 2.2. â€£ SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") and ([4.17](https://arxiv.org/html/2510.11829v1#S4.E17 "In Proposition 4.5. â€£ 4.2 The Convergence of the Value Function â€£ 4 Convergence Results under Delta Initial Distribution â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")), bÎ±^b^{\widehat{\alpha}} is uniform Lipschitz in xx (with Lipschitz constant L+ÎºL+\kappa),
and applying the estimate ([4.12](https://arxiv.org/html/2510.11829v1#S4.E12 "In 4.1 The Convergence of Optimal Policies â€£ 4 Convergence Results under Delta Initial Distribution â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")), we deduce easily that

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼[|XtÎ±^âˆ’XtÎ±^k|2]â‰¤2Tâˆ«0t[(L+Îº)2ğ”¼[|XsÎ±^âˆ’XsÎ±^k|2]ds+2â€‹c2k2ln[TTâˆ’t].\displaystyle\mathbb{E}[|X^{\widehat{\alpha}}\_{t}-X^{\widehat{\alpha}^{k}}\_{t}|^{2}]\leq 2T\int\_{0}^{t}\Big[(L+\kappa)^{2}\mathbb{E}[|X^{\widehat{\alpha}}\_{s}-X^{\widehat{\alpha}^{k}}\_{s}|^{2}]ds+\frac{2c^{2}}{k^{2}}\ln\Big[\frac{T}{T-t}\Big]. |  | (4.21) |

In what follows let us denote C>0C>0 to be a generic constant depending only on LL, Îº\kappa, cc, but independent of kk, and we allow it to vary from line to line. Then, by a simple calculation using Gronwallâ€™s inequality, we see that ([4.21](https://arxiv.org/html/2510.11829v1#S4.E21 "In 4.2 The Convergence of the Value Function â€£ 4 Convergence Results under Delta Initial Distribution â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) lead to that

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â€‹[|XtÎ±^âˆ’XtÎ±^k|2]â‰¤Ck2â€‹(lnâ¡Tâˆ’lnâ¡(Tâˆ’t))â€‹eCâ€‹t,tâˆˆ[0,T).\displaystyle\mathbb{E}[|X^{\widehat{\alpha}}\_{t}-X^{\widehat{\alpha}^{k}}\_{t}|^{2}]\leq\frac{C}{k^{2}}(\ln T-\ln(T-t))e^{Ct},\qquad t\in[0,T). |  | (4.22) |

Furthermore, for any Îµ>0\varepsilon>0, using the monotonicity of the log function we deduce from ([4.22](https://arxiv.org/html/2510.11829v1#S4.E22 "In 4.2 The Convergence of the Value Function â€£ 4 Convergence Results under Delta Initial Distribution â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) that

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[|XTâˆ’ÎµÎ±^âˆ’XTâˆ’ÎµÎ±^k|2]â‰¤Ck2â€‹(lnâ¡Tâˆ’lnâ¡Îµ)\displaystyle\mathbb{E}[|X^{\widehat{\alpha}}\_{T-\varepsilon}-X^{\widehat{\alpha}^{k}}\_{T-\varepsilon}|^{2}]\leq\frac{C}{k^{2}}(\ln T-\ln\varepsilon) |  |

It then follows that

|  |  |  |
| --- | --- | --- |
|  | W2â€‹(â„™XTâˆ’ÎµÎ±^,â„™XTâˆ’ÎµÎ±^k)â‰¤ğ”¼â€‹[|XTâˆ’ÎµÎ±^âˆ’XTâˆ’ÎµÎ±^k|2]1/2â‰¤Ckâ€‹lnâ¡Tâˆ’lnâ¡Îµ.\displaystyle W\_{2}(\mathbb{P}\_{X^{\widehat{\alpha}}\_{T-\varepsilon}},\mathbb{P}\_{X^{\widehat{\alpha}^{k}}\_{T-\varepsilon}})\leq\mathbb{E}[|X^{\widehat{\alpha}}\_{T-\varepsilon}-X^{\widehat{\alpha}^{k}}\_{T-\varepsilon}|^{2}]^{1/2}\leq\frac{\sqrt{C}}{k}\sqrt{\ln T-\ln\varepsilon}. |  |

Finally, since the function bÎ±^=b+Î±^b^{\widehat{\alpha}}=b+\widehat{\alpha} is Lipschitz, by standard ğ•ƒ2\mathbb{L}^{2}-continuity result of SDE, we have

|  |  |  |
| --- | --- | --- |
|  | W2â€‹(â„™XTâˆ’ÎµÎ±^,â„™XTÎ±^)â‰¤Câ€‹Îµ,\displaystyle{W}\_{2}(\mathbb{P}\_{X^{\widehat{\alpha}}\_{T-\varepsilon}},\mathbb{P}\_{X^{\widehat{\alpha}}\_{T}})\leq C\varepsilon, |  |

and consequently, noting that â„™XTÎ±^=Î¼tar\mathbb{P}\_{X^{\widehat{\alpha}}\_{T}}={\mu\_{\rm tar}}, we obtain

|  |  |  |
| --- | --- | --- |
|  | W2â€‹(â„™XTâˆ’ÎµÎ±^,Î¼tar)â‰¤W2â€‹(â„™XTâˆ’ÎµÎ±^,â„™XTâˆ’ÎµÎ±^k)+W2â€‹(â„™XTâˆ’ÎµÎ±^,â„™XTÎ±^)â‰¤Câ€‹(lnâ¡Tâˆ’lnâ¡Îµ)k+Câ€‹Îµ,\displaystyle{W}\_{2}(\mathbb{P}\_{X^{\widehat{\alpha}}\_{T-\varepsilon}},{\mu\_{\rm tar}})\leq W\_{2}(\mathbb{P}\_{X^{\widehat{\alpha}}\_{T-\varepsilon}},\mathbb{P}\_{X^{\widehat{\alpha}^{k}}\_{T-\varepsilon}})+{W}\_{2}(\mathbb{P}\_{X^{\widehat{\alpha}}\_{T-\varepsilon}},\mathbb{P}\_{X^{\widehat{\alpha}}\_{T}})\leq\frac{\sqrt{C(\ln T-\ln\varepsilon)}}{k}+C\varepsilon, |  |

proving ([4.18](https://arxiv.org/html/2510.11829v1#S4.E18 "In Proposition 4.5. â€£ 4.2 The Convergence of the Value Function â€£ 4 Convergence Results under Delta Initial Distribution â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")), whence the proposition.
âˆ

## 5 Stability of the Solutions to the SBP

We note that all the results in the previous section are based on an important assumption: Î¼ini=Î´x0{\mu\_{\rm ini}}=\delta\_{x\_{0}}, for some x0âˆˆâ„dx\_{0}\in\mathbb{R}^{d}. In this and the next section, we shall extend the results to more general initial condition Î¼iniâˆˆğ’«2â€‹(â„d){\mu\_{\rm ini}}\in\mathscr{P}\_{2}(\mathbb{R}^{d}), and establish a similar rate of convergence.

We shall begin an important aspect in probability theory, which is the basis for the so-called stability issues of the solutions to the classic SchrÃ¶dinger bridge problem. For notational convenience, we still denote pâ€‹(â‹…,â‹…;â‹…,â‹…)p(\cdot,\cdot;\cdot,\cdot) to be the transition density of a standard â„d\mathbb{R}^{d}-valued diffusion ([3.1](https://arxiv.org/html/2510.11829v1#S3.E1 "In 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")). We begin with the following well-known result in diffusion theory (cf. e.g., [[7](https://arxiv.org/html/2510.11829v1#bib.bib7)]).

###### Proposition 5.1 ([[7](https://arxiv.org/html/2510.11829v1#bib.bib7)]).

For any Î¼0,Î¼Tâˆˆğ’«â€‹(â„d)\mu\_{0},\mu\_{T}\in\mathscr{P}(\mathbb{R}^{d}), there exists a unique pair of Ïƒ\sigma-finite measures Î½0,Î½Tâˆˆâ„³â€‹(â„d)\nu\_{0},\nu\_{T}\in\mathscr{M}(\mathbb{R}^{d}) such that the measure Ï€âˆˆğ’«â€‹(â„dÃ—â„d)\pi\in\mathscr{P}(\mathbb{R}^{d}\times\mathbb{R}^{d}) defined by

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ï€â€‹(E)=âˆ«Epâ€‹(T,y;0,x)â€‹Î½0â€‹(dâ€‹x)â€‹Î½Tâ€‹(dâ€‹y),Eâˆˆâ„¬â€‹(â„dÃ—â„d)\displaystyle\pi(E)=\int\_{E}p(T,y;0,x)\nu\_{0}(dx)\nu\_{T}(dy),\qquad E\in\mathscr{B}(\mathbb{R}^{d}\times\mathbb{R}^{d}) |  | (5.1) |

has marginals Î¼0\mu\_{0} and Î¼T\mu\_{T}. Furthermore, Î½T\nu\_{T} and Î¼T\mu\_{T} (resp. Î½0\nu\_{0} and Î¼0\mu\_{0}) are mutually absolutely continuous,
denoted by Î½Tâ‰ƒÎ¼T\nu\_{T}\simeq\mu\_{T} (resp. Î½0â‰ƒÎ¼0\nu\_{0}\simeq\mu\_{0}).
âˆ

Following Proposition [5.1](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem1 "Proposition 5.1 ([7]). â€£ 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday"), let us denote a (well-defined) mapping
ğ’¯:ğ’«2â€‹(â„d)Ã—ğ’«2â€‹(â„d)â†¦â„³â€‹(â„d)Ã—â„³â€‹(â„d){\cal T}:\mathscr{P}\_{2}(\mathbb{R}^{d})\times\mathscr{P}\_{2}(\mathbb{R}^{d})\mapsto\mathscr{M}(\mathbb{R}^{d})\times\mathscr{M}(\mathbb{R}^{d}) by ğ’¯â€‹(Î¼0,Î¼T)=(Î½0,Î½T){\cal T}(\mu\_{0},\mu\_{T})=(\nu\_{0},\nu\_{T}). In particular, in what follows we shall often fix Î¼0=Î¼iniâˆˆğ’«2â€‹(â„d)\mu\_{0}={\mu\_{\rm ini}}\in\mathscr{P}\_{2}(\mathbb{R}^{d}), and focus mainly on Î½T\nu\_{T}.
Note that in Proposition [5.1](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem1 "Proposition 5.1 ([7]). â€£ 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") the measures (Î½0,Î½T)(\nu\_{0},\nu\_{T}) are only Ïƒ\sigma-finite in general, to facilitate our discussion, we shall consider, for a given Î¼0\mu\_{0}, the following set:

|  |  |  |
| --- | --- | --- |
|  | ğ’ŸÎ¼0:={Î¼âˆˆğ’«2â€‹(â„d):ğ’¯â€‹(Î¼0,Î¼)â‰ªLâ€‹eâ€‹bâ€‹(â‹…);ğ’¯â€‹(Î¼0,Î¼)â€‹(â„dÃ—â„d)<âˆ}.\displaystyle\mathscr{D}\_{\mu\_{0}}:=\{\mu\in\mathscr{P}\_{2}(\mathbb{R}^{d}):{\cal T}(\mu\_{0},\mu)\ll Leb(\cdot);\,{\cal T}(\mu\_{0},\mu)(\mathbb{R}^{d}\times\mathbb{R}^{d})<\infty\}. |  |

Here Lâ€‹eâ€‹bâ€‹(â‹…)Leb(\cdot) denotes the Lebesgue measure on â„dÃ—â„d\mathbb{R}^{d}\times\mathbb{R}^{d}. In the case when Î¼0=Î¼ini\mu\_{0}={\mu\_{\rm ini}} is fixed in the discussion, we shall simply denote ğ’Ÿ=ğ’ŸÎ¼ini\mathscr{D}=\mathscr{D}\_{{\mu\_{\rm ini}}} when context is clear.

We note that if Î¼âˆˆğ’Ÿ\mu\in\mathscr{D} and (Î½0,Î½T)=ğ’¯â€‹(Î¼ini,Î¼)(\nu\_{0},\nu\_{T})={\cal T}({\mu\_{\rm ini}},\mu), then
Î½T\nu\_{T} must have a density function, which we shall denote by ÏÎ¼âˆˆğ•ƒ1â€‹(â„d)\rho^{\mu}\in\mathbb{L}^{1}(\mathbb{R}^{d}).
Moreover, we define an operator S:ğ’«2â€‹(â„d)â†’ğ’«2â€‹(â„d)S:\mathscr{P}\_{2}(\mathbb{R}^{d})\to\mathscr{P}\_{2}(\mathbb{R}^{d}) by

|  |  |  |  |
| --- | --- | --- | --- |
|  | Sâ€‹[Î¼]â€‹(dâ€‹y)=âˆ«â„dpâ€‹(T,y;0,x)â€‹Î¼â€‹(dâ€‹x)â€‹ğ‘‘y,Î¼âˆˆğ’«2â€‹(â„d).\displaystyle S[\mu](dy)=\int\_{\mathbb{R}^{d}}p(T,y;0,x)\mu(dx)dy,\qquad\mu\in\mathscr{P}\_{2}(\mathbb{R}^{d}). |  | (5.2) |

Clearly, if Î¼âˆˆğ’«â€‹(â„d)\mu\in\mathscr{P}(\mathbb{R}^{d}), then Sâ€‹[Î¼]â€‹(dâ€‹y)=fXT0,Î¼â€‹(y)â€‹dâ€‹yS[\mu](dy)=f\_{X^{0,\mu}\_{T}}(y)dy, where X0,Î¼={Xt0,Î¼}tâˆˆ[0,T]X^{0,\mu}=\{X^{0,\mu}\_{t}\}\_{t\in[0,T]} denotes the solution to ([3.1](https://arxiv.org/html/2510.11829v1#S3.E1 "In 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) with X00,Î¼âˆ¼Î¼X^{0,\mu}\_{0}\sim\mu.
But the operator SS can be naturally extended to any Î¼âˆˆâ„³â€‹(â„d)\mu\in\mathscr{M}(\mathbb{R}^{d}), provided the right-hand side of ([5.2](https://arxiv.org/html/2510.11829v1#S5.E2 "In 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) is well-defined.

Let us now recall a well-known analogue of Lemma [3.1](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem1 "Lemma 3.1 ( [18, Theorem 3.1]). â€£ 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") in the case of general initial condition Î¼iniâˆˆğ’«2â€‹(â„d){\mu\_{\rm ini}}\in\mathscr{P}\_{2}(\mathbb{R}^{d}).

###### Proposition 5.2 ([[18](https://arxiv.org/html/2510.11829v1#bib.bib18), Theorem 3.2]).

Let Î¼iniâˆˆğ’«2â€‹(â„d){\mu\_{\rm ini}}\in\mathscr{P}\_{2}(\mathbb{R}^{d}), and assume that DKLâ€‹(Î¼iniâˆ¥Î½0)<âˆD\_{\rm KL}({\mu\_{\rm ini}}\|\nu\_{0})<\infty and DKLâ€‹(Î¼tarâˆ¥Sâ€‹[Î½0])<âˆD\_{\rm KL}({\mu\_{\rm tar}}\|S[\nu\_{0}])<\infty. Then, the optimal control for the (original) SBP ([2.5](https://arxiv.org/html/2510.11829v1#S2.E5 "In SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday"))-([2.6](https://arxiv.org/html/2510.11829v1#S2.E6 "In SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) is given by Î±^t=âˆ‡logâ¡hâ€‹(t,XtÎ±^)\widehat{\alpha}\_{t}=\nabla\log h(t,X^{\widehat{\alpha}}\_{t}) where, denoting ÏÎ¼tarâ€‹(â‹…)\rho^{\mu\_{\rm tar}}(\cdot) to be the density function of Î½T\nu\_{T},

|  |  |  |  |
| --- | --- | --- | --- |
|  | hâ€‹(t,x):=âˆ«â„dpâ€‹(T,z;t,x)â€‹ÏÎ¼tarâ€‹(z)â€‹ğ‘‘z.\displaystyle h(t,x):=\int\_{\mathbb{R}^{d}}p(T,z;t,x)\rho^{\mu\_{\rm tar}}(z)dz. |  | (5.3) |

Moreover, it holds that

|  |  |  |  |
| --- | --- | --- | --- |
|  | Jâ€‹(Î±^)=âˆ«â„dlogâ¡ÏÎ¼tarâ€‹(y)â€‹Î¼tarâ€‹(dâ€‹y)âˆ’DKLâ€‹(Î¼iniâˆ¥Î½0).J(\widehat{\alpha})=\int\_{\mathbb{R}^{d}}\log\rho^{\mu\_{\rm tar}}(y){\mu\_{\rm tar}}(dy)-D\_{\rm KL}({\mu\_{\rm ini}}\|\nu\_{0}). |  | (5.4) |

We note that in the above DKLâ€‹(Î¼iniâˆ¥Î½0)=âˆ«logâ¡Î¼iniâ€‹(dâ€‹x)Î½0â€‹(dâ€‹x)â€‹Î¼iniâ€‹(dâ€‹x)D\_{\rm KL}({\mu\_{\rm ini}}\|\nu\_{0})=\int\log\frac{{\mu\_{\rm ini}}(dx)}{\nu\_{0}(dx)}{\mu\_{\rm ini}}(dx) (see footnote 1), and ([5.1](https://arxiv.org/html/2510.11829v1#S5.E1 "In Proposition 5.1 ([7]). â€£ 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) implies that Î¼iniâ€‹(dâ€‹x)Î½0â€‹(dâ€‹x)=âˆ«pâ€‹(T,y;0,x)â€‹ÏÎ¼tarâ€‹(y)â€‹ğ‘‘y\frac{{\mu\_{\rm ini}}(dx)}{\nu\_{0}(dx)}=\int p(T,y;0,x)\rho^{\mu\_{\rm tar}}(y)dy. Therefore ([5.4](https://arxiv.org/html/2510.11829v1#S5.E4 "In Proposition 5.2 ([18, Theorem 3.2]). â€£ 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) can be rewritten as

|  |  |  |  |
| --- | --- | --- | --- |
|  | Jâ€‹(Î±^)\displaystyle J(\widehat{\alpha}) | =âˆ«â„dlogâ¡ÏÎ¼tarâ€‹(y)â€‹Î¼tarâ€‹(dâ€‹y)âˆ’âˆ«â„dlogâ¡(âˆ«â„dpâ€‹(T,y;0,x)â€‹ÏÎ¼tarâ€‹(y)â€‹ğ‘‘y)â€‹Î¼iniâ€‹(dâ€‹x)\displaystyle=\int\_{\mathbb{R}^{d}}\log\rho^{\mu\_{\rm tar}}(y){\mu\_{\rm tar}}(dy)-\int\_{\mathbb{R}^{d}}\log\Big(\int\_{\mathbb{R}^{d}}p(T,y;0,x)\rho^{\mu\_{\rm tar}}(y)dy\Big){\mu\_{\rm ini}}(dx) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =ğ”¼â€‹[logâ¡ÏÎ¼tarâ€‹(XTÎ±^)]âˆ’âˆ«â„dlogâ¡hâ€‹(0,x)â€‹Î¼iniâ€‹(dâ€‹x)=ğ”¼â€‹[logâ¡ÏÎ¼tarâ€‹(XTÎ±^)]âˆ’ğ”¼â€‹[logâ¡hâ€‹(0,X0Î±^)].\displaystyle=\mathbb{E}[\log\rho^{\mu\_{\rm tar}}(X^{\widehat{\alpha}}\_{T})]-\int\_{\mathbb{R}^{d}}\log h(0,x){\mu\_{\rm ini}}(dx)=\mathbb{E}\big[\log\rho^{\mu\_{\rm tar}}(X^{\widehat{\alpha}}\_{T})\big]-\mathbb{E}[\log h(0,X^{\widehat{\alpha}}\_{0})]. |  |

Moreover, for fixed Î¼iniâˆˆğ’«2â€‹(â„d){\mu\_{\rm ini}}\in\mathscr{P}\_{2}(\mathbb{R}^{d}) and Î¼âˆˆğ’Ÿ=ğ’ŸÎ¼ini\mu\in\mathscr{D}=\mathscr{D}\_{{\mu\_{\rm ini}}},
we define hÎ¼â€‹(t,x)=âˆ«â„dpâ€‹(T,z;t,x)â€‹ÏÎ¼â€‹(z)â€‹ğ‘‘zh^{\mu}(t,x)=\int\_{\mathbb{R}^{d}}p(T,z;t,x)\rho^{\mu}(z)dz. Then, we have the following result.

###### Lemma 5.3 ([[29](https://arxiv.org/html/2510.11829v1#bib.bib29), Lemma 3.1]).

Let Î¼âˆˆğ’Ÿ=ğ’ŸÎ¼ini\mu\in\mathscr{D}=\mathscr{D}\_{{\mu\_{\rm ini}}}. Then, for any {Î±t}âˆˆğ•ƒğ”½02â€‹([0,T])\{\alpha\_{t}\}\in\mathbb{L}^{2}\_{\mathbb{F}^{0}}([0,T]), it holds that

|  |  |  |
| --- | --- | --- |
|  | Jâ€‹(Î±)â‰¥ğ”¼â€‹[logâ¡ÏÎ¼â€‹(XTÎ±)]âˆ’ğ”¼â€‹[logâ¡hÎ¼â€‹(0,X0Î±)].\displaystyle J(\alpha)\geq\mathbb{E}[\log\rho^{\mu}(X^{\alpha}\_{T})]-\mathbb{E}[\log h^{\mu}(0,X^{\alpha}\_{0})]. |  |

The equality holds when Î±t=Î±tÎ¼=âˆ‡logâ¡hÎ¼â€‹(t,XtÎ±Î¼)\alpha\_{t}=\alpha^{\mu}\_{t}=\nabla\log h^{\mu}(t,X^{\alpha^{\mu}}\_{t}), tâˆˆ[0,T]t\in[0,T] and XTÎ±Î¼âˆ¼Î¼X^{\alpha^{\mu}}\_{T}\sim\mu.
âˆ

From Proposition [5.2](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem2 "Proposition 5.2 ([18, Theorem 3.2]). â€£ 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") and Lemma [5.3](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem3 "Lemma 5.3 ([29, Lemma 3.1]). â€£ 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") we see that the density function ÏÎ¼\rho^{\mu} plays an important role in the structure of the solution of SBP. We shall be particularly interested in the continuous dependence of ÏÎ¼:=Î“1â€‹(Î¼)\rho^{\mu}:=\Gamma\_{1}(\mu) on Î¼âˆˆğ’«2â€‹(â„d)\mu\in\mathscr{P}\_{2}(\mathbb{R}^{d}), which we shall refer to as the Stability of the
SBP, borrowing the well-known concept of the SBP theory (cf. e.g., [[46](https://arxiv.org/html/2510.11829v1#bib.bib46), [22](https://arxiv.org/html/2510.11829v1#bib.bib22), [10](https://arxiv.org/html/2510.11829v1#bib.bib10)]). In light of ([6.4](https://arxiv.org/html/2510.11829v1#S6.E4 "In 6 Existence of optimal control and convergence for general ğœ‡áµ¢â‚™áµ¢ â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")), we see that if both Î¼ini{\mu\_{\rm ini}} and Î¼\mu have densities, then so does ğ’¯â€‹(Î¼ini,Î¼){\cal T}({\mu\_{\rm ini}},\mu).
Furthermore, in light of ([5.3](https://arxiv.org/html/2510.11829v1#S5.E3 "In Proposition 5.2 ([18, Theorem 3.2]). â€£ 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")), for any Ïâˆˆğ•ƒ1â€‹(â„d)\rho\in\mathbb{L}^{1}(\mathbb{R}^{d}), we define hÏâ€‹(t,x)=âˆ«â„dpâ€‹(T,z;t,x)â€‹Ïâ€‹(z)â€‹ğ‘‘zh^{\rho}(t,x)=\int\_{\mathbb{R}^{d}}p(T,z;t,x)\rho(z)dz. Then clearly we have hÎ¼â€‹(t,x)=hÏÎ¼â€‹(t,x)h^{\mu}(t,x)=h^{\rho^{\mu}}(t,x).

To continue our discussion, we shall identify a set â„°âŠ‚ğ’«2â€‹(â„d)\mathscr{E}\subset\mathscr{P}\_{2}(\mathbb{R}^{d})
on which an argument based on Schauderâ€™s fixed-point theorem can be carried out. We begin by denoting

|  |  |  |
| --- | --- | --- |
|  | ğ’¦:={Î¼âˆˆğ’«2â€‹(â„d):Î¼Â has densityÂ fÎ¼âˆˆğ•ƒ1â€‹(â„d)}.\displaystyle{\cal K}:=\Big\{\mu\in\mathscr{P}\_{2}(\mathbb{R}^{d}):\mbox{$\mu$ has density $f\_{\mu}\in\mathbb{L}^{1}(\mathbb{R}^{d})$}\Big\}. |  |

Furthermore, we shall make use of the following assumption.

###### Assumption 5.4.

There exists a function gâˆˆğ•ƒ2â€‹(â„d;(0,1])g\in\mathbb{L}^{2}(\mathbb{R}^{d};(0,1]), with âˆ«â„d|x|2â€‹gâ€‹(x)â€‹ğ‘‘s<âˆ\int\_{\mathbb{R}^{d}}|x|^{2}g(x)ds<\infty, and a constant K>0K>0, such that â€–fÎ¼g2â€–âˆâ‰¤K\Big\|\frac{f\_{\mu}}{g^{2}}\Big\|\_{\infty}\leq K, for all Î¼âˆˆğ’¦\mu\in\mathcal{K}.

We shall consider the following two sets that will play a crucial role in our discussion.

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„°:={Î¼âˆˆğ’¦:Â AssumptionÂ [5.4](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem4 "Assumption 5.4. â€£ 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")Â holds}âŠ‚ğ’«2â€‹(â„d);ğ’®â„°:={fÎ¼:Î¼âˆˆâ„°}âŠ‚ğ•ƒ1â€‹(â„d).\displaystyle\mathscr{E}:=\left\{\mu\in{\cal K}:\text{ Assumption \ref{assum:g} holds}\right\}\subset\mathscr{P}\_{2}(\mathbb{R}^{d});\quad{\cal S}\_{\mathscr{E}}:=\left\{f\_{\mu}:\mu\in\mathscr{E}\right\}\subset\mathbb{L}^{1}(\mathbb{R}^{d}). |  | (5.5) |

###### Remark 5.5.

A typical example of the function gg in Assumption [5.4](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem4 "Assumption 5.4. â€£ 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") is eâˆ’câ€‹|x|e^{-c|x|} or eâˆ’câ€‹|x|2e^{-c|x|^{2}}, xâˆˆâ„dx\in\mathbb{R}^{d}, c>0c>0. In such a case we see that part (i) holds for all p>0p>0. Assumption [5.4](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem4 "Assumption 5.4. â€£ 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") amounts to saying that we focus only on those density functions that have a similar decay rate to function gg at xâˆ¼âˆx\sim\infty. In fact, in light of the estimate ([3.2](https://arxiv.org/html/2510.11829v1#S3.E2 "In 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")), such a property holds essentially for all transition probabilities of diffusion processes.
âˆ

The following lemma lists some basic properties of the set â„°\mathscr{E} (or Assumption [5.4](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem4 "Assumption 5.4. â€£ 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")).

###### Lemma 5.6.

Assume that Assumption [5.4](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem4 "Assumption 5.4. â€£ 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") is in force. Then it holds that

(i) The set {fÎ¼}Î¼âˆˆâ„°\{f\_{\mu}\}\_{\mu\in\mathscr{E}} is uniformly bounded in ğ•ƒ2â€‹(â„d)\mathbb{L}^{2}(\mathbb{R}^{d}).

(ii) The set {fÎ¼}Î¼âˆˆâ„°\{f\_{\mu}\}\_{\mu\in\mathscr{E}} is uniformly integrable in ğ’«2â€‹(â„d)\mathscr{P}\_{2}(\mathbb{R}^{d}), in the sense that

|  |  |  |  |
| --- | --- | --- | --- |
|  | limRâ†’âˆsupÎ¼âˆˆâ„°âˆ«{|x|â‰¥R}|x|2â€‹fÎ¼â€‹(x)â€‹ğ‘‘x=0.\displaystyle\lim\_{R\to\infty}\sup\_{\mu\in\mathscr{E}}\int\_{\{|x|\geq R\}}|x|^{2}f\_{\mu}(x)dx=0. |  | (5.6) |

(iii) If {Î¼n}nâ‰¥1âŠ‚â„°\{\mu\_{n}\}\_{{n\geq 1}}\subset\mathscr{E} such that Î¼nâ‡’Î¼\mu\_{n}\Rightarrow\mu, as nâ†’âˆn\to\infty, then â€–fÎ¼nâˆ’fÎ¼â€–ğ•ƒ1â†’0\|f\_{\mu\_{n}}-f\_{\mu}\|\_{\mathbb{L}^{1}}\to 0.

###### Proof.

For any Î¼âˆˆâ„°\mu\in\mathscr{E}, we note that 0<gâ€‹(x)â‰¤10<g(x)\leq 1, and by assumption,

|  |  |  |
| --- | --- | --- |
|  | âˆ«â„d|fÎ¼â€‹(x)|2â€‹ğ‘‘xâ‰¤K2â€‹âˆ«â„d|gâ€‹(x)|4â€‹ğ‘‘sâ‰¤K2â€‹â€–gâ€–ğ•ƒ22,\int\_{\mathbb{R}^{d}}|f\_{\mu}(x)|^{2}dx\leq K^{2}\int\_{\mathbb{R}^{d}}|g(x)|^{4}ds\leq K^{2}\|g\|^{2}\_{\mathbb{L}^{2}}, |  |

That is {fÎ¼}Î¼âˆˆâ„°\{f\_{\mu}\}\_{\mu\in\mathscr{E}} is uniformly bounded (by Kâ€‹â€–gâ€–ğ•ƒ2K\|g\|\_{\mathbb{L}^{2}}) in ğ•ƒ2â€‹(â„d)\mathbb{L}^{2}(\mathbb{R}^{d}), proving (i).

Similarly, for any Î¼âˆˆâ„°\mu\in\mathscr{E}, by the absolute continuity of the integral we have

|  |  |  |
| --- | --- | --- |
|  | supÎ¼âˆˆâ„°âˆ«{|x|â‰¥R}|x|2â€‹fÎ¼â€‹(x)â€‹ğ‘‘xâ‰¤Kâ€‹âˆ«{|x|â‰¥R}|x|2â€‹gâ€‹(x)â€‹ğ‘‘xâ†’0,asÂ Râ†’âˆ,\displaystyle\sup\_{\mu\in\mathscr{E}}\int\_{\{|x|\geq R\}}|x|^{2}f\_{\mu}(x)dx\leq K\int\_{\{|x|\geq R\}}|x|^{2}g(x)dx\to 0,\quad\mbox{\rm as $R\to\infty$,} |  |

This proves ([5.6](https://arxiv.org/html/2510.11829v1#S5.E6 "In Lemma 5.6. â€£ 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")), whence (ii).

The proof of part (iii) is slightly more involved, which is in the spirit of the so-called ScheffÃ©â€™s theorem (cf. [[59](https://arxiv.org/html/2510.11829v1#bib.bib59)]).
We note that Î¼nâ‡’Î¼\mu\_{n}\Rightarrow\mu amounts to saying that fÎ¼nâ€‹â‡€wfÎ¼f\_{\mu\_{n}}\mathop{\mathrel{\mathop{\kern 0.0pt\rightharpoonup}\limits^{w}}}f\_{\mu}, as nâ†’âˆn\to\infty, in ğ•ƒ2â€‹(â„d)\mathbb{L}^{2}(\mathbb{R}^{d}). To show fÎ¼nâ†’fÎ¼f\_{\mu\_{n}}\to f\_{\mu} in ğ•ƒ1â€‹(â„d)\mathbb{L}^{1}(\mathbb{R}^{d}), we first consider, for each m>0m>0, the smooth mollifiers Ï†mâˆˆâ„‚âˆâ€‹(â„d;â„+)\varphi^{m}\in\mathbb{C}^{\infty}(\mathbb{R}^{d};\mathbb{R}\_{+}) such that âˆ«â„dÏ†mâ€‹(z)â€‹ğ‘‘z=1\int\_{\mathbb{R}^{d}}\varphi^{m}(z)dz=1, mâ‰¥1m\geq 1, and denote

|  |  |  |
| --- | --- | --- |
|  | fÎ¼nmâ€‹(x)=[Ï†mâˆ—fÎ¼n]â€‹(y)=âˆ«â„dÏ†mâ€‹(xâˆ’z)â€‹fÎ¼nâ€‹(z)â€‹ğ‘‘z;fÎ¼mâ€‹(x)=[Ï†mâˆ—fÎ¼]â€‹(x),xâˆˆâ„d.f^{m}\_{\mu\_{n}}(x)=[\varphi^{m}\*f\_{\mu\_{n}}](y)=\int\_{\mathbb{R}^{d}}\varphi^{m}(x-z)f\_{\mu\_{n}}(z)dz;\quad f^{m}\_{\mu}(x)=[\varphi^{m}\*f\_{\mu}](x),\quad x\in\mathbb{R}^{d}. |  |

Then it is clear that for each nâˆˆâ„•n\in\mathbb{N}, limmâ†’âˆfÎ¼nmâ€‹(x)=fÎ¼nâ€‹(x)\lim\_{m\to\infty}f^{m}\_{\mu\_{n}}(x)=f\_{\mu\_{n}}(x) and limmâ†’âˆfÎ¼mâ€‹(x)=fÎ¼â€‹(x)\lim\_{m\to\infty}f^{m}\_{\mu}(x)=f\_{\mu}(x), for a.e. xâˆˆâ„dx\in\mathbb{R}^{d}. We should remark that the convergence is uniform in nn. Indeed, by Assumption [5.4](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem4 "Assumption 5.4. â€£ 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") and Dominated Convergence Theorem we have, as mâ†’âˆm\to\infty, for all nâ‰¥0n\geq 0,
and xâˆˆâ„dx\in\mathbb{R}^{d},

|  |  |  |
| --- | --- | --- |
|  | |fÎ¼nmâ€‹(x)âˆ’fÎ¼nâ€‹(x)|â‰¤âˆ«â„d|Ï†mâ€‹(xâˆ’z)âˆ’Î´xâ€‹(z)|â€‹fÎ¼nâ€‹(z)â€‹ğ‘‘zâ‰¤Kâ€‹âˆ«â„d|Ï†mâ€‹(xâˆ’z)âˆ’Î´xâ€‹(z)|â€‹g2â€‹(z)â€‹ğ‘‘zâ†’0.|f^{m}\_{\mu\_{n}}(x)-f\_{\mu\_{n}}(x)|\leq\int\_{\mathbb{R}^{d}}|\varphi^{m}(x-z)-\delta\_{x}(z)|f\_{\mu\_{n}}(z)dz\leq K\int\_{\mathbb{R}^{d}}|\varphi^{m}(x-z)-\delta\_{x}(z)|g^{2}(z)dz\to 0. |  |

Furthermore, since supÎ¼âˆˆâ„°|fÎ¼|â‰¤Kâ€‹g2âˆˆğ•ƒ1â€‹(â„d)\sup\_{\mu\in\mathscr{E}}|f\_{\mu}|\leq Kg^{2}\in\mathbb{L}^{1}(\mathbb{R}^{d}), by Dominated Convergence Theorem we have limmâ†’âˆfÎ¼nm=fÎ¼n\lim\_{m\to\infty}f^{m}\_{\mu\_{n}}=f\_{\mu\_{n}} in ğ•ƒ1â€‹(â„d)\mathbb{L}^{1}(\mathbb{R}^{d}), uniformly for nâ‰¥0n\geq 0. That is, for any Îµ>0\varepsilon>0, there exists
Mâ€‹(Îµ)>0M(\varepsilon)>0, such that for all nâ‰¥1n\geq 1, it holds that

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–fÎ¼nmâˆ’fÎ¼nâ€–ğ•ƒ1<Îµ3;â€–fÎ¼mâˆ’fÎ¼â€–ğ•ƒ1<Îµ3,wheneverÂ m>M.\displaystyle\|f^{m}\_{\mu\_{n}}-f\_{\mu\_{n}}\|\_{\mathbb{L}^{1}}<\frac{\varepsilon}{3};\quad\|f^{m}\_{\mu}-f\_{\mu}\|\_{\mathbb{L}^{1}}<\frac{\varepsilon}{3},\quad\mbox{whenever $m>M$.} |  | (5.7) |

In the sequel we fix m>Mâ€‹(Îµ)m>M(\varepsilon), and take a closer look at the sequence {fÎ¼nm}nâ‰¥1\{f^{m}\_{\mu\_{n}}\}\_{n\geq 1}. Clearly, each fÎ¼nmf^{m}\_{\mu\_{n}} is still a density function, and it holds that

|  |  |  |  |
| --- | --- | --- | --- |
|  | supn|fÎ¼nmâ€‹(y)|â‰¤supn(Ï†mâˆ—|fÎ¼n|)â€‹(y)â‰¤K.\displaystyle\sup\_{n}|f^{m}\_{\mu\_{n}}(y)|\leq\sup\_{n}(\varphi^{m}\*|f\_{\mu\_{n}}|)(y)\leq K. |  | (5.8) |

Moreover, since Ï†m\varphi^{m} is continuous, thus for any x,yâˆˆâ„dx,y\in\mathbb{R}^{d},
applying the Dominated Convergence Theorem we have

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | |fÎ¼nmâ€‹(x+y)âˆ’fÎ¼nmâ€‹(x)|\displaystyle|f^{m}\_{\mu\_{n}}(x+y)-f^{m}\_{\mu\_{n}}(x)| | â‰¤\displaystyle\leq | âˆ«â„d|Ï†mâ€‹(x+yâˆ’z)âˆ’Ï†mâ€‹(xâˆ’z)|â€‹|fÎ¼nâ€‹(z)|â€‹ğ‘‘z\displaystyle\int\_{\mathbb{R}^{d}}|\varphi^{m}(x+y-z)-\varphi^{m}(x-z)||f\_{\mu\_{n}}(z)|dz |  |
|  |  | â‰¤\displaystyle\leq | Kâ€‹âˆ«â„d|Ï†mâ€‹(zâ€²âˆ’y)âˆ’Ï†mâ€‹(zâ€²)|â€‹ğ‘‘zâ€²â†’0,asÂ yâ†’0.\displaystyle K\int\_{\mathbb{R}^{d}}|\varphi^{m}(z^{\prime}-y)-\varphi^{m}(z^{\prime})|dz^{\prime}\to 0,\quad\mbox{as $y\to 0$.} |  |

Clearly, the convergence above is uniform in nn. That is, the sequence {fÎ¼nm}{nâ‰¥1}\{f^{m}\_{\mu\_{n}}\}\_{\{n\geq 1\}} is so-called asymptotically equi-continuous in the sense of Sweeting [[59](https://arxiv.org/html/2510.11829v1#bib.bib59)]. This, together with ([5.8](https://arxiv.org/html/2510.11829v1#S5.E8 "In 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")), implies that limnâ†’âˆfÎ¼nm=fÎ¼m\displaystyle\lim\_{n\to\infty}f^{m}\_{\mu\_{n}}=f^{m}\_{\mu}, uniformly on compacts in â„d\mathbb{R}^{d} (cf. [[59](https://arxiv.org/html/2510.11829v1#bib.bib59), Theorem 1]). Applying the Dominated Convergence Theorem again we have limnâ†’âˆâ€–fÎ¼nmâˆ’fÎ¼mâ€–ğ•ƒ1=0\lim\_{n\to\infty}\|f^{m}\_{\mu\_{n}}-f^{m}\_{\mu}\|\_{\mathbb{L}^{1}}=0. That is, for the given Îµ>0\varepsilon>0 in ([5.7](https://arxiv.org/html/2510.11829v1#S5.E7 "In 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")), there exists N>0N>0 such that
â€–fÎ¼nmâˆ’fÎ¼mâ€–ğ•ƒ1<Îµ3\|f^{m}\_{\mu\_{n}}-f^{m}\_{\mu}\|\_{\mathbb{L}^{1}}<\frac{\varepsilon}{3}, whenever n>Nn>N. This, together with ([5.7](https://arxiv.org/html/2510.11829v1#S5.E7 "In 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")), yields

|  |  |  |
| --- | --- | --- |
|  | â€–fÎ¼nâˆ’fÎ¼â€–ğ•ƒ1â‰¤â€–fÎ¼nâˆ’fÎ¼nmâ€–ğ•ƒ1+â€–fÎ¼nmâˆ’fÎ¼mâ€–ğ•ƒ1+â€–fÎ¼mâˆ’fÎ¼â€–ğ•ƒ1<Îµ3+Îµ3+Îµ3=Îµ,n>N,\|f\_{\mu\_{n}}-f\_{\mu}\|\_{\mathbb{L}^{1}}\leq\|f\_{\mu\_{n}}-f^{m}\_{\mu\_{n}}\|\_{\mathbb{L}^{1}}+\|f^{m}\_{\mu\_{n}}-f^{m}\_{\mu}\|\_{\mathbb{L}^{1}}+\|f^{m}\_{\mu}-f\_{\mu}\|\_{\mathbb{L}^{1}}<\frac{\varepsilon}{3}+\frac{\varepsilon}{3}+\frac{\varepsilon}{3}=\varepsilon,\quad n>N, |  |

proving (iii), whence the Lemma.
âˆ

We are now ready to study our main stability result. More precisely, we shall argue that the mapping Î“1:ğ’«2â€‹(â„d)â†¦ğ•ƒ1â€‹(â„d)\Gamma\_{1}:\mathscr{P}\_{2}(\mathbb{R}^{d})\mapsto\mathbb{L}^{1}(\mathbb{R}^{d}) is continuous. That is, that Î¼n\mu\_{n} weakly converges to Î¼tar{\mu\_{\rm tar}} in Prohorov metric would imply that ÏÎ¼n\rho^{\mu\_{n}} converges to ÏÎ¼tar\rho^{{\mu\_{\rm tar}}} in ğ•ƒ1â€‹(â„d)\mathbb{L}^{1}(\mathbb{R}^{d}). Such a result, to the best of our knowledge, is novel in the literature.

To simplify our discussion, in what follows, we assume T=1T=1, and denote the measure Î¼\mu in ([5.1](https://arxiv.org/html/2510.11829v1#S5.E1 "In Proposition 5.1 ([7]). â€£ 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) by Ï€\pi for notational clarity. Recall that Ï€\pi has marginals Î¼ini{\mu\_{\rm ini}} and Î¼\mu, and in what follows, we shall assume that Î¼ini{\mu\_{\rm ini}} is fixed and Î¼âˆˆâ„°\mu\in\mathscr{E}. Let us now consider the following entropic optimal transport problem:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Iâ€‹(Î¼):=infÏ€âˆˆÎ â€‹(Î¼ini,Î¼)âˆ«â„dÃ—â„dcâ€‹(x,y)â€‹Ï€â€‹(dâ€‹xâ€‹dâ€‹y)+DKLâ€‹(Ï€âˆ¥Î¼iniâŠ—Î¼),I(\mu):=\inf\_{\pi\in\Pi({\mu\_{\rm ini}},\mu)}\int\_{\mathbb{R}^{d}\times\mathbb{R}^{d}}{\textbf{c}}(x,y)\pi(dxdy)+D\_{\rm KL}(\pi\|{\mu\_{\rm ini}}\otimes\mu), |  | (5.9) |

where Î â€‹(Î¼ini,Î¼)\Pi({\mu\_{\rm ini}},\mu) is the set of all coupling probability measures Ï€\pi on â„dÃ—â„d\mathbb{R}^{d}\times\mathbb{R}^{d} with marginals Î¼ini{\mu\_{\rm ini}} and Î¼\mu; and câ€‹(â‹…,â‹…){\textbf{c}}(\cdot,\cdot) is a continuous cost function. It is well-known (see, e.g., [[22](https://arxiv.org/html/2510.11829v1#bib.bib22), [16](https://arxiv.org/html/2510.11829v1#bib.bib16), [31](https://arxiv.org/html/2510.11829v1#bib.bib31), [46](https://arxiv.org/html/2510.11829v1#bib.bib46)]) that the minimization ([5.9](https://arxiv.org/html/2510.11829v1#S5.E9 "In 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) admits a unique solution Ï€^\widehat{\pi}, whose density takes the form:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ï€^â€‹(dâ€‹xâ€‹dâ€‹y)=expâ¡(âˆ’câ€‹(x,y)+Ï•Î¼â€‹(x)+ÏˆÎ¼â€‹(y))â€‹Î¼iniâ€‹(dâ€‹x)â€‹Î¼â€‹(dâ€‹y),\widehat{\pi}(dxdy)=\exp\big(-{\textbf{c}}(x,y)+\phi^{\mu}(x)+\psi^{\mu}(y)\big){\mu\_{\rm ini}}(dx)\mu(dy), |  | (5.10) |

where Ï•Î¼,ÏˆÎ¼\phi^{\mu},\psi^{\mu}: â„dâ†’â„\mathbb{R}^{d}\to\mathbb{R} are two measurable functions, often referred to as the SchrÃ¶dinger potentials. It is clear that the pair (Ï•Î¼,ÏˆÎ¼)(\phi^{\mu},\psi^{\mu}) is unique up to an additive constant. That is, if (Ï•Î¼,ÏˆÎ¼)(\phi^{\mu},\psi^{\mu}) is a pair of SchrÃ¶dinger potentials, then so is (Ï•Î¼+c,ÏˆÎ¼âˆ’c)(\phi^{\mu}+c,\psi^{\mu}-c). Furthermore, since both Î¼ini{\mu\_{\rm ini}} and Î¼\mu are probability measures, we can easily choose a constant cc so that
the following symmetric normalization holds:

|  |  |  |  |
| --- | --- | --- | --- |
|  | âˆ«Ï•Î¼â€‹(x)â€‹Î¼iniâ€‹(dâ€‹x)=âˆ«ÏˆÎ¼â€‹(y)â€‹Î¼â€‹(dâ€‹y).\displaystyle\int\phi^{\mu}(x){\mu\_{\rm ini}}(dx)=\int\psi^{\mu}(y)\mu(dy). |  | (5.11) |

(Otherwise we take c=12â€‹[âˆ’âˆ«Ï•Î¼â€‹(x)â€‹Î¼iniâ€‹(dâ€‹x)+âˆ«ÏˆÎ¼â€‹(y)â€‹Î¼â€‹(dâ€‹y)]c=\frac{1}{2}\big[-\int\phi^{\mu}(x){\mu\_{\rm ini}}(dx)+\int\psi^{\mu}(y)\mu(dy)\big].) Note that under the symmetric normalization, the SchrÃ¶dinger potentials is unique. The following stability result for the mappings Î¼â†¦(Ï•Î¼,ÏˆÎ¼)\mu\mapsto(\phi^{\mu},\psi^{\mu}) is crucial for our discussion.

###### Lemma 5.7 ([[10](https://arxiv.org/html/2510.11829v1#bib.bib10), Theorem 1.1]).

Assume that the cost function câ€‹(â‹…,â‹…)âˆˆâ„‚k+1â€‹(â„dÃ—â„d){\textbf{c}}(\cdot,\cdot)\in\mathbb{C}^{k+1}(\mathbb{R}^{d}\times\mathbb{R}^{d}) for some kâˆˆâ„•k\in\mathbb{N}. Then there exists C>0C>0 depending only on â€–câ€–â„‚k+1\|{\textbf{c}}\|\_{\mathbb{C}^{k+1}}, such that for all Î¼1\mu\_{1}, Î¼2âˆˆğ’«2â€‹(â„d)\mu\_{2}\in\mathscr{P}\_{2}(\mathbb{R}^{d}), it holds that

|  |  |  |
| --- | --- | --- |
|  | â€–(Ï•Î¼1âˆ’Ï•Î¼2,ÏˆÎ¼1âˆ’ÏˆÎ¼2)â€–âˆ—â‰¤Câ€‹W2â€‹(Î¼1,Î¼2),\|(\phi^{\mu\_{1}}-\phi^{\mu\_{2}},\psi^{\mu\_{1}}-\psi^{\mu\_{2}})\|\_{\*}\leq CW\_{2}(\mu\_{1},\ \mu\_{2}), |  |

where â€–(Ï•,Ïˆ)â€–âˆ—:=infcâˆˆâ„{â€–Ï•âˆ’câ€–â„‚kâ€‹(â„d)+â€–Ïˆ+câ€–â„‚kâ€‹(â„d)}\|(\phi,\psi)\|\_{\*}:=\inf\_{c\in\mathbb{R}}\left\{\|\phi-c\|\_{\mathbb{C}^{k}(\mathbb{R}^{d})}+\|\psi+c\|\_{\mathbb{C}^{k}(\mathbb{R}^{d})}\right\}.
âˆ

We now proceed to prove the main result of this section. To begin with, let us consider the entropic optimal transport problem ([5.9](https://arxiv.org/html/2510.11829v1#S5.E9 "In 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) with câ€‹(x,y):=âˆ’logâ¡pâ€‹(1,y;0,x){\textbf{c}}(x,y):=-\log p(1,y;0,x), x,yâˆˆâ„dx,y\in\mathbb{R}^{d}, where pâ€‹(s,y;t,x)p(s,y;t,x), 0â‰¤t<sâ‰¤10\leq t<s\leq 1 and x,yâˆˆâ„dx,y\in\mathbb{R}^{d}, is the transition density of the diffusion ([3.1](https://arxiv.org/html/2510.11829v1#S3.E1 "In 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")). By ([5.10](https://arxiv.org/html/2510.11829v1#S5.E10 "In 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")), for fixed Î¼ini,Î¼âˆˆğ’«2â€‹(â„d){\mu\_{\rm ini}},\mu\in\mathscr{P}\_{2}(\mathbb{R}^{d}), the unique solution for this entropic optimal transport problem is given by (see also [[46](https://arxiv.org/html/2510.11829v1#bib.bib46)])

|  |  |  |
| --- | --- | --- |
|  | Ï€^â€‹(dâ€‹xâ€‹dâ€‹y)=pâ€‹(1,y;0,x)â€‹eÏ•Î¼â€‹(x)+ÏˆÎ¼â€‹(y)â€‹f0â€‹(x)â€‹fÎ¼â€‹(y)â€‹dâ€‹xâ€‹dâ€‹y,\displaystyle\widehat{\pi}(dxdy)=p(1,y;0,x)e^{\phi^{\mu}(x)+\psi^{\mu}(y)}f\_{0}(x)f\_{\mu}(y)dxdy, |  |

where Ï•Î¼\phi^{\mu} and ÏˆÎ¼\psi^{\mu} are the SchrÃ¶dinger potentials, and we shall enforce the symmetric normalization so that
they satisfy ([5.11](https://arxiv.org/html/2510.11829v1#S5.E11 "In 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")). Since Ï€^\widehat{\pi} has the marginals Î¼ini{\mu\_{\rm ini}} and Î¼\mu, by the uniqueness of (Î½0,Î½)=ğ’¯â€‹(Î¼ini,Î¼)(\nu\_{0},\nu)={\cal T}({\mu\_{\rm ini}},\mu), whence (Ï0,ÏÎ¼)(\rho\_{0},\rho^{\mu}), in Proposition [5.1](https://arxiv.org/html/2510.11829v1#S5.E1 "In Proposition 5.1 ([7]). â€£ 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday"), we can conclude that

|  |  |  |
| --- | --- | --- |
|  | pâ€‹(1,y;0,x)â€‹Ï0â€‹(x)â€‹ÏÎ¼â€‹(y)=pâ€‹(1,y;0,x)â€‹eÏ•Î¼â€‹(x)+ÏˆÎ¼â€‹(y)â€‹f0â€‹(x)â€‹fÎ¼â€‹(y),x,yâˆˆâ„d.\displaystyle p(1,y;0,x)\rho\_{0}(x)\rho^{\mu}(y)=p(1,y;0,x)e^{\phi^{\mu}(x)+\psi^{\mu}(y)}f\_{0}(x)f\_{\mu}(y),\qquad x,y\in\mathbb{R}^{d}. |  |

An easy argument of separation of variables then yields that

|  |  |  |  |
| --- | --- | --- | --- |
|  | ÏÎ¼â€‹(y)=eÏˆÎ¼â€‹(y)â€‹fÎ¼â€‹(y);Ï0â€‹(x)=eÏ•Î¼â€‹(x)â€‹f0â€‹(x),x,yâˆˆâ„d.\displaystyle\rho^{\mu}(y)=e^{\psi^{\mu}(y)}f\_{\mu}(y);\quad\rho\_{0}(x)=e^{\phi^{\mu}(x)}f\_{0}(x),\qquad x,y\in\mathbb{R}^{d}. |  | (5.12) |

Now note that the transition density pâ€‹(â‹…,â‹…;â‹…,â‹…)p(\cdot,\cdot\,;\cdot,\cdot) is a classical solution to the Kolmogorov PDE. Thanks to Assumption [2.2](https://arxiv.org/html/2510.11829v1#S2.ThmTheorem2 "Assumption 2.2. â€£ SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday"), we can assume without loss of generality that câ€‹(â‹…,â‹…)=âˆ’logâ¡pâ€‹(1,â‹…;0,â‹…)âˆˆâ„‚2â€‹(â„dÃ—â„d){\textbf{c}}(\cdot,\cdot)=-\log p(1,\cdot\,;0,\cdot)\in\mathbb{C}^{2}(\mathbb{R}^{d}\times\mathbb{R}^{d}). Thus
according to Lemma [5.7](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem7 "Lemma 5.7 ([10, Theorem 1.1]). â€£ 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") and noting the definition of âˆ¥â‹…âˆ¥âˆ—\|\cdot\|\_{\*}, we see that, modulo some constant normalization, we have that the
SchrÃ¶dinger potential (Ï•Î¼n,ÏˆÎ¼n)(\phi^{\mu\_{n}},\psi^{\mu\_{n}}) itself satisfies the estimate:

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–Ï•Î¼nâˆ’Ï•Î¼â€–ğ•ƒâˆ+â€–ÏˆÎ¼nâˆ’ÏˆÎ¼â€–ğ•ƒâˆâ‰¤Câ€‹W2â€‹(Î¼n,Î¼).\|{\phi}^{\mu\_{n}}-\phi^{\mu}\|\_{\mathbb{L}^{\infty}}+\|{\psi}^{\mu\_{n}}-\psi^{\mu}\|\_{\mathbb{L}^{\infty}}\leq CW\_{2}(\mu\_{n},\mu). |  | (5.13) |

Here in the above the constant C>0C>0 depending only on â€–câ€–â„‚2\|{\textbf{c}}\|\_{\mathbb{C}^{2}}, but independent of nn.

Furthermore, we note that câˆˆâ„‚2{\textbf{c}}\in\mathbb{C}^{2} also lead to the following a priori estimate of the SchrÃ¶dinger potential (see, e.g., [[46](https://arxiv.org/html/2510.11829v1#bib.bib46), Lemma 2.1]):

|  |  |  |  |
| --- | --- | --- | --- |
|  | ÏˆÎ¼(y)â‰¤âˆ«â„dc(x,y)Î¼ini(dx)=:Î¾(y),yâˆˆâ„d.\displaystyle\psi^{\mu}(y)\leq\int\_{\mathbb{R}^{d}}{\textbf{c}}(x,y){\mu\_{\rm ini}}(dx)=:\xi(y),\quad y\in\mathbb{R}^{d}. |  | (5.14) |

Recall the fundamental estimate ([3.2](https://arxiv.org/html/2510.11829v1#S3.E2 "In 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) and the definition of câ€‹(â‹…,â‹…){\textbf{c}}(\cdot,\cdot), it is readily seen that Î¾â€‹(y)âˆ¼Î»â€‹|y|2\xi(y)\sim\lambda|y|^{2}, as yâ†’âˆy\to\infty, for some constant Î»>0\lambda>0 depending only on the coefficient bâ€‹(â‹…,â‹…)b(\cdot,\cdot) in SDE([3.1](https://arxiv.org/html/2510.11829v1#S3.E1 "In 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")). In light of Remark [5.5](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem5 "Remark 5.5. â€£ 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday"), we shall now assume, without loss of generality, that in Assumption [5.4](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem4 "Assumption 5.4. â€£ 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") the control function gg satisfies

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î·â€‹(â‹…):=eÎ¾â€‹(â‹…)â€‹g2â€‹(â‹…)âˆˆğ•ƒ1â€‹(â„d).\displaystyle\eta(\cdot):=e^{\xi(\cdot)}g^{2}(\cdot)\in\mathbb{L}^{1}(\mathbb{R}^{d}). |  | (5.15) |

Now for any fÎ¼âˆˆğ’®â„°f\_{\mu}\in{\cal S}\_{\mathscr{E}}, by Assumption [5.4](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem4 "Assumption 5.4. â€£ 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") and ([5.14](https://arxiv.org/html/2510.11829v1#S5.E14 "In 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) we have

|  |  |  |
| --- | --- | --- |
|  | 0â‰¤ÏÎ¼â€‹(y)=eÏˆÎ¼â€‹(y)â€‹fÎ¼â€‹(y)â‰¤eÎ¾â€‹(y)â€‹fÎ¼â€‹(y)â‰¤eÎ¾â€‹(y)â€‹Kâ€‹g2â€‹(y)â‰¤Kâ€‹Î·â€‹(y),yâˆˆâ„d.0\leq\rho^{\mu}(y)=e^{\psi^{\mu}(y)}\ f\_{\mu}(y)\leq e^{\xi(y)}f\_{\mu}(y)\leq e^{\xi(y)}Kg^{2}(y)\leq K\eta(y),\quad y\in\mathbb{R}^{d}. |  |

Consequently, we conclude that ÏÎ¼âˆˆğ•ƒ1â€‹(â„d)\rho^{\mu}\in\mathbb{L}^{1}(\mathbb{R}^{d}) for any Î¼âˆˆâ„°\mu\in\mathscr{E}, thanks to ([5.15](https://arxiv.org/html/2510.11829v1#S5.E15 "In 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")).

Bearing the above discussion in mind, we are now ready to present the main result of this section.

###### Proposition 5.8.

Assume that Assumptions [2.2](https://arxiv.org/html/2510.11829v1#S2.ThmTheorem2 "Assumption 2.2. â€£ SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") and [5.4](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem4 "Assumption 5.4. â€£ 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") are in force. Assume further that {Î¼n}nâ‰¥1âŠ‚â„°\{\mu\_{n}\}\_{n\geq 1}\subset\mathscr{E} and Î¼nâŸ¹Î¼\mu\_{n}\Longrightarrow\mu in Prohorov metric. Then â€–ÏÎ¼nâˆ’ÏÎ¼â€–ğ•ƒ1=â€–Î“1â€‹(Î¼n)âˆ’Î“1â€‹(Î¼)â€–ğ•ƒ1â†’0\|\rho^{\mu\_{n}}-\rho^{\mu}\|\_{\mathbb{L}^{1}}=\|\Gamma\_{1}(\mu\_{n})-\Gamma\_{1}(\mu)\|\_{\mathbb{L}^{1}}\to 0, as nâ†’âˆn\to\infty.

###### Proof.

Assume {Î¼n}nâ‰¥1âŠ‚â„°\{\mu\_{n}\}\_{n\geq 1}\subset\mathscr{E}, and Î¼nâŸ¹Î¼\mu\_{n}\Longrightarrow\mu, in Prohorov metric. By Lemma [5.6](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem6 "Lemma 5.6. â€£ 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")-(ii), {Î¼n}\{\mu\_{n}\} is uniformly integrable in ğ•ƒ2\mathbb{L}^{2}, thanks to Assumption [5.4](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem4 "Assumption 5.4. â€£ 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday"), and thus by the relationship between Wasserstein distance and Prohorov metric (see, [[65](https://arxiv.org/html/2510.11829v1#bib.bib65), Theorem 7.12]), we have W2â€‹(Î¼n,Î¼)â†’0W\_{2}(\mu\_{n},\mu)\to 0, as nâ†’âˆn\to\infty. Thus, if follows from ([5.13](https://arxiv.org/html/2510.11829v1#S5.E13 "In 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) that â€–ÏˆÎ¼nâˆ’ÏˆÎ¼â€–âˆâ†’0\|\psi^{\mu\_{n}}-\psi^{\mu}\|\_{\infty}\to 0, as nâ†’âˆn\to\infty.

Next, for each Î¼nâˆˆâ„°\mu\_{n}\in\mathscr{E}, nâ‰¥1n\geq 1, and Î¼\mu, we apply ([5.12](https://arxiv.org/html/2510.11829v1#S5.E12 "In 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) and write

|  |  |  |
| --- | --- | --- |
|  | ÏÎ¼nâ€‹(y)=eÏˆÎ¼nâ€‹(y)â€‹fÎ¼nâ€‹(y),ÏÎ¼â€‹(y)=eÏˆÎ¼â€‹(y)â€‹fÎ¼tarâ€‹(y),x,yâˆˆâ„d.\displaystyle\rho^{\mu\_{n}}(y)=e^{\psi^{\mu\_{n}}(y)}f\_{\mu\_{n}}(y),\quad\rho^{\mu}(y)=e^{\psi^{\mu}(y)}f\_{{\mu\_{\rm tar}}}(y),\qquad x,y\in\mathbb{R}^{d}. |  |

Therefore, for yâˆˆâ„dy\in\mathbb{R}^{d}, we have

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | |ÏÎ¼â€‹(y)âˆ’ÏÎ¼nâ€‹(y)|\displaystyle|\rho^{\mu}(y)-\rho^{\mu\_{n}}(y)| | =\displaystyle= | |eÏˆÎ¼â€‹(y)â€‹fÎ¼â€‹(y)âˆ’eÏˆÎ¼nâ€‹(y)â€‹fÎ¼nâ€‹(y)|\displaystyle\big|e^{\psi^{\mu}(y)}f\_{\mu}(y)-e^{\psi^{\mu\_{n}}(y)}f\_{\mu\_{n}}(y)\big| |  |
|  |  | â‰¤\displaystyle\leq | |eÏˆÎ¼â€‹(y)âˆ’eÏˆÎ¼nâ€‹(y)|fÎ¼n(y)+eÏˆÎ¼â€‹(y)|fÎ¼n(y)âˆ’fÎ¼(y)|=:In1(y)+In2(y),\displaystyle\big|e^{\psi^{\mu}(y)}-e^{\psi^{\mu\_{n}}(y)}\big|f\_{\mu\_{n}}(y)+e^{\psi^{\mu}(y)}\big|f\_{\mu\_{n}}(y)-f\_{\mu}(y)\big|=:I^{1}\_{n}(y)+I\_{n}^{2}(y), |  |

where IniI^{i}\_{n}, i=1,2i=1,2 are defined in an obvious way. It then suffices to show that both In1I^{1}\_{n} and In2â†’0I^{2}\_{n}\to 0 in ğ•ƒ1\mathbb{L}^{1}, as nâ†’âˆn\to\infty.

To this end, we first recall that â€–ÏˆÎ¼nâˆ’ÏˆÎ¼â€–âˆâ†’0\|\psi^{\mu\_{n}}-\psi^{\mu}\|\_{\infty}\to 0, as nâ†’âˆn\to\infty. Hence there exists N>0N>0, such that ÏˆÎ¼nâ€‹(y)â‰¤ÏˆÎ¼â€‹(y)+1\psi^{\mu\_{n}}(y)\leq\psi^{\mu}(y)+1, for all yâˆˆâ„dy\in\mathbb{R}^{d}, whenever nâ‰¥Nn\geq N.
Thus, for nâ‰¥Nn\geq N, we have

|  |  |  |
| --- | --- | --- |
|  | 0â‰¤In1â€‹(y)â‰¤(|eÏˆÎ¼â€‹(y)|+|eÏˆÎ¼nâ€‹(y)|)â€‹fÎ¼nâ€‹(y)â‰¤2â€‹eÏˆÎ¼â€‹(y)+1â€‹fÎ¼nâ€‹(y)â‰¤2â€‹eâ‹…eÎ¾â€‹(y)â€‹g2â€‹(y)=2â€‹eâ€‹Î·â€‹(y).0\leq I\_{n}^{1}(y)\leq\big(\big|e^{\psi^{\mu}(y)}\big|+\big|e^{\psi^{\mu\_{n}}(y)}\big|\big)f\_{\mu\_{n}}(y)\leq 2e^{\psi^{\mu}(y)+1}f\_{\mu\_{n}}(y)\leq 2e\cdot e^{\xi(y)}g^{2}(y)=2e\eta(y). |  |

Here in the above, the last inequality holds due to Assumption [5.4](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem4 "Assumption 5.4. â€£ 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") and ([5.15](https://arxiv.org/html/2510.11829v1#S5.E15 "In 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")). Since Î·âˆˆğ•ƒ1\eta\in\mathbb{L}^{1} by ([5.15](https://arxiv.org/html/2510.11829v1#S5.E15 "In 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")), the Dominated Convergence Theorem implies that In1â€‹(â‹…)I\_{n}^{1}(\cdot) converges to 0 in ğ•ƒ1â€‹(â„2)\mathbb{L}^{1}(\mathbb{R}^{2}) as nâ†’âˆn\to\infty, because ÏˆÎ¼n{\psi^{\mu\_{n}}} converges uniformly to ÏˆÎ¼\psi^{\mu} on â„d\mathbb{R}^{d}.

Finally, since In2â€‹(y)â‰¤2â€‹Î·â€‹(y)I\_{n}^{2}(y)\leq 2\eta(y), and fÎ¼nâ†’fÎ¼f\_{\mu\_{n}}\to f\_{\mu} in ğ•ƒ1â€‹(â„d)\mathbb{L}^{1}(\mathbb{R}^{d}), thanks to Lemma [5.6](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem6 "Lemma 5.6. â€£ 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")-(iii), we can apply Dominated Convergence again to get In2â€‹(â‹…)I\_{n}^{2}(\cdot) converges to 0 in ğ•ƒ1â€‹(â„2)\mathbb{L}^{1}(\mathbb{R}^{2}), as nâ†’âˆn\to\infty, proving the proposition.
âˆ

## 6 Existence of optimal control and convergence for general Î¼ini{\mu\_{\rm ini}}

In this section, we shall extend the results of Â§3 and show that the Problem [2.4](https://arxiv.org/html/2510.11829v1#S2.ThmTheorem4 "Problem 2.4 (Soft-constrained SchrÃ¶dinger Bridge Problem (SCSBP)). â€£ SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") has solution for each kâˆˆâ„•k\in\mathbb{N} when Î¼ini{\mu\_{\rm ini}} is an arbitrary distribution in ğ’«2â€‹(â„d)\mathscr{P}\_{2}(\mathbb{R}^{d}). To be more precise, for fixed kâˆˆâ„•k\in\mathbb{N},
let Jkâ€‹(Î±)J^{k}(\alpha) be the cost functional in Problem [2.4](https://arxiv.org/html/2510.11829v1#S2.ThmTheorem4 "Problem 2.4 (Soft-constrained SchrÃ¶dinger Bridge Problem (SCSBP)). â€£ SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday"). Applying Lemma [5.3](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem3 "Lemma 5.3 ([29, Lemma 3.1]). â€£ 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday"), for any Î¼âˆˆğ’Ÿ\mu\in\mathscr{D}, we have

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | Jkâ€‹(Î±)=ğ”¼â€‹[12â€‹âˆ«0T|Î±s|2â€‹ğ‘‘s+kâ€‹Gâ€‹(â„™XTÎ±;Î¼tar)]â‰¥kâ€‹Gâ€‹(â„™XTÎ±;Î¼tar)+ğ”¼â€‹[logâ¡ÏÎ¼â€‹(XTÎ±)]âˆ’ğ”¼â€‹[logâ¡hÎ¼â€‹(0,X0Î±)]\displaystyle\begin{split}J^{k}(\alpha)&=\mathbb{E}\Big[\frac{1}{2}\int\_{0}^{T}|\alpha\_{s}|^{2}ds+kG(\mathbb{P}\_{X^{\alpha}\_{T}};{\mu\_{\rm tar}})\Big]\\ &\geq kG(\mathbb{P}\_{X^{\alpha}\_{T}};{\mu\_{\rm tar}})+\mathbb{E}[\log\rho^{\mu}(X^{\alpha}\_{T})]-\mathbb{E}[\log h^{\mu}(0,X^{\alpha}\_{0})]\end{split} | |  | (6.1) |

and the equality holds when Î±t=Î±tÎ¼=âˆ‡logâ¡hÎ¼â€‹(t,XtÎ±Î¼)\alpha\_{t}=\alpha^{\mu}\_{t}=\nabla\log h^{\mu}(t,X^{\alpha^{\mu}}\_{t}) and XTÎ±Î¼âˆ¼Î¼X^{\alpha^{\mu}}\_{T}\sim\mu. Our main goal of this
section is to determine the density Ï^â€‹(â‹…)\widehat{\rho}(\cdot), such that Î±^t=âˆ‡logâ¡hÎ±^â€‹(t,XtÎ±^)\widehat{\alpha}\_{t}=\nabla\log h^{\widehat{\alpha}}(t,X^{\widehat{\alpha}}\_{t}) is the optimal control to Problem [2.4](https://arxiv.org/html/2510.11829v1#S2.ThmTheorem4 "Problem 2.4 (Soft-constrained SchrÃ¶dinger Bridge Problem (SCSBP)). â€£ SchrÃ¶dinger Bridge and Related Control Problem. â€£ 2 Preliminary â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday"), where hÎ±^â€‹(t,x)=ğ”¼t,xâ€‹[Ï^â€‹(XT)]:=ğ”¼â€‹[Ï^â€‹(XT)|Xt=x]h^{\widehat{\alpha}}(t,x)=\mathbb{E}\_{t,x}[\widehat{\rho}(X\_{T})]:=\mathbb{E}[\widehat{\rho}(X\_{T})|X\_{t}=x].

Before we proceed, let us first introduce some notations. First, for any Î¼âˆˆğ’«2â€‹(â„d)\mu\in\mathscr{P}\_{2}(\mathbb{R}^{d}), we denote fÎ¼f\_{\mu} to be its density function, whenever exists. In particular, we define f0=fÎ¼inif\_{0}=f\_{{\mu\_{\rm ini}}}.
To be consistent with the previous discussions, we recall the mapping (Î½0,Î½T)=ğ’¯â€‹(Î¼ini,Î¼)(\nu\_{0},\nu\_{T})={\cal T}({\mu\_{\rm ini}},\mu) for Î¼ini,Î¼âˆˆğ’«2â€‹(â„d){\mu\_{\rm ini}},\mu\in\mathscr{P}\_{2}(\mathbb{R}^{d}), and for fixed Î¼ini{\mu\_{\rm ini}}, we denote fÎ½0=Ï0f\_{\nu\_{0}}=\rho\_{0} and fÎ½T=ÏÎ¼=Î“1â€‹(Î¼)f\_{\nu\_{T}}=\rho^{\mu}=\Gamma\_{1}(\mu). Furthermore, from ([5.1](https://arxiv.org/html/2510.11829v1#S5.E1 "In Proposition 5.1 ([7]). â€£ 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) we see that, for any Î¼ini,Î¼âˆˆğ’«2â€‹(â„d){\mu\_{\rm ini}},\mu\in\mathscr{P}\_{2}(\mathbb{R}^{d})

|  |  |  |
| --- | --- | --- |
|  | Î¼iniâ€‹(dâ€‹x)Î½0â€‹(dâ€‹x)=f0â€‹(x)Ï0â€‹(x)=âˆ«â„dPâ€‹(T,y;0,x)â€‹ÏÎ¼â€‹(y)â€‹ğ‘‘y,Î¼â€‹(dâ€‹y)Î½Tâ€‹(dâ€‹y)=fÎ¼â€‹(y)ÏÎ¼â€‹(y)=âˆ«â„dpâ€‹(T,y;0,x)â€‹Ï0â€‹(x)â€‹ğ‘‘x.\displaystyle\frac{{\mu\_{\rm ini}}(dx)}{\nu\_{0}(dx)}=\frac{f\_{0}(x)}{\rho\_{0}(x)}=\int\_{\mathbb{R}^{d}}P(T,y;0,x)\rho^{\mu}(y)dy,\quad\frac{\mu(dy)}{\nu\_{T}(dy)}=\frac{f\_{\mu}(y)}{\rho^{\mu}(y)}=\int\_{\mathbb{R}^{d}}p(T,y;0,x)\rho\_{0}(x)dx. |  |

In other words, we can write

|  |  |  |  |
| --- | --- | --- | --- |
|  | {Ï0â€‹(x)=f0â€‹(x)âˆ«â„dPâ€‹(T,y;0,x)â€‹ÏÎ¼â€‹(y)â€‹ğ‘‘y=f0â€‹(x)hÎ¼â€‹(0,x),ÏÎ¼â€‹(y)=fÎ¼â€‹(y)âˆ«â„dpâ€‹(T,y;0,x)â€‹Ï0â€‹(x)â€‹ğ‘‘x=fÎ¼â€‹(y)âˆ«â„dpâ€‹(T,y;0,x)â€‹f0â€‹(x)hÎ¼â€‹(0,x)â€‹ğ‘‘x.\displaystyle\left\{\begin{array}[]{lll}\displaystyle\rho\_{0}(x)=\frac{f\_{0}(x)}{\int\_{\mathbb{R}^{d}}P(T,y;0,x)\rho^{\mu}(y)dy}=\frac{f\_{0}(x)}{h^{\mu}(0,x)}\vskip 6.0pt plus 2.0pt minus 2.0pt,\\ \displaystyle\rho^{\mu}(y)=\frac{f\_{\mu}(y)}{\int\_{\mathbb{R}^{d}}p(T,y;0,x)\rho\_{0}(x)dx}=\frac{f\_{\mu}(y)}{\int\_{\mathbb{R}^{d}}p(T,y;0,x)\frac{f\_{0}(x)}{h^{\mu}(0,x)}dx}.\end{array}\right. |  | (6.4) |

We now give the heuristic idea of the construction of "solution mapping" Î“\Gamma. Let Î¼ini{\mu\_{\rm ini}} be given. For any Î¼âˆˆğ’«2â€‹(â„d)\mu\in\mathscr{P}\_{2}(\mathbb{R}^{d}),
first apply Lemma [5.3](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem3 "Lemma 5.3 ([29, Lemma 3.1]). â€£ 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") to get the feedback control
Î±tÎ¼=âˆ‡logâ¡hÎ¼â€‹(t,XtÎ±Î¼)\alpha^{\mu}\_{t}=\nabla\log h^{\mu}(t,X^{\alpha^{\mu}}\_{t}) so that â„™XTÎ±Î¼=Î¼\mathbb{P}\_{X^{\alpha^{\mu}}\_{T}}=\mu and

|  |  |  |  |
| --- | --- | --- | --- |
|  | Jâ€‹(Î±Î¼)=ğ”¼â€‹[logâ¡ÏÎ¼â€‹(XTÎ±Î¼)]âˆ’ğ”¼â€‹[logâ¡hÎ¼â€‹(0,X0Î±Î¼)].\displaystyle J(\alpha^{\mu})=\mathbb{E}[\log\rho^{\mu}(X^{\alpha^{\mu}}\_{T})]-\mathbb{E}[\log h^{\mu}(0,X^{\alpha^{\mu}}\_{0})]. |  | (6.5) |

In what follows we fix kâˆˆâ„•k\in\mathbb{N}. To find the Î¼^k\widehat{\mu}^{k} such that Jkâ€‹(Î±Î¼^k)=infJkâ€‹(Î±)J^{k}(\alpha^{\widehat{\mu}^{k}})=\inf J^{k}(\alpha), we consider a mapping:
Î“2:ğ•ƒ1â€‹(â„d)â†’ğ’«2â€‹(â„d)\Gamma\_{2}:\mathbb{L}^{1}(\mathbb{R}^{d})\to\mathscr{P}\_{2}(\mathbb{R}^{d}) by Î“2â€‹(ÏÎ¼)=Î¼â€²\Gamma\_{2}(\rho^{\mu})=\mu^{\prime} where

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î¼â€²=argâ€‹minÎ¼Â¯âˆˆğ’«2â€‹(â„d)â¡{kâ€‹Gâ€‹(Î¼Â¯)+âˆ«â„dlogâ¡ÏÎ¼â€‹(y)â€‹Î¼Â¯â€‹(dâ€‹y)}.\mu^{\prime}=\operatorname\*{arg\,min}\_{\bar{\mu}\in\mathscr{P}\_{2}(\mathbb{R}^{d})}\Big\{kG(\bar{\mu})+\int\_{\mathbb{R}^{d}}\log\rho^{\mu}(y)\bar{\mu}(dy)\Big\}. |  | (6.6) |

Finally, we define Î“=Î“2âˆ˜Î“1:ğ’«2â€‹(â„d)â†¦ğ’«2â€‹(â„d)\Gamma=\Gamma\_{2}\circ\Gamma\_{1}:\mathscr{P}\_{2}(\mathbb{R}^{d})\mapsto\mathscr{P}\_{2}(\mathbb{R}^{d}), and we shall argue that the mapping Î“\Gamma has a fixed point Î¼^âˆˆâ„°\widehat{\mu}\in\mathscr{E}, where â„°\mathscr{E} is defined by ([5.5](https://arxiv.org/html/2510.11829v1#S5.E5 "In 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")). Clearly, if Î“â€‹(Î¼^)=Î¼^\Gamma(\widehat{\mu})=\widehat{\mu}, then we can still define Î±^=Î±Î¼^\widehat{\alpha}=\alpha^{\widehat{\mu}}, and by Lemma [5.3](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem3 "Lemma 5.3 ([29, Lemma 3.1]). â€£ 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") we have â„™XÎ±Î¼^=Î¼^\mathbb{P}\_{X^{\alpha^{\widehat{\mu}}}}=\widehat{\mu}. Thus by ([6.5](https://arxiv.org/html/2510.11829v1#S6.E5 "In 6 Existence of optimal control and convergence for general ğœ‡áµ¢â‚™áµ¢ â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")), for any Î±âˆˆğ•ƒğ”½02â€‹([0,T])\alpha\in\mathbb{L}^{2}\_{\mathbb{F}^{0}}([0,T]) we have

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | Jkâ€‹(Î±^)\displaystyle J^{k}(\widehat{\alpha}) | =\displaystyle= | Jâ€‹(Î±^)+kâ€‹Gâ€‹(Î¼^)=kâ€‹Gâ€‹(Î¼^)+ğ”¼â€‹[logâ¡ÏÎ¼^â€‹(XTÎ±^)]âˆ’ğ”¼â€‹[logâ¡hÎ¼^â€‹(0,X0Î±^)]\displaystyle J(\widehat{\alpha})+kG(\widehat{\mu})=kG(\widehat{\mu})+\mathbb{E}[\log\rho^{\widehat{\mu}}(X^{\widehat{\alpha}}\_{T})]-\mathbb{E}[\log h^{\widehat{\mu}}(0,X^{\widehat{\alpha}}\_{0})] |  |
|  |  | â‰¤\displaystyle\leq | kâ€‹Gâ€‹(â„™XTÎ±)+ğ”¼â€‹[logâ¡ÏÎ¼^â€‹(XTÎ±)]âˆ’ğ”¼â€‹[logâ¡hÎ¼^â€‹(0,X0Î±)]â‰¤Jkâ€‹(Î±).\displaystyle kG(\mathbb{P}\_{X^{\alpha}\_{T}})+\mathbb{E}[\log\rho^{\widehat{\mu}}(X^{\alpha}\_{T})]-\mathbb{E}[\log h^{\widehat{\mu}}(0,X^{\alpha}\_{0})]\leq J^{k}(\alpha). |  |

Here in the above the first inequality is due to ([6.6](https://arxiv.org/html/2510.11829v1#S6.E6 "In 6 Existence of optimal control and convergence for general ğœ‡áµ¢â‚™áµ¢ â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")), and the last inequality is due to ([6.1](https://arxiv.org/html/2510.11829v1#S6.E1 "In 6 Existence of optimal control and convergence for general ğœ‡áµ¢â‚™áµ¢ â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")). This shows that Î±^\widehat{\alpha} is the minimizer of Jkâ€‹(â‹…)J^{k}(\cdot).

We now show that the set â„°âŠ‚ğ’ŸÎ¼ini\mathscr{E}\subset\mathscr{D}\_{{\mu\_{\rm ini}}} defined by ([5.5](https://arxiv.org/html/2510.11829v1#S5.E5 "In 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) satisfies all the necessary properties, thanks to the Lemma [5.6](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem6 "Lemma 5.6. â€£ 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") and Proposition [5.8](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem8 "Proposition 5.8. â€£ 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") that we established in the last section, so that the mapping Î“\Gamma possesses a fixed point on â„°\mathscr{E} by Schauderâ€™s fixed-point theorem.
Our main result is as follows.

###### Theorem 6.1.

Assume that Assumptions
[5.4](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem4 "Assumption 5.4. â€£ 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")
is in force. Consider the set â„°\mathscr{E} defined by ([5.5](https://arxiv.org/html/2510.11829v1#S5.E5 "In 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")). Then the following hold:

(i) â„°\mathscr{E} is convex and closed under Prohorov metric, and ğ’®â„°{\cal S}\_{\mathscr{E}} is convex and closed in ğ•ƒ1â€‹(â„d)\mathbb{L}^{1}(\mathbb{R}^{d});

(ii) Î“â€‹(â„°)âŠ†â„°\Gamma(\mathscr{E})\subseteq\mathscr{E}, and is precompact in ğ’«2â€‹(â„d)\mathscr{P}\_{2}(\mathbb{R}^{d}), under both Prohorov and Wasserstein metric;

(iii) Î“\Gamma is continuous on â„°\mathscr{E}, under Prohorov metric.

Consequently, the mapping Î“\Gamma has a fixed point in â„°\mathscr{E}.

###### Proof.

Since the last statement is a direct consequence of Schauderâ€™s fixed point theorem, applying to the space ğ’«â€‹(â„d)\mathscr{P}(\mathbb{R}^{d}) with Prohorov metric, we need only prove the properties (i)-(iii).

(i) is obvious.

(ii) By definition of ğ’®â„°{\cal S}\_{\mathscr{E}} we rewrite ([6.6](https://arxiv.org/html/2510.11829v1#S6.E6 "In 6 Existence of optimal control and convergence for general ğœ‡áµ¢â‚™áµ¢ â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) as

|  |  |  |
| --- | --- | --- |
|  | fÎ¼â€²=argâ€‹minfÎ¼Â¯âˆˆğ’®â„°â¡{kâ€‹Gâ€‹(Î¼Â¯)+âˆ«â„dlogâ¡ÏÎ¼â€‹(y)â€‹fÎ¼Â¯â€‹(y)â€‹ğ‘‘y}.f\_{\mu^{\prime}}=\operatorname\*{arg\,min}\_{f\_{\bar{\mu}}\in{\cal S}\_{\mathscr{E}}}\Big\{kG({\bar{\mu}})+\int\_{\mathbb{R}^{d}}\log\rho^{\mu}(y)f\_{\bar{\mu}}(y)dy\Big\}. |  |

Since ğ’®â„°{\cal S}\_{\mathscr{E}} is convex and closed in ğ•ƒ1â€‹(â„d)\mathbb{L}^{1}(\mathbb{R}^{d}), it follows that fÎ¼â€²âˆˆğ’®â„°f\_{\mu^{\prime}}\in{\cal S}\_{\mathscr{E}}, and thus Î“â€‹(â„°)âŠ†â„°\Gamma(\mathscr{E})\subseteq\mathscr{E}.
We are to show that Î“â€‹(â„°)\Gamma(\mathscr{E}) is precompact in ğ’«2â€‹(â„d)\mathscr{P}\_{2}(\mathbb{R}^{d}).

To this end, let {Î¼n}âŠ†Î“â€‹(â„°)\{\mu\_{n}\}\subseteq\Gamma(\mathscr{E}) be any sequence, we shall find a subsequence {Î¼nk}kâ‰¥1\{\mu\_{n\_{k}}\}\_{k\geq 1} such that limkâ†’âˆÎ¼nk=Î¼âˆˆğ’«2â€‹(â„2)\lim\_{k\to\infty}\mu\_{n\_{k}}=\mu\in\mathscr{P}\_{2}(\mathbb{R}^{2}), under both Prohorov metric and W2W\_{2}-metric. Since Î“â€‹(â„°)âŠ†â„°\Gamma(\mathscr{E})\subseteq\mathscr{E}, by Lemma [5.6](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem6 "Lemma 5.6. â€£ 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")-(i), {fÎ¼n}\{f\_{\mu\_{n}}\} is bounded in
ğ•ƒ2â€‹(â„d)\mathbb{L}^{2}(\mathbb{R}^{d}). Thus by Banach-Alaoglu Theorem and noting that ğ•ƒ2\mathbb{L}^{2} is reflexive, {fÎ¼n}\{f\_{\mu\_{n}}\} is weakly compact, that is, there exists a subsequence {fÎ¼nk}\{f\_{\mu\_{n\_{k}}}\} such that fÎ¼nkâ€‹â‡€wfÎ¼f\_{\mu\_{n\_{k}}}\mathop{\mathrel{\mathop{\kern 0.0pt\rightharpoonup}\limits^{w}}}f\_{\mu} in ğ•ƒ2â€‹(â„d)\mathbb{L}^{2}(\mathbb{R}^{d}), as kâ†’âˆk\to\infty. But this amounts to
saying the Î¼nkâ‡’Î¼\mu\_{n\_{k}}\Rightarrow\mu in Prohorov metric. This, together with Lemma [5.6](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem6 "Lemma 5.6. â€£ 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")-(ii) and the relationship between Wasserstein distance and weak convergence (see, e.g., [[65](https://arxiv.org/html/2510.11829v1#bib.bib65), Theorem 7.12]), leads to that limkâ†’âˆÎ¼nk=Î¼\lim\_{k\to\infty}\mu\_{n\_{k}}=\mu
in ğ’«2â€‹(â„d)\mathscr{P}\_{2}(\mathbb{R}^{d}), proving (ii).

(iii) Let us assume that {Î¼n}âŠ‚â„°\{\mu\_{n}\}\subset\mathscr{E} such that Î¼nâ‡’Î¼\mu\_{n}\Rightarrow\mu in Prohorov metric.
The stability result in Proposition [5.8](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem8 "Proposition 5.8. â€£ 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") shows that ÏÎ¼n=Î“1â€‹(Î¼n)â†’Î“1â€‹(Î¼)=ÏÎ¼âˆˆğ’®\rho^{\mu\_{n}}=\Gamma\_{1}(\mu\_{n})\to\Gamma\_{1}(\mu)=\rho^{\mu}\in{\cal S} in ğ•ƒ1â€‹(â„d)\mathbb{L}^{1}(\mathbb{R}^{d}). Next, we show that Î“2â€‹(ÏÎ¼n)â‡’Î“2â€‹(ÏÎ¼)\Gamma\_{2}(\rho^{\mu\_{n}})\Rightarrow\Gamma\_{2}(\rho^{\mu}) in Prohorov metric. Recall the definition of Î“2\Gamma\_{2}, we define a family of
functionals on â„°\mathscr{E}: for each k,nâˆˆâ„•k,n\in\mathbb{N} and Î¼Â¯âˆˆâ„°\bar{\mu}\in\mathscr{E},

|  |  |  |  |
| --- | --- | --- | --- |
|  | {Fnkâ€‹(Î¼Â¯):=kâ€‹Gâ€‹(Î¼Â¯)+âˆ«â„dlogâ¡ÏÎ¼nâ€‹(y)â€‹Î¼Â¯â€‹(dâ€‹y);Fkâ€‹(Î¼Â¯):=kâ€‹Gâ€‹(Î¼Â¯)+âˆ«â„dlogâ¡ÏÎ¼â€‹(y)â€‹Î¼Â¯â€‹(dâ€‹y).\displaystyle\left\{\begin{array}[]{lll}\displaystyle F^{k}\_{n}(\bar{\mu}):=kG(\bar{\mu})+\int\_{\mathbb{R}^{d}}\log\rho^{\mu\_{n}}(y)\bar{\mu}(dy);\vskip 6.0pt plus 2.0pt minus 2.0pt\\ \displaystyle F^{k}(\bar{\mu}):=kG(\bar{\mu})+\int\_{\mathbb{R}^{d}}\log\rho^{\mu}(y)\bar{\mu}(dy).\end{array}\right. |  | (6.9) |

Then Î¼nâ€²=Î“2â€‹(ÏÎ¼n):=argâ€‹minÎ¼Â¯âˆˆâ„°â¡Fnkâ€‹(Î¼Â¯)\mu\_{n}^{\prime}=\Gamma\_{2}(\rho^{\mu\_{n}}):=\operatorname\*{arg\,min}\_{\bar{\mu}\in\mathscr{E}}F^{k}\_{n}(\bar{\mu}) and Î¼â€²=Î“2â€‹(ÏÎ¼):=argâ€‹minÎ¼Â¯âˆˆâ„°â¡Fkâ€‹(Î¼Â¯)\mu^{\prime}=\Gamma\_{2}(\rho^{\mu}):=\operatorname\*{arg\,min}\_{\bar{\mu}\in\mathscr{E}}F^{k}(\bar{\mu}).

To show that the minimizers Î¼nâ€²â‡’Î¼â€²\mu^{\prime}\_{n}\Rightarrow\mu^{\prime}, we shall invoke
the notion of Î“\Gamma-convergence (cf. [[19](https://arxiv.org/html/2510.11829v1#bib.bib19)]). To be more precise, a sequence {Fn}nâ‰¥1\{F\_{n}\}\_{n\geq 1} is said to Î“\Gamma-converge to
FF as nâ†’âˆn\to\infty if

|  |  |  |  |
| --- | --- | --- | --- |
|  | {For every sequenceÂ Î¼Â¯nâ‡’Î¼Â¯, it holds thatÂ Fâ€‹(Î¼Â¯)â‰¤limÂ¯nFnkâ€‹(Î¼Â¯n);There exists a sequenceÂ Î¼Â¯nâ‡’Î¼Â¯, such thatÂ Fâ€‹(Î¼Â¯)â‰¥limÂ¯nFnâ€‹(Î¼Â¯n).\displaystyle\left\{\begin{array}[]{lll}\mbox{For every sequence $\bar{\mu}\_{n}\Rightarrow\bar{\mu}$, it holds that $\displaystyle F(\bar{\mu})\leq\mathop{\underline{\rm lim}}\_{n}F^{k}\_{n}(\bar{\mu}\_{n})$};\vskip 6.0pt plus 2.0pt minus 2.0pt\\ \mbox{There exists a sequence $\bar{\mu}\_{n}\Rightarrow\bar{\mu}$, such that $\displaystyle F(\bar{\mu})\geq\mathop{\overline{\rm lim}}\_{n}F\_{n}(\bar{\mu}\_{n})$.}\end{array}\right. |  | (6.12) |

Now, note that GG is convex and â„°\mathscr{E} is compact under Prohorov metric, we see that both {Fnk}\{F^{k}\_{n}\} and FkF^{k} are coercive (in the sense that there exists minimizing sequence in â„°âŠ‚ğ’«2â€‹(â„d)\mathscr{E}\subset\mathscr{P}\_{2}(\mathbb{R}^{d})). Thus, in light of the Î“\Gamma-convergence result (see [[19](https://arxiv.org/html/2510.11829v1#bib.bib19), Theorem 7.1]), in order to show Î¼nâ€²â‡’Î¼â€²\mu\_{n}^{\prime}\Rightarrow\mu^{\prime}, it suffices to check that {Fnk}\{F^{k}\_{n}\} Î“\Gamma-converges to FkF^{k}, for each kk. To see this, for any {Î¼Â¯n}âŠ‚â„°\{\bar{\mu}\_{n}\}\subset\mathscr{E}, such that Î¼Â¯nâ‡’Î¼Â¯\bar{\mu}\_{n}\Rightarrow\bar{\mu}, by Lemma [5.6](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem6 "Lemma 5.6. â€£ 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")-(iii), we have fÎ¼Â¯nâ†’fÎ¼Â¯f\_{\bar{\mu}\_{n}}\to f\_{\bar{\mu}} in
ğ•ƒ1\mathbb{L}^{1}, and therefore by Dominated Convergence,

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  |  | |âˆ«â„d(logÏÎ¼n(y)Î¼Â¯n(dy)âˆ’âˆ«â„dlogÏÎ¼(y)Î¼Â¯(dy)|\displaystyle\Big|\int\_{\mathbb{R}^{d}}(\log\rho^{\mu\_{n}}(y)\bar{\mu}\_{n}(dy)-\int\_{\mathbb{R}^{d}}\log\rho^{\mu}(y)\bar{\mu}(dy)\Big| |  |
|  |  | â‰¤\displaystyle\leq | âˆ«â„d|logâ¡ÏÎ¼nâ€‹(y)|â€‹|fÎ¼Â¯nâ€‹(y)âˆ’fÎ¼Â¯â€‹(y)|â€‹ğ‘‘y+âˆ«â„d|logâ¡ÏÎ¼nâ€‹(y)âˆ’logâ¡ÏÎ¼â€‹(y)|â€‹Î¼Â¯â€‹(dâ€‹y)\displaystyle\int\_{\mathbb{R}^{d}}|\log\rho^{\mu\_{n}}(y)||f\_{\bar{\mu}\_{n}}(y)-f\_{\bar{\mu}}(y)|dy+\int\_{\mathbb{R}^{d}}|\log\rho^{\mu\_{n}}(y)-\log\rho^{\mu}(y)|\bar{\mu}(dy) |  |
|  |  | â‰¤\displaystyle\leq | logâ¡(KÎ´)â€‹â€–fÎ¼Â¯nâˆ’fÎ¼Â¯â€–ğ•ƒ1+âˆ«â„d|logâ¡ÏÎ¼nâ€‹(y)âˆ’logâ¡ÏÎ¼â€‹(y)|â€‹Î¼Â¯â€‹(dâ€‹y)â†’0,asÂ nâ†’âˆ.\displaystyle\log\big(\frac{K}{\delta}\big)\|f\_{\bar{\mu}\_{n}}-f\_{\bar{\mu}}\|\_{\mathbb{L}^{1}}+\int\_{\mathbb{R}^{d}}|\log\rho^{\mu\_{n}}(y)-\log\rho^{\mu}(y)|\bar{\mu}(dy)\to 0,\quad\mbox{as $n\to\infty$.} |  |

Finally, since Gâ€‹(â‹…)G(\cdot) is continuous on ğ’«2â€‹(â„d)\mathscr{P}\_{2}(\mathbb{R}^{d}), by definition ([6.9](https://arxiv.org/html/2510.11829v1#S6.E9 "In 6 Existence of optimal control and convergence for general ğœ‡áµ¢â‚™áµ¢ â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) we see that Fnkâ€‹(Î¼Â¯n)â†’Fkâ€‹(Î¼Â¯)F^{k}\_{n}(\bar{\mu}\_{n})\to F^{k}(\bar{\mu}) whenever Î¼Â¯nâ‡’Î¼Â¯\bar{\mu}\_{n}\Rightarrow\bar{\mu}. Thus by ([6.12](https://arxiv.org/html/2510.11829v1#S6.E12 "In 6 Existence of optimal control and convergence for general ğœ‡áµ¢â‚™áµ¢ â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) we see that {Fnk}\{F^{k}\_{n}\} Î“\Gamma-converges to FkF^{k}, as nâ†’âˆn\to\infty. This completes the proof.
âˆ

Finally, we shall establish an
analogue of Theorem [4.1](https://arxiv.org/html/2510.11829v1#S4.ThmTheorem1 "Theorem 4.1. â€£ 4.1 The Convergence of Optimal Policies â€£ 4 Convergence Results under Delta Initial Distribution â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") in the case of general Î¼iniâˆˆğ’«2â€‹(â„d){\mu\_{\rm ini}}\in\mathscr{P}\_{2}(\mathbb{R}^{d}). For technical convenience, in what follows we shall make use of the following extra assumptions to facilitate our discussion. Recall the function Î¾\xi and Î·\eta defined by ([5.14](https://arxiv.org/html/2510.11829v1#S5.E14 "In 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) and ([5.15](https://arxiv.org/html/2510.11829v1#S5.E15 "In 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")), respectively.

###### Assumption 6.2.

(i) The penalty function GG satisfies Gâ€‹(Î¼;Î¼tar)â‰¥W2â€‹(Î¼,Î¼tar)G(\mu;\mu\_{\rm tar})\geq W\_{2}(\mu,\mu\_{\rm tar});

(ii) In Assumption [3.2](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem2 "Assumption 3.2. â€£ 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")-(ii), the function Ï•\phi satisfies â€–Ï•â€‹(â‹…)â€‹eÎ¾â€‹(â‹…)â€–âˆ<âˆ\|\phi(\cdot)e^{\xi(\cdot)}\|\_{\infty}<\infty;

(iii) For any R>0R>0, there exists MR>0M\_{R}>0, such that

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â€‹[Î·2â€‹(XTt,x)]=âˆ«â„dÎ·2â€‹(y)â€‹pâ€‹(T,y;t,x)â€‹ğ‘‘yâ‰¤MR,(t,x)âˆˆ[0,T]Ã—BR,\displaystyle\mathbb{E}[\eta^{2}(X^{t,x}\_{T})]=\int\_{\mathbb{R}^{d}}\eta^{2}(y)p(T,y;t,x)dy\leq M\_{R},\qquad(t,x)\in[0,T]\times B\_{R}, |  | (6.13) |

where BR:={xâˆˆâ„d:|x|â‰¤R}B\_{R}:=\{x\in\mathbb{R}^{d}:|x|\leq R\}.
âˆ

###### Remark 6.3.

(1) Assumption [6.2](https://arxiv.org/html/2510.11829v1#S6.ThmTheorem2 "Assumption 6.2. â€£ 6 Existence of optimal control and convergence for general ğœ‡áµ¢â‚™áµ¢ â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")-(i) is not overly restrictive, and can be justified by Example [3.5](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem5 "Example 3.5. â€£ 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday").

(2) Note that the function Ï•\phi in Assumption [3.2](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem2 "Assumption 3.2. â€£ 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")-(ii) satisfies Ï•â€‹(y)â€‹eÎ»â€‹|y|2â‰¤C\phi(y)e^{\lambda|y|^{2}}\leq C, for some Î»>0\lambda>0 and that Î¾â€‹(y)âˆ¼Î»â€²â€‹|y|2\xi(y)\sim\lambda^{\prime}|y|^{2}, as |y|â†’âˆ|y|\to\infty, Assumption [6.2](https://arxiv.org/html/2510.11829v1#S6.ThmTheorem2 "Assumption 6.2. â€£ 6 Existence of optimal control and convergence for general ğœ‡áµ¢â‚™áµ¢ â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")-(ii) amounts to saying that Ï•\phi and Î¾\xi are compatible.

(3) While Assumption [6.2](https://arxiv.org/html/2510.11829v1#S6.ThmTheorem2 "Assumption 6.2. â€£ 6 Existence of optimal control and convergence for general ğœ‡áµ¢â‚™áµ¢ â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")-(iii) is slightly stronger than the requirement ([5.15](https://arxiv.org/html/2510.11829v1#S5.E15 "In 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")), it would be trivial if the mapping (t,x)â†¦ğ”¼â€‹[Î·2â€‹(XTt,x)](t,x)\mapsto\mathbb{E}[\eta^{2}(X^{t,x}\_{T})] is continuous, which is by no means stringent.
âˆ

Our last result of this section is the following.

###### Theorem 6.4.

Assume that the Assumptions [3.2](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem2 "Assumption 3.2. â€£ 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday"), [5.4](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem4 "Assumption 5.4. â€£ 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") and [6.2](https://arxiv.org/html/2510.11829v1#S6.ThmTheorem2 "Assumption 6.2. â€£ 6 Existence of optimal control and convergence for general ğœ‡áµ¢â‚™áµ¢ â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") are in force. Let Î¼iniâˆˆâ„°{\mu\_{\rm ini}}\in\mathscr{E} be given, and
let Î±^â€‹(t,x)\widehat{\alpha}(t,x) and Î±^kâ€‹(t,x)\widehat{\alpha}^{k}(t,x), (t,x)âˆˆ[0,T]Ã—â„d(t,x)\in[0,T]\times\mathbb{R}^{d} are optimal controls given in Proposition [3.6](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem6 "Proposition 3.6. â€£ 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") and Lemma [5.3](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem3 "Lemma 5.3 ([29, Lemma 3.1]). â€£ 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday"), respectively. Then, for any R>0R>0, there exists CR>0C\_{R}>0, such that for any kâˆˆâ„•k\in\mathbb{N}, it holds that

|  |  |  |  |
| --- | --- | --- | --- |
|  | âˆ«0T|Î±^kâ€‹(t,x)âˆ’Î±^â€‹(t,x)|â€‹ğ‘‘tâ‰¤CRk,(t,x)âˆˆ[0,T]Ã—BR.\displaystyle\int\_{0}^{T}|\widehat{\alpha}^{k}(t,x)-\widehat{\alpha}(t,x)|dt\leq\frac{C\_{R}}{k},\qquad(t,x)\in[0,T]\times B\_{R}. |  | (6.14) |

###### Proof.

We begin by denoting Î¼k:=â„™XTÎ±^k\mu^{k}:=\mathbb{P}\_{X\_{T}^{\widehat{\alpha}^{k}}}, Î¼tar:=â„™XTÎ±^{\mu\_{\rm tar}}:=\mathbb{P}\_{X\_{T}^{\widehat{\alpha}}}, and let ÏÎ¼k=Î“1â€‹(Î¼k)\rho^{\mu\_{k}}=\Gamma\_{1}(\mu\_{k}), kâˆˆâ„•k\in\mathbb{N},
ÏÎ¼tar=Î“1â€‹(Î¼tar)\rho^{\mu\_{\rm tar}}=\Gamma\_{1}({\mu\_{\rm tar}}), respectively, as we defined before. Next, applying ([5.12](https://arxiv.org/html/2510.11829v1#S5.E12 "In 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")), we have

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | |ÏÎ¼kâ€‹(y)âˆ’ÏÎ¼tarâ€‹(y)|=|eÏˆÎ¼kâ€‹(y)â€‹fÎ¼kâ€‹(y)âˆ’eÏˆÎ¼tarâ€‹(y)â€‹fÎ¼tarâ€‹(y)|â‰¤eÏˆÎ¼tarâ€‹(y)â€‹|fÎ¼kâ€‹(y)âˆ’fÎ¼tarâ€‹(y)|+fÎ¼kâ€‹(y)â€‹|eÏˆÎ¼kâ€‹(y)âˆ’eÏˆÎ¼tarâ€‹(y)|.\displaystyle\begin{split}|\rho^{\mu\_{k}}(y)-\rho^{\mu\_{\rm tar}}(y)|&=|e^{\psi^{\mu\_{k}}(y)}f\_{\mu\_{k}}(y)-e^{\psi^{{\mu\_{\rm tar}}}(y)}f\_{\mu\_{\rm tar}}(y)|\\ &\leq e^{\psi^{{\mu\_{\rm tar}}}(y)}|f\_{\mu\_{k}}(y)-f\_{\mu\_{\rm tar}}(y)|+f\_{\mu\_{k}}(y)|e^{\psi^{\mu\_{k}}(y)}-e^{\psi^{{\mu\_{\rm tar}}}(y)}|.\end{split} | |  | (6.15) |

Let us now recall some facts from Â§4. First, note that the optimality of Î¼k\mu\_{k} implies that Gâ€‹(Î¼k)â‰¤CkG(\mu\_{k})\leq\frac{C}{k} (cf. ([4.3](https://arxiv.org/html/2510.11829v1#S4.E3 "In 4 Convergence Results under Delta Initial Distribution â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday"))), for some generic constant C>0C>0 independent of kk, which we shall allow to vary from line to line below. Thus, by virtue of Assumption [3.2](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem2 "Assumption 3.2. â€£ 3 Existence of Optimal Policies for SCSBPâ€™s â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")-(ii), we can write

|  |  |  |
| --- | --- | --- |
|  | |fÎ¼kâ€‹(y)âˆ’fÎ¼tarâ€‹(y)|â‰¤Ckâ€‹Ï•â€‹(y),yâˆˆâ„d,|f\_{\mu\_{k}}(y)-f\_{\mu\_{\rm tar}}(y)|\leq\frac{C}{k}\phi(y),\qquad y\in\mathbb{R}^{d}, |  |

where Ï•â€‹(y)â€‹eÎ¾â€‹(y)â‰¤C\phi(y)e^{\xi(y)}\leq C, yâˆˆâ„dy\in\mathbb{R}^{d}, thanks to Assumption [6.2](https://arxiv.org/html/2510.11829v1#S6.ThmTheorem2 "Assumption 6.2. â€£ 6 Existence of optimal control and convergence for general ğœ‡áµ¢â‚™áµ¢ â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")-(ii). Furthermore, under Assumption [6.2](https://arxiv.org/html/2510.11829v1#S6.ThmTheorem2 "Assumption 6.2. â€£ 6 Existence of optimal control and convergence for general ğœ‡áµ¢â‚™áµ¢ â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")-(i), we can assume without loss of generality that the SchrÃ¶dinger potentials ÏˆÎ¼k\psi^{\mu\_{k}} and ÏˆÎ¼tar\psi^{{\mu\_{\rm tar}}} all satisfy estimates ([5.13](https://arxiv.org/html/2510.11829v1#S5.E13 "In 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) and ([5.14](https://arxiv.org/html/2510.11829v1#S5.E14 "In 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")). Consequently, by Assumption [6.2](https://arxiv.org/html/2510.11829v1#S6.ThmTheorem2 "Assumption 6.2. â€£ 6 Existence of optimal control and convergence for general ğœ‡áµ¢â‚™áµ¢ â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")-(i) and an easy application of Lemma [5.7](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem7 "Lemma 5.7 ([10, Theorem 1.1]). â€£ 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") and Newton-Leibniz formula we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | |eÏˆÎ¼tarâ€‹(y)âˆ’eÏˆÎ¼tarâ€‹(y)|\displaystyle|e^{\psi^{{\mu\_{\rm tar}}}(y)}-e^{\psi^{{\mu\_{\rm tar}}}(y)}| | =|ÏˆÎ¼tar(y)âˆ’ÏˆÎ¼tar(y)|âˆ«01exp{ÏˆÎ¼tar(y)+Î¸(ÏˆÎ¼k(y)âˆ’ÏˆÎ¼tar(y)}dÎ¸\displaystyle=|{\psi^{{\mu\_{\rm tar}}}(y)}-{\psi^{{\mu\_{\rm tar}}}(y)}|\int\_{0}^{1}\exp\{\psi^{\mu\_{\rm tar}}(y)+\theta(\psi^{\mu\_{k}}(y)-\psi^{\mu\_{\rm tar}}(y)\}d\theta |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤Câ€‹eÎ¾â€‹(y)â€‹W2â€‹(Î¼k,Î¼tar)â‰¤Câ€‹eÎ¾â€‹(y)â€‹Gâ€‹(Î¼k)â‰¤Ckâ€‹eÎ¾â€‹(y),yâˆˆâ„d.\displaystyle\leq Ce^{\xi(y)}W\_{2}(\mu\_{k},{\mu\_{\rm tar}})\leq Ce^{\xi(y)}G(\mu\_{k})\leq\frac{C}{k}e^{\xi(y)},\quad y\in\mathbb{R}^{d}. |  |

Summarizing above and recalling Assumption [5.4](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem4 "Assumption 5.4. â€£ 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday") and ([5.15](https://arxiv.org/html/2510.11829v1#S5.E15 "In 5 Stability of the Solutions to the SBP â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")), we derive from ([6.15](https://arxiv.org/html/2510.11829v1#S6.E15 "In 6 Existence of optimal control and convergence for general ğœ‡áµ¢â‚™áµ¢ â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) that

|  |  |  |
| --- | --- | --- |
|  | |ÏÎ¼kâ€‹(y)âˆ’ÏÎ¼tarâ€‹(y)|â‰¤Ckâ€‹(eÏˆÎ¼tarâ€‹(y)â€‹Ï•â€‹(y)+fÎ¼kâ€‹(y)â€‹eÎ¾â€‹(y))â‰¤Ckâ€‹(eÎ¾â€‹(y)â€‹Ï•â€‹(y)+g2â€‹(y)â€‹eÎ¾â€‹(y))â‰¤Ckâ€‹(1+Î·â€‹(y)),\displaystyle|\rho^{\mu\_{k}}(y)-\rho^{\mu\_{\rm tar}}(y)|\leq\frac{C}{k}\left(e^{\psi^{{\mu\_{\rm tar}}}(y)}\phi(y)+f\_{\mu\_{k}}(y)e^{\xi(y)}\right)\leq\frac{C}{k}\left(e^{\xi(y)}\phi(y)+g^{2}(y)e^{\xi(y)}\right)\leq\frac{C}{k}(1+\eta(y)), |  |

and therefore, given R>0R>0, and (t,x)âˆˆ[0,T]Ã—BR(t,x)\in[0,T]\times B\_{R}, we apply Assumption [6.2](https://arxiv.org/html/2510.11829v1#S6.ThmTheorem2 "Assumption 6.2. â€£ 6 Existence of optimal control and convergence for general ğœ‡áµ¢â‚™áµ¢ â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")-(iii) to get

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â€‹[|ÏÎ¼kâ€‹(XTt,x)âˆ’ÏÎ¼tarâ€‹(XTt,x)|]â‰¤ğ”¼â€‹[|ÏÎ¼kâ€‹(XTt,x)âˆ’ÏÎ¼tarâ€‹(XTt,x)|2]12â‰¤CRk,\displaystyle\mathbb{E}[|\rho^{\mu\_{k}}(X^{t,x}\_{T})-\rho^{\mu\_{\rm tar}}(X^{t,x}\_{T})|]\leq\mathbb{E}[|\rho^{\mu\_{k}}(X^{t,x}\_{T})-\rho^{\mu\_{\rm tar}}(X^{t,x}\_{T})|^{2}]^{\frac{1}{2}}\leq\frac{C\_{R}}{k}, |  | (6.16) |

where CR>0C\_{R}>0 is some constant depending on the generic constant CC above and MRM\_{R} in ([6.13](https://arxiv.org/html/2510.11829v1#S6.E13 "In Assumption 6.2. â€£ 6 Existence of optimal control and convergence for general ğœ‡áµ¢â‚™áµ¢ â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")).

To complete the proof, let us recall that optimal strategies are of the form Î±^kâ€‹(t,x)=âˆ‡logâ¡hkâ€‹(t,x)\widehat{\alpha}^{k}(t,x)=\nabla\log h^{k}(t,x), kâˆˆâ„•k\in\mathbb{N}, and Î±^â€‹(t,x)=âˆ‡logâ¡hâ€‹(t,x)\widehat{\alpha}(t,x)=\nabla\log h(t,x), and hkâ€‹(t,x)h^{k}(t,x) and hâ€‹(t,x)h(t,x) are the solutions to the respective PDEs:

|  |  |  |  |
| --- | --- | --- | --- |
|  | {âˆ‚thkâ€‹(t,x)+â„’tâ€‹hkâ€‹(t,x)=0;hkâ€‹(T,x)=ÏÎ¼kâ€‹(x).{âˆ‚thâ€‹(t,x)+â„’tâ€‹hâ€‹(t,x)=0;hâ€‹(T,x)=ÏÎ¼tarâ€‹(x),\displaystyle\begin{cases}\partial\_{t}h^{k}(t,x)+\mathscr{L}\_{t}h^{k}(t,x)=0;\\ h^{k}(T,x)=\rho^{\mu\_{k}}(x).\end{cases}\quad\quad\begin{cases}\partial\_{t}h(t,x)+\mathscr{L}\_{t}h(t,x)=0;\\ h(T,x)=\rho^{\mu\_{\rm tar}}(x),\end{cases} |  | (6.17) |

Furthermore, noting that hkâ€‹(t,x)=ğ”¼â€‹[ÏÎ¼kâ€‹(XTt,x)]h^{k}(t,x)=\mathbb{E}[\rho^{\mu\_{k}}(X^{t,x}\_{T})], hâ€‹(t,x)=ğ”¼â€‹[ÏÎ¼tarâ€‹(XTt,x)]h(t,x)=\mathbb{E}[\rho^{{\mu\_{\rm tar}}}(X^{t,x}\_{T})], and by the Bismut-Elworthy-Li formula we have

|  |  |  |
| --- | --- | --- |
|  | âˆ‡hkâ€‹(t,x)=ğ”¼â€‹[ÏÎ¼kâ€‹(XTt,x)â€‹NTt,x];âˆ‡hâ€‹(t,x)=ğ”¼â€‹[ÏÎ¼tarâ€‹(XTt,x)â€‹NTt,x].\nabla h^{k}(t,x)=\mathbb{E}[\rho^{\mu\_{k}}(X^{t,x}\_{T})N^{t,x}\_{T}];\quad\nabla h(t,x)=\mathbb{E}[\rho^{{\mu\_{\rm tar}}}(X^{t,x}\_{T})N^{t,x}\_{T}]. |  |

Thus, we have |âˆ‡hâ€‹(t,x)|â‰¤ğ”¼â€‹[Î·2â€‹(XTt,x)]12â€‹ğ”¼â€‹[|NTt,x|]12â‰¤CRTâˆ’t|\nabla h(t,x)|\leq\mathbb{E}[\eta^{2}(X^{t,x}\_{T})]^{\frac{1}{2}}\mathbb{E}[|N^{t,x}\_{T}|]^{\frac{1}{2}}\leq\frac{C\_{R}}{\sqrt{T-t}}, whenever (t,x)âˆˆ[0,T]Ã—BR(t,x)\in[0,T]\times B\_{R}, and a similar argument as in ([4.9](https://arxiv.org/html/2510.11829v1#S4.E9 "In 4.1 The Convergence of Optimal Policies â€£ 4 Convergence Results under Delta Initial Distribution â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) and ([4.11](https://arxiv.org/html/2510.11829v1#S4.E11 "In 4.1 The Convergence of Optimal Policies â€£ 4 Convergence Results under Delta Initial Distribution â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")), together with the estimate ([6.16](https://arxiv.org/html/2510.11829v1#S6.E16 "In 6 Existence of optimal control and convergence for general ğœ‡áµ¢â‚™áµ¢ â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")), leads to that

|  |  |  |  |
| --- | --- | --- | --- |
|  | |âˆ‡hkâ€‹(t,x)âˆ’âˆ‡hâ€‹(t,x)|â‰¤CRkâ€‹Tâˆ’t;|hkâ€‹(t,x)âˆ’hâ€‹(t,x)|â‰¤CRk,(t,x)âˆˆ[0,T]Ã—BR.\displaystyle|\nabla h^{k}(t,x)-\nabla h(t,x)|\leq\frac{C\_{R}}{k\sqrt{T-t}};\quad|h^{k}(t,x)-h(t,x)|\leq\frac{C\_{R}}{k},\quad(t,x)\in[0,T]\times B\_{R}. |  | (6.18) |

Finally, we note that by definition the function hh is positive everywhere (unless ÏÎ¼tarâ‰¡0\rho^{\mu\_{\rm tar}}\equiv 0), and being the classical solution to the PDE ([6.17](https://arxiv.org/html/2510.11829v1#S6.E17 "In 6 Existence of optimal control and convergence for general ğœ‡áµ¢â‚™áµ¢ â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) it is continuous. Thus, given R>0R>0, hâ€‹(t,x)â‰¥Î´R>0h(t,x)\geq\delta\_{R}>0, for all (t,x)âˆˆ[0,T]Ã—BR(t,x)\in[0,T]\times B\_{R}. Since ([6.18](https://arxiv.org/html/2510.11829v1#S6.E18 "In 6 Existence of optimal control and convergence for general ğœ‡áµ¢â‚™áµ¢ â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")) implies that hkh^{k} converges to hh uniformly on compacts, thus it must hold that
hkâ€‹(t,x)â‰¥Î´R/2h^{k}(t,x)\geq\delta\_{R}/2, for (t,x)âˆˆ[0,T]Ã—BR(t,x)\in[0,T]\times B\_{R}, and kk large enough. We therefore conclude,
similar to ([4.1](https://arxiv.org/html/2510.11829v1#S4.Ex5 "4.1 The Convergence of Optimal Policies â€£ 4 Convergence Results under Delta Initial Distribution â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")), that

|  |  |  |  |
| --- | --- | --- | --- |
|  | |Î±^kâ€‹(t,x)âˆ’Î±^â€‹(t,x)|\displaystyle|\widehat{\alpha}^{k}(t,x)-\widehat{\alpha}(t,x)| | â‰¤|âˆ‡hkâ€‹(t,x)âˆ’âˆ‡hâ€‹(t,x)hkâ€‹(t,x)|+|âˆ‡hâ€‹(t,x)|â€‹|hâ€‹(t,x)âˆ’hkâ€‹(t,x)hkâ€‹(t,x)â€‹hâ€‹(t,x)|â‰¤CRkâ€‹Tâˆ’t,\displaystyle\leq\left|\frac{\nabla h^{k}(t,x)-\nabla h(t,x)}{h^{k}(t,x)}\right|+|\nabla h(t,x)|\left|\frac{h(t,x)-h^{k}(t,x)}{h^{k}(t,x)h(t,x)}\right|\leq\frac{C\_{R}}{k\sqrt{T-t}}, |  |

as kâ†’âˆk\to\infty, for (t,x)âˆˆ[0,T]Ã—BR(t,x)\in[0,T]\times B\_{R}, where CRC\_{R} depends on MRM\_{R} and Î´R\delta\_{R} above, but independent of kk. Integrating in tt we obtain ([6.14](https://arxiv.org/html/2510.11829v1#S6.E14 "In Theorem 6.4. â€£ 6 Existence of optimal control and convergence for general ğœ‡áµ¢â‚™áµ¢ â€£ SchrÃ¶dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, HuyÃªn Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhouâ€™s 60th birthday")).
âˆ

## 7 Conclusion

We study the soft-constrained SchrÃ¶dinger bridge problem (SCSBP) as a flexible alternative to the classical formulation for generative modeling. By replacing hard terminal constraints with a general penalty function, the SCSBP potentially offers greater flexibility and stability for generative AI tasks. Moreover, we establish linear convergence of both the value functions and the optimal controls as the penalty parameter tends to infinity, thereby providing a theoretical guarantee for the framework.

In future work, we will develop efficient algorithms for learning the SCSBP solutions and test the performance on benchmark generative AI tasks. This will allow us to translate the theoretical framework into practical tools, further demonstrating the potential of regularized stochastic control formulations for modern generative modeling.

## References

* [1]

  B.Â Acciaio, S.Â Eckstein, and S.Â Hou.
  Time-causal vae: Robust financial time series generator.
  arXiv preprint arXiv:2411.02947, 2024.
* [2]

  A.Â Alouadi, B.Â Barreau, L.Â Carlier, and H.Â Pham.
  Robust time series generation via schr\\backslash" odinger bridge: a
  comprehensive evaluation.
  arXiv preprint arXiv:2503.02943, 2025.
* [3]

  D.Â G. Aronson.
  Bounds for the fundamental solution of a parabolic equation.
  1967.
* [4]

  Y.Â Bai, E.Â Yang, B.Â Han, Y.Â Yang, J.Â Li, Y.Â Mao, G.Â Niu, and T.Â Liu.
  Understanding and improving early stopping for learning with noisy
  labels.
  Advances in Neural Information Processing Systems,
  34:24392â€“24403, 2021.
* [5]

  J.-D. Benamou, G.Â Chazareix, and G.Â Loeper.
  From entropic transport to martingale transport, and applications to
  model calibration.
  arXiv preprint arXiv:2406.11537, 2024.
* [6]

  J.Â Betker, G.Â Goh, L.Â Jing, T.Â Brooks, J.Â Wang, L.Â Li, L.Â Ouyang, J.Â Zhuang,
  J.Â Lee, Y.Â Guo, etÂ al.
  Improving image generation with better captions.
  Computer Science. https://cdn. openai. com/papers/dall-e-3.
  pdf, 2(3):8, 2023.
* [7]

  A.Â Beurling.
  An automorphism of product measures.
  Annals of Mathematics, 72(1):189â€“200, 1960.
* [8]

  J.-M. Bismut.
  Large Deviation and Malliavin Calculus, volumeÂ 45 of Progress in Mathematics.
  BirkhÃ¤ser, 1984.
* [9]

  H.Â Cao, H.Â Gu, X.Â Guo, and M.Â Rosenbaum.
  Risk of transfer learning and its applications in finance.
  arXiv preprint arXiv:2311.03283, 2023.
* [10]

  G.Â Carlier, L.Â Chizat, and M.Â Laborde.
  Displacement smoothness of entropic optimal transport.
  ESAIM: Control, Optimisation and Calculus of Variations, 30:25,
  2024.
* [11]

  M.Â Chen, K.Â Huang, T.Â Zhao, and M.Â Wang.
  Score approximation, estimation and distribution recovery of
  diffusion models on low-dimensional data.
  In International Conference on Machine Learning, pages
  4672â€“4712. PMLR, 2023.
* [12]

  M.Â Chen, R.Â Xu, Y.Â Xu, and R.Â Zhang.
  Diffusion factor models: Generating high-dimensional returns with
  factor structure.
  arXiv preprint arXiv:2504.06566, 2025.
* [13]

  T.Â Chen, G.-H. Liu, and E.Â A. Theodorou.
  Likelihood training of schr\\backslash" odinger bridge using
  forward-backward sdes theory.
  arXiv preprint arXiv:2110.11291, 2021.
* [14]

  Y.Â Chen, T.Â T. Georgiou, and M.Â Pavon.
  Stochastic control liaisons: Richard sinkhorn meets gaspard monge on
  a schrodinger bridge.
  Siam Review, 63(2):249â€“313, 2021.
* [15]

  Z.Â Chen, A.Â Mustafi, P.Â Glaser, A.Â Korba, A.Â Gretton, and B.Â K. Sriperumbudur.
  (de)-regularized maximum mean discrepancy gradient flow.
  arXiv preprint arXiv:2409.14980, 2024.
* [16]

  A.Â Chiarini, G.Â Conforti, G.Â Greco, and L.Â Tamanini.
  Gradient estimates for the schrÃ¶dinger potentials: convergence to
  the brenier map and quantitative stability.
  Communications in Partial Differential Equations,
  48(6):895â€“943, 2023.
* [17]

  S.-N. Chow, W.Â Li, C.Â Mou, and H.Â Zhou.
  Dynamical schrÃ¶dinger bridge problems on graphs.
  Journal of Dynamics and Differential Equations, pages 1â€“20,
  2022.
* [18]

  P.Â DaiÂ Pra.
  A stochastic control approach to reciprocal diffusion processes.
  Applied mathematics and Optimization, 23(1):313â€“329, 1991.
* [19]

  G.Â DalÂ Maso.
  Introduction to Gamma-convergence.
  Springer Science + Business Media, LLC, 1993.
* [20]

  V.Â DeÂ Bortoli, J.Â Thornton, J.Â Heng, and A.Â Doucet.
  Diffusion schrÃ¶dinger bridge with applications to score-based
  generative modeling.
  Advances in Neural Information Processing Systems,
  34:17695â€“17709, 2021.
* [21]

  W.Â E. Deming and F.Â F. Stephan.
  On a least squares adjustment of a sampled frequency table when the
  expected marginal totals are known.
  The Annals of Mathematical Statistics, 11(4):427â€“444, 1940.
* [22]

  V.Â Divol, J.Â Niles-Weed, and A.-A. Pooladian.
  Tight stability bounds for entropic brenier maps.
  International Mathematics Research Notices, 2025(7):rnaf078,
  2025.
* [23]

  D.Â A. Edwards.
  On the kantorovish-rubinstein theorem.
  Expositiones Mathematicae, 29:387â€“398, 2011.
* [24]

  K.Â D. Elworthy and X.Â M. Li.
  Formulae for the derivatives of heat semigroups.
  Journal of Functional Analysis, 125:252â€“286, 1994.
* [25]

  H.Â FÃ¶llmer.
  Random fields and diffusion processes.
  In Ã‰cole dâ€™Ã‰tÃ© de ProbabilitÃ©s de Saint-Flour
  XVâ€“XVII, 1985â€“87, pages 101â€“203. Springer, 2006.
* [26]

  R.Â Fortet.
  RÃ©solution dâ€™un systÃ¨me dâ€™Ã©quations de m.
  schrÃ¶dinger.
  Journal de mathÃ©matiques pures et appliquÃ©es,
  19(1-4):83â€“105, 1940.
* [27]

  E.Â FourniÃ©, J.-M. Lasry, J.Â Lebuchoux, P.-L. Lions, and N.Â Touzi.
  Applications of malliavin calculus to monte carlo methods in finance.
  Finance and Stochastics, 3:391â€“412, 1999.
* [28]

  H.Â Fu, Z.Â Yang, M.Â Wang, and M.Â Chen.
  Unveil conditional diffusion models with classifier-free guidance: A
  sharp statistical theory.
  arXiv preprint arXiv:2403.11968, 2024.
* [29]

  J.Â Garg, X.Â Zhang, and Q.Â Zhou.
  Soft-constrained schrÃ¶dinger bridge: a stochastic control
  approach.
  In International Conference on Artificial Intelligence and
  Statistics, pages 4429â€“4437. PMLR, 2024.
* [30]

  X.Â Gu, C.Â Du, T.Â Pang, C.Â Li, M.Â Lin, and Y.Â Wang.
  On memorization in diffusion models.
  arXiv preprint arXiv:2310.02664, 2023.
* [31]

  F.Â F. Gunsilius.
  On the convergence rate of potentials of brenier maps.
  Econometric Theory, 38(2):381â€“417, 2022.
* [32]

  M.Â Hamdouche, P.Â Henry-Labordere, and H.Â Pham.
  Generative modeling for time series via schr {\{\\backslash" o}\}
  dinger bridge.
  arXiv preprint arXiv:2304.05093, 2023.
* [33]

  Y.Â Han, M.Â Razaviyayn, and R.Â Xu.
  Neural network-based score estimation in diffusion models:
  Optimization and generalization.
  arXiv preprint arXiv:2401.15604, 2024.
* [34]

  Y.Â Han, M.Â Razaviyayn, and R.Â Xu.
  Stochastic control for fine-tuning diffusion models: Optimality,
  regularity, and convergence.
  arXiv preprint arXiv:2412.18164, 2024.
* [35]

  C.Â HernÃ¡ndez and L.Â Tangpi.
  Propagation of chaos for mean field schr\\backslash" odinger
  problems.
  arXiv preprint arXiv:2304.09340, 2023.
* [36]

  J.Â Ho, A.Â Jain, and P.Â Abbeel.
  Denoising diffusion probabilistic models.
  In Neurips, volumeÂ 33, pages 6840â€“6851, 2020.
* [37]

  B.Â Jamison.
  The markov processes of schrÃ¶dinger.
  Zeitschrift fÃ¼r Wahrscheinlichkeitstheorie und verwandte
  Gebiete, 32(4):323â€“331, 1975.
* [38]

  L.Â Kong, H.Â Wang, T.Â Wang, G.Â Xiong, and M.Â Tambe.
  Composite flow matching for reinforcement learning with
  shifted-dynamics data.
  arXiv preprint arXiv:2505.23062, 2025.
* [39]

  S.Â Kullback and R.Â Leibler.
  On information and sufficiency.
  Annals of Mathematical Statistics, 22(1):79â€“86, 1951.
* [40]

  C.Â LÃ©onard.
  A survey of the schr\\backslash" odinger problem and some of its
  connections with optimal transport.
  arXiv preprint arXiv:1308.0215, 2013.
* [41]

  P.Â Li, Z.Â Li, H.Â Zhang, and J.Â Bian.
  On the generalization properties of diffusion models.
  Advances in Neural Information Processing Systems,
  36:2097â€“2127, 2023.
* [42]

  G.Â Loeper, J.Â Obloj, and B.Â Joseph.
  Calibration of local volatility models with stochastic interest rates
  using optimal transport.
  arXiv preprint arXiv:2305.00200, 2023.
* [43]

  J.Â Ma and J.Â Zhang.
  Representation theorems for backward stochastic differential
  equations.
  Annals of Applied Probability, 12(4):1390â€“1418, 2002.
* [44]

  E.Â F. Montesuma, F.Â M.Â N. Mboula, and A.Â Souloumiac.
  Recent advances in optimal transport for machine learning.
  IEEE Transactions on Pattern Analysis and Machine Intelligence,
  2024.
* [45]

  T.Â Moon, M.Â Choi, G.Â Lee, J.-W. Ha, and J.Â Lee.
  Fine-tuning diffusion models with limited data.
  In NeurIPS 2022 Workshop on Score-Based Methods, 2022.
* [46]

  M.Â Nutz and J.Â Wiesel.
  Entropic optimal transport: Convergence of potentials.
  Probability Theory and Related Fields, 184(1):401â€“424, 2022.
* [47]

  OpenAI.
  Sora: Creating video from text.
  <https://openai.com/sora>, 2024.
* [48]

  Y.Â Ouyang, L.Â Xie, H.Â Zha, and G.Â Cheng.
  Transfer learning for diffusion models.
  Advances in Neural Information Processing Systems,
  37:136962â€“136989, 2024.
* [49]

  S.Â Peluchetti.
  Diffusion bridge mixture transports, schrÃ¶dinger bridge problems
  and generative modeling.
  Journal of Machine Learning Research, 24(374):1â€“51, 2023.
* [50]

  P.Â E. Protter.
  Stochastic Integration and Differential Equations.
  Springer-Verlag, Heidelberg, second edition, 2005.
* [51]

  A.Â Ramesh, P.Â Dhariwal, A.Â Nichol, C.Â Chu, and M.Â Chen.
  Hierarchical text-conditional image generation with clip latents.
  arXiv preprint arXiv:2204.06125, 1(2):3, 2022.
* [52]

  L.Â Richter and J.Â Berner.
  Improved sampling via learned diffusions.
  arXiv preprint arXiv:2307.01198, 2023.
* [53]

  R.Â Rombach, A.Â Blattmann, D.Â Lorenz, P.Â Esser, and B.Â Ommer.
  High-resolution image synthesis with latent diffusion models.
  In Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition, pages 10684â€“10695, 2022.
* [54]

  Y.Â Shi, V.Â DeÂ Bortoli, A.Â Campbell, and A.Â Doucet.
  Diffusion schrÃ¶dinger bridge matching.
  Advances in Neural Information Processing Systems, 36, 2024.
* [55]

  J.Â Sohl-Dickstein, E.Â Weiss, N.Â Maheswaranathan, and S.Â Ganguli.
  Deep unsupervised learning using nonequilibrium thermodynamics.
  In International Conference on Machine Learning, pages
  2256â€“2265. PMLR, 2015.
* [56]

  K.-U. Song.
  Applying regularized schr\\backslash" odinger-bridge-based
  stochastic process in generative modeling.
  arXiv preprint arXiv:2208.07131, 2022.
* [57]

  Y.Â Song and S.Â Ermon.
  Generative modeling by estimating gradients of the data distribution.
  Advances in neural information processing systems, 32, 2019.
* [58]

  Y.Â Song, J.Â Sohl-Dickstein, D.Â P. Kingma, A.Â Kumar, S.Â Ermon, and B.Â Poole.
  Score-based generative modeling through stochastic differential
  equations.
  arXiv preprint arXiv:2011.13456, 2020.
* [59]

  T.Â Sweeting.
  On a converse to scheffÃ©â€™s theorem.
  The Annals of Statistics, 14(3):1252â€“1256, 1986.
* [60]

  W.Â Tang.
  Fine-tuning of diffusion models via stochastic control: entropy
  regularization and beyond.
  arXiv preprint arXiv:2403.06279, 2024.
* [61]

  L.Â Torrey and J.Â Shavlik.
  Transfer learning.
  In Handbook of research on machine learning applications and
  trends: algorithms, methods, and techniques, pages 242â€“264. IGI Global
  Scientific Publishing, 2010.
* [62]

  M.Â Uehara, Y.Â Zhao, K.Â Black, E.Â Hajiramezanali, G.Â Scalia, N.Â L. Diamant,
  A.Â M. Tseng, T.Â Biancalani, and S.Â Levine.
  Fine-tuning of continuous-time diffusion models as
  entropy-regularized control.
  arXiv preprint arXiv:2402.15194, 2024.
* [63]

  F.Â Vargas, S.Â Padhy, D.Â Blessing, and N.Â NÃ¼sken.
  Transport meets variational inference: Controlled monte carlo
  diffusions.
  arXiv preprint arXiv:2307.01050, 2023.
* [64]

  F.Â Vargas, P.Â Thodoroff, A.Â Lamacraft, and N.Â Lawrence.
  Solving schrÃ¶dinger bridges via maximum likelihood.
  Entropy, 23(9):1134, 2021.
* [65]

  C.Â Villani.
  Topics in Optimal Transportation.
  Graduate Studies in Mathematics, 58. AMS, Providence, RI, 2003.
* [66]

  G.Â Wang, Y.Â Jiao, Q.Â Xu, Y.Â Wang, and C.Â Yang.
  Deep generative learning via schrÃ¶dinger bridge.
  In International conference on machine learning, pages
  10794â€“10804. PMLR, 2021.
* [67]

  L.Â Winkler, C.Â Ojeda, and M.Â Opper.
  A score-based approach for training schrÃ¶dinger bridges for data
  modelling.
  Entropy, 25(2):316, 2023.
* [68]

  T.Â Xu, L.Â K. Wenliang, M.Â Munn, and B.Â Acciaio.
  Cot-gan: Generating sequential data via causal optimal transport.
  Advances in neural information processing systems,
  33:8798â€“8809, 2020.
* [69]

  J.Â Zhang.
  Backward stochastic differential equations.
  Springer, 2017.
* [70]

  H.Â Zhao, H.Â Chen, Y.Â Guo, G.Â I. Winata, T.Â Ou, Z.Â Huang, D.Â D. Yao, and
  W.Â Tang.
  Fine-tuning diffusion generative models via rich preference
  optimization.
  arXiv preprint arXiv:2503.11720, 2025.