---
authors:
- Jin Ma
- Ying Tan
- Renyuan Xu
doc_id: arxiv:2510.11829v1
family_id: arxiv:2510.11829
is_current: true
taxonomy:
  alpha_families: []
  asset_classes: []
  horizons: []
  themes: []
title: 'Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence
  analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama
  Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs
  60th birthday'
url_abs: http://arxiv.org/abs/2510.11829v1
url_html: https://arxiv.org/html/2510.11829v1
venue: arXiv q-fin
version: 1
year: 2025
---


Jin Ma, ¬†Ying Tan, ¬†and
Renyuan Xu
Department of Mathematics, University of Southern California, Los Angeles, CA, 90089.
Email: jinma@usc.edu. This author is supported in part by US NSF grants DMS#2205972 and #2510403.
Department of Statistics and Applied Probability, University of California, Santa Barbara, CA 93106. Email: yingtan@ucsb.edu.Department of Management of Science & Engineering, Stanford University, Stanford, CA 94305. Email: renyuanxu@stanford.edu. This author is supported in part by the NSF CAREER Award DMS-2524465.

(October 13, 2025)

###### Abstract

Generative AI can be framed as the problem of learning a model that maps simple reference measures into complex data distributions, and it has recently found a strong connection to the classical theory of the Schr√∂dinger bridge problems (SBPs)
due partly to their common nature of interpolating between prescribed marginals via entropy-regularized stochastic dynamics. However, the classical SBP enforces hard terminal constraints, which often leads to instability in practical implementations, especially in high-dimensional or data-scarce regimes.
To address this challenge, we follow the idea of the so-called soft-constrained Schr√∂dinger bridge problem(SCSBP), in which the terminal constraint is replaced by a general penalty function. This relaxation leads to a more flexible stochastic control formulation of McKean‚ÄìVlasov type.

We establish the existence of optimal solutions for all penalty levels and prove that, as the penalty grows, both the controls and value functions converge to those of the classical SBP at a linear rate. Our analysis builds on Doob‚Äôs hh-transform representations, the stability results of Schr√∂dinger potentials, Œì\Gamma-convergence, and a novel fixed-point argument that couples an optimization problem over the space of measures with an auxiliary entropic optimal transport problem. These results not only provide the first quantitative convergence guarantees for soft-constrained bridges but also shed light on how penalty regularization enables robust generative modeling, fine-tuning, and transfer learning.

Keywords. Schr√∂dinger bridge, soft-constrained Schr√∂dinger bridge, entropic optimal transport stability, Schr√∂dinger potentials, generative AI, hh-transform, converse of Scheff√©‚Äôs Theorem, Œì\Gamma-convergence, Schauder‚Äôs fixed-point.

2020 AMS Mathematics subject classification: 60H10, 60J60, 49J21, 68T01, 93E20.

## 1 Introduction

Generative modeling provides a powerful framework for synthesizing data that preserves the statistical structure of real-world samples while introducing controlled variability. Among the most prominent approaches, diffusion models ‚Äî such as those introduced by [[55](https://arxiv.org/html/2510.11829v1#bib.bib55), [57](https://arxiv.org/html/2510.11829v1#bib.bib57), [58](https://arxiv.org/html/2510.11829v1#bib.bib58), [36](https://arxiv.org/html/2510.11829v1#bib.bib36)] ‚Äî have achieved remarkable success, underpinning state-of-the-art systems like DALL¬∑E 2 and 3 [[51](https://arxiv.org/html/2510.11829v1#bib.bib51), [6](https://arxiv.org/html/2510.11829v1#bib.bib6)], Stable Diffusion [[53](https://arxiv.org/html/2510.11829v1#bib.bib53)], and Sora [[47](https://arxiv.org/html/2510.11829v1#bib.bib47)]. These models learn to reverse a diffusion process that gradually adds noise to data, enabling the generation of realistic samples from pure noise. Such a structure, namely, transforming a noise distribution into a data distribution, closely mirrors the Schr√∂dinger bridge problem (or dynamic optimal transport), which has recently gained renewed attention as a principled framework for generative modeling due to its structural parallels with diffusion models and its ability to interpolate between distributions in a statistically grounded manner.

The Schr√∂dinger bridge problem (SBP for short), originally proposed as an entropy-regularized variant of optimal transport, seeks the most likely evolution of a process, subject to a reference diffusion process, that matches prescribed marginal distributions Œºini,Œºtar{\mu\_{\rm ini}},{\mu\_{\rm tar}} at two endpoints. Under suitable regularity conditions, the optimally controlled process remains a diffusion but with an additional drift term added to the reference process. This result has been established through various approaches and levels of generality, with seminal contributions by [[26](https://arxiv.org/html/2510.11829v1#bib.bib26), [7](https://arxiv.org/html/2510.11829v1#bib.bib7), [37](https://arxiv.org/html/2510.11829v1#bib.bib37), [25](https://arxiv.org/html/2510.11829v1#bib.bib25), [18](https://arxiv.org/html/2510.11829v1#bib.bib18)].

The recent generative modeling literature has seen a surge in the use of Schr√∂dinger bridges. In these applications, one typically starts or chooses some distribution Œºini{\mu\_{\rm ini}} that is easy to sample from, and tries to "learn" an unknown distribution Œºtar{\mu\_{\rm tar}} of a given data set. By numerically approximating the solution to the Schr√∂dinger bridge problem, one can generate unlimited samples (i.e., synthetic data points that resemble the original data set). One such algorithm is presented by De Bortoli et al. [[20](https://arxiv.org/html/2510.11829v1#bib.bib20)] and Vargas et al.¬†[[64](https://arxiv.org/html/2510.11829v1#bib.bib64)], who approximate the iterative proportional fitting procedure (Deming-Stephan [[21](https://arxiv.org/html/2510.11829v1#bib.bib21)]), using score matching with neural networks and maximum likelihood, respectively.
Concurrently, Wang et al. [[66](https://arxiv.org/html/2510.11829v1#bib.bib66)] proposed a two-stage method with an auxiliary bridge handling possible non-smooth Œºtar{\mu\_{\rm tar}}. Some more recent developments include
[[13](https://arxiv.org/html/2510.11829v1#bib.bib13), [56](https://arxiv.org/html/2510.11829v1#bib.bib56), [49](https://arxiv.org/html/2510.11829v1#bib.bib49), [52](https://arxiv.org/html/2510.11829v1#bib.bib52), [67](https://arxiv.org/html/2510.11829v1#bib.bib67), [32](https://arxiv.org/html/2510.11829v1#bib.bib32), [63](https://arxiv.org/html/2510.11829v1#bib.bib63), [54](https://arxiv.org/html/2510.11829v1#bib.bib54), [62](https://arxiv.org/html/2510.11829v1#bib.bib62), [2](https://arxiv.org/html/2510.11829v1#bib.bib2)] as well as developing optimal transport techniques for generative AI tasks [[44](https://arxiv.org/html/2510.11829v1#bib.bib44), [5](https://arxiv.org/html/2510.11829v1#bib.bib5), [42](https://arxiv.org/html/2510.11829v1#bib.bib42), [68](https://arxiv.org/html/2510.11829v1#bib.bib68), [1](https://arxiv.org/html/2510.11829v1#bib.bib1)].

However, the classical SBP imposes hard terminal constraints on the marginal distributions, which can result in computational difficulties, instability in high-dimensional settings, and limited adaptability when aligning with empirical data in generative tasks. In practice, most numerical schemes for solving the SBP rely on iterative procedures that alternately relax the initial and terminal constraints. These algorithms can exhibit instability, particularly when the two constraints differ significantly, and their convergence guarantees for general target distributions remain an open problem in the literature.

In this paper, in light of Garg et al. [[29](https://arxiv.org/html/2510.11829v1#bib.bib29)], we study a soft-constrained Schr√∂dinger bridge problem (SCSBP).
Mathematically, we consider a (smooth) penalty function G:ùí´2‚Äã(‚Ñùd)‚Ü¶‚Ñù+G:\mathscr{P}\_{2}(\mathbb{R}^{d})\mapsto\mathbb{R}\_{+}, satisfying G‚Äã(Œº;Œºtar)=0G(\mu;{\mu\_{\rm tar}})=0,
where ùí´2‚Äã(‚Ñùd)\mathscr{P}\_{2}(\mathbb{R}^{d}) denotes the 2-Wasserstein space on ‚Ñùd\mathbb{R}^{d}, and Œºtar‚ààùí´2‚Äã(‚Ñùd){\mu\_{\rm tar}}\in\mathscr{P}\_{2}(\mathbb{R}^{d}) is some given ‚Äútarget‚Äù distribution. For each k‚àà‚Ñïk\in\mathbb{N} and a given initial distribution Œºini‚ààùí´2‚Äã(‚Ñùd){\mu\_{\rm ini}}\in\mathscr{P}\_{2}(\mathbb{R}^{d}), we consider the following stochastic control problem with dynamics:

|  |  |  |
| --- | --- | --- |
|  | d‚ÄãXtŒ±=(b‚Äã(t,XtŒ±)+œÉ‚Äã(t)‚ÄãŒ±t)‚Äãd‚Äãt+œÉ‚Äã(t)‚Äãd‚ÄãWt,‚Ñô‚àò(X0Œ±)‚àí1=Œºini,dX^{\alpha}\_{t}=\big(b(t,X^{\alpha}\_{t})+\sigma(t)\alpha\_{t}\big)dt+\sigma(t)dW\_{t},\quad\mathbb{P}\circ(X\_{0}^{\alpha})^{-1}={\mu\_{\rm ini}}, |  |

and cost functional

|  |  |  |
| --- | --- | --- |
|  | Jk‚Äã(Œ±)=ùîº‚Äã[12‚Äã‚à´0T|Œ±s|2‚Äãùëës+k‚ÄãG‚Äã(‚ÑôXTŒ±)],J^{k}(\alpha)=\mathbb{E}\left[\frac{1}{2}\int\_{0}^{T}|\alpha\_{s}|^{2}ds+kG(\mathbb{P}\_{X^{\alpha}\_{T}})\right], |  |

where ‚ÑôXTŒ±\mathbb{P}\_{X^{\alpha}\_{T}} is the law of XTŒ±X^{\alpha}\_{T} and the control Œ±\alpha is chosen from a square-integrable progressively measurable admissible control set ùíú{\cal A}. The goal is to find Vk:=infŒ±‚ààùíúJk‚Äã(Œ±)V^{k}:=\inf\_{\alpha\in{\cal A}}J^{k}(\alpha) and optimal control Œ±k\alpha^{k}, for each k‚àà‚Ñïk\in\mathbb{N}, and study the limiting behavior of {Œ±k}\{\alpha^{k}\} and {Vk}\{V^{k}\}.
Clearly, the dependence of the terminal cost on ‚ÑôXTŒ±\mathbb{P}\_{X^{\alpha}\_{T}} renders this relaxed formulation a non-standard stochastic control problem, leading to a McKean-Vlasov type of stochastic control. In contrast to Garg et al. [[29](https://arxiv.org/html/2510.11829v1#bib.bib29)], which focuses on the case where the penalty
GG is given by the KL divergence and
Œºini\mu\_{\rm ini} is a delta measure, we investigate the problem under more general cost functions and initial distributions.

Compared to existing methods using SBP under hard constraints, there are several advantages to considering SCSBP. First, when the KL divergence between Œºtar{\mu\_{\rm tar}} and ‚ÑôXTŒ±\mathbb{P}\_{X^{\alpha}\_{T}} is infinite, the Schr√∂dinger bridge does not admit a solution, whereas SCSBP always does Garg et al. [[29](https://arxiv.org/html/2510.11829v1#bib.bib29)]. More importantly, the penalty parameter kk acts as a regularization factor, preventing the algorithm from overfitting to Œºtar\mu\_{\rm tar}, which is crucial for certain generative modeling tasks with limited data [[30](https://arxiv.org/html/2510.11829v1#bib.bib30), [45](https://arxiv.org/html/2510.11829v1#bib.bib45)]. In addition, SCSBP provides a more general framework in generative AI, with applications beyond pre-training in data generation, and can be applied to fine-tuning and transfer learning (see Examples [2.6](https://arxiv.org/html/2510.11829v1#S2.ThmTheorem6 "Example 2.6 (Fine-tuning under a reward signal). ‚Ä£ Applications in Generative AI. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") and [2.7](https://arxiv.org/html/2510.11829v1#S2.ThmTheorem7 "Example 2.7 (Transfer learning). ‚Ä£ Applications in Generative AI. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")).

### 1.1 Outline of the Main Results and Contributions

The soft-constrained Schr√∂dinger bridge problem (SCSBP) studied in this paper replaces the terminal distribution constraint by a general penalty function, which leads to a McKean‚ÄìVlasov type stochastic control problem. The main results include the existence of the optimal solution to the SCSBP at each penalty level kk, and the convergence of the solutions to the SCSBP to that of the SBP, in terms
of both the optimal policy and the corresponding value function, as k‚Üí‚àûk\to\infty.

More precisely, we begin with the special case where the initial measure is a delta measure. In this setting, we derive the explicit form of the optimal control policy for SCSBP via Doob‚Äôs hh-transform (Proposition¬†[3.6](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem6 "Proposition 3.6. ‚Ä£ 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) and more importantly, we establish a linear convergence rate for the optimal control (Theorem¬†[4.1](https://arxiv.org/html/2510.11829v1#S4.ThmTheorem1 "Theorem 4.1. ‚Ä£ 4.1 The Convergence of Optimal Policies ‚Ä£ 4 Convergence Results under Delta Initial Distribution ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")). To the best of our knowledge, such a rate of convergence is novel in the literature. Moreover, by applying the so-called early stopping, we are able to obtain the linear convergence results for the corresponding value functions (Proposition¬†[4.4](https://arxiv.org/html/2510.11829v1#S4.ThmTheorem4 "Proposition 4.4. ‚Ä£ 4.2 The Convergence of the Value Function ‚Ä£ 4 Convergence Results under Delta Initial Distribution ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) as well as the Wasserstein distance between the target distribution and the output distribution of the SCSBP (Proposition¬†[4.5](https://arxiv.org/html/2510.11829v1#S4.ThmTheorem5 "Proposition 4.5. ‚Ä£ 4.2 The Convergence of the Value Function ‚Ä£ 4 Convergence Results under Delta Initial Distribution ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")).

The similar results in the case of a general initial distribution is much more involved. Among other things, we establish and/or extend some recently observed stability results of the solutions to the SBP, as the foundation for a fixed-point argument. A key element in our argument is the continuous dependence (or stability) of a mapping that is well known in the (static) optimal transport literature. Specifically, for any (Œºini,Œºtar)‚ààùí´2‚Äã(‚Ñùd)√óùí´2‚Äã(‚Ñùd)({\mu\_{\rm ini}},{\mu\_{\rm tar}})\in\mathscr{P}\_{2}(\mathbb{R}^{d})\times\mathscr{P}\_{2}(\mathbb{R}^{d}), it is known (cf. e.g., [[7](https://arxiv.org/html/2510.11829v1#bib.bib7)]) that there exists a unique pair of œÉ\sigma-finite measures (ŒΩ0,ŒΩ):=ùíØ‚Äã(Œºini,Œºtar)(\nu\_{0},\nu):={\cal T}({\mu\_{\rm ini}},{\mu\_{\rm tar}}) such that the measure œÄ‚ààùí´2‚Äã(‚Ñùd√ó‚Ñùd)\pi\in\mathscr{P}\_{2}(\mathbb{R}^{d}\times\mathbb{R}^{d}) defined by

|  |  |  |  |
| --- | --- | --- | --- |
|  | œÄ‚Äã(E)=‚à´Ep‚Äã(T,y;0,x)‚ÄãŒΩ0‚Äã(d‚Äãx)‚ÄãŒΩ‚Äã(d‚Äãy),E‚àà‚Ñ¨‚Äã(‚Ñùd√ó‚Ñùd)\displaystyle\pi(E)=\int\_{E}p(T,y;0,x)\nu\_{0}(dx)\nu(dy),\qquad E\in\mathscr{B}(\mathbb{R}^{d}\times\mathbb{R}^{d}) |  | (1.1) |

has the marginals Œºini{\mu\_{\rm ini}} and Œº\mu, where p‚Äã(‚ãÖ,‚ãÖ;‚ãÖ,‚ãÖ)p(\cdot,\cdot;\cdot,\cdot) is the transition density of a given diffusion process. If we fix Œºini{\mu\_{\rm ini}}, and denote œÅŒº\rho^{\mu} to be the density of the measure ŒΩ=ùíØ‚Äã(Œº)\nu={\cal T}(\mu), Œº‚ààùí´2‚Äã(‚Ñùd)\mu\in\mathscr{P}\_{2}(\mathbb{R}^{d}), then it turns out that the solution to the SCSBP is the fixed-point of the mapping Œì:ùí´2‚Äã(‚Ñùd)‚Ü¶ùí´2‚Äã(‚Ñùd)\Gamma:\mathscr{P}\_{2}(\mathbb{R}^{d})\mapsto\mathscr{P}\_{2}(\mathbb{R}^{d}), where

|  |  |  |  |
| --- | --- | --- | --- |
|  | Œì‚Äã(Œº):=arg‚ÄãminŒΩ‚ààùí´2‚Äã(‚Ñùd)‚Å°{k‚ÄãG‚Äã(ŒΩ)+ùîºX‚àºŒΩ‚Äã[log‚Å°œÅŒº‚Äã(X)]}.\displaystyle\Gamma(\mu):={\operatorname\*{arg\,min}}\_{\nu\in\mathscr{P}\_{2}(\mathbb{R}^{d})}\left\{kG(\nu)+\mathbb{E}\_{X\sim\nu}[\log\rho^{\mu}(X)]\right\}. |  | (1.2) |

The successful application of Schauder‚Äôs fixed-point theorem on the Wasserstein space relies on several key elements, in particular a crucial continuous dependence result of the mapping Œº‚Ü¶œÅŒº\mu\mapsto\rho^{\mu}, for which we introduce an auxiliary entropic optimal transport problem, and identify its solution to the measure œÄ\pi in ([1.1](https://arxiv.org/html/2510.11829v1#S1.E1 "In 1.1 Outline of the Main Results and Contributions ‚Ä£ 1 Introduction ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")).
By utilizing some important stability results of the corresponding Schr√∂dinger potentials (Proposition [5.8](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem8 "Proposition 5.8. ‚Ä£ 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")), together with some arguments in the spirit of the converse of Scheff√©‚Äôs theorem (i.e., the weak convergence in Prohorov metric vs. the convergence in densities) as well as the so-called Œì\Gamma-convergence of the minimizers of the optimization problem ([1.2](https://arxiv.org/html/2510.11829v1#S1.E2 "In 1.1 Outline of the Main Results and Contributions ‚Ä£ 1 Introduction ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")), we are able to verify the required properties so the mapping Œì\Gamma has a fixed-point (Theorem¬†[6.1](https://arxiv.org/html/2510.11829v1#S6.ThmTheorem1 "Theorem 6.1. ‚Ä£ 6 Existence of optimal control and convergence for general ùúá·µ¢‚Çô·µ¢ ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")).
As a direct consequence, we establish that the optimal control of the SCSBP converges linearly with respect to the penalty parameter kk (Theorem¬†[4.1](https://arxiv.org/html/2510.11829v1#S4.ThmTheorem1 "Theorem 4.1. ‚Ä£ 4.1 The Convergence of Optimal Policies ‚Ä£ 4 Convergence Results under Delta Initial Distribution ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")). We believe that such a fixed-point perspective is novel in the literature, as it not only offers a constructive framework for characterizing solutions in the general case but also yields insights into how the penalty parameter affects the convergence rate.

### 1.2 Closely Related Literature

Our general formulation is largely inspired by Garg et al. [[29](https://arxiv.org/html/2510.11829v1#bib.bib29)], which investigates the SCSBP with the KL divergence as the penalty function GG. Within that framework, the authors established the asymptotic convergence of the optimal policies as the penalty parameter kk tends to infinity, under the assumption that the initial measure is a delta measure. However, the use of KL divergence presents practical difficulties: if the model distribution Œº\mu assigns zero probability to any region where the data distribution Œºtar\mu\_{\rm tar} has positive mass, then KL‚Äã(Œºtar‚à•Œº)=+‚àû\mathrm{KL}(\mu\_{\rm tar}\|\mu)=+\infty, rendering the divergence ill-posed under support mismatch [[15](https://arxiv.org/html/2510.11829v1#bib.bib15), [38](https://arxiv.org/html/2510.11829v1#bib.bib38)]. Moreover, a delta initial measure is rarely employed in generative tasks, as it lacks the diversity and randomness required for effective training. Finally, no convergence rate is quantified therein.

On a technical level, our formulation is closely related to Hern√°ndez-Tangpi [[35](https://arxiv.org/html/2510.11829v1#bib.bib35)], in which
the authors use a probabilistic approach to recast a mean-field Schr√∂dinger bridge into a stochastic optimization problem with McKean-Vlasov dynamics, and connect the optimal control to a solution to a forward backward SDE of McKean-Vlasov type (MKV FBSDE). However, given the generality of the drift, diffusion, and running cost functions, the associated MKV FBSDE is derived without a discussion of uniqueness. In fact, it is not completely obvious that a McKean-Vlasov-type SBP can be converted to a McKean-Vlasov stochastic control problem via the usual Girsanov theorem argument, as we show in Remark [2.1](https://arxiv.org/html/2510.11829v1#S2.ThmTheorem1 "Remark 2.1 (Subtlety in formulating the McKean-Vlasov version of the problem). ‚Ä£ Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") below. Moreover, the conditions imposed on the penalty function
GG are abstract and can be difficult to verify in common examples. By contrast, our framework leverages the PDE formulation and Doob‚Äôs hh-transform representation, requiring only mild growth conditions on
GG and control of density gaps (see Assumption¬†([3.2](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem2 "Assumption 3.2. ‚Ä£ 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday"))). Several concrete examples of admissible
GG are provided in Example [3.4](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem4 "Example 3.4. ‚Ä£ 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") and [3.5](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem5 "Example 3.5. ‚Ä£ 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday").

The rest of the paper is organized as follows. Section [2](https://arxiv.org/html/2510.11829v1#S2 "2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") introduces the necessary concepts and notations. In particular, we present the connection between the underlying SBP and its stochastic control formulation, and introduce the notion of the SCSBP together with some potential applications. Section [3](https://arxiv.org/html/2510.11829v1#S3 "3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") is devoted to the existence of optimal policies for the SCSBP at each penalty parameter kk, while Section [4.1](https://arxiv.org/html/2510.11829v1#S4.SS1 "4.1 The Convergence of Optimal Policies ‚Ä£ 4 Convergence Results under Delta Initial Distribution ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") establishes that the penalized optimal policies converge to those of the original SBP as k‚Üí‚àûk\to\infty, with a linear rate of convergence. In addition, we prove convergence of the corresponding value functions and quantify the distance between the terminal distribution and the target distribution in terms of the Wasserstein distance. Sections [5](https://arxiv.org/html/2510.11829v1#S5 "5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") and [6](https://arxiv.org/html/2510.11829v1#S6 "6 Existence of optimal control and convergence for general ùúá·µ¢‚Çô·µ¢ ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") are devoted to the case with a general initial distribution. A crucial stability result is established in Section [5](https://arxiv.org/html/2510.11829v1#S5 "5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday"), via the stability of Schr√∂dinger potentials of an auxiliary entropy optimal transport problem, and in Section [6](https://arxiv.org/html/2510.11829v1#S6 "6 Existence of optimal control and convergence for general ùúá·µ¢‚Çô·µ¢ ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") we complete the fixed-point argument.

## 2 Preliminary

Throughout this paper, we consider a generic Euclidean space ùïè\mathbb{X}, and regardless of its dimension, we denote (‚ãÖ,‚ãÖ)(\cdot,\cdot) and |‚ãÖ||\cdot| be its inner product and norm, respectively. We denote ‚ÑÇ‚Äã([0,T];ùïè)\mathbb{C}([0,T];\mathbb{X}) to be the space of ùïè\mathbb{X}-valued continuous functions defined on [0,T][0,T] with the usual sup-norm, and in particular, we denote ‚ÑÇTd:=‚ÑÇ‚Äã([0,T];‚Ñùd)\mathbb{C}^{d}\_{T}:=\mathbb{C}([0,T];\mathbb{R}^{d}), and let ‚Ñ¨‚Äã(‚ÑÇTd)\mathscr{B}(\mathbb{C}\_{T}^{d}) be its topological Borel field.
We shall consider the following
canonical probabilistic space: (Œ©,‚Ñ±,‚Ñô)(\Omega,{\cal F},\mathbb{P}), where (Œ©,‚Ñ±):=(‚ÑÇTd,‚Ñ¨‚Äã(‚ÑÇTd))(\Omega,{\cal F}):=(\mathbb{C}^{d}\_{T},\mathscr{B}(\mathbb{C}^{d}\_{T})) and ‚Ñô‚ààùí´‚Äã(Œ©)\mathbb{P}\in\mathscr{P}(\Omega), the space of all probability measures defined on (Œ©,‚Ñ±)(\Omega,{\cal F}). Finally, we let ‚Ñô0‚ààùí´‚Äã(Œ©)\mathbb{P}^{0}\in\mathscr{P}(\Omega) be
the Wiener measure on (Œ©,‚Ñ±)(\Omega,{\cal F}); Wt‚Äã(œâ):=œâ‚Äã(t)W\_{t}(\omega):=\omega(t), œâ‚ààŒ©\omega\in\Omega, the canonical process; and ùîΩ0:={‚Ñ±t0}t‚àà[0,T]\mathbb{F}^{0}:=\{{\cal F}^{0}\_{t}\}\_{t\in[0,T]}, where ‚Ñ±t0:=‚Ñ¨t(‚ÑÇTd):=œÉ{œâ(‚ãÖ‚àßt):œâ‚àà‚ÑÇTd}{\cal F}^{0}\_{t}:=\mathscr{B}\_{t}(\mathbb{C}\_{T}^{d}):=\sigma\{\omega(\cdot\wedge t):\omega\in\mathbb{C}^{d}\_{T}\}, t‚àà[0,T]t\in[0,T]. We assume that ùîΩ0\mathbb{F}^{0} has the usual ‚Ñô0\mathbb{P}^{0}-augmentation so that it satisfies the usual hypotheses (cf. e.g., [[50](https://arxiv.org/html/2510.11829v1#bib.bib50)]), and for p‚â•1p\geq 1, we denote ùïÉùîΩ0p‚Äã([0,T];ùïè)\mathbb{L}^{p}\_{\mathbb{F}^{0}}([0,T];\mathbb{X}) to be all ùïè\mathbb{X}-valued, pp-integrable, ùîΩ0\mathbb{F}^{0}-adapted processes. Finally, we denote ‚Ñ≥‚Äã(‚Ñùd)\mathscr{M}(\mathbb{R}^{d}) to be all œÉ\sigma-finite measures on ‚Ñùd\mathbb{R}^{d} and ùí´p‚Äã(‚Ñùd)\mathscr{P}\_{p}(\mathbb{R}^{d}) to be all probability measures with finite pp-th moment on ‚Ñùd\mathbb{R}^{d} equipped with pp-Wasserstein metric, denoted by Wp‚Äã(‚ãÖ,‚ãÖ)W\_{p}(\cdot,\cdot).

We recall that a classical Schr√∂dinger Bridge Problem (SBP) amounts to finding, for
‚Ñô‚ààùí´‚Äã(Œ©)\mathbb{P}\in\mathscr{P}(\Omega),

|  |  |  |  |
| --- | --- | --- | --- |
|  | V‚Äã(‚Ñô):=inf‚Ñö‚ààùí´‚Ä≤DKL‚Äã(‚Ñö‚à•‚Ñô),\displaystyle V(\mathbb{P}):=\inf\_{\mathbb{Q}\in\mathscr{P}^{\prime}}D\_{\rm KL}(\mathbb{Q}\|\mathbb{P}), |  | (2.1) |

where ùí´‚Ä≤‚äÇùí´‚Äã(Œ©)\mathscr{P}^{\prime}\subset\mathscr{P}(\Omega) is a given admissible set, and DKL(‚ãÖ‚à•‚ãÖ)D\_{\rm KL}(\,\cdot\,\|\,\cdot\,) is the so-called Kullback-Leibler Divergence or the Relative Entropy (cf. [[39](https://arxiv.org/html/2510.11829v1#bib.bib39)]), defined by

|  |  |  |
| --- | --- | --- |
|  | DKL(‚Ñö‚à•‚Ñô):={ùîº‚Ñö‚Äã[log‚Å°(d‚Äã‚Ñöd‚Äã‚Ñô)],if¬†‚Äãd‚Äã‚Ñö‚â™d‚Äã‚Ñô;‚àû,other wise.\displaystyle D\_{\rm KL}(\mathbb{Q}\|\mathbb{P}):=\begin{cases}\mathbb{E}^{\mathbb{Q}}\big[\log\big(\frac{d\mathbb{Q}}{d\mathbb{P}}\big)\big],&{\text{if }}d\mathbb{Q}\ll d\mathbb{P};\\ \infty,&\text{other wise}.\end{cases} |  |

We remark here that the KL-divergence DKL(‚ãÖ‚à•‚ãÖ)D\_{\rm KL}(\,\cdot\,\|\,\cdot\,) can be easily extended to any œÉ\sigma-finite measures222In this case DKL‚Äã(‚Ñö‚à•‚Ñô):=‚à´log‚Å°(‚Ñö‚Äã(d‚Äãx)‚Ñô‚Äã(d‚Äãx))‚Äã‚Ñö‚Äã(d‚Äãx)D\_{\rm KL}(\mathbb{Q}\|\mathbb{P}):=\int\log\big(\frac{\mathbb{Q}(dx)}{\mathbb{P}(dx)}\big)\mathbb{Q}(dx), if d‚Äã‚Ñö‚â™d‚Äã‚Ñôd\mathbb{Q}\ll d\mathbb{P}.. In what follows, we shall focus on the case when
‚Ñö=‚Ñô‚àòX‚àí1\mathbb{Q}=\mathbb{P}\,\circ\,X^{-1} for some ‚Ñùd\mathbb{R}^{d}-valued continuous process XX defined on the canonical space with some ‚Ñô‚ààùí´‚Äã(Œ©)\mathbb{P}\in\mathscr{P}(\Omega), such that ‚Ñö0:=‚Ñô‚àòX0‚àí1=Œºini\mathbb{Q}\_{0}\negthinspace:=\negthinspace\mathbb{P}\circ{X\_{0}}^{-1}={\mu\_{\rm ini}}, ‚ÑöT:=‚Ñô‚àòXT‚àí1=Œºtar\mathbb{Q}\_{T}\negthinspace:=\negthinspace\mathbb{P}\circ{X\_{T}}^{-1}\negthinspace=\negthinspace{\mu\_{\rm tar}}\negthinspace, and denote ùí´‚Ä≤:=ùí´‚Äã(Œºini,Œºtar)‚äÜùí´‚Äã(Œ©)\mathscr{P}^{\prime}:=\mathscr{P}({\mu\_{\rm ini}},{\mu\_{\rm tar}})\subseteq\mathscr{P}(\Omega) be all such ‚Äúpath measures‚Äù.

We note that if ‚Ñô=‚Ñô0\mathbb{P}=\mathbb{P}^{0} is Wiener measure and ‚Ñö\mathbb{Q} is equivalent to ‚Ñô0\mathbb{P}^{0}, then by the Cameron-Martin-Girsanov theorem, there exists Œ±‚ààùïÉùîΩ02‚Äã([0,T];‚Ñùd√ód)\alpha\in\mathbb{L}^{2}\_{\mathbb{F}^{0}}([0,T];\mathbb{R}^{d\times d}), such that

|  |  |  |
| --- | --- | --- |
|  | d‚Äã‚Ñöd‚Äã‚Ñô0|‚Ñ±t:=‚Ñ∞t‚Äã(Œ±)=exp‚Å°{‚à´0tŒ±s‚ÄãùëëWs‚àí12‚Äã‚à´0t|Œ±s|2‚Äãùëës},t‚àà[0,T],\frac{d\mathbb{Q}}{d\mathbb{P}^{0}}\Big|\_{{\cal F}\_{t}}:=\mathscr{E}\_{t}(\alpha)=\exp\Big\{\int\_{0}^{t}\alpha\_{s}dW\_{s}-\frac{1}{2}\int\_{0}^{t}|\alpha\_{s}|^{2}ds\Big\},\quad t\in[0,T], |  |

is a ‚Ñô0\mathbb{P}^{0}-martingale, and W~t:=Wt‚àí‚à´0tŒ±s‚Äãùëës\widetilde{W}\_{t}:=W\_{t}-\int\_{0}^{t}\alpha\_{s}ds, t‚àà[0,T]t\in[0,T], is a ‚Ñö\mathbb{Q}-Brownian motion. It can then be easily calculated that

|  |  |  |  |
| --- | --- | --- | --- |
|  | DKL‚Äã(‚Ñö‚à•‚Ñô0)=12‚Äãùîº‚Ñö‚Äã[‚à´0T|Œ±t|2‚Äãùëët].\displaystyle D\_{\rm KL}(\mathbb{Q}\|\mathbb{P}^{0})=\frac{1}{2}\mathbb{E}^{\mathbb{Q}}\Big[\int\_{0}^{T}|\alpha\_{t}|^{2}dt\Big]. |  | (2.2) |

#### Schr√∂dinger Bridge and Related Control Problem.

In light of ([2.2](https://arxiv.org/html/2510.11829v1#S2.E2 "In 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")), one can easily associate a SBP to a stochastic control problem (see, e.g., [[14](https://arxiv.org/html/2510.11829v1#bib.bib14), ¬ß4.4], [[17](https://arxiv.org/html/2510.11829v1#bib.bib17), ¬ß1] and [[40](https://arxiv.org/html/2510.11829v1#bib.bib40), p36]). Consider, for example, a standard SDE
on (Œ©,‚Ñ±,‚Ñô0)(\Omega,{\cal F},\mathbb{P}^{0}):

|  |  |  |  |
| --- | --- | --- | --- |
|  | d‚ÄãXt=b‚Äã(t,Xt)‚Äãd‚Äãt+d‚ÄãWt,X0‚àºŒºini.\displaystyle dX\_{t}=b(t,X\_{t})dt+dW\_{t},\quad X\_{0}\sim{\mu\_{\rm ini}}. |  | (2.3) |

Denote ‚Ñô=‚ÑôX:=‚Ñô0‚àòX‚àí1‚ààùí´‚Äã(Œ©)\mathbb{P}=\mathbb{P}\_{X}:=\mathbb{P}^{0}\circ X^{-1}\in\mathscr{P}(\Omega).
Then we consider the following SBP:

|  |  |  |  |
| --- | --- | --- | --- |
|  | V‚Äã(Œºini,Œºtar):=inf‚Ñö‚ààùí´‚Äã(Œºini,Œºtar)DKL‚Äã(‚Ñö‚à•‚Ñô).V({\mu\_{\rm ini}},{\mu\_{\rm tar}}):=\inf\_{\mathbb{Q}\in\mathscr{P}({\mu\_{\rm ini}},{\mu\_{\rm tar}})}D\_{\rm KL}(\mathbb{Q}\|\mathbb{P}). |  | (2.4) |

Similar to ([2.2](https://arxiv.org/html/2510.11829v1#S2.E2 "In 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")), we can recast ([2.4](https://arxiv.org/html/2510.11829v1#S2.E4 "In Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) as the following stochastic control problem:

|  |  |  |  |
| --- | --- | --- | --- |
|  | V‚Äã(Œºini,Œºtar)=infŒ±‚ààùíúJ‚Äã(Œ±)=infŒ±‚ààùíúùîº‚Ñö‚Äã[12‚Äã‚à´0T|Œ±s|2‚Äãùëës],\displaystyle V({\mu\_{\rm ini}},{\mu\_{\rm tar}})=\inf\_{\alpha\in\mathcal{A}}J(\alpha)=\inf\_{\alpha\in\mathcal{A}}\mathbb{E}^{\mathbb{Q}}\Big[\frac{1}{2}\int\_{0}^{T}|\alpha\_{s}|^{2}ds\Big], |  | (2.5) |

where ‚Ñö‚ààùí´‚Äã(Œ©)\mathbb{Q}\in\mathscr{P}(\Omega) is such that d‚Äã‚Ñöd‚Äã‚Ñô=‚Ñ∞‚Äã(Œ±)\frac{d\mathbb{Q}}{d\mathbb{P}}=\mathscr{E}(\alpha) for some Œ±‚ààùíú‚äÜùïÉùîΩ02‚Äã([0,T])\alpha\in\mathcal{A}\subseteq\mathbb{L}^{2}\_{\mathbb{F}^{0}}([0,T]), under which the underlying controlled dynamics takes the form:

|  |  |  |  |
| --- | --- | --- | --- |
|  | d‚ÄãXtŒ±=[b‚Äã(t,XtŒ±)+Œ±t]‚Äãd‚Äãt+d‚ÄãW~t,‚Ñö‚àò(X0Œ±)‚àí1=Œºini,‚Ñö‚àò(XTŒ±)‚àí1=Œºtar,\displaystyle dX^{\alpha}\_{t}=[b(t,X^{\alpha}\_{t})+\alpha\_{t}]dt+d\widetilde{W}\_{t},\quad\mathbb{Q}\circ(X\_{0}^{\alpha})^{-1}={\mu\_{\rm ini}},\quad\mathbb{Q}\circ(X\_{T}^{\alpha})^{-1}={\mu\_{\rm tar}}, |  | (2.6) |

where W~\widetilde{W} is a ‚Ñö\mathbb{Q}-Brownian motion.

###### Remark 2.1 (Subtlety in formulating the McKean-Vlasov version of the problem).

It is rather tempting to apply the idea above to the so-called McKean-Vlasov SBP (MVSBP). Consider, for example,
the following McKean-Vlasov SDE on (Œ©,‚Ñ±,‚Ñô0)(\Omega,{\cal F},\mathbb{P}^{0}):

|  |  |  |
| --- | --- | --- |
|  | d‚ÄãXt=b‚Äã(t,Xt,‚Ñôt)‚Äãd‚Äãt+d‚ÄãWt,X0‚àºŒºini.\displaystyle dX\_{t}=b(t,X\_{t},\mathbb{P}\_{t})dt+dW\_{t},\quad X\_{0}\sim{\mu\_{\rm ini}}. |  |

where, again, we denote ‚Ñô=‚Ñô0‚àòX‚àí1‚ààùí´2‚Äã(Œ©)\mathbb{P}=\mathbb{P}^{0}\circ X^{-1}\in\mathscr{P}\_{2}(\Omega), and ‚Ñôt:=‚Ñô0‚àòXt‚àí1‚ààùí´2‚Äã(‚Ñùd)\mathbb{P}\_{t}:=\mathbb{P}^{0}\circ X^{-1}\_{t}\in\mathscr{P}\_{2}(\mathbb{R}^{d}), t‚àà[0,T]t\in[0,T].
Similar to ([2.4](https://arxiv.org/html/2510.11829v1#S2.E4 "In Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) we can define an SBP, and let us
refer to it as an MVSBP. Again, by ([2.2](https://arxiv.org/html/2510.11829v1#S2.E2 "In 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")), we can recast such MVSBP as the following (weak form) stochastic control problem:

|  |  |  |  |
| --- | --- | --- | --- |
|  | V‚Äã(Œºini,Œºtar):=infŒ±ùîº‚Ñö‚Äã[‚à´0T12‚Äã|Œ±t|2‚Äãd‚Å°t],\displaystyle V({\mu\_{\rm ini}},{\mu\_{\rm tar}}):=\inf\_{\alpha}\mathbb{E}^{\mathbb{Q}}\Big[\int\_{0}^{T}\frac{1}{2}|\alpha\_{t}|^{2}\operatorname{{\rm d}}t\Big], |  | (2.7) |

where ‚Ñö‚ààùí´‚Äã(Œ©)\mathbb{Q}\in\mathscr{P}(\Omega) is such that the underlying controlled process X=XŒ±X=X^{\alpha} satisfies, under ‚Ñö\mathbb{Q}:

|  |  |  |  |
| --- | --- | --- | --- |
|  | d‚ÄãXt=[Œ±t+b‚Äã(t,Xt,‚Ñôt)]‚Äãd‚Äãt+d‚ÄãW~t,X0‚àºŒºini,XT‚àºŒºtar,\displaystyle dX\_{t}=[\alpha\_{t}+b(t,X\_{t},\mathbb{P}\_{t})]dt+d\widetilde{W}\_{t},\quad X\_{0}\sim{\mu\_{\rm ini}},\quad X\_{T}\sim{\mu\_{\rm tar}}, |  | (2.8) |

where W~\widetilde{W} is a ‚Ñö\mathbb{Q}-Brownian motion, and we can assume that Œ±‚ààùïÉùîΩ02‚Äã([0,T])\alpha\in\mathbb{L}^{2}\_{\mathbb{F}^{0}}([0,T]). However, by looking at ([2.8](https://arxiv.org/html/2510.11829v1#S2.E8 "In Remark 2.1 (Subtlety in formulating the McKean-Vlasov version of the problem). ‚Ä£ Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) more closely we see that the problem ([2.7](https://arxiv.org/html/2510.11829v1#S2.E7 "In Remark 2.1 (Subtlety in formulating the McKean-Vlasov version of the problem). ‚Ä£ Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) and ([2.8](https://arxiv.org/html/2510.11829v1#S2.E8 "In Remark 2.1 (Subtlety in formulating the McKean-Vlasov version of the problem). ‚Ä£ Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) do not form a McKean-Vlasov control problem, since ‚Ñôt‚â†‚Ñö‚àò(Xt)‚àí1\mathbb{P}\_{t}\neq\mathbb{Q}\circ(X\_{t})^{-1}(!). Therefore, an MVSBP should be formulated more carefully so as to connect to an McKean-Vlasov stochastic control problem.
‚àé

Ideally, the optimal solution to the Schr√∂dinger bridge problem ([2.5](https://arxiv.org/html/2510.11829v1#S2.E5 "In Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday"))-([2.6](https://arxiv.org/html/2510.11829v1#S2.E6 "In Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) provides a transport map from the initial distribution Œºini{\mu\_{\rm ini}} to the target distribution Œºtar{\mu\_{\rm tar}}. This transport is interpolated by a diffusion process that most closely resembles the canonical Brownian motion in the space of path measures. However, designing training algorithms to (approximately) learn the optimal solution to ([2.5](https://arxiv.org/html/2510.11829v1#S2.E5 "In Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday"))‚Äì([2.6](https://arxiv.org/html/2510.11829v1#S2.E6 "In Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) typically involves an iterative scheme that alternately relaxes the initial and terminal constraints [[20](https://arxiv.org/html/2510.11829v1#bib.bib20), [64](https://arxiv.org/html/2510.11829v1#bib.bib64), [66](https://arxiv.org/html/2510.11829v1#bib.bib66), [54](https://arxiv.org/html/2510.11829v1#bib.bib54)], and whose convergence rate and computational complexity in high-dimensional settings remain unclear.

While there could be techniques in stochastic control theory to deal with such a constraint, we shall follow the idea of [[35](https://arxiv.org/html/2510.11829v1#bib.bib35)], by approximating the original control problem by a family of unconstrained
McKean-Vlasov stochastic control problems with terminal penalties. More precisely, we shall allow the law of XTŒ±X^{\alpha}\_{T} in ([2.6](https://arxiv.org/html/2510.11829v1#S2.E6 "In Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) to be different from Œºtar{\mu\_{\rm tar}}, but add a corresponding penalty function to it in the cost functional J‚Äã(‚ãÖ)J(\cdot).

To this end, let us still denote ùíú‚äÜùí´‚Äã(Œ©){\cal A}\subseteq\mathscr{P}(\Omega) to be the set of all ‚Ñö‚ààùí´‚Äã(Œ©)\mathbb{Q}\in\mathscr{P}(\Omega) such that

(i) d‚Äã‚Ñöd‚Äã‚Ñô|‚Ñ±T=‚Ñ∞‚Äã(Œ±)=exp‚Å°{‚à´0TŒ±s‚ÄãùëëWs‚àí12‚Äã‚à´0T|Œ±s|2‚Äãùëës}\frac{d\mathbb{Q}}{d\mathbb{P}}\Big|\_{{\cal F}\_{T}}=\mathscr{E}(\alpha)=\exp\big\{\int\_{0}^{T}\alpha\_{s}dW\_{s}-\frac{1}{2}\int\_{0}^{T}|\alpha\_{s}|^{2}ds\big\}, Œ±‚ààùïÉùîΩ02‚Äã([0,T];‚Ñùd√ód)\alpha\in\mathbb{L}^{2}\_{\mathbb{F}^{0}}([0,T];\mathbb{R}^{d\times d});

(ii) Under ‚Ñö\mathbb{Q}, the underlying state process XX follows the dynamics:

|  |  |  |  |
| --- | --- | --- | --- |
|  | d‚ÄãXtŒ±=[b‚Äã(t,XtŒ±)+œÉ‚Äã(t)‚ÄãŒ±t]‚Äãd‚Äãt+œÉ‚Äã(t)‚Äãd‚ÄãW~t,‚Ñö‚àò(X0Œ±)‚àí1=Œºini,\displaystyle dX^{\alpha}\_{t}=[b(t,X^{\alpha}\_{t})+\sigma(t)\alpha\_{t}]dt+\sigma(t)d\widetilde{W}\_{t},\quad\mathbb{Q}\circ(X\_{0}^{\alpha})^{-1}={\mu\_{\rm ini}}, |  | (2.9) |

where W~\widetilde{W} is a ‚Ñö\mathbb{Q}-Brownian motion.

Throughout this paper, we shall make the following Standing Assumption on the coefficients bb and œÉ\sigma.

###### Assumption 2.2.

The coefficients b:[0,T]√ó‚Ñùd‚Ü¶‚Ñùdb:[0,T]\times\mathbb{R}^{d}\mapsto\mathbb{R}^{d} and œÉ:[0,T]‚Ü¶‚Ñùd√ód\sigma:[0,T]\mapsto\mathbb{R}^{d\times d} are given deterministic continuous functions, such that there exists L>0L>0, it holds that

|  |  |  |
| --- | --- | --- |
|  | |b‚Äã(t,x)‚àíb‚Äã(t,y)|‚â§L‚Äã|x‚àíy|,t‚àà[0,T],x,y‚àà‚Ñùd.|b(t,x)-b(t,y)|\leq L|x-y|,\qquad t\in[0,T],~x,y\in\mathbb{R}^{d}. |  |

Clearly, under Assumption [2.2](https://arxiv.org/html/2510.11829v1#S2.ThmTheorem2 "Assumption 2.2. ‚Ä£ Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday"), the SDE ([2.9](https://arxiv.org/html/2510.11829v1#S2.E9 "In Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) has a unique strong solution XŒ±X^{\alpha} on (Œ©,‚Ñ±,‚Ñô0)(\Omega,{\cal F},\mathbb{P}^{0}) for any given Œ±‚ààùïÉùîΩ01‚Äã([0,T];‚Ñùd√ód)\alpha\in\mathbb{L}^{1}\_{\mathbb{F}^{0}}([0,T];\mathbb{R}^{d\times d}) (see [[50](https://arxiv.org/html/2510.11829v1#bib.bib50), [69](https://arxiv.org/html/2510.11829v1#bib.bib69)]). We shall often identify ‚Ñö‚ààùíú\mathbb{Q}\in{\cal A} with its associated process Œ±\alpha, and denote ‚Ñö‚àºŒ±\mathbb{Q}\sim\alpha and Œ±‚ààùíú\alpha\in{\cal A} when the context is clear. The key element of the soft-constrained Schr√∂dinger Bridge Problem is the following penalty function.

###### Definition 2.3.

A smooth function G‚Äã(‚ãÖ)=G‚Äã(‚ãÖ;Œºtar):ùí´2‚Äã(‚Ñùd)‚Üí[0,‚àû)G(\cdot)=G(\cdot;{\mu\_{\rm tar}}):\mathscr{P}\_{2}(\mathbb{R}^{d})\to[0,\infty) is called a smooth penalty function associated to Œºtar‚ààùí´2‚Äã(‚Ñùd){\mu\_{\rm tar}}\in\mathscr{P}\_{2}(\mathbb{R}^{d}) if:
G‚Äã(Œº;Œºtar)=0G(\mu;{\mu\_{\rm tar}})=0 if and only if Œº=Œºtar\mu={\mu\_{\rm tar}}.
‚àé

Now let us introduce the following family of McKean-Vlasov-type stochastic control problems:

###### Problem 2.4 (Soft-constrained Schr√∂dinger Bridge Problem (SCSBP)).

For k‚àà‚Ñïk\in\mathbb{N}, find Œ±k‚ààùíú\alpha^{k}\in{\cal A} such that Vk:=Jk‚Äã(Œ±^k)=infŒ±‚ààùíúJk‚Äã(Œ±)V^{k}:=J^{k}(\widehat{\alpha}^{k})=\inf\_{\alpha\in{\cal A}}J^{k}(\alpha), where

|  |  |  |  |
| --- | --- | --- | --- |
|  | Jk‚Äã(Œ±)=ùîº‚Ñö‚Äã[12‚Äã‚à´0T|Œ±s|2‚Äãùëës+k‚ÄãG‚Äã(‚ÑöXTŒ±)],\displaystyle J^{k}(\alpha)=\mathbb{E}^{\mathbb{Q}}\left[\frac{1}{2}\int\_{0}^{T}|\alpha\_{s}|^{2}ds+kG(\mathbb{Q}\_{X^{\alpha}\_{T}})\right], |  | (2.10) |

and G‚Äã(‚ãÖ)=G‚Äã(‚ãÖ;Œºtar)G(\cdot)=G(\cdot;{\mu\_{\rm tar}}) is the given penalty function satisfying Definition [2.3](https://arxiv.org/html/2510.11829v1#S2.ThmTheorem3 "Definition 2.3. ‚Ä£ Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") and ‚Ñö‚àºŒ±\mathbb{Q}\sim\alpha.
‚àé

#### Applications in Generative AI.

We remark that the SCSBP Problem [2.4](https://arxiv.org/html/2510.11829v1#S2.ThmTheorem4 "Problem 2.4 (Soft-constrained Schr√∂dinger Bridge Problem (SCSBP)). ‚Ä£ Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")
offers a general framework that can be applied to address multiple problems in generative AI. We briefly mention a few motivational examples.

###### Example 2.5 (Data generation).

The goal of generative AI is to train a data generation procedure using a finite number of iid. data samples {x1,‚ãØ,xN}\{x\_{1},\cdots,x\_{N}\} under a (unknown) target distribution Œºtar{\mu\_{\rm tar}}, in order to simulate unlimited number of data samples whose underlying distribution is close to Œºtar{\mu\_{\rm tar}} [[58](https://arxiv.org/html/2510.11829v1#bib.bib58), [36](https://arxiv.org/html/2510.11829v1#bib.bib36), [33](https://arxiv.org/html/2510.11829v1#bib.bib33)].

To cast this problem into our framework, we can take, for example, Œºini=ùí©‚Äã(0,I){\mu\_{\rm ini}}=\mathcal{N}(0,I) and Œºtar=pdata{\mu\_{\rm tar}}=p\_{\rm data} in the theoretical framework (or Œºtar=1N‚Äã‚àëi=1NŒ¥xi{\mu\_{\rm tar}}=\frac{1}{N}\sum\_{i=1}^{N}\delta\_{x\_{i}} in the practical implementation). Then the optimal control Œ±‚àó\alpha^{\*} of SCSBP leads to a controlled process (XtŒ±‚àó)0‚â§t‚â§T(X\_{t}^{\alpha^{\*}})\_{0\leq t\leq T} that simulates the data output XTŒ±‚àóX\_{T}^{\alpha^{\*}}. Our key results (see Theorem [4.1](https://arxiv.org/html/2510.11829v1#S4.ThmTheorem1 "Theorem 4.1. ‚Ä£ 4.1 The Convergence of Optimal Policies ‚Ä£ 4 Convergence Results under Delta Initial Distribution ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") and Theorem [6.4](https://arxiv.org/html/2510.11829v1#S6.ThmTheorem4 "Theorem 6.4. ‚Ä£ 6 Existence of optimal control and convergence for general ùúá·µ¢‚Çô·µ¢ ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") below) show that the terminal measure ‚ÑöXTŒ±‚àó\mathbb{Q}\_{X\_{T}^{\alpha^{\*}}} is close to Œºtar{\mu\_{\rm tar}}, when kk is sufficiently large.
‚àé

###### Example 2.6 (Fine-tuning under a reward signal).

Fine-tuning a diffusion model means taking a pre-trained model and training it further on a smaller, task-specific dataset so it learns to generate outputs more suited to that new context [[60](https://arxiv.org/html/2510.11829v1#bib.bib60), [70](https://arxiv.org/html/2510.11829v1#bib.bib70), [62](https://arxiv.org/html/2510.11829v1#bib.bib62), [34](https://arxiv.org/html/2510.11829v1#bib.bib34)]. For example, a diffusion model trained on general images can be fine-tuned to generate a specific style (evaluated via a reward function). This process updates the model‚Äôs parameters just enough to adapt to the new data, without starting training from scratch.

In terms of our framework, we can consider ([2.3](https://arxiv.org/html/2510.11829v1#S2.E3 "In Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) as a pre-trained model with the drift b‚Äã(t,x):=sŒ∏^‚Äã(t,x)b(t,x):=s\_{\widehat{\theta}}(t,x) being a well-trained score function, and Œ∏^\widehat{\theta} is the trained parameter. Note that, as the result of pre-training, the output measure ‚ÑöXT\mathbb{Q}\_{X\_{T}} is sufficiently close to some data distribution Œºtar{\mu\_{\rm tar}}. We then introduce a fine-tuning procedure through a reference measure prefp\_{\rm ref} with density exp‚Å°(r‚Äã(x))‚à´‚Ñùdexp‚Å°(r‚Äã(x))‚Äãùëëx\frac{\exp(r(x))}{\int\_{\mathbb{R}^{d}}\exp(r(x))dx}, where r:‚Ñùd‚Üí‚Ñùr:\mathbb{R}^{d}\rightarrow\mathbb{R} is a given reward function satisfying ‚à´‚Ñùdexp‚Å°(r‚Äã(x))‚Äãùëëx<‚àû\int\_{\mathbb{R}^{d}}\exp(r(x))dx<\infty. Now replacing Œºtar=pref{\mu\_{\rm tar}}=p\_{\rm ref}, the optimal control Œ±‚àó\alpha^{\*} of the corresponding SCSBP can then serve as the fine-tuning score function; and consequently, the new drift term b‚Äã(t,XtŒ±‚Å£‚àó)+œÉ‚Äã(t)‚ÄãŒ±‚àób(t,X^{\alpha\*}\_{t})+\sigma(t)\alpha^{\*} acts as a combined score function.

Clearly, in this application the penalty parameter kk should not be chosen too large; otherwise, the effect of the preference function may dominate the fidelity to the original data distribution. With an appropriately selected kk, the resulting measure ‚ÑöXTŒ±‚àó\mathbb{Q}\_{X^{\alpha^{\*}}\_{T}} not only reflects pdatap\_{\rm data} but also integrates the reward function rr. In contrast, we remark that the classic SBP ([2.1](https://arxiv.org/html/2510.11829v1#S2.E1 "In 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) is not capable of handling this application as it has a pre-fixed target distribution.
‚àé

###### Example 2.7 (Transfer learning).

Transfer learning is a machine learning approach where knowledge gained from a ‚Äúsource task" is reused to improve learning in a related but different ‚Äútarget task" [[9](https://arxiv.org/html/2510.11829v1#bib.bib9), [61](https://arxiv.org/html/2510.11829v1#bib.bib61), [48](https://arxiv.org/html/2510.11829v1#bib.bib48)]. In what follows we shall consider transfer learning in the context of data generation.

Let us consider a source task (Ysou)(Y\_{\rm sou}), characterized by a distribution psoup\_{\rm sou}, and a target task (Ytar)(Y\_{\rm tar}) with distribution ptarp\_{\rm tar}. Typically, psoup\_{\rm sou} and ptarp\_{\rm tar} are assumed to be close under a certain divergence or distance function G‚Äã(ptar;psou)G(p\_{\rm tar};p\_{\rm sou}) (assuming G‚â•0G\geq 0), such as the Wasserstein distance [[48](https://arxiv.org/html/2510.11829v1#bib.bib48)].

To fit the transfer learning into our framework, we can take Œºini=psou{\mu\_{\rm ini}}=p\_{\rm sou}, Œºtar=ptar{\mu\_{\rm tar}}=p\_{\rm tar}, and set b‚â°0b\equiv 0 for simplicity. In this case, if we choose Œ±=0\alpha=0, and X0‚àºpsouX\_{0}\sim p\_{\rm sou}, then XT0=X0+WT‚àºpsou‚àóùí©‚Äã(0,T‚ÄãùïÄd)X^{0}\_{T}=X\_{0}+W\_{T}\sim p\_{\rm sou}\*\mathcal{N}(0,T\mathbb{I}\_{d}), where ùí©‚Äã(0,T‚ÄãùïÄd)=‚Ñô0‚àòWT‚àí1\mathcal{N}(0,T{\mathbb{I}\_{d}})=\mathbb{P}^{0}\circ W\_{T}^{-1} and ùïÄd\mathbb{I}\_{d} denotes the d√ódd\times d identity matrix. Thus, denoting the optimal control by Œ±^\widehat{\alpha} and noting that Œ±‚â°0\alpha\equiv 0 is sub-optimal, we must have

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Ñö‚Äã[12‚Äã‚à´0T|Œ±^s|2‚Äãùëës]‚â§ùîº‚Ñö‚Äã[12‚Äã‚à´0T|Œ±^s|2‚Äãùëës+k‚ÄãG‚Äã(‚ÑöXTŒ±^)]‚â§k‚ÄãG‚Äã(psou‚àóùí©‚Äã(0,T‚ÄãùïÄd);ptar).\displaystyle\mathbb{E}^{\mathbb{Q}}\left[\frac{1}{2}\int\_{0}^{T}|\widehat{\alpha}\_{s}|^{2}ds\right]\leq\mathbb{E}^{\mathbb{Q}}\left[\frac{1}{2}\int\_{0}^{T}|\widehat{\alpha}\_{s}|^{2}ds+kG(\mathbb{Q}\_{X^{\widehat{\alpha}}\_{T}})\right]\leq kG(p\_{\rm sou}\*\mathcal{N}(0,T{\mathbb{I}\_{d}});p\_{\rm tar}). |  |

This implies that the optimal control Œ±^\widehat{\alpha} has a small L2L^{2}-norm, indicating only minor adjustments are required during sampling‚Äîprovided kk is not too large.
‚àé

## 3 Existence of Optimal Policies for SCSBP‚Äôs

In this section we study the stochastic control problem ([2.9](https://arxiv.org/html/2510.11829v1#S2.E9 "In Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday"))-([2.10](https://arxiv.org/html/2510.11829v1#S2.E10 "In Problem 2.4 (Soft-constrained Schr√∂dinger Bridge Problem (SCSBP)). ‚Ä£ Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) and the associated soft-constrained SBP. In particular, we shall prove that the optimal control for each k‚àà‚Ñïk\in\mathbb{N} exists and in next section we will show that these optimal policies will converge to the solution of the original SBP, with a linear rate of convergence. We shall assume that the target distribution for the SCSBP has a
density ftar‚ààùïÉ1‚Äã(‚Ñùd)f\_{\rm tar}\in\mathbb{L}^{1}(\mathbb{R}^{d}). Also, we shall assume
œÉ‚Äã(‚ãÖ)=Id\sigma(\cdot)=I\_{d}, that is, in what follows we assume that the underlying diffusion takes the form:

|  |  |  |  |
| --- | --- | --- | --- |
|  | d‚ÄãXt=b‚Äã(t,Xt)‚Äãd‚Äãt+d‚ÄãWt,X0‚àºŒºini,t‚àà[0,T],\displaystyle dX\_{t}=b(t,X\_{t})dt+dW\_{t},\quad X\_{0}\sim{\mu\_{\rm ini}},\quad t\in[0,T], |  | (3.1) |

where WW is the canonical Brownian motion under ‚Ñô0\mathbb{P}^{0}. Let p‚Äã(‚ãÖ,‚ãÖ;‚ãÖ,‚ãÖ)p(\cdot,\cdot;\cdot,\cdot) denote the transition density of the solution XX, so that ‚Ñô0‚Äã{Xs‚ààd‚Äãz|Xt=x}=p‚Äã(s,z;t,x)‚Äãd‚Äãz\mathbb{P}^{0}\{X\_{s}\in dz|X\_{t}=x\}=p(s,z;t,x)dz, 0‚â§t<s‚â§T0\leq t<s\leq T, z,x‚àà‚Ñùdz,x\in\mathbb{R}^{d}. Then, it is well known that p‚Äã(‚ãÖ,‚ãÖ;‚ãÖ,‚ãÖ)p(\cdot,\cdot;\cdot,\cdot) is the fundamental solution to Kolmogorov backward (parabolic) PDE, and under mild conditions (see, e.g., [[3](https://arxiv.org/html/2510.11829v1#bib.bib3)]), there exist c1c\_{1}, c2c\_{2}, Œª\lambda, Œõ>0\Lambda>0, it holds that

|  |  |  |  |
| --- | --- | --- | --- |
|  | c1‚Äã(s‚àít)‚àíd2‚Äãe‚àíŒª‚Äã|z‚àíx|2s‚àít<p‚Äã(s,z;t,x)<c2‚Äã(s‚àít)‚àíd2‚Äãe‚àíŒõ‚Äã|z‚àíx|24‚Äã(s‚àít).c\_{1}(s-t)^{-\frac{d}{2}}e^{-\frac{\lambda|z-x|^{2}}{s-t}}<p(s,z;t,x)<c\_{2}(s-t)^{-\frac{d}{2}}e^{-\frac{\Lambda|z-x|^{2}}{4(s-t)}}. |  | (3.2) |

Keeping the original SBP ([2.4](https://arxiv.org/html/2510.11829v1#S2.E4 "In Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) and ([2.5](https://arxiv.org/html/2510.11829v1#S2.E5 "In Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) associated with ([3.1](https://arxiv.org/html/2510.11829v1#S3.E1 "In 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) in mind, let us now recall the Problem [2.4](https://arxiv.org/html/2510.11829v1#S2.ThmTheorem4 "Problem 2.4 (Soft-constrained Schr√∂dinger Bridge Problem (SCSBP)). ‚Ä£ Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") and the cost functional Jk‚Äã(Œ±)J^{k}(\alpha) defined by ([2.10](https://arxiv.org/html/2510.11829v1#S2.E10 "In Problem 2.4 (Soft-constrained Schr√∂dinger Bridge Problem (SCSBP)). ‚Ä£ Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")). For notational simplicity in what follows we shall identify ‚Ñö‚ààùíú\mathbb{Q}\in{\cal A} by ‚Ñö=‚ÑôXŒ±=‚Ñô0‚àò(XŒ±)‚àí1\mathbb{Q}=\mathbb{P}\_{X^{\alpha}}=\mathbb{P}^{0}\circ(X^{\alpha})^{-1}. Clearly, we have ‚ÑôX0=‚ÑôX\mathbb{P}\_{X^{0}}=\mathbb{P}\_{X}, where XX solves ([3.1](https://arxiv.org/html/2510.11829v1#S3.E1 "In 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")). Furthermore, we shall denote ùîº‚Äã[‚ãÖ]=ùîº‚Ñô0‚Äã[‚ãÖ]\mathbb{E}[\cdot]=\mathbb{E}^{\mathbb{P}^{0}}[\cdot] when context is clear, and for each k‚àà‚Ñïk\in\mathbb{N} we can easily check that

|  |  |  |  |
| --- | --- | --- | --- |
|  | Jk‚Äã(Œ±)=ùîº‚Äã[12‚Äã‚à´0T|Œ±s|2‚Äãùëës+k‚ÄãG‚Äã(‚ÑôXTŒ±)]=DKL‚Äã(‚ÑôXŒ±‚à•‚ÑôX)+k‚ÄãG‚Äã(‚ÑôXTŒ±).\displaystyle J^{k}(\alpha)=\mathbb{E}\Big[\frac{1}{2}\int\_{0}^{T}|\alpha\_{s}|^{2}ds+kG(\mathbb{P}\_{X^{\alpha}\_{T}})\Big]=D\_{\rm KL}(\mathbb{P}\_{X^{\alpha}}\|\mathbb{P}\_{X})+kG(\mathbb{P}\_{X^{\alpha}\_{T}}). |  | (3.3) |

Now let us define, for each k‚àà‚Ñïk\in\mathbb{N}, a mapping Dk‚Äã(‚ãÖ):ùí´2‚Äã(‚Ñùd)‚Üí‚ÑùD\_{k}(\cdot):\mathscr{P}\_{2}(\mathbb{R}^{d})\to\mathbb{R} by

|  |  |  |
| --- | --- | --- |
|  | Dk‚Äã(Œº)=DKL‚Äã(Œº‚à•‚ÑôXT)+k‚ÄãG‚Äã(Œº),D\_{k}(\mu)=D\_{\rm KL}(\mu\|\mathbb{P}\_{X\_{T}})+kG(\mu), |  |

and note that DKL‚Äã(‚ÑôXŒ±‚à•‚ÑôX)‚â•DKL‚Äã(‚ÑôXTŒ±‚à•‚ÑôXT)D\_{\rm KL}(\mathbb{P}\_{X^{\alpha}}\|\mathbb{P}\_{X})\geq D\_{\rm KL}(\mathbb{P}\_{X^{\alpha}\_{T}}\|\mathbb{P}\_{X\_{T}}),
we deduce from ([3.3](https://arxiv.org/html/2510.11829v1#S3.E3 "In 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) that

|  |  |  |  |
| --- | --- | --- | --- |
|  | Jk‚Äã(Œ±)‚â•DKL‚Äã(‚ÑôXTŒ±‚à•‚ÑôXT)+k‚ÄãG‚Äã(‚ÑôXTŒ±)=Dk‚Äã(‚ÑôXTŒ±).\displaystyle J^{k}(\alpha)\geq D\_{\rm KL}(\mathbb{P}\_{X^{\alpha}\_{T}}\|\mathbb{P}\_{X\_{T}})+kG(\mathbb{P}\_{X^{\alpha}\_{T}})=D\_{k}(\mathbb{P}\_{X^{\alpha}\_{T}}). |  | (3.4) |

If Œ±^\widehat{\alpha} is the optimal control corresponding to the original SBP, that is, ‚ÑôXTŒ±^=Œºtar\mathbb{P}\_{X^{\widehat{\alpha}}\_{T}}={\mu\_{\rm tar}}, then by definition of the penalty function G‚Äã(‚ãÖ)G(\cdot), we should have G‚Äã(Œºtar)=0G({\mu\_{\rm tar}})=0, and therefore,

|  |  |  |
| --- | --- | --- |
|  | Dk‚Äã(‚ÑôXTŒ±^)=Dk‚Äã(Œºtar)=DKL‚Äã(Œºtar‚à•‚ÑôXT)+k‚ÄãG‚Äã(Œºtar)=DKL‚Äã(Œºtar‚à•‚ÑôXT).\displaystyle D\_{k}(\mathbb{P}\_{X^{\widehat{\alpha}}\_{T}})=D\_{k}({\mu\_{\rm tar}})=D\_{\rm KL}({\mu\_{\rm tar}}\|\mathbb{P}\_{X\_{T}})+kG({\mu\_{\rm tar}})=D\_{\rm KL}({\mu\_{\rm tar}}\|\mathbb{P}\_{X\_{T}}). |  |

Throughout the rest of this section, we shall focus on the special case: Œºini=Œ¥x0{\mu\_{\rm ini}}=\delta\_{x\_{0}} for some x0‚àà‚Ñùdx\_{0}\in\mathbb{R}^{d}. The case with general initial distribution Œºini{\mu\_{\rm ini}} will be studied in Sections [5](https://arxiv.org/html/2510.11829v1#S5 "5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") and [6](https://arxiv.org/html/2510.11829v1#S6 "6 Existence of optimal control and convergence for general ùúá·µ¢‚Çô·µ¢ ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday"). We begin with the following well-known
result from [[18](https://arxiv.org/html/2510.11829v1#bib.bib18)], which will play an important role in our discussion.

###### Lemma 3.1 ( [[18](https://arxiv.org/html/2510.11829v1#bib.bib18), Theorem 3.1]).

Let XX be a weak solution to ([3.1](https://arxiv.org/html/2510.11829v1#S3.E1 "In 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) with X0=x0‚àà‚ÑùdX\_{0}=x\_{0}\in\mathbb{R}^{d} (i.e., Œºini=Œ¥x0{\mu\_{\rm ini}}=\delta\_{x\_{0}}). Assume that DKL‚Äã(Œºtar‚à•‚ÑôXT)<‚àûD\_{\rm KL}({\mu\_{\rm tar}}\|\mathbb{P}\_{X\_{T}})<\infty. Then, the optimal solution to the SBP ([2.4](https://arxiv.org/html/2510.11829v1#S2.E4 "In Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday"))-([2.5](https://arxiv.org/html/2510.11829v1#S2.E5 "In Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) is given by Œ±^t=‚àálog‚Å°h‚Äã(t,XtŒ±^)\widehat{\alpha}\_{t}=\nabla\log h(t,X^{\widehat{\alpha}}\_{t}), where

|  |  |  |  |
| --- | --- | --- | --- |
|  | h‚Äã(t,x)=‚à´‚Ñùdp‚Äã(T,z;t,x)‚Äãftar‚Äã(z)p‚Äã(T,z;0,x0)‚Äãùëëz:=ùîº‚Äã[ftar‚Äã(XT)p‚Äã(T,XT;0,x0)|Xt=x],\displaystyle h(t,x)=\int\_{\mathbb{R}^{d}}p(T,z;t,x)\frac{f\_{\rm tar}(z)}{p(T,z;0,x\_{0})}dz:=\mathbb{E}\Big[\frac{f\_{\rm tar}(X\_{T})}{p(T,X\_{T};0,x\_{0})}\Big|X\_{t}=x\Big], |  | (3.5) |

for (t,x)‚àà[0,T]√ó‚Ñùd(t,x)\in[0,T]\times\mathbb{R}^{d}.
‚àé

Next, we make the following assumptions on the penalty function GG:

###### Assumption 3.2.

(i) There exists some small constant Œµ>0\varepsilon>0 such that

|  |  |  |  |
| --- | --- | --- | --- |
|  | G‚Äã(Œº)‚Üí+‚àû,as‚Äã‚ÄñŒº‚Äñ2+Œµ‚Üí+‚àû.G(\mu)\rightarrow+\infty,\quad\mathrm{as}\,\,\|\mu\|^{2+\varepsilon}\rightarrow+\infty. |  | (3.6) |

where ‚ÄñŒº‚Äñp:=‚à´‚Ñùd|x|p‚ÄãŒº‚Äã(d‚Äãx)\|\mu\|^{p}:=\int\_{\mathbb{R}^{d}}|x|^{p}\mu(dx) for any p>0p>0.

(ii) There exist C,Œª>0C,\lambda>0, and a function œï:‚Ñùd‚Üí(0,1]\phi:\mathbb{R}^{d}\to(0,1] satisfying œï‚Äã(x)‚ÄãeŒª‚Äã|x‚àíx0|2‚â§C\phi(x)e^{\lambda|x-x\_{0}|^{2}}\leq C, such that for any Œº‚ààùí´2‚Äã(‚Ñùd)\mu\in\mathscr{P}\_{2}(\mathbb{R}^{d}) with density function fŒºf\_{\mu}, it holds that

|  |  |  |  |
| --- | --- | --- | --- |
|  | |fŒº‚Äã(x)‚àíftar‚Äã(x)|‚â§C‚Äãœï‚Äã(x)‚ÄãG‚Äã(Œº),x‚àà‚Ñùd.\displaystyle|f\_{\mu}(x)-f\_{\rm tar}(x)|\leq C\phi(x)G(\mu),\quad x\in\mathbb{R}^{d}. |  | (3.7) |

###### Remark 3.3.

(i) The function G‚Äã(Œº)G(\mu) on the right hand side of ([3.7](https://arxiv.org/html/2510.11829v1#S3.E7 "In Assumption 3.2. ‚Ä£ 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) should read |G‚Äã(Œº)‚àíG‚Äã(Œºtar)||G(\mu)-G({\mu\_{\rm tar}})|, as G‚Äã(Œºtar)=0G({\mu\_{\rm tar}})=0, which essentially states that if Œº\mu is close to Œºtar{\mu\_{\rm tar}} in terms of GG, then fŒºf\_{\mu} is close to ftarf\_{\rm tar} in ùïÉ1\mathbb{L}^{1}.

(ii) A slightly stronger consequence of ([3.7](https://arxiv.org/html/2510.11829v1#S3.E7 "In Assumption 3.2. ‚Ä£ 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) is the following. Recall the (generalized) Kantorovich and Rubinstein dual representation (cf. e.g., [[23](https://arxiv.org/html/2510.11829v1#bib.bib23)]): denoting Lip(1)(1) to be all Lipschitz functions œÜ:‚Ñùd‚Üí‚Ñù\varphi:\mathbb{R}^{d}\to\mathbb{R} with Lipschitz constant Lip‚â§œÜ1{}\_{\varphi}\leq 1 (hence |œÜ‚Äã(x)|‚â§C‚Äã(1+|x|)|\varphi(x)|\leq C(1+|x|)), then it holds that

|  |  |  |
| --- | --- | --- |
|  | W1‚Äã(Œº,Œºtar)=supœÜ‚ààL‚Äãi‚Äãp‚Äã(1){‚à´‚ÑùdœÜ‚Äã(x)‚Äã(fŒº‚Äã(x)‚àíftar‚Äã(x))‚Äãùëëx}‚â§C‚ÄãG‚Äã(Œº)‚Äã‚à´‚Ñùd(1+|x|)‚Äãœï‚Äã(x)‚Äãùëëx.\displaystyle W\_{1}(\mu,{\mu\_{\rm tar}})=\sup\_{\varphi\in Lip(1)}\left\{\int\_{\mathbb{R}^{d}}\varphi(x)(f\_{\mu}(x)-f\_{\rm tar}(x))dx\right\}\leq CG(\mu)\int\_{\mathbb{R}^{d}}(1+|x|)\phi(x)dx. |  |

This suggests that G‚Äã(Œº)‚àº0G(\mu)\sim 0 implies that Œº\mu is close to Œºtar{\mu\_{\rm tar}} in the sense of W1W\_{1}.
‚àé

Before we proceed, let us give two examples that justify Assumption [3.2](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem2 "Assumption 3.2. ‚Ä£ 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday").

###### Example 3.4.

We consider the class of Œº\mu and Œºtar{\mu\_{\rm tar}} such that

|  |  |  |
| --- | --- | --- |
|  | G‚Äã(Œº):=‚à´‚Ñù|x|p‚Äã|fŒº‚Äã(x)‚àíftar‚Äã(x)|‚Äãùëëx,G(\mu):=\int\_{\mathbb{R}}|x|^{p}|f\_{\mu}(x)-f\_{\rm tar}(x)|dx, |  |

is well-defined for a given pp.
Clearly, Definition [2.3](https://arxiv.org/html/2510.11829v1#S2.ThmTheorem3 "Definition 2.3. ‚Ä£ Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")-(i) and Assumption [3.2](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem2 "Assumption 3.2. ‚Ä£ 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")-(i) are satisfied when p>2p>2. Assumption [3.2](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem2 "Assumption 3.2. ‚Ä£ 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")-(i) holds when

|  |  |  |
| --- | --- | --- |
|  | œï‚Äã(x)‚â§|fŒº‚Äã(x)‚àíftar‚Äã(x)|c‚Äã‚à´‚Ñù|x|p‚Äã|fŒº‚Äã(x)‚àíftar‚Äã(x)|‚Äãùëëx,\displaystyle\phi(x)\leq\frac{|f\_{\mu}(x)-f\_{\rm tar}(x)|}{c\int\_{\mathbb{R}}|x|^{p}|f\_{\mu}(x)-f\_{\rm tar}(x)|dx}, |  |

for all Œº\mu in the collection one may consider.
‚àé

Another natural example of GG satisfying Assumption [3.2](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem2 "Assumption 3.2. ‚Ä£ 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") would be the Wasserstein distance or the KL divergence, augmented with a small ‚Äúguardrail‚Äù term that enforces the uniform (weighted) pointwise control in ([3.7](https://arxiv.org/html/2510.11829v1#S3.E7 "In Assumption 3.2. ‚Ä£ 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")). This guardrail can be taken as a weighted L‚àûL\_{\infty} norm, a H√∂lder ‚ÑÇŒ±\mathbb{C}^{\alpha} seminorm, or an RKHS norm (e.g., with kernel k‚Äã(x,y)=œï‚Äã(x)‚Äãœï‚Äã(y)‚ÄãŒ∫‚Äã(x‚àíy)k(x,y)=\phi(x)\phi(y)\kappa(x-y)). In next example we illustrate such a choice with the Wasserstein distance plus an L‚àûL\_{\infty} guardrail.

###### Example 3.5.

Consider the case that Œºtar‚ààùí´p‚Äã(‚Ñùd){\mu\_{\rm tar}}\in\mathscr{P}\_{p}(\mathbb{R}^{d}) with p>2p>2.
We define, for c>0c>0 and œï‚Äã(x)=exp‚Å°(‚àíŒª‚Äã|x‚àíx0|2)\phi(x)=\exp(-\lambda|x-x\_{0}|^{2}) with some x0‚àà‚Ñùdx\_{0}\in\mathbb{R}^{d},

|  |  |  |
| --- | --- | --- |
|  | G‚Äã(Œº):=W2‚Äã(Œº,Œºtar)+c‚Äã‚ÄñfŒº‚àíftarœï‚ÄñL‚àû.\displaystyle G(\mu):=W\_{2}(\mu,{\mu\_{\rm tar}})+c\Big\|\frac{f\_{\mu}-f\_{\rm tar}}{\phi}\Big\|\_{L^{\infty}}. |  |

Then, it is easy to check that

|  |  |  |
| --- | --- | --- |
|  | |fŒº‚Äã(x)‚àíftar‚Äã(x)|‚â§‚ÄñfŒº‚àíftarœï‚ÄñL‚àû‚Äãœï‚Äã(x)‚â§1c‚Äãœï‚Äã(x)‚ÄãG‚Äã(Œº).\displaystyle|f\_{\mu}(x)-f\_{\rm tar}(x)|\leq\Big\|\frac{f\_{\mu}-f\_{\rm tar}}{\phi}\Big\|\_{L^{\infty}}\phi(x)\leq\frac{1}{c}\phi(x)G(\mu). |  |

Thus ([3.7](https://arxiv.org/html/2510.11829v1#S3.E7 "In Assumption 3.2. ‚Ä£ 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) holds and œï‚Äã(x)‚ÄãeŒª‚Äã|x‚àíx0|2‚â§C\phi(x)e^{\lambda|x-x\_{0}|^{2}}\leq C holds with C=max‚Å°{1,1c}C=\max\{1,\frac{1}{c}\}.

Let {Œºn}n‚â•1‚äÇùí´2‚Äã(‚Ñùd)\{\mu\_{n}\}\_{n\geq 1}\subset\mathscr{P}\_{2}(\mathbb{R}^{d}) with ‚ÄñŒºn‚Äñ2+Œµ‚Üí‚àû\|\mu\_{n}\|^{2+\varepsilon}\rightarrow\infty. We claim that G‚Äã(Œºn)G(\mu\_{n}) must be unbounded. Indeed, suppose not. Then there exists M,M‚Ä≤>0M,M^{\prime}>0 such that
W22‚Äã(Œºn,Œºtar)‚â§MW\_{2}^{2}(\mu\_{n},{\mu\_{\rm tar}})\leq M and ‚Äñœï‚àí1‚Äã(fŒºn‚àíftar)‚ÄñL‚àû<M‚Ä≤\|\phi^{-1}(f\_{\mu\_{n}}-f\_{\rm tar})\|\_{L^{\infty}}<M^{\prime} for all n‚àà‚Ñï+n\in\mathbb{N}\_{+}. Hence fŒºn‚Äã(x)‚â§ftar‚Äã(x)+M‚Ä≤‚Äãœï‚Äã(x)f\_{\mu\_{n}}(x)\leq f\_{\rm tar}(x)+M^{\prime}\phi(x), x‚àà‚Ñùdx\in\mathbb{R}^{d}.
Integrating against |x|2+Œµ|x|^{2+\varepsilon} and using the facts that Œºtar‚ààùí´2+Œµ‚Äã(‚Ñùd){\mu\_{\rm tar}}\in\mathscr{P}\_{2+\varepsilon}(\mathbb{R}^{d}) with Œµ=p‚àí2>0\varepsilon=p-2>0 and ‚à´|x|2+Œµ‚Äãœï‚Äã(x)‚Äãùëëx<‚àû\int|x|^{2+\varepsilon}\phi(x)dx<\infty, we have

|  |  |  |
| --- | --- | --- |
|  | ‚ÄñŒºn‚Äñ2+Œµ‚â§‚ÄñŒºtar‚Äñ2+Œµ+M‚Ä≤‚Äã‚à´|x|2+Œµ‚Äãœï‚Äã(x)‚Äãùëëx<‚àû,n‚àà‚Ñï.\displaystyle\|\mu\_{n}\|^{2+\varepsilon}\leq\|{\mu\_{\rm tar}}\|^{2+\varepsilon}+M^{\prime}\int|x|^{2+\varepsilon}\phi(x)dx<\infty,\qquad n\in\mathbb{N}. |  |

This contradicts the fact that ‚ÄñŒºn‚Äñ2+Œµ‚Üí‚àû\|\mu\_{n}\|^{2+\varepsilon}\rightarrow\infty, proving the claim. Hence
([3.6](https://arxiv.org/html/2510.11829v1#S3.E6 "In Assumption 3.2. ‚Ä£ 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) holds.
‚àé

We are now ready to investigate the existence of optimal control of Problem [2.4](https://arxiv.org/html/2510.11829v1#S2.ThmTheorem4 "Problem 2.4 (Soft-constrained Schr√∂dinger Bridge Problem (SCSBP)). ‚Ä£ Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") for each k‚àà‚Ñïk\in\mathbb{N}, which would be essential for our approximation scheme. Recall that in the rest of the section we assume that Œºini=Œ¥x0{\mu\_{\rm ini}}=\delta\_{x\_{0}} for some x0‚àà‚Ñùdx\_{0}\in\mathbb{R}^{d}.
To begin with, we first claim that for each k‚àà‚Ñïk\in\mathbb{N}, there exists Œº^k‚ààùí´2‚Äã(‚Ñùd)\widehat{\mu}\_{k}\in\mathscr{P}\_{2}(\mathbb{R}^{d}) such that the static optimization problem on the measure space has a solution

|  |  |  |  |
| --- | --- | --- | --- |
|  | Dk‚Äã(Œº^k)=infŒº‚ààùí´2‚Äã(‚Ñùd)Dk‚Äã(Œº).\displaystyle D\_{k}(\widehat{\mu}\_{k})=\inf\_{\mu\in\mathscr{P}\_{2}(\mathbb{R}^{d})}D\_{k}(\mu). |  | (3.8) |

Indeed, let XX be the solution to uncontrolled SDE ([2.6](https://arxiv.org/html/2510.11829v1#S2.E6 "In Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")), and Œºtar{\mu\_{\rm tar}} be given such that DKL‚Äã(Œºtar‚à•‚ÑôXT)<‚àûD\_{\rm KL}({\mu\_{\rm tar}}\|\mathbb{P}\_{X\_{T}})<\infty. Since G‚Äã(Œºtar)=0G({\mu\_{\rm tar}})=0, we have

|  |  |  |
| --- | --- | --- |
|  | m:=Dk‚Äã(Œºtar)=DKL‚Äã(Œºtar‚à•‚ÑôXT)+k‚ÄãG‚Äã(Œºtar)=DKL‚Äã(Œºtar‚à•‚ÑôXT)<‚àû.\displaystyle m:=D\_{k}({\mu\_{\rm tar}})=D\_{\rm KL}({\mu\_{\rm tar}}\|\mathbb{P}\_{X\_{T}})+kG({\mu\_{\rm tar}})=D\_{\rm KL}({\mu\_{\rm tar}}\|\mathbb{P}\_{X\_{T}})<\infty. |  |

Next, let us define, for fixed k‚àà‚Ñïk\in\mathbb{N}, a set

|  |  |  |
| --- | --- | --- |
|  | ùíÆk:={Œº‚ààùí´2‚Äã(‚Ñùd):Dk‚Äã(Œº)‚â§m}.\displaystyle\mathcal{S}\_{k}:=\Big\{\mu\in\mathscr{P}\_{2}(\mathbb{R}^{d}):D\_{k}(\mu)\leq m\Big\}. |  |

Clearly, ùíÆk‚â†‚àÖ\mathcal{S}\_{k}\neq\emptyset since Œºtar‚ààùíÆk{\mu\_{\rm tar}}\in\mathcal{S}\_{k}, and by ([3.6](https://arxiv.org/html/2510.11829v1#S3.E6 "In Assumption 3.2. ‚Ä£ 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")), there exists Mk>0M\_{k}>0 such that
‚ÄñŒº‚Äñ2+Œµ‚â§Mk\|\mu\|^{2+\varepsilon}\leq M\_{k}, for all Œº‚ààùíÆk\mu\in\mathcal{S}\_{k}. Thus ùíÆk{\cal S}\_{k} is uniformly integrable in ùïÉ2\mathbb{L}^{2}. Now let {Œºk(i)}i=1‚àû‚äÇùí´2‚Äã(‚Ñùd)\{\mu\_{k}^{(i)}\}\_{i=1}^{\infty}\subset\mathscr{P}\_{2}(\mathbb{R}^{d}) be a minimizing sequence, namely,

|  |  |  |
| --- | --- | --- |
|  | limi‚Üí‚àûDk‚Äã(Œºk(i))=infŒº‚ààùí´2‚Äã(‚Ñùd)Dk‚Äã(Œº).\displaystyle\lim\_{i\rightarrow\infty}D\_{k}(\mu\_{k}^{(i)})=\inf\_{\mu\in\mathscr{P}\_{2}(\mathbb{R}^{d})}D\_{k}(\mu). |  |

Since infŒº‚ààùí´2‚Äã(‚Ñùd)Dk‚Äã(Œº)‚â§Dk‚Äã(Œºtar)=m\inf\_{\mu\in\mathscr{P}\_{2}(\mathbb{R}^{d})}D\_{k}(\mu)\leq D\_{k}({\mu\_{\rm tar}})=m, we may assume without loss of generality that

{Œºk(i)}i=1‚àû‚äÇùíÆk\{\mu\_{k}^{(i)}\}\_{i=1}^{\infty}\subset\mathcal{S}\_{k}. Since ùíÆk\mathcal{S}\_{k} is uniformly integrable and is tight
in ùí´2‚Äã(‚Ñùd)\mathscr{P}\_{2}(\mathbb{R}^{d}), there exists subsequence {Œºk(il)}l=1‚àû\{\mu\_{k}^{(i\_{l})}\}\_{l=1}^{\infty} such that
Œº^k:=liml‚Üí‚àûŒºk(il)‚ààùí´2‚Äã(‚Ñùd)\widehat{\mu}\_{k}:=\lim\_{l\rightarrow\infty}\mu\_{k}^{(i\_{l})}\in\mathscr{P}\_{2}(\mathbb{R}^{d})333This follows from the result on
Wasserstein distance vs. weak convergence (see, e.g., [[65](https://arxiv.org/html/2510.11829v1#bib.bib65), Theorem 7.12]), which states that Wp‚Äã(Œºk,Œº)‚Üí0W\_{p}(\mu\_{k},\mu)\to 0 if and only if Œºk‚ÜíŒº\mu\_{k}\to\mu weakly, and limR‚Üí‚àûlim¬Øk‚Üí‚àû‚à´{d‚Äã(x,x0)‚â•R}d‚Äã(x,x0)p‚ÄãŒºk‚Äã(d‚Äãx)=0\lim\_{R\to\infty}\mathop{\overline{\rm lim}}\_{k\to\infty}\int\_{\{d(x,x\_{0})\geq R\}}d(x,x\_{0})^{p}\mu\_{k}(dx)=0..
Since the mapping Œº‚Ü¶Dk‚Äã(Œº)\mu\mapsto D\_{k}(\mu) is continuous, we have

|  |  |  |
| --- | --- | --- |
|  | Dk‚Äã(Œº^k)=DKL‚Äã(Œº^k‚à•‚ÑôXT)+k‚ÄãG‚Äã(Œº^k)=infŒº‚ààùí´2‚Äã(‚Ñùd)Dk‚Äã(Œº),\displaystyle D\_{k}(\widehat{\mu}\_{k})=D\_{\rm KL}(\widehat{\mu}\_{k}\|\mathbb{P}\_{X\_{T}})+kG(\widehat{\mu}\_{k})=\inf\_{\mu\in\mathscr{P}\_{2}(\mathbb{R}^{d})}D\_{k}(\mu), |  |

proving the claim. Furthermore, if we denote the density of ‚ÑôXT\mathbb{P}\_{X\_{T}} by fXTf\_{X\_{T}}, and note that DKL‚Äã(Œº^k;‚ÑôXT)‚â§Dk‚Äã(Œº^k)‚â§m<‚àûD\_{\rm KL}(\widehat{\mu}\_{k};\mathbb{P}\_{X\_{T}})\leq D\_{k}(\widehat{\mu}\_{k})\leq m<\infty, we know that d‚ÄãŒº^kd‚Äã‚ÑôXT\frac{d\widehat{\mu}\_{k}}{d\mathbb{P}\_{X\_{T}}} exists and

|  |  |  |
| --- | --- | --- |
|  | d‚ÄãŒº^kd‚Äãx(x)=d‚ÄãŒº^kd‚Äã‚ÑôXT‚ãÖfXT(x)=:fk(x).\frac{d\widehat{\mu}\_{k}}{dx}(x)=\frac{d\widehat{\mu}\_{k}}{d\mathbb{P}\_{X\_{T}}}\cdot f\_{X\_{T}}(x)=:f\_{k}(x). |  |

Keeping the above discussion in mind, we are now ready to prove the following theorem.

###### Proposition 3.6.

Assume that Assumption [3.2](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem2 "Assumption 3.2. ‚Ä£ 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") is in force, and that Œºini=Œ¥x0{\mu\_{\rm ini}}=\delta\_{x\_{0}}, x0‚àà‚Ñùdx\_{0}\in\mathbb{R}^{d}.
Then, for each k‚àà‚Ñïk\in\mathbb{N}, the optimal control for Problem [2.4](https://arxiv.org/html/2510.11829v1#S2.ThmTheorem4 "Problem 2.4 (Soft-constrained Schr√∂dinger Bridge Problem (SCSBP)). ‚Ä£ Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday"), denoted by Œ±^k\widehat{\alpha}^{k}, exists. Furthermore, Œ±^k\widehat{\alpha}^{k} has the following explicit feedback form: Œ±^tk:=‚àálog‚Å°hk‚Äã(t,XtŒ±^k)\widehat{\alpha}^{k}\_{t}:=\nabla\log h^{k}(t,X^{\widehat{\alpha}^{k}}\_{t}), where

|  |  |  |  |
| --- | --- | --- | --- |
|  | hk‚Äã(t,x)=‚à´‚Ñùdfk‚Äã(z)p‚Äã(T,z;0,x0)‚Äãp‚Äã(T,z;t,x)‚Äãùëëz=ùîº‚Äã[fk‚Äã(XT)p‚Äã(T,XT;0,x0)|Xt=x].\displaystyle h^{k}(t,x)=\int\_{\mathbb{R}^{d}}\frac{f\_{k}(z)}{p(T,z;0,x\_{0})}p(T,z;t,x)dz=\mathbb{E}\Big[\frac{f\_{k}(X\_{T})}{p(T,X\_{T};0,x\_{0})}\Big|X\_{t}=x\Big]. |  | (3.9) |

###### Proof.

Let k‚àà‚Ñïk\in\mathbb{N} be fixed, and let Œº^k\widehat{\mu}\_{k} be the minimizer of Dk‚Äã(‚ãÖ)D\_{k}(\cdot) defined by ([3.8](https://arxiv.org/html/2510.11829v1#S3.E8 "In 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")). Then, by ([3.4](https://arxiv.org/html/2510.11829v1#S3.E4 "In 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")), for
any Œ±‚ààùïÉùîΩ02‚Äã([0,T])\alpha\in\mathbb{L}^{2}\_{\mathbb{F}^{0}}([0,T]), we have

|  |  |  |
| --- | --- | --- |
|  | Jk‚Äã(Œ±)‚â•Dk‚Äã(‚ÑôXTŒ±)‚â•Dk‚Äã(Œº^k),\displaystyle J^{k}(\alpha)\geq D\_{k}(\mathbb{P}\_{X^{\alpha}\_{T}})\geq D\_{k}(\widehat{\mu}\_{k}), |  |

Therefore, in order to find the optimal control for Problem [2.4](https://arxiv.org/html/2510.11829v1#S2.ThmTheorem4 "Problem 2.4 (Soft-constrained Schr√∂dinger Bridge Problem (SCSBP)). ‚Ä£ Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") , it suffices to find Œ±^k\widehat{\alpha}^{k} such that (i) XTŒ±^k‚àºŒº^k\ X^{\widehat{\alpha}^{k}}\_{T}\sim\widehat{\mu}\_{k}; and (ii) Jk‚Äã(Œ±^k)=Dk‚Äã(‚ÑôXTŒ±^k)J^{k}(\widehat{\alpha}^{k})=D\_{k}(\mathbb{P}\_{X^{\widehat{\alpha}^{k}}\_{T}}).

To this end, we first recall that Œº^k\widehat{\mu}\_{k} is the minimizer of the function Dk‚Äã(‚ãÖ)D\_{k}(\cdot) with density fkf\_{k}. Next, we apply Lemma [3.1](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem1 "Lemma 3.1 ( [18, Theorem 3.1]). ‚Ä£ 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") with Œºtar{\mu\_{\rm tar}} being replaced by Œº^k\widehat{\mu}\_{k} to get the optimal control Œ±^k‚ààùí´‚Äã(Œºini,Œº^k)\widehat{\alpha}^{k}\in\mathscr{P}({\mu\_{\rm ini}},\widehat{\mu}\_{k}) for the original SBP ([2.5](https://arxiv.org/html/2510.11829v1#S2.E5 "In Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday"))-([2.6](https://arxiv.org/html/2510.11829v1#S2.E6 "In Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")), which satisfies Œ±^tk=‚àálog‚Å°hk‚Äã(t,XtŒ±^k)\widehat{\alpha}^{k}\_{t}=\nabla\log h^{k}(t,X^{\widehat{\alpha}^{k}}\_{t}), where hkh^{k} is defined by ([3.9](https://arxiv.org/html/2510.11829v1#S3.E9 "In Proposition 3.6. ‚Ä£ 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")), and XTŒ±^k‚àºŒº^k\ X^{\widehat{\alpha}^{k}}\_{T}\sim\widehat{\mu}\_{k}. Now, note that for this SBP we have

|  |  |  |
| --- | --- | --- |
|  | V‚Äã(Œºini,Œº^k)=12‚Äãùîº‚Äã[‚à´0T|Œ±^sk|2‚Äãùëës]=DKL‚Äã(Œº^k‚à•‚ÑôXT),V({\mu\_{\rm ini}},\widehat{\mu}\_{k})=\frac{1}{2}\mathbb{E}\Big[\int\_{0}^{T}|\widehat{\alpha}^{k}\_{s}|^{2}ds\Big]=D\_{\rm KL}(\widehat{\mu}\_{k}\|\mathbb{P}\_{X\_{T}}), |  |

we conclude that

|  |  |  |
| --- | --- | --- |
|  | Jk‚Äã(Œ±^k)=DKL‚Äã(Œº^k‚à•‚ÑôXT)+k‚ÄãG‚Äã(Œº^k)=Dk‚Äã(Œº^k)=Dk‚Äã(‚ÑôXTŒ±^k).J^{k}(\widehat{\alpha}^{k})=D\_{\rm KL}(\widehat{\mu}\_{k}\|\mathbb{P}\_{X\_{T}})+kG(\widehat{\mu}\_{k})=D\_{k}(\widehat{\mu}\_{k})=D\_{k}(\mathbb{P}\_{X^{\widehat{\alpha}^{k}}\_{T}}). |  |

In other words, Œ±^k\widehat{\alpha}^{k} is indeed the optimal control for the Problem [2.4](https://arxiv.org/html/2510.11829v1#S2.ThmTheorem4 "Problem 2.4 (Soft-constrained Schr√∂dinger Bridge Problem (SCSBP)). ‚Ä£ Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday"), proving the proposition.
‚àé

## 4 Convergence Results under Delta Initial Distribution

We make the following two observations. First, if we denote g‚Äã(x):=ftar‚Äã(x)p‚Äã(T,x;0,x0)g(x):=\frac{f\_{\rm tar}(x)}{p(T,x;0,x\_{0})}, then by ([3.5](https://arxiv.org/html/2510.11829v1#S3.E5 "In Lemma 3.1 ( [18, Theorem 3.1]). ‚Ä£ 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) we can write h‚Äã(t,x)=ùîºt,x‚Äã[g‚Äã(XT)]:=ùîº‚Äã[g‚Äã(XT)|Xt=x]h(t,x)=\mathbb{E}\_{t,x}[g(X\_{T})]:=\mathbb{E}[g(X\_{T})|X\_{t}=x], where XX is the solution to ([3.1](https://arxiv.org/html/2510.11829v1#S3.E1 "In 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) with X0=x0X\_{0}=x\_{0}.
By Feynman-Kac formula, we see that hh satisfy the PDE:

|  |  |  |  |
| --- | --- | --- | --- |
|  | {‚àÇth‚Äã(t,x)+‚Ñít‚Äãh‚Äã(t,x)=0;h‚Äã(T,x)=g‚Äã(x)=ftar‚Äã(x)p‚Äã(T,x;0,x0),\displaystyle\begin{cases}\partial\_{t}h(t,x)+\mathscr{L}\_{t}h(t,x)=0;\\ h(T,x)=g(x)=\frac{f\_{\rm tar}(x)}{p(T,x;0,x\_{0})},\end{cases} |  | (4.1) |

where the infinitesimal generator ‚Ñít\mathscr{L}\_{t} is defined by ‚Ñít:=b‚Äã(t,x)‚ãÖ‚àá+12‚ÄãŒî\mathscr{L}\_{t}:=b(t,x)\cdot\nabla+\frac{1}{2}\Delta. Similarly, we define gk‚Äã(x):=fk‚Äã(x)p‚Äã(T,x;0,x0)g\_{k}(x):=\frac{f\_{k}(x)}{p(T,x;0,x\_{0})}, then the function
hk‚Äã(t,x)h^{k}(t,x) can also be represented as the solution of the PDE:

|  |  |  |  |
| --- | --- | --- | --- |
|  | {‚àÇthk‚Äã(t,x)+‚Ñít‚Äãhk‚Äã(t,x)=0;hk‚Äã(T,x)=gk‚Äã(x)=fk‚Äã(x)p‚Äã(T,x;0,x0).\displaystyle\begin{cases}\partial\_{t}h^{k}(t,x)+\mathscr{L}\_{t}h^{k}(t,x)=0;\\ h^{k}(T,x)=g\_{k}(x)=\frac{f\_{k}(x)}{p(T,x;0,x\_{0})}.\end{cases} |  | (4.2) |

Recall that Œº^k‚ààùíÆk\widehat{\mu}\_{k}\in{\cal S}\_{k}, we have k‚ÄãG‚Äã(Œº^k)‚â§Dk‚Äã(Œº^k)‚â§mkG(\widehat{\mu}\_{k})\leq D\_{k}(\widehat{\mu}\_{k})\leq m, or

|  |  |  |  |
| --- | --- | --- | --- |
|  | G‚Äã(Œº^k)‚â§m/k.G(\widehat{\mu}\_{k})\leq m/k. |  | (4.3) |

Then Assumption [3.2](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem2 "Assumption 3.2. ‚Ä£ 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")-(ii) amounts to saying that |fk‚Äã(x)‚àíftar‚Äã(x)|‚â§C‚Ä≤k|f\_{k}(x)-f\_{\rm tar}(x)|\leq\frac{C^{\prime}}{k} with constant C‚Ä≤=C‚Äãm‚ÄãMC^{\prime}=CmM.
In other words, for all x‚àà‚Ñùdx\in\mathbb{R}^{d}, as k‚Üí‚àûk\to\infty we have

|  |  |  |
| --- | --- | --- |
|  | {|g‚Äã(x)‚àígk‚Äã(x)|=|h‚Äã(T,x)‚àíhk‚Äã(T,x)|=1p‚Äã(T,x;0,x0)‚Äã|ftar‚Äã(x)‚àífk‚Äã(x)|‚Üí0|h‚Äã(t,x)‚àíhk‚Äã(t,x)|‚â§‚à´‚Ñùdp‚Äã(T,z;t,x)p‚Äã(T,z;0,x0)‚Äã|ftar‚Äã(z)‚àífk‚Äã(z)|‚Äãùëëz‚Üí0,\displaystyle\left\{\begin{array}[]{lll}\displaystyle|g(x)-g\_{k}(x)|=|h(T,x)-h^{k}(T,x)|=\frac{1}{p(T,x;0,x\_{0})}|f\_{\rm tar}(x)-f\_{k}(x)|\to 0\vskip 6.0pt plus 2.0pt minus 2.0pt\\ \displaystyle|h(t,x)-h^{k}(t,x)|\leq\int\_{\mathbb{R}^{d}}\frac{p(T,z;t,x)}{p(T,z;0,x\_{0})}|f\_{\rm tar}(z)-f\_{k}(z)|dz\to 0,\end{array}\right. |  |

We shall use these facts to study the convergence of the optimal policies in the next subsection.

### 4.1 The Convergence of Optimal Policies

We shall now argue that the optimal controls for Problem [2.4](https://arxiv.org/html/2510.11829v1#S2.ThmTheorem4 "Problem 2.4 (Soft-constrained Schr√∂dinger Bridge Problem (SCSBP)). ‚Ä£ Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday"), {Œ±^k‚Äã(‚ãÖ,‚ãÖ)}\{\widehat{\alpha}^{k}(\cdot,\cdot)\}, given by Proposition [3.6](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem6 "Proposition 3.6. ‚Ä£ 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday"), actually converges to the solution of the original SBP Œ±^‚Äã(‚ãÖ,‚ãÖ)\widehat{\alpha}(\cdot,\cdot) given by Lemma [3.1](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem1 "Lemma 3.1 ( [18, Theorem 3.1]). ‚Ä£ 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday"), and also establish its rate of convergence. More precisely, we have the following theorem.

###### Theorem 4.1.

Assume that the Assumption [3.2](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem2 "Assumption 3.2. ‚Ä£ 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") is in force, and that Œºini=Œ¥x0{\mu\_{\rm ini}}=\delta\_{x\_{0}} for some x0‚àà‚Ñùdx\_{0}\in\mathbb{R}^{d}.
Furthermore, assume that there exists constants C,Œ¥>0C,\delta>0, such that Œ¥‚â§g‚Äã(x),gk‚Äã(x)‚â§C\delta\leq g(x),g\_{k}(x)\leq C, x‚àà‚Ñùdx\in\mathbb{R}^{d}, k‚àà‚Ñïk\in\mathbb{N}. Let Œ±^‚Äã(t,x)\widehat{\alpha}(t,x) and Œ±^k‚Äã(t,x)\widehat{\alpha}^{k}(t,x), (t,x)‚àà[0,T]√ó‚Ñùd(t,x)\in[0,T]\times\mathbb{R}^{d} be the optimal controls given in Lemma [3.1](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem1 "Lemma 3.1 ( [18, Theorem 3.1]). ‚Ä£ 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") and Proposition [3.6](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem6 "Proposition 3.6. ‚Ä£ 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday"), respectively. Then, it holds that

|  |  |  |
| --- | --- | --- |
|  | ‚à´0T|Œ±^k‚Äã(t,x)‚àíŒ±^‚Äã(t,x)|‚Äãùëët‚â§Ck,x‚àà‚Ñùd,\int\_{0}^{T}|\widehat{\alpha}^{k}(t,x)-\widehat{\alpha}(t,x)|dt\leq\frac{C}{k},\qquad x\in\mathbb{R}^{d}, |  |

where C>0C>0 is some constant independent of kk.

###### Remark 4.2.

We note that the assumption Œ¥‚â§g‚Äã(x)=ftar‚Äã(x)P‚Äã(T,x;0,x0)‚â§C\delta\leq g(x)=\frac{f\_{\rm tar}(x)}{P(T,x;0,x\_{0})}\leq C (resp. Œ¥‚â§gk‚Äã(x)‚â§C\delta\leq g\_{k}(x)\leq C) amounts to saying that ftar‚Äã(x)f\_{\rm tar}(x) (resp. fk‚Äã(x)f\_{k}(x)) ‚àùP‚Äã(T,x;0,x0)\propto\,P(T,x;0,x\_{0}) as x‚Üí‚àûx\to\infty, which is not particularly a stringent condition in light of the general estimate ([3.2](https://arxiv.org/html/2510.11829v1#S3.E2 "In 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")), and the arbitrariness of the sample data selection for the data generation procedure.
‚àé

###### Proof.

First, by definition Œ±^‚Äã(t,x)=‚àálog‚Å°h‚Äã(t,x)\widehat{\alpha}(t,x)=\nabla\log h(t,x) and Œ±^k‚Äã(t,x)=‚àálog‚Å°hk‚Äã(t,x)\widehat{\alpha}^{k}(t,x)=\nabla\log h^{k}(t,x), where hkh^{k} and hh are the solution to ([4.1](https://arxiv.org/html/2510.11829v1#S4.E1 "In 4 Convergence Results under Delta Initial Distribution ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) and ([4.2](https://arxiv.org/html/2510.11829v1#S4.E2 "In 4 Convergence Results under Delta Initial Distribution ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")), respectively, and ‚àá=‚àÇx\nabla=\partial\_{x}. We can easily deduce that

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | |Œ±^k‚Äã(t,x)‚àíŒ±^‚Äã(t,x)|\displaystyle|\widehat{\alpha}^{k}(t,x)-\widehat{\alpha}(t,x)| | =\displaystyle= | |‚àálog‚Å°hk‚Äã(t,x)‚àí‚àálog‚Å°h‚Äã(t,x)|=|‚àáhk‚Äã(t,x)hk‚Äã(t,x)‚àí‚àáh‚Äã(t,x)h‚Äã(t,x)|\displaystyle|\nabla\log h^{k}(t,x)-\nabla\log h(t,x)|=\left|\frac{\nabla h^{k}(t,x)}{h^{k}(t,x)}-\frac{\nabla h(t,x)}{h(t,x)}\right| |  |
|  |  | =\displaystyle= | |‚àáhk‚Äã(t,x)‚Äãh‚Äã(t,x)‚àí‚àáh‚Äã(t,x)‚Äãh‚Äã(t,x)+‚àáh‚Äã(t,x)‚Äãh‚Äã(t,x)‚àí‚àáh‚Äã(t,x)‚Äãhk‚Äã(t,x)hk‚Äã(t,x)‚Äãh‚Äã(t,x)|\displaystyle\left|\frac{\nabla h^{k}(t,x)h(t,x)-\nabla h(t,x)h(t,x)+\nabla h(t,x)h(t,x)-\nabla h(t,x)h^{k}(t,x)}{h^{k}(t,x)h(t,x)}\right| |  |
|  |  | ‚â§\displaystyle\leq | |‚àáhk‚Äã(t,x)‚àí‚àáh‚Äã(t,x)hk‚Äã(t,x)|+|‚àáh‚Äã(t,x)|‚Äã|h‚Äã(t,x)‚àíhk‚Äã(t,x)hk‚Äã(t,x)‚Äãh‚Äã(t,x)|:=I1+I2.\displaystyle\left|\frac{\nabla h^{k}(t,x)-\nabla h(t,x)}{h^{k}(t,x)}\right|+|\nabla h(t,x)|\left|\frac{h(t,x)-h^{k}(t,x)}{h^{k}(t,x)h(t,x)}\right|:=I\_{1}+I\_{2}. |  |

We now estimate I1I\_{1} and I2I\_{2}, respectively. To this end we first apply the well-known Bismut-Elworthy-Li formula [[8](https://arxiv.org/html/2510.11829v1#bib.bib8), [24](https://arxiv.org/html/2510.11829v1#bib.bib24)] (see also the representation formula in [[27](https://arxiv.org/html/2510.11829v1#bib.bib27), [43](https://arxiv.org/html/2510.11829v1#bib.bib43)]) to get

|  |  |  |  |
| --- | --- | --- | --- |
|  | {‚àáh‚Äã(t,x)=‚àÇxùîºt,x‚Äã[g‚Äã(XT)]=ùîºt,x‚Äã[g‚Äã(XT)‚ÄãNT],‚àáhk‚Äã(t,x)=‚àÇxùîºt,x‚Äã[gk‚Äã(XT)]=ùîºt,x‚Äã[gk‚Äã(XT)‚ÄãNT],\displaystyle\left\{\begin{array}[]{lll}\nabla h(t,x)=\partial\_{x}\mathbb{E}\_{t,x}[g(X\_{T})]=\mathbb{E}\_{t,x}\big[g(X\_{T})N\_{T}\big],\vskip 6.0pt plus 2.0pt minus 2.0pt\\ \nabla h^{k}(t,x)=\partial\_{x}\mathbb{E}\_{t,x}[g\_{k}(X\_{T})]=\mathbb{E}\_{t,x}\big[g\_{k}(X\_{T})N\_{T}\big],\end{array}\right. |  | (4.8) |

where,

|  |  |  |
| --- | --- | --- |
|  | Ns=Nst,x:=1s‚àít‚Äã‚à´ts(‚àáXrt,x)‚ä§‚ÄãùëëWr,s‚àà[t,T],\displaystyle N\_{s}=N^{t,x}\_{s}:=\frac{1}{s-t}\int\_{t}^{s}(\nabla X^{t,x}\_{r})^{\top}dW\_{r},\qquad s\in[t,T], |  |

and ‚àáX=‚àáXt,x\nabla X=\nabla X^{t,x} is a ‚Ñùd√ód\mathbb{R}^{d\times d}-valued variational process satisfying the (random) ODE:

|  |  |  |
| --- | --- | --- |
|  | ‚àÇxjXsi=Œ¥i‚Äãj+‚à´ts‚àë‚Ñì=1d‚àÇx‚Ñìbi‚Äã(r,Xr)‚Äã‚àÇxjXr‚Ñì‚Äãd‚Äãr,1‚â§i,j‚â§d,s‚àà[t,T].\displaystyle\partial\_{x^{j}}X^{i}\_{s}=\delta\_{ij}+\int\_{t}^{s}\sum\_{\ell=1}^{d}\partial\_{x^{\ell}}b^{i}(r,X\_{r})\partial\_{x^{j}}X^{\ell}\_{r}dr,\qquad 1\leq i,j\leq d,\quad s\in[t,T]. |  |

Furthermore,
one can easily check that

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Äã[|‚àáXst,x|2]‚â§C‚ÄãeC‚Äã(s‚àít),ùîº‚Äã[|Nst,x|2]‚â§Cs‚àít‚ÄãeC‚Äã(s‚àít),0‚â§t‚â§s‚â§T.\displaystyle\mathbb{E}\big[|\nabla X^{t,x}\_{s}|^{2}\big]\leq Ce^{C(s-t)},\qquad\mathbb{E}\big[|N^{t,x}\_{s}|^{2}\big]\leq{C\over s-t}e^{C(s-t)},\qquad 0\leq t\leq s\leq T. |  |

Therefore, denoting C>0C>0 to be a generic constant that is allowed to vary from line to line, and applying Assumption [3.7](https://arxiv.org/html/2510.11829v1#S3.E7 "In Assumption 3.2. ‚Ä£ 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") and estimate ([3.2](https://arxiv.org/html/2510.11829v1#S3.E2 "In 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) we have

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | |‚àáhk‚Äã(t,x)‚àí‚àáh‚Äã(t,x)|‚â§ùîº‚Äã[|g‚Äã(XTt,x)‚àígk‚Äã(XTt,x)|‚Äã|NTt,x|]=ùîº‚Äã[|fk‚Äã(XTt,x)‚àíftar‚Äã(XTt,x)p‚Äã(T,XTt,x;0,x0)|‚Äã|NTt,x|]‚â§(ùîº‚Äã[|NTt,x|2])12‚Äã[ùîº‚Äã|fk‚Äã(XTt,x)‚àíftar‚Äã(XTt,x)p‚Äã(T,XTt,x;0,x0)|2]12‚â§C‚ÄãeC‚Äã(T‚àít)T‚àít‚ÄãG‚Äã(Œº^k)‚Äã[ùîº‚Äã|œï‚Äã(XTt,x)p‚Äã(T,XTt,x;0,x0)|2]12‚â§Ck‚ÄãT‚àít.\displaystyle\begin{split}|\nabla h^{k}(t,x)-\nabla h(t,x)|&\leq\mathbb{E}\big[|g(X^{t,x}\_{T})-g\_{k}(X^{t,x}\_{T})||N^{t,x}\_{T}|\big]=\mathbb{E}\Big[\Big|\frac{f\_{k}(X^{t,x}\_{T})-f\_{\rm tar}(X^{t,x}\_{T})}{p(T,X^{t,x}\_{T};0,x\_{0})}\Big||N^{t,x}\_{T}|\Big]\\ &\leq\Big(\mathbb{E}[|N^{t,x}\_{T}|^{2}]\Big)^{\frac{1}{2}}\Big[\mathbb{E}\Big|\frac{f\_{k}(X^{t,x}\_{T})-f\_{\rm tar}(X^{t,x}\_{T})}{p(T,X^{t,x}\_{T};0,x\_{0})}\Big|^{2}\Big]^{\frac{1}{2}}\\ &\leq\frac{Ce^{C(T-t)}}{\sqrt{T-t}}G(\widehat{\mu}\_{k})\Big[\mathbb{E}\Big|\frac{\phi(X^{t,x}\_{T})}{p(T,X^{t,x}\_{T};0,x\_{0})}\Big|^{2}\Big]^{\frac{1}{2}}\leq\frac{C}{k\sqrt{T-t}}.\end{split} | |  | (4.9) |

Next, we note that by assumption Œ¥‚â§g‚Äã(x),gk‚Äã(x)‚â§C\delta\leq g(x),g\_{k}(x)\leq C for all x‚àà‚Ñùdx\in\mathbb{R}^{d} and k‚àà‚Ñïk\in\mathbb{N}, by the weak maximum principle we conclude that as the solutions to the PDEs ([4.1](https://arxiv.org/html/2510.11829v1#S4.E1 "In 4 Convergence Results under Delta Initial Distribution ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) and ([4.2](https://arxiv.org/html/2510.11829v1#S4.E2 "In 4 Convergence Results under Delta Initial Distribution ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")), respectively, it holds that Œ¥‚â§h‚Äã(t,x),hk‚Äã(t,x)‚â§C\delta\leq h(t,x),h^{k}(t,x)\leq C, for all (t,x)‚àà‚Ñùd√ó[0,T)(t,x)\in\mathbb{R}^{d}\times[0,T).
Consequently, we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | I1‚â§CŒ¥‚Äãk‚ÄãT‚àít‚â§Ck‚ÄãT‚àít.I\_{1}\leq\frac{C}{\delta k\sqrt{T-t}}\leq\frac{C}{k\sqrt{T-t}}. |  | (4.10) |

Similarly, we can argue that |‚àáh‚Äã(t,x)|‚â§CT‚àít|\nabla h(t,x)|\leq\frac{C}{\sqrt{T-t}}, and that

|  |  |  |  |
| --- | --- | --- | --- |
|  | |hk‚Äã(t,x)‚àíh‚Äã(t,x)|‚â§ùîº‚Äã[|fk‚Äã(XTt,x)‚àíftar‚Äã(XTt,x)p‚Äã(T,XTt,x;0,x0)|]‚â§C‚ÄãG‚Äã(Œº^k)‚Äãùîº‚Äã[|œï‚Äã(XTt,x)p‚Äã(T,XTt,x;0,x0)|]‚â§Ck,\displaystyle|h^{k}(t,x)-h(t,x)|\leq\mathbb{E}\Big[\Big|\frac{f\_{k}(X^{t,x}\_{T})-f\_{\rm tar}(X^{t,x}\_{T})}{p(T,X^{t,x}\_{T};0,x\_{0})}\Big|\Big]\leq CG(\widehat{\mu}\_{k})\mathbb{E}\Big[\Big|\frac{\phi(X^{t,x}\_{T})}{p(T,X^{t,x}\_{T};0,x\_{0})}\Big|\Big]\leq\frac{C}{k}, |  | (4.11) |

where the last inequality is due to ([4.3](https://arxiv.org/html/2510.11829v1#S4.E3 "In 4 Convergence Results under Delta Initial Distribution ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) and hence I2‚â§Ck‚ÄãT‚àítI\_{2}\leq\frac{C}{k\sqrt{T-t}}. This, together with ([4.10](https://arxiv.org/html/2510.11829v1#S4.E10 "In 4.1 The Convergence of Optimal Policies ‚Ä£ 4 Convergence Results under Delta Initial Distribution ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) and ([4.1](https://arxiv.org/html/2510.11829v1#S4.Ex5 "4.1 The Convergence of Optimal Policies ‚Ä£ 4 Convergence Results under Delta Initial Distribution ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")), we obtain

|  |  |  |  |
| --- | --- | --- | --- |
|  | |Œ±^k‚Äã(t,x)‚àíŒ±^‚Äã(t,x)|‚â§Ck‚ÄãT‚àít\displaystyle|\widehat{\alpha}^{k}(t,x)-\widehat{\alpha}(t,x)|\leq\frac{C}{k\sqrt{T-t}} |  | (4.12) |

and hence convergence result :

|  |  |  |
| --- | --- | --- |
|  | ‚à´0T|Œ±^k‚Äã(t,x)‚àíŒ±^‚Äã(t,x)|‚Äãùëët‚â§‚à´0TCk‚ÄãT‚àít‚Äãùëët‚â§C‚ÄãTk,\displaystyle\int\_{0}^{T}|\widehat{\alpha}^{k}(t,x)-\widehat{\alpha}(t,x)|dt\leq\int\_{0}^{T}\frac{C}{k\sqrt{T-t}}dt\leq\frac{C\sqrt{T}}{k}, |  |

proving the theorem.
‚àé

###### Remark 4.3.

A particular example is when we take the penalty function G‚Äã(Œº)=DKL‚Äã(Œº‚à•Œºtar)G(\mu)=D\_{\rm KL}(\mu\|{\mu\_{\rm tar}}). In this case, it is known
(see, e.g., [[29](https://arxiv.org/html/2510.11829v1#bib.bib29), Theorem 2]) that the optimal control for ([2.9](https://arxiv.org/html/2510.11829v1#S2.E9 "In Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday"))-([2.10](https://arxiv.org/html/2510.11829v1#S2.E10 "In Problem 2.4 (Soft-constrained Schr√∂dinger Bridge Problem (SCSBP)). ‚Ä£ Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) is given by
Œ±^tk=‚àálog‚Å°hk‚Äã(XtŒ±^k,t)\widehat{\alpha}\_{t}^{k}=\nabla\log h^{k}(X\_{t}^{\widehat{\alpha}^{k}},t), where

|  |  |  |
| --- | --- | --- |
|  | hk(t,x)=dk‚àí1‚à´p(T,z,;t,x)(ftar‚Äã(z)p‚Äã(T,z;0,x0))kk+1dz,\displaystyle h^{k}(t,x)=d\_{k}^{-1}\int p(T,z,;t,x)\Big(\frac{f\_{\rm tar}(z)}{p(T,z;0,x\_{0})}\Big)^{\frac{k}{k+1}}dz, |  |

with dk=‚à´ftar‚Äã(x)k1+k‚Äãp‚Äã(x,T|x0,0)11+k‚Äãùëëxd\_{k}=\int f\_{\rm tar}(x)^{\frac{k}{1+k}}p(x,T\,|\,x\_{0},0)^{\frac{1}{1+k}}dx. In addition, Jk‚Äã(Œ±^)=‚àí(1+k)‚Äãlog‚Å°(Ck)J^{k}(\widehat{\alpha})=-(1+k)\,\log(C\_{k}).
Consequently, Assumption [3.2](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem2 "Assumption 3.2. ‚Ä£ 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")-(ii) can be reduced to that ùîº‚Äã[|ftar‚Äã(XT)p‚Äã(T,XT;0,x0)|2]\mathbb{E}\Big[\big|\frac{f\_{\rm tar}(X\_{T})}{p(T,X\_{T};0,x\_{0})}\big|^{2}\Big] is bounded (see Assumption [5.4](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem4 "Assumption 5.4. ‚Ä£ 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") for similar conditions); and the linear rate of convergence can be proved with the same arguments.
‚àé

### 4.2 The Convergence of the Value Function

Having worked out the convergence analysis for the optimal controls, it is natural to extend the results to the convergence of value functions. However, the singularity at the terminal time TT in ([4.12](https://arxiv.org/html/2510.11829v1#S4.E12 "In 4.1 The Convergence of Optimal Policies ‚Ä£ 4 Convergence Results under Delta Initial Distribution ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) requires some technical care. It turns out that the popular notion of early stopping in diffusion models as well as the flow-based method literature [[4](https://arxiv.org/html/2510.11829v1#bib.bib4), [41](https://arxiv.org/html/2510.11829v1#bib.bib41), [33](https://arxiv.org/html/2510.11829v1#bib.bib33)] is exactly the remedy to this issue.

To be more precise, for any Œµ>0\varepsilon>0, we introduce the following Œµ\varepsilon-value function.

|  |  |  |
| --- | --- | --- |
|  | JŒµ‚Äã(Œ±):=ùîº‚Äã[‚à´0T‚àíŒµ12‚Äã|Œ±|2‚Äãùëët].\displaystyle J\_{\varepsilon}(\alpha):=\mathbb{E}\left[\int\_{0}^{T-\varepsilon}\frac{1}{2}|\alpha|^{2}dt\right]. |  |

There are many practical reasons, mainly for computational purposes, to invoke the notion of early stopping, as elaborated in
[[4](https://arxiv.org/html/2510.11829v1#bib.bib4), [41](https://arxiv.org/html/2510.11829v1#bib.bib41), [33](https://arxiv.org/html/2510.11829v1#bib.bib33)]. But on the other hand, it is clear that the Œµ\varepsilon-value function effectively excludes the singularity at the terminal time TT. This leads to the following straightforward result.

###### Proposition 4.4.

Assume that all the assumptions of Theorem [4.1](https://arxiv.org/html/2510.11829v1#S4.ThmTheorem1 "Theorem 4.1. ‚Ä£ 4.1 The Convergence of Optimal Policies ‚Ä£ 4 Convergence Results under Delta Initial Distribution ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") are in force. Then, for any Œµ>0\varepsilon>0, there exists a generic constant C:=C‚Äã(Œµ)=ùí™‚Äã(1Œµ)>0C:=C(\varepsilon)=\mathcal{O}(\frac{1}{\sqrt{\varepsilon}})>0, independent of kk, such that

|  |  |  |  |
| --- | --- | --- | --- |
|  | |JŒµ‚Äã(Œ±^k)‚àíJŒµ‚Äã(Œ±^)|‚â§Ck,k‚àà‚Ñï.\displaystyle|J\_{\varepsilon}(\widehat{\alpha}^{k})-J\_{\varepsilon}(\widehat{\alpha})|\leq\frac{C}{k},\qquad k\in\mathbb{N}. |  | (4.13) |

where Œ±^k\widehat{\alpha}^{k} and Œ±^\widehat{\alpha} are the optimal controls in Theorem [4.1](https://arxiv.org/html/2510.11829v1#S4.ThmTheorem1 "Theorem 4.1. ‚Ä£ 4.1 The Convergence of Optimal Policies ‚Ä£ 4 Convergence Results under Delta Initial Distribution ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday"), respectively.

###### Proof.

The proof is straightforward. For any k‚àà‚Ñïk\in\mathbb{N}, let Œ±^k\widehat{\alpha}^{k} and Œ±^\widehat{\alpha} be the optimal controls in Theorem [4.1](https://arxiv.org/html/2510.11829v1#S4.ThmTheorem1 "Theorem 4.1. ‚Ä£ 4.1 The Convergence of Optimal Policies ‚Ä£ 4 Convergence Results under Delta Initial Distribution ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday"), respectively. Then, for any Œµ>0\varepsilon>0, applying ([4.12](https://arxiv.org/html/2510.11829v1#S4.E12 "In 4.1 The Convergence of Optimal Policies ‚Ä£ 4 Convergence Results under Delta Initial Distribution ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) we have

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | |JŒµ‚Äã(Œ±^k)‚àíJŒµ‚Äã(Œ±^)|\displaystyle|J\_{\varepsilon}(\widehat{\alpha}^{k})-J\_{\varepsilon}(\widehat{\alpha})|\negthinspace | ‚â§\displaystyle\negthinspace\leq\negthinspace | ùîº‚Äã[12‚Äã‚à´0T‚àíŒµ||Œ±^sk|2‚àí|Œ±^s|2|‚Äãùëës]‚â§ùîº‚Äã[12‚Äã‚à´0T‚àíŒµ|Œ±^sk‚àíŒ±^s|‚Äã(|Œ±^sk|+|Œ±^s|)‚Äãùëës]\displaystyle\negthinspace\mathbb{E}\Big[\frac{1}{2}\int\_{0}^{T-\varepsilon}\big||\widehat{\alpha}^{k}\_{s}|^{2}-|\widehat{\alpha}\_{s}|^{2}\big|ds\Big]\leq\mathbb{E}\Big[\frac{1}{2}\int\_{0}^{T-\varepsilon}\big|\widehat{\alpha}^{k}\_{s}-\widehat{\alpha}\_{s}\big|\big(|\widehat{\alpha}^{k}\_{s}|+|\widehat{\alpha}\_{s}|\big)ds\Big] |  |
|  |  | ‚â§\displaystyle\negthinspace\leq\negthinspace | ck‚Äãùîº‚Äã[12‚Äã‚à´0T‚àíŒµ1T‚àís‚Äã(|Œ±^sk|+|Œ±^s|)‚Äãùëës]‚â§ck‚ÄãŒµ‚Äãùîº‚Äã[12‚Äã‚à´0T(|Œ±^sk|+|Œ±^s|)‚Äãùëës],\displaystyle\negthinspace\frac{c}{k}\mathbb{E}\Big[\frac{1}{2}\int\_{0}^{T-\varepsilon}\frac{1}{\sqrt{T-s}}\big(|\widehat{\alpha}^{k}\_{s}|+|\widehat{\alpha}\_{s}|\big)ds\Big]\leq\frac{c}{k\sqrt{\varepsilon}}\mathbb{E}\Big[\frac{1}{2}\int\_{0}^{T}\big(|\widehat{\alpha}^{k}\_{s}|+|\widehat{\alpha}\_{s}|\big)\,ds\Big], |  |

where
the last inequality is due to the fact that 1T‚àís‚â§1Œµ\frac{1}{\sqrt{T-s}}\leq\frac{1}{\sqrt{\varepsilon}} for s‚àà[0,T‚àíŒµ]s\in[0,T-\varepsilon]. To further bound ([4.2](https://arxiv.org/html/2510.11829v1#S4.Ex12 "4.2 The Convergence of the Value Function ‚Ä£ 4 Convergence Results under Delta Initial Distribution ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")), we recall that the definitions of J‚Äã(‚ãÖ)J(\cdot) ([2.5](https://arxiv.org/html/2510.11829v1#S2.E5 "In Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) and Jk‚Äã(‚ãÖ)J^{k}(\cdot) ([2.10](https://arxiv.org/html/2510.11829v1#S2.E10 "In Problem 2.4 (Soft-constrained Schr√∂dinger Bridge Problem (SCSBP)). ‚Ä£ Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")), k‚àà‚Ñïk\in\mathbb{N}, and define

|  |  |  |
| --- | --- | --- |
|  | V‚àó=J‚Äã(Œ±^)=infŒ±‚ààùíúJ‚Äã(Œ±);Vk,‚àó=Jk‚Äã(Œ±^k)=infŒ±‚ààùíúJk‚Äã(Œ±).\displaystyle V^{\*}=J(\widehat{\alpha})=\inf\_{\alpha\in\mathcal{A}}J(\alpha);\qquad V^{k,\*}=J^{k}(\widehat{\alpha}^{k})=\inf\_{\alpha\in\mathcal{A}}J^{k}(\alpha). |  |

We should note that XŒ±^X^{\widehat{\alpha}} follows the constrained dynamics ([2.6](https://arxiv.org/html/2510.11829v1#S2.E6 "In Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")), whereas XŒ±^kX^{\widehat{\alpha}^{k}} follows the soft-constrained dynamics ([2.9](https://arxiv.org/html/2510.11829v1#S2.E9 "In Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")). Clearly, by definition ([2.10](https://arxiv.org/html/2510.11829v1#S2.E10 "In Problem 2.4 (Soft-constrained Schr√∂dinger Bridge Problem (SCSBP)). ‚Ä£ Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) we have

|  |  |  |
| --- | --- | --- |
|  | supk‚â•1Jk‚Äã(Œ±)={ùîº‚Äã[‚à´0T12‚Äã|Œ±t|2‚Äãd‚Å°t]¬†if¬†‚Äã‚ÑôXTŒ±=Œºtar‚àûotherwise.\displaystyle\sup\_{k\geq 1}J^{k}(\alpha)=\begin{cases}\displaystyle\mathbb{E}\Big[\int\_{0}^{T}\frac{1}{2}|\alpha\_{t}|^{2}\operatorname{{\rm d}}t\Big]&\textrm{ if }\,\,\mathbb{P}\_{X\_{T}^{\alpha}}={\mu\_{\rm tar}}\\ \infty&\textrm{otherwise}.\end{cases} |  |

Thus, since Œ±^\widehat{\alpha} satisfies the constraint dynamics ([2.6](https://arxiv.org/html/2510.11829v1#S2.E6 "In Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")), we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | V‚àó=J‚Äã(Œ±^)=infŒ±‚ààùíúsupk‚â•1Jk‚Äã(Œ±)‚â•infŒ±‚ààùíúJk‚Äã(Œ±)=Jk‚Äã(Œ±^k)=Vk,‚àó,k‚àà‚Ñï.\displaystyle V^{\*}=J(\widehat{\alpha})=\inf\_{\alpha\in\mathcal{A}}\sup\_{k\geq 1}J^{k}(\alpha)\geq\inf\_{\alpha\in\mathcal{A}}J^{k}(\alpha)=J^{k}(\widehat{\alpha}^{k})=V^{k,\*},\qquad k\in\mathbb{N}. |  | (4.15) |

Consequently, we have, for each k‚àà‚Ñïk\in\mathbb{N}, a simple application of Cauchy‚ÄìSchwarz inequality and the fact ([4.15](https://arxiv.org/html/2510.11829v1#S4.E15 "In 4.2 The Convergence of the Value Function ‚Ä£ 4 Convergence Results under Delta Initial Distribution ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) yields

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùîº‚Äã[12‚Äã‚à´0T(|Œ±^sk|+|Œ±^s|)‚Äãùëës]‚â§T2‚Äã(ùîº‚Äã[‚à´0T|Œ±^sk|2‚Äãùëës])1/2+(ùîº‚Äã[‚à´0T|Œ±^s|2‚Äãùëës])1/2‚â§T‚ÄãV‚àó.\displaystyle\mathbb{E}\Big[\frac{1}{2}\int\_{0}^{T}\big(|\widehat{\alpha}^{k}\_{s}|+|\widehat{\alpha}\_{s}|\big)ds\Big]\leq\frac{{\sqrt{T}}}{2}\Big(\mathbb{E}\Big[\int\_{0}^{T}|\widehat{\alpha}^{k}\_{s}|^{2}ds\Big]\Big)^{1/2}+\Big(\mathbb{E}\Big[\int\_{0}^{T}|\widehat{\alpha}\_{s}|^{2}ds\Big]\Big)^{1/2}\leq\sqrt{T}\sqrt{V^{\*}}. |  | (4.16) |

Combining ([4.2](https://arxiv.org/html/2510.11829v1#S4.Ex12 "4.2 The Convergence of the Value Function ‚Ä£ 4 Convergence Results under Delta Initial Distribution ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) and ([4.16](https://arxiv.org/html/2510.11829v1#S4.E16 "In 4.2 The Convergence of the Value Function ‚Ä£ 4 Convergence Results under Delta Initial Distribution ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")), we obtain ([4.13](https://arxiv.org/html/2510.11829v1#S4.E13 "In Proposition 4.4. ‚Ä£ 4.2 The Convergence of the Value Function ‚Ä£ 4 Convergence Results under Delta Initial Distribution ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")).
‚àé

Besides the convergence of the value functions, another important convergence, that relies crucially on the convergence of the optimal controls, is the convergence of the terminal law ‚ÑôXTŒ±^k\mathbb{P}\_{X^{\widehat{\alpha}^{k}}\_{T}} (with respect to the target distribution Œºtar{\mu\_{\rm tar}}), measured, for instance, in the Wasserstein distance. Again, to avoid the technicalities that the singularity at terminal time TT might cause, we shall focus on the early stopped state XT‚àíŒµŒ±^kX^{\widehat{\alpha}^{k}}\_{T-\varepsilon}, which is a commonly used criterion in statistical estimation results for generative diffusion models (see, e.g., [[28](https://arxiv.org/html/2510.11829v1#bib.bib28), [33](https://arxiv.org/html/2510.11829v1#bib.bib33), [12](https://arxiv.org/html/2510.11829v1#bib.bib12)]). More precisely, we have the following result.

###### Proposition 4.5.

Let all assumptions in Theorem [4.1](https://arxiv.org/html/2510.11829v1#S4.ThmTheorem1 "Theorem 4.1. ‚Ä£ 4.1 The Convergence of Optimal Policies ‚Ä£ 4 Convergence Results under Delta Initial Distribution ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") be in force. Assume further that the optimal policy Œ±^\widehat{\alpha} of the original SBP is Lipschitz in xx: there exists Œ∫>0\kappa>0, such that

|  |  |  |  |
| --- | --- | --- | --- |
|  | |Œ±^‚Äã(t,x)‚àíŒ±^‚Äã(t,y)|‚â§Œ∫‚Äã|x‚àíy|,t‚àà[0,T].\displaystyle|\widehat{\alpha}(t,x)-\widehat{\alpha}(t,y)|\leq\kappa|x-y|,\qquad t\in[0,T]. |  | (4.17) |

Then there exists a constant C>0C>0, depending on the Lipschitz constants LL in Assumption [2.2](https://arxiv.org/html/2510.11829v1#S2.ThmTheorem2 "Assumption 2.2. ‚Ä£ Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") and Œ∫\kappa in ([4.17](https://arxiv.org/html/2510.11829v1#S4.E17 "In Proposition 4.5. ‚Ä£ 4.2 The Convergence of the Value Function ‚Ä£ 4 Convergence Results under Delta Initial Distribution ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")), but independent of k‚àà‚Ñïk\in\mathbb{N}, such that for any Œµ>0\varepsilon>0, it holds that

|  |  |  |  |
| --- | --- | --- | --- |
|  | W2‚Äã(‚ÑôXT‚àíŒµŒ±^k,Œºtar)‚â§C‚Äãln‚Å°T‚àíln‚Å°Œµk+C‚ÄãŒµ.\displaystyle{W}\_{2}(\mathbb{P}\_{X^{\widehat{\alpha}^{k}}\_{T-\varepsilon}},{\mu\_{\rm tar}})\leq\frac{C\sqrt{\ln T-\ln\varepsilon}}{k}+C\varepsilon. |  | (4.18) |

In particular, if we choose Œµ=1k\varepsilon=\frac{1}{k}, then it holds that

|  |  |  |  |
| --- | --- | --- | --- |
|  | W2(‚ÑôXT‚àíŒµŒ±^k,Œºtar)‚â§Ck(ln‚Å°k+ln‚Å°T)+1)=ùí™(ln‚Å°kk).\displaystyle{W}\_{2}(\mathbb{P}\_{X^{\widehat{\alpha}^{k}}\_{T-\varepsilon}},{\mu\_{\rm tar}})\leq\frac{C}{k}\big(\sqrt{\ln k}+\sqrt{\ln T})+1\big)=\mathcal{O}\Big(\frac{\sqrt{\ln k}}{k}\Big). |  | (4.19) |

###### Remark 4.6.

(i) The linear (i.e., ‚àº1k\sim\frac{1}{k}) "closeness" between the law of the optimal state and Œºtar{\mu\_{\rm tar}} has appeared several times so far. For example, ([4.3](https://arxiv.org/html/2510.11829v1#S4.E3 "In 4 Convergence Results under Delta Initial Distribution ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) implies that G‚Äã(‚ÑôXTŒ±^k)=G‚Äã(‚ÑôXTŒ±^k;Œºtar)‚â§ckG(\mathbb{P}\_{X^{\widehat{\alpha}^{k}}\_{T}})=G(\mathbb{P}\_{X^{\widehat{\alpha}^{k}}\_{T}};{\mu\_{\rm tar}})\leq\frac{c}{k}, and by
Remark [3.3](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem3 "Remark 3.3. ‚Ä£ 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")-(ii), this implies that W1‚Äã(‚ÑôXTŒ±^k,Œºtar)‚àº1kW\_{1}(\mathbb{P}\_{X^{\widehat{\alpha}^{k}}\_{T}},{\mu\_{\rm tar}})\sim\frac{1}{k}. The result in ([4.19](https://arxiv.org/html/2510.11829v1#S4.E19 "In Proposition 4.5. ‚Ä£ 4.2 The Convergence of the Value Function ‚Ä£ 4 Convergence Results under Delta Initial Distribution ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) is in the same spirit, by under the stronger W2{W}\_{2}-distance, but compensated by an early stopping.

(ii) The Lipschitz condition ([4.17](https://arxiv.org/html/2510.11829v1#S4.E17 "In Proposition 4.5. ‚Ä£ 4.2 The Convergence of the Value Function ‚Ä£ 4 Convergence Results under Delta Initial Distribution ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) for the optimal control Œ±^\widehat{\alpha} is not unusual in the diffusion model literature (see, e.g., [[60](https://arxiv.org/html/2510.11829v1#bib.bib60), [11](https://arxiv.org/html/2510.11829v1#bib.bib11), [12](https://arxiv.org/html/2510.11829v1#bib.bib12)]). In fact, this can be argued via regularity of the solution to the PDE ([4.1](https://arxiv.org/html/2510.11829v1#S4.E1 "In 4 Convergence Results under Delta Initial Distribution ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) combined with the speed of decay of the density ftarf\_{\rm tar}, which can be assumed and analyzed rigorously (see Assumption [5.4](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem4 "Assumption 5.4. ‚Ä£ 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") below). We therefore consider such an assumption non-stringent.
‚àé

[Proof of Proposition [4.5](https://arxiv.org/html/2510.11829v1#S4.ThmTheorem5 "Proposition 4.5. ‚Ä£ 4.2 The Convergence of the Value Function ‚Ä£ 4 Convergence Results under Delta Initial Distribution ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday").]
First note that XŒ±^X^{\widehat{\alpha}} and XŒ±^kX^{\widehat{\alpha}^{k}} satisfy the following SDEs, respectively:

|  |  |  |  |
| --- | --- | --- | --- |
|  | {d‚ÄãXtŒ±^=[b‚Äã(t,XtŒ±^)+Œ±^t‚Äã(XtŒ±^)]‚Äãd‚Äãt+d‚ÄãWtX0Œ±^=x0;d‚ÄãXtŒ±^k=[b‚Äã(t,XtŒ±^k)+Œ±^tk‚Äã(XtŒ±^k)]‚Äãd‚Äãt+d‚ÄãWt,X0Œ±^=x0.\displaystyle\begin{cases}dX^{\widehat{\alpha}}\_{t}=[b(t,X^{\widehat{\alpha}}\_{t})+\widehat{\alpha}\_{t}(X^{\widehat{\alpha}}\_{t})]dt+dW\_{t}\qquad\quad&X^{\widehat{\alpha}}\_{0}=x\_{0};\\ dX^{\widehat{\alpha}^{k}}\_{t}=[b(t,X^{\widehat{\alpha}^{k}}\_{t})+\widehat{\alpha}\_{t}^{k}(X^{\widehat{\alpha}^{k}}\_{t})]dt+dW\_{t},&X^{\widehat{\alpha}}\_{0}=x\_{0}.\end{cases} |  | (4.20) |

Let us now denote Œ±^t‚Äã(x)=Œ±^‚Äã(t,x)\widehat{\alpha}\_{t}(x)=\widehat{\alpha}(t,x), Œ±^tk‚Äã(x)=Œ±^k‚Äã(t,x)\widehat{\alpha}^{k}\_{t}(x)=\widehat{\alpha}^{k}(t,x), and define

|  |  |  |
| --- | --- | --- |
|  | bŒ±^‚Äã(t,x)=b‚Äã(t,x)+Œ±^t‚Äã(x),Œî‚ÄãŒ±^tk‚Äã(x)=Œ±^tk‚Äã(x)‚àíŒ±^t‚Äã(x),(t,x)‚àà[0,T]√ó‚Ñùd.b^{\widehat{\alpha}}(t,x)=b(t,x)+\widehat{\alpha}\_{t}(x),\quad\Delta\widehat{\alpha}^{k}\_{t}(x)=\widehat{\alpha}^{k}\_{t}(x)-\widehat{\alpha}\_{t}(x),\qquad(t,x)\in[0,T]\times\mathbb{R}^{d}. |  |

Then we see that SDE ([4.20](https://arxiv.org/html/2510.11829v1#S4.E20 "In 4.2 The Convergence of the Value Function ‚Ä£ 4 Convergence Results under Delta Initial Distribution ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) can be written as

|  |  |  |
| --- | --- | --- |
|  | {d‚ÄãXtŒ±^=bŒ±^‚Äã(t,XtŒ±^)‚Äãd‚Äãt+d‚ÄãWtX0Œ±^=x0;d‚ÄãXtŒ±^k=[bŒ±^‚Äã(t,XtŒ±^k)+Œî‚ÄãŒ±^tk‚Äã(XtŒ±^k)]‚Äãd‚Äãt+d‚ÄãWt,X0Œ±^=x0.\displaystyle\begin{cases}dX^{\widehat{\alpha}}\_{t}=b^{\widehat{\alpha}}(t,X^{\widehat{\alpha}}\_{t})dt+dW\_{t}\qquad\quad&X^{\widehat{\alpha}}\_{0}=x\_{0};\\ dX^{\widehat{\alpha}^{k}}\_{t}=[b^{\widehat{\alpha}}(t,X^{\widehat{\alpha}^{k}}\_{t})+\Delta\widehat{\alpha}\_{t}^{k}(X^{\widehat{\alpha}^{k}}\_{t})]dt+dW\_{t},&X^{\widehat{\alpha}}\_{0}=x\_{0}.\end{cases} |  |

That is,

|  |  |  |
| --- | --- | --- |
|  | XtŒ±^‚àíXtŒ±^k=‚à´0t[bŒ±^‚Äã(s,XsŒ±^)‚àíbŒ±^‚Äã(s,XsŒ±^k)+Œî‚ÄãŒ±^sk‚Äã(XsŒ±^k)]‚Äãùëës,t‚àà[0,T]\displaystyle X^{\widehat{\alpha}}\_{t}-X^{\widehat{\alpha}^{k}}\_{t}=\int\_{0}^{t}[b^{\widehat{\alpha}}(s,X^{\widehat{\alpha}}\_{s})-b^{\widehat{\alpha}}(s,X^{\widehat{\alpha}^{k}}\_{s})+\Delta\widehat{\alpha}\_{s}^{k}(X^{\widehat{\alpha}^{k}}\_{s})]ds,\qquad t\in[0,T] |  |

Note that by Assumption [2.2](https://arxiv.org/html/2510.11829v1#S2.ThmTheorem2 "Assumption 2.2. ‚Ä£ Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") and ([4.17](https://arxiv.org/html/2510.11829v1#S4.E17 "In Proposition 4.5. ‚Ä£ 4.2 The Convergence of the Value Function ‚Ä£ 4 Convergence Results under Delta Initial Distribution ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")), bŒ±^b^{\widehat{\alpha}} is uniform Lipschitz in xx (with Lipschitz constant L+Œ∫L+\kappa),
and applying the estimate ([4.12](https://arxiv.org/html/2510.11829v1#S4.E12 "In 4.1 The Convergence of Optimal Policies ‚Ä£ 4 Convergence Results under Delta Initial Distribution ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")), we deduce easily that

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùîº[|XtŒ±^‚àíXtŒ±^k|2]‚â§2T‚à´0t[(L+Œ∫)2ùîº[|XsŒ±^‚àíXsŒ±^k|2]ds+2‚Äãc2k2ln[TT‚àít].\displaystyle\mathbb{E}[|X^{\widehat{\alpha}}\_{t}-X^{\widehat{\alpha}^{k}}\_{t}|^{2}]\leq 2T\int\_{0}^{t}\Big[(L+\kappa)^{2}\mathbb{E}[|X^{\widehat{\alpha}}\_{s}-X^{\widehat{\alpha}^{k}}\_{s}|^{2}]ds+\frac{2c^{2}}{k^{2}}\ln\Big[\frac{T}{T-t}\Big]. |  | (4.21) |

In what follows let us denote C>0C>0 to be a generic constant depending only on LL, Œ∫\kappa, cc, but independent of kk, and we allow it to vary from line to line. Then, by a simple calculation using Gronwall‚Äôs inequality, we see that ([4.21](https://arxiv.org/html/2510.11829v1#S4.E21 "In 4.2 The Convergence of the Value Function ‚Ä£ 4 Convergence Results under Delta Initial Distribution ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) lead to that

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùîº‚Äã[|XtŒ±^‚àíXtŒ±^k|2]‚â§Ck2‚Äã(ln‚Å°T‚àíln‚Å°(T‚àít))‚ÄãeC‚Äãt,t‚àà[0,T).\displaystyle\mathbb{E}[|X^{\widehat{\alpha}}\_{t}-X^{\widehat{\alpha}^{k}}\_{t}|^{2}]\leq\frac{C}{k^{2}}(\ln T-\ln(T-t))e^{Ct},\qquad t\in[0,T). |  | (4.22) |

Furthermore, for any Œµ>0\varepsilon>0, using the monotonicity of the log function we deduce from ([4.22](https://arxiv.org/html/2510.11829v1#S4.E22 "In 4.2 The Convergence of the Value Function ‚Ä£ 4 Convergence Results under Delta Initial Distribution ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) that

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Äã[|XT‚àíŒµŒ±^‚àíXT‚àíŒµŒ±^k|2]‚â§Ck2‚Äã(ln‚Å°T‚àíln‚Å°Œµ)\displaystyle\mathbb{E}[|X^{\widehat{\alpha}}\_{T-\varepsilon}-X^{\widehat{\alpha}^{k}}\_{T-\varepsilon}|^{2}]\leq\frac{C}{k^{2}}(\ln T-\ln\varepsilon) |  |

It then follows that

|  |  |  |
| --- | --- | --- |
|  | W2‚Äã(‚ÑôXT‚àíŒµŒ±^,‚ÑôXT‚àíŒµŒ±^k)‚â§ùîº‚Äã[|XT‚àíŒµŒ±^‚àíXT‚àíŒµŒ±^k|2]1/2‚â§Ck‚Äãln‚Å°T‚àíln‚Å°Œµ.\displaystyle W\_{2}(\mathbb{P}\_{X^{\widehat{\alpha}}\_{T-\varepsilon}},\mathbb{P}\_{X^{\widehat{\alpha}^{k}}\_{T-\varepsilon}})\leq\mathbb{E}[|X^{\widehat{\alpha}}\_{T-\varepsilon}-X^{\widehat{\alpha}^{k}}\_{T-\varepsilon}|^{2}]^{1/2}\leq\frac{\sqrt{C}}{k}\sqrt{\ln T-\ln\varepsilon}. |  |

Finally, since the function bŒ±^=b+Œ±^b^{\widehat{\alpha}}=b+\widehat{\alpha} is Lipschitz, by standard ùïÉ2\mathbb{L}^{2}-continuity result of SDE, we have

|  |  |  |
| --- | --- | --- |
|  | W2‚Äã(‚ÑôXT‚àíŒµŒ±^,‚ÑôXTŒ±^)‚â§C‚ÄãŒµ,\displaystyle{W}\_{2}(\mathbb{P}\_{X^{\widehat{\alpha}}\_{T-\varepsilon}},\mathbb{P}\_{X^{\widehat{\alpha}}\_{T}})\leq C\varepsilon, |  |

and consequently, noting that ‚ÑôXTŒ±^=Œºtar\mathbb{P}\_{X^{\widehat{\alpha}}\_{T}}={\mu\_{\rm tar}}, we obtain

|  |  |  |
| --- | --- | --- |
|  | W2‚Äã(‚ÑôXT‚àíŒµŒ±^,Œºtar)‚â§W2‚Äã(‚ÑôXT‚àíŒµŒ±^,‚ÑôXT‚àíŒµŒ±^k)+W2‚Äã(‚ÑôXT‚àíŒµŒ±^,‚ÑôXTŒ±^)‚â§C‚Äã(ln‚Å°T‚àíln‚Å°Œµ)k+C‚ÄãŒµ,\displaystyle{W}\_{2}(\mathbb{P}\_{X^{\widehat{\alpha}}\_{T-\varepsilon}},{\mu\_{\rm tar}})\leq W\_{2}(\mathbb{P}\_{X^{\widehat{\alpha}}\_{T-\varepsilon}},\mathbb{P}\_{X^{\widehat{\alpha}^{k}}\_{T-\varepsilon}})+{W}\_{2}(\mathbb{P}\_{X^{\widehat{\alpha}}\_{T-\varepsilon}},\mathbb{P}\_{X^{\widehat{\alpha}}\_{T}})\leq\frac{\sqrt{C(\ln T-\ln\varepsilon)}}{k}+C\varepsilon, |  |

proving ([4.18](https://arxiv.org/html/2510.11829v1#S4.E18 "In Proposition 4.5. ‚Ä£ 4.2 The Convergence of the Value Function ‚Ä£ 4 Convergence Results under Delta Initial Distribution ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")), whence the proposition.
‚àé

## 5 Stability of the Solutions to the SBP

We note that all the results in the previous section are based on an important assumption: Œºini=Œ¥x0{\mu\_{\rm ini}}=\delta\_{x\_{0}}, for some x0‚àà‚Ñùdx\_{0}\in\mathbb{R}^{d}. In this and the next section, we shall extend the results to more general initial condition Œºini‚ààùí´2‚Äã(‚Ñùd){\mu\_{\rm ini}}\in\mathscr{P}\_{2}(\mathbb{R}^{d}), and establish a similar rate of convergence.

We shall begin an important aspect in probability theory, which is the basis for the so-called stability issues of the solutions to the classic Schr√∂dinger bridge problem. For notational convenience, we still denote p‚Äã(‚ãÖ,‚ãÖ;‚ãÖ,‚ãÖ)p(\cdot,\cdot;\cdot,\cdot) to be the transition density of a standard ‚Ñùd\mathbb{R}^{d}-valued diffusion ([3.1](https://arxiv.org/html/2510.11829v1#S3.E1 "In 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")). We begin with the following well-known result in diffusion theory (cf. e.g., [[7](https://arxiv.org/html/2510.11829v1#bib.bib7)]).

###### Proposition 5.1 ([[7](https://arxiv.org/html/2510.11829v1#bib.bib7)]).

For any Œº0,ŒºT‚ààùí´‚Äã(‚Ñùd)\mu\_{0},\mu\_{T}\in\mathscr{P}(\mathbb{R}^{d}), there exists a unique pair of œÉ\sigma-finite measures ŒΩ0,ŒΩT‚àà‚Ñ≥‚Äã(‚Ñùd)\nu\_{0},\nu\_{T}\in\mathscr{M}(\mathbb{R}^{d}) such that the measure œÄ‚ààùí´‚Äã(‚Ñùd√ó‚Ñùd)\pi\in\mathscr{P}(\mathbb{R}^{d}\times\mathbb{R}^{d}) defined by

|  |  |  |  |
| --- | --- | --- | --- |
|  | œÄ‚Äã(E)=‚à´Ep‚Äã(T,y;0,x)‚ÄãŒΩ0‚Äã(d‚Äãx)‚ÄãŒΩT‚Äã(d‚Äãy),E‚àà‚Ñ¨‚Äã(‚Ñùd√ó‚Ñùd)\displaystyle\pi(E)=\int\_{E}p(T,y;0,x)\nu\_{0}(dx)\nu\_{T}(dy),\qquad E\in\mathscr{B}(\mathbb{R}^{d}\times\mathbb{R}^{d}) |  | (5.1) |

has marginals Œº0\mu\_{0} and ŒºT\mu\_{T}. Furthermore, ŒΩT\nu\_{T} and ŒºT\mu\_{T} (resp. ŒΩ0\nu\_{0} and Œº0\mu\_{0}) are mutually absolutely continuous,
denoted by ŒΩT‚âÉŒºT\nu\_{T}\simeq\mu\_{T} (resp. ŒΩ0‚âÉŒº0\nu\_{0}\simeq\mu\_{0}).
‚àé

Following Proposition [5.1](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem1 "Proposition 5.1 ([7]). ‚Ä£ 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday"), let us denote a (well-defined) mapping
ùíØ:ùí´2‚Äã(‚Ñùd)√óùí´2‚Äã(‚Ñùd)‚Ü¶‚Ñ≥‚Äã(‚Ñùd)√ó‚Ñ≥‚Äã(‚Ñùd){\cal T}:\mathscr{P}\_{2}(\mathbb{R}^{d})\times\mathscr{P}\_{2}(\mathbb{R}^{d})\mapsto\mathscr{M}(\mathbb{R}^{d})\times\mathscr{M}(\mathbb{R}^{d}) by ùíØ‚Äã(Œº0,ŒºT)=(ŒΩ0,ŒΩT){\cal T}(\mu\_{0},\mu\_{T})=(\nu\_{0},\nu\_{T}). In particular, in what follows we shall often fix Œº0=Œºini‚ààùí´2‚Äã(‚Ñùd)\mu\_{0}={\mu\_{\rm ini}}\in\mathscr{P}\_{2}(\mathbb{R}^{d}), and focus mainly on ŒΩT\nu\_{T}.
Note that in Proposition [5.1](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem1 "Proposition 5.1 ([7]). ‚Ä£ 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") the measures (ŒΩ0,ŒΩT)(\nu\_{0},\nu\_{T}) are only œÉ\sigma-finite in general, to facilitate our discussion, we shall consider, for a given Œº0\mu\_{0}, the following set:

|  |  |  |
| --- | --- | --- |
|  | ùíüŒº0:={Œº‚ààùí´2‚Äã(‚Ñùd):ùíØ‚Äã(Œº0,Œº)‚â™L‚Äãe‚Äãb‚Äã(‚ãÖ);ùíØ‚Äã(Œº0,Œº)‚Äã(‚Ñùd√ó‚Ñùd)<‚àû}.\displaystyle\mathscr{D}\_{\mu\_{0}}:=\{\mu\in\mathscr{P}\_{2}(\mathbb{R}^{d}):{\cal T}(\mu\_{0},\mu)\ll Leb(\cdot);\,{\cal T}(\mu\_{0},\mu)(\mathbb{R}^{d}\times\mathbb{R}^{d})<\infty\}. |  |

Here L‚Äãe‚Äãb‚Äã(‚ãÖ)Leb(\cdot) denotes the Lebesgue measure on ‚Ñùd√ó‚Ñùd\mathbb{R}^{d}\times\mathbb{R}^{d}. In the case when Œº0=Œºini\mu\_{0}={\mu\_{\rm ini}} is fixed in the discussion, we shall simply denote ùíü=ùíüŒºini\mathscr{D}=\mathscr{D}\_{{\mu\_{\rm ini}}} when context is clear.

We note that if Œº‚ààùíü\mu\in\mathscr{D} and (ŒΩ0,ŒΩT)=ùíØ‚Äã(Œºini,Œº)(\nu\_{0},\nu\_{T})={\cal T}({\mu\_{\rm ini}},\mu), then
ŒΩT\nu\_{T} must have a density function, which we shall denote by œÅŒº‚ààùïÉ1‚Äã(‚Ñùd)\rho^{\mu}\in\mathbb{L}^{1}(\mathbb{R}^{d}).
Moreover, we define an operator S:ùí´2‚Äã(‚Ñùd)‚Üíùí´2‚Äã(‚Ñùd)S:\mathscr{P}\_{2}(\mathbb{R}^{d})\to\mathscr{P}\_{2}(\mathbb{R}^{d}) by

|  |  |  |  |
| --- | --- | --- | --- |
|  | S‚Äã[Œº]‚Äã(d‚Äãy)=‚à´‚Ñùdp‚Äã(T,y;0,x)‚ÄãŒº‚Äã(d‚Äãx)‚Äãùëëy,Œº‚ààùí´2‚Äã(‚Ñùd).\displaystyle S[\mu](dy)=\int\_{\mathbb{R}^{d}}p(T,y;0,x)\mu(dx)dy,\qquad\mu\in\mathscr{P}\_{2}(\mathbb{R}^{d}). |  | (5.2) |

Clearly, if Œº‚ààùí´‚Äã(‚Ñùd)\mu\in\mathscr{P}(\mathbb{R}^{d}), then S‚Äã[Œº]‚Äã(d‚Äãy)=fXT0,Œº‚Äã(y)‚Äãd‚ÄãyS[\mu](dy)=f\_{X^{0,\mu}\_{T}}(y)dy, where X0,Œº={Xt0,Œº}t‚àà[0,T]X^{0,\mu}=\{X^{0,\mu}\_{t}\}\_{t\in[0,T]} denotes the solution to ([3.1](https://arxiv.org/html/2510.11829v1#S3.E1 "In 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) with X00,Œº‚àºŒºX^{0,\mu}\_{0}\sim\mu.
But the operator SS can be naturally extended to any Œº‚àà‚Ñ≥‚Äã(‚Ñùd)\mu\in\mathscr{M}(\mathbb{R}^{d}), provided the right-hand side of ([5.2](https://arxiv.org/html/2510.11829v1#S5.E2 "In 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) is well-defined.

Let us now recall a well-known analogue of Lemma [3.1](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem1 "Lemma 3.1 ( [18, Theorem 3.1]). ‚Ä£ 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") in the case of general initial condition Œºini‚ààùí´2‚Äã(‚Ñùd){\mu\_{\rm ini}}\in\mathscr{P}\_{2}(\mathbb{R}^{d}).

###### Proposition 5.2 ([[18](https://arxiv.org/html/2510.11829v1#bib.bib18), Theorem 3.2]).

Let Œºini‚ààùí´2‚Äã(‚Ñùd){\mu\_{\rm ini}}\in\mathscr{P}\_{2}(\mathbb{R}^{d}), and assume that DKL‚Äã(Œºini‚à•ŒΩ0)<‚àûD\_{\rm KL}({\mu\_{\rm ini}}\|\nu\_{0})<\infty and DKL‚Äã(Œºtar‚à•S‚Äã[ŒΩ0])<‚àûD\_{\rm KL}({\mu\_{\rm tar}}\|S[\nu\_{0}])<\infty. Then, the optimal control for the (original) SBP ([2.5](https://arxiv.org/html/2510.11829v1#S2.E5 "In Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday"))-([2.6](https://arxiv.org/html/2510.11829v1#S2.E6 "In Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) is given by Œ±^t=‚àálog‚Å°h‚Äã(t,XtŒ±^)\widehat{\alpha}\_{t}=\nabla\log h(t,X^{\widehat{\alpha}}\_{t}) where, denoting œÅŒºtar‚Äã(‚ãÖ)\rho^{\mu\_{\rm tar}}(\cdot) to be the density function of ŒΩT\nu\_{T},

|  |  |  |  |
| --- | --- | --- | --- |
|  | h‚Äã(t,x):=‚à´‚Ñùdp‚Äã(T,z;t,x)‚ÄãœÅŒºtar‚Äã(z)‚Äãùëëz.\displaystyle h(t,x):=\int\_{\mathbb{R}^{d}}p(T,z;t,x)\rho^{\mu\_{\rm tar}}(z)dz. |  | (5.3) |

Moreover, it holds that

|  |  |  |  |
| --- | --- | --- | --- |
|  | J‚Äã(Œ±^)=‚à´‚Ñùdlog‚Å°œÅŒºtar‚Äã(y)‚ÄãŒºtar‚Äã(d‚Äãy)‚àíDKL‚Äã(Œºini‚à•ŒΩ0).J(\widehat{\alpha})=\int\_{\mathbb{R}^{d}}\log\rho^{\mu\_{\rm tar}}(y){\mu\_{\rm tar}}(dy)-D\_{\rm KL}({\mu\_{\rm ini}}\|\nu\_{0}). |  | (5.4) |

We note that in the above DKL‚Äã(Œºini‚à•ŒΩ0)=‚à´log‚Å°Œºini‚Äã(d‚Äãx)ŒΩ0‚Äã(d‚Äãx)‚ÄãŒºini‚Äã(d‚Äãx)D\_{\rm KL}({\mu\_{\rm ini}}\|\nu\_{0})=\int\log\frac{{\mu\_{\rm ini}}(dx)}{\nu\_{0}(dx)}{\mu\_{\rm ini}}(dx) (see footnote 1), and ([5.1](https://arxiv.org/html/2510.11829v1#S5.E1 "In Proposition 5.1 ([7]). ‚Ä£ 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) implies that Œºini‚Äã(d‚Äãx)ŒΩ0‚Äã(d‚Äãx)=‚à´p‚Äã(T,y;0,x)‚ÄãœÅŒºtar‚Äã(y)‚Äãùëëy\frac{{\mu\_{\rm ini}}(dx)}{\nu\_{0}(dx)}=\int p(T,y;0,x)\rho^{\mu\_{\rm tar}}(y)dy. Therefore ([5.4](https://arxiv.org/html/2510.11829v1#S5.E4 "In Proposition 5.2 ([18, Theorem 3.2]). ‚Ä£ 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) can be rewritten as

|  |  |  |  |
| --- | --- | --- | --- |
|  | J‚Äã(Œ±^)\displaystyle J(\widehat{\alpha}) | =‚à´‚Ñùdlog‚Å°œÅŒºtar‚Äã(y)‚ÄãŒºtar‚Äã(d‚Äãy)‚àí‚à´‚Ñùdlog‚Å°(‚à´‚Ñùdp‚Äã(T,y;0,x)‚ÄãœÅŒºtar‚Äã(y)‚Äãùëëy)‚ÄãŒºini‚Äã(d‚Äãx)\displaystyle=\int\_{\mathbb{R}^{d}}\log\rho^{\mu\_{\rm tar}}(y){\mu\_{\rm tar}}(dy)-\int\_{\mathbb{R}^{d}}\log\Big(\int\_{\mathbb{R}^{d}}p(T,y;0,x)\rho^{\mu\_{\rm tar}}(y)dy\Big){\mu\_{\rm ini}}(dx) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =ùîº‚Äã[log‚Å°œÅŒºtar‚Äã(XTŒ±^)]‚àí‚à´‚Ñùdlog‚Å°h‚Äã(0,x)‚ÄãŒºini‚Äã(d‚Äãx)=ùîº‚Äã[log‚Å°œÅŒºtar‚Äã(XTŒ±^)]‚àíùîº‚Äã[log‚Å°h‚Äã(0,X0Œ±^)].\displaystyle=\mathbb{E}[\log\rho^{\mu\_{\rm tar}}(X^{\widehat{\alpha}}\_{T})]-\int\_{\mathbb{R}^{d}}\log h(0,x){\mu\_{\rm ini}}(dx)=\mathbb{E}\big[\log\rho^{\mu\_{\rm tar}}(X^{\widehat{\alpha}}\_{T})\big]-\mathbb{E}[\log h(0,X^{\widehat{\alpha}}\_{0})]. |  |

Moreover, for fixed Œºini‚ààùí´2‚Äã(‚Ñùd){\mu\_{\rm ini}}\in\mathscr{P}\_{2}(\mathbb{R}^{d}) and Œº‚ààùíü=ùíüŒºini\mu\in\mathscr{D}=\mathscr{D}\_{{\mu\_{\rm ini}}},
we define hŒº‚Äã(t,x)=‚à´‚Ñùdp‚Äã(T,z;t,x)‚ÄãœÅŒº‚Äã(z)‚Äãùëëzh^{\mu}(t,x)=\int\_{\mathbb{R}^{d}}p(T,z;t,x)\rho^{\mu}(z)dz. Then, we have the following result.

###### Lemma 5.3 ([[29](https://arxiv.org/html/2510.11829v1#bib.bib29), Lemma 3.1]).

Let Œº‚ààùíü=ùíüŒºini\mu\in\mathscr{D}=\mathscr{D}\_{{\mu\_{\rm ini}}}. Then, for any {Œ±t}‚ààùïÉùîΩ02‚Äã([0,T])\{\alpha\_{t}\}\in\mathbb{L}^{2}\_{\mathbb{F}^{0}}([0,T]), it holds that

|  |  |  |
| --- | --- | --- |
|  | J‚Äã(Œ±)‚â•ùîº‚Äã[log‚Å°œÅŒº‚Äã(XTŒ±)]‚àíùîº‚Äã[log‚Å°hŒº‚Äã(0,X0Œ±)].\displaystyle J(\alpha)\geq\mathbb{E}[\log\rho^{\mu}(X^{\alpha}\_{T})]-\mathbb{E}[\log h^{\mu}(0,X^{\alpha}\_{0})]. |  |

The equality holds when Œ±t=Œ±tŒº=‚àálog‚Å°hŒº‚Äã(t,XtŒ±Œº)\alpha\_{t}=\alpha^{\mu}\_{t}=\nabla\log h^{\mu}(t,X^{\alpha^{\mu}}\_{t}), t‚àà[0,T]t\in[0,T] and XTŒ±Œº‚àºŒºX^{\alpha^{\mu}}\_{T}\sim\mu.
‚àé

From Proposition [5.2](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem2 "Proposition 5.2 ([18, Theorem 3.2]). ‚Ä£ 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") and Lemma [5.3](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem3 "Lemma 5.3 ([29, Lemma 3.1]). ‚Ä£ 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") we see that the density function œÅŒº\rho^{\mu} plays an important role in the structure of the solution of SBP. We shall be particularly interested in the continuous dependence of œÅŒº:=Œì1‚Äã(Œº)\rho^{\mu}:=\Gamma\_{1}(\mu) on Œº‚ààùí´2‚Äã(‚Ñùd)\mu\in\mathscr{P}\_{2}(\mathbb{R}^{d}), which we shall refer to as the Stability of the
SBP, borrowing the well-known concept of the SBP theory (cf. e.g., [[46](https://arxiv.org/html/2510.11829v1#bib.bib46), [22](https://arxiv.org/html/2510.11829v1#bib.bib22), [10](https://arxiv.org/html/2510.11829v1#bib.bib10)]). In light of ([6.4](https://arxiv.org/html/2510.11829v1#S6.E4 "In 6 Existence of optimal control and convergence for general ùúá·µ¢‚Çô·µ¢ ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")), we see that if both Œºini{\mu\_{\rm ini}} and Œº\mu have densities, then so does ùíØ‚Äã(Œºini,Œº){\cal T}({\mu\_{\rm ini}},\mu).
Furthermore, in light of ([5.3](https://arxiv.org/html/2510.11829v1#S5.E3 "In Proposition 5.2 ([18, Theorem 3.2]). ‚Ä£ 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")), for any œÅ‚ààùïÉ1‚Äã(‚Ñùd)\rho\in\mathbb{L}^{1}(\mathbb{R}^{d}), we define hœÅ‚Äã(t,x)=‚à´‚Ñùdp‚Äã(T,z;t,x)‚ÄãœÅ‚Äã(z)‚Äãùëëzh^{\rho}(t,x)=\int\_{\mathbb{R}^{d}}p(T,z;t,x)\rho(z)dz. Then clearly we have hŒº‚Äã(t,x)=hœÅŒº‚Äã(t,x)h^{\mu}(t,x)=h^{\rho^{\mu}}(t,x).

To continue our discussion, we shall identify a set ‚Ñ∞‚äÇùí´2‚Äã(‚Ñùd)\mathscr{E}\subset\mathscr{P}\_{2}(\mathbb{R}^{d})
on which an argument based on Schauder‚Äôs fixed-point theorem can be carried out. We begin by denoting

|  |  |  |
| --- | --- | --- |
|  | ùí¶:={Œº‚ààùí´2‚Äã(‚Ñùd):Œº¬†has density¬†fŒº‚ààùïÉ1‚Äã(‚Ñùd)}.\displaystyle{\cal K}:=\Big\{\mu\in\mathscr{P}\_{2}(\mathbb{R}^{d}):\mbox{$\mu$ has density $f\_{\mu}\in\mathbb{L}^{1}(\mathbb{R}^{d})$}\Big\}. |  |

Furthermore, we shall make use of the following assumption.

###### Assumption 5.4.

There exists a function g‚ààùïÉ2‚Äã(‚Ñùd;(0,1])g\in\mathbb{L}^{2}(\mathbb{R}^{d};(0,1]), with ‚à´‚Ñùd|x|2‚Äãg‚Äã(x)‚Äãùëës<‚àû\int\_{\mathbb{R}^{d}}|x|^{2}g(x)ds<\infty, and a constant K>0K>0, such that ‚ÄñfŒºg2‚Äñ‚àû‚â§K\Big\|\frac{f\_{\mu}}{g^{2}}\Big\|\_{\infty}\leq K, for all Œº‚ààùí¶\mu\in\mathcal{K}.

We shall consider the following two sets that will play a crucial role in our discussion.

|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚Ñ∞:={Œº‚ààùí¶:¬†Assumption¬†[5.4](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem4 "Assumption 5.4. ‚Ä£ 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")¬†holds}‚äÇùí´2‚Äã(‚Ñùd);ùíÆ‚Ñ∞:={fŒº:Œº‚àà‚Ñ∞}‚äÇùïÉ1‚Äã(‚Ñùd).\displaystyle\mathscr{E}:=\left\{\mu\in{\cal K}:\text{ Assumption \ref{assum:g} holds}\right\}\subset\mathscr{P}\_{2}(\mathbb{R}^{d});\quad{\cal S}\_{\mathscr{E}}:=\left\{f\_{\mu}:\mu\in\mathscr{E}\right\}\subset\mathbb{L}^{1}(\mathbb{R}^{d}). |  | (5.5) |

###### Remark 5.5.

A typical example of the function gg in Assumption [5.4](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem4 "Assumption 5.4. ‚Ä£ 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") is e‚àíc‚Äã|x|e^{-c|x|} or e‚àíc‚Äã|x|2e^{-c|x|^{2}}, x‚àà‚Ñùdx\in\mathbb{R}^{d}, c>0c>0. In such a case we see that part (i) holds for all p>0p>0. Assumption [5.4](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem4 "Assumption 5.4. ‚Ä£ 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") amounts to saying that we focus only on those density functions that have a similar decay rate to function gg at x‚àº‚àûx\sim\infty. In fact, in light of the estimate ([3.2](https://arxiv.org/html/2510.11829v1#S3.E2 "In 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")), such a property holds essentially for all transition probabilities of diffusion processes.
‚àé

The following lemma lists some basic properties of the set ‚Ñ∞\mathscr{E} (or Assumption [5.4](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem4 "Assumption 5.4. ‚Ä£ 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")).

###### Lemma 5.6.

Assume that Assumption [5.4](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem4 "Assumption 5.4. ‚Ä£ 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") is in force. Then it holds that

(i) The set {fŒº}Œº‚àà‚Ñ∞\{f\_{\mu}\}\_{\mu\in\mathscr{E}} is uniformly bounded in ùïÉ2‚Äã(‚Ñùd)\mathbb{L}^{2}(\mathbb{R}^{d}).

(ii) The set {fŒº}Œº‚àà‚Ñ∞\{f\_{\mu}\}\_{\mu\in\mathscr{E}} is uniformly integrable in ùí´2‚Äã(‚Ñùd)\mathscr{P}\_{2}(\mathbb{R}^{d}), in the sense that

|  |  |  |  |
| --- | --- | --- | --- |
|  | limR‚Üí‚àûsupŒº‚àà‚Ñ∞‚à´{|x|‚â•R}|x|2‚ÄãfŒº‚Äã(x)‚Äãùëëx=0.\displaystyle\lim\_{R\to\infty}\sup\_{\mu\in\mathscr{E}}\int\_{\{|x|\geq R\}}|x|^{2}f\_{\mu}(x)dx=0. |  | (5.6) |

(iii) If {Œºn}n‚â•1‚äÇ‚Ñ∞\{\mu\_{n}\}\_{{n\geq 1}}\subset\mathscr{E} such that Œºn‚áíŒº\mu\_{n}\Rightarrow\mu, as n‚Üí‚àûn\to\infty, then ‚ÄñfŒºn‚àífŒº‚ÄñùïÉ1‚Üí0\|f\_{\mu\_{n}}-f\_{\mu}\|\_{\mathbb{L}^{1}}\to 0.

###### Proof.

For any Œº‚àà‚Ñ∞\mu\in\mathscr{E}, we note that 0<g‚Äã(x)‚â§10<g(x)\leq 1, and by assumption,

|  |  |  |
| --- | --- | --- |
|  | ‚à´‚Ñùd|fŒº‚Äã(x)|2‚Äãùëëx‚â§K2‚Äã‚à´‚Ñùd|g‚Äã(x)|4‚Äãùëës‚â§K2‚Äã‚Äñg‚ÄñùïÉ22,\int\_{\mathbb{R}^{d}}|f\_{\mu}(x)|^{2}dx\leq K^{2}\int\_{\mathbb{R}^{d}}|g(x)|^{4}ds\leq K^{2}\|g\|^{2}\_{\mathbb{L}^{2}}, |  |

That is {fŒº}Œº‚àà‚Ñ∞\{f\_{\mu}\}\_{\mu\in\mathscr{E}} is uniformly bounded (by K‚Äã‚Äñg‚ÄñùïÉ2K\|g\|\_{\mathbb{L}^{2}}) in ùïÉ2‚Äã(‚Ñùd)\mathbb{L}^{2}(\mathbb{R}^{d}), proving (i).

Similarly, for any Œº‚àà‚Ñ∞\mu\in\mathscr{E}, by the absolute continuity of the integral we have

|  |  |  |
| --- | --- | --- |
|  | supŒº‚àà‚Ñ∞‚à´{|x|‚â•R}|x|2‚ÄãfŒº‚Äã(x)‚Äãùëëx‚â§K‚Äã‚à´{|x|‚â•R}|x|2‚Äãg‚Äã(x)‚Äãùëëx‚Üí0,as¬†R‚Üí‚àû,\displaystyle\sup\_{\mu\in\mathscr{E}}\int\_{\{|x|\geq R\}}|x|^{2}f\_{\mu}(x)dx\leq K\int\_{\{|x|\geq R\}}|x|^{2}g(x)dx\to 0,\quad\mbox{\rm as $R\to\infty$,} |  |

This proves ([5.6](https://arxiv.org/html/2510.11829v1#S5.E6 "In Lemma 5.6. ‚Ä£ 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")), whence (ii).

The proof of part (iii) is slightly more involved, which is in the spirit of the so-called Scheff√©‚Äôs theorem (cf. [[59](https://arxiv.org/html/2510.11829v1#bib.bib59)]).
We note that Œºn‚áíŒº\mu\_{n}\Rightarrow\mu amounts to saying that fŒºn‚Äã‚áÄwfŒºf\_{\mu\_{n}}\mathop{\mathrel{\mathop{\kern 0.0pt\rightharpoonup}\limits^{w}}}f\_{\mu}, as n‚Üí‚àûn\to\infty, in ùïÉ2‚Äã(‚Ñùd)\mathbb{L}^{2}(\mathbb{R}^{d}). To show fŒºn‚ÜífŒºf\_{\mu\_{n}}\to f\_{\mu} in ùïÉ1‚Äã(‚Ñùd)\mathbb{L}^{1}(\mathbb{R}^{d}), we first consider, for each m>0m>0, the smooth mollifiers œÜm‚àà‚ÑÇ‚àû‚Äã(‚Ñùd;‚Ñù+)\varphi^{m}\in\mathbb{C}^{\infty}(\mathbb{R}^{d};\mathbb{R}\_{+}) such that ‚à´‚ÑùdœÜm‚Äã(z)‚Äãùëëz=1\int\_{\mathbb{R}^{d}}\varphi^{m}(z)dz=1, m‚â•1m\geq 1, and denote

|  |  |  |
| --- | --- | --- |
|  | fŒºnm‚Äã(x)=[œÜm‚àófŒºn]‚Äã(y)=‚à´‚ÑùdœÜm‚Äã(x‚àíz)‚ÄãfŒºn‚Äã(z)‚Äãùëëz;fŒºm‚Äã(x)=[œÜm‚àófŒº]‚Äã(x),x‚àà‚Ñùd.f^{m}\_{\mu\_{n}}(x)=[\varphi^{m}\*f\_{\mu\_{n}}](y)=\int\_{\mathbb{R}^{d}}\varphi^{m}(x-z)f\_{\mu\_{n}}(z)dz;\quad f^{m}\_{\mu}(x)=[\varphi^{m}\*f\_{\mu}](x),\quad x\in\mathbb{R}^{d}. |  |

Then it is clear that for each n‚àà‚Ñïn\in\mathbb{N}, limm‚Üí‚àûfŒºnm‚Äã(x)=fŒºn‚Äã(x)\lim\_{m\to\infty}f^{m}\_{\mu\_{n}}(x)=f\_{\mu\_{n}}(x) and limm‚Üí‚àûfŒºm‚Äã(x)=fŒº‚Äã(x)\lim\_{m\to\infty}f^{m}\_{\mu}(x)=f\_{\mu}(x), for a.e. x‚àà‚Ñùdx\in\mathbb{R}^{d}. We should remark that the convergence is uniform in nn. Indeed, by Assumption [5.4](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem4 "Assumption 5.4. ‚Ä£ 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") and Dominated Convergence Theorem we have, as m‚Üí‚àûm\to\infty, for all n‚â•0n\geq 0,
and x‚àà‚Ñùdx\in\mathbb{R}^{d},

|  |  |  |
| --- | --- | --- |
|  | |fŒºnm‚Äã(x)‚àífŒºn‚Äã(x)|‚â§‚à´‚Ñùd|œÜm‚Äã(x‚àíz)‚àíŒ¥x‚Äã(z)|‚ÄãfŒºn‚Äã(z)‚Äãùëëz‚â§K‚Äã‚à´‚Ñùd|œÜm‚Äã(x‚àíz)‚àíŒ¥x‚Äã(z)|‚Äãg2‚Äã(z)‚Äãùëëz‚Üí0.|f^{m}\_{\mu\_{n}}(x)-f\_{\mu\_{n}}(x)|\leq\int\_{\mathbb{R}^{d}}|\varphi^{m}(x-z)-\delta\_{x}(z)|f\_{\mu\_{n}}(z)dz\leq K\int\_{\mathbb{R}^{d}}|\varphi^{m}(x-z)-\delta\_{x}(z)|g^{2}(z)dz\to 0. |  |

Furthermore, since supŒº‚àà‚Ñ∞|fŒº|‚â§K‚Äãg2‚ààùïÉ1‚Äã(‚Ñùd)\sup\_{\mu\in\mathscr{E}}|f\_{\mu}|\leq Kg^{2}\in\mathbb{L}^{1}(\mathbb{R}^{d}), by Dominated Convergence Theorem we have limm‚Üí‚àûfŒºnm=fŒºn\lim\_{m\to\infty}f^{m}\_{\mu\_{n}}=f\_{\mu\_{n}} in ùïÉ1‚Äã(‚Ñùd)\mathbb{L}^{1}(\mathbb{R}^{d}), uniformly for n‚â•0n\geq 0. That is, for any Œµ>0\varepsilon>0, there exists
M‚Äã(Œµ)>0M(\varepsilon)>0, such that for all n‚â•1n\geq 1, it holds that

|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚ÄñfŒºnm‚àífŒºn‚ÄñùïÉ1<Œµ3;‚ÄñfŒºm‚àífŒº‚ÄñùïÉ1<Œµ3,whenever¬†m>M.\displaystyle\|f^{m}\_{\mu\_{n}}-f\_{\mu\_{n}}\|\_{\mathbb{L}^{1}}<\frac{\varepsilon}{3};\quad\|f^{m}\_{\mu}-f\_{\mu}\|\_{\mathbb{L}^{1}}<\frac{\varepsilon}{3},\quad\mbox{whenever $m>M$.} |  | (5.7) |

In the sequel we fix m>M‚Äã(Œµ)m>M(\varepsilon), and take a closer look at the sequence {fŒºnm}n‚â•1\{f^{m}\_{\mu\_{n}}\}\_{n\geq 1}. Clearly, each fŒºnmf^{m}\_{\mu\_{n}} is still a density function, and it holds that

|  |  |  |  |
| --- | --- | --- | --- |
|  | supn|fŒºnm‚Äã(y)|‚â§supn(œÜm‚àó|fŒºn|)‚Äã(y)‚â§K.\displaystyle\sup\_{n}|f^{m}\_{\mu\_{n}}(y)|\leq\sup\_{n}(\varphi^{m}\*|f\_{\mu\_{n}}|)(y)\leq K. |  | (5.8) |

Moreover, since œÜm\varphi^{m} is continuous, thus for any x,y‚àà‚Ñùdx,y\in\mathbb{R}^{d},
applying the Dominated Convergence Theorem we have

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | |fŒºnm‚Äã(x+y)‚àífŒºnm‚Äã(x)|\displaystyle|f^{m}\_{\mu\_{n}}(x+y)-f^{m}\_{\mu\_{n}}(x)| | ‚â§\displaystyle\leq | ‚à´‚Ñùd|œÜm‚Äã(x+y‚àíz)‚àíœÜm‚Äã(x‚àíz)|‚Äã|fŒºn‚Äã(z)|‚Äãùëëz\displaystyle\int\_{\mathbb{R}^{d}}|\varphi^{m}(x+y-z)-\varphi^{m}(x-z)||f\_{\mu\_{n}}(z)|dz |  |
|  |  | ‚â§\displaystyle\leq | K‚Äã‚à´‚Ñùd|œÜm‚Äã(z‚Ä≤‚àíy)‚àíœÜm‚Äã(z‚Ä≤)|‚Äãùëëz‚Ä≤‚Üí0,as¬†y‚Üí0.\displaystyle K\int\_{\mathbb{R}^{d}}|\varphi^{m}(z^{\prime}-y)-\varphi^{m}(z^{\prime})|dz^{\prime}\to 0,\quad\mbox{as $y\to 0$.} |  |

Clearly, the convergence above is uniform in nn. That is, the sequence {fŒºnm}{n‚â•1}\{f^{m}\_{\mu\_{n}}\}\_{\{n\geq 1\}} is so-called asymptotically equi-continuous in the sense of Sweeting [[59](https://arxiv.org/html/2510.11829v1#bib.bib59)]. This, together with ([5.8](https://arxiv.org/html/2510.11829v1#S5.E8 "In 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")), implies that limn‚Üí‚àûfŒºnm=fŒºm\displaystyle\lim\_{n\to\infty}f^{m}\_{\mu\_{n}}=f^{m}\_{\mu}, uniformly on compacts in ‚Ñùd\mathbb{R}^{d} (cf. [[59](https://arxiv.org/html/2510.11829v1#bib.bib59), Theorem 1]). Applying the Dominated Convergence Theorem again we have limn‚Üí‚àû‚ÄñfŒºnm‚àífŒºm‚ÄñùïÉ1=0\lim\_{n\to\infty}\|f^{m}\_{\mu\_{n}}-f^{m}\_{\mu}\|\_{\mathbb{L}^{1}}=0. That is, for the given Œµ>0\varepsilon>0 in ([5.7](https://arxiv.org/html/2510.11829v1#S5.E7 "In 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")), there exists N>0N>0 such that
‚ÄñfŒºnm‚àífŒºm‚ÄñùïÉ1<Œµ3\|f^{m}\_{\mu\_{n}}-f^{m}\_{\mu}\|\_{\mathbb{L}^{1}}<\frac{\varepsilon}{3}, whenever n>Nn>N. This, together with ([5.7](https://arxiv.org/html/2510.11829v1#S5.E7 "In 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")), yields

|  |  |  |
| --- | --- | --- |
|  | ‚ÄñfŒºn‚àífŒº‚ÄñùïÉ1‚â§‚ÄñfŒºn‚àífŒºnm‚ÄñùïÉ1+‚ÄñfŒºnm‚àífŒºm‚ÄñùïÉ1+‚ÄñfŒºm‚àífŒº‚ÄñùïÉ1<Œµ3+Œµ3+Œµ3=Œµ,n>N,\|f\_{\mu\_{n}}-f\_{\mu}\|\_{\mathbb{L}^{1}}\leq\|f\_{\mu\_{n}}-f^{m}\_{\mu\_{n}}\|\_{\mathbb{L}^{1}}+\|f^{m}\_{\mu\_{n}}-f^{m}\_{\mu}\|\_{\mathbb{L}^{1}}+\|f^{m}\_{\mu}-f\_{\mu}\|\_{\mathbb{L}^{1}}<\frac{\varepsilon}{3}+\frac{\varepsilon}{3}+\frac{\varepsilon}{3}=\varepsilon,\quad n>N, |  |

proving (iii), whence the Lemma.
‚àé

We are now ready to study our main stability result. More precisely, we shall argue that the mapping Œì1:ùí´2‚Äã(‚Ñùd)‚Ü¶ùïÉ1‚Äã(‚Ñùd)\Gamma\_{1}:\mathscr{P}\_{2}(\mathbb{R}^{d})\mapsto\mathbb{L}^{1}(\mathbb{R}^{d}) is continuous. That is, that Œºn\mu\_{n} weakly converges to Œºtar{\mu\_{\rm tar}} in Prohorov metric would imply that œÅŒºn\rho^{\mu\_{n}} converges to œÅŒºtar\rho^{{\mu\_{\rm tar}}} in ùïÉ1‚Äã(‚Ñùd)\mathbb{L}^{1}(\mathbb{R}^{d}). Such a result, to the best of our knowledge, is novel in the literature.

To simplify our discussion, in what follows, we assume T=1T=1, and denote the measure Œº\mu in ([5.1](https://arxiv.org/html/2510.11829v1#S5.E1 "In Proposition 5.1 ([7]). ‚Ä£ 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) by œÄ\pi for notational clarity. Recall that œÄ\pi has marginals Œºini{\mu\_{\rm ini}} and Œº\mu, and in what follows, we shall assume that Œºini{\mu\_{\rm ini}} is fixed and Œº‚àà‚Ñ∞\mu\in\mathscr{E}. Let us now consider the following entropic optimal transport problem:

|  |  |  |  |
| --- | --- | --- | --- |
|  | I‚Äã(Œº):=infœÄ‚ààŒ†‚Äã(Œºini,Œº)‚à´‚Ñùd√ó‚Ñùdc‚Äã(x,y)‚ÄãœÄ‚Äã(d‚Äãx‚Äãd‚Äãy)+DKL‚Äã(œÄ‚à•Œºini‚äóŒº),I(\mu):=\inf\_{\pi\in\Pi({\mu\_{\rm ini}},\mu)}\int\_{\mathbb{R}^{d}\times\mathbb{R}^{d}}{\textbf{c}}(x,y)\pi(dxdy)+D\_{\rm KL}(\pi\|{\mu\_{\rm ini}}\otimes\mu), |  | (5.9) |

where Œ†‚Äã(Œºini,Œº)\Pi({\mu\_{\rm ini}},\mu) is the set of all coupling probability measures œÄ\pi on ‚Ñùd√ó‚Ñùd\mathbb{R}^{d}\times\mathbb{R}^{d} with marginals Œºini{\mu\_{\rm ini}} and Œº\mu; and c‚Äã(‚ãÖ,‚ãÖ){\textbf{c}}(\cdot,\cdot) is a continuous cost function. It is well-known (see, e.g., [[22](https://arxiv.org/html/2510.11829v1#bib.bib22), [16](https://arxiv.org/html/2510.11829v1#bib.bib16), [31](https://arxiv.org/html/2510.11829v1#bib.bib31), [46](https://arxiv.org/html/2510.11829v1#bib.bib46)]) that the minimization ([5.9](https://arxiv.org/html/2510.11829v1#S5.E9 "In 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) admits a unique solution œÄ^\widehat{\pi}, whose density takes the form:

|  |  |  |  |
| --- | --- | --- | --- |
|  | œÄ^‚Äã(d‚Äãx‚Äãd‚Äãy)=exp‚Å°(‚àíc‚Äã(x,y)+œïŒº‚Äã(x)+œàŒº‚Äã(y))‚ÄãŒºini‚Äã(d‚Äãx)‚ÄãŒº‚Äã(d‚Äãy),\widehat{\pi}(dxdy)=\exp\big(-{\textbf{c}}(x,y)+\phi^{\mu}(x)+\psi^{\mu}(y)\big){\mu\_{\rm ini}}(dx)\mu(dy), |  | (5.10) |

where œïŒº,œàŒº\phi^{\mu},\psi^{\mu}: ‚Ñùd‚Üí‚Ñù\mathbb{R}^{d}\to\mathbb{R} are two measurable functions, often referred to as the Schr√∂dinger potentials. It is clear that the pair (œïŒº,œàŒº)(\phi^{\mu},\psi^{\mu}) is unique up to an additive constant. That is, if (œïŒº,œàŒº)(\phi^{\mu},\psi^{\mu}) is a pair of Schr√∂dinger potentials, then so is (œïŒº+c,œàŒº‚àíc)(\phi^{\mu}+c,\psi^{\mu}-c). Furthermore, since both Œºini{\mu\_{\rm ini}} and Œº\mu are probability measures, we can easily choose a constant cc so that
the following symmetric normalization holds:

|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚à´œïŒº‚Äã(x)‚ÄãŒºini‚Äã(d‚Äãx)=‚à´œàŒº‚Äã(y)‚ÄãŒº‚Äã(d‚Äãy).\displaystyle\int\phi^{\mu}(x){\mu\_{\rm ini}}(dx)=\int\psi^{\mu}(y)\mu(dy). |  | (5.11) |

(Otherwise we take c=12‚Äã[‚àí‚à´œïŒº‚Äã(x)‚ÄãŒºini‚Äã(d‚Äãx)+‚à´œàŒº‚Äã(y)‚ÄãŒº‚Äã(d‚Äãy)]c=\frac{1}{2}\big[-\int\phi^{\mu}(x){\mu\_{\rm ini}}(dx)+\int\psi^{\mu}(y)\mu(dy)\big].) Note that under the symmetric normalization, the Schr√∂dinger potentials is unique. The following stability result for the mappings Œº‚Ü¶(œïŒº,œàŒº)\mu\mapsto(\phi^{\mu},\psi^{\mu}) is crucial for our discussion.

###### Lemma 5.7 ([[10](https://arxiv.org/html/2510.11829v1#bib.bib10), Theorem 1.1]).

Assume that the cost function c‚Äã(‚ãÖ,‚ãÖ)‚àà‚ÑÇk+1‚Äã(‚Ñùd√ó‚Ñùd){\textbf{c}}(\cdot,\cdot)\in\mathbb{C}^{k+1}(\mathbb{R}^{d}\times\mathbb{R}^{d}) for some k‚àà‚Ñïk\in\mathbb{N}. Then there exists C>0C>0 depending only on ‚Äñc‚Äñ‚ÑÇk+1\|{\textbf{c}}\|\_{\mathbb{C}^{k+1}}, such that for all Œº1\mu\_{1}, Œº2‚ààùí´2‚Äã(‚Ñùd)\mu\_{2}\in\mathscr{P}\_{2}(\mathbb{R}^{d}), it holds that

|  |  |  |
| --- | --- | --- |
|  | ‚Äñ(œïŒº1‚àíœïŒº2,œàŒº1‚àíœàŒº2)‚Äñ‚àó‚â§C‚ÄãW2‚Äã(Œº1,Œº2),\|(\phi^{\mu\_{1}}-\phi^{\mu\_{2}},\psi^{\mu\_{1}}-\psi^{\mu\_{2}})\|\_{\*}\leq CW\_{2}(\mu\_{1},\ \mu\_{2}), |  |

where ‚Äñ(œï,œà)‚Äñ‚àó:=infc‚àà‚Ñù{‚Äñœï‚àíc‚Äñ‚ÑÇk‚Äã(‚Ñùd)+‚Äñœà+c‚Äñ‚ÑÇk‚Äã(‚Ñùd)}\|(\phi,\psi)\|\_{\*}:=\inf\_{c\in\mathbb{R}}\left\{\|\phi-c\|\_{\mathbb{C}^{k}(\mathbb{R}^{d})}+\|\psi+c\|\_{\mathbb{C}^{k}(\mathbb{R}^{d})}\right\}.
‚àé

We now proceed to prove the main result of this section. To begin with, let us consider the entropic optimal transport problem ([5.9](https://arxiv.org/html/2510.11829v1#S5.E9 "In 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) with c‚Äã(x,y):=‚àílog‚Å°p‚Äã(1,y;0,x){\textbf{c}}(x,y):=-\log p(1,y;0,x), x,y‚àà‚Ñùdx,y\in\mathbb{R}^{d}, where p‚Äã(s,y;t,x)p(s,y;t,x), 0‚â§t<s‚â§10\leq t<s\leq 1 and x,y‚àà‚Ñùdx,y\in\mathbb{R}^{d}, is the transition density of the diffusion ([3.1](https://arxiv.org/html/2510.11829v1#S3.E1 "In 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")). By ([5.10](https://arxiv.org/html/2510.11829v1#S5.E10 "In 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")), for fixed Œºini,Œº‚ààùí´2‚Äã(‚Ñùd){\mu\_{\rm ini}},\mu\in\mathscr{P}\_{2}(\mathbb{R}^{d}), the unique solution for this entropic optimal transport problem is given by (see also [[46](https://arxiv.org/html/2510.11829v1#bib.bib46)])

|  |  |  |
| --- | --- | --- |
|  | œÄ^‚Äã(d‚Äãx‚Äãd‚Äãy)=p‚Äã(1,y;0,x)‚ÄãeœïŒº‚Äã(x)+œàŒº‚Äã(y)‚Äãf0‚Äã(x)‚ÄãfŒº‚Äã(y)‚Äãd‚Äãx‚Äãd‚Äãy,\displaystyle\widehat{\pi}(dxdy)=p(1,y;0,x)e^{\phi^{\mu}(x)+\psi^{\mu}(y)}f\_{0}(x)f\_{\mu}(y)dxdy, |  |

where œïŒº\phi^{\mu} and œàŒº\psi^{\mu} are the Schr√∂dinger potentials, and we shall enforce the symmetric normalization so that
they satisfy ([5.11](https://arxiv.org/html/2510.11829v1#S5.E11 "In 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")). Since œÄ^\widehat{\pi} has the marginals Œºini{\mu\_{\rm ini}} and Œº\mu, by the uniqueness of (ŒΩ0,ŒΩ)=ùíØ‚Äã(Œºini,Œº)(\nu\_{0},\nu)={\cal T}({\mu\_{\rm ini}},\mu), whence (œÅ0,œÅŒº)(\rho\_{0},\rho^{\mu}), in Proposition [5.1](https://arxiv.org/html/2510.11829v1#S5.E1 "In Proposition 5.1 ([7]). ‚Ä£ 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday"), we can conclude that

|  |  |  |
| --- | --- | --- |
|  | p‚Äã(1,y;0,x)‚ÄãœÅ0‚Äã(x)‚ÄãœÅŒº‚Äã(y)=p‚Äã(1,y;0,x)‚ÄãeœïŒº‚Äã(x)+œàŒº‚Äã(y)‚Äãf0‚Äã(x)‚ÄãfŒº‚Äã(y),x,y‚àà‚Ñùd.\displaystyle p(1,y;0,x)\rho\_{0}(x)\rho^{\mu}(y)=p(1,y;0,x)e^{\phi^{\mu}(x)+\psi^{\mu}(y)}f\_{0}(x)f\_{\mu}(y),\qquad x,y\in\mathbb{R}^{d}. |  |

An easy argument of separation of variables then yields that

|  |  |  |  |
| --- | --- | --- | --- |
|  | œÅŒº‚Äã(y)=eœàŒº‚Äã(y)‚ÄãfŒº‚Äã(y);œÅ0‚Äã(x)=eœïŒº‚Äã(x)‚Äãf0‚Äã(x),x,y‚àà‚Ñùd.\displaystyle\rho^{\mu}(y)=e^{\psi^{\mu}(y)}f\_{\mu}(y);\quad\rho\_{0}(x)=e^{\phi^{\mu}(x)}f\_{0}(x),\qquad x,y\in\mathbb{R}^{d}. |  | (5.12) |

Now note that the transition density p‚Äã(‚ãÖ,‚ãÖ;‚ãÖ,‚ãÖ)p(\cdot,\cdot\,;\cdot,\cdot) is a classical solution to the Kolmogorov PDE. Thanks to Assumption [2.2](https://arxiv.org/html/2510.11829v1#S2.ThmTheorem2 "Assumption 2.2. ‚Ä£ Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday"), we can assume without loss of generality that c‚Äã(‚ãÖ,‚ãÖ)=‚àílog‚Å°p‚Äã(1,‚ãÖ;0,‚ãÖ)‚àà‚ÑÇ2‚Äã(‚Ñùd√ó‚Ñùd){\textbf{c}}(\cdot,\cdot)=-\log p(1,\cdot\,;0,\cdot)\in\mathbb{C}^{2}(\mathbb{R}^{d}\times\mathbb{R}^{d}). Thus
according to Lemma [5.7](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem7 "Lemma 5.7 ([10, Theorem 1.1]). ‚Ä£ 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") and noting the definition of ‚à•‚ãÖ‚à•‚àó\|\cdot\|\_{\*}, we see that, modulo some constant normalization, we have that the
Schr√∂dinger potential (œïŒºn,œàŒºn)(\phi^{\mu\_{n}},\psi^{\mu\_{n}}) itself satisfies the estimate:

|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚ÄñœïŒºn‚àíœïŒº‚ÄñùïÉ‚àû+‚ÄñœàŒºn‚àíœàŒº‚ÄñùïÉ‚àû‚â§C‚ÄãW2‚Äã(Œºn,Œº).\|{\phi}^{\mu\_{n}}-\phi^{\mu}\|\_{\mathbb{L}^{\infty}}+\|{\psi}^{\mu\_{n}}-\psi^{\mu}\|\_{\mathbb{L}^{\infty}}\leq CW\_{2}(\mu\_{n},\mu). |  | (5.13) |

Here in the above the constant C>0C>0 depending only on ‚Äñc‚Äñ‚ÑÇ2\|{\textbf{c}}\|\_{\mathbb{C}^{2}}, but independent of nn.

Furthermore, we note that c‚àà‚ÑÇ2{\textbf{c}}\in\mathbb{C}^{2} also lead to the following a priori estimate of the Schr√∂dinger potential (see, e.g., [[46](https://arxiv.org/html/2510.11829v1#bib.bib46), Lemma 2.1]):

|  |  |  |  |
| --- | --- | --- | --- |
|  | œàŒº(y)‚â§‚à´‚Ñùdc(x,y)Œºini(dx)=:Œæ(y),y‚àà‚Ñùd.\displaystyle\psi^{\mu}(y)\leq\int\_{\mathbb{R}^{d}}{\textbf{c}}(x,y){\mu\_{\rm ini}}(dx)=:\xi(y),\quad y\in\mathbb{R}^{d}. |  | (5.14) |

Recall the fundamental estimate ([3.2](https://arxiv.org/html/2510.11829v1#S3.E2 "In 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) and the definition of c‚Äã(‚ãÖ,‚ãÖ){\textbf{c}}(\cdot,\cdot), it is readily seen that Œæ‚Äã(y)‚àºŒª‚Äã|y|2\xi(y)\sim\lambda|y|^{2}, as y‚Üí‚àûy\to\infty, for some constant Œª>0\lambda>0 depending only on the coefficient b‚Äã(‚ãÖ,‚ãÖ)b(\cdot,\cdot) in SDE([3.1](https://arxiv.org/html/2510.11829v1#S3.E1 "In 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")). In light of Remark [5.5](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem5 "Remark 5.5. ‚Ä£ 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday"), we shall now assume, without loss of generality, that in Assumption [5.4](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem4 "Assumption 5.4. ‚Ä£ 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") the control function gg satisfies

|  |  |  |  |
| --- | --- | --- | --- |
|  | Œ∑‚Äã(‚ãÖ):=eŒæ‚Äã(‚ãÖ)‚Äãg2‚Äã(‚ãÖ)‚ààùïÉ1‚Äã(‚Ñùd).\displaystyle\eta(\cdot):=e^{\xi(\cdot)}g^{2}(\cdot)\in\mathbb{L}^{1}(\mathbb{R}^{d}). |  | (5.15) |

Now for any fŒº‚ààùíÆ‚Ñ∞f\_{\mu}\in{\cal S}\_{\mathscr{E}}, by Assumption [5.4](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem4 "Assumption 5.4. ‚Ä£ 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") and ([5.14](https://arxiv.org/html/2510.11829v1#S5.E14 "In 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) we have

|  |  |  |
| --- | --- | --- |
|  | 0‚â§œÅŒº‚Äã(y)=eœàŒº‚Äã(y)‚ÄãfŒº‚Äã(y)‚â§eŒæ‚Äã(y)‚ÄãfŒº‚Äã(y)‚â§eŒæ‚Äã(y)‚ÄãK‚Äãg2‚Äã(y)‚â§K‚ÄãŒ∑‚Äã(y),y‚àà‚Ñùd.0\leq\rho^{\mu}(y)=e^{\psi^{\mu}(y)}\ f\_{\mu}(y)\leq e^{\xi(y)}f\_{\mu}(y)\leq e^{\xi(y)}Kg^{2}(y)\leq K\eta(y),\quad y\in\mathbb{R}^{d}. |  |

Consequently, we conclude that œÅŒº‚ààùïÉ1‚Äã(‚Ñùd)\rho^{\mu}\in\mathbb{L}^{1}(\mathbb{R}^{d}) for any Œº‚àà‚Ñ∞\mu\in\mathscr{E}, thanks to ([5.15](https://arxiv.org/html/2510.11829v1#S5.E15 "In 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")).

Bearing the above discussion in mind, we are now ready to present the main result of this section.

###### Proposition 5.8.

Assume that Assumptions [2.2](https://arxiv.org/html/2510.11829v1#S2.ThmTheorem2 "Assumption 2.2. ‚Ä£ Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") and [5.4](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem4 "Assumption 5.4. ‚Ä£ 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") are in force. Assume further that {Œºn}n‚â•1‚äÇ‚Ñ∞\{\mu\_{n}\}\_{n\geq 1}\subset\mathscr{E} and Œºn‚üπŒº\mu\_{n}\Longrightarrow\mu in Prohorov metric. Then ‚ÄñœÅŒºn‚àíœÅŒº‚ÄñùïÉ1=‚ÄñŒì1‚Äã(Œºn)‚àíŒì1‚Äã(Œº)‚ÄñùïÉ1‚Üí0\|\rho^{\mu\_{n}}-\rho^{\mu}\|\_{\mathbb{L}^{1}}=\|\Gamma\_{1}(\mu\_{n})-\Gamma\_{1}(\mu)\|\_{\mathbb{L}^{1}}\to 0, as n‚Üí‚àûn\to\infty.

###### Proof.

Assume {Œºn}n‚â•1‚äÇ‚Ñ∞\{\mu\_{n}\}\_{n\geq 1}\subset\mathscr{E}, and Œºn‚üπŒº\mu\_{n}\Longrightarrow\mu, in Prohorov metric. By Lemma [5.6](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem6 "Lemma 5.6. ‚Ä£ 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")-(ii), {Œºn}\{\mu\_{n}\} is uniformly integrable in ùïÉ2\mathbb{L}^{2}, thanks to Assumption [5.4](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem4 "Assumption 5.4. ‚Ä£ 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday"), and thus by the relationship between Wasserstein distance and Prohorov metric (see, [[65](https://arxiv.org/html/2510.11829v1#bib.bib65), Theorem 7.12]), we have W2‚Äã(Œºn,Œº)‚Üí0W\_{2}(\mu\_{n},\mu)\to 0, as n‚Üí‚àûn\to\infty. Thus, if follows from ([5.13](https://arxiv.org/html/2510.11829v1#S5.E13 "In 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) that ‚ÄñœàŒºn‚àíœàŒº‚Äñ‚àû‚Üí0\|\psi^{\mu\_{n}}-\psi^{\mu}\|\_{\infty}\to 0, as n‚Üí‚àûn\to\infty.

Next, for each Œºn‚àà‚Ñ∞\mu\_{n}\in\mathscr{E}, n‚â•1n\geq 1, and Œº\mu, we apply ([5.12](https://arxiv.org/html/2510.11829v1#S5.E12 "In 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) and write

|  |  |  |
| --- | --- | --- |
|  | œÅŒºn‚Äã(y)=eœàŒºn‚Äã(y)‚ÄãfŒºn‚Äã(y),œÅŒº‚Äã(y)=eœàŒº‚Äã(y)‚ÄãfŒºtar‚Äã(y),x,y‚àà‚Ñùd.\displaystyle\rho^{\mu\_{n}}(y)=e^{\psi^{\mu\_{n}}(y)}f\_{\mu\_{n}}(y),\quad\rho^{\mu}(y)=e^{\psi^{\mu}(y)}f\_{{\mu\_{\rm tar}}}(y),\qquad x,y\in\mathbb{R}^{d}. |  |

Therefore, for y‚àà‚Ñùdy\in\mathbb{R}^{d}, we have

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | |œÅŒº‚Äã(y)‚àíœÅŒºn‚Äã(y)|\displaystyle|\rho^{\mu}(y)-\rho^{\mu\_{n}}(y)| | =\displaystyle= | |eœàŒº‚Äã(y)‚ÄãfŒº‚Äã(y)‚àíeœàŒºn‚Äã(y)‚ÄãfŒºn‚Äã(y)|\displaystyle\big|e^{\psi^{\mu}(y)}f\_{\mu}(y)-e^{\psi^{\mu\_{n}}(y)}f\_{\mu\_{n}}(y)\big| |  |
|  |  | ‚â§\displaystyle\leq | |eœàŒº‚Äã(y)‚àíeœàŒºn‚Äã(y)|fŒºn(y)+eœàŒº‚Äã(y)|fŒºn(y)‚àífŒº(y)|=:In1(y)+In2(y),\displaystyle\big|e^{\psi^{\mu}(y)}-e^{\psi^{\mu\_{n}}(y)}\big|f\_{\mu\_{n}}(y)+e^{\psi^{\mu}(y)}\big|f\_{\mu\_{n}}(y)-f\_{\mu}(y)\big|=:I^{1}\_{n}(y)+I\_{n}^{2}(y), |  |

where IniI^{i}\_{n}, i=1,2i=1,2 are defined in an obvious way. It then suffices to show that both In1I^{1}\_{n} and In2‚Üí0I^{2}\_{n}\to 0 in ùïÉ1\mathbb{L}^{1}, as n‚Üí‚àûn\to\infty.

To this end, we first recall that ‚ÄñœàŒºn‚àíœàŒº‚Äñ‚àû‚Üí0\|\psi^{\mu\_{n}}-\psi^{\mu}\|\_{\infty}\to 0, as n‚Üí‚àûn\to\infty. Hence there exists N>0N>0, such that œàŒºn‚Äã(y)‚â§œàŒº‚Äã(y)+1\psi^{\mu\_{n}}(y)\leq\psi^{\mu}(y)+1, for all y‚àà‚Ñùdy\in\mathbb{R}^{d}, whenever n‚â•Nn\geq N.
Thus, for n‚â•Nn\geq N, we have

|  |  |  |
| --- | --- | --- |
|  | 0‚â§In1‚Äã(y)‚â§(|eœàŒº‚Äã(y)|+|eœàŒºn‚Äã(y)|)‚ÄãfŒºn‚Äã(y)‚â§2‚ÄãeœàŒº‚Äã(y)+1‚ÄãfŒºn‚Äã(y)‚â§2‚Äãe‚ãÖeŒæ‚Äã(y)‚Äãg2‚Äã(y)=2‚Äãe‚ÄãŒ∑‚Äã(y).0\leq I\_{n}^{1}(y)\leq\big(\big|e^{\psi^{\mu}(y)}\big|+\big|e^{\psi^{\mu\_{n}}(y)}\big|\big)f\_{\mu\_{n}}(y)\leq 2e^{\psi^{\mu}(y)+1}f\_{\mu\_{n}}(y)\leq 2e\cdot e^{\xi(y)}g^{2}(y)=2e\eta(y). |  |

Here in the above, the last inequality holds due to Assumption [5.4](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem4 "Assumption 5.4. ‚Ä£ 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") and ([5.15](https://arxiv.org/html/2510.11829v1#S5.E15 "In 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")). Since Œ∑‚ààùïÉ1\eta\in\mathbb{L}^{1} by ([5.15](https://arxiv.org/html/2510.11829v1#S5.E15 "In 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")), the Dominated Convergence Theorem implies that In1‚Äã(‚ãÖ)I\_{n}^{1}(\cdot) converges to 0 in ùïÉ1‚Äã(‚Ñù2)\mathbb{L}^{1}(\mathbb{R}^{2}) as n‚Üí‚àûn\to\infty, because œàŒºn{\psi^{\mu\_{n}}} converges uniformly to œàŒº\psi^{\mu} on ‚Ñùd\mathbb{R}^{d}.

Finally, since In2‚Äã(y)‚â§2‚ÄãŒ∑‚Äã(y)I\_{n}^{2}(y)\leq 2\eta(y), and fŒºn‚ÜífŒºf\_{\mu\_{n}}\to f\_{\mu} in ùïÉ1‚Äã(‚Ñùd)\mathbb{L}^{1}(\mathbb{R}^{d}), thanks to Lemma [5.6](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem6 "Lemma 5.6. ‚Ä£ 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")-(iii), we can apply Dominated Convergence again to get In2‚Äã(‚ãÖ)I\_{n}^{2}(\cdot) converges to 0 in ùïÉ1‚Äã(‚Ñù2)\mathbb{L}^{1}(\mathbb{R}^{2}), as n‚Üí‚àûn\to\infty, proving the proposition.
‚àé

## 6 Existence of optimal control and convergence for general Œºini{\mu\_{\rm ini}}

In this section, we shall extend the results of ¬ß3 and show that the Problem [2.4](https://arxiv.org/html/2510.11829v1#S2.ThmTheorem4 "Problem 2.4 (Soft-constrained Schr√∂dinger Bridge Problem (SCSBP)). ‚Ä£ Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") has solution for each k‚àà‚Ñïk\in\mathbb{N} when Œºini{\mu\_{\rm ini}} is an arbitrary distribution in ùí´2‚Äã(‚Ñùd)\mathscr{P}\_{2}(\mathbb{R}^{d}). To be more precise, for fixed k‚àà‚Ñïk\in\mathbb{N},
let Jk‚Äã(Œ±)J^{k}(\alpha) be the cost functional in Problem [2.4](https://arxiv.org/html/2510.11829v1#S2.ThmTheorem4 "Problem 2.4 (Soft-constrained Schr√∂dinger Bridge Problem (SCSBP)). ‚Ä£ Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday"). Applying Lemma [5.3](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem3 "Lemma 5.3 ([29, Lemma 3.1]). ‚Ä£ 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday"), for any Œº‚ààùíü\mu\in\mathscr{D}, we have

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | Jk‚Äã(Œ±)=ùîº‚Äã[12‚Äã‚à´0T|Œ±s|2‚Äãùëës+k‚ÄãG‚Äã(‚ÑôXTŒ±;Œºtar)]‚â•k‚ÄãG‚Äã(‚ÑôXTŒ±;Œºtar)+ùîº‚Äã[log‚Å°œÅŒº‚Äã(XTŒ±)]‚àíùîº‚Äã[log‚Å°hŒº‚Äã(0,X0Œ±)]\displaystyle\begin{split}J^{k}(\alpha)&=\mathbb{E}\Big[\frac{1}{2}\int\_{0}^{T}|\alpha\_{s}|^{2}ds+kG(\mathbb{P}\_{X^{\alpha}\_{T}};{\mu\_{\rm tar}})\Big]\\ &\geq kG(\mathbb{P}\_{X^{\alpha}\_{T}};{\mu\_{\rm tar}})+\mathbb{E}[\log\rho^{\mu}(X^{\alpha}\_{T})]-\mathbb{E}[\log h^{\mu}(0,X^{\alpha}\_{0})]\end{split} | |  | (6.1) |

and the equality holds when Œ±t=Œ±tŒº=‚àálog‚Å°hŒº‚Äã(t,XtŒ±Œº)\alpha\_{t}=\alpha^{\mu}\_{t}=\nabla\log h^{\mu}(t,X^{\alpha^{\mu}}\_{t}) and XTŒ±Œº‚àºŒºX^{\alpha^{\mu}}\_{T}\sim\mu. Our main goal of this
section is to determine the density œÅ^‚Äã(‚ãÖ)\widehat{\rho}(\cdot), such that Œ±^t=‚àálog‚Å°hŒ±^‚Äã(t,XtŒ±^)\widehat{\alpha}\_{t}=\nabla\log h^{\widehat{\alpha}}(t,X^{\widehat{\alpha}}\_{t}) is the optimal control to Problem [2.4](https://arxiv.org/html/2510.11829v1#S2.ThmTheorem4 "Problem 2.4 (Soft-constrained Schr√∂dinger Bridge Problem (SCSBP)). ‚Ä£ Schr√∂dinger Bridge and Related Control Problem. ‚Ä£ 2 Preliminary ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday"), where hŒ±^‚Äã(t,x)=ùîºt,x‚Äã[œÅ^‚Äã(XT)]:=ùîº‚Äã[œÅ^‚Äã(XT)|Xt=x]h^{\widehat{\alpha}}(t,x)=\mathbb{E}\_{t,x}[\widehat{\rho}(X\_{T})]:=\mathbb{E}[\widehat{\rho}(X\_{T})|X\_{t}=x].

Before we proceed, let us first introduce some notations. First, for any Œº‚ààùí´2‚Äã(‚Ñùd)\mu\in\mathscr{P}\_{2}(\mathbb{R}^{d}), we denote fŒºf\_{\mu} to be its density function, whenever exists. In particular, we define f0=fŒºinif\_{0}=f\_{{\mu\_{\rm ini}}}.
To be consistent with the previous discussions, we recall the mapping (ŒΩ0,ŒΩT)=ùíØ‚Äã(Œºini,Œº)(\nu\_{0},\nu\_{T})={\cal T}({\mu\_{\rm ini}},\mu) for Œºini,Œº‚ààùí´2‚Äã(‚Ñùd){\mu\_{\rm ini}},\mu\in\mathscr{P}\_{2}(\mathbb{R}^{d}), and for fixed Œºini{\mu\_{\rm ini}}, we denote fŒΩ0=œÅ0f\_{\nu\_{0}}=\rho\_{0} and fŒΩT=œÅŒº=Œì1‚Äã(Œº)f\_{\nu\_{T}}=\rho^{\mu}=\Gamma\_{1}(\mu). Furthermore, from ([5.1](https://arxiv.org/html/2510.11829v1#S5.E1 "In Proposition 5.1 ([7]). ‚Ä£ 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) we see that, for any Œºini,Œº‚ààùí´2‚Äã(‚Ñùd){\mu\_{\rm ini}},\mu\in\mathscr{P}\_{2}(\mathbb{R}^{d})

|  |  |  |
| --- | --- | --- |
|  | Œºini‚Äã(d‚Äãx)ŒΩ0‚Äã(d‚Äãx)=f0‚Äã(x)œÅ0‚Äã(x)=‚à´‚ÑùdP‚Äã(T,y;0,x)‚ÄãœÅŒº‚Äã(y)‚Äãùëëy,Œº‚Äã(d‚Äãy)ŒΩT‚Äã(d‚Äãy)=fŒº‚Äã(y)œÅŒº‚Äã(y)=‚à´‚Ñùdp‚Äã(T,y;0,x)‚ÄãœÅ0‚Äã(x)‚Äãùëëx.\displaystyle\frac{{\mu\_{\rm ini}}(dx)}{\nu\_{0}(dx)}=\frac{f\_{0}(x)}{\rho\_{0}(x)}=\int\_{\mathbb{R}^{d}}P(T,y;0,x)\rho^{\mu}(y)dy,\quad\frac{\mu(dy)}{\nu\_{T}(dy)}=\frac{f\_{\mu}(y)}{\rho^{\mu}(y)}=\int\_{\mathbb{R}^{d}}p(T,y;0,x)\rho\_{0}(x)dx. |  |

In other words, we can write

|  |  |  |  |
| --- | --- | --- | --- |
|  | {œÅ0‚Äã(x)=f0‚Äã(x)‚à´‚ÑùdP‚Äã(T,y;0,x)‚ÄãœÅŒº‚Äã(y)‚Äãùëëy=f0‚Äã(x)hŒº‚Äã(0,x),œÅŒº‚Äã(y)=fŒº‚Äã(y)‚à´‚Ñùdp‚Äã(T,y;0,x)‚ÄãœÅ0‚Äã(x)‚Äãùëëx=fŒº‚Äã(y)‚à´‚Ñùdp‚Äã(T,y;0,x)‚Äãf0‚Äã(x)hŒº‚Äã(0,x)‚Äãùëëx.\displaystyle\left\{\begin{array}[]{lll}\displaystyle\rho\_{0}(x)=\frac{f\_{0}(x)}{\int\_{\mathbb{R}^{d}}P(T,y;0,x)\rho^{\mu}(y)dy}=\frac{f\_{0}(x)}{h^{\mu}(0,x)}\vskip 6.0pt plus 2.0pt minus 2.0pt,\\ \displaystyle\rho^{\mu}(y)=\frac{f\_{\mu}(y)}{\int\_{\mathbb{R}^{d}}p(T,y;0,x)\rho\_{0}(x)dx}=\frac{f\_{\mu}(y)}{\int\_{\mathbb{R}^{d}}p(T,y;0,x)\frac{f\_{0}(x)}{h^{\mu}(0,x)}dx}.\end{array}\right. |  | (6.4) |

We now give the heuristic idea of the construction of "solution mapping" Œì\Gamma. Let Œºini{\mu\_{\rm ini}} be given. For any Œº‚ààùí´2‚Äã(‚Ñùd)\mu\in\mathscr{P}\_{2}(\mathbb{R}^{d}),
first apply Lemma [5.3](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem3 "Lemma 5.3 ([29, Lemma 3.1]). ‚Ä£ 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") to get the feedback control
Œ±tŒº=‚àálog‚Å°hŒº‚Äã(t,XtŒ±Œº)\alpha^{\mu}\_{t}=\nabla\log h^{\mu}(t,X^{\alpha^{\mu}}\_{t}) so that ‚ÑôXTŒ±Œº=Œº\mathbb{P}\_{X^{\alpha^{\mu}}\_{T}}=\mu and

|  |  |  |  |
| --- | --- | --- | --- |
|  | J‚Äã(Œ±Œº)=ùîº‚Äã[log‚Å°œÅŒº‚Äã(XTŒ±Œº)]‚àíùîº‚Äã[log‚Å°hŒº‚Äã(0,X0Œ±Œº)].\displaystyle J(\alpha^{\mu})=\mathbb{E}[\log\rho^{\mu}(X^{\alpha^{\mu}}\_{T})]-\mathbb{E}[\log h^{\mu}(0,X^{\alpha^{\mu}}\_{0})]. |  | (6.5) |

In what follows we fix k‚àà‚Ñïk\in\mathbb{N}. To find the Œº^k\widehat{\mu}^{k} such that Jk‚Äã(Œ±Œº^k)=infJk‚Äã(Œ±)J^{k}(\alpha^{\widehat{\mu}^{k}})=\inf J^{k}(\alpha), we consider a mapping:
Œì2:ùïÉ1‚Äã(‚Ñùd)‚Üíùí´2‚Äã(‚Ñùd)\Gamma\_{2}:\mathbb{L}^{1}(\mathbb{R}^{d})\to\mathscr{P}\_{2}(\mathbb{R}^{d}) by Œì2‚Äã(œÅŒº)=Œº‚Ä≤\Gamma\_{2}(\rho^{\mu})=\mu^{\prime} where

|  |  |  |  |
| --- | --- | --- | --- |
|  | Œº‚Ä≤=arg‚ÄãminŒº¬Ø‚ààùí´2‚Äã(‚Ñùd)‚Å°{k‚ÄãG‚Äã(Œº¬Ø)+‚à´‚Ñùdlog‚Å°œÅŒº‚Äã(y)‚ÄãŒº¬Ø‚Äã(d‚Äãy)}.\mu^{\prime}=\operatorname\*{arg\,min}\_{\bar{\mu}\in\mathscr{P}\_{2}(\mathbb{R}^{d})}\Big\{kG(\bar{\mu})+\int\_{\mathbb{R}^{d}}\log\rho^{\mu}(y)\bar{\mu}(dy)\Big\}. |  | (6.6) |

Finally, we define Œì=Œì2‚àòŒì1:ùí´2‚Äã(‚Ñùd)‚Ü¶ùí´2‚Äã(‚Ñùd)\Gamma=\Gamma\_{2}\circ\Gamma\_{1}:\mathscr{P}\_{2}(\mathbb{R}^{d})\mapsto\mathscr{P}\_{2}(\mathbb{R}^{d}), and we shall argue that the mapping Œì\Gamma has a fixed point Œº^‚àà‚Ñ∞\widehat{\mu}\in\mathscr{E}, where ‚Ñ∞\mathscr{E} is defined by ([5.5](https://arxiv.org/html/2510.11829v1#S5.E5 "In 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")). Clearly, if Œì‚Äã(Œº^)=Œº^\Gamma(\widehat{\mu})=\widehat{\mu}, then we can still define Œ±^=Œ±Œº^\widehat{\alpha}=\alpha^{\widehat{\mu}}, and by Lemma [5.3](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem3 "Lemma 5.3 ([29, Lemma 3.1]). ‚Ä£ 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") we have ‚ÑôXŒ±Œº^=Œº^\mathbb{P}\_{X^{\alpha^{\widehat{\mu}}}}=\widehat{\mu}. Thus by ([6.5](https://arxiv.org/html/2510.11829v1#S6.E5 "In 6 Existence of optimal control and convergence for general ùúá·µ¢‚Çô·µ¢ ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")), for any Œ±‚ààùïÉùîΩ02‚Äã([0,T])\alpha\in\mathbb{L}^{2}\_{\mathbb{F}^{0}}([0,T]) we have

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | Jk‚Äã(Œ±^)\displaystyle J^{k}(\widehat{\alpha}) | =\displaystyle= | J‚Äã(Œ±^)+k‚ÄãG‚Äã(Œº^)=k‚ÄãG‚Äã(Œº^)+ùîº‚Äã[log‚Å°œÅŒº^‚Äã(XTŒ±^)]‚àíùîº‚Äã[log‚Å°hŒº^‚Äã(0,X0Œ±^)]\displaystyle J(\widehat{\alpha})+kG(\widehat{\mu})=kG(\widehat{\mu})+\mathbb{E}[\log\rho^{\widehat{\mu}}(X^{\widehat{\alpha}}\_{T})]-\mathbb{E}[\log h^{\widehat{\mu}}(0,X^{\widehat{\alpha}}\_{0})] |  |
|  |  | ‚â§\displaystyle\leq | k‚ÄãG‚Äã(‚ÑôXTŒ±)+ùîº‚Äã[log‚Å°œÅŒº^‚Äã(XTŒ±)]‚àíùîº‚Äã[log‚Å°hŒº^‚Äã(0,X0Œ±)]‚â§Jk‚Äã(Œ±).\displaystyle kG(\mathbb{P}\_{X^{\alpha}\_{T}})+\mathbb{E}[\log\rho^{\widehat{\mu}}(X^{\alpha}\_{T})]-\mathbb{E}[\log h^{\widehat{\mu}}(0,X^{\alpha}\_{0})]\leq J^{k}(\alpha). |  |

Here in the above the first inequality is due to ([6.6](https://arxiv.org/html/2510.11829v1#S6.E6 "In 6 Existence of optimal control and convergence for general ùúá·µ¢‚Çô·µ¢ ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")), and the last inequality is due to ([6.1](https://arxiv.org/html/2510.11829v1#S6.E1 "In 6 Existence of optimal control and convergence for general ùúá·µ¢‚Çô·µ¢ ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")). This shows that Œ±^\widehat{\alpha} is the minimizer of Jk‚Äã(‚ãÖ)J^{k}(\cdot).

We now show that the set ‚Ñ∞‚äÇùíüŒºini\mathscr{E}\subset\mathscr{D}\_{{\mu\_{\rm ini}}} defined by ([5.5](https://arxiv.org/html/2510.11829v1#S5.E5 "In 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) satisfies all the necessary properties, thanks to the Lemma [5.6](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem6 "Lemma 5.6. ‚Ä£ 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") and Proposition [5.8](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem8 "Proposition 5.8. ‚Ä£ 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") that we established in the last section, so that the mapping Œì\Gamma possesses a fixed point on ‚Ñ∞\mathscr{E} by Schauder‚Äôs fixed-point theorem.
Our main result is as follows.

###### Theorem 6.1.

Assume that Assumptions
[5.4](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem4 "Assumption 5.4. ‚Ä£ 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")
is in force. Consider the set ‚Ñ∞\mathscr{E} defined by ([5.5](https://arxiv.org/html/2510.11829v1#S5.E5 "In 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")). Then the following hold:

(i) ‚Ñ∞\mathscr{E} is convex and closed under Prohorov metric, and ùíÆ‚Ñ∞{\cal S}\_{\mathscr{E}} is convex and closed in ùïÉ1‚Äã(‚Ñùd)\mathbb{L}^{1}(\mathbb{R}^{d});

(ii) Œì‚Äã(‚Ñ∞)‚äÜ‚Ñ∞\Gamma(\mathscr{E})\subseteq\mathscr{E}, and is precompact in ùí´2‚Äã(‚Ñùd)\mathscr{P}\_{2}(\mathbb{R}^{d}), under both Prohorov and Wasserstein metric;

(iii) Œì\Gamma is continuous on ‚Ñ∞\mathscr{E}, under Prohorov metric.

Consequently, the mapping Œì\Gamma has a fixed point in ‚Ñ∞\mathscr{E}.

###### Proof.

Since the last statement is a direct consequence of Schauder‚Äôs fixed point theorem, applying to the space ùí´‚Äã(‚Ñùd)\mathscr{P}(\mathbb{R}^{d}) with Prohorov metric, we need only prove the properties (i)-(iii).

(i) is obvious.

(ii) By definition of ùíÆ‚Ñ∞{\cal S}\_{\mathscr{E}} we rewrite ([6.6](https://arxiv.org/html/2510.11829v1#S6.E6 "In 6 Existence of optimal control and convergence for general ùúá·µ¢‚Çô·µ¢ ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) as

|  |  |  |
| --- | --- | --- |
|  | fŒº‚Ä≤=arg‚ÄãminfŒº¬Ø‚ààùíÆ‚Ñ∞‚Å°{k‚ÄãG‚Äã(Œº¬Ø)+‚à´‚Ñùdlog‚Å°œÅŒº‚Äã(y)‚ÄãfŒº¬Ø‚Äã(y)‚Äãùëëy}.f\_{\mu^{\prime}}=\operatorname\*{arg\,min}\_{f\_{\bar{\mu}}\in{\cal S}\_{\mathscr{E}}}\Big\{kG({\bar{\mu}})+\int\_{\mathbb{R}^{d}}\log\rho^{\mu}(y)f\_{\bar{\mu}}(y)dy\Big\}. |  |

Since ùíÆ‚Ñ∞{\cal S}\_{\mathscr{E}} is convex and closed in ùïÉ1‚Äã(‚Ñùd)\mathbb{L}^{1}(\mathbb{R}^{d}), it follows that fŒº‚Ä≤‚ààùíÆ‚Ñ∞f\_{\mu^{\prime}}\in{\cal S}\_{\mathscr{E}}, and thus Œì‚Äã(‚Ñ∞)‚äÜ‚Ñ∞\Gamma(\mathscr{E})\subseteq\mathscr{E}.
We are to show that Œì‚Äã(‚Ñ∞)\Gamma(\mathscr{E}) is precompact in ùí´2‚Äã(‚Ñùd)\mathscr{P}\_{2}(\mathbb{R}^{d}).

To this end, let {Œºn}‚äÜŒì‚Äã(‚Ñ∞)\{\mu\_{n}\}\subseteq\Gamma(\mathscr{E}) be any sequence, we shall find a subsequence {Œºnk}k‚â•1\{\mu\_{n\_{k}}\}\_{k\geq 1} such that limk‚Üí‚àûŒºnk=Œº‚ààùí´2‚Äã(‚Ñù2)\lim\_{k\to\infty}\mu\_{n\_{k}}=\mu\in\mathscr{P}\_{2}(\mathbb{R}^{2}), under both Prohorov metric and W2W\_{2}-metric. Since Œì‚Äã(‚Ñ∞)‚äÜ‚Ñ∞\Gamma(\mathscr{E})\subseteq\mathscr{E}, by Lemma [5.6](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem6 "Lemma 5.6. ‚Ä£ 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")-(i), {fŒºn}\{f\_{\mu\_{n}}\} is bounded in
ùïÉ2‚Äã(‚Ñùd)\mathbb{L}^{2}(\mathbb{R}^{d}). Thus by Banach-Alaoglu Theorem and noting that ùïÉ2\mathbb{L}^{2} is reflexive, {fŒºn}\{f\_{\mu\_{n}}\} is weakly compact, that is, there exists a subsequence {fŒºnk}\{f\_{\mu\_{n\_{k}}}\} such that fŒºnk‚Äã‚áÄwfŒºf\_{\mu\_{n\_{k}}}\mathop{\mathrel{\mathop{\kern 0.0pt\rightharpoonup}\limits^{w}}}f\_{\mu} in ùïÉ2‚Äã(‚Ñùd)\mathbb{L}^{2}(\mathbb{R}^{d}), as k‚Üí‚àûk\to\infty. But this amounts to
saying the Œºnk‚áíŒº\mu\_{n\_{k}}\Rightarrow\mu in Prohorov metric. This, together with Lemma [5.6](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem6 "Lemma 5.6. ‚Ä£ 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")-(ii) and the relationship between Wasserstein distance and weak convergence (see, e.g., [[65](https://arxiv.org/html/2510.11829v1#bib.bib65), Theorem 7.12]), leads to that limk‚Üí‚àûŒºnk=Œº\lim\_{k\to\infty}\mu\_{n\_{k}}=\mu
in ùí´2‚Äã(‚Ñùd)\mathscr{P}\_{2}(\mathbb{R}^{d}), proving (ii).

(iii) Let us assume that {Œºn}‚äÇ‚Ñ∞\{\mu\_{n}\}\subset\mathscr{E} such that Œºn‚áíŒº\mu\_{n}\Rightarrow\mu in Prohorov metric.
The stability result in Proposition [5.8](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem8 "Proposition 5.8. ‚Ä£ 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") shows that œÅŒºn=Œì1‚Äã(Œºn)‚ÜíŒì1‚Äã(Œº)=œÅŒº‚ààùíÆ\rho^{\mu\_{n}}=\Gamma\_{1}(\mu\_{n})\to\Gamma\_{1}(\mu)=\rho^{\mu}\in{\cal S} in ùïÉ1‚Äã(‚Ñùd)\mathbb{L}^{1}(\mathbb{R}^{d}). Next, we show that Œì2‚Äã(œÅŒºn)‚áíŒì2‚Äã(œÅŒº)\Gamma\_{2}(\rho^{\mu\_{n}})\Rightarrow\Gamma\_{2}(\rho^{\mu}) in Prohorov metric. Recall the definition of Œì2\Gamma\_{2}, we define a family of
functionals on ‚Ñ∞\mathscr{E}: for each k,n‚àà‚Ñïk,n\in\mathbb{N} and Œº¬Ø‚àà‚Ñ∞\bar{\mu}\in\mathscr{E},

|  |  |  |  |
| --- | --- | --- | --- |
|  | {Fnk‚Äã(Œº¬Ø):=k‚ÄãG‚Äã(Œº¬Ø)+‚à´‚Ñùdlog‚Å°œÅŒºn‚Äã(y)‚ÄãŒº¬Ø‚Äã(d‚Äãy);Fk‚Äã(Œº¬Ø):=k‚ÄãG‚Äã(Œº¬Ø)+‚à´‚Ñùdlog‚Å°œÅŒº‚Äã(y)‚ÄãŒº¬Ø‚Äã(d‚Äãy).\displaystyle\left\{\begin{array}[]{lll}\displaystyle F^{k}\_{n}(\bar{\mu}):=kG(\bar{\mu})+\int\_{\mathbb{R}^{d}}\log\rho^{\mu\_{n}}(y)\bar{\mu}(dy);\vskip 6.0pt plus 2.0pt minus 2.0pt\\ \displaystyle F^{k}(\bar{\mu}):=kG(\bar{\mu})+\int\_{\mathbb{R}^{d}}\log\rho^{\mu}(y)\bar{\mu}(dy).\end{array}\right. |  | (6.9) |

Then Œºn‚Ä≤=Œì2‚Äã(œÅŒºn):=arg‚ÄãminŒº¬Ø‚àà‚Ñ∞‚Å°Fnk‚Äã(Œº¬Ø)\mu\_{n}^{\prime}=\Gamma\_{2}(\rho^{\mu\_{n}}):=\operatorname\*{arg\,min}\_{\bar{\mu}\in\mathscr{E}}F^{k}\_{n}(\bar{\mu}) and Œº‚Ä≤=Œì2‚Äã(œÅŒº):=arg‚ÄãminŒº¬Ø‚àà‚Ñ∞‚Å°Fk‚Äã(Œº¬Ø)\mu^{\prime}=\Gamma\_{2}(\rho^{\mu}):=\operatorname\*{arg\,min}\_{\bar{\mu}\in\mathscr{E}}F^{k}(\bar{\mu}).

To show that the minimizers Œºn‚Ä≤‚áíŒº‚Ä≤\mu^{\prime}\_{n}\Rightarrow\mu^{\prime}, we shall invoke
the notion of Œì\Gamma-convergence (cf. [[19](https://arxiv.org/html/2510.11829v1#bib.bib19)]). To be more precise, a sequence {Fn}n‚â•1\{F\_{n}\}\_{n\geq 1} is said to Œì\Gamma-converge to
FF as n‚Üí‚àûn\to\infty if

|  |  |  |  |
| --- | --- | --- | --- |
|  | {For every sequence¬†Œº¬Øn‚áíŒº¬Ø, it holds that¬†F‚Äã(Œº¬Ø)‚â§lim¬ØnFnk‚Äã(Œº¬Øn);There exists a sequence¬†Œº¬Øn‚áíŒº¬Ø, such that¬†F‚Äã(Œº¬Ø)‚â•lim¬ØnFn‚Äã(Œº¬Øn).\displaystyle\left\{\begin{array}[]{lll}\mbox{For every sequence $\bar{\mu}\_{n}\Rightarrow\bar{\mu}$, it holds that $\displaystyle F(\bar{\mu})\leq\mathop{\underline{\rm lim}}\_{n}F^{k}\_{n}(\bar{\mu}\_{n})$};\vskip 6.0pt plus 2.0pt minus 2.0pt\\ \mbox{There exists a sequence $\bar{\mu}\_{n}\Rightarrow\bar{\mu}$, such that $\displaystyle F(\bar{\mu})\geq\mathop{\overline{\rm lim}}\_{n}F\_{n}(\bar{\mu}\_{n})$.}\end{array}\right. |  | (6.12) |

Now, note that GG is convex and ‚Ñ∞\mathscr{E} is compact under Prohorov metric, we see that both {Fnk}\{F^{k}\_{n}\} and FkF^{k} are coercive (in the sense that there exists minimizing sequence in ‚Ñ∞‚äÇùí´2‚Äã(‚Ñùd)\mathscr{E}\subset\mathscr{P}\_{2}(\mathbb{R}^{d})). Thus, in light of the Œì\Gamma-convergence result (see [[19](https://arxiv.org/html/2510.11829v1#bib.bib19), Theorem 7.1]), in order to show Œºn‚Ä≤‚áíŒº‚Ä≤\mu\_{n}^{\prime}\Rightarrow\mu^{\prime}, it suffices to check that {Fnk}\{F^{k}\_{n}\} Œì\Gamma-converges to FkF^{k}, for each kk. To see this, for any {Œº¬Øn}‚äÇ‚Ñ∞\{\bar{\mu}\_{n}\}\subset\mathscr{E}, such that Œº¬Øn‚áíŒº¬Ø\bar{\mu}\_{n}\Rightarrow\bar{\mu}, by Lemma [5.6](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem6 "Lemma 5.6. ‚Ä£ 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")-(iii), we have fŒº¬Øn‚ÜífŒº¬Øf\_{\bar{\mu}\_{n}}\to f\_{\bar{\mu}} in
ùïÉ1\mathbb{L}^{1}, and therefore by Dominated Convergence,

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  |  | |‚à´‚Ñùd(logœÅŒºn(y)Œº¬Øn(dy)‚àí‚à´‚ÑùdlogœÅŒº(y)Œº¬Ø(dy)|\displaystyle\Big|\int\_{\mathbb{R}^{d}}(\log\rho^{\mu\_{n}}(y)\bar{\mu}\_{n}(dy)-\int\_{\mathbb{R}^{d}}\log\rho^{\mu}(y)\bar{\mu}(dy)\Big| |  |
|  |  | ‚â§\displaystyle\leq | ‚à´‚Ñùd|log‚Å°œÅŒºn‚Äã(y)|‚Äã|fŒº¬Øn‚Äã(y)‚àífŒº¬Ø‚Äã(y)|‚Äãùëëy+‚à´‚Ñùd|log‚Å°œÅŒºn‚Äã(y)‚àílog‚Å°œÅŒº‚Äã(y)|‚ÄãŒº¬Ø‚Äã(d‚Äãy)\displaystyle\int\_{\mathbb{R}^{d}}|\log\rho^{\mu\_{n}}(y)||f\_{\bar{\mu}\_{n}}(y)-f\_{\bar{\mu}}(y)|dy+\int\_{\mathbb{R}^{d}}|\log\rho^{\mu\_{n}}(y)-\log\rho^{\mu}(y)|\bar{\mu}(dy) |  |
|  |  | ‚â§\displaystyle\leq | log‚Å°(KŒ¥)‚Äã‚ÄñfŒº¬Øn‚àífŒº¬Ø‚ÄñùïÉ1+‚à´‚Ñùd|log‚Å°œÅŒºn‚Äã(y)‚àílog‚Å°œÅŒº‚Äã(y)|‚ÄãŒº¬Ø‚Äã(d‚Äãy)‚Üí0,as¬†n‚Üí‚àû.\displaystyle\log\big(\frac{K}{\delta}\big)\|f\_{\bar{\mu}\_{n}}-f\_{\bar{\mu}}\|\_{\mathbb{L}^{1}}+\int\_{\mathbb{R}^{d}}|\log\rho^{\mu\_{n}}(y)-\log\rho^{\mu}(y)|\bar{\mu}(dy)\to 0,\quad\mbox{as $n\to\infty$.} |  |

Finally, since G‚Äã(‚ãÖ)G(\cdot) is continuous on ùí´2‚Äã(‚Ñùd)\mathscr{P}\_{2}(\mathbb{R}^{d}), by definition ([6.9](https://arxiv.org/html/2510.11829v1#S6.E9 "In 6 Existence of optimal control and convergence for general ùúá·µ¢‚Çô·µ¢ ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) we see that Fnk‚Äã(Œº¬Øn)‚ÜíFk‚Äã(Œº¬Ø)F^{k}\_{n}(\bar{\mu}\_{n})\to F^{k}(\bar{\mu}) whenever Œº¬Øn‚áíŒº¬Ø\bar{\mu}\_{n}\Rightarrow\bar{\mu}. Thus by ([6.12](https://arxiv.org/html/2510.11829v1#S6.E12 "In 6 Existence of optimal control and convergence for general ùúá·µ¢‚Çô·µ¢ ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) we see that {Fnk}\{F^{k}\_{n}\} Œì\Gamma-converges to FkF^{k}, as n‚Üí‚àûn\to\infty. This completes the proof.
‚àé

Finally, we shall establish an
analogue of Theorem [4.1](https://arxiv.org/html/2510.11829v1#S4.ThmTheorem1 "Theorem 4.1. ‚Ä£ 4.1 The Convergence of Optimal Policies ‚Ä£ 4 Convergence Results under Delta Initial Distribution ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") in the case of general Œºini‚ààùí´2‚Äã(‚Ñùd){\mu\_{\rm ini}}\in\mathscr{P}\_{2}(\mathbb{R}^{d}). For technical convenience, in what follows we shall make use of the following extra assumptions to facilitate our discussion. Recall the function Œæ\xi and Œ∑\eta defined by ([5.14](https://arxiv.org/html/2510.11829v1#S5.E14 "In 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) and ([5.15](https://arxiv.org/html/2510.11829v1#S5.E15 "In 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")), respectively.

###### Assumption 6.2.

(i) The penalty function GG satisfies G‚Äã(Œº;Œºtar)‚â•W2‚Äã(Œº,Œºtar)G(\mu;\mu\_{\rm tar})\geq W\_{2}(\mu,\mu\_{\rm tar});

(ii) In Assumption [3.2](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem2 "Assumption 3.2. ‚Ä£ 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")-(ii), the function œï\phi satisfies ‚Äñœï‚Äã(‚ãÖ)‚ÄãeŒæ‚Äã(‚ãÖ)‚Äñ‚àû<‚àû\|\phi(\cdot)e^{\xi(\cdot)}\|\_{\infty}<\infty;

(iii) For any R>0R>0, there exists MR>0M\_{R}>0, such that

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùîº‚Äã[Œ∑2‚Äã(XTt,x)]=‚à´‚ÑùdŒ∑2‚Äã(y)‚Äãp‚Äã(T,y;t,x)‚Äãùëëy‚â§MR,(t,x)‚àà[0,T]√óBR,\displaystyle\mathbb{E}[\eta^{2}(X^{t,x}\_{T})]=\int\_{\mathbb{R}^{d}}\eta^{2}(y)p(T,y;t,x)dy\leq M\_{R},\qquad(t,x)\in[0,T]\times B\_{R}, |  | (6.13) |

where BR:={x‚àà‚Ñùd:|x|‚â§R}B\_{R}:=\{x\in\mathbb{R}^{d}:|x|\leq R\}.
‚àé

###### Remark 6.3.

(1) Assumption [6.2](https://arxiv.org/html/2510.11829v1#S6.ThmTheorem2 "Assumption 6.2. ‚Ä£ 6 Existence of optimal control and convergence for general ùúá·µ¢‚Çô·µ¢ ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")-(i) is not overly restrictive, and can be justified by Example [3.5](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem5 "Example 3.5. ‚Ä£ 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday").

(2) Note that the function œï\phi in Assumption [3.2](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem2 "Assumption 3.2. ‚Ä£ 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")-(ii) satisfies œï‚Äã(y)‚ÄãeŒª‚Äã|y|2‚â§C\phi(y)e^{\lambda|y|^{2}}\leq C, for some Œª>0\lambda>0 and that Œæ‚Äã(y)‚àºŒª‚Ä≤‚Äã|y|2\xi(y)\sim\lambda^{\prime}|y|^{2}, as |y|‚Üí‚àû|y|\to\infty, Assumption [6.2](https://arxiv.org/html/2510.11829v1#S6.ThmTheorem2 "Assumption 6.2. ‚Ä£ 6 Existence of optimal control and convergence for general ùúá·µ¢‚Çô·µ¢ ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")-(ii) amounts to saying that œï\phi and Œæ\xi are compatible.

(3) While Assumption [6.2](https://arxiv.org/html/2510.11829v1#S6.ThmTheorem2 "Assumption 6.2. ‚Ä£ 6 Existence of optimal control and convergence for general ùúá·µ¢‚Çô·µ¢ ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")-(iii) is slightly stronger than the requirement ([5.15](https://arxiv.org/html/2510.11829v1#S5.E15 "In 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")), it would be trivial if the mapping (t,x)‚Ü¶ùîº‚Äã[Œ∑2‚Äã(XTt,x)](t,x)\mapsto\mathbb{E}[\eta^{2}(X^{t,x}\_{T})] is continuous, which is by no means stringent.
‚àé

Our last result of this section is the following.

###### Theorem 6.4.

Assume that the Assumptions [3.2](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem2 "Assumption 3.2. ‚Ä£ 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday"), [5.4](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem4 "Assumption 5.4. ‚Ä£ 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") and [6.2](https://arxiv.org/html/2510.11829v1#S6.ThmTheorem2 "Assumption 6.2. ‚Ä£ 6 Existence of optimal control and convergence for general ùúá·µ¢‚Çô·µ¢ ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") are in force. Let Œºini‚àà‚Ñ∞{\mu\_{\rm ini}}\in\mathscr{E} be given, and
let Œ±^‚Äã(t,x)\widehat{\alpha}(t,x) and Œ±^k‚Äã(t,x)\widehat{\alpha}^{k}(t,x), (t,x)‚àà[0,T]√ó‚Ñùd(t,x)\in[0,T]\times\mathbb{R}^{d} are optimal controls given in Proposition [3.6](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem6 "Proposition 3.6. ‚Ä£ 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") and Lemma [5.3](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem3 "Lemma 5.3 ([29, Lemma 3.1]). ‚Ä£ 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday"), respectively. Then, for any R>0R>0, there exists CR>0C\_{R}>0, such that for any k‚àà‚Ñïk\in\mathbb{N}, it holds that

|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚à´0T|Œ±^k‚Äã(t,x)‚àíŒ±^‚Äã(t,x)|‚Äãùëët‚â§CRk,(t,x)‚àà[0,T]√óBR.\displaystyle\int\_{0}^{T}|\widehat{\alpha}^{k}(t,x)-\widehat{\alpha}(t,x)|dt\leq\frac{C\_{R}}{k},\qquad(t,x)\in[0,T]\times B\_{R}. |  | (6.14) |

###### Proof.

We begin by denoting Œºk:=‚ÑôXTŒ±^k\mu^{k}:=\mathbb{P}\_{X\_{T}^{\widehat{\alpha}^{k}}}, Œºtar:=‚ÑôXTŒ±^{\mu\_{\rm tar}}:=\mathbb{P}\_{X\_{T}^{\widehat{\alpha}}}, and let œÅŒºk=Œì1‚Äã(Œºk)\rho^{\mu\_{k}}=\Gamma\_{1}(\mu\_{k}), k‚àà‚Ñïk\in\mathbb{N},
œÅŒºtar=Œì1‚Äã(Œºtar)\rho^{\mu\_{\rm tar}}=\Gamma\_{1}({\mu\_{\rm tar}}), respectively, as we defined before. Next, applying ([5.12](https://arxiv.org/html/2510.11829v1#S5.E12 "In 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")), we have

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | |œÅŒºk‚Äã(y)‚àíœÅŒºtar‚Äã(y)|=|eœàŒºk‚Äã(y)‚ÄãfŒºk‚Äã(y)‚àíeœàŒºtar‚Äã(y)‚ÄãfŒºtar‚Äã(y)|‚â§eœàŒºtar‚Äã(y)‚Äã|fŒºk‚Äã(y)‚àífŒºtar‚Äã(y)|+fŒºk‚Äã(y)‚Äã|eœàŒºk‚Äã(y)‚àíeœàŒºtar‚Äã(y)|.\displaystyle\begin{split}|\rho^{\mu\_{k}}(y)-\rho^{\mu\_{\rm tar}}(y)|&=|e^{\psi^{\mu\_{k}}(y)}f\_{\mu\_{k}}(y)-e^{\psi^{{\mu\_{\rm tar}}}(y)}f\_{\mu\_{\rm tar}}(y)|\\ &\leq e^{\psi^{{\mu\_{\rm tar}}}(y)}|f\_{\mu\_{k}}(y)-f\_{\mu\_{\rm tar}}(y)|+f\_{\mu\_{k}}(y)|e^{\psi^{\mu\_{k}}(y)}-e^{\psi^{{\mu\_{\rm tar}}}(y)}|.\end{split} | |  | (6.15) |

Let us now recall some facts from ¬ß4. First, note that the optimality of Œºk\mu\_{k} implies that G‚Äã(Œºk)‚â§CkG(\mu\_{k})\leq\frac{C}{k} (cf. ([4.3](https://arxiv.org/html/2510.11829v1#S4.E3 "In 4 Convergence Results under Delta Initial Distribution ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday"))), for some generic constant C>0C>0 independent of kk, which we shall allow to vary from line to line below. Thus, by virtue of Assumption [3.2](https://arxiv.org/html/2510.11829v1#S3.ThmTheorem2 "Assumption 3.2. ‚Ä£ 3 Existence of Optimal Policies for SCSBP‚Äôs ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")-(ii), we can write

|  |  |  |
| --- | --- | --- |
|  | |fŒºk‚Äã(y)‚àífŒºtar‚Äã(y)|‚â§Ck‚Äãœï‚Äã(y),y‚àà‚Ñùd,|f\_{\mu\_{k}}(y)-f\_{\mu\_{\rm tar}}(y)|\leq\frac{C}{k}\phi(y),\qquad y\in\mathbb{R}^{d}, |  |

where œï‚Äã(y)‚ÄãeŒæ‚Äã(y)‚â§C\phi(y)e^{\xi(y)}\leq C, y‚àà‚Ñùdy\in\mathbb{R}^{d}, thanks to Assumption [6.2](https://arxiv.org/html/2510.11829v1#S6.ThmTheorem2 "Assumption 6.2. ‚Ä£ 6 Existence of optimal control and convergence for general ùúá·µ¢‚Çô·µ¢ ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")-(ii). Furthermore, under Assumption [6.2](https://arxiv.org/html/2510.11829v1#S6.ThmTheorem2 "Assumption 6.2. ‚Ä£ 6 Existence of optimal control and convergence for general ùúá·µ¢‚Çô·µ¢ ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")-(i), we can assume without loss of generality that the Schr√∂dinger potentials œàŒºk\psi^{\mu\_{k}} and œàŒºtar\psi^{{\mu\_{\rm tar}}} all satisfy estimates ([5.13](https://arxiv.org/html/2510.11829v1#S5.E13 "In 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) and ([5.14](https://arxiv.org/html/2510.11829v1#S5.E14 "In 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")). Consequently, by Assumption [6.2](https://arxiv.org/html/2510.11829v1#S6.ThmTheorem2 "Assumption 6.2. ‚Ä£ 6 Existence of optimal control and convergence for general ùúá·µ¢‚Çô·µ¢ ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")-(i) and an easy application of Lemma [5.7](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem7 "Lemma 5.7 ([10, Theorem 1.1]). ‚Ä£ 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") and Newton-Leibniz formula we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | |eœàŒºtar‚Äã(y)‚àíeœàŒºtar‚Äã(y)|\displaystyle|e^{\psi^{{\mu\_{\rm tar}}}(y)}-e^{\psi^{{\mu\_{\rm tar}}}(y)}| | =|œàŒºtar(y)‚àíœàŒºtar(y)|‚à´01exp{œàŒºtar(y)+Œ∏(œàŒºk(y)‚àíœàŒºtar(y)}dŒ∏\displaystyle=|{\psi^{{\mu\_{\rm tar}}}(y)}-{\psi^{{\mu\_{\rm tar}}}(y)}|\int\_{0}^{1}\exp\{\psi^{\mu\_{\rm tar}}(y)+\theta(\psi^{\mu\_{k}}(y)-\psi^{\mu\_{\rm tar}}(y)\}d\theta |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§C‚ÄãeŒæ‚Äã(y)‚ÄãW2‚Äã(Œºk,Œºtar)‚â§C‚ÄãeŒæ‚Äã(y)‚ÄãG‚Äã(Œºk)‚â§Ck‚ÄãeŒæ‚Äã(y),y‚àà‚Ñùd.\displaystyle\leq Ce^{\xi(y)}W\_{2}(\mu\_{k},{\mu\_{\rm tar}})\leq Ce^{\xi(y)}G(\mu\_{k})\leq\frac{C}{k}e^{\xi(y)},\quad y\in\mathbb{R}^{d}. |  |

Summarizing above and recalling Assumption [5.4](https://arxiv.org/html/2510.11829v1#S5.ThmTheorem4 "Assumption 5.4. ‚Ä£ 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday") and ([5.15](https://arxiv.org/html/2510.11829v1#S5.E15 "In 5 Stability of the Solutions to the SBP ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")), we derive from ([6.15](https://arxiv.org/html/2510.11829v1#S6.E15 "In 6 Existence of optimal control and convergence for general ùúá·µ¢‚Çô·µ¢ ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) that

|  |  |  |
| --- | --- | --- |
|  | |œÅŒºk‚Äã(y)‚àíœÅŒºtar‚Äã(y)|‚â§Ck‚Äã(eœàŒºtar‚Äã(y)‚Äãœï‚Äã(y)+fŒºk‚Äã(y)‚ÄãeŒæ‚Äã(y))‚â§Ck‚Äã(eŒæ‚Äã(y)‚Äãœï‚Äã(y)+g2‚Äã(y)‚ÄãeŒæ‚Äã(y))‚â§Ck‚Äã(1+Œ∑‚Äã(y)),\displaystyle|\rho^{\mu\_{k}}(y)-\rho^{\mu\_{\rm tar}}(y)|\leq\frac{C}{k}\left(e^{\psi^{{\mu\_{\rm tar}}}(y)}\phi(y)+f\_{\mu\_{k}}(y)e^{\xi(y)}\right)\leq\frac{C}{k}\left(e^{\xi(y)}\phi(y)+g^{2}(y)e^{\xi(y)}\right)\leq\frac{C}{k}(1+\eta(y)), |  |

and therefore, given R>0R>0, and (t,x)‚àà[0,T]√óBR(t,x)\in[0,T]\times B\_{R}, we apply Assumption [6.2](https://arxiv.org/html/2510.11829v1#S6.ThmTheorem2 "Assumption 6.2. ‚Ä£ 6 Existence of optimal control and convergence for general ùúá·µ¢‚Çô·µ¢ ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")-(iii) to get

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùîº‚Äã[|œÅŒºk‚Äã(XTt,x)‚àíœÅŒºtar‚Äã(XTt,x)|]‚â§ùîº‚Äã[|œÅŒºk‚Äã(XTt,x)‚àíœÅŒºtar‚Äã(XTt,x)|2]12‚â§CRk,\displaystyle\mathbb{E}[|\rho^{\mu\_{k}}(X^{t,x}\_{T})-\rho^{\mu\_{\rm tar}}(X^{t,x}\_{T})|]\leq\mathbb{E}[|\rho^{\mu\_{k}}(X^{t,x}\_{T})-\rho^{\mu\_{\rm tar}}(X^{t,x}\_{T})|^{2}]^{\frac{1}{2}}\leq\frac{C\_{R}}{k}, |  | (6.16) |

where CR>0C\_{R}>0 is some constant depending on the generic constant CC above and MRM\_{R} in ([6.13](https://arxiv.org/html/2510.11829v1#S6.E13 "In Assumption 6.2. ‚Ä£ 6 Existence of optimal control and convergence for general ùúá·µ¢‚Çô·µ¢ ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")).

To complete the proof, let us recall that optimal strategies are of the form Œ±^k‚Äã(t,x)=‚àálog‚Å°hk‚Äã(t,x)\widehat{\alpha}^{k}(t,x)=\nabla\log h^{k}(t,x), k‚àà‚Ñïk\in\mathbb{N}, and Œ±^‚Äã(t,x)=‚àálog‚Å°h‚Äã(t,x)\widehat{\alpha}(t,x)=\nabla\log h(t,x), and hk‚Äã(t,x)h^{k}(t,x) and h‚Äã(t,x)h(t,x) are the solutions to the respective PDEs:

|  |  |  |  |
| --- | --- | --- | --- |
|  | {‚àÇthk‚Äã(t,x)+‚Ñít‚Äãhk‚Äã(t,x)=0;hk‚Äã(T,x)=œÅŒºk‚Äã(x).{‚àÇth‚Äã(t,x)+‚Ñít‚Äãh‚Äã(t,x)=0;h‚Äã(T,x)=œÅŒºtar‚Äã(x),\displaystyle\begin{cases}\partial\_{t}h^{k}(t,x)+\mathscr{L}\_{t}h^{k}(t,x)=0;\\ h^{k}(T,x)=\rho^{\mu\_{k}}(x).\end{cases}\quad\quad\begin{cases}\partial\_{t}h(t,x)+\mathscr{L}\_{t}h(t,x)=0;\\ h(T,x)=\rho^{\mu\_{\rm tar}}(x),\end{cases} |  | (6.17) |

Furthermore, noting that hk‚Äã(t,x)=ùîº‚Äã[œÅŒºk‚Äã(XTt,x)]h^{k}(t,x)=\mathbb{E}[\rho^{\mu\_{k}}(X^{t,x}\_{T})], h‚Äã(t,x)=ùîº‚Äã[œÅŒºtar‚Äã(XTt,x)]h(t,x)=\mathbb{E}[\rho^{{\mu\_{\rm tar}}}(X^{t,x}\_{T})], and by the Bismut-Elworthy-Li formula we have

|  |  |  |
| --- | --- | --- |
|  | ‚àáhk‚Äã(t,x)=ùîº‚Äã[œÅŒºk‚Äã(XTt,x)‚ÄãNTt,x];‚àáh‚Äã(t,x)=ùîº‚Äã[œÅŒºtar‚Äã(XTt,x)‚ÄãNTt,x].\nabla h^{k}(t,x)=\mathbb{E}[\rho^{\mu\_{k}}(X^{t,x}\_{T})N^{t,x}\_{T}];\quad\nabla h(t,x)=\mathbb{E}[\rho^{{\mu\_{\rm tar}}}(X^{t,x}\_{T})N^{t,x}\_{T}]. |  |

Thus, we have |‚àáh‚Äã(t,x)|‚â§ùîº‚Äã[Œ∑2‚Äã(XTt,x)]12‚Äãùîº‚Äã[|NTt,x|]12‚â§CRT‚àít|\nabla h(t,x)|\leq\mathbb{E}[\eta^{2}(X^{t,x}\_{T})]^{\frac{1}{2}}\mathbb{E}[|N^{t,x}\_{T}|]^{\frac{1}{2}}\leq\frac{C\_{R}}{\sqrt{T-t}}, whenever (t,x)‚àà[0,T]√óBR(t,x)\in[0,T]\times B\_{R}, and a similar argument as in ([4.9](https://arxiv.org/html/2510.11829v1#S4.E9 "In 4.1 The Convergence of Optimal Policies ‚Ä£ 4 Convergence Results under Delta Initial Distribution ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) and ([4.11](https://arxiv.org/html/2510.11829v1#S4.E11 "In 4.1 The Convergence of Optimal Policies ‚Ä£ 4 Convergence Results under Delta Initial Distribution ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")), together with the estimate ([6.16](https://arxiv.org/html/2510.11829v1#S6.E16 "In 6 Existence of optimal control and convergence for general ùúá·µ¢‚Çô·µ¢ ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")), leads to that

|  |  |  |  |
| --- | --- | --- | --- |
|  | |‚àáhk‚Äã(t,x)‚àí‚àáh‚Äã(t,x)|‚â§CRk‚ÄãT‚àít;|hk‚Äã(t,x)‚àíh‚Äã(t,x)|‚â§CRk,(t,x)‚àà[0,T]√óBR.\displaystyle|\nabla h^{k}(t,x)-\nabla h(t,x)|\leq\frac{C\_{R}}{k\sqrt{T-t}};\quad|h^{k}(t,x)-h(t,x)|\leq\frac{C\_{R}}{k},\quad(t,x)\in[0,T]\times B\_{R}. |  | (6.18) |

Finally, we note that by definition the function hh is positive everywhere (unless œÅŒºtar‚â°0\rho^{\mu\_{\rm tar}}\equiv 0), and being the classical solution to the PDE ([6.17](https://arxiv.org/html/2510.11829v1#S6.E17 "In 6 Existence of optimal control and convergence for general ùúá·µ¢‚Çô·µ¢ ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) it is continuous. Thus, given R>0R>0, h‚Äã(t,x)‚â•Œ¥R>0h(t,x)\geq\delta\_{R}>0, for all (t,x)‚àà[0,T]√óBR(t,x)\in[0,T]\times B\_{R}. Since ([6.18](https://arxiv.org/html/2510.11829v1#S6.E18 "In 6 Existence of optimal control and convergence for general ùúá·µ¢‚Çô·µ¢ ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")) implies that hkh^{k} converges to hh uniformly on compacts, thus it must hold that
hk‚Äã(t,x)‚â•Œ¥R/2h^{k}(t,x)\geq\delta\_{R}/2, for (t,x)‚àà[0,T]√óBR(t,x)\in[0,T]\times B\_{R}, and kk large enough. We therefore conclude,
similar to ([4.1](https://arxiv.org/html/2510.11829v1#S4.Ex5 "4.1 The Convergence of Optimal Policies ‚Ä£ 4 Convergence Results under Delta Initial Distribution ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")), that

|  |  |  |  |
| --- | --- | --- | --- |
|  | |Œ±^k‚Äã(t,x)‚àíŒ±^‚Äã(t,x)|\displaystyle|\widehat{\alpha}^{k}(t,x)-\widehat{\alpha}(t,x)| | ‚â§|‚àáhk‚Äã(t,x)‚àí‚àáh‚Äã(t,x)hk‚Äã(t,x)|+|‚àáh‚Äã(t,x)|‚Äã|h‚Äã(t,x)‚àíhk‚Äã(t,x)hk‚Äã(t,x)‚Äãh‚Äã(t,x)|‚â§CRk‚ÄãT‚àít,\displaystyle\leq\left|\frac{\nabla h^{k}(t,x)-\nabla h(t,x)}{h^{k}(t,x)}\right|+|\nabla h(t,x)|\left|\frac{h(t,x)-h^{k}(t,x)}{h^{k}(t,x)h(t,x)}\right|\leq\frac{C\_{R}}{k\sqrt{T-t}}, |  |

as k‚Üí‚àûk\to\infty, for (t,x)‚àà[0,T]√óBR(t,x)\in[0,T]\times B\_{R}, where CRC\_{R} depends on MRM\_{R} and Œ¥R\delta\_{R} above, but independent of kk. Integrating in tt we obtain ([6.14](https://arxiv.org/html/2510.11829v1#S6.E14 "In Theorem 6.4. ‚Ä£ 6 Existence of optimal control and convergence for general ùúá·µ¢‚Çô·µ¢ ‚Ä£ Schr√∂dinger bridge for generative AI: Soft-constrained formulation and convergence analysis1footnote 11footnote 1We thank helpful comments and discussions from Rama Cont, Huy√™n Pham, Xin Zhang, and Xun Yu Zhou. In celebration of Prof. Xun Yu Zhou‚Äôs 60th birthday")).
‚àé

## 7 Conclusion

We study the soft-constrained Schr√∂dinger bridge problem (SCSBP) as a flexible alternative to the classical formulation for generative modeling. By replacing hard terminal constraints with a general penalty function, the SCSBP potentially offers greater flexibility and stability for generative AI tasks. Moreover, we establish linear convergence of both the value functions and the optimal controls as the penalty parameter tends to infinity, thereby providing a theoretical guarantee for the framework.

In future work, we will develop efficient algorithms for learning the SCSBP solutions and test the performance on benchmark generative AI tasks. This will allow us to translate the theoretical framework into practical tools, further demonstrating the potential of regularized stochastic control formulations for modern generative modeling.

## References

* [1]

  B.¬†Acciaio, S.¬†Eckstein, and S.¬†Hou.
  Time-causal vae: Robust financial time series generator.
  arXiv preprint arXiv:2411.02947, 2024.
* [2]

  A.¬†Alouadi, B.¬†Barreau, L.¬†Carlier, and H.¬†Pham.
  Robust time series generation via schr\\backslash" odinger bridge: a
  comprehensive evaluation.
  arXiv preprint arXiv:2503.02943, 2025.
* [3]

  D.¬†G. Aronson.
  Bounds for the fundamental solution of a parabolic equation.
  1967.
* [4]

  Y.¬†Bai, E.¬†Yang, B.¬†Han, Y.¬†Yang, J.¬†Li, Y.¬†Mao, G.¬†Niu, and T.¬†Liu.
  Understanding and improving early stopping for learning with noisy
  labels.
  Advances in Neural Information Processing Systems,
  34:24392‚Äì24403, 2021.
* [5]

  J.-D. Benamou, G.¬†Chazareix, and G.¬†Loeper.
  From entropic transport to martingale transport, and applications to
  model calibration.
  arXiv preprint arXiv:2406.11537, 2024.
* [6]

  J.¬†Betker, G.¬†Goh, L.¬†Jing, T.¬†Brooks, J.¬†Wang, L.¬†Li, L.¬†Ouyang, J.¬†Zhuang,
  J.¬†Lee, Y.¬†Guo, et¬†al.
  Improving image generation with better captions.
  Computer Science. https://cdn. openai. com/papers/dall-e-3.
  pdf, 2(3):8, 2023.
* [7]

  A.¬†Beurling.
  An automorphism of product measures.
  Annals of Mathematics, 72(1):189‚Äì200, 1960.
* [8]

  J.-M. Bismut.
  Large Deviation and Malliavin Calculus, volume¬†45 of Progress in Mathematics.
  Birkh√§ser, 1984.
* [9]

  H.¬†Cao, H.¬†Gu, X.¬†Guo, and M.¬†Rosenbaum.
  Risk of transfer learning and its applications in finance.
  arXiv preprint arXiv:2311.03283, 2023.
* [10]

  G.¬†Carlier, L.¬†Chizat, and M.¬†Laborde.
  Displacement smoothness of entropic optimal transport.
  ESAIM: Control, Optimisation and Calculus of Variations, 30:25,
  2024.
* [11]

  M.¬†Chen, K.¬†Huang, T.¬†Zhao, and M.¬†Wang.
  Score approximation, estimation and distribution recovery of
  diffusion models on low-dimensional data.
  In International Conference on Machine Learning, pages
  4672‚Äì4712. PMLR, 2023.
* [12]

  M.¬†Chen, R.¬†Xu, Y.¬†Xu, and R.¬†Zhang.
  Diffusion factor models: Generating high-dimensional returns with
  factor structure.
  arXiv preprint arXiv:2504.06566, 2025.
* [13]

  T.¬†Chen, G.-H. Liu, and E.¬†A. Theodorou.
  Likelihood training of schr\\backslash" odinger bridge using
  forward-backward sdes theory.
  arXiv preprint arXiv:2110.11291, 2021.
* [14]

  Y.¬†Chen, T.¬†T. Georgiou, and M.¬†Pavon.
  Stochastic control liaisons: Richard sinkhorn meets gaspard monge on
  a schrodinger bridge.
  Siam Review, 63(2):249‚Äì313, 2021.
* [15]

  Z.¬†Chen, A.¬†Mustafi, P.¬†Glaser, A.¬†Korba, A.¬†Gretton, and B.¬†K. Sriperumbudur.
  (de)-regularized maximum mean discrepancy gradient flow.
  arXiv preprint arXiv:2409.14980, 2024.
* [16]

  A.¬†Chiarini, G.¬†Conforti, G.¬†Greco, and L.¬†Tamanini.
  Gradient estimates for the schr√∂dinger potentials: convergence to
  the brenier map and quantitative stability.
  Communications in Partial Differential Equations,
  48(6):895‚Äì943, 2023.
* [17]

  S.-N. Chow, W.¬†Li, C.¬†Mou, and H.¬†Zhou.
  Dynamical schr√∂dinger bridge problems on graphs.
  Journal of Dynamics and Differential Equations, pages 1‚Äì20,
  2022.
* [18]

  P.¬†Dai¬†Pra.
  A stochastic control approach to reciprocal diffusion processes.
  Applied mathematics and Optimization, 23(1):313‚Äì329, 1991.
* [19]

  G.¬†Dal¬†Maso.
  Introduction to Gamma-convergence.
  Springer Science + Business Media, LLC, 1993.
* [20]

  V.¬†De¬†Bortoli, J.¬†Thornton, J.¬†Heng, and A.¬†Doucet.
  Diffusion schr√∂dinger bridge with applications to score-based
  generative modeling.
  Advances in Neural Information Processing Systems,
  34:17695‚Äì17709, 2021.
* [21]

  W.¬†E. Deming and F.¬†F. Stephan.
  On a least squares adjustment of a sampled frequency table when the
  expected marginal totals are known.
  The Annals of Mathematical Statistics, 11(4):427‚Äì444, 1940.
* [22]

  V.¬†Divol, J.¬†Niles-Weed, and A.-A. Pooladian.
  Tight stability bounds for entropic brenier maps.
  International Mathematics Research Notices, 2025(7):rnaf078,
  2025.
* [23]

  D.¬†A. Edwards.
  On the kantorovish-rubinstein theorem.
  Expositiones Mathematicae, 29:387‚Äì398, 2011.
* [24]

  K.¬†D. Elworthy and X.¬†M. Li.
  Formulae for the derivatives of heat semigroups.
  Journal of Functional Analysis, 125:252‚Äì286, 1994.
* [25]

  H.¬†F√∂llmer.
  Random fields and diffusion processes.
  In √âcole d‚Äô√ât√© de Probabilit√©s de Saint-Flour
  XV‚ÄìXVII, 1985‚Äì87, pages 101‚Äì203. Springer, 2006.
* [26]

  R.¬†Fortet.
  R√©solution d‚Äôun syst√®me d‚Äô√©quations de m.
  schr√∂dinger.
  Journal de math√©matiques pures et appliqu√©es,
  19(1-4):83‚Äì105, 1940.
* [27]

  E.¬†Fourni√©, J.-M. Lasry, J.¬†Lebuchoux, P.-L. Lions, and N.¬†Touzi.
  Applications of malliavin calculus to monte carlo methods in finance.
  Finance and Stochastics, 3:391‚Äì412, 1999.
* [28]

  H.¬†Fu, Z.¬†Yang, M.¬†Wang, and M.¬†Chen.
  Unveil conditional diffusion models with classifier-free guidance: A
  sharp statistical theory.
  arXiv preprint arXiv:2403.11968, 2024.
* [29]

  J.¬†Garg, X.¬†Zhang, and Q.¬†Zhou.
  Soft-constrained schr√∂dinger bridge: a stochastic control
  approach.
  In International Conference on Artificial Intelligence and
  Statistics, pages 4429‚Äì4437. PMLR, 2024.
* [30]

  X.¬†Gu, C.¬†Du, T.¬†Pang, C.¬†Li, M.¬†Lin, and Y.¬†Wang.
  On memorization in diffusion models.
  arXiv preprint arXiv:2310.02664, 2023.
* [31]

  F.¬†F. Gunsilius.
  On the convergence rate of potentials of brenier maps.
  Econometric Theory, 38(2):381‚Äì417, 2022.
* [32]

  M.¬†Hamdouche, P.¬†Henry-Labordere, and H.¬†Pham.
  Generative modeling for time series via schr {\{\\backslash" o}\}
  dinger bridge.
  arXiv preprint arXiv:2304.05093, 2023.
* [33]

  Y.¬†Han, M.¬†Razaviyayn, and R.¬†Xu.
  Neural network-based score estimation in diffusion models:
  Optimization and generalization.
  arXiv preprint arXiv:2401.15604, 2024.
* [34]

  Y.¬†Han, M.¬†Razaviyayn, and R.¬†Xu.
  Stochastic control for fine-tuning diffusion models: Optimality,
  regularity, and convergence.
  arXiv preprint arXiv:2412.18164, 2024.
* [35]

  C.¬†Hern√°ndez and L.¬†Tangpi.
  Propagation of chaos for mean field schr\\backslash" odinger
  problems.
  arXiv preprint arXiv:2304.09340, 2023.
* [36]

  J.¬†Ho, A.¬†Jain, and P.¬†Abbeel.
  Denoising diffusion probabilistic models.
  In Neurips, volume¬†33, pages 6840‚Äì6851, 2020.
* [37]

  B.¬†Jamison.
  The markov processes of schr√∂dinger.
  Zeitschrift f√ºr Wahrscheinlichkeitstheorie und verwandte
  Gebiete, 32(4):323‚Äì331, 1975.
* [38]

  L.¬†Kong, H.¬†Wang, T.¬†Wang, G.¬†Xiong, and M.¬†Tambe.
  Composite flow matching for reinforcement learning with
  shifted-dynamics data.
  arXiv preprint arXiv:2505.23062, 2025.
* [39]

  S.¬†Kullback and R.¬†Leibler.
  On information and sufficiency.
  Annals of Mathematical Statistics, 22(1):79‚Äì86, 1951.
* [40]

  C.¬†L√©onard.
  A survey of the schr\\backslash" odinger problem and some of its
  connections with optimal transport.
  arXiv preprint arXiv:1308.0215, 2013.
* [41]

  P.¬†Li, Z.¬†Li, H.¬†Zhang, and J.¬†Bian.
  On the generalization properties of diffusion models.
  Advances in Neural Information Processing Systems,
  36:2097‚Äì2127, 2023.
* [42]

  G.¬†Loeper, J.¬†Obloj, and B.¬†Joseph.
  Calibration of local volatility models with stochastic interest rates
  using optimal transport.
  arXiv preprint arXiv:2305.00200, 2023.
* [43]

  J.¬†Ma and J.¬†Zhang.
  Representation theorems for backward stochastic differential
  equations.
  Annals of Applied Probability, 12(4):1390‚Äì1418, 2002.
* [44]

  E.¬†F. Montesuma, F.¬†M.¬†N. Mboula, and A.¬†Souloumiac.
  Recent advances in optimal transport for machine learning.
  IEEE Transactions on Pattern Analysis and Machine Intelligence,
  2024.
* [45]

  T.¬†Moon, M.¬†Choi, G.¬†Lee, J.-W. Ha, and J.¬†Lee.
  Fine-tuning diffusion models with limited data.
  In NeurIPS 2022 Workshop on Score-Based Methods, 2022.
* [46]

  M.¬†Nutz and J.¬†Wiesel.
  Entropic optimal transport: Convergence of potentials.
  Probability Theory and Related Fields, 184(1):401‚Äì424, 2022.
* [47]

  OpenAI.
  Sora: Creating video from text.
  <https://openai.com/sora>, 2024.
* [48]

  Y.¬†Ouyang, L.¬†Xie, H.¬†Zha, and G.¬†Cheng.
  Transfer learning for diffusion models.
  Advances in Neural Information Processing Systems,
  37:136962‚Äì136989, 2024.
* [49]

  S.¬†Peluchetti.
  Diffusion bridge mixture transports, schr√∂dinger bridge problems
  and generative modeling.
  Journal of Machine Learning Research, 24(374):1‚Äì51, 2023.
* [50]

  P.¬†E. Protter.
  Stochastic Integration and Differential Equations.
  Springer-Verlag, Heidelberg, second edition, 2005.
* [51]

  A.¬†Ramesh, P.¬†Dhariwal, A.¬†Nichol, C.¬†Chu, and M.¬†Chen.
  Hierarchical text-conditional image generation with clip latents.
  arXiv preprint arXiv:2204.06125, 1(2):3, 2022.
* [52]

  L.¬†Richter and J.¬†Berner.
  Improved sampling via learned diffusions.
  arXiv preprint arXiv:2307.01198, 2023.
* [53]

  R.¬†Rombach, A.¬†Blattmann, D.¬†Lorenz, P.¬†Esser, and B.¬†Ommer.
  High-resolution image synthesis with latent diffusion models.
  In Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition, pages 10684‚Äì10695, 2022.
* [54]

  Y.¬†Shi, V.¬†De¬†Bortoli, A.¬†Campbell, and A.¬†Doucet.
  Diffusion schr√∂dinger bridge matching.
  Advances in Neural Information Processing Systems, 36, 2024.
* [55]

  J.¬†Sohl-Dickstein, E.¬†Weiss, N.¬†Maheswaranathan, and S.¬†Ganguli.
  Deep unsupervised learning using nonequilibrium thermodynamics.
  In International Conference on Machine Learning, pages
  2256‚Äì2265. PMLR, 2015.
* [56]

  K.-U. Song.
  Applying regularized schr\\backslash" odinger-bridge-based
  stochastic process in generative modeling.
  arXiv preprint arXiv:2208.07131, 2022.
* [57]

  Y.¬†Song and S.¬†Ermon.
  Generative modeling by estimating gradients of the data distribution.
  Advances in neural information processing systems, 32, 2019.
* [58]

  Y.¬†Song, J.¬†Sohl-Dickstein, D.¬†P. Kingma, A.¬†Kumar, S.¬†Ermon, and B.¬†Poole.
  Score-based generative modeling through stochastic differential
  equations.
  arXiv preprint arXiv:2011.13456, 2020.
* [59]

  T.¬†Sweeting.
  On a converse to scheff√©‚Äôs theorem.
  The Annals of Statistics, 14(3):1252‚Äì1256, 1986.
* [60]

  W.¬†Tang.
  Fine-tuning of diffusion models via stochastic control: entropy
  regularization and beyond.
  arXiv preprint arXiv:2403.06279, 2024.
* [61]

  L.¬†Torrey and J.¬†Shavlik.
  Transfer learning.
  In Handbook of research on machine learning applications and
  trends: algorithms, methods, and techniques, pages 242‚Äì264. IGI Global
  Scientific Publishing, 2010.
* [62]

  M.¬†Uehara, Y.¬†Zhao, K.¬†Black, E.¬†Hajiramezanali, G.¬†Scalia, N.¬†L. Diamant,
  A.¬†M. Tseng, T.¬†Biancalani, and S.¬†Levine.
  Fine-tuning of continuous-time diffusion models as
  entropy-regularized control.
  arXiv preprint arXiv:2402.15194, 2024.
* [63]

  F.¬†Vargas, S.¬†Padhy, D.¬†Blessing, and N.¬†N√ºsken.
  Transport meets variational inference: Controlled monte carlo
  diffusions.
  arXiv preprint arXiv:2307.01050, 2023.
* [64]

  F.¬†Vargas, P.¬†Thodoroff, A.¬†Lamacraft, and N.¬†Lawrence.
  Solving schr√∂dinger bridges via maximum likelihood.
  Entropy, 23(9):1134, 2021.
* [65]

  C.¬†Villani.
  Topics in Optimal Transportation.
  Graduate Studies in Mathematics, 58. AMS, Providence, RI, 2003.
* [66]

  G.¬†Wang, Y.¬†Jiao, Q.¬†Xu, Y.¬†Wang, and C.¬†Yang.
  Deep generative learning via schr√∂dinger bridge.
  In International conference on machine learning, pages
  10794‚Äì10804. PMLR, 2021.
* [67]

  L.¬†Winkler, C.¬†Ojeda, and M.¬†Opper.
  A score-based approach for training schr√∂dinger bridges for data
  modelling.
  Entropy, 25(2):316, 2023.
* [68]

  T.¬†Xu, L.¬†K. Wenliang, M.¬†Munn, and B.¬†Acciaio.
  Cot-gan: Generating sequential data via causal optimal transport.
  Advances in neural information processing systems,
  33:8798‚Äì8809, 2020.
* [69]

  J.¬†Zhang.
  Backward stochastic differential equations.
  Springer, 2017.
* [70]

  H.¬†Zhao, H.¬†Chen, Y.¬†Guo, G.¬†I. Winata, T.¬†Ou, Z.¬†Huang, D.¬†D. Yao, and
  W.¬†Tang.
  Fine-tuning diffusion generative models via rich preference
  optimization.
  arXiv preprint arXiv:2503.11720, 2025.