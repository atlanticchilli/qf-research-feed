---
authors:
- Jian'an Zhang
doc_id: arxiv:2510.04555v1
family_id: arxiv:2510.04555
is_current: true
taxonomy:
  alpha_families: []
  asset_classes: []
  horizons: []
  themes: []
title: 'Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with
  a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets'
url_abs: http://arxiv.org/abs/2510.04555v1
url_html: https://arxiv.org/html/2510.04555v1
venue: arXiv q-fin
version: 1
year: 2025
---


ZhangJian‚Äôan
  
Guanghua School of Management, Peking University
  
Peking University
  
Beijing, China
  
2501111059@stu.pku.edu.cn

###### Abstract

We introduce Tail-Safe, a deployability-oriented framework for derivatives hedging that unifies *distributional, risk-sensitive reinforcement learning* with a *white-box* safety layer tailored to financial constraints.
The learning module combines an IQN-based distributional critic with a CVaR objective (IQN‚ÄìCVaR‚ÄìPPO), and is stabilized by a *Tail-Coverage Controller* that regulates quantile sampling via temperature tilting and tail-boosting to preserve effective tail mass at small Œ±\alpha.
The safety module solves a convex *CBF‚ÄìQP* at every step, enforcing discrete-time control barrier inequalities together with a finance-specific constraint set (ellipsoidal no-trade band, box/rate limits, and a sign-consistency gate).
Crucially, the safety layer is *explainable*: it exposes active sets, tightness, rate utilization, gate scores, slack, and solver status‚Äîa self-contained audit trail aligned with model-risk governance.

We provide formal guarantees: (i) *robust forward invariance* of the safe set under bounded model mismatch; (ii) a *minimal-deviation projection* interpretation of the QP; (iii) a *KL‚ÄìDRO* upper bound showing that per-state KL regularization controls worst-case CVaR; (iv) *concentration* and sample-complexity of the temperature-tilted CVaR estimator with explicit coverage-mismatch terms; (v) a *CVaR trust-region* improvement inequality under KL-limited updates; (vi) *feasibility persistence* via expiry-aware NTB shrinkage and rate tightening; and (vii) *negative-advantage suppression* induced by the sign-consistency gate.
Empirically, in arbitrage-free, microstructure-aware synthetic markets (SSVI‚Üí\!\rightarrowDupire‚Üí\!\rightarrowVIX with ABIDES/MockLOB execution), Tail-Safe improves left-tail risk while preserving central performance and yields zero hard-constraint violations whenever the QP is feasible with zero slack.
We also map telemetry to governance workflows (dashboards, triggers, and incident taxonomy) to support auditability.
Limitations include the use of synthetic environments, simplified execution, and the absence of real-data replay; these are deliberate to isolate methodological contributions and are actionable in future work.

*K*eywords‚ÄÇrisk-sensitive reinforcement learning, distributional RL, CVaR, trust-region methods, control barrier functions (CBFs), quadratic programming (QP), safe RL, explainable AI, deep hedging, arbitrage-free volatility (SSVI, Dupire), VIX, market microstructure, distributionally robust optimization (DRO), PPO, IQN, model risk governance

## 1 Introduction

Modern AI agents for derivatives hedging and portfolio risk management must remain *tail-safe*, *robust under distribution shift*, and *auditable* for model risk governance and regulatory review. In equity‚Äìvolatility books such as SPX‚ÄìVIX, a small number of rare but consequential events (flash crashes, volatility spikes, liquidity droughts) dominate economic capital, PnL attribution, and operational risk. Agents optimized purely for risk-neutral or average performance‚Äîe.g., standard reinforcement learning (RL) objectives or black-box deep hedgers‚Äîcan look competitive ex post yet still incur catastrophic left-tail losses, violate hard business rules (leverage, short-sale limits, liquidity/min-trade-size, turnover and drawdown constraints), or fail under microstructure stress¬†[[6](https://arxiv.org/html/2510.04555v1#bib.bib6), [10](https://arxiv.org/html/2510.04555v1#bib.bib10), [11](https://arxiv.org/html/2510.04555v1#bib.bib11), [12](https://arxiv.org/html/2510.04555v1#bib.bib12)]. These shortcomings are amplified when training is performed in stylized simulators that under-represent jump risk, regime changes, or execution frictions. At the same time, realistic deployment requires reasoning about market impact, order-book dynamics, and latency that classical optimal execution models capture only partially¬†[[1](https://arxiv.org/html/2510.04555v1#bib.bib1), [5](https://arxiv.org/html/2510.04555v1#bib.bib5), [2](https://arxiv.org/html/2510.04555v1#bib.bib2)]. Taken together, these pressures motivate methods that (i) *optimize tail risk* rather than only the mean, (ii) *guarantee state-wise safety by construction* at execution time, and (iii) *explain interventions* through telemetry that supports model validation and external audit.

##### From risk-neutral RL to risk-sensitive, distributional RL.

A central ingredient is to reason about *distributions of returns* rather than their expectations. Distributional RL explicitly models the random return ZœÄZ^{\pi} and its quantiles, enabling direct access to tail statistics and risk measures (e.g., VaR/CVaR) that are meaningful for trading books¬†[[13](https://arxiv.org/html/2510.04555v1#bib.bib13), [14](https://arxiv.org/html/2510.04555v1#bib.bib14)]. On-policy trust-region style updates (TRPO/PPO) stabilize training through KL control and clipping, reducing policy oscillations that can otherwise translate into erratic trading¬†[[15](https://arxiv.org/html/2510.04555v1#bib.bib15), [16](https://arxiv.org/html/2510.04555v1#bib.bib16)]. Building on these foundations, risk-sensitive RL incorporates coherent risk measures and chance constraints into the objective or constraints of the Markov decision process, yielding actor‚Äìcritic and policy-gradient algorithms with convergence guarantees¬†[[17](https://arxiv.org/html/2510.04555v1#bib.bib17), [19](https://arxiv.org/html/2510.04555v1#bib.bib19), [18](https://arxiv.org/html/2510.04555v1#bib.bib18)]. Recent advances scale CVaR-style training, improve tail estimators, and study policy updates under distribution shift and partial model misspecification¬†[[35](https://arxiv.org/html/2510.04555v1#bib.bib35), [36](https://arxiv.org/html/2510.04555v1#bib.bib36), [37](https://arxiv.org/html/2510.04555v1#bib.bib37)]. These directions are especially pertinent in finance, where left-tail losses dominate risk budgets, and where audits require transparent objectives and measurable risk appetites¬†[[9](https://arxiv.org/html/2510.04555v1#bib.bib9), [11](https://arxiv.org/html/2510.04555v1#bib.bib11), [10](https://arxiv.org/html/2510.04555v1#bib.bib10), [12](https://arxiv.org/html/2510.04555v1#bib.bib12)]. In this work we adopt distributional critics and quantile-based losses together with explicit CVaR optimization to shape the tail, while retaining PPO-style update discipline to keep policy drift controlled during training.

##### Safety by construction via control barrier functions (CBFs).

Even with risk-sensitive training, *state-wise* safety constraints (e.g., leverage ‚â§\leq cap, inventory/rate boxes, NTB and sign consistency) must be satisfied at each decision, not only on average. Typical safe RL approaches enforce constraints asymptotically, in expectation, or via learned proxies, leaving residual violation risk during exploration or regime shifts¬†[[20](https://arxiv.org/html/2510.04555v1#bib.bib20), [21](https://arxiv.org/html/2510.04555v1#bib.bib21), [22](https://arxiv.org/html/2510.04555v1#bib.bib22)]. CBF-based safety layers provide a complementary, *white-box* mechanism: a control barrier function h‚Äã(x)h(x) defines a forward-invariant safe set ùíÆ={x:h‚Äã(x)‚â•0}\mathcal{S}=\{x:h(x)\geq 0\}; at each step a convex quadratic program (QP) minimally modifies a nominal action u0u\_{0} to ensure a discrete-time CBF condition such as

|  |  |  |
| --- | --- | --- |
|  | h‚Äã(xt+1)‚àí(1‚àíŒ±)‚Äãh‚Äã(xt)‚â•‚ÄÑ0,h(x\_{t+1})-(1-\alpha)h(x\_{t})\;\geq\;0, |  |

subject to box/rate and market constraints, where Œ±‚àà(0,1]\alpha\in(0,1] regulates conservatism¬†[[23](https://arxiv.org/html/2510.04555v1#bib.bib23), [24](https://arxiv.org/html/2510.04555v1#bib.bib24)]. The CBF literature now includes high-order and robust variants, differentiable formulations for learning, discrete-time treatments, and observer/disturbance-aware designs suitable for implementation¬†[[25](https://arxiv.org/html/2510.04555v1#bib.bib25), [26](https://arxiv.org/html/2510.04555v1#bib.bib26), [27](https://arxiv.org/html/2510.04555v1#bib.bib27), [28](https://arxiv.org/html/2510.04555v1#bib.bib28), [29](https://arxiv.org/html/2510.04555v1#bib.bib29), [30](https://arxiv.org/html/2510.04555v1#bib.bib30), [31](https://arxiv.org/html/2510.04555v1#bib.bib31), [32](https://arxiv.org/html/2510.04555v1#bib.bib32)]. For financial RL, a CBF‚ÄìQP safety layer is attractive because it (i) encodes leverage, liquidity, and sign-consistency gates *by construction*; (ii) exposes KKT multipliers, active sets, and slack, yielding human-interpretable *reasons* for action overrides; and (iii) composes with arbitrary nominal policies without retraining.

##### Robustness to model misspecification via distributional robustness and KL control.

Agents trained on synthetic markets inevitably face deployment regimes with different volatility-of-volatility, jump intensities, liquidity states, and order-flow autocorrelations. Distributionally robust MDPs (DRMDPs) guard against such misspecification by optimizing worst-case value within ambiguity sets defined by ff-divergence or Wasserstein balls; practical algorithms cover online/offline and model-based/model-free settings¬†[[37](https://arxiv.org/html/2510.04555v1#bib.bib37), [35](https://arxiv.org/html/2510.04555v1#bib.bib35), [36](https://arxiv.org/html/2510.04555v1#bib.bib36), [38](https://arxiv.org/html/2510.04555v1#bib.bib38)]. In parallel, KL-regularized policy updates‚Äîas in TRPO/PPO‚Äîadmit a robust-optimization reading: a per-state KL penalty is the Fenchel dual of an adversary constrained to a KL ball around the behavior distribution, providing an interpretable *risk-budget knob* for conservatism during tail-focused learning¬†[[15](https://arxiv.org/html/2510.04555v1#bib.bib15), [16](https://arxiv.org/html/2510.04555v1#bib.bib16)]. We further view adversarial/stress-testing methods from robust RL as complementary tools to expose fragilities before live trading¬†[[34](https://arxiv.org/html/2510.04555v1#bib.bib34), [33](https://arxiv.org/html/2510.04555v1#bib.bib33)].

##### Finance-grounded synthetic evaluation.

Auditable studies require *arbitrage-free* yet *stressable* simulators. Practitioners routinely calibrate SSVI volatility surfaces under static no-arbitrage constraints¬†[[3](https://arxiv.org/html/2510.04555v1#bib.bib3)] and derive state-dependent dynamics via Dupire local volatility¬†[[4](https://arxiv.org/html/2510.04555v1#bib.bib4)]. VIX-consistent legs can be inferred from the surface, enabling joint SPX‚ÄìVIX exposures. Execution realism demands microstructure-aware simulators and impact models: Almgren‚ÄìChriss captures linear temporary impact and risk/cost trade-offs, while Obizhaeva‚ÄìWang models transient resilience¬†[[1](https://arxiv.org/html/2510.04555v1#bib.bib1), [5](https://arxiv.org/html/2510.04555v1#bib.bib5)]. Open-source agent-based platforms such as ABIDES and ABIDES-Gym bring limit-order books, latencies, and interacting agents to RL research, bridging the gap between stylized price processes and exchange-like environments¬†[[7](https://arxiv.org/html/2510.04555v1#bib.bib7), [8](https://arxiv.org/html/2510.04555v1#bib.bib8), [39](https://arxiv.org/html/2510.04555v1#bib.bib39)]. This stack‚ÄîSSVI‚Üí\!\toDupire‚Üí\!\toVIX with LOB execution and impact‚Äîhas become common in surveys and practice for reproducible stress testing and policy benchmarking¬†[[10](https://arxiv.org/html/2510.04555v1#bib.bib10), [11](https://arxiv.org/html/2510.04555v1#bib.bib11), [12](https://arxiv.org/html/2510.04555v1#bib.bib12)].

##### This paper: Tail-Safe hedging with an explainable safety layer.

We propose Tail-Safe, a hybrid *learn‚Äìthen‚Äìfilter* framework for SPX‚ÄìVIX hedging that combines risk-sensitive learning with white-box safety and robustness. (i) The learning component trains a distributional critic with CVaR objectives, stabilized by a *quantile-coverage controller* that tracks the effective tail mass during training to reduce estimator variance and policy churn. (ii) The execution component enforces *state-wise* constraints via a finance-specific CBF‚ÄìQP layer that implements NTB shrinkage, box/rate limits, and sign-consistency, while exporting telemetry (active-set identity, tightest constraint, slack/rate utilization) suitable for audit. Policy updates follow KL-regularized PPO with an EMA reference policy to cap per-epoch drift¬†[[16](https://arxiv.org/html/2510.04555v1#bib.bib16)]. Evaluation occurs in an arbitrage-free SSVI‚Üí\!\rightarrowDupire‚Üí\!\rightarrowVIX pipeline with ABIDES/LOB execution and classical impact models¬†[[3](https://arxiv.org/html/2510.04555v1#bib.bib3), [4](https://arxiv.org/html/2510.04555v1#bib.bib4), [7](https://arxiv.org/html/2510.04555v1#bib.bib7), [1](https://arxiv.org/html/2510.04555v1#bib.bib1), [5](https://arxiv.org/html/2510.04555v1#bib.bib5)].

##### Mathematical guarantees and explainability (paper highlights).

To meet top-tier standards and strengthen interpretability, we develop and prove:

* ‚Ä¢

  Discrete-time invariance & minimal-deviation (Theorem¬†1). For the proposed discrete-time CBF constraints, if the QP is feasible with zero slack, the safe set is forward-invariant. The HH-metric objective selects the uniquely closest feasible action to the nominal proposal, making the safety intervention *predictable* and *stable*¬†[[23](https://arxiv.org/html/2510.04555v1#bib.bib23), [25](https://arxiv.org/html/2510.04555v1#bib.bib25)].
* ‚Ä¢

  DRO‚ÄìKL equivalence (Theorem¬†2). A per-state KL penalty in PPO corresponds to the Fenchel dual of a KL-ball worst-case objective, quantifying conservatism and providing a tunable *risk budget* that links trust-region size to adversarial uncertainty¬†[[35](https://arxiv.org/html/2510.04555v1#bib.bib35), [36](https://arxiv.org/html/2510.04555v1#bib.bib36), [15](https://arxiv.org/html/2510.04555v1#bib.bib15)].
* ‚Ä¢

  Tail-coverage stabilization (Theorem¬†3). A PID-style quantile-coverage controller stabilizes the Monte Carlo CVaR estimator by regulating effective tail mass around a scheduled Œ±\alpha, yielding finite-sample variance control and bounding the gap between scheduled and realized tail coverage under mild regularity.
* ‚Ä¢

  Feasibility under tail guards (Proposition¬†1). With expiry-aware NTB shrinkage, sign-consistency gates, and realistic microstructure and inventory envelopes (rate/box limits), the safety QP admits a nonempty feasible set and avoids deadlock in stressed conditions.
* ‚Ä¢

  Telemetry‚ÄìKKT correspondence (Lemma¬†1). Tightness indicators and dual variables map one-to-one to business-rule interventions (leverage/short-sale/rate caps), producing auditable, human-readable explanations of *why* and *by how much* a proposed trade was modified.

##### Contributions.

(1) Tail-risk learning with stability. We integrate distributional RL (IQN) and CVaR objectives with a *quantile-coverage controller* and PPO+KL updates, improving left-tail behavior without large policy drift¬†[[13](https://arxiv.org/html/2510.04555v1#bib.bib13), [14](https://arxiv.org/html/2510.04555v1#bib.bib14), [16](https://arxiv.org/html/2510.04555v1#bib.bib16)].
  
(2) White-box CBF‚ÄìQP safety. We design a finance-specific CBF‚ÄìQP that enforces NTB, box/rate, and sign-consistency gates with telemetry, enabling *explainable* state-wise safety consistent with the control literature¬†[[23](https://arxiv.org/html/2510.04555v1#bib.bib23), [25](https://arxiv.org/html/2510.04555v1#bib.bib25), [22](https://arxiv.org/html/2510.04555v1#bib.bib22)].
  
(3) Robustness to shift. We couple KL-regularized updates with DRO-motivated penalties to guard against simulator misspecification and out-of-distribution stress¬†[[35](https://arxiv.org/html/2510.04555v1#bib.bib35), [36](https://arxiv.org/html/2510.04555v1#bib.bib36), [37](https://arxiv.org/html/2510.04555v1#bib.bib37)].
  
(4) Finance-grounded evaluation. We build an arbitrage-free SSVI/Dupire/VIX simulator with ABIDES execution and impact models to enable reproducible, stressable studies aligned with practitioner workflows¬†[[3](https://arxiv.org/html/2510.04555v1#bib.bib3), [4](https://arxiv.org/html/2510.04555v1#bib.bib4), [7](https://arxiv.org/html/2510.04555v1#bib.bib7), [1](https://arxiv.org/html/2510.04555v1#bib.bib1), [5](https://arxiv.org/html/2510.04555v1#bib.bib5)].

##### Positioning and outlook.

Compared with black-box deep hedgers or unconstrained RL, Tail-Safe delivers: (i) *hard safety* by construction with audit-ready telemetry; (ii) *tail shaping* via stabilized CVaR learning; and (iii) *robustness* through DRO/KL regularization‚Äîall within markets that respect no-arbitrage and microstructure. This responds to recent surveys calling for safe, trustworthy, and explainable financial RL¬†[[10](https://arxiv.org/html/2510.04555v1#bib.bib10), [11](https://arxiv.org/html/2510.04555v1#bib.bib11), [12](https://arxiv.org/html/2510.04555v1#bib.bib12)]. Beyond SPX‚ÄìVIX, the framework extends to multi-asset books with cross-gamma and funding constraints, richer impact/latency models, and formal certificates for state-dependent CBF margins and DRMDPs¬†[[25](https://arxiv.org/html/2510.04555v1#bib.bib25), [38](https://arxiv.org/html/2510.04555v1#bib.bib38)]. We view these directions as necessary steps toward deployable, regulator-ready AI agents for real-world hedging.

## 2 Preliminaries & Problem Setting

We consider a synthetic yet finance-grounded evaluation stack for equity‚Äìvolatility hedging (e.g., SPX‚ÄìVIX) that is *arbitrage-free*, *microstructure-aware*, and *stressable*.
The market generator follows a calibrated no-arbitrage implied-volatility pipeline SSVI‚Üí\!\rightarrowDupire‚Üí\!\rightarrowVIX, while order execution is simulated through an agent-based limit-order-book (LOB) environment (ABIDES/MockLOB) with temporary and transient impact.
This section formalizes the environment, the agent interface (states/actions/inventory), loss and tail-risk metrics, and the occupancy and KL notions used later.

### 2.1 Arbitrage-Free Volatility Surfaces via SSVI

For each maturity œÑ>0\tau>0, denote by w‚Äã(k,œÑ)w(k,\tau) the total implied variance at log-moneyness k=log‚Å°(K/FœÑ)k=\log(K/F\_{\tau}).
We parameterize ww using the *SSVI* family¬†[[3](https://arxiv.org/html/2510.04555v1#bib.bib3)]:

|  |  |  |  |
| --- | --- | --- | --- |
|  | w‚Äã(k,œÑ)=Œ∏‚Äã(œÑ)2‚Äã(1+œÅ‚Äã(œÑ)‚ÄãœÜ‚Äã(œÑ)‚Äãk+(œÜ‚Äã(œÑ)‚Äãk+œÅ‚Äã(œÑ))2+1‚àíœÅ‚Äã(œÑ)2),w(k,\tau)\;=\;\frac{\theta(\tau)}{2}\left(1\;+\;\rho(\tau)\,\varphi(\tau)\,k\;+\;\sqrt{\big(\varphi(\tau)\,k+\rho(\tau)\big)^{2}+1-\rho(\tau)^{2}}\right), |  | (1) |

where Œ∏‚Äã(œÑ)>0\theta(\tau)>0 is the ATM total variance term-structure, œÜ‚Äã(œÑ)>0\varphi(\tau)>0 the shape, and œÅ‚Äã(œÑ)‚àà(‚àí1,1)\rho(\tau)\!\in\!(-1,1) the skew parameter.
SSVI admits tractable *static no-arbitrage* conditions (no butterfly/calendar arbitrage) as simple inequalities on (Œ∏,œÜ,œÅ)(\theta,\varphi,\rho) across maturities, yielding a calibrated surface free of static arbitrage¬†[[3](https://arxiv.org/html/2510.04555v1#bib.bib3)].
We fit (Œ∏,œÜ,œÅ)(\theta,\varphi,\rho) on SPX option data slices or stylized templates and use¬†([1](https://arxiv.org/html/2510.04555v1#S2.E1 "In 2.1 Arbitrage-Free Volatility Surfaces via SSVI ‚Ä£ 2 Preliminaries & Problem Setting ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")) as the *sole* input to downstream local-volatility and variance measures.

### 2.2 Local Volatility via Dupire

Let C‚Äã(t,K)C(t,K) be the time-tt undiscounted call price with strike KK implied by the SSVI surface.
Under mild regularity and risk-neutrality, the *Dupire* local variance œÉloc2‚Äã(t,K)\sigma\_{\mathrm{loc}}^{2}(t,K) satisfies¬†[[4](https://arxiv.org/html/2510.04555v1#bib.bib4)]:

|  |  |  |  |
| --- | --- | --- | --- |
|  | œÉloc2‚Äã(t,K)=‚àÇtC‚Äã(t,K)12‚ÄãK2‚Äã‚àÇK‚ÄãKC‚Äã(t,K),‚àÇK‚ÄãKC‚Äã(t,K)>0.\sigma\_{\mathrm{loc}}^{2}(t,K)\;=\;\frac{\partial\_{t}C(t,K)}{\tfrac{1}{2}K^{2}\,\partial\_{KK}C(t,K)}\,,\qquad\partial\_{KK}C(t,K)>0. |  | (2) |

We numerically evaluate the derivatives of CC from the SSVI-implied smiles and generate price paths by simulating the local-volatility diffusion d‚ÄãSt=rt‚ÄãSt‚Äãd‚Äãt+œÉloc‚Äã(t,St)‚ÄãSt‚Äãd‚ÄãWtdS\_{t}=r\_{t}S\_{t}\,dt+\sigma\_{\mathrm{loc}}(t,S\_{t})S\_{t}\,dW\_{t}, where rtr\_{t} is the (possibly term-structured) risk-free rate.
This produces an arbitrage-consistent equity leg with skews and term-structure inherited from SSVI.

### 2.3 VIX Leg from Surface-Consistent Variance

Consistent with variance-swap replication¬†[[41](https://arxiv.org/html/2510.04555v1#bib.bib41)], the *30-day VIX* at time tt can be computed from out-of-the-money option prices on the surface as (continuous limit, simplified form; see Cboe white paper¬†[[42](https://arxiv.org/html/2510.04555v1#bib.bib42)]):

|  |  |  |  |
| --- | --- | --- | --- |
|  | VIX2‚Äã(t)=2T‚Äãer‚ÄãT‚Äã(‚à´0FP‚Äã(K,T)K2‚ÄãùëëK+‚à´F‚àûC‚Äã(K,T)K2‚ÄãùëëK),T=30D,\mathrm{VIX}^{2}(t)\;=\;\frac{2}{T}\,e^{rT}\left(\int\_{0}^{F}\frac{P(K,T)}{K^{2}}\,dK\;+\;\int\_{F}^{\infty}\frac{C(K,T)}{K^{2}}\,dK\right),\qquad T=\text{30D}, |  | (3) |

where FF is the forward, and P,CP,C are OTM put/call prices derived from the SSVI surface at horizon TT.
We thus construct a *surface-consistent* VIX leg‚Äîeither as a tradable future proxy or as options on VIX‚Äîensuring joint equity‚Äìvolatility dynamics coherent with the SSVI/Dupire world.

### 2.4 Execution, Microstructure, and Impact

We embed the above market into an agent-based discrete-event LOB simulator using ABIDES/ABIDES-Gym¬†[[7](https://arxiv.org/html/2510.04555v1#bib.bib7), [40](https://arxiv.org/html/2510.04555v1#bib.bib40)], which exposes realistic order matching, latency, and interacting background agents.
Our execution price model follows classical impact literature¬†[[1](https://arxiv.org/html/2510.04555v1#bib.bib1), [5](https://arxiv.org/html/2510.04555v1#bib.bib5), [46](https://arxiv.org/html/2510.04555v1#bib.bib46)]:

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | ptexec\displaystyle p\_{t}^{\mathrm{exec}} | =mt+st2‚Äãsgn‚Äã(ut)+Œ∑‚Äãut+‚àëj=0‚àûG‚Äã(j)‚Äãut‚àíj,\displaystyle=m\_{t}\;+\;\frac{s\_{t}}{2}\,\mathrm{sgn}(u\_{t})\;+\;\eta\,u\_{t}\;+\;\sum\_{j=0}^{\infty}G(j)\,u\_{t-j}, |  | (4) |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | qt+1\displaystyle q\_{t+1} | =qt+ut,Œ†t+1‚àíŒ†t=qt‚Äã(mt+1‚àímt)‚àíCost‚Äã(ut),\displaystyle=q\_{t}+u\_{t},\qquad\Pi\_{t+1}-\Pi\_{t}\;=\;q\_{t}\,(m\_{t+1}-m\_{t})\;-\;\mathrm{Cost}(u\_{t}), |  | (5) |

where mtm\_{t} is the mid-price, sts\_{t} the spread, utu\_{t} the signed trade size, Œ∑\eta the *temporary* (linear) impact coefficient (Almgren‚ÄìChriss), and G‚Äã(‚ãÖ)G(\cdot) a *transient* resilience kernel (Obizhaeva‚ÄìWang).
The realized trading cost Cost‚Äã(ut)\mathrm{Cost}(u\_{t}) aggregates spread, temporary impact, and transient slippage implied by the ABIDES fill process.
We adopt *no-dynamic-arbitrage* constraints on impact to avoid price manipulation¬†[[46](https://arxiv.org/html/2510.04555v1#bib.bib46)].

##### Stress dimensions.

For systematic OOD testing, we perturb SSVI parameters (Œ∏,œÜ,œÅ)(\theta,\varphi,\rho) (level/slope/curvature), equity‚Äìvol correlation, and impact strength/decay, as well as time-to-expiry (near-expiry regimes).
All stress configurations remain free of *static* arbitrage by construction (¬ß[2.1](https://arxiv.org/html/2510.04555v1#S2.SS1 "2.1 Arbitrage-Free Volatility Surfaces via SSVI ‚Ä£ 2 Preliminaries & Problem Setting ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")).

### 2.5 Task, Agent Interface, and Loss

We consider a finite-horizon, discrete-time hedging task t=0,1,‚Ä¶,T‚àí1t=0,1,\dots,T{-}1.
At time tt, the agent observes a state xt‚àà‚Ñùdx\_{t}\in\mathbb{R}^{d} (prices/Greeks/surface features/time-to-expiry/inventory/LOB and execution features) and issues an action ut‚àà‚Ñùmu\_{t}\in\mathbb{R}^{m} (trade vector across SPX/VIX instruments).
Inventory qtq\_{t} evolves per¬†([4](https://arxiv.org/html/2510.04555v1#S2.E4 "In 2.4 Execution, Microstructure, and Impact ‚Ä£ 2 Preliminaries & Problem Setting ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")).
Let Œ†T‚Äã(œÄ;œâ)\Pi\_{T}(\pi;\omega) denote terminal P&L under policy œÄ\pi on path œâ\omega; we define *loss* as LT‚Äã(œÄ;œâ)=‚àíŒ†T‚Äã(œÄ;œâ)L\_{T}(\pi;\omega)=-\,\Pi\_{T}(\pi;\omega), including transaction costs and slippage from the execution adapter.

### 2.6 Risk Measures: VaR and ES/CVaR

For a confidence level Œ±‚àà(0,1)\alpha\in(0,1) and random loss LL, define

|  |  |  |  |
| --- | --- | --- | --- |
|  | VaRŒ±(L)=inf{z‚àà‚Ñù:‚Ñô(L‚â§z)‚â•Œ±},ESŒ±(L)=ùîº[L|L‚â•VaRŒ±(L)],\operatorname{VaR}\_{\alpha}(L)\;=\;\inf\{z\in\mathbb{R}:\;\mathbb{P}(L\leq z)\geq\alpha\},\qquad\operatorname{ES}\_{\alpha}(L)\;=\;\mathbb{E}\!\left[L\,\middle|\,L\geq\operatorname{VaR}\_{\alpha}(L)\right], |  | (6) |

i.e., *Expected Shortfall* (a.k.a. CVaR) is the conditional mean of losses beyond VaR.
ES is a coherent risk measure¬†[[44](https://arxiv.org/html/2510.04555v1#bib.bib44)] and admits convex optimization surrogates¬†[[43](https://arxiv.org/html/2510.04555v1#bib.bib43)].
We will evaluate policies using absolute-loss VaRŒ±\operatorname{VaR}\_{\alpha} and ESŒ±\operatorname{ES}\_{\alpha} together with central-performance ratios.

### 2.7 Occupancy Measures and Policy KL

Let œÄ‚Äã(u|x)\pi(u|x) denote the stochastic policy, and let dœÄ‚Äã(x)d\_{\pi}(x) be the *state occupancy* over the finite horizon:
dœÄ‚Äã(x)=1T‚Äã‚àët=0T‚àí1‚ÑôœÄ‚Äã(xt=x)d\_{\pi}(x)=\frac{1}{T}\sum\_{t=0}^{T-1}\mathbb{P}\_{\pi}(x\_{t}=x).
We also use the *discounted* occupancy when appropriate.
For conservatism, per-state *Kullback‚ÄìLeibler* (KL) regularization between the updated policy and a reference œÄref\pi\_{\mathrm{ref}} is defined as

|  |  |  |  |
| --- | --- | --- | --- |
|  | KL(œÄ(‚ãÖ|x)‚à•œÄref(‚ãÖ|x))=‚à´œÄ(u|x)logœÄ‚Äã(u|x)œÄref‚Äã(u|x)du,\mathrm{KL}\!\big(\pi(\cdot|x)\,\|\,\pi\_{\mathrm{ref}}(\cdot|x)\big)\;=\;\int\pi(u|x)\,\log\frac{\pi(u|x)}{\pi\_{\mathrm{ref}}(u|x)}\,du, |  | (7) |

and aggregated across x‚àºdœÄx\sim d\_{\pi}.
KL plays a dual role: (i) as a *trust-region* step-size proxy in policy optimization and (ii) as a distributionally robust regularizer (Fenchel dual of a KL-ball worst-case objective), thus controlling sensitivity to simulator misspecification¬†[[45](https://arxiv.org/html/2510.04555v1#bib.bib45), [43](https://arxiv.org/html/2510.04555v1#bib.bib43)].

### 2.8 Notation Summary

Table 1: Notation used throughout the paper (symbols refer to their values at time tt unless otherwise noted).

| Symbol | Meaning |
| --- | --- |
| StS\_{t} | Underlying equity mid-price; KK strike; FF forward |
| w‚Äã(k,œÑ)w(k,\tau) | Total implied variance at log-moneyness kk and maturity œÑ\tau (SSVI; Eq.¬†([1](https://arxiv.org/html/2510.04555v1#S2.E1 "In 2.1 Arbitrage-Free Volatility Surfaces via SSVI ‚Ä£ 2 Preliminaries & Problem Setting ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"))) |
| œÉloc‚Äã(t,S)\sigma\_{\mathrm{loc}}(t,S) | Dupire local volatility (Eq.¬†([2](https://arxiv.org/html/2510.04555v1#S2.E2 "In 2.2 Local Volatility via Dupire ‚Ä£ 2 Preliminaries & Problem Setting ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"))) |
| VIX‚Äã(t)\mathrm{VIX}(t) | 30D variance index computed from OTM option integrals (Eq.¬†([3](https://arxiv.org/html/2510.04555v1#S2.E3 "In 2.3 VIX Leg from Surface-Consistent Variance ‚Ä£ 2 Preliminaries & Problem Setting ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"))) |
| mt,stm\_{t},s\_{t} | LOB mid-price and bid‚Äìask spread |
| ut,qtu\_{t},q\_{t} | Trade vector (signed); inventory vector |
| ptexecp\_{t}^{\mathrm{exec}} | Execution price with spread/temporary/transient impact (Eq.¬†([4](https://arxiv.org/html/2510.04555v1#S2.E4 "In 2.4 Execution, Microstructure, and Impact ‚Ä£ 2 Preliminaries & Problem Setting ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"))) |
| Œ†T\Pi\_{T} | Terminal P&L; LT=‚àíŒ†TL\_{T}=-\Pi\_{T} loss |
| VaRŒ±,ESŒ±\operatorname{VaR}\_{\alpha},\operatorname{ES}\_{\alpha} | Tail risk measures at confidence Œ±\alpha (¬ß[2.6](https://arxiv.org/html/2510.04555v1#S2.SS6 "2.6 Risk Measures: VaR and ES/CVaR ‚Ä£ 2 Preliminaries & Problem Setting ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")) |
| œÄ‚Äã(u|x)\pi(u|x), œÄref‚Äã(u|x)\pi\_{\mathrm{ref}}(u|x) | Stochastic policy and EMA reference policy |
| dœÄ‚Äã(x)d\_{\pi}(x) | (Discounted) state occupancy under œÄ\pi (¬ß[2.7](https://arxiv.org/html/2510.04555v1#S2.SS7 "2.7 Occupancy Measures and Policy KL ‚Ä£ 2 Preliminaries & Problem Setting ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")) |
| KL‚Äã(œÄ‚à•œÄref)\mathrm{KL}(\pi\|\pi\_{\mathrm{ref}}) | Per-state Kullback‚ÄìLeibler divergence (Eq.¬†([7](https://arxiv.org/html/2510.04555v1#S2.E7 "In 2.7 Occupancy Measures and Policy KL ‚Ä£ 2 Preliminaries & Problem Setting ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"))) |
| rtr\_{t} | Risk-free rate; TT hedging horizon length (steps); Œî‚Äãt\Delta t step size |

##### Remark (calibration and units).

All option prices are undiscounted unless stated otherwise; rtr\_{t} denotes continuously compounded rates.
We report P&L in currency units and normalize risk (e.g., ES) by notional or premium where indicated for comparability across scenarios.

## 3 Method: Tail-Safe Hedging Framework

This section presents Tail-Safe: a hybrid *learn‚Äìthen‚Äìfilter* framework that couples risk-sensitive, distributional reinforcement learning with a *white-box* CBF‚ÄìQP safety layer.
Section¬†[3.1](https://arxiv.org/html/2510.04555v1#S3.SS1 "3.1 Risk-Sensitive RL with IQN‚ÄìCVaR‚ÄìPPO ‚Ä£ 3 Method: Tail-Safe Hedging Framework ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") details the IQN‚ÄìCVaR‚ÄìPPO learner and its KL-/entropy-regularized objective;
Section¬†[3.2](https://arxiv.org/html/2510.04555v1#S3.SS2 "3.2 Tail-Coverage Controller: Temperature Sampling and Tail-Boost ‚Ä£ 3 Method: Tail-Safe Hedging Framework ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") introduces a *Tail-Coverage Controller* that stabilizes low-Œ±\alpha CVaR estimation via temperature-based quantile sampling and tail-boosting;
Section¬†[3.3](https://arxiv.org/html/2510.04555v1#S3.SS3 "3.3 White-Box CBF‚ÄìQP Safety Layer ‚Ä£ 3 Method: Tail-Safe Hedging Framework ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") specifies the discrete-time CBF constraints, the ellipsoidal no-trade band (NTB), box/rate limits, and a sign-consistency gate, together with audit-ready telemetry.
Figure¬†[1](https://arxiv.org/html/2510.04555v1#S3.F1 "Figure 1 ‚Ä£ 3 Method: Tail-Safe Hedging Framework ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") summarizes the overall architecture.

![Refer to caption](model.png)


Figure 1: Tail-Safe overview. (a) Market & Execution: SSVI ‚Üí\rightarrow Dupire ‚Üí\rightarrow 30D VIX; ABIDES/MockLOB execution with temporary and transient impact.
(b) IQN‚ÄìCVaR‚ÄìPPO: distributional critic via quantile regression, PPO with KL and entropy regularization.
(c) White-box CBF‚ÄìQP safety layer: discrete-time CBF, ellipsoidal NTB, box/rate limits, and a sign-consistency gate solved as a convex QP.
(d) Telemetry & risk metrics: VaR/ES\mathrm{VaR}/\mathrm{ES}, policy KL, tail coverage, active-set, tightest-constraint ID, rate utilization, gate score, and slack.

### 3.1 Risk-Sensitive RL with IQN‚ÄìCVaR‚ÄìPPO

##### Quantile networks and the CVaR objective.

Distributional RL models the return distribution ZœÄ‚Äã(x,u)Z\_{\pi}(x,u) instead of its mean¬†[[13](https://arxiv.org/html/2510.04555v1#bib.bib13)].
Implicit Quantile Networks (IQN) learn a differentiable quantile function Qœà‚Äã(x,u;œÑ)‚âàFZœÄ(‚ãÖ|x,u)‚àí1‚Äã(œÑ)Q\_{\psi}(x,u;\tau)\!\approx\!F^{-1}\_{Z\_{\pi}(\cdot|x,u)}(\tau)¬†[[14](https://arxiv.org/html/2510.04555v1#bib.bib14)].
Let loss be L=‚àíZL=-Z. For xx fixed, the conditional CVaR admits the quantile integral

|  |  |  |  |
| --- | --- | --- | --- |
|  | CVaRŒ±‚Äã(L|x)=1Œ±‚Äã‚à´0Œ±FL|x‚àí1‚Äã(œÑ)‚ÄãùëëœÑ‚âà1K‚ÄãŒ±‚Äã‚àëk=1Kùüè‚Äã{œÑk‚â§Œ±}‚ÄãL^‚Äã(x,œÑk),\mathrm{CVaR}\_{\alpha}(L\,|\,x)\;=\;\frac{1}{\alpha}\int\_{0}^{\alpha}F^{-1}\_{L\,|\,x}(\tau)\,d\tau\;\approx\;\frac{1}{K\alpha}\sum\_{k=1}^{K}\mathbf{1}\{\tau\_{k}\leq\alpha\}\,\widehat{L}(x,\tau\_{k}), |  | (8) |

with œÑk‚àºùí∞‚Äã(0,1)\tau\_{k}\!\sim\!\mathcal{U}(0,1) (or, in our case, a temperature-tilted distribution; cf. ¬ß[3.2](https://arxiv.org/html/2510.04555v1#S3.SS2 "3.2 Tail-Coverage Controller: Temperature Sampling and Tail-Boost ‚Ä£ 3 Method: Tail-Safe Hedging Framework ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")) and L^‚Äã(x,œÑ)=‚àíùîºu‚àºœÄ(‚ãÖ|x)‚Äã[Qœà‚Äã(x,u;œÑ)]\widehat{L}(x,\tau)=-\,\mathbb{E}\_{u\sim\pi(\cdot|x)}[Q\_{\psi}(x,u;\tau)].
At the episode level we use the Rockafellar‚ÄìUryasev CVaR surrogate¬†[[43](https://arxiv.org/html/2510.04555v1#bib.bib43)] (or its quantile approximation) and train with Monte Carlo estimates.

##### CVaR-weighted advantage and PPO updates.

Let VŒ±œÄ‚Äã(x)‚âàCVaRŒ±‚Äã(L|x)V\_{\alpha}^{\pi}(x)\!\approx\!\mathrm{CVaR}\_{\alpha}(L\,|\,x) and define a *CVaR-weighted* generalized advantage

|  |  |  |  |
| --- | --- | --- | --- |
|  | At(Œ±)‚âà‚àël=0‚àû(Œ≥‚ÄãŒª)l‚Äã(‚Ñìt+l‚àíV^Œ±‚Äã(xt+l)+Œ≥‚ÄãV^Œ±‚Äã(xt+l+1)),A\_{t}^{(\alpha)}\;\approx\;\sum\_{l=0}^{\infty}(\gamma\lambda)^{l}\Big(\ell\_{t+l}-\widehat{V}\_{\alpha}(x\_{t+l})+\gamma\,\widehat{V}\_{\alpha}(x\_{t+l+1})\Big), |  | (9) |

where ‚Ñìt\ell\_{t} is a one-step loss (including spread, temporary impact, and transient slippage) and the structure mirrors GAE¬†[[47](https://arxiv.org/html/2510.04555v1#bib.bib47)] with a CVaR baseline.
The actor uses PPO¬†[[16](https://arxiv.org/html/2510.04555v1#bib.bib16)] with KL and entropy regularization:

|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚Ñíactor=ùîºt[min(rt(Œ∏)At(Œ±),clip(rt(Œ∏),1‚àíœµ,1+œµ)At(Œ±))]+ŒªKLùîºx[KL(œÄŒ∏(‚ãÖ|x)‚à•œÄref(‚ãÖ|x))]+Œªentùîºx[‚Ñã(œÄŒ∏(‚ãÖ|x))],\mathcal{L}\_{\text{actor}}=\mathbb{E}\_{t}\!\left[\min\!\Big(r\_{t}(\theta)\,A\_{t}^{(\alpha)},\;\mathrm{clip}(r\_{t}(\theta),1-\epsilon,1+\epsilon)\,A\_{t}^{(\alpha)}\Big)\right]\;+\;\lambda\_{\mathrm{KL}}\,\mathbb{E}\_{x}\big[\mathrm{KL}(\pi\_{\theta}(\cdot|x)\,\|\,\pi\_{\mathrm{ref}}(\cdot|x))\big]\;+\;\lambda\_{\mathrm{ent}}\,\mathbb{E}\_{x}[\mathcal{H}(\pi\_{\theta}(\cdot|x))], |  | (10) |

with rt‚Äã(Œ∏)=œÄŒ∏‚Äã(ut|xt)/œÄŒ∏old‚Äã(ut|xt)r\_{t}(\theta)=\pi\_{\theta}(u\_{t}|x\_{t})/\pi\_{\theta\_{\mathrm{old}}}(u\_{t}|x\_{t}) and œÄref\pi\_{\mathrm{ref}} an EMA reference policy.
The KL term serves as a trust-region proxy¬†[[15](https://arxiv.org/html/2510.04555v1#bib.bib15), [48](https://arxiv.org/html/2510.04555v1#bib.bib48)] and admits a DRO interpretation (see ¬ß4).
The critic minimizes the quantile Huber loss for QœàQ\_{\psi}¬†[[14](https://arxiv.org/html/2510.04555v1#bib.bib14)].

##### Implementation notes.

Trajectories are collected *through* the safety filter (Sec.¬†[3.3](https://arxiv.org/html/2510.04555v1#S3.SS3 "3.3 White-Box CBF‚ÄìQP Safety Layer ‚Ä£ 3 Method: Tail-Safe Hedging Framework ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")), i.e., the actor proposes utnomu\_{t}^{\mathrm{nom}} which is minimally corrected by the CBF‚ÄìQP into utu\_{t}.
We log solver telemetry (active constraints, tightness, rate utilization, gate scores, slack, status/time) to support both training diagnostics and audit trails.
KL and entropy coefficients can be scheduled to avoid premature collapse and to match the tightening of Œ±\alpha¬†[[16](https://arxiv.org/html/2510.04555v1#bib.bib16)].

### 3.2 Tail-Coverage Controller: Temperature Sampling and Tail-Boost

##### Motivation.

For small Œ±\alpha (e.g., 1%‚àí5%1\%\!-\!5\%), uniform quantile sampling yields few tail samples and high variance, destabilizing training.
We therefore introduce *temperature-tilted* quantile sampling and an explicit *tail-boost*, combined with a PID controller to track a target *effective tail mass*.

##### Temperature-tilted sampling and importance weights.

Define the sampling density over œÑ‚àà[0,1]\tau\in[0,1]:

|  |  |  |  |
| --- | --- | --- | --- |
|  | pT‚Äã(œÑ)‚àùe‚àíœÑ/T,T‚àà[Tmin,Tmax],p\_{T}(\tau)\;\propto\;e^{-\tau/T},\qquad T\in[T\_{\min},T\_{\max}], |  | (11) |

so that smaller TT emphasizes low quantiles.
With œÑk‚àºpT\tau\_{k}\!\sim\!p\_{T}, we use self-normalized importance weights wk‚àù1/pT‚Äã(œÑk)w\_{k}\propto 1/p\_{T}(\tau\_{k}) to form an unbiased CVaR estimator (concentration bounds in ¬ß4; see also standard results on self-normalized importance sampling¬†[[50](https://arxiv.org/html/2510.04555v1#bib.bib50)]).
Additionally, for œÑ‚â§Œ±\tau\leq\alpha we assign a *tail-boost* factor Œ≥tail‚â•1\gamma\_{\mathrm{tail}}\!\geq\!1 to increase the effective tail count.

##### Coverage metric and PID tracking.

Let the *effective tail mass* within a minibatch be

|  |  |  |
| --- | --- | --- |
|  | w^=1K‚Äã‚àëk=1Kùüè‚Äã{œÑk‚â§Œ±}.\widehat{w}\;=\;\frac{1}{K}\sum\_{k=1}^{K}\mathbf{1}\{\tau\_{k}\leq\alpha\}. |  |

Given a target wtargetw\_{\mathrm{target}} (e.g., 1.5‚ÄãŒ±1.5\alpha), define error e=wtarget‚àíw^e=w\_{\mathrm{target}}-\widehat{w} and update (T,Œ≥tail)(T,\gamma\_{\mathrm{tail}}) via discrete PID (clipped to feasible ranges):

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | Tn+1\displaystyle T\_{n+1} | =clip‚Äã(Tn+Œ∫P‚Äãen+Œ∫I‚Äã‚àëj=1nej+Œ∫D‚Äã(en‚àíen‚àí1),Tmin,Tmax),\displaystyle=\mathrm{clip}\!\left(T\_{n}+\kappa\_{P}e\_{n}+\kappa\_{I}\sum\_{j=1}^{n}e\_{j}+\kappa\_{D}(e\_{n}-e\_{n-1}),\;T\_{\min},T\_{\max}\right), |  | (12) |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | Œ≥tail,n+1\displaystyle\gamma\_{\mathrm{tail},\,n+1} | =clip‚Äã(Œ≥tail,n+Œ∑P‚Äãen+Œ∑I‚Äã‚àëj=1nej+Œ∑D‚Äã(en‚àíen‚àí1),Œ≥min,Œ≥max).\displaystyle=\mathrm{clip}\!\left(\gamma\_{\mathrm{tail},\,n}+\eta\_{P}e\_{n}+\eta\_{I}\sum\_{j=1}^{n}e\_{j}+\eta\_{D}(e\_{n}-e\_{n-1}),\;\gamma\_{\min},\gamma\_{\max}\right). |  | (13) |

PID design and anti-windup follow classical practice¬†[[51](https://arxiv.org/html/2510.04555v1#bib.bib51)].
We employ an Œ±\alpha schedule that tightens from a permissive level (e.g., 0.100.10) toward the target (e.g., 0.0250.025), while the controller stabilizes w^\widehat{w}; the PPO KL penalty can be increased in tandem to limit policy drift¬†[[16](https://arxiv.org/html/2510.04555v1#bib.bib16), [15](https://arxiv.org/html/2510.04555v1#bib.bib15), [48](https://arxiv.org/html/2510.04555v1#bib.bib48)].

### 3.3 White-Box CBF‚ÄìQP Safety Layer

##### Discrete-time CBF constraints.

Let hi‚Äã(x)‚â•0h\_{i}(x)\!\geq\!0 denote the ii-th safety function and consider a local affine state update xt+1=f‚Äã(xt)+g‚Äã(xt)‚Äãutx\_{t+1}=f(x\_{t})+g(x\_{t})u\_{t}.
We enforce discrete-time CBF conditions¬†[[23](https://arxiv.org/html/2510.04555v1#bib.bib23), [24](https://arxiv.org/html/2510.04555v1#bib.bib24)]:

|  |  |  |  |
| --- | --- | --- | --- |
|  | hi‚Äã(f‚Äã(xt)+g‚Äã(xt)‚Äãut)‚àí(1‚àíŒ∫i‚ÄãŒî‚Äãt)‚Äãhi‚Äã(xt)‚â•‚àíŒ∂i,Œ∂i‚â•0,Œ∫i>0,h\_{i}\!\big(f(x\_{t})+g(x\_{t})u\_{t}\big)\;-\;(1-\kappa\_{i}\Delta t)\,h\_{i}(x\_{t})\;\geq\;-\,\zeta\_{i},\qquad\zeta\_{i}\geq 0,\;\kappa\_{i}>0, |  | (14) |

where slack variables are heavily penalized and only activated when unavoidable (robust margins are analyzed in ¬ß4).

##### Ellipsoidal NTB, box/rate limits, and a sign-consistency gate.

Let b‚ãÜ‚Äã(x)b^{\star}(x) be a target exposure vector (e.g., delta/vega) and b‚Äã(x,u)b(x,u) the exposure after action uu.
Define the *ellipsoidal no-trade band* (NTB)

|  |  |  |  |
| --- | --- | --- | --- |
|  | e‚Äã(x,u)=b‚Äã(x,u)‚àíb‚ãÜ‚Äã(x),e‚ä§‚ÄãM‚Äãe‚â§bmax,M‚âª0,e(x,u)\;=\;b(x,u)-b^{\star}(x),\qquad e^{\top}Me\;\leq\;b\_{\max},\quad M\succ 0, |  | (15) |

and *box/rate* limits umin‚â§ut‚â§umaxu\_{\min}\!\leq\!u\_{t}\!\leq\!u\_{\max}, ‚Äñut‚àíut‚àí1‚Äñ2‚â§rmax\|u\_{t}-u\_{t-1}\|\_{2}\!\leq\!r\_{\max}.
We further require a *sign-consistency gate*

|  |  |  |  |
| --- | --- | --- | --- |
|  | gcons‚Äã(x,u)=minj=1,‚Ä¶,J‚Å°‚ü®u,‚àá^‚ÄãŒ†(j)‚Äã(x)‚ü©‚àíŒ¥adv‚â•‚ÄÑ0,g\_{\mathrm{cons}}(x,u)\;=\;\min\_{j=1,\dots,J}\,\langle u,\widehat{\nabla}\Pi^{(j)}(x)\rangle\;-\;\delta\_{\mathrm{adv}}\;\geq\;0, |  | (16) |

so that trades align with an ensemble of interpretable signals (e.g., advantage-proxy gradients from the distributional critic or pricing/hedging sensitivities)¬†[[53](https://arxiv.org/html/2510.04555v1#bib.bib53)].
Near expiry or under extreme volatility, we shrink bmax‚ÜêŒ∑b‚Äãbmaxb\_{\max}\!\leftarrow\!\eta\_{b}b\_{\max} and tighten rmax‚ÜêŒ∑r‚Äãrmaxr\_{\max}\!\leftarrow\!\eta\_{r}r\_{\max} with Œ∑b,Œ∑r‚àà(0,1)\eta\_{b},\eta\_{r}\in(0,1) to improve feasibility and stability.

##### QP formulation and minimal-deviation projection.

Given the actor‚Äôs proposal utnomu\_{t}^{\mathrm{nom}}, we compute the closest safe action utu\_{t} by solving the convex QP

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | minut,Œ∂‚â•0\displaystyle\min\_{u\_{t},\;\zeta\geq 0}\quad | 12‚Äã(ut‚àíutnom)‚ä§‚ÄãH‚Äã(ut‚àíutnom)+c‚ä§‚Äãut+œÅ‚Äã‚ÄñŒ∂‚Äñ1\displaystyle\frac{1}{2}\,(u\_{t}-u\_{t}^{\mathrm{nom}})^{\top}H\,(u\_{t}-u\_{t}^{\mathrm{nom}})\;+\;c^{\top}u\_{t}\;+\;\rho\,\|\zeta\|\_{1} |  | (17) |
|  |  |  |  |
| --- | --- | --- | --- |
|  | s.t. | CBF:¬†‚Äãhi‚Äã(f‚Äã(xt)+g‚Äã(xt)‚Äãut)‚àí(1‚àíŒ∫i‚ÄãŒî‚Äãt)‚Äãhi‚Äã(xt)‚â•‚àíŒ∂i,‚àÄi,\displaystyle\text{CBF: }h\_{i}(f(x\_{t})+g(x\_{t})u\_{t})-(1-\kappa\_{i}\Delta t)\,h\_{i}(x\_{t})\geq-\zeta\_{i},\;\forall i, |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | NTB:¬†‚Äãe‚Äã(xt,ut)‚ä§‚ÄãM‚Äãe‚Äã(xt,ut)‚â§bmax,Box/Rate:¬†‚Äãumin‚â§ut‚â§umax,‚Äñut‚àíut‚àí1‚Äñ2‚â§rmax,\displaystyle\text{NTB: }e(x\_{t},u\_{t})^{\top}Me(x\_{t},u\_{t})\leq b\_{\max},\qquad\text{Box/Rate: }u\_{\min}\!\leq\!u\_{t}\!\leq\!u\_{\max},\;\|u\_{t}-u\_{t-1}\|\_{2}\leq r\_{\max}, |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | Gate:¬†‚Äãgcons‚Äã(xt,ut)‚â•0.\displaystyle\text{Gate: }g\_{\mathrm{cons}}(x\_{t},u\_{t})\geq 0. |  |

Here H‚âª0H\!\succ\!0 defines the deviation metric, cc encodes linear trading frictions, and œÅ‚â´0\rho\!\gg\!0 penalizes any slack.
We use OSQP¬†[[52](https://arxiv.org/html/2510.04555v1#bib.bib52)] with warm starts for efficiency and robustness; see [[49](https://arxiv.org/html/2510.04555v1#bib.bib49)] for background on convex QPs.
When Œ∂=0\zeta=0,¬†([14](https://arxiv.org/html/2510.04555v1#S3.E14 "In Discrete-time CBF constraints. ‚Ä£ 3.3 White-Box CBF‚ÄìQP Safety Layer ‚Ä£ 3 Method: Tail-Safe Hedging Framework ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")) implies forward invariance of the safe set; the quadratic objective makes utu\_{t} the HH-metric projection of utnomu\_{t}^{\mathrm{nom}} onto the feasible set (formalized in ¬ß4).

##### Telemetry for auditability and operations.

For each step, the solver returns: active\_set (indices of active constraints), tightest\_id, rate\_util =‚Äñut‚àíut‚àí1‚Äñ2/rmax=\|u\_{t}{-}u\_{t-1}\|\_{2}/r\_{\max}, gate\_score =gcons‚Äã(xt,ut)=g\_{\mathrm{cons}}(x\_{t},u\_{t}), slack\_sum =‚ÄñŒ∂‚Äñ1=\|\zeta\|\_{1}, and solver\_status/time.
We penalize nonzero slack or non-optimal statuses in the RL reward and log incidents for post-hoc audit, closing the loop between *explainable interception* and *governance*.

##### Pseudocode: training loop and safety filter.

Algorithm 1  Tail-Safe IQN‚ÄìCVaR‚ÄìPPO (on-policy training)

1:‚ÄÇInitialize actor Œ∏\theta, critic œà\psi, reference policy œÄref‚ÜêœÄŒ∏\pi\_{\mathrm{ref}}\!\leftarrow\!\pi\_{\theta}, temperature TT, tail-boost Œ≥tail\gamma\_{\mathrm{tail}}, and target coverage wtargetw\_{\mathrm{target}}.

2:‚ÄÇfor iterations k=1,2,‚Ä¶k=1,2,\dots do

3:‚ÄÉ‚ÄÇCollect trajectories using the safety filter (Alg.¬†[2](https://arxiv.org/html/2510.04555v1#alg2 "Algorithm 2 ‚Ä£ Pseudocode: training loop and safety filter. ‚Ä£ 3.3 White-Box CBF‚ÄìQP Safety Layer ‚Ä£ 3 Method: Tail-Safe Hedging Framework ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")) to obtain {(xt,ut,rt,telemetryt)}\{(x\_{t},u\_{t},r\_{t},\text{telemetry}\_{t})\}.

4:‚ÄÉ‚ÄÇSample quantiles œÑk‚àºpT\tau\_{k}\!\sim\!p\_{T} (Eq.¬†([11](https://arxiv.org/html/2510.04555v1#S3.E11 "In Temperature-tilted sampling and importance weights. ‚Ä£ 3.2 Tail-Coverage Controller: Temperature Sampling and Tail-Boost ‚Ä£ 3 Method: Tail-Safe Hedging Framework ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"))) and apply tail-boost to œÑ‚â§Œ±\tau\!\leq\!\alpha.

5:‚ÄÉ‚ÄÇUpdate critic QœàQ\_{\psi} by minimizing the quantile Huber loss (IQN).

6:‚ÄÉ‚ÄÇEstimate VŒ±V\_{\alpha} and A(Œ±)A^{(\alpha)} via Eqs.¬†([8](https://arxiv.org/html/2510.04555v1#S3.E8 "In Quantile networks and the CVaR objective. ‚Ä£ 3.1 Risk-Sensitive RL with IQN‚ÄìCVaR‚ÄìPPO ‚Ä£ 3 Method: Tail-Safe Hedging Framework ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"))‚Äì([9](https://arxiv.org/html/2510.04555v1#S3.E9 "In CVaR-weighted advantage and PPO updates. ‚Ä£ 3.1 Risk-Sensitive RL with IQN‚ÄìCVaR‚ÄìPPO ‚Ä£ 3 Method: Tail-Safe Hedging Framework ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")); update actor by minimizing Eq.¬†([10](https://arxiv.org/html/2510.04555v1#S3.E10 "In CVaR-weighted advantage and PPO updates. ‚Ä£ 3.1 Risk-Sensitive RL with IQN‚ÄìCVaR‚ÄìPPO ‚Ä£ 3 Method: Tail-Safe Hedging Framework ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")).

7:‚ÄÉ‚ÄÇCompute w^=1K‚Äã‚àëùüè‚Äã{œÑ‚â§Œ±}\widehat{w}=\frac{1}{K}\sum\mathbf{1}\{\tau\leq\alpha\} and update (T,Œ≥tail)(T,\gamma\_{\mathrm{tail}}) using the PID rules¬†([12](https://arxiv.org/html/2510.04555v1#S3.E12 "In Coverage metric and PID tracking. ‚Ä£ 3.2 Tail-Coverage Controller: Temperature Sampling and Tail-Boost ‚Ä£ 3 Method: Tail-Safe Hedging Framework ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")) (with clipping).

8:‚ÄÉ‚ÄÇTighten Œ±\alpha according to a schedule; update œÄref\pi\_{\mathrm{ref}} via EMA; log policy KL and telemetry summaries.

9:‚ÄÇend for




Algorithm 2  CBF‚ÄìQP safety filter (per step)

1:‚ÄÇInputs: xtx\_{t}, proposed action utnomu\_{t}^{\mathrm{nom}}, previous action ut‚àí1u\_{t-1}, params (H,M,bmax,rmax,umin,umax,Œ∫,Œî‚Äãt)(H,M,b\_{\max},r\_{\max},u\_{\min},u\_{\max},\kappa,\Delta t).

2:‚ÄÇFormulate QP¬†([17](https://arxiv.org/html/2510.04555v1#S3.E17 "In QP formulation and minimal-deviation projection. ‚Ä£ 3.3 White-Box CBF‚ÄìQP Safety Layer ‚Ä£ 3 Method: Tail-Safe Hedging Framework ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")) with discrete CBF, NTB, box/rate limits, and sign-consistency gate; near expiry, shrink bmax‚ÜêŒ∑b‚Äãbmaxb\_{\max}\!\leftarrow\!\eta\_{b}b\_{\max} and rmax‚ÜêŒ∑r‚Äãrmaxr\_{\max}\!\leftarrow\!\eta\_{r}r\_{\max}.

3:‚ÄÇSolve the QP (warm-start) to obtain utu\_{t}, active set, tightest constraint, slack Œ∂\zeta, and solver status/time.

4:‚ÄÇEmit telemetry: active\_set, tightest\_id, rate\_util, gate\_score, slack\_sum, solver\_status/time.

5:‚ÄÇIf Œ∂>0\zeta>0 or status ‚â†\neq optimal, add a penalty to the RL reward and log the event; otherwise execute utu\_{t} and advance the environment to xt+1x\_{t+1}.

6:‚ÄÇReturn utu\_{t} and telemetry.

## 4 Theoretical Results

We formalize guarantees for the proposed Tail-Safe framework.
Our results cover (i) *robust forward invariance* of the discrete-time CBF constraints under bounded model mismatch,
(ii) the *minimal-deviation* nature of the QP safety layer,
(iii) a *KL‚ÄìDRO* upper bound linking per-state KL regularization to distributional robustness,
(iv) *concentration* and sample-complexity of the temperature-tilted CVaR estimator with a coverage controller,
(v) a *trust-region improvement* inequality for CVaR with KL-limited policy updates,
(vi) *feasibility persistence* under tail guards, and
(vii) *negative-advantage suppression* induced by the sign-consistency gate.
Complete proofs are deferred to Appendix¬†A (with subsections indicated after each result).

Throughout, let ‚à•‚ãÖ‚à•\|\cdot\| denote the Euclidean norm, ‚ü®‚ãÖ,‚ãÖ‚ü©\langle\cdot,\cdot\rangle the Euclidean inner product, and ùîπr‚Äã(x)\mathbb{B}\_{r}(x) the closed ball of radius rr.
We reuse notation from Sections¬†[2](https://arxiv.org/html/2510.04555v1#S2 "2 Preliminaries & Problem Setting ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")‚Äì[3](https://arxiv.org/html/2510.04555v1#S3 "3 Method: Tail-Safe Hedging Framework ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets").

### Theorem 1 (robust forward invariance of the safety set)

###### Assumption 1 (Local dynamics and mismatch).

There exist locally Lipschitz functions f,gf,g and an additive disturbance wtw\_{t} such that the state update obeys
xt+1=f‚Äã(xt)+g‚Äã(xt)‚Äãut+wtx\_{t+1}=f(x\_{t})+g(x\_{t})u\_{t}+w\_{t} with ‚Äñwt‚Äñ‚â§w¬Ø\|w\_{t}\|\leq\bar{w} almost surely.
For each barrier hi:‚Ñùd‚Üí‚Ñùh\_{i}:\mathbb{R}^{d}\to\mathbb{R} there is an Li>0L\_{i}>0 with |hi‚Äã(x)‚àíhi‚Äã(y)|‚â§Li‚Äã‚Äñx‚àíy‚Äñ|h\_{i}(x)-h\_{i}(y)|\leq L\_{i}\|x-y\|.

###### Assumption 2 (Discrete-time CBF constraint with margin).

At time tt, the QP safety layer enforces for each ii:

|  |  |  |
| --- | --- | --- |
|  | hi‚Äã(f‚Äã(xt)+g‚Äã(xt)‚Äãut)‚àí(1‚àíŒ∫i‚ÄãŒî‚Äãt)‚Äãhi‚Äã(xt)‚â•Œµi,Œ∫i>0,h\_{i}\!\big(f(x\_{t})+g(x\_{t})u\_{t}\big)-(1-\kappa\_{i}\Delta t)\,h\_{i}(x\_{t})\;\geq\;\varepsilon\_{i},\qquad\kappa\_{i}>0, |  |

with margin Œµi‚â•Li‚Äãw¬Ø\varepsilon\_{i}\geq L\_{i}\bar{w}.

###### Theorem 1 (Robust forward invariance).

Under Assumptions¬†[1](https://arxiv.org/html/2510.04555v1#Thmassumption1 "Assumption 1 (Local dynamics and mismatch). ‚Ä£ Theorem 1 (robust forward invariance of the safety set) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")‚Äì[2](https://arxiv.org/html/2510.04555v1#Thmassumption2 "Assumption 2 (Discrete-time CBF constraint with margin). ‚Ä£ Theorem 1 (robust forward invariance of the safety set) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"), if hi‚Äã(xt)‚â•0h\_{i}(x\_{t})\geq 0 for all ii, then hi‚Äã(xt+1)‚â•0h\_{i}(x\_{t+1})\geq 0 for all ii.
Hence the safe set ùíû:={x:hi‚Äã(x)‚â•0,‚àÄi}\mathcal{C}:=\{x:\,h\_{i}(x)\geq 0,\,\forall i\} is forward invariant.

##### Proof sketch.

Using Lipschitzness,
hi‚Äã(xt+1)‚â•hi‚Äã(f+g‚Äãut)‚àíLi‚Äã‚Äñwt‚Äñ‚â•(1‚àíŒ∫i‚ÄãŒî‚Äãt)‚Äãhi‚Äã(xt)+Œµi‚àíLi‚Äãw¬Ø‚â•0h\_{i}(x\_{t+1})\geq h\_{i}(f+gu\_{t})-L\_{i}\|w\_{t}\|\geq(1-\kappa\_{i}\Delta t)h\_{i}(x\_{t})+\varepsilon\_{i}-L\_{i}\bar{w}\geq 0.
A full inductive argument is given in Appendix¬†A.1.
(See also robust CBF analyses such as [[30](https://arxiv.org/html/2510.04555v1#bib.bib30), [31](https://arxiv.org/html/2510.04555v1#bib.bib31)].)

### Proposition 1 (minimal-deviation HH-metric projection)

###### Assumption 3 (Convex feasibility).

For fixed xtx\_{t}, the feasible action set ùíÆ‚Äã(xt)\mathcal{S}(x\_{t}) induced by the constraints in¬†([14](https://arxiv.org/html/2510.04555v1#S3.E14 "In Discrete-time CBF constraints. ‚Ä£ 3.3 White-Box CBF‚ÄìQP Safety Layer ‚Ä£ 3 Method: Tail-Safe Hedging Framework ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")), ([15](https://arxiv.org/html/2510.04555v1#S3.E15 "In Ellipsoidal NTB, box/rate limits, and a sign-consistency gate. ‚Ä£ 3.3 White-Box CBF‚ÄìQP Safety Layer ‚Ä£ 3 Method: Tail-Safe Hedging Framework ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")), and the box/rate and gate constraints is nonempty, closed, and convex in uu (this holds under affine-in-uu CBF surrogates/linearizations and convex NTB/box/rate/gate specifications).

###### Proposition 1 (Shifted projection).

Let H‚âª0H\!\succ\!0, c‚àà‚Ñùmc\in\mathbb{R}^{m}, and œÅ\rho be sufficiently large so that the QP¬†([17](https://arxiv.org/html/2510.04555v1#S3.E17 "In QP formulation and minimal-deviation projection. ‚Ä£ 3.3 White-Box CBF‚ÄìQP Safety Layer ‚Ä£ 3 Method: Tail-Safe Hedging Framework ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")) is solved with Œ∂=0\zeta=0.
Then its unique optimizer ut‚ãÜu\_{t}^{\star} satisfies

|  |  |  |
| --- | --- | --- |
|  | ut‚ãÜ=arg‚Å°minu‚ààùíÆ‚Äã(xt)‚Å°12‚Äã‚Äñu‚àí(utnom‚àíH‚àí1‚Äãc)‚ÄñH2,u\_{t}^{\star}\;=\;\arg\min\_{u\in\mathcal{S}(x\_{t})}\frac{1}{2}\,\|u-(u\_{t}^{\mathrm{nom}}-H^{-1}c)\|\_{H}^{2}, |  |

i.e., ut‚ãÜu\_{t}^{\star} is the HH-metric projection of the *shifted anchor* utnom‚àíH‚àí1‚Äãcu\_{t}^{\mathrm{nom}}-H^{-1}c onto ùíÆ‚Äã(xt)\mathcal{S}(x\_{t}).

##### Proof sketch.

Completing the square yields
12‚Äã(u‚àíunom)‚ä§‚ÄãH‚Äã(u‚àíunom)+c‚ä§‚Äãu=12‚Äã‚Äñu‚àí(unom‚àíH‚àí1‚Äãc)‚ÄñH2+const\tfrac{1}{2}(u-u^{\mathrm{nom}})^{\top}H(u-u^{\mathrm{nom}})+c^{\top}u=\tfrac{1}{2}\|u-(u^{\mathrm{nom}}-H^{-1}c)\|\_{H}^{2}+\mathrm{const}.
Strict convexity plus convex feasibility implies uniqueness; KKT conditions characterize the projection.
Details are in Appendix¬†A.2 (cf. [[49](https://arxiv.org/html/2510.04555v1#bib.bib49)]).

### Theorem 2 (KL‚ÄìDRO upper bound and conservatism of per-state KL)

###### Assumption 4 (CVaR surrogate).

For Œ±‚àà(0,1)\alpha\in(0,1) and any threshold t‚àà‚Ñùt\in\mathbb{R}, define œït‚Äã(z)=(z‚àít)+\phi\_{t}(z)=(z-t)\_{+} and
CVaRŒ±‚Äã(L)=mint‚Å°t+1Œ±‚Äãùîº‚Äã[œït‚Äã(L)]\mathrm{CVaR}\_{\alpha}(L)=\min\_{t}\,t+\tfrac{1}{\alpha}\,\mathbb{E}[\phi\_{t}(L)]¬†[[43](https://arxiv.org/html/2510.04555v1#bib.bib43)].

###### Assumption 5 (Path-wise KL radius and occupancy control).

Let ùí¨œÅ={Q:KL‚Äã(Q‚à•P)‚â§œÅ}\mathcal{Q}\_{\rho}=\{Q:\mathrm{KL}(Q\|P)\leq\rho\} be a KL ball around a reference path distribution PP (the simulator/behavior distribution).
Assume per-state policy KL is bounded: KL(œÄ‚Ä≤(‚ãÖ|x)‚à•œÄref(‚ãÖ|x))‚â§Œ≤\mathrm{KL}(\pi^{\prime}(\cdot|x)\|\pi\_{\mathrm{ref}}(\cdot|x))\leq\beta for all xx in the support,
and the induced pathwise KL satisfies œÅ‚â§Cocc‚ÄãŒ≤\rho\leq C\_{\mathrm{occ}}\,\beta for some constant CoccC\_{\mathrm{occ}} depending on the horizon and mixing/occupancy properties (cf. Pinsker-type arguments and standard occupancy coupling).

###### Theorem 2 (Donsker‚ÄìVaradhan bound and per-state KL conservatism).

For any Œ∑>0\eta>0,

|  |  |  |
| --- | --- | --- |
|  | supQ‚ààùí¨œÅCVaRŒ±‚Äã(L)‚â§mint‚àà‚Ñù‚Å°{t+1Œ±‚ÄãŒ∑‚Äã(œÅ+log‚Å°ùîºP‚Äã[eŒ∑‚Äãœït‚Äã(L)])}.\sup\_{Q\in\mathcal{Q}\_{\rho}}\mathrm{CVaR}\_{\alpha}(L)\;\leq\;\min\_{t\in\mathbb{R}}\left\{\,t+\frac{1}{\alpha\eta}\Big(\rho+\log\mathbb{E}\_{P}\!\big[e^{\eta\,\phi\_{t}(L)}\big]\Big)\right\}. |  |

Moreover, under Assumption¬†[5](https://arxiv.org/html/2510.04555v1#Thmassumption5 "Assumption 5 (Path-wise KL radius and occupancy control). ‚Ä£ Theorem 2 (KL‚ÄìDRO upper bound and conservatism of per-state KL) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") the RHS is upper-bounded by the same expression with œÅ\rho replaced by Cocc‚ÄãŒ≤C\_{\mathrm{occ}}\beta.
Hence, penalizing per-state KL by ŒªKL‚ãÖùîºx[KL(œÄ(‚ãÖ|x)‚à•œÄref(‚ãÖ|x))]\lambda\_{\mathrm{KL}}\!\cdot\!\mathbb{E}\_{x}[\mathrm{KL}(\pi(\cdot|x)\|\pi\_{\mathrm{ref}}(\cdot|x))] controls a KL‚ÄìDRO upper bound on the CVaR surrogate and thus quantifies conservatism.

##### Proof sketch.

Apply the Donsker‚ÄìVaradhan variational inequality to ùîºQ‚Äã[œït‚Äã(L)]\mathbb{E}\_{Q}[\phi\_{t}(L)]¬†[[54](https://arxiv.org/html/2510.04555v1#bib.bib54)], then minimize over tt.
Relate pathwise KL to the expected per-state KL via occupancy coupling and Pinsker‚Äôs inequality (see, e.g., [[15](https://arxiv.org/html/2510.04555v1#bib.bib15), [48](https://arxiv.org/html/2510.04555v1#bib.bib48), [55](https://arxiv.org/html/2510.04555v1#bib.bib55), [56](https://arxiv.org/html/2510.04555v1#bib.bib56)]).
Appendix A.3 provides details.

### Theorem 3 (bias/variance and sample complexity of the CVaR estimator)

###### Assumption 6 (Temperature sampling, bounded importance weights, bounded loss).

Quantiles are sampled from pT‚Äã(œÑ)‚àùe‚àíœÑ/Tp\_{T}(\tau)\propto e^{-\tau/T} on [0,1][0,1] with T‚àà[Tmin,Tmax]T\in[T\_{\min},T\_{\max}] and pT‚Äã(œÑ)‚â•pmin>0p\_{T}(\tau)\geq p\_{\min}>0.
Self-normalized importance weights are wk‚àù1/pT‚Äã(œÑk)w\_{k}\propto 1/p\_{T}(\tau\_{k}) (normalized within the batch).
Losses are almost surely bounded: |L|‚â§B|L|\leq B.

Define the self-normalized estimator (with tail-boost implemented by oversampling, absorbed into pTp\_{T}):

|  |  |  |
| --- | --- | --- |
|  | CVaR^Œ±=‚àëk=1Kwk‚Äã‚Äâ1‚Äã{œÑk‚â§Œ±}‚ÄãL(œÑk)‚àëk=1Kwk‚Äã‚Äâ1‚Äã{œÑk‚â§Œ±},Œ±eff:=ùîº‚Äã[ùüè‚Äã{œÑ‚â§Œ±}].\widehat{\mathrm{CVaR}}\_{\alpha}=\frac{\sum\_{k=1}^{K}w\_{k}\,\mathbf{1}\{\tau\_{k}\leq\alpha\}\,L^{(\tau\_{k})}}{\sum\_{k=1}^{K}w\_{k}\,\mathbf{1}\{\tau\_{k}\leq\alpha\}},\quad\alpha\_{\mathrm{eff}}:=\mathbb{E}[\mathbf{1}\{\tau\leq\alpha\}]\,. |  |

###### Theorem 3 (Concentration of the temperature-tilted CVaR estimator).

Under Assumption¬†[6](https://arxiv.org/html/2510.04555v1#Thmassumption6 "Assumption 6 (Temperature sampling, bounded importance weights, bounded loss). ‚Ä£ Theorem 3 (bias/variance and sample complexity of the CVaR estimator) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"), for any Œ¥‚àà(0,1)\delta\in(0,1), with probability at least 1‚àíŒ¥1-\delta,

|  |  |  |
| --- | --- | --- |
|  | |CVaR^Œ±‚àíCVaRŒ±|‚â§C1‚ÄãB‚Äãlog‚Å°(2/Œ¥)K‚ÄãŒ±eff‚èüvariance term+C2‚ÄãB‚Äã|Œ±eff‚àíŒ±|‚èücoverage mismatch,\big|\widehat{\mathrm{CVaR}}\_{\alpha}-\mathrm{CVaR}\_{\alpha}\big|\;\leq\;\underbrace{C\_{1}\,B\,\sqrt{\frac{\log(2/\delta)}{K\,\alpha\_{\mathrm{eff}}}}}\_{\text{variance term}}\;+\;\underbrace{C\_{2}\,B\,\big|\alpha\_{\mathrm{eff}}-\alpha\big|}\_{\text{coverage mismatch}}, |  |

for absolute constants C1,C2C\_{1},C\_{2} depending on pminp\_{\min} and the self-normalization scheme.
In particular, the PID controller that tracks Œ±eff‚âàwtarget\alpha\_{\mathrm{eff}}\!\approx\!w\_{\mathrm{target}} (with wtargetw\_{\mathrm{target}} close to Œ±\alpha) reduces the coverage-mismatch bias and improves the rate constant in the variance term.

##### Proof sketch.

Combine self-normalized importance sampling concentration (e.g., empirical Bernstein/Hoeffding-style bounds for ratio estimators) with bounded weights and losses¬†[[50](https://arxiv.org/html/2510.04555v1#bib.bib50)].
A first-order expansion quantifies the effect of replacing Œ±\alpha by Œ±eff\alpha\_{\mathrm{eff}}; the controller reduces this mismatch.
See Appendix¬†A.4 for a full derivation.

### Theorem 4 (trust-region improvement inequality for CVaR with KL limits)

###### Assumption 7 (Per-state KL constraint and smoothness).

For a policy update œÄ‚Ä≤=œÄ+Œî\pi^{\prime}\!=\!\pi+\Delta, assume KL(œÄ‚Ä≤(‚ãÖ|x)‚à•œÄ(‚ãÖ|x))‚â§Œ≤\mathrm{KL}(\pi^{\prime}(\cdot|x)\|\pi(\cdot|x))\leq\beta for all xx and that the one-step loss ‚Ñì\ell is LL-Lipschitz in actions and states under the dynamics in Assumption¬†[1](https://arxiv.org/html/2510.04555v1#Thmassumption1 "Assumption 1 (Local dynamics and mismatch). ‚Ä£ Theorem 1 (robust forward invariance of the safety set) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets").
Let JŒ±‚Äã(œÄ)=CVaRŒ±‚Äã(LT‚Äã(œÄ))J\_{\alpha}(\pi)=\mathrm{CVaR}\_{\alpha}(L\_{T}(\pi)) for horizon TT.

###### Theorem 4 (CVaR trust-region improvement).

There exists a constant CŒ±>0C\_{\alpha}>0 (depending on LL, TT, and loss bounds) such that

|  |  |  |
| --- | --- | --- |
|  | JŒ±‚Äã(œÄ‚Ä≤)‚â§JŒ±‚Äã(œÄ)+ùîºx‚àºdœÄ,u‚àºœÄ‚Äã[œâ‚Äã(x,u)‚ÄãA~œÄ(Œ±)‚Äã(x,u)]+CŒ±‚ÄãŒ≤+o‚Äã(‚ÄñŒî‚Äñ),J\_{\alpha}(\pi^{\prime})\;\leq\;J\_{\alpha}(\pi)\;+\;\mathbb{E}\_{x\sim d\_{\pi},\,u\sim\pi}\!\big[\omega(x,u)\,\tilde{A}\_{\pi}^{(\alpha)}(x,u)\big]\;+\;C\_{\alpha}\,\sqrt{\beta}\;+\;o(\|\Delta\|), |  |

where œâ=œÄ‚Ä≤(‚ãÖ|x)/œÄ(‚ãÖ|x)\omega=\pi^{\prime}(\cdot|x)/\pi(\cdot|x) and A~œÄ(Œ±)\tilde{A}\_{\pi}^{(\alpha)} is the CVaR-weighted advantage (cf. ([9](https://arxiv.org/html/2510.04555v1#S3.E9 "In CVaR-weighted advantage and PPO updates. ‚Ä£ 3.1 Risk-Sensitive RL with IQN‚ÄìCVaR‚ÄìPPO ‚Ä£ 3 Method: Tail-Safe Hedging Framework ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"))).
Thus, a small KL radius Œ≤\beta controls the degradation term, yielding a trust-region style guarantee for the CVaR objective.

##### Proof sketch.

Adapt the performance-difference lemma to the CVaR surrogate (replace value with CVaR baseline), then bound the state-distribution shift by total variation and Pinsker‚Äôs inequality TV‚â§12‚ÄãKL\mathrm{TV}\leq\sqrt{\tfrac{1}{2}\mathrm{KL}}¬†[[15](https://arxiv.org/html/2510.04555v1#bib.bib15), [48](https://arxiv.org/html/2510.04555v1#bib.bib48), [55](https://arxiv.org/html/2510.04555v1#bib.bib55)].
Smoothness of ‚Ñì\ell and the horizon accumulation yield CŒ±‚ÄãŒ≤C\_{\alpha}\sqrt{\beta}.
A detailed derivation is given in Appendix¬†A.5.

### Theorem 5 (feasibility persistence under tail guards)

###### Assumption 8 (Lipschitz constraints and affine exposure map).

Assume e‚Äã(x,u)=A‚Äã(x)‚Äãu‚àíd‚Äã(x)e(x,u)=A(x)u-d(x) with AA, dd locally Lipschitz, M‚âª0M\succ 0, and that box/rate sets are convex.
Let the CBF surrogates used in the QP be affine in uu for fixed xx.

###### Assumption 9 (Margins at time tt).

Suppose at time tt the NTB and rate constraints hold with margins
e‚Äã(xt,ut)‚ä§‚ÄãM‚Äãe‚Äã(xt,ut)‚â§bmax‚àíŒ¥be(x\_{t},u\_{t})^{\top}Me(x\_{t},u\_{t})\leq b\_{\max}-\delta\_{b} and ‚Äñut‚àíut‚àí1‚Äñ2‚â§rmax‚àíŒ¥r\|u\_{t}-u\_{t-1}\|\_{2}\leq r\_{\max}-\delta\_{r} for some Œ¥b,Œ¥r>0\delta\_{b},\delta\_{r}>0.

###### Theorem 5 (Persistence via NTB shrinkage and rate tightening).

Under Assumptions¬†[1](https://arxiv.org/html/2510.04555v1#Thmassumption1 "Assumption 1 (Local dynamics and mismatch). ‚Ä£ Theorem 1 (robust forward invariance of the safety set) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"), [8](https://arxiv.org/html/2510.04555v1#Thmassumption8 "Assumption 8 (Lipschitz constraints and affine exposure map). ‚Ä£ Theorem 5 (feasibility persistence under tail guards) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"), and [9](https://arxiv.org/html/2510.04555v1#Thmassumption9 "Assumption 9 (Margins at time ùë°). ‚Ä£ Theorem 5 (feasibility persistence under tail guards) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"),
there exist shrinkage factors Œ∑b,Œ∑r‚àà(0,1)\eta\_{b},\eta\_{r}\in(0,1), computable from local Lipschitz constants of (A,d,f,g)(A,d,f,g) and (Œ∫i)(\kappa\_{i}) and the disturbance bound w¬Ø\bar{w}, such that replacing bmax‚ÜêŒ∑b‚Äãbmaxb\_{\max}\!\leftarrow\!\eta\_{b}b\_{\max} and rmax‚ÜêŒ∑r‚Äãrmaxr\_{\max}\!\leftarrow\!\eta\_{r}r\_{\max} guarantees that the QP at time t+1t{+}1 remains feasible with Œ∂=0\zeta=0.
Equivalently, the feasible set intersection (CBF, NTB, box, rate, gate) remains nonempty at t+1t{+}1.

##### Proof sketch.

Use tube arguments: with Lipschitz dynamics and constraints, the next-state feasible set contains a ball around the previous safe action projected by the rate set.
Choosing (Œ∑b,Œ∑r)(\eta\_{b},\eta\_{r}) to upper bound drift induced by (xt‚Üíxt+1)(x\_{t}\!\to\!x\_{t+1}) and disturbance keeps the intersection nonempty.
Full details appear in Appendix¬†A.6 (see also viability arguments in robust CBF literature¬†[[30](https://arxiv.org/html/2510.04555v1#bib.bib30)]).

### Proposition 2 (negative-advantage suppression by sign-consistency)

###### Assumption 10 (Gate alignment and mismatch).

For each xx, let gcons‚Äã(x,u)=minj‚â§J‚Å°‚ü®u,‚àá^‚ÄãŒ†(j)‚Äã(x)‚ü©‚àíŒ¥advg\_{\mathrm{cons}}(x,u)=\min\_{j\leq J}\langle u,\widehat{\nabla}\Pi^{(j)}(x)\rangle-\delta\_{\mathrm{adv}} with Œ¥adv‚â•0\delta\_{\mathrm{adv}}\geq 0.
Assume there exists a unit vector v‚Äã(x)v(x) such that
‚Äñ‚àá^‚ÄãŒ†(j)‚Äã(x)‚àív‚Äã(x)‚Äñ‚â§œµg\|\widehat{\nabla}\Pi^{(j)}(x)-v(x)\|\leq\epsilon\_{g} for all jj, and that the CVaR-weighted advantage satisfies a local linearization
A~œÄ(Œ±)‚Äã(x,u)‚âà‚ü®u,‚àáA~œÄ(Œ±)‚Äã(x)‚ü©\tilde{A}\_{\pi}^{(\alpha)}(x,u)\approx\langle u,\nabla\tilde{A}\_{\pi}^{(\alpha)}(x)\rangle with ‚à†‚Äã(‚àáA~œÄ(Œ±)‚Äã(x),v‚Äã(x))‚â§œµŒ∏\angle(\nabla\tilde{A}\_{\pi}^{(\alpha)}(x),v(x))\leq\epsilon\_{\theta}.

###### Proposition 2 (Gate-induced lower bound).

Under Assumption¬†[10](https://arxiv.org/html/2510.04555v1#Thmassumption10 "Assumption 10 (Gate alignment and mismatch). ‚Ä£ Proposition 2 (negative-advantage suppression by sign-consistency) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"), any action uu passing the gate (gcons‚Äã(x,u)‚â•0g\_{\mathrm{cons}}(x,u)\geq 0) obeys

|  |  |  |
| --- | --- | --- |
|  | ùîº[A~œÄ(Œ±)(x,u)|x]‚â•‚àíŒæ(œµg,œµŒ∏,Œ¥adv,‚à•u‚à•),\mathbb{E}\!\left[\tilde{A}\_{\pi}^{(\alpha)}(x,u)\,\middle|\,x\right]\;\geq\;-\,\xi(\epsilon\_{g},\epsilon\_{\theta},\delta\_{\mathrm{adv}},\|u\|), |  |

for an explicit function Œæ\xi that vanishes as (œµg,œµŒ∏)‚Üí0(\epsilon\_{g},\epsilon\_{\theta})\!\to\!0 and increases with Œ¥adv\delta\_{\mathrm{adv}} and ‚Äñu‚Äñ\|u\|.
In particular, for sufficiently small alignment errors the gate suppresses negative CVaR-advantage trades up to a controlled tolerance.

##### Proof sketch.

Gate feasibility implies ‚ü®u,v‚Äã(x)‚ü©‚â•Œ¥adv‚àí‚Äñu‚Äñ‚Äãœµg\langle u,v(x)\rangle\geq\delta\_{\mathrm{adv}}-\|u\|\epsilon\_{g}.
Using the angle bound between v‚Äã(x)v(x) and ‚àáA~œÄ(Œ±)‚Äã(x)\nabla\tilde{A}\_{\pi}^{(\alpha)}(x) and Cauchy‚ÄìSchwarz yields
‚ü®u,‚àáA~œÄ(Œ±)‚Äã(x)‚ü©‚â•‚Äñu‚Äñ‚Äã(Œ¥adv/‚Äñu‚Äñ‚àíœµg)‚Äãcos‚Å°œµŒ∏‚àí‚Äñu‚Äñ‚Äãùí™‚Äã(œµg)\langle u,\nabla\tilde{A}\_{\pi}^{(\alpha)}(x)\rangle\geq\|u\|(\delta\_{\mathrm{adv}}/\|u\|-\epsilon\_{g})\cos\epsilon\_{\theta}-\|u\|\,\mathcal{O}(\epsilon\_{g}),
which provides the claimed lower bound.
Formal constants are derived in Appendix¬†A.7.

Remarks.
(i) Theorems¬†[1](https://arxiv.org/html/2510.04555v1#Thmtheorem1 "Theorem 1 (Robust forward invariance). ‚Ä£ Theorem 1 (robust forward invariance of the safety set) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") and [5](https://arxiv.org/html/2510.04555v1#Thmtheorem5 "Theorem 5 (Persistence via NTB shrinkage and rate tightening). ‚Ä£ Theorem 5 (feasibility persistence under tail guards) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") formalize *hard safety* by construction plus feasibility resilience under tail guards, connecting solver telemetry to auditable interventions.
(ii) Theorems¬†[2](https://arxiv.org/html/2510.04555v1#Thmtheorem2 "Theorem 2 (Donsker‚ÄìVaradhan bound and per-state KL conservatism). ‚Ä£ Theorem 2 (KL‚ÄìDRO upper bound and conservatism of per-state KL) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") and [4](https://arxiv.org/html/2510.04555v1#Thmtheorem4 "Theorem 4 (CVaR trust-region improvement). ‚Ä£ Theorem 4 (trust-region improvement inequality for CVaR with KL limits) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") justify the PPO+KL design for tail-risk optimization: per-state KL acts as a DRO-style conservatism control while ensuring trust-region stability.
(iii) Theorem¬†[3](https://arxiv.org/html/2510.04555v1#Thmtheorem3 "Theorem 3 (Concentration of the temperature-tilted CVaR estimator). ‚Ä£ Theorem 3 (bias/variance and sample complexity of the CVaR estimator) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") shows why the coverage controller reduces variance and bias in CVaR estimation as Œ±\alpha tightens.

## 5 Experiments in Arbitrage-Free Synthetic Markets

We evaluate Tail-Safe in a synthetic yet finance-grounded environment that is (i) *arbitrage-free* by construction (SSVI‚Üí\!\rightarrowDupire‚Üí\!\rightarrowVIX), (ii) *microstructure-aware* through ABIDES/MockLOB execution with spread, temporary, and transient impact, and (iii) *stressable* along interpretable dimensions (level/slope/curvature of the volatility surface, equity‚Äìvolatility correlation, impact strength/decay, and time-to-expiry).
All results are reported with *unified sample sizes* and *multiple random seeds*, with uncertainty quantified by paired bootstrap confidence intervals¬†[[58](https://arxiv.org/html/2510.04555v1#bib.bib58), [59](https://arxiv.org/html/2510.04555v1#bib.bib59)].
This section details the protocol, metrics, results, ablations, and safety telemetry.

### 5.1 Protocol

##### ID/OOD split and stress dimensions.

We generate a calibrated SSVI surface family w‚Äã(k,œÑ)w(k,\tau) satisfying static no-arbitrage (Sec.¬†[2.1](https://arxiv.org/html/2510.04555v1#S2.SS1 "2.1 Arbitrage-Free Volatility Surfaces via SSVI ‚Ä£ 2 Preliminaries & Problem Setting ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")), transform it to Dupire local volatility (Sec.¬†[2.2](https://arxiv.org/html/2510.04555v1#S2.SS2 "2.2 Local Volatility via Dupire ‚Ä£ 2 Preliminaries & Problem Setting ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")), and construct a surface-consistent 30D VIX leg (Sec.¬†[2.3](https://arxiv.org/html/2510.04555v1#S2.SS3 "2.3 VIX Leg from Surface-Consistent Variance ‚Ä£ 2 Preliminaries & Problem Setting ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")).
We define *in-distribution* (ID) scenarios by sampling around the baseline calibration and *out-of-distribution* (OOD) scenarios by perturbing:

1. 1.

   Level/slope/curvature of the SSVI parameters (Œ∏,œÜ,œÅ)(\theta,\varphi,\rho) across maturities;
2. 2.

   Equity‚Äìvolatility correlation regimes (including extreme negative spikes);
3. 3.

   Impact strength/decay in the execution model (spread, temporary coefficient Œ∑\eta, transient kernel GG);
4. 4.

   Time-to-expiry, emphasizing near-expiry regimes where greeks and liquidity risks intensify.

All stressed configurations remain free of *static* arbitrage by construction.

##### Unified sample sizes and multiple seeds.

To ensure fair comparisons, each method/variant is evaluated on the *same* set of scenario seeds and the same number of paths per seed.
When legacy artifacts contain differing sample sizes (e.g., n=400n{=}400 vs. n=200n{=}200), we *re-subsample/pair* to a common effective nn per seed, and we report all statistics with *paired* uncertainty across methods.
This protocol reduces variance inflation and follows best practices in RL evaluation¬†[[60](https://arxiv.org/html/2510.04555v1#bib.bib60)].

##### Training and evaluation schedules.

Policies are trained with the IQN‚ÄìCVaR‚ÄìPPO learner (Sec.¬†[3.1](https://arxiv.org/html/2510.04555v1#S3.SS1 "3.1 Risk-Sensitive RL with IQN‚ÄìCVaR‚ÄìPPO ‚Ä£ 3 Method: Tail-Safe Hedging Framework ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")) using the Tail-Coverage Controller (Sec.¬†[3.2](https://arxiv.org/html/2510.04555v1#S3.SS2 "3.2 Tail-Coverage Controller: Temperature Sampling and Tail-Boost ‚Ä£ 3 Method: Tail-Safe Hedging Framework ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")) and the white-box CBF‚ÄìQP safety layer (Sec.¬†[3.3](https://arxiv.org/html/2510.04555v1#S3.SS3 "3.3 White-Box CBF‚ÄìQP Safety Layer ‚Ä£ 3 Method: Tail-Safe Hedging Framework ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")) *active during data collection and evaluation*.
The CVaR level Œ±\alpha tightens from a permissive value (e.g., 0.100.10) toward the target (e.g., 0.0250.025) according to a cosine schedule; KL and entropy coefficients are co-scheduled to limit policy drift.
All wall-clock budgets, iteration counts, and solver tolerances are reported in the reproducibility checklist (Appendix¬†C).

### 5.2 Metrics

##### Tail risk and central performance.

We report absolute-loss VaRŒ±\operatorname{VaR}\_{\alpha} and ESŒ±\operatorname{ES}\_{\alpha} (Sec.¬†[2.6](https://arxiv.org/html/2510.04555v1#S2.SS6 "2.6 Risk Measures: VaR and ES/CVaR ‚Ä£ 2 Preliminaries & Problem Setting ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")), along with mean, standard deviation, and portfolio-style ratios:

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | Sharpe | =ùîº‚Äã[Œ†]Var‚Äã(Œ†),(zero risk-free benchmark)‚Äã[[61](https://arxiv.org/html/2510.04555v1#bib.bib61)],\displaystyle=\frac{\mathbb{E}[\Pi]}{\sqrt{\mathrm{Var}(\Pi)}},\qquad\text{(zero risk-free benchmark)}~\cite[cite]{[\@@bibref{}{Sharpe1994}{}{}]}, |  | (18) |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | Sortino | =ùîº‚Äã[Œ†]ùîº[min(Œ†,0)2],(downside deviation)‚Äã[[62](https://arxiv.org/html/2510.04555v1#bib.bib62)],\displaystyle=\frac{\mathbb{E}[\Pi]}{\sqrt{\mathbb{E}[\min(\Pi,0)^{2}]}},\qquad\text{(downside deviation)}~\cite[cite]{[\@@bibref{}{SortinoPrice1994}{}{}]}, |  | (19) |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | Œ©‚Äã(œÑ0)\displaystyle\Omega(\tau\_{0}) | =‚à´œÑ0‚àû(1‚àíFŒ†‚Äã(z))‚Äãùëëz‚à´‚àí‚àûœÑ0FŒ†‚Äã(z)‚Äãùëëz,œÑ0=0,(Omega ratio)‚Äã[[63](https://arxiv.org/html/2510.04555v1#bib.bib63)].\displaystyle=\frac{\int\_{\tau\_{0}}^{\infty}\big(1-F\_{\Pi}(z)\big)\,dz}{\int\_{-\infty}^{\tau\_{0}}F\_{\Pi}(z)\,dz},\quad\tau\_{0}{=}0,\qquad\text{(Omega ratio)}~\cite[cite]{[\@@bibref{}{KeatingShadwick2002}{}{}]}. |  | (20) |

We visualize distributions with ECDFs and histograms and annotate tail quantiles.

##### Stability and regularization.

We track (i) *policy KL step* ùîºx[KL(œÄ(‚ãÖ|x)‚à•œÄref(‚ãÖ|x))]\mathbb{E}\_{x}[\mathrm{KL}(\pi(\cdot|x)\|\pi\_{\mathrm{ref}}(\cdot|x))] per update, (ii) *effective tail coverage* w^\widehat{w} relative to the target wtargetw\_{\mathrm{target}}, and (iii) *entropy* of the policy.

##### Safety telemetry.

From the CBF‚ÄìQP solver, we log active\_set, tightest\_id, rate\_util, gate\_score, slack\_sum, and solver\_status/time (Sec.¬†[3.3](https://arxiv.org/html/2510.04555v1#S3.SS3 "3.3 White-Box CBF‚ÄìQP Safety Layer ‚Ä£ 3 Method: Tail-Safe Hedging Framework ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")), and report:
(i) frequency of the tightest constraints, (ii) slack mass and feasibility rate, and (iii) median/P95 solver time per step.

##### Uncertainty quantification and tests.

For each statistic we report 95%95\% paired bootstrap CIs across scenario paths and seeds¬†[[58](https://arxiv.org/html/2510.04555v1#bib.bib58), [59](https://arxiv.org/html/2510.04555v1#bib.bib59)].
When multiple metrics are tested simultaneously, pp-values are FDR-adjusted using Benjamini‚ÄìHochberg¬†[[64](https://arxiv.org/html/2510.04555v1#bib.bib64)].
Effect sizes for pairwise comparisons are summarized via the Vargha‚ÄìDelaney A^12\hat{A}\_{12} statistic¬†[[65](https://arxiv.org/html/2510.04555v1#bib.bib65)].

### 5.3 Results

##### Distributional evidence.

Figure¬†[2](https://arxiv.org/html/2510.04555v1#S5.F2 "Figure 2 ‚Ä£ Distributional evidence. ‚Ä£ 5.3 Results ‚Ä£ 5 Experiments in Arbitrage-Free Synthetic Markets ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") overlays ECDFs of P&L; Tail-Safe shifts the 11‚Äì3%3\% left tail rightwards while keeping the central mass comparable to the QP-only baseline (consistent with the CVaR objective).
Figure¬†[3](https://arxiv.org/html/2510.04555v1#S5.F3 "Figure 3 ‚Ä£ Distributional evidence. ‚Ä£ 5.3 Results ‚Ä£ 5 Experiments in Arbitrage-Free Synthetic Markets ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") shows thinner negative tails for Tail-Safe; premium-normalized variants (not shown) display narrower dispersion.

![Refer to caption](overlay_ecdf.png)


Figure 2: PnL ECDF (overlay). Tail-Safe vs. QP-only baseline on matched scenarios and unified sample sizes. Shaded bands (if present) indicate 95%95\% paired bootstrap CIs at each quantile (Appendix¬†B).

![Refer to caption](overlay_hist.png)


Figure 3: PnL distribution (overlay). Negative tails thin out under Tail-Safe, while the bulk remains comparable. Vertical lines annotate VaRŒ±\operatorname{VaR}\_{\alpha} and ESŒ±\operatorname{ES}\_{\alpha}.

##### Per-variant panels.

Figure¬†[4](https://arxiv.org/html/2510.04555v1#S5.F4 "Figure 4 ‚Ä£ Per-variant panels. ‚Ä£ 5.3 Results ‚Ä£ 5 Experiments in Arbitrage-Free Synthetic Markets ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") reports ECDF/histogram for the QP-only baseline; Figure¬†[5](https://arxiv.org/html/2510.04555v1#S5.F5 "Figure 5 ‚Ä£ Per-variant panels. ‚Ä£ 5.3 Results ‚Ä£ 5 Experiments in Arbitrage-Free Synthetic Markets ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") reports the same for Tail-Safe.
In both, tail markers highlight VaRŒ±\operatorname{VaR}\_{\alpha} and ESŒ±\operatorname{ES}\_{\alpha} at the evaluation Œ±\alpha level.

![Refer to caption](baseline_ecdf.png)


(a) Baseline ECDF

![Refer to caption](baseline_hist.png)


(b) Baseline histogram (+VaR/ES)

Figure 4: QP-only Baseline. Unified samples; tail annotations correspond to Œ±\alpha used at evaluation.



![Refer to caption](tailsafe_ecdf.png)


(a) Tail-Safe ECDF

![Refer to caption](tailsafe_hist.png)


(b) Tail-Safe histogram (+VaR/ES)

Figure 5: Tail-Safe. Left-tail improvement with central mass preserved

##### Training dynamics.

We observe small per-update policy KL (e.g., within a narrow band consistent with the PPO clip parameter), stable effective tail coverage w^‚âàwtarget\widehat{w}\!\approx\!w\_{\mathrm{target}} across the Œ±\alpha schedule, and smooth critic losses; a single conservative spike in KL can occur when tightening Œ±\alpha, after which EMA referencing re-stabilizes updates (Appendix¬†C: logs and plots).

### 5.4 Ablations

We ablate the key design components to attribute gains and understand failure modes:

1. 1.

   No coverage controller (uniform quantiles): degrades tail estimation and increases training variance; CVaR improvements diminish, especially at small Œ±\alpha.
2. 2.

   No KL‚ÄìDRO (ŒªKL=0\lambda\_{\mathrm{KL}}{=}0): increases policy drift; occasional regressions in tail metrics under OOD stress.
3. 3.

   No tail guards in QP (no NTB shrinkage / no sign gate): higher drawdowns near expiry and during correlation spikes; feasibility rate decreases.
4. 4.

   Risk-neutral RL + CBF‚ÄìQP (no CVaR objective): preserves hard safety but does not systematically improve the left tail relative to Tail-Safe.

Table 2: Ablation matrix (qualitative summary). ‚Äú‚Üë\uparrow‚Äù/‚Äú‚Üì\downarrow‚Äù indicate consistent directional changes vs. Tail-Safe; blank indicates neutral/mixed.

| Variant | ESŒ± | VaRŒ± | Mean/Sharpe | Coverage w^\widehat{w} | Feasibility | Solver time |
| --- | --- | --- | --- | --- | --- | --- |
| No coverage controller | ‚Üë\uparrow (worse) | ‚Üë\uparrow | ‚Üì\downarrow | unstable | ‚àº\sim | ‚àº\sim |
| No KL‚ÄìDRO | ‚Üë\uparrow (worse) | ‚Üë\uparrow | mixed | ‚àº\sim | ‚àº\sim | ‚àº\sim |
| No tail guards (QP) | ‚Üë\uparrow near expiry | ‚Üë\uparrow | ‚Üì\downarrow | ‚àº\sim | ‚Üì\downarrow | ‚àº\sim |
| Risk-neutral RL + CBF‚ÄìQP | ‚Üë\uparrow vs. Tail-Safe | ‚Üë\uparrow | mixed | ‚àº\sim | ‚àº\sim | ‚àº\sim |

### 5.5 Safety Telemetry

##### Active constraints and tightness.

We report the empirical distribution of tightest\_id across time/episodes (Appendix¬†C: bar plots).
Near expiry, NTB and rate caps dominate; during volatility spikes, the sign-consistency gate activates more frequently, suppressing reactive trades.

##### Slack and feasibility.

By construction, when the QP is feasible with Œ∂=0\zeta{=}0, there are *zero* hard-constraint violations (Theorem¬†[1](https://arxiv.org/html/2510.04555v1#Thmtheorem1 "Theorem 1 (Robust forward invariance). ‚Ä£ Theorem 1 (robust forward invariance of the safety set) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")).
We tabulate the feasibility rate and the distribution of slack\_sum; infeasibility incidents are rare and trigger a shaped penalty and logging (Appendix¬†C: incident table).

##### Solver latency.

We summarize solver time per step (median/P95) and success rates for OSQP.
Warm starts and modest conditioning of HH keep solve times stable across stress regimes.

### 5.6 Takeaways

Tail-Safe delivers (i) *hard safety with explanations* (active-set, tightness, rate/gate telemetry), (ii) *tail shaping* (improved left-tail metrics as Œ±\alpha tightens) with comparable central performance, and (iii) *robustness under shift* via KL-regularized updates and tail-coverage stabilization.
These properties hold across ID and OOD stress regimes in arbitrage-free, microstructure-aware markets.

## 6 Explainability & Governance

This section operationalizes Tail-Safe for auditability and oversight.
We (i) map solver and learner *telemetry* to concrete *governance workflows* (who inspects which signals and how thresholds trigger actions),
(ii) instantiate common *business rules* (leverage, liquidity, short-sale, drawdown) as white-box CBF constraints together with human-readable interception rationales, and
(iii) define an incident taxonomy and escalation playbook consistent with financial model-risk guidance (e.g., SR¬†11-7, BCBS¬†239, NIST AI RMF)¬†[[66](https://arxiv.org/html/2510.04555v1#bib.bib66), [67](https://arxiv.org/html/2510.04555v1#bib.bib67), [68](https://arxiv.org/html/2510.04555v1#bib.bib68)].

### 6.1 Telemetry-to-Governance Mapping

The CBF‚ÄìQP solver and the RL learner emit stepwise and aggregate telemetry (Sec.¬†[3.3](https://arxiv.org/html/2510.04555v1#S3.SS3 "3.3 White-Box CBF‚ÄìQP Safety Layer ‚Ä£ 3 Method: Tail-Safe Hedging Framework ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")):
active\_set, tightest\_id, rate\_util, gate\_score, slack\_sum, solver\_status/time, the per-update policy KL\_step, the effective tail coverage w^\widehat{w}, and the CVaR schedule Œ±\alpha.
Table¬†[3](https://arxiv.org/html/2510.04555v1#S6.T3 "Table 3 ‚Ä£ 6.1 Telemetry-to-Governance Mapping ‚Ä£ 6 Explainability & Governance ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") translates these signals into dashboard tiles, thresholds, owners, and actions.
Thresholds are conservative defaults; institution-specific limits can be codified in a policy registry with versioning (Appendix¬†C).

Table 3: Telemetry ‚Üí\rightarrow governance workflow mapping. ‚ÄúOwner‚Äù follows a RACI-style assignment: Trader (TR), Market Risk (MR), Model Risk Management (MRM), Compliance (CO), Operations (OPS), Internal Audit (IA).

| Signal | Dashboard tile | Threshold (example) | Owner | Action on breach |
| --- | --- | --- | --- | --- |
| tightest\_id | Constraint frequency barplot | NTB or Rate >> 60% for 1d | MR ‚Üí\rightarrow TR | Tighten bands; reschedule |
| slack\_sum | Slack mass time series | Any >0>0 (event) | MRM/CO | Incident; penalty; freeze size |
| solver\_status/time | P95 latency/solve status | P95 >> 20ms or nonoptimal >>0.1% | OPS | Warm-start; failover; add capacity |
| rate\_util | Rate utilization histogram | P95 >0.9>0.9 for 1h | MR ‚Üí\rightarrow TR | Cut rmaxr\_{\max}; stagger hedges |
| gate\_score | Gate pass-rate | Pass-rate <85%<85\% intraday | MRM | Audit; recal Œ¥adv\delta\_{\mathrm{adv}} |
| KL\_step | Policy KL per update | P95 >Œ≤‚ãÜ>\beta^{\star} | MRM | ‚ÜëŒªKL\uparrow\lambda\_{\mathrm{KL}}; slow schedule |
| w^\widehat{w} vs. wtargetw\_{\mathrm{target}} | Tail coverage gauge | |w^‚àíwtarget|>0.02|\widehat{w}-w\_{\mathrm{target}}|>0.02 | MRM | PID retune; ‚Üë\uparrowbatch |
| Œ±\alpha schedule | CVaR level tracker | Failed to tighten on time | MRM | Hold; check variance |

##### Explanation artifacts.

For each interception (i.e., when utnom‚â†utu\_{t}^{\mathrm{nom}}\neq u\_{t}), we log a structured *Explanation Record*:
(i) IDs/names of the active constraints (from active\_set),
(ii) the *tightest* constraint and its slack/multiplier,
(iii) the HH-metric deviation ‚Äñut‚àíutnom‚ÄñH\|u\_{t}{-}u\_{t}^{\mathrm{nom}}\|\_{H},
(iv) human-readable rule names (e.g., ‚ÄúLeverage limit‚Äù), and
(v) a natural-language rationale template (below).
Records are immutable and indexed for audit queries (Appendix¬†C.2: storage schema).

### 6.2 Business Rules as CBF Constraints (with Explanations)

We instantiate common policy limits as barrier functions hi‚Äã(x)‚â•0h\_{i}(x)\!\geq\!0; each one admits a plain-language explanation and a quantitative telemetry view.

Table 4: Business rules ‚Üí\rightarrow CBF instantiation and explanation.

| Rule | CBF form (illustrative) | Explanation (human-readable) |
| --- | --- | --- |
| Leverage cap | hlev‚Äã(x)=Lmax‚àíLev‚Äã(x)h\_{\mathrm{lev}}(x)=L\_{\max}-\mathrm{Lev}(x) | ‚ÄúLeverage cannot exceed LmaxL\_{\max}; trade reduced to keep hlev‚â•0h\_{\mathrm{lev}}\geq 0.‚Äù |
| Liquidity budget | hliq‚Äã(x,u)=Vavail‚Äã(x)‚àí‚Äñu‚ÄñŒõh\_{\mathrm{liq}}(x,u)=V\_{\mathrm{avail}}(x)-\|u\|\_{\Lambda} | ‚ÄúOrder size limited by available depth/impact budget.‚Äù |
| Short-sale limit | hshort‚Äã(x)=Qmin‚àíqshort‚Äã(x)h\_{\mathrm{short}}(x)=Q\_{\min}-q\_{\mathrm{short}}(x) | ‚ÄúShort inventory cannot breach QminQ\_{\min}.‚Äù |
| Drawdown guard | hdd‚Äã(x)=Dmax‚àíDD‚Äã(x)h\_{\mathrm{dd}}(x)=D\_{\max}-\mathrm{DD}(x) | ‚ÄúCumulative drawdown kept below DmaxD\_{\max}.‚Äù |
| Rate cap | implicit via ‚Äñut‚àíut‚àí1‚Äñ2‚â§rmax\|u\_{t}{-}u\_{t-1}\|\_{2}\leq r\_{\max} | ‚ÄúAdjustment rate bounded by rmaxr\_{\max} to avoid whipsaw.‚Äù |
| NTB (ellipsoid) | e‚ä§‚ÄãM‚Äãe‚â§bmaxe^{\top}Me\leq b\_{\max} | ‚ÄúExposure error confined within the no-trade band.‚Äù |
| Sign gate | gcons‚Äã(x,u)‚â•0g\_{\mathrm{cons}}(x,u)\geq 0 | ‚ÄúTrade direction must align with signals beyond Œ¥adv\delta\_{\mathrm{adv}}.‚Äù |

##### Illustrative explanations.

For each rule, the solver‚Äôs KKT multipliers quantify *how binding* the rule is; we surface them as ‚Äútightness‚Äù bars in the dashboard.
For example, if tightest\_id corresponds to *Rate cap*, the explanation highlights the capped step and suggests staggering or a temporary rmaxr\_{\max} reduction.

### 6.3 Incident Taxonomy and Escalation Playbook

We classify events into three severities with actionable responses and owners.

Table 5: Incident taxonomy and escalation.

| Severity | Condition | Owner | Response |
| --- | --- | --- | --- |
| S1 (soft intercept) | utnom‚â†utu\_{t}^{\mathrm{nom}}\neq u\_{t}, ‚ÄñŒ∂‚Äñ1=0\|\zeta\|\_{1}=0 | TR/MR | Log record; no halt; monitor tightest frequencies |
| S2 (hard intercept) | ‚ÄñŒ∂‚Äñ1>0\|\zeta\|\_{1}>0 or repeated rate saturation | MRM/CO | Apply penalty; partial freeze; review constraints; RCA within 1d |
| S3 (solver failure) | Non-optimal status or timeout | OPS/MRM | Failover; revert to baseline QP-only; RCA within 4h; postmortem |

##### Trigger logic (runtime).

The following policies are enforced online and recorded for audit (pseudo-code; full governance rules in Appendix¬†C.3):

* ‚Ä¢

  Slack trigger: if slack\_sum>0\texttt{slack\\_sum}>0 for kk consecutive steps (k‚àà[1,3]k\!\in\![1,3]), (i) downscale action norm by Œ∑<1\eta\!<\!1, (ii) raise alert to MRM, (iii) freeze size upon repeat.
* ‚Ä¢

  Rate saturation trigger: if rate\_util>0.95\texttt{rate\\_util}>0.95 at P95 over 10 minutes, tighten rmaxr\_{\max} by factor Œ∑r\eta\_{r} and stagger orders.
* ‚Ä¢

  Gate trigger: if gate pass-rate drops below 85%85\% intraday, increase Œ¥adv\delta\_{\mathrm{adv}} and review signals; if unresolved, switch to QP-only baseline.
* ‚Ä¢

  KL drift trigger: if KL\_step>Œ≤‚ãÜ\texttt{KL\\_step}>\beta^{\star} at P95 for 5 updates, raise ŒªKL\lambda\_{\mathrm{KL}} and slow the Œ±\alpha schedule.

### 6.4 Audit Queries and Periodic Reviews

In addition to runtime triggers, periodic reviews (weekly/monthly) support compliance and internal audit:

1. 1.

   Constraint mix: distribution of tightest\_id by regime (ID/OOD, expiry buckets), highlighting persistent bottlenecks.
2. 2.

   Intercept cost: average HH-norm deviation ‚Äñu‚àíunom‚ÄñH\|u{-}u^{\mathrm{nom}}\|\_{H} and its contribution to P&L shortfall (Appendix¬†C.4 decomposition).
3. 3.

   Feasibility trend: time series of feasibility rate and slack mass; root-cause analysis for S2/S3 spikes.
4. 4.

   Tail control: realized ESŒ±\operatorname{ES}\_{\alpha} vs. scheduled Œ±\alpha; coverage tracking (w^,wtarget)(\widehat{w},w\_{\mathrm{target}}).
5. 5.

   Change control: diffs in policy parameters and constraint registries; sign-offs by MRM/CO; SR¬†11-7 documentation hooks¬†[[66](https://arxiv.org/html/2510.04555v1#bib.bib66)].

##### Human-readable & machine-readable parity.

Every interception has (i) a natural-language rationale (template above) and (ii) a structured record (IDs, multipliers, thresholds, margins).
This parity serves both reviewers and automated conformance checks (BCBS¬†239 data lineage and auditability requirements)¬†[[67](https://arxiv.org/html/2510.04555v1#bib.bib67)].

### 6.5 Risk, Compliance, and Scope Notes

*Scope.* Tail-Safe enforces selected constraints by construction and exposes telemetry for others.
Constraints whose surrogates are only approximate (e.g., complex liquidity or borrow availability) are flagged as ‚Äúmonitor-only‚Äù with thresholds and escalation but not guaranteed by the CBF invariance (Sec.¬†[4](https://arxiv.org/html/2510.04555v1#S4 "4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")).

*Residual risk.* When the QP is infeasible (rare), we document the fallback (penalty + baseline), the magnitude/duration, and the mitigation timeline, aligning with MRM incident handling¬†[[66](https://arxiv.org/html/2510.04555v1#bib.bib66), [68](https://arxiv.org/html/2510.04555v1#bib.bib68)].
Broader regulatory mapping (e.g., model lifecycle, validation independence, change control) is included in Appendix¬†C (Governance Checklist).

## 7 Limitations and Scope

We summarize the principal limitations of Tail-Safe to clarify the scope of our claims. These caveats are intended to be candid and actionable without diminishing the theoretical contributions in Sections¬†[3](https://arxiv.org/html/2510.04555v1#S3 "3 Method: Tail-Safe Hedging Framework ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")‚Äì[4](https://arxiv.org/html/2510.04555v1#S4 "4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"). Where appropriate, we outline concrete paths to address each limitation in future work.

##### Synthetic market only.

All experiments (Sec.¬†[5](https://arxiv.org/html/2510.04555v1#S5 "5 Experiments in Arbitrage-Free Synthetic Markets ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")) are conducted in an *arbitrage-free but synthetic* environment built from SSVI¬†‚Üí\rightarrow¬†Dupire¬†‚Üí\rightarrow¬†VIX (¬ß[2.1](https://arxiv.org/html/2510.04555v1#S2.SS1 "2.1 Arbitrage-Free Volatility Surfaces via SSVI ‚Ä£ 2 Preliminaries & Problem Setting ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")‚Äì[2.3](https://arxiv.org/html/2510.04555v1#S2.SS3 "2.3 VIX Leg from Surface-Consistent Variance ‚Ä£ 2 Preliminaries & Problem Setting ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")).
While this stack ensures static no-arbitrage and inherits stylized skew/term-structure, it does not capture the full richness of historical dynamics (macro news, regime shifts, structural breaks, jumps, cross-sectional co-movement, borrow frictions, corporate actions).
*Scope:* Our empirical claims (tail-shaping, feasibility rates, telemetry behavior) pertain to this controlled setting.
*Next steps:* historical *replay* against top-of-book/L1 or full LOB data; *semi-synthetic* playback injecting real returns with simulated fills; and *conditional stress* using realized event calendars.

##### Simplified execution and microstructure.

The execution adapter abstracts to spread, linear temporary impact, and a transient resilience kernel (Sec.¬†[2.4](https://arxiv.org/html/2510.04555v1#S2.SS4 "2.4 Execution, Microstructure, and Impact ‚Ä£ 2 Preliminaries & Problem Setting ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")).
It omits queue-reactive dynamics, order priority, asymmetric information/adverse selection, hidden liquidity, fee/rebate tiers, tick-size effects, and venue fragmentation.
*Scope:* Results on cost and latency are indicative, not definitive, for live markets.
*Next steps:* queue-based impact models, order-cancels-replace loops, latency/jitter modeling, cross-venue routing, and *empirical* calibration to venue-level impact curves.

##### Approximation in safety-layer modeling.

The discrete-time CBF constraints rely on local models xt+1=f‚Äã(xt)+g‚Äã(xt)‚Äãutx\_{t+1}=f(x\_{t})+g(x\_{t})u\_{t} and Lipschitz/mismatch bounds (Assumption¬†[1](https://arxiv.org/html/2510.04555v1#Thmassumption1 "Assumption 1 (Local dynamics and mismatch). ‚Ä£ Theorem 1 (robust forward invariance of the safety set) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")).
Barrier functions encode business rules via convex surrogates and linearizations in uu (Assumption¬†[3](https://arxiv.org/html/2510.04555v1#Thmassumption3 "Assumption 3 (Convex feasibility). ‚Ä£ Proposition 1 (minimal-deviation ùêª-metric projection) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")).
*Scope:* The *forward-invariance guarantee* (Theorem¬†[1](https://arxiv.org/html/2510.04555v1#Thmtheorem1 "Theorem 1 (Robust forward invariance). ‚Ä£ Theorem 1 (robust forward invariance of the safety set) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")) holds when the QP is feasible with zero slack (‚ÄâŒ∂=0\zeta{=}0) and the margin condition is satisfied.
When the QP is infeasible (rare in our synthetic tests), the layer reverts to penalties and logging; it does not *guarantee* satisfaction.
*Next steps:* robust/tube CBFs, disturbance observers, set-valued dynamics, and feasibility recovery strategies; formal analysis of linearization error budgets (Appendix¬†A.6 road map).

##### Estimator variance and controller tuning.

The temperature-tilted CVaR estimator is self-normalized and subject to variance/bias trade-offs controlled by (T,Œ≥tail)(T,\gamma\_{\mathrm{tail}}) and the coverage controller (Sec.¬†[3.2](https://arxiv.org/html/2510.04555v1#S3.SS2 "3.2 Tail-Coverage Controller: Temperature Sampling and Tail-Boost ‚Ä£ 3 Method: Tail-Safe Hedging Framework ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")).
*Scope:* The concentration result (Theorem¬†[3](https://arxiv.org/html/2510.04555v1#Thmtheorem3 "Theorem 3 (Concentration of the temperature-tilted CVaR estimator). ‚Ä£ Theorem 3 (bias/variance and sample complexity of the CVaR estimator) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")) holds under bounded losses and weights, and for an *effective* tail mass close to the target; misspecification of these conditions can inflate variance.
*Next steps:* adaptive batch-sizing, control-theoretic anti-windup, and doubly-robust estimators for tail integrals.

##### Trust-region and robustness assumptions.

The DRO bound (Theorem¬†[2](https://arxiv.org/html/2510.04555v1#Thmtheorem2 "Theorem 2 (Donsker‚ÄìVaradhan bound and per-state KL conservatism). ‚Ä£ Theorem 2 (KL‚ÄìDRO upper bound and conservatism of per-state KL) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")) relies on occupancy coupling to relate pathwise and per-state KL;
the CVaR trust-region result (Theorem¬†[4](https://arxiv.org/html/2510.04555v1#Thmtheorem4 "Theorem 4 (CVaR trust-region improvement). ‚Ä£ Theorem 4 (trust-region improvement inequality for CVaR with KL limits) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")) uses Lipschitz smoothness and Pinsker-type bounds to control distribution shift.
*Scope:* These are standard but *local* assumptions; violations (e.g., abrupt policy changes, heavy-tailed losses) can weaken constants or rates.
*Next steps:* stronger coupling via mixing coefficients or spectral gaps; nonasymptotic pathwise bounds; heavy-tail robustification.

##### Metric coverage.

We optimize and report CVaR/ES at a given Œ±\alpha (¬ß[2.6](https://arxiv.org/html/2510.04555v1#S2.SS6 "2.6 Risk Measures: VaR and ES/CVaR ‚Ä£ 2 Preliminaries & Problem Setting ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")); other tail functionals (extreme quantiles, expectiles, drawdown risk, spectral measures) are not directly optimized.
*Scope:* Improvements outside the targeted Œ±\alpha are empirical rather than guaranteed.
*Next steps:* multi-level or spectral risk objectives; path-dependent risk control (drawdown CBFs) with proofs paralleling Theorem¬†[1](https://arxiv.org/html/2510.04555v1#Thmtheorem1 "Theorem 1 (Robust forward invariance). ‚Ä£ Theorem 1 (robust forward invariance of the safety set) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets").

##### Generalization across books and instruments.

We focus on a stylized SPX‚ÄìVIX hedging book.
*Scope:* Constraints and signals (NTB, sign gate) are finance-specific but not yet validated for rates/credit/commodities or multi-currency portfolios.
*Next steps:* multi-asset extension with cross-gamma/vega and inventory coupling; borrow/rehypothecation constraints; FX basis and curve risk.

##### Governance and compliance boundaries.

The telemetry-to-governance mapping (Sec.¬†[6](https://arxiv.org/html/2510.04555v1#S6 "6 Explainability & Governance ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")) aligns with common guidance, but it is not a substitute for institution-specific model governance (validation independence, challenger models, change control).
*Scope:* We provide an *operational* starting point (dashboards, triggers, audit records) rather than a complete compliance program.
*Next steps:* integration with enterprise MRM workflows (SR¬†11-7/BCBS¬†239/NIST AI RMF), automated conformance checks, and periodic model risk reviews.

##### Ethical and adversarial considerations.

We do not model adversarial behavior (probing, spoofing, latency games) against the agent or the execution venue.
*Scope:* Safety guarantees are not security guarantees.
*Next steps:* adversarial stress testing, red-team evaluations, and anomaly detection hooks tied to telemetry.

##### Computational constraints.

Real-time feasibility depends on solver latency, model size, and hardware. Our OSQP-based implementation meets timing in the synthetic setup, but live deployments may face tighter budgets.
*Scope:* The results demonstrate feasibility *in silico*.
*Next steps:* problem sparsity exploitation, warm-start policies, batching, and hardware acceleration.

##### Summary of claims.

(i) *Hard safety* is guaranteed by construction *only* when the QP is feasible with zero slack and under the margin conditions (Thm.¬†[1](https://arxiv.org/html/2510.04555v1#Thmtheorem1 "Theorem 1 (Robust forward invariance). ‚Ä£ Theorem 1 (robust forward invariance of the safety set) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"));
(ii) *Tail improvements* are demonstrated empirically in arbitrage-free synthetic markets and supported by estimator concentration (Thm.¬†[3](https://arxiv.org/html/2510.04555v1#Thmtheorem3 "Theorem 3 (Concentration of the temperature-tilted CVaR estimator). ‚Ä£ Theorem 3 (bias/variance and sample complexity of the CVaR estimator) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")) and trust-region reasoning (Thm.¬†[4](https://arxiv.org/html/2510.04555v1#Thmtheorem4 "Theorem 4 (CVaR trust-region improvement). ‚Ä£ Theorem 4 (trust-region improvement inequality for CVaR with KL limits) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"));
(iii) *Explainability* follows from the CBF‚ÄìQP‚Äôs active-set/dual telemetry and the projection interpretation (Prop.¬†[1](https://arxiv.org/html/2510.04555v1#Thmproposition1 "Proposition 1 (Shifted projection). ‚Ä£ Proposition 1 (minimal-deviation ùêª-metric projection) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")).
Real-data backtesting and broader market coverage are deliberately left to future work to preserve a clean separation between theory and synthetic evaluation.

## 8 Related Work

We review connections to prior work on (i) deep hedging and RL in finance, (ii) risk-sensitive/CVaR reinforcement learning, (iii) safety filters and control-barrier-function (CBF) methods for safe RL, (iv) distributionally robust RL (DRO-RL), and (v) explainability and governance for financial ML. Our approach differs by combining *distributional, CVaR-optimized learning* with a *white-box CBF‚ÄìQP safety layer* that provides *stepwise audit telemetry* and by establishing *theoretical guarantees* (Sec.¬†[4](https://arxiv.org/html/2510.04555v1#S4 "4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")) that link KL-regularized policy updates to a KL‚ÄìDRO upper bound and a CVaR trust-region improvement inequality, while proving robust forward invariance and feasibility persistence for the safety layer.

##### Deep hedging and RL in finance.

Deep hedging learns nonparametric hedge policies under frictions and liquidity constraints¬†[[6](https://arxiv.org/html/2510.04555v1#bib.bib6)], and recent surveys catalogue advances in trading/hedging RL¬†[[10](https://arxiv.org/html/2510.04555v1#bib.bib10), [11](https://arxiv.org/html/2510.04555v1#bib.bib11), [12](https://arxiv.org/html/2510.04555v1#bib.bib12)].
Other lines integrate microstructure and execution‚Äîfrom Almgren‚ÄìChriss and Obizhaeva‚ÄìWang impact models¬†[[1](https://arxiv.org/html/2510.04555v1#bib.bib1), [5](https://arxiv.org/html/2510.04555v1#bib.bib5)] to agent-based LOB simulators (ABIDES)¬†[[7](https://arxiv.org/html/2510.04555v1#bib.bib7), [40](https://arxiv.org/html/2510.04555v1#bib.bib40)] and market-microstructure monographs¬†[[77](https://arxiv.org/html/2510.04555v1#bib.bib77)].
Tail-Safe differs in three respects: (i) it *optimizes a tail risk* (CVaR) rather than mean performance, (ii) it enforces *hard state-wise constraints by construction* using a CBF‚ÄìQP filter with telemetry, and (iii) it provides *formal guarantees* on safety invariance, DRO conservatism via KL, and trust-region CVaR improvement (Theorems¬†[1](https://arxiv.org/html/2510.04555v1#Thmtheorem1 "Theorem 1 (Robust forward invariance). ‚Ä£ Theorem 1 (robust forward invariance of the safety set) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"), [2](https://arxiv.org/html/2510.04555v1#Thmtheorem2 "Theorem 2 (Donsker‚ÄìVaradhan bound and per-state KL conservatism). ‚Ä£ Theorem 2 (KL‚ÄìDRO upper bound and conservatism of per-state KL) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"), [4](https://arxiv.org/html/2510.04555v1#Thmtheorem4 "Theorem 4 (CVaR trust-region improvement). ‚Ä£ Theorem 4 (trust-region improvement inequality for CVaR with KL limits) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")).

##### Risk-sensitive RL and CVaR objectives.

Risk-sensitive RL with coherent risk measures (e.g., CVaR) has been studied via policy gradients and actor‚Äìcritic methods¬†[[17](https://arxiv.org/html/2510.04555v1#bib.bib17), [19](https://arxiv.org/html/2510.04555v1#bib.bib19), [18](https://arxiv.org/html/2510.04555v1#bib.bib18)], with distributional critics (IQN) offering flexible quantile modeling¬†[[13](https://arxiv.org/html/2510.04555v1#bib.bib13), [14](https://arxiv.org/html/2510.04555v1#bib.bib14)].
Recent works explore scalable or constrained CVaR optimization and robust variants¬†[[37](https://arxiv.org/html/2510.04555v1#bib.bib37), [35](https://arxiv.org/html/2510.04555v1#bib.bib35), [36](https://arxiv.org/html/2510.04555v1#bib.bib36)].
Our contribution is orthogonal: we introduce a *coverage-controlled* quantile sampling scheme that directly stabilizes the *estimation error* of CVaR at small Œ±\alpha (Theorem¬†[3](https://arxiv.org/html/2510.04555v1#Thmtheorem3 "Theorem 3 (Concentration of the temperature-tilted CVaR estimator). ‚Ä£ Theorem 3 (bias/variance and sample complexity of the CVaR estimator) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")), and couple it with a *white-box* safety layer; we also provide a *CVaR trust-region inequality* under per-state KL limits (Theorem¬†[4](https://arxiv.org/html/2510.04555v1#Thmtheorem4 "Theorem 4 (CVaR trust-region improvement). ‚Ä£ Theorem 4 (trust-region improvement inequality for CVaR with KL limits) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")).

##### Safe RL: shielding, projection, and CBF-based filters.

Safe RL surveys cover constraint handling via penalties, Lagrangians, shielding, and model-based fallback¬†[[69](https://arxiv.org/html/2510.04555v1#bib.bib69), [22](https://arxiv.org/html/2510.04555v1#bib.bib22)].
Shielding for discrete MDPs uses a precomputed safety automaton to override unsafe actions¬†[[70](https://arxiv.org/html/2510.04555v1#bib.bib70)]; action projection in continuous control enforces linearized safety via a QP layer¬†[[71](https://arxiv.org/html/2510.04555v1#bib.bib71)].
In control theory, CBF‚ÄìQP methods enforce forward invariance of safe sets at each step¬†[[23](https://arxiv.org/html/2510.04555v1#bib.bib23), [24](https://arxiv.org/html/2510.04555v1#bib.bib24)], with robust/high-order/differentiable extensions¬†[[30](https://arxiv.org/html/2510.04555v1#bib.bib30), [26](https://arxiv.org/html/2510.04555v1#bib.bib26), [27](https://arxiv.org/html/2510.04555v1#bib.bib27), [25](https://arxiv.org/html/2510.04555v1#bib.bib25)].
Differentiable optimization layers (QP/convex) have also been embedded in deep models¬†[[72](https://arxiv.org/html/2510.04555v1#bib.bib72), [73](https://arxiv.org/html/2510.04555v1#bib.bib73)].
Tail-Safe leverages *discrete-time* CBF constraints specialized to finance (ellipsoidal NTB, box/rate, sign-consistency gate), *proves* robust forward invariance under bounded mismatch (Theorem¬†[1](https://arxiv.org/html/2510.04555v1#Thmtheorem1 "Theorem 1 (Robust forward invariance). ‚Ä£ Theorem 1 (robust forward invariance of the safety set) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")) and minimal-deviation projection (Proposition¬†[1](https://arxiv.org/html/2510.04555v1#Thmproposition1 "Proposition 1 (Shifted projection). ‚Ä£ Proposition 1 (minimal-deviation ùêª-metric projection) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")), and *surfaces KKT/active-set telemetry* to enable audit workflows (Sec.¬†[6](https://arxiv.org/html/2510.04555v1#S6 "6 Explainability & Governance ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")).
Compared to generic projection/shielding, our layer is domain-specific (exposure- and microstructure-aware) and backed by an incident-handling governance mapping.

##### Distributionally robust RL (DRO-RL).

Robust MDPs with uncertainty sets date back to classical analyses¬†[[74](https://arxiv.org/html/2510.04555v1#bib.bib74), [75](https://arxiv.org/html/2510.04555v1#bib.bib75)], with modern variants using ff-divergences or Wasserstein ambiguity sets for policy evaluation and learning¬†[[76](https://arxiv.org/html/2510.04555v1#bib.bib76), [35](https://arxiv.org/html/2510.04555v1#bib.bib35), [36](https://arxiv.org/html/2510.04555v1#bib.bib36)].
In parallel, KL-regularized policy optimization (TRPO/PPO) has a long-standing trust-region interpretation¬†[[15](https://arxiv.org/html/2510.04555v1#bib.bib15), [16](https://arxiv.org/html/2510.04555v1#bib.bib16)] and has been adapted to constrained settings¬†[[48](https://arxiv.org/html/2510.04555v1#bib.bib48)].
We bridge these threads by showing that per-state KL penalties control a *KL‚ÄìDRO upper bound* on the CVaR surrogate (Theorem¬†[2](https://arxiv.org/html/2510.04555v1#Thmtheorem2 "Theorem 2 (Donsker‚ÄìVaradhan bound and per-state KL conservatism). ‚Ä£ Theorem 2 (KL‚ÄìDRO upper bound and conservatism of per-state KL) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")) and yield a *CVaR trust-region improvement* bound (Theorem¬†[4](https://arxiv.org/html/2510.04555v1#Thmtheorem4 "Theorem 4 (CVaR trust-region improvement). ‚Ä£ Theorem 4 (trust-region improvement inequality for CVaR with KL limits) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"))‚Äîto our knowledge, such a pairing of DRO interpretation and CVaR performance-difference analysis has not been stated explicitly for the finance setting with a safety filter.

##### Explainability and governance in financial ML.

Interpretability surveys argue for transparent mechanisms over post-hoc explanations in high-stakes domains¬†[[78](https://arxiv.org/html/2510.04555v1#bib.bib78), [79](https://arxiv.org/html/2510.04555v1#bib.bib79), [80](https://arxiv.org/html/2510.04555v1#bib.bib80)].
Financial firms operate under model-risk governance such as SR¬†11-7 and BCBS¬†239, requiring documentation, monitoring, and auditable decision records.
Our telemetry-to-governance mapping (Sec.¬†[6](https://arxiv.org/html/2510.04555v1#S6 "6 Explainability & Governance ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")) uses *white-box* CBF‚ÄìQP artifacts (active sets, multipliers, slack, rate utilization, gate scores) to produce *structured* and *human-readable* rationale for each interception, aligning methodologically with those expectations while preserving by-construction safety guarantees.

##### Summary of differences.

Relative to prior deep hedging and RL-in-finance work, Tail-Safe provides (i) *formal tail-risk learning guarantees* (coverage-controlled CVaR estimation and trust-region inequality), (ii) a *finance-specialized, auditable* CBF‚ÄìQP safety layer with invariance and projection properties, and (iii) a *DRO-consistent* view of KL-regularized updates.
This combination targets *deployable* hedging: robust to distribution shift, safe by construction, and explainable to risk managers and auditors.

## 9 Conclusion

We presented Tail-Safe, a deployability-oriented framework for hedging under market frictions that unifies *distributional, risk-sensitive learning* with a *white-box* safety layer.
On the learning side, we optimize tail risk via IQN‚ÄìCVaR‚ÄìPPO and stabilize small-Œ±\alpha training through a *Tail-Coverage Controller* that regulates quantile sampling with temperature and tail-boost.
On the safety side, we enforce state-wise constraints by construction with a finance-specialized CBF‚ÄìQP filter‚Äîcombining discrete-time CBF inequalities, an ellipsoidal no-trade band (NTB), box/rate limits, and a sign-consistency gate‚Äîand expose *audit-ready telemetry* (active sets, tightness, slack, rate utilization, gate scores, solver status).

##### Theoretical guarantees.

We established a suite of results that elevate Tail-Safe from a pragmatic recipe to a principled method:
(i) *robust forward invariance* of the safety set under bounded model mismatch (Theorem¬†[1](https://arxiv.org/html/2510.04555v1#Thmtheorem1 "Theorem 1 (Robust forward invariance). ‚Ä£ Theorem 1 (robust forward invariance of the safety set) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"));
(ii) a *minimal-deviation* projection interpretation of the safety QP (Proposition¬†[1](https://arxiv.org/html/2510.04555v1#Thmproposition1 "Proposition 1 (Shifted projection). ‚Ä£ Proposition 1 (minimal-deviation ùêª-metric projection) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"));
(iii) a *KL‚ÄìDRO upper bound* connecting per-state KL regularization to distributionally robust CVaR control (Theorem¬†[2](https://arxiv.org/html/2510.04555v1#Thmtheorem2 "Theorem 2 (Donsker‚ÄìVaradhan bound and per-state KL conservatism). ‚Ä£ Theorem 2 (KL‚ÄìDRO upper bound and conservatism of per-state KL) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"));
(iv) *concentration and sample-complexity* of the temperature-tilted CVaR estimator with explicit coverage-mismatch terms (Theorem¬†[3](https://arxiv.org/html/2510.04555v1#Thmtheorem3 "Theorem 3 (Concentration of the temperature-tilted CVaR estimator). ‚Ä£ Theorem 3 (bias/variance and sample complexity of the CVaR estimator) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"));
(v) a *CVaR trust-region improvement* inequality under KL limits (Theorem¬†[4](https://arxiv.org/html/2510.04555v1#Thmtheorem4 "Theorem 4 (CVaR trust-region improvement). ‚Ä£ Theorem 4 (trust-region improvement inequality for CVaR with KL limits) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"));
(vi) *feasibility persistence* guarantees for expiry-aware NTB shrinkage and rate tightening (Theorem¬†[5](https://arxiv.org/html/2510.04555v1#Thmtheorem5 "Theorem 5 (Persistence via NTB shrinkage and rate tightening). ‚Ä£ Theorem 5 (feasibility persistence under tail guards) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")); and
(vii) *negative-advantage suppression* induced by the sign-consistency gate (Proposition¬†[2](https://arxiv.org/html/2510.04555v1#Thmproposition2 "Proposition 2 (Gate-induced lower bound). ‚Ä£ Proposition 2 (negative-advantage suppression by sign-consistency) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")).
Formal proofs are deferred to Appendix¬†A.

##### Empirical evidence and governance.

In arbitrage-free, microstructure-aware synthetic markets (SSVI‚Üí\!\rightarrowDupire‚Üí\!\rightarrowVIX with ABIDES/MockLOB execution), Tail-Safe achieves *left-tail improvements* while preserving central performance, with *zero* hard-constraint violations whenever the QP is feasible with zero slack.
Results are reported with unified sample sizes, multiple seeds, and paired bootstrap confidence intervals.
Beyond performance metrics, we demonstrate *operational explainability*: solver/learner telemetry is mapped to governance dashboards and incident playbooks (Sec.¬†[6](https://arxiv.org/html/2510.04555v1#S6 "6 Explainability & Governance ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")), providing structured, human-readable rationales for each interception alongside machine-verifiable records.

##### Scope and limitations.

Our empirical claims are confined to synthetic but finance-grounded settings; the execution layer abstracts queue-reactive and venue-specific effects; and some guarantees require feasibility and margin conditions (Sec.¬†[7](https://arxiv.org/html/2510.04555v1#S7 "7 Limitations and Scope ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")).
These boundaries are deliberate: they isolate the methodological core and create a clean landing zone for subsequent validation.

##### Outlook.

We see three promising thrusts for future work.
*(A) Real-data validation:* historical replays with top-of-book or full LOB feeds; semi-synthetic playback that couples realized returns with simulated fills; and event-conditioned stress studies.
*(B) Richer safety and robustness:* queue-reactive impact, borrow/rehypothecation constraints, drawdown and path-dependent CBFs, heavy-tail robust estimators, and tighter pathwise DRO couplings.
*(C) Broader portfolios and lifecycles:* multi-asset books with cross-gamma/vega limits, cross-currency hedging, offline/online hybrids, and deeper integration with enterprise model-risk workflows (change control, challenger models, periodic validation).

##### Final remark.

Tail-Safe aims to narrow the gap between modern RL and institutional requirements by offering *tail-aware learning*, *by-construction safety*, and *auditability* in one coherent framework.
We hope the blend of theory, engineering, and governance presented here provides a reproducible baseline‚Äîand a stepping stone toward trustworthy, regulation-ready AI for financial risk management.

## Appendix A Proofs for Section¬†[4](https://arxiv.org/html/2510.04555v1#S4 "4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")

This appendix provides complete proofs for Theorems¬†[1](https://arxiv.org/html/2510.04555v1#Thmtheorem1 "Theorem 1 (Robust forward invariance). ‚Ä£ Theorem 1 (robust forward invariance of the safety set) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"), [2](https://arxiv.org/html/2510.04555v1#Thmtheorem2 "Theorem 2 (Donsker‚ÄìVaradhan bound and per-state KL conservatism). ‚Ä£ Theorem 2 (KL‚ÄìDRO upper bound and conservatism of per-state KL) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"), [3](https://arxiv.org/html/2510.04555v1#Thmtheorem3 "Theorem 3 (Concentration of the temperature-tilted CVaR estimator). ‚Ä£ Theorem 3 (bias/variance and sample complexity of the CVaR estimator) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"), [4](https://arxiv.org/html/2510.04555v1#Thmtheorem4 "Theorem 4 (CVaR trust-region improvement). ‚Ä£ Theorem 4 (trust-region improvement inequality for CVaR with KL limits) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"), [5](https://arxiv.org/html/2510.04555v1#Thmtheorem5 "Theorem 5 (Persistence via NTB shrinkage and rate tightening). ‚Ä£ Theorem 5 (feasibility persistence under tail guards) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") and Propositions¬†[1](https://arxiv.org/html/2510.04555v1#Thmproposition1 "Proposition 1 (Shifted projection). ‚Ä£ Proposition 1 (minimal-deviation ùêª-metric projection) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"), [2](https://arxiv.org/html/2510.04555v1#Thmproposition2 "Proposition 2 (Gate-induced lower bound). ‚Ä£ Proposition 2 (negative-advantage suppression by sign-consistency) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets").
We use the notation and assumptions introduced in Sections¬†[2](https://arxiv.org/html/2510.04555v1#S2 "2 Preliminaries & Problem Setting ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")‚Äì[4](https://arxiv.org/html/2510.04555v1#S4 "4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets").
For a positive definite matrix H‚âª0H\succ 0, we write ‚Äñv‚ÄñH:=v‚ä§‚ÄãH‚Äãv\|v\|\_{H}:=\sqrt{v^{\top}Hv} and ‚ü®a,b‚ü©H:=a‚ä§‚ÄãH‚Äãb\langle a,b\rangle\_{H}:=a^{\top}Hb.

### A.1 Proof of Theorem¬†[1](https://arxiv.org/html/2510.04555v1#Thmtheorem1 "Theorem 1 (Robust forward invariance). ‚Ä£ Theorem 1 (robust forward invariance of the safety set) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") (robust forward invariance)

###### Lemma 1 (Lipschitz pushforward under bounded disturbance).

Let h:‚Ñùd‚Üí‚Ñùh:\mathbb{R}^{d}\!\to\!\mathbb{R} be LL-Lipschitz. If ‚Äñw‚Äñ‚â§w¬Ø\|w\|\leq\bar{w}, then for any z‚àà‚Ñùdz\in\mathbb{R}^{d},
h‚Äã(z+w)‚â•h‚Äã(z)‚àíL‚Äãw¬Ø.h(z+w)\geq h(z)-L\bar{w}.

###### Proof.

By Lipschitz continuity, |h‚Äã(z+w)‚àíh‚Äã(z)|‚â§L‚Äã‚Äñw‚Äñ‚â§L‚Äãw¬Ø|h(z+w)-h(z)|\leq L\|w\|\leq L\bar{w}, hence h‚Äã(z+w)‚â•h‚Äã(z)‚àíL‚Äãw¬Øh(z+w)\geq h(z)-L\bar{w}.
‚àé

###### Proof of Theorem¬†[1](https://arxiv.org/html/2510.04555v1#Thmtheorem1 "Theorem 1 (Robust forward invariance). ‚Ä£ Theorem 1 (robust forward invariance of the safety set) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets").

Fix tt and an index ii. By Assumption¬†[1](https://arxiv.org/html/2510.04555v1#Thmassumption1 "Assumption 1 (Local dynamics and mismatch). ‚Ä£ Theorem 1 (robust forward invariance of the safety set) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"), xt+1=f‚Äã(xt)+g‚Äã(xt)‚Äãut+wtx\_{t+1}=f(x\_{t})+g(x\_{t})u\_{t}+w\_{t} with ‚Äñwt‚Äñ‚â§w¬Ø\|w\_{t}\|\leq\bar{w}, and by Assumption¬†[2](https://arxiv.org/html/2510.04555v1#Thmassumption2 "Assumption 2 (Discrete-time CBF constraint with margin). ‚Ä£ Theorem 1 (robust forward invariance of the safety set) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | hi‚Äã(f‚Äã(xt)+g‚Äã(xt)‚Äãut)‚àí(1‚àíŒ∫i‚ÄãŒî‚Äãt)‚Äãhi‚Äã(xt)‚â•Œµi‚â•Li‚Äãw¬Ø.h\_{i}\!\big(f(x\_{t})+g(x\_{t})u\_{t}\big)-(1-\kappa\_{i}\Delta t)h\_{i}(x\_{t})\geq\varepsilon\_{i}\geq L\_{i}\bar{w}. |  | (21) |

Applying Lemma¬†[1](https://arxiv.org/html/2510.04555v1#Thmlemma1 "Lemma 1 (Lipschitz pushforward under bounded disturbance). ‚Ä£ A.1 Proof of Theorem 1 (robust forward invariance) ‚Ä£ Appendix A Proofs for Section 4 ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") with z=f‚Äã(xt)+g‚Äã(xt)‚Äãutz=f(x\_{t})+g(x\_{t})u\_{t} and w=wtw=w\_{t} gives

|  |  |  |
| --- | --- | --- |
|  | hi‚Äã(xt+1)‚â•hi‚Äã(f‚Äã(xt)+g‚Äã(xt)‚Äãut)‚àíLi‚Äãw¬Ø‚â•([21](https://arxiv.org/html/2510.04555v1#A1.E21 "In A.1 Proof of Theorem 1 (robust forward invariance) ‚Ä£ Appendix A Proofs for Section 4 ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"))(1‚àíŒ∫i‚ÄãŒî‚Äãt)‚Äãhi‚Äã(xt)+Œµi‚àíLi‚Äãw¬Ø‚â•(1‚àíŒ∫i‚ÄãŒî‚Äãt)‚Äãhi‚Äã(xt).h\_{i}(x\_{t+1})\;\geq\;h\_{i}\!\big(f(x\_{t})+g(x\_{t})u\_{t}\big)-L\_{i}\bar{w}\;\stackrel{{\scriptstyle\eqref{eq:margin-app}}}{{\geq}}\;(1-\kappa\_{i}\Delta t)h\_{i}(x\_{t})+\varepsilon\_{i}-L\_{i}\bar{w}\;\geq\;(1-\kappa\_{i}\Delta t)h\_{i}(x\_{t}). |  |

If hi‚Äã(xt)‚â•0h\_{i}(x\_{t})\geq 0, then hi‚Äã(xt+1)‚â•0h\_{i}(x\_{t+1})\geq 0. By induction over tt, the set ùíû={x:hi‚Äã(x)‚â•0,‚àÄi}\mathcal{C}=\{x:\,h\_{i}(x)\geq 0,\,\forall i\} is forward invariant.
‚àé

### A.2 Proof of Proposition¬†[1](https://arxiv.org/html/2510.04555v1#Thmproposition1 "Proposition 1 (Shifted projection). ‚Ä£ Proposition 1 (minimal-deviation ùêª-metric projection) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") (minimal-deviation projection)

###### Lemma 2 (Strict convexity and uniqueness).

Under Assumption¬†[3](https://arxiv.org/html/2510.04555v1#Thmassumption3 "Assumption 3 (Convex feasibility). ‚Ä£ Proposition 1 (minimal-deviation ùêª-metric projection) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"), if H‚âª0H\succ 0 and the feasible set ùíÆ‚Äã(xt)\mathcal{S}(x\_{t}) is nonempty, closed, and convex, then the QP¬†([17](https://arxiv.org/html/2510.04555v1#S3.E17 "In QP formulation and minimal-deviation projection. ‚Ä£ 3.3 White-Box CBF‚ÄìQP Safety Layer ‚Ä£ 3 Method: Tail-Safe Hedging Framework ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")) with Œ∂=0\zeta=0 has a unique minimizer.

###### Proof.

The objective u‚Ü¶12‚Äã‚Äñu‚àíunom‚ÄñH2+c‚ä§‚Äãuu\mapsto\frac{1}{2}\|u-u^{\mathrm{nom}}\|\_{H}^{2}+c^{\top}u is strictly convex due to H‚âª0H\succ 0. A strictly convex function over a nonempty, closed, convex set attains its minimum at a unique point.
‚àé

###### Proof of Proposition¬†[1](https://arxiv.org/html/2510.04555v1#Thmproposition1 "Proposition 1 (Shifted projection). ‚Ä£ Proposition 1 (minimal-deviation ùêª-metric projection) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets").

Completing the square yields

|  |  |  |
| --- | --- | --- |
|  | 12‚Äã(u‚àíunom)‚ä§‚ÄãH‚Äã(u‚àíunom)+c‚ä§‚Äãu=12‚Äã‚Äñu‚àí(unom‚àíH‚àí1‚Äãc)‚ÄñH2+const,\frac{1}{2}(u-u^{\mathrm{nom}})^{\top}H(u-u^{\mathrm{nom}})+c^{\top}u=\frac{1}{2}\big\|u-(u^{\mathrm{nom}}-H^{-1}c)\big\|\_{H}^{2}+\mathrm{const}, |  |

where the constant does not depend on uu. Hence minimizing the QP with Œ∂=0\zeta=0 over ùíÆ‚Äã(xt)\mathcal{S}(x\_{t}) is equivalent to projecting unom‚àíH‚àí1‚Äãcu^{\mathrm{nom}}-H^{-1}c onto ùíÆ‚Äã(xt)\mathcal{S}(x\_{t}) in the HH-norm. Uniqueness follows from Lemma¬†[2](https://arxiv.org/html/2510.04555v1#Thmlemma2 "Lemma 2 (Strict convexity and uniqueness). ‚Ä£ A.2 Proof of Proposition 1 (minimal-deviation projection) ‚Ä£ Appendix A Proofs for Section 4 ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets").
‚àé

### A.3 Proof of Theorem¬†[2](https://arxiv.org/html/2510.04555v1#Thmtheorem2 "Theorem 2 (Donsker‚ÄìVaradhan bound and per-state KL conservatism). ‚Ä£ Theorem 2 (KL‚ÄìDRO upper bound and conservatism of per-state KL) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") (KL‚ÄìDRO upper bound)

We recall the Donsker‚ÄìVaradhan variational inequality (e.g., [[54](https://arxiv.org/html/2510.04555v1#bib.bib54), [55](https://arxiv.org/html/2510.04555v1#bib.bib55)]): for any measurable œÜ\varphi and probability measures Q,PQ,P with Q‚â™PQ\ll P,

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùîºQ‚Äã[œÜ]‚â§1Œ∑‚Äã(KL‚Äã(Q‚à•P)+log‚Å°ùîºP‚Äã[eŒ∑‚ÄãœÜ]),‚àÄŒ∑>0.\mathbb{E}\_{Q}[\varphi]\;\leq\;\frac{1}{\eta}\Big(\mathrm{KL}(Q\|P)+\log\mathbb{E}\_{P}[e^{\eta\varphi}]\Big),\quad\forall\,\eta>0. |  | (22) |

###### Proof of Theorem¬†[2](https://arxiv.org/html/2510.04555v1#Thmtheorem2 "Theorem 2 (Donsker‚ÄìVaradhan bound and per-state KL conservatism). ‚Ä£ Theorem 2 (KL‚ÄìDRO upper bound and conservatism of per-state KL) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets").

By Assumption¬†[4](https://arxiv.org/html/2510.04555v1#Thmassumption4 "Assumption 4 (CVaR surrogate). ‚Ä£ Theorem 2 (KL‚ÄìDRO upper bound and conservatism of per-state KL) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"), for any t‚àà‚Ñùt\in\mathbb{R},

|  |  |  |
| --- | --- | --- |
|  | CVaRŒ±‚Äã(L)=mint‚Å°{t+1Œ±‚ÄãùîºQ‚Äã[(L‚àít)+]}‚â§t+1Œ±‚ÄãùîºQ‚Äã[œït‚Äã(L)],\mathrm{CVaR}\_{\alpha}(L)\;=\;\min\_{t}\,\Big\{t+\frac{1}{\alpha}\mathbb{E}\_{Q}[(L-t)\_{+}]\Big\}\;\leq\;t+\frac{1}{\alpha}\mathbb{E}\_{Q}[\phi\_{t}(L)], |  |

where œït‚Äã(z)=(z‚àít)+\phi\_{t}(z)=(z-t)\_{+}. Taking the supremum over Q‚ààùí¨œÅ={Q:KL‚Äã(Q‚à•P)‚â§œÅ}Q\in\mathcal{Q}\_{\rho}=\{Q:\mathrm{KL}(Q\|P)\leq\rho\} and applying ([22](https://arxiv.org/html/2510.04555v1#A1.E22 "In A.3 Proof of Theorem 2 (KL‚ÄìDRO upper bound) ‚Ä£ Appendix A Proofs for Section 4 ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")) to œÜ=œït‚Äã(L)\varphi=\phi\_{t}(L) gives

|  |  |  |
| --- | --- | --- |
|  | supQ‚ààùí¨œÅCVaRŒ±‚Äã(L)‚â§mint‚Å°{t+1Œ±‚ÄãsupQ‚ààùí¨œÅùîºQ‚Äã[œït‚Äã(L)]}‚â§mint‚Å°{t+1Œ±‚ÄãŒ∑‚Äã(œÅ+log‚Å°ùîºP‚Äã[eŒ∑‚Äãœït‚Äã(L)])}.\sup\_{Q\in\mathcal{Q}\_{\rho}}\mathrm{CVaR}\_{\alpha}(L)\;\leq\;\min\_{t}\left\{t+\frac{1}{\alpha}\sup\_{Q\in\mathcal{Q}\_{\rho}}\mathbb{E}\_{Q}[\phi\_{t}(L)]\right\}\;\leq\;\min\_{t}\left\{t+\frac{1}{\alpha\eta}\Big(\rho+\log\mathbb{E}\_{P}[e^{\eta\phi\_{t}(L)}]\Big)\right\}. |  |

Under Assumption¬†[5](https://arxiv.org/html/2510.04555v1#Thmassumption5 "Assumption 5 (Path-wise KL radius and occupancy control). ‚Ä£ Theorem 2 (KL‚ÄìDRO upper bound and conservatism of per-state KL) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"), the pathwise radius satisfies œÅ‚â§Cocc‚ÄãŒ≤\rho\leq C\_{\mathrm{occ}}\beta, thus yielding the second claim. This shows that penalizing the per-state KL to keep Œ≤\beta small controls a KL‚ÄìDRO upper bound on the CVaR surrogate.
‚àé

### A.4 Proof of Theorem¬†[3](https://arxiv.org/html/2510.04555v1#Thmtheorem3 "Theorem 3 (Concentration of the temperature-tilted CVaR estimator). ‚Ä£ Theorem 3 (bias/variance and sample complexity of the CVaR estimator) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") (temperature-tilted CVaR concentration)

We treat CVaR^Œ±\widehat{\mathrm{CVaR}}\_{\alpha} as a self-normalized importance sampling (SNIS) ratio estimator over the *tail block* {œÑ‚â§Œ±}\{\tau\leq\alpha\}. Let Ik=ùüè‚Äã{œÑk‚â§Œ±}I\_{k}=\mathbf{1}\{\tau\_{k}\leq\alpha\} and define

|  |  |  |
| --- | --- | --- |
|  | Xk=wk‚ÄãIk‚ÄãL(œÑk),Yk=wk‚ÄãIk,CVaR^Œ±=‚àëk=1KXk‚àëk=1KYk.X\_{k}\;=\;w\_{k}I\_{k}L^{(\tau\_{k})},\qquad Y\_{k}\;=\;w\_{k}I\_{k},\qquad\widehat{\mathrm{CVaR}}\_{\alpha}\;=\;\frac{\sum\_{k=1}^{K}X\_{k}}{\sum\_{k=1}^{K}Y\_{k}}. |  |

We use that |L|‚â§B|L|\leq B and pT‚Äã(œÑ)‚â•pmin>0p\_{T}(\tau)\geq p\_{\min}>0 (Assumption¬†[6](https://arxiv.org/html/2510.04555v1#Thmassumption6 "Assumption 6 (Temperature sampling, bounded importance weights, bounded loss). ‚Ä£ Theorem 3 (bias/variance and sample complexity of the CVaR estimator) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")), hence wk‚â§1/pmin=:Wmaxw\_{k}\leq 1/p\_{\min}=:W\_{\max} and |Xk|‚â§B‚ÄãWmax|X\_{k}|\leq BW\_{\max}, 0‚â§Yk‚â§Wmax0\leq Y\_{k}\leq W\_{\max}.

###### Lemma 3 (Concentration for SNIS ratios with bounded weights).

Let (Xk,Yk)k=1K(X\_{k},Y\_{k})\_{k=1}^{K} be i.i.d. pairs with |Xk|‚â§a|X\_{k}|\leq a, 0‚â§Yk‚â§b0\leq Y\_{k}\leq b, and denote
ŒºX=ùîº‚Äã[X1]\mu\_{X}=\mathbb{E}[X\_{1}], ŒºY=ùîº‚Äã[Y1]>0\mu\_{Y}=\mathbb{E}[Y\_{1}]>0. Then for any Œ¥‚àà(0,1)\delta\in(0,1), with probability at least 1‚àíŒ¥1-\delta,

|  |  |  |
| --- | --- | --- |
|  | |‚àëk=1KXk‚àëk=1KYk‚àíŒºXŒºY|‚â§2‚ÄãaŒºY‚Äãlog‚Å°(4/Œ¥)2‚ÄãK+2‚Äãb‚Äã|ŒºX|ŒºY2‚Äãlog‚Å°(4/Œ¥)2‚ÄãK.\left|\frac{\sum\_{k=1}^{K}X\_{k}}{\sum\_{k=1}^{K}Y\_{k}}-\frac{\mu\_{X}}{\mu\_{Y}}\right|\;\leq\;\frac{2a}{\mu\_{Y}}\sqrt{\frac{\log(4/\delta)}{2K}}\;+\;\frac{2b|\mu\_{X}|}{\mu\_{Y}^{2}}\sqrt{\frac{\log(4/\delta)}{2K}}. |  |

###### Proof.

Write SX:=‚àëk=1KXkS\_{X}:=\sum\_{k=1}^{K}X\_{k}, SY:=‚àëk=1KYkS\_{Y}:=\sum\_{k=1}^{K}Y\_{k}. Note that

|  |  |  |
| --- | --- | --- |
|  | SXSY‚àíŒºXŒºY=SX‚ÄãŒºY‚àíŒºX‚ÄãSYSY‚ÄãŒºY=ŒºY‚Äã(SX‚àíK‚ÄãŒºX)‚àíŒºX‚Äã(SY‚àíK‚ÄãŒºY)SY‚ÄãŒºY.\frac{S\_{X}}{S\_{Y}}-\frac{\mu\_{X}}{\mu\_{Y}}=\frac{S\_{X}\mu\_{Y}-\mu\_{X}S\_{Y}}{S\_{Y}\mu\_{Y}}=\frac{\mu\_{Y}(S\_{X}-K\mu\_{X})-\mu\_{X}(S\_{Y}-K\mu\_{Y})}{S\_{Y}\mu\_{Y}}. |  |

Hence on any event where SY>0S\_{Y}>0,

|  |  |  |  |
| --- | --- | --- | --- |
|  | |SXSY‚àíŒºXŒºY|‚â§ŒºY‚Äã|SX‚àíK‚ÄãŒºX|+|ŒºX|‚Äã|SY‚àíK‚ÄãŒºY|SY‚ÄãŒºY.\left|\frac{S\_{X}}{S\_{Y}}-\frac{\mu\_{X}}{\mu\_{Y}}\right|\leq\frac{\mu\_{Y}\,|S\_{X}-K\mu\_{X}|+|\mu\_{X}|\,|S\_{Y}-K\mu\_{Y}|}{S\_{Y}\,\mu\_{Y}}. |  | (23) |

We control numerator and denominator separately.

Step 1: Hoeffding bounds for SXS\_{X} and SYS\_{Y}.
Since Xk‚àà[‚àía,a]X\_{k}\in[-a,a], Hoeffding‚Äôs inequality yields, for any tX>0t\_{X}>0,

|  |  |  |
| --- | --- | --- |
|  | ‚Ñô‚Äã(|1K‚ÄãSX‚àíŒºX|‚â•tX)‚â§2‚Äãexp‚Å°(‚àí2‚ÄãK‚ÄãtX2(2‚Äãa)2)=2‚Äãexp‚Å°(‚àíK‚ÄãtX22‚Äãa2).\mathbb{P}\!\left(\left|\frac{1}{K}S\_{X}-\mu\_{X}\right|\geq t\_{X}\right)\leq 2\exp\!\left(-\frac{2Kt\_{X}^{2}}{(2a)^{2}}\right)=2\exp\!\left(-\frac{Kt\_{X}^{2}}{2a^{2}}\right). |  |

Similarly, Yk‚àà[0,b]Y\_{k}\in[0,b] implies, for any tY>0t\_{Y}>0,

|  |  |  |
| --- | --- | --- |
|  | ‚Ñô‚Äã(|1K‚ÄãSY‚àíŒºY|‚â•tY)‚â§2‚Äãexp‚Å°(‚àí2‚ÄãK‚ÄãtY2b2).\mathbb{P}\!\left(\left|\frac{1}{K}S\_{Y}-\mu\_{Y}\right|\geq t\_{Y}\right)\leq 2\exp\!\left(-\frac{2Kt\_{Y}^{2}}{b^{2}}\right). |  |

Choose

|  |  |  |
| --- | --- | --- |
|  | tX=a‚Äã2‚Äãlog‚Å°(4/Œ¥)K,tY=b2‚ÄãK‚Äãlog‚Å°(4/Œ¥).t\_{X}\;=\;a\sqrt{\frac{2\log(4/\delta)}{K}},\qquad t\_{Y}\;=\;\frac{b}{\sqrt{2K}}\sqrt{\log(4/\delta)}. |  |

Then, by a union bound, with probability at least 1‚àíŒ¥1-\delta we have simultaneously

|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚Ñ∞:|SX‚àíKŒºX|‚â§KtX,|SY‚àíKŒºY|‚â§KtY.\mathcal{E}:\quad|S\_{X}-K\mu\_{X}|\leq Kt\_{X},\qquad|S\_{Y}-K\mu\_{Y}|\leq Kt\_{Y}. |  | (24) |

Step 2: keep the denominator away from zero.
On ‚Ñ∞\mathcal{E} we have the deterministic lower bound

|  |  |  |
| --- | --- | --- |
|  | SY‚â•K‚Äã(ŒºY‚àítY).S\_{Y}\;\geq\;K(\mu\_{Y}-t\_{Y}). |  |

If tY‚â§ŒºY/2t\_{Y}\leq\mu\_{Y}/2 (which is a mild requirement for KK; see below), then SY‚â•K‚ÄãŒºY/2S\_{Y}\geq K\mu\_{Y}/2. Even without this simplification, from¬†([23](https://arxiv.org/html/2510.04555v1#A1.E23 "In A.4 Proof of Theorem 3 (temperature-tilted CVaR concentration) ‚Ä£ Appendix A Proofs for Section 4 ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")) and¬†([24](https://arxiv.org/html/2510.04555v1#A1.E24 "In A.4 Proof of Theorem 3 (temperature-tilted CVaR concentration) ‚Ä£ Appendix A Proofs for Section 4 ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")) we obtain on ‚Ñ∞\mathcal{E}:

|  |  |  |  |
| --- | --- | --- | --- |
|  | |SXSY‚àíŒºXŒºY|\displaystyle\left|\frac{S\_{X}}{S\_{Y}}-\frac{\mu\_{X}}{\mu\_{Y}}\right| | ‚â§ŒºY‚Äã(K‚ÄãtX)+|ŒºX|‚Äã(K‚ÄãtY)K‚Äã(ŒºY‚àítY)‚ÄãŒºY\displaystyle\leq\frac{\mu\_{Y}\,(Kt\_{X})+|\mu\_{X}|\,(Kt\_{Y})}{K(\mu\_{Y}-t\_{Y})\,\mu\_{Y}} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =tXŒºY‚àítY+|ŒºX|ŒºY‚ãÖtYŒºY‚àítY.\displaystyle=\frac{t\_{X}}{\mu\_{Y}-t\_{Y}}\;+\;\frac{|\mu\_{X}|}{\mu\_{Y}}\cdot\frac{t\_{Y}}{\mu\_{Y}-t\_{Y}}. |  |

If tY‚â§ŒºY/2t\_{Y}\leq\mu\_{Y}/2, then ŒºY‚àítY‚â•ŒºY/2\mu\_{Y}-t\_{Y}\geq\mu\_{Y}/2, hence

|  |  |  |
| --- | --- | --- |
|  | |SXSY‚àíŒºXŒºY|‚â§2‚ÄãtXŒºY+2‚Äã|ŒºX|ŒºY2‚ÄãtY.\left|\frac{S\_{X}}{S\_{Y}}-\frac{\mu\_{X}}{\mu\_{Y}}\right|\;\leq\;\frac{2t\_{X}}{\mu\_{Y}}\;+\;\frac{2|\mu\_{X}|}{\mu\_{Y}^{2}}\,t\_{Y}. |  |

Finally substitute our tX,tYt\_{X},t\_{Y} choices to get

|  |  |  |
| --- | --- | --- |
|  | |SXSY‚àíŒºXŒºY|‚â§2‚ÄãaŒºY‚Äãlog‚Å°(4/Œ¥)2‚ÄãK+2‚Äãb‚Äã|ŒºX|ŒºY2‚Äãlog‚Å°(4/Œ¥)2‚ÄãK,\left|\frac{S\_{X}}{S\_{Y}}-\frac{\mu\_{X}}{\mu\_{Y}}\right|\;\leq\;\frac{2a}{\mu\_{Y}}\sqrt{\frac{\log(4/\delta)}{2K}}\;+\;\frac{2b|\mu\_{X}|}{\mu\_{Y}^{2}}\sqrt{\frac{\log(4/\delta)}{2K}}, |  |

which is the claimed bound.

Remark on the condition tY‚â§ŒºY/2t\_{Y}\leq\mu\_{Y}/2.
Because tY=b2‚ÄãK‚Äãlog‚Å°(4/Œ¥)t\_{Y}=\frac{b}{\sqrt{2K}}\sqrt{\log(4/\delta)}, a sufficient condition is
K‚â•2‚Äãb2ŒºY2‚Äãlog‚Å°4Œ¥K\geq\frac{2b^{2}}{\mu\_{Y}^{2}}\log\!\frac{4}{\delta}.
If KK is smaller, we can keep the more general denominator factor ŒºY‚àítY\mu\_{Y}-t\_{Y} in the bound, i.e.,

|  |  |  |
| --- | --- | --- |
|  | |SXSY‚àíŒºXŒºY|‚â§tXŒºY‚àítY+|ŒºX|ŒºY‚ãÖtYŒºY‚àítY,\left|\frac{S\_{X}}{S\_{Y}}-\frac{\mu\_{X}}{\mu\_{Y}}\right|\leq\frac{t\_{X}}{\mu\_{Y}-t\_{Y}}+\frac{|\mu\_{X}|}{\mu\_{Y}}\cdot\frac{t\_{Y}}{\mu\_{Y}-t\_{Y}}, |  |

which reduces to the displayed result once ŒºY‚àítY‚â•ŒºY/2\mu\_{Y}-t\_{Y}\geq\mu\_{Y}/2.
‚àé

###### Lemma 4 (Tail-mass and divergence control).

Let œÑ‚àºpT\tau\sim p\_{T} on [0,1][0,1] and I=ùüè‚Äã{œÑ‚â§Œ±}I=\mathbf{1}\{\tau\leq\alpha\}, Œ±‚àà(0,1)\alpha\in(0,1). Define the *effective tail mass* Œ±eff:=ùîº‚Äã[I]=‚à´0Œ±pT‚Äã(œÑ)‚ÄãùëëœÑ\alpha\_{\mathrm{eff}}:=\mathbb{E}[I]=\int\_{0}^{\alpha}p\_{T}(\tau)\,d\tau. Then:

1. (i)

   For *unnormalized* importance weights w‚Äã(œÑ)=1/pT‚Äã(œÑ)w(\tau)=1/p\_{T}(\tau), one has

   |  |  |  |
   | --- | --- | --- |
   |  | ŒºY:=ùîº‚Äã[w‚Äã(œÑ)‚ÄãI]=‚à´0Œ±1pT‚Äã(œÑ)‚ÄãpT‚Äã(œÑ)‚ÄãùëëœÑ=Œ±.\mu\_{Y}\;:=\;\mathbb{E}[w(\tau)\,I]=\int\_{0}^{\alpha}\frac{1}{p\_{T}(\tau)}\,p\_{T}(\tau)\,d\tau=\alpha. |  |
2. (ii)

   The deviation of the effective tail mass from the uniform baseline is controlled by total variation:

   |  |  |  |
   | --- | --- | --- |
   |  | |Œ±eff‚àíŒ±|=|‚à´0Œ±(pT‚Äã(œÑ)‚àí1)‚ÄãùëëœÑ|‚â§‚ÄñpT‚àíUnif‚ÄñTV:=12‚Äã‚à´01|pT‚Äã(œÑ)‚àí1|‚ÄãùëëœÑ.|\alpha\_{\mathrm{eff}}-\alpha|=\left|\int\_{0}^{\alpha}\big(p\_{T}(\tau)-1\big)\,d\tau\right|\;\leq\;\|p\_{T}-\mathrm{Unif}\|\_{\mathrm{TV}}:=\frac{1}{2}\int\_{0}^{1}\big|p\_{T}(\tau)-1\big|\,d\tau. |  |

###### Proof.

(i) is a direct computation using the definition of unnormalized importance weights. In particular, the expected *weighted* indicator integrates to the Lebesgue measure of the tail set [0,Œ±][0,\alpha], namely Œ±\alpha.

For (ii), recall the variational characterization of total variation distance between two probability measures with densities pTp\_{T} and qq on [0,1][0,1]:

|  |  |  |
| --- | --- | --- |
|  | ‚ÄñpT‚àíq‚ÄñTV=supA‚äÜ[0,1]|‚à´A(pT‚Äã(œÑ)‚àíq‚Äã(œÑ))‚ÄãùëëœÑ|=12‚Äã‚à´01|pT‚Äã(œÑ)‚àíq‚Äã(œÑ)|‚ÄãùëëœÑ.\|p\_{T}-q\|\_{\mathrm{TV}}=\sup\_{A\subseteq[0,1]}\left|\int\_{A}\big(p\_{T}(\tau)-q(\tau)\big)\,d\tau\right|=\frac{1}{2}\int\_{0}^{1}|p\_{T}(\tau)-q(\tau)|\,d\tau. |  |

Taking q‚â°1q\equiv 1 (the density of the Unif‚Äã[0,1]\mathrm{Unif}[0,1] law) and the particular measurable set A=[0,Œ±]A=[0,\alpha] gives

|  |  |  |
| --- | --- | --- |
|  | |Œ±eff‚àíŒ±|=|‚à´[0,Œ±](pT‚àí1)‚ÄãùëëœÑ|‚â§supA|‚à´A(pT‚àí1)‚ÄãùëëœÑ|=‚ÄñpT‚àíUnif‚ÄñTV.|\alpha\_{\mathrm{eff}}-\alpha|=\left|\int\_{[0,\alpha]}(p\_{T}-1)\,d\tau\right|\leq\sup\_{A}\left|\int\_{A}(p\_{T}-1)\,d\tau\right|=\|p\_{T}-\mathrm{Unif}\|\_{\mathrm{TV}}. |  |

This proves the claim.

Remark (self-normalized weights).
If one uses *self-normalized* importance weights w~k=w‚Äã(œÑk)/‚àëj=1Kw‚Äã(œÑj)\tilde{w}\_{k}=w(\tau\_{k})/\sum\_{j=1}^{K}w(\tau\_{j}) *within a batch*, then ùîº‚Äã[w~‚ÄãI]\mathbb{E}[\tilde{w}\,I] no longer equals Œ±\alpha exactly because of the random denominator. However, for i.i.d. sampling and bounded weights (w‚â§1/pminw\leq 1/p\_{\min}), the deviation is of order O‚Ñô‚Äã(1/K)O\_{\mathbb{P}}(1/\sqrt{K}) by a delta-method expansion of the ratio estimator. In our concentration analysis we upper bound the ratio using the *unnormalized* moments (ŒºX,ŒºY)(\mu\_{X},\mu\_{Y}) and control the self-normalization effect inside the constants (cf. Lemma¬†[3](https://arxiv.org/html/2510.04555v1#Thmlemma3 "Lemma 3 (Concentration for SNIS ratios with bounded weights). ‚Ä£ A.4 Proof of Theorem 3 (temperature-tilted CVaR concentration) ‚Ä£ Appendix A Proofs for Section 4 ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") and Theorem¬†[3](https://arxiv.org/html/2510.04555v1#Thmtheorem3 "Theorem 3 (Concentration of the temperature-tilted CVaR estimator). ‚Ä£ Theorem 3 (bias/variance and sample complexity of the CVaR estimator) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")).
‚àé

###### Proof of Theorem¬†[3](https://arxiv.org/html/2510.04555v1#Thmtheorem3 "Theorem 3 (Concentration of the temperature-tilted CVaR estimator). ‚Ä£ Theorem 3 (bias/variance and sample complexity of the CVaR estimator) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets").

Let Uk:=ùüè‚Äã{œÑk‚â§Œ±}U\_{k}:=\mathbf{1}\{\tau\_{k}\leq\alpha\}, w‚Äã(œÑ)=1/pT‚Äã(œÑ)w(\tau)=1/p\_{T}(\tau), and define

|  |  |  |
| --- | --- | --- |
|  | Xk:=w‚Äã(œÑk)‚ÄãUk‚ÄãL(œÑk),Yk:=w‚Äã(œÑk)‚ÄãUk,SX:=‚àëk=1KXk,SY:=‚àëk=1KYk,X\_{k}:=w(\tau\_{k})\,U\_{k}\,L^{(\tau\_{k})},\qquad Y\_{k}:=w(\tau\_{k})\,U\_{k},\qquad S\_{X}:=\sum\_{k=1}^{K}X\_{k},\quad S\_{Y}:=\sum\_{k=1}^{K}Y\_{k}, |  |

so that the self-normalized importance-sampling (SNIS) estimator can be written as
CVaR^Œ±=SX/SY\widehat{\mathrm{CVaR}}\_{\alpha}=S\_{X}/S\_{Y}. Assume |L|‚â§B|L|\leq B and pT‚Äã(œÑ)‚â•pmin>0p\_{T}(\tau)\geq p\_{\min}>0, hence w‚Äã(œÑ)‚â§Wmax:=1/pminw(\tau)\leq W\_{\max}:=1/p\_{\min}. Set
ŒºX:=ùîº‚Äã[X1]\mu\_{X}:=\mathbb{E}[X\_{1}], ŒºY:=ùîº‚Äã[Y1]\mu\_{Y}:=\mathbb{E}[Y\_{1}]; for *unnormalized* weights one has ŒºY=Œ±\mu\_{Y}=\alpha (Lemma¬†[4](https://arxiv.org/html/2510.04555v1#Thmlemma4 "Lemma 4 (Tail-mass and divergence control). ‚Ä£ A.4 Proof of Theorem 3 (temperature-tilted CVaR concentration) ‚Ä£ Appendix A Proofs for Section 4 ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")(i)). Let Œ±eff:=‚Ñô‚Äã(œÑ‚â§Œ±)=‚à´0Œ±pT‚Äã(œÑ)‚ÄãùëëœÑ\alpha\_{\mathrm{eff}}:=\mathbb{P}(\tau\leq\alpha)=\int\_{0}^{\alpha}p\_{T}(\tau)d\tau.

##### Step 1: Bernstein bounds for the numerator and denominator.

We have |Xk|‚â§B‚ÄãWmax|X\_{k}|\leq BW\_{\max}, 0‚â§Yk‚â§Wmax0\leq Y\_{k}\leq W\_{\max}, and

|  |  |  |
| --- | --- | --- |
|  | œÉX2:=Var‚Äã(X1)‚â§ùîº‚Äã[X12]‚â§B2‚ÄãWmax2‚ÄãŒ±eff,œÉY2:=Var‚Äã(Y1)‚â§ùîº‚Äã[Y12]‚â§Wmax2‚ÄãŒ±eff.\sigma\_{X}^{2}:=\mathrm{Var}(X\_{1})\leq\mathbb{E}[X\_{1}^{2}]\leq B^{2}W\_{\max}^{2}\,\alpha\_{\mathrm{eff}},\qquad\sigma\_{Y}^{2}:=\mathrm{Var}(Y\_{1})\leq\mathbb{E}[Y\_{1}^{2}]\leq W\_{\max}^{2}\,\alpha\_{\mathrm{eff}}. |  |

Bernstein‚Äôs inequality yields, for any tX,tY>0t\_{X},t\_{Y}>0,

|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚Ñô‚Äã(|SXK‚àíŒºX|‚â•tX)\displaystyle\mathbb{P}\!\left(\left|\frac{S\_{X}}{K}-\mu\_{X}\right|\geq t\_{X}\right) | ‚â§2‚Äãexp‚Å°(‚àíK‚ÄãtX22‚ÄãœÉX2+23‚ÄãB‚ÄãWmax‚ÄãtX),\displaystyle\leq 2\exp\!\left(-\frac{Kt\_{X}^{2}}{2\sigma\_{X}^{2}+\frac{2}{3}BW\_{\max}t\_{X}}\right), |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚Ñô‚Äã(|SYK‚àíŒºY|‚â•tY)\displaystyle\mathbb{P}\!\left(\left|\frac{S\_{Y}}{K}-\mu\_{Y}\right|\geq t\_{Y}\right) | ‚â§2‚Äãexp‚Å°(‚àíK‚ÄãtY22‚ÄãœÉY2+23‚ÄãWmax‚ÄãtY).\displaystyle\leq 2\exp\!\left(-\frac{Kt\_{Y}^{2}}{2\sigma\_{Y}^{2}+\frac{2}{3}W\_{\max}t\_{Y}}\right). |  |

Choose absolute constants c1,c2>0c\_{1},c\_{2}>0 and set

|  |  |  |
| --- | --- | --- |
|  | tX:=c1‚ÄãB‚ÄãWmax‚Äã(2‚ÄãŒ±eff‚Äãlog‚Å°(4/Œ¥)K+log‚Å°(4/Œ¥)K),tY:=c2‚ÄãWmax‚Äã(2‚ÄãŒ±eff‚Äãlog‚Å°(4/Œ¥)K+log‚Å°(4/Œ¥)K).t\_{X}:=c\_{1}\,BW\_{\max}\!\left(\sqrt{\frac{2\alpha\_{\mathrm{eff}}\log(4/\delta)}{K}}+\frac{\log(4/\delta)}{K}\right),\quad t\_{Y}:=c\_{2}\,W\_{\max}\!\left(\sqrt{\frac{2\alpha\_{\mathrm{eff}}\log(4/\delta)}{K}}+\frac{\log(4/\delta)}{K}\right). |  |

By a union bound, with probability at least 1‚àíŒ¥1-\delta the event

|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚Ñ∞:|SXK‚àíŒºX|‚â§tX,|SYK‚àíŒºY|‚â§tY\mathcal{E}:\quad\Big|\tfrac{S\_{X}}{K}-\mu\_{X}\Big|\leq t\_{X},\qquad\Big|\tfrac{S\_{Y}}{K}-\mu\_{Y}\Big|\leq t\_{Y} |  | (25) |

holds simultaneously.

##### Step 2: ratio linearization and denominator control.

Using the algebraic identity

|  |  |  |
| --- | --- | --- |
|  | SXSY‚àíŒºXŒºY=ŒºY‚Äã(SX‚àíK‚ÄãŒºX)‚àíŒºX‚Äã(SY‚àíK‚ÄãŒºY)SY‚ÄãŒºY,\frac{S\_{X}}{S\_{Y}}-\frac{\mu\_{X}}{\mu\_{Y}}=\frac{\mu\_{Y}(S\_{X}-K\mu\_{X})-\mu\_{X}(S\_{Y}-K\mu\_{Y})}{S\_{Y}\,\mu\_{Y}}, |  |

we obtain, on {SY>0}\{S\_{Y}>0\},

|  |  |  |  |
| --- | --- | --- | --- |
|  | |SXSY‚àíŒºXŒºY|‚â§ŒºY‚Äã|SX‚àíK‚ÄãŒºX|+|ŒºX|‚Äã|SY‚àíK‚ÄãŒºY|SY‚ÄãŒºY.\left|\frac{S\_{X}}{S\_{Y}}-\frac{\mu\_{X}}{\mu\_{Y}}\right|\leq\frac{\mu\_{Y}|S\_{X}-K\mu\_{X}|+|\mu\_{X}||S\_{Y}-K\mu\_{Y}|}{S\_{Y}\,\mu\_{Y}}. |  | (26) |

On ‚Ñ∞\mathcal{E}, SY‚â•K‚Äã(ŒºY‚àítY)S\_{Y}\geq K(\mu\_{Y}-t\_{Y}). For unnormalized weights, ŒºY=Œ±\mu\_{Y}=\alpha; if KK is such that tY‚â§Œ±/2t\_{Y}\leq\alpha/2 (e.g., K‚â•2‚ÄãWmax2Œ±2‚Äãlog‚Å°(4/Œ¥)K\geq\tfrac{2W\_{\max}^{2}}{\alpha^{2}}\log(4/\delta) suffices), then SY‚â•K‚ÄãŒ±/2S\_{Y}\geq K\alpha/2 and therefore, combining ([25](https://arxiv.org/html/2510.04555v1#A1.E25 "In Step 1: Bernstein bounds for the numerator and denominator. ‚Ä£ A.4 Proof of Theorem 3 (temperature-tilted CVaR concentration) ‚Ä£ Appendix A Proofs for Section 4 ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")) and ([26](https://arxiv.org/html/2510.04555v1#A1.E26 "In Step 2: ratio linearization and denominator control. ‚Ä£ A.4 Proof of Theorem 3 (temperature-tilted CVaR concentration) ‚Ä£ Appendix A Proofs for Section 4 ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")),

|  |  |  |  |
| --- | --- | --- | --- |
|  | |SXSY‚àíŒºXŒºY|‚â§2‚ÄãtXŒ±+2‚Äã|ŒºX|Œ±2‚ÄãtY.\left|\frac{S\_{X}}{S\_{Y}}-\frac{\mu\_{X}}{\mu\_{Y}}\right|\leq\frac{2t\_{X}}{\alpha}+\frac{2|\mu\_{X}|}{\alpha^{2}}t\_{Y}. |  | (27) |

Moreover |ŒºX|=|ùîº‚Äã[L‚ÄãY]|‚â§B‚Äãùîº‚Äã[Y]=B‚ÄãŒ±|\mu\_{X}|=|\mathbb{E}[L\,Y]|\leq B\,\mathbb{E}[Y]=B\alpha. Substituting tX,tYt\_{X},t\_{Y} into ([27](https://arxiv.org/html/2510.04555v1#A1.E27 "In Step 2: ratio linearization and denominator control. ‚Ä£ A.4 Proof of Theorem 3 (temperature-tilted CVaR concentration) ‚Ä£ Appendix A Proofs for Section 4 ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")) yields, on ‚Ñ∞\mathcal{E},

|  |  |  |
| --- | --- | --- |
|  | |CVaR^Œ±‚àíCVaRŒ±|‚â§C1‚ÄãB‚Äã(WmaxŒ±)‚ÄãŒ±eff‚Äãlog‚Å°(4/Œ¥)K+C1‚Ä≤‚ÄãB‚Äã(WmaxŒ±)‚Äãlog‚Å°(4/Œ¥)K,\left|\widehat{\mathrm{CVaR}}\_{\alpha}-\mathrm{CVaR}\_{\alpha}\right|\leq C\_{1}B\left(\frac{W\_{\max}}{\alpha}\right)\sqrt{\frac{\alpha\_{\mathrm{eff}}\log(4/\delta)}{K}}\;+\;C\_{1}^{\prime}B\left(\frac{W\_{\max}}{\alpha}\right)\frac{\log(4/\delta)}{K}, |  |

for absolute constants C1,C1‚Ä≤C\_{1},C\_{1}^{\prime} (absorbing c1,c2c\_{1},c\_{2}). Treating the evaluation level Œ±\alpha as a fixed constant and absorbing WmaxW\_{\max} into C1C\_{1}, the leading term takes the canonical form

|  |  |  |
| --- | --- | --- |
|  | |CVaR^Œ±‚àíCVaRŒ±|‚â§C1‚ÄãB‚Äãlog‚Å°(4/Œ¥)K‚ÄãŒ±eff+C1‚Ä≤‚ÄãB‚Äãlog‚Å°(4/Œ¥)K.\left|\widehat{\mathrm{CVaR}}\_{\alpha}-\mathrm{CVaR}\_{\alpha}\right|\leq C\_{1}B\sqrt{\frac{\log(4/\delta)}{K\,\alpha\_{\mathrm{eff}}}}\;+\;C\_{1}^{\prime}B\frac{\log(4/\delta)}{K}. |  |

##### Step 3: coverage mismatch between Œ±eff\alpha\_{\mathrm{eff}} and Œ±\alpha.

Write CVaRŒ≤=1Œ≤‚Äã‚à´0Œ≤q‚Äã(u)‚Äãùëëu\mathrm{CVaR}\_{\beta}=\frac{1}{\beta}\int\_{0}^{\beta}q(u)\,du, q‚Äã(u):=FL‚àí1‚Äã(u)q(u):=F^{-1}\_{L}(u).
For |q|‚â§B|q|\leq B and any Œ≤,Œ≤‚Ä≤>0\beta,\beta^{\prime}>0,

|  |  |  |
| --- | --- | --- |
|  | |CVaRŒ≤‚àíCVaRŒ≤‚Ä≤|‚â§|1Œ≤‚àí1Œ≤‚Ä≤|‚Äã|‚à´0min‚Å°{Œ≤,Œ≤‚Ä≤}q|+1max‚Å°{Œ≤,Œ≤‚Ä≤}‚Äã|‚à´min‚Å°{Œ≤,Œ≤‚Ä≤}max‚Å°{Œ≤,Œ≤‚Ä≤}q|‚â§C2‚ÄãB‚Äã|Œ≤‚àíŒ≤‚Ä≤|,\left|\mathrm{CVaR}\_{\beta}-\mathrm{CVaR}\_{\beta^{\prime}}\right|\leq\left|\frac{1}{\beta}-\frac{1}{\beta^{\prime}}\right|\!\left|\int\_{0}^{\min\{\beta,\beta^{\prime}\}}q\right|+\frac{1}{\max\{\beta,\beta^{\prime}\}}\left|\int\_{\min\{\beta,\beta^{\prime}\}}^{\max\{\beta,\beta^{\prime}\}}q\right|\leq C\_{2}B\,|\beta-\beta^{\prime}|, |  |

for a constant C2C\_{2} depending only on a lower bound of Œ≤,Œ≤‚Ä≤\beta,\beta^{\prime} (e.g., C2‚â§2/Œ±minC\_{2}\leq 2/\alpha\_{\min} if Œ≤,Œ≤‚Ä≤‚àà[Œ±min,1]\beta,\beta^{\prime}\in[\alpha\_{\min},1]). Hence

|  |  |  |
| --- | --- | --- |
|  | |CVaRŒ±eff‚àíCVaRŒ±|‚â§C2‚ÄãB‚Äã|Œ±eff‚àíŒ±|‚â§C2‚ÄãB‚Äã‚ÄñpT‚àíUnif‚ÄñTV,\big|\mathrm{CVaR}\_{\alpha\_{\mathrm{eff}}}-\mathrm{CVaR}\_{\alpha}\big|\leq C\_{2}B\,|\alpha\_{\mathrm{eff}}-\alpha|\leq C\_{2}B\,\|p\_{T}-\mathrm{Unif}\|\_{\mathrm{TV}}, |  |

where the last inequality uses Lemma¬†[4](https://arxiv.org/html/2510.04555v1#Thmlemma4 "Lemma 4 (Tail-mass and divergence control). ‚Ä£ A.4 Proof of Theorem 3 (temperature-tilted CVaR concentration) ‚Ä£ Appendix A Proofs for Section 4 ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")(ii). This is precisely the *coverage-mismatch* term controlled by the coverage PID.

##### Step 4: union bound and conclusion.

Combining the SNIS ratio deviation (Step¬†2) on event ‚Ñ∞\mathcal{E} (which holds with probability ‚â•1‚àíŒ¥\geq 1-\delta) with the coverage-mismatch bound (Step¬†3), and absorbing the lower-order O‚Äã(log‚Å°K/K)O(\log K/K) term into the leading constant, we obtain that with probability at least 1‚àíŒ¥1-\delta,

|  |  |  |
| --- | --- | --- |
|  | |CVaR^Œ±‚àíCVaRŒ±|‚â§C1‚ÄãB‚Äãlog‚Å°(2/Œ¥)K‚ÄãŒ±eff+C2‚ÄãB‚Äã|Œ±eff‚àíŒ±|\boxed{\;\left|\widehat{\mathrm{CVaR}}\_{\alpha}-\mathrm{CVaR}\_{\alpha}\right|\;\leq\;C\_{1}B\sqrt{\frac{\log(2/\delta)}{K\,\alpha\_{\mathrm{eff}}}}\;+\;C\_{2}B\,|\alpha\_{\mathrm{eff}}-\alpha|\;} |  |

for universal constants C1,C2C\_{1},C\_{2} depending only on pminp\_{\min} (via WmaxW\_{\max}) and the admissible range of Œ±\alpha. This proves Theorem¬†[3](https://arxiv.org/html/2510.04555v1#Thmtheorem3 "Theorem 3 (Concentration of the temperature-tilted CVaR estimator). ‚Ä£ Theorem 3 (bias/variance and sample complexity of the CVaR estimator) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets").
‚àé

### A.5 Proof of Theorem¬†[4](https://arxiv.org/html/2510.04555v1#Thmtheorem4 "Theorem 4 (CVaR trust-region improvement). ‚Ä£ Theorem 4 (trust-region improvement inequality for CVaR with KL limits) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") (CVaR trust-region improvement)

We adapt the policy performance difference argument to the CVaR surrogate in a finite-horizon setting.

###### Lemma 5 (Occupancy shift under per-state KL).

Let {P(‚ãÖ‚à£x,a)}\{P(\cdot\mid x,a)\} be the Markov kernel of the environment on a finite horizon t=0,‚Ä¶,T‚àí1t=0,\dots,T{-}1, and let ptœÄp\_{t}^{\pi}, ptœÄ‚Ä≤p\_{t}^{\pi^{\prime}} be the state marginals at step tt under policies œÄ\pi and œÄ‚Ä≤\pi^{\prime} with the same initial distribution p0p\_{0}. Define the (undiscounted) occupancy measures
dœÄ:=1T‚Äã‚àët=0T‚àí1ptœÄ,dœÄ‚Ä≤:=1T‚Äã‚àët=0T‚àí1ptœÄ‚Ä≤.d\_{\pi}:=\frac{1}{T}\sum\_{t=0}^{T-1}p\_{t}^{\pi},\;d\_{\pi^{\prime}}:=\frac{1}{T}\sum\_{t=0}^{T-1}p\_{t}^{\pi^{\prime}}.
Assume (Assumption¬†[7](https://arxiv.org/html/2510.04555v1#Thmassumption7 "Assumption 7 (Per-state KL constraint and smoothness). ‚Ä£ Theorem 4 (trust-region improvement inequality for CVaR with KL limits) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")) that for all xx,
KL(œÄ‚Ä≤(‚ãÖ‚à£x)‚à•œÄ(‚ãÖ‚à£x))‚â§Œ≤.\mathrm{KL}\!\big(\pi^{\prime}(\cdot\mid x)\,\|\,\pi(\cdot\mid x)\big)\leq\beta.
Then

|  |  |  |
| --- | --- | --- |
|  | ‚ÄñdœÄ‚Ä≤‚àídœÄ‚Äñ1‚â§CT‚ÄãŒ≤,CT=12‚Äã(T‚àí1),\|d\_{\pi^{\prime}}-d\_{\pi}\|\_{1}\;\leq\;C\_{T}\,\sqrt{\beta},\qquad C\_{T}=\sqrt{\tfrac{1}{2}}\,(T-1)\,, |  |

and more generally CT=O‚Äã(T)C\_{T}=O(T) if one replaces the crude stepwise accumulation by a mixing-aware constant (see the remark below).

###### Proof.

Step 1: one-step TV deviation between policies via Pinsker.
By Pinsker‚Äôs inequality, for every state xx,

|  |  |  |
| --- | --- | --- |
|  | TV(œÄ‚Ä≤(‚ãÖ‚à£x),œÄ(‚ãÖ‚à£x)):=12‚à•œÄ‚Ä≤(‚ãÖ‚à£x)‚àíœÄ(‚ãÖ‚à£x)‚à•1‚â§12KL(œÄ‚Ä≤(‚ãÖ‚à£x)‚à•œÄ(‚ãÖ‚à£x))‚â§Œµ,Œµ:=Œ≤2.\mathrm{TV}\big(\pi^{\prime}(\cdot\mid x),\pi(\cdot\mid x)\big):=\tfrac{1}{2}\|\pi^{\prime}(\cdot\mid x)-\pi(\cdot\mid x)\|\_{1}\;\leq\;\sqrt{\tfrac{1}{2}\,\mathrm{KL}\big(\pi^{\prime}(\cdot\mid x)\,\|\,\pi(\cdot\mid x)\big)}\;\leq\;\varepsilon,\quad\varepsilon:=\sqrt{\tfrac{\beta}{2}}. |  |

Step 2: recursion for state-marginal differences.
Let Œît:=‚ÄñptœÄ‚Ä≤‚àíptœÄ‚Äñ1\Delta\_{t}:=\|p\_{t}^{\pi^{\prime}}-p\_{t}^{\pi}\|\_{1}. Since p0œÄ‚Ä≤=p0œÄp\_{0}^{\pi^{\prime}}=p\_{0}^{\pi}, we have Œî0=0\Delta\_{0}=0. The next-step marginals satisfy

|  |  |  |
| --- | --- | --- |
|  | pt+1œÄ‚Äã(x‚Ä≤)=‚àëx‚àëaptœÄ‚Äã(x)‚ÄãœÄ‚Äã(a‚à£x)‚ÄãP‚Äã(x‚Ä≤‚à£x,a),pt+1œÄ‚Ä≤‚Äã(x‚Ä≤)=‚àëx‚àëaptœÄ‚Ä≤‚Äã(x)‚ÄãœÄ‚Ä≤‚Äã(a‚à£x)‚ÄãP‚Äã(x‚Ä≤‚à£x,a).p\_{t+1}^{\pi}(x^{\prime})=\sum\_{x}\sum\_{a}\,p\_{t}^{\pi}(x)\,\pi(a\mid x)\,P(x^{\prime}\mid x,a),\qquad p\_{t+1}^{\pi^{\prime}}(x^{\prime})=\sum\_{x}\sum\_{a}\,p\_{t}^{\pi^{\prime}}(x)\,\pi^{\prime}(a\mid x)\,P(x^{\prime}\mid x,a). |  |

Subtract and take ‚Ñì1\ell\_{1}-norm:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Œît+1\displaystyle\Delta\_{t+1} | =‚Äñpt+1œÄ‚Ä≤‚àípt+1œÄ‚Äñ1\displaystyle=\big\|p\_{t+1}^{\pi^{\prime}}-p\_{t+1}^{\pi}\big\|\_{1} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§‚à•‚àëx(ptœÄ‚Ä≤(x)‚àíptœÄ(x))‚àëaœÄ(a‚à£x)P(‚ãÖ‚à£x,a)‚à•1+‚à•‚àëxptœÄ‚Ä≤(x)‚àëa(œÄ‚Ä≤(a‚à£x)‚àíœÄ(a‚à£x))P(‚ãÖ‚à£x,a)‚à•1\displaystyle\leq\left\|\sum\_{x}\big(p\_{t}^{\pi^{\prime}}(x)-p\_{t}^{\pi}(x)\big)\,\sum\_{a}\pi(a\mid x)\,P(\cdot\mid x,a)\right\|\_{1}+\left\|\sum\_{x}p\_{t}^{\pi^{\prime}}(x)\,\sum\_{a}\big(\pi^{\prime}(a\mid x)-\pi(a\mid x)\big)\,P(\cdot\mid x,a)\right\|\_{1} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =:At+Bt.\displaystyle=:A\_{t}+B\_{t}. |  |

For AtA\_{t}, note that for any probability vector qq and any Markov kernel KK, ‚Äñq‚ÄãK‚Äñ1=‚Äñq‚Äñ1\|qK\|\_{1}=\|q\|\_{1}; hence

|  |  |  |
| --- | --- | --- |
|  | At‚â§‚àëx|ptœÄ‚Ä≤‚Äã(x)‚àíptœÄ‚Äã(x)|=Œît.A\_{t}\leq\sum\_{x}|p\_{t}^{\pi^{\prime}}(x)-p\_{t}^{\pi}(x)|=\Delta\_{t}. |  |

For BtB\_{t}, use convexity of the ‚Ñì1\ell\_{1}-norm and that each P(‚ãÖ‚à£x,a)P(\cdot\mid x,a) is a probability vector:

|  |  |  |
| --- | --- | --- |
|  | Bt‚â§‚àëxptœÄ‚Ä≤(x)‚à•œÄ‚Ä≤(‚ãÖ‚à£x)‚àíœÄ(‚ãÖ‚à£x)‚à•1‚â§2Œµ‚àëxptœÄ‚Ä≤(x)=2Œµ.B\_{t}\leq\sum\_{x}p\_{t}^{\pi^{\prime}}(x)\,\big\|\pi^{\prime}(\cdot\mid x)-\pi(\cdot\mid x)\big\|\_{1}\leq 2\varepsilon\sum\_{x}p\_{t}^{\pi^{\prime}}(x)=2\varepsilon. |  |

Therefore

|  |  |  |
| --- | --- | --- |
|  | Œît+1‚â§Œît+2‚ÄãŒµ,‚áíŒît‚â§‚ÄÑ2‚Äãt‚ÄãŒµby induction (with¬†Œî0=0).\Delta\_{t+1}\;\leq\;\Delta\_{t}+2\varepsilon,\qquad\Rightarrow\qquad\Delta\_{t}\;\leq\;2t\,\varepsilon\quad\text{by induction (with $\Delta\_{0}=0$).} |  |

Step 3: average (undiscounted) occupancy deviation.
By convexity of ‚à•‚ãÖ‚à•1\|\cdot\|\_{1} and the definition dœÄ=1T‚Äã‚àët=0T‚àí1ptœÄd\_{\pi}=\tfrac{1}{T}\sum\_{t=0}^{T-1}p\_{t}^{\pi},

|  |  |  |
| --- | --- | --- |
|  | ‚ÄñdœÄ‚Ä≤‚àídœÄ‚Äñ1=‚Äñ1T‚Äã‚àët=0T‚àí1(ptœÄ‚Ä≤‚àíptœÄ)‚Äñ1‚â§1T‚Äã‚àët=0T‚àí1Œît‚â§1T‚Äã‚àët=0T‚àí12‚Äãt‚ÄãŒµ=Œµ‚Äã(T‚àí1)=12‚Äã(T‚àí1)‚ÄãŒ≤.\|d\_{\pi^{\prime}}-d\_{\pi}\|\_{1}=\left\|\frac{1}{T}\sum\_{t=0}^{T-1}\big(p\_{t}^{\pi^{\prime}}-p\_{t}^{\pi}\big)\right\|\_{1}\;\leq\;\frac{1}{T}\sum\_{t=0}^{T-1}\Delta\_{t}\;\leq\;\frac{1}{T}\sum\_{t=0}^{T-1}2t\,\varepsilon=\varepsilon\,(T-1)=\sqrt{\tfrac{1}{2}}\,(T-1)\sqrt{\beta}. |  |

This proves the stated bound with CT=12‚Äã(T‚àí1)C\_{T}=\sqrt{\tfrac{1}{2}}\,(T-1).
‚àé

*Remark (mixing-aware constants).*
If the Markov chain under œÄ\pi has a Dobrushin (contraction) coefficient Œ∑‚àà[0,1)\eta\in[0,1) so that
‚Äñq‚ÄãKœÄ‚àíq‚Ä≤‚ÄãKœÄ‚Äñ1‚â§Œ∑‚Äã‚Äñq‚àíq‚Ä≤‚Äñ1\|qK^{\pi}-q^{\prime}K^{\pi}\|\_{1}\leq\eta\,\|q-q^{\prime}\|\_{1} for all distributions q,q‚Ä≤q,q^{\prime} (here KœÄK^{\pi} is the one-step kernel marginalized by œÄ\pi), then the recursion improves to
Œît+1‚â§Œ∑‚ÄãŒît+2‚ÄãŒµ\Delta\_{t+1}\leq\eta\,\Delta\_{t}+2\varepsilon and one gets
Œît‚â§2‚ÄãŒµ‚Äã1‚àíŒ∑t1‚àíŒ∑\Delta\_{t}\leq 2\varepsilon\,\frac{1-\eta^{t}}{1-\eta} and
‚ÄñdœÄ‚Ä≤‚àídœÄ‚Äñ1‚â§2‚ÄãŒµT‚Äã(1‚àíŒ∑)‚Äã‚àët=0T‚àí1(1‚àíŒ∑t)‚â§2‚ÄãŒµ1‚àíŒ∑\|d\_{\pi^{\prime}}-d\_{\pi}\|\_{1}\leq\frac{2\varepsilon}{T(1-\eta)}\sum\_{t=0}^{T-1}(1-\eta^{t})\leq\frac{2\varepsilon}{1-\eta},
which removes the linear dependence on TT in the well-mixed regime.

###### Lemma 6 (CVaR performance expansion under importance reweighting).

Fix horizon TT and confidence Œ±‚àà(0,1)\alpha\in(0,1). Let JŒ±‚Äã(œÄ)=CVaRŒ±‚Äã(LT‚Äã(œÄ))J\_{\alpha}(\pi)=\mathrm{CVaR}\_{\alpha}(L\_{T}(\pi)) be defined via the Rockafellar‚ÄìUryasev surrogate:

|  |  |  |
| --- | --- | --- |
|  | JŒ±‚Äã(œÄ)=mint‚àà‚Ñù‚Å°{t+1Œ±‚ÄãùîºœÄ‚Äã[(LT‚àít)+]}.J\_{\alpha}(\pi)\;=\;\min\_{t\in\mathbb{R}}\left\{\,t+\frac{1}{\alpha}\,\mathbb{E}\_{\pi}\big[(L\_{T}-t)\_{+}\big]\right\}. |  |

Let t‚ãÜ‚Äã(œÄ)t^{\star}(\pi) be any minimizer for œÄ\pi, and define the per-step CVaR advantage A~œÄ(Œ±)‚Äã(x,u)\tilde{A}\_{\pi}^{(\alpha)}(x,u) relative to the baseline induced by t‚ãÜ‚Äã(œÄ)t^{\star}(\pi).111Formally, one can define a stepwise surrogate ft(Œ±)‚Äã(x,u)f\_{t}^{(\alpha)}(x,u) whose cumulative expectation equals Œ±‚àí1‚ÄãùîºœÄ‚Äã[(LT‚àít‚ãÜ‚Äã(œÄ))+]‚àíŒ±‚àí1‚ÄãùîºœÄ‚Äã[(LT‚àít‚ãÜ‚Äã(œÄ))+‚à£xt]\alpha^{-1}\mathbb{E}\_{\pi}[(L\_{T}-t^{\star}(\pi))\_{+}]-\alpha^{-1}\mathbb{E}\_{\pi}[(L\_{T}-t^{\star}(\pi))\_{+}\,\mid\,x\_{t}] and set A~œÄ(Œ±)‚Äã(x,u)=ft(Œ±)‚Äã(x,u)\tilde{A}\_{\pi}^{(\alpha)}(x,u)=f\_{t}^{(\alpha)}(x,u) so that ùîºu‚àºœÄ(‚ãÖ‚à£x)‚Äã[A~œÄ(Œ±)‚Äã(x,u)]=0\mathbb{E}\_{u\sim\pi(\cdot\mid x)}[\tilde{A}\_{\pi}^{(\alpha)}(x,u)]=0. This mirrors the construction of GAE but for the CVaR surrogate.
Then, for a small policy perturbation œÄ‚Ä≤=œÄ+Œî\pi^{\prime}=\pi+\Delta,

|  |  |  |
| --- | --- | --- |
|  | JŒ±‚Äã(œÄ‚Ä≤)=JŒ±‚Äã(œÄ)+ùîºx‚àºdœÄ,u‚àºœÄ(‚ãÖ‚à£x)‚Äã[œâ‚Äã(x,u)‚ÄãA~œÄ(Œ±)‚Äã(x,u)]+R‚Äã(Œî),J\_{\alpha}(\pi^{\prime})=J\_{\alpha}(\pi)+\mathbb{E}\_{x\sim d\_{\pi},\,u\sim\pi(\cdot\mid x)}\!\big[\omega(x,u)\,\tilde{A}\_{\pi}^{(\alpha)}(x,u)\big]+R(\Delta), |  |

where œâ‚Äã(x,u):=œÄ‚Ä≤‚Äã(u‚à£x)/œÄ‚Äã(u‚à£x)\omega(x,u):=\pi^{\prime}(u\mid x)/\pi(u\mid x) and the remainder satisfies

|  |  |  |
| --- | --- | --- |
|  | |R‚Äã(Œî)|‚â§C‚Ä≤‚Äã‚ÄñdœÄ‚Ä≤‚àídœÄ‚Äñ1,|R(\Delta)|\;\leq\;C^{\prime}\,\|d\_{\pi^{\prime}}-d\_{\pi}\|\_{1}, |  |

with C‚Ä≤C^{\prime} depending on the Lipschitz/boundedness constants of the surrogate and the one-step loss (Assumption¬†[7](https://arxiv.org/html/2510.04555v1#Thmassumption7 "Assumption 7 (Per-state KL constraint and smoothness). ‚Ä£ Theorem 4 (trust-region improvement inequality for CVaR with KL limits) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")).

###### Proof.

Step 1: fix the threshold and linearize the objective.
By definition of t‚ãÜ‚Äã(œÄ)t^{\star}(\pi),

|  |  |  |
| --- | --- | --- |
|  | JŒ±‚Äã(œÄ)=t‚ãÜ‚Äã(œÄ)+1Œ±‚ÄãùîºœÄ‚Äã[œï‚Äã(LT)],œï‚Äã(z):=(z‚àít‚ãÜ‚Äã(œÄ))+.J\_{\alpha}(\pi)=t^{\star}(\pi)+\frac{1}{\alpha}\,\mathbb{E}\_{\pi}\big[\phi(L\_{T})\big],\quad\phi(z):=(z-t^{\star}(\pi))\_{+}. |  |

For the perturbed policy, we have the upper bound

|  |  |  |
| --- | --- | --- |
|  | JŒ±‚Äã(œÄ‚Ä≤)‚â§t‚ãÜ‚Äã(œÄ)+1Œ±‚ÄãùîºœÄ‚Ä≤‚Äã[œï‚Äã(LT)],J\_{\alpha}(\pi^{\prime})\;\leq\;t^{\star}(\pi)+\frac{1}{\alpha}\,\mathbb{E}\_{\pi^{\prime}}\big[\phi(L\_{T})\big], |  |

since t‚ãÜ‚Äã(œÄ)t^{\star}(\pi) need not be optimal for œÄ‚Ä≤\pi^{\prime}.

Step 2: decompose the change of expectation by steps.
Let gt‚Äã(x,u)g\_{t}(x,u) denote the (measurable) contribution at step tt to the surrogate such that

|  |  |  |
| --- | --- | --- |
|  | 1Œ±‚ÄãùîºœÄ‚Äã[œï‚Äã(LT)]=1T‚Äã‚àët=0T‚àí1ùîºx‚àºptœÄ‚Äãùîºu‚àºœÄ(‚ãÖ‚à£x)‚Äã[gt‚Äã(x,u)],\frac{1}{\alpha}\,\mathbb{E}\_{\pi}\big[\phi(L\_{T})\big]\;=\;\frac{1}{T}\sum\_{t=0}^{T-1}\mathbb{E}\_{x\sim p\_{t}^{\pi}}\,\mathbb{E}\_{u\sim\pi(\cdot\mid x)}[\,g\_{t}(x,u)\,], |  |

and the analogous identity holds for œÄ‚Ä≤\pi^{\prime}. (This is standard: unroll the horizon-TT expectation and assign to each (xt,ut)(x\_{t},u\_{t}) the conditional-increment of the CVaR surrogate; exact form is immaterial for the linearization.)

Hence

|  |  |  |  |
| --- | --- | --- | --- |
|  | 1Œ±‚ÄãùîºœÄ‚Ä≤‚Äã[œï‚Äã(LT)]‚àí1Œ±‚ÄãùîºœÄ‚Äã[œï‚Äã(LT)]\displaystyle\frac{1}{\alpha}\,\mathbb{E}\_{\pi^{\prime}}[\phi(L\_{T})]-\frac{1}{\alpha}\,\mathbb{E}\_{\pi}[\phi(L\_{T})] | =1T‚Äã‚àët=0T‚àí1{ùîºx‚àºptœÄ‚Ä≤‚Äãùîºu‚àºœÄ‚Ä≤(‚ãÖ‚à£x)‚Äã[gt‚Äã(x,u)]‚àíùîºx‚àºptœÄ‚Äãùîºu‚àºœÄ(‚ãÖ‚à£x)‚Äã[gt‚Äã(x,u)]}\displaystyle=\frac{1}{T}\sum\_{t=0}^{T-1}\!\left\{\mathbb{E}\_{x\sim p\_{t}^{\pi^{\prime}}}\mathbb{E}\_{u\sim\pi^{\prime}(\cdot\mid x)}[g\_{t}(x,u)]-\mathbb{E}\_{x\sim p\_{t}^{\pi}}\mathbb{E}\_{u\sim\pi(\cdot\mid x)}[g\_{t}(x,u)]\right\} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =1T‚Äã‚àët=0T‚àí1ùîºx‚àºptœÄ‚Äãùîºu‚àºœÄ(‚ãÖ‚à£x)‚Äã[(œâ‚Äã(x,u)‚àí1)‚Äãgt‚Äã(x,u)]‚èüimportance term\displaystyle=\underbrace{\frac{1}{T}\sum\_{t=0}^{T-1}\mathbb{E}\_{x\sim p\_{t}^{\pi}}\mathbb{E}\_{u\sim\pi(\cdot\mid x)}\!\big[(\omega(x,u)-1)\,g\_{t}(x,u)\big]}\_{\text{importance term}} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +1T‚Äã‚àët=0T‚àí1ùîºx‚àºptœÄ‚Ä≤‚àíptœÄ‚Äãùîºu‚àºœÄ‚Ä≤(‚ãÖ‚à£x)‚Äã[gt‚Äã(x,u)]‚èüstate-distribution remainder.\displaystyle\qquad+\underbrace{\frac{1}{T}\sum\_{t=0}^{T-1}\mathbb{E}\_{x\sim p\_{t}^{\pi^{\prime}}-p\_{t}^{\pi}}\mathbb{E}\_{u\sim\pi^{\prime}(\cdot\mid x)}[g\_{t}(x,u)]}\_{\text{state-distribution remainder}}. |  |

Step 3: identify the CVaR advantage and bound the remainder.
Define the baseline bt‚Äã(x):=ùîºu‚àºœÄ(‚ãÖ‚à£x)‚Äã[gt‚Äã(x,u)]b\_{t}(x):=\mathbb{E}\_{u\sim\pi(\cdot\mid x)}[g\_{t}(x,u)] and set
A~œÄ(Œ±)‚Äã(x,u):=gt‚Äã(x,u)‚àíbt‚Äã(x)\tilde{A}\_{\pi}^{(\alpha)}(x,u):=g\_{t}(x,u)-b\_{t}(x) so that
ùîºu‚àºœÄ(‚ãÖ‚à£x)‚Äã[A~œÄ(Œ±)‚Äã(x,u)]=0\mathbb{E}\_{u\sim\pi(\cdot\mid x)}[\tilde{A}\_{\pi}^{(\alpha)}(x,u)]=0 for all xx.
Then the importance term equals

|  |  |  |
| --- | --- | --- |
|  | 1T‚Äã‚àët=0T‚àí1ùîºx‚àºptœÄ‚Äãùîºu‚àºœÄ(‚ãÖ‚à£x)‚Äã[œâ‚Äã(x,u)‚ÄãA~œÄ(Œ±)‚Äã(x,u)],\frac{1}{T}\sum\_{t=0}^{T-1}\mathbb{E}\_{x\sim p\_{t}^{\pi}}\mathbb{E}\_{u\sim\pi(\cdot\mid x)}\!\big[\omega(x,u)\,\tilde{A}\_{\pi}^{(\alpha)}(x,u)\big], |  |

and by convexity of the ‚Ñì1\ell\_{1}-norm,

|  |  |  |
| --- | --- | --- |
|  | 1T‚Äã‚àët=0T‚àí1ùîºx‚àºptœÄ‚Äã[‚ãÖ]=ùîºx‚àºdœÄ‚Äã[‚ãÖ].\frac{1}{T}\sum\_{t=0}^{T-1}\mathbb{E}\_{x\sim p\_{t}^{\pi}}[\cdot]=\mathbb{E}\_{x\sim d\_{\pi}}[\cdot]. |  |

For the remainder, assume |gt‚Äã(x,u)|‚â§G|g\_{t}(x,u)|\leq G uniformly (this follows from the bounded one-step loss and the Lipschitz/smoothness in Assumption¬†[7](https://arxiv.org/html/2510.04555v1#Thmassumption7 "Assumption 7 (Per-state KL constraint and smoothness). ‚Ä£ Theorem 4 (trust-region improvement inequality for CVaR with KL limits) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")); then

|  |  |  |
| --- | --- | --- |
|  | |1T‚Äã‚àët=0T‚àí1ùîºx‚àºptœÄ‚Ä≤‚àíptœÄ‚Äãùîºu‚àºœÄ‚Ä≤(‚ãÖ‚à£x)‚Äã[gt‚Äã(x,u)]|‚â§GT‚Äã‚àët=0T‚àí1‚ÄñptœÄ‚Ä≤‚àíptœÄ‚Äñ1=G‚Äã‚ÄñdœÄ‚Ä≤‚àídœÄ‚Äñ1.\left|\frac{1}{T}\sum\_{t=0}^{T-1}\mathbb{E}\_{x\sim p\_{t}^{\pi^{\prime}}-p\_{t}^{\pi}}\mathbb{E}\_{u\sim\pi^{\prime}(\cdot\mid x)}[g\_{t}(x,u)]\right|\;\leq\;\frac{G}{T}\sum\_{t=0}^{T-1}\|p\_{t}^{\pi^{\prime}}-p\_{t}^{\pi}\|\_{1}\;=\;G\,\|d\_{\pi^{\prime}}-d\_{\pi}\|\_{1}. |  |

Collecting all pieces,

|  |  |  |
| --- | --- | --- |
|  | JŒ±‚Äã(œÄ‚Ä≤)‚àíJŒ±‚Äã(œÄ)‚â§ùîºx‚àºdœÄ,u‚àºœÄ‚Äã[œâ‚Äã(x,u)‚ÄãA~œÄ(Œ±)‚Äã(x,u)]+G‚Äã‚ÄñdœÄ‚Ä≤‚àídœÄ‚Äñ1,J\_{\alpha}(\pi^{\prime})-J\_{\alpha}(\pi)\;\leq\;\mathbb{E}\_{x\sim d\_{\pi},\,u\sim\pi}\!\big[\omega(x,u)\,\tilde{A}\_{\pi}^{(\alpha)}(x,u)\big]+G\,\|d\_{\pi^{\prime}}-d\_{\pi}\|\_{1}, |  |

and a symmetric argument (or replacing œÄ\pi and œÄ‚Ä≤\pi^{\prime}) yields the same lower bound up to constants, which we summarize as
|R‚Äã(Œî)|‚â§C‚Ä≤‚Äã‚ÄñdœÄ‚Ä≤‚àídœÄ‚Äñ1|R(\Delta)|\leq C^{\prime}\|d\_{\pi^{\prime}}-d\_{\pi}\|\_{1}.
‚àé

###### Proof of Theorem¬†[4](https://arxiv.org/html/2510.04555v1#Thmtheorem4 "Theorem 4 (CVaR trust-region improvement). ‚Ä£ Theorem 4 (trust-region improvement inequality for CVaR with KL limits) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets").

Recall from Lemma¬†[6](https://arxiv.org/html/2510.04555v1#Thmlemma6 "Lemma 6 (CVaR performance expansion under importance reweighting). ‚Ä£ A.5 Proof of Theorem 4 (CVaR trust-region improvement) ‚Ä£ Appendix A Proofs for Section 4 ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") that for any horizon TT and confidence level Œ±‚àà(0,1)\alpha\in(0,1),

|  |  |  |  |
| --- | --- | --- | --- |
|  | JŒ±‚Äã(œÄ‚Ä≤)=JŒ±‚Äã(œÄ)+ùîºx‚àºdœÄ,u‚àºœÄ(‚ãÖ‚à£x)‚Äã[œâ‚Äã(x,u)‚ÄãA~œÄ(Œ±)‚Äã(x,u)]‚èüimportance (first-order) term+R‚Äã(Œî)‚èüstate-distribution remainder,J\_{\alpha}(\pi^{\prime})=J\_{\alpha}(\pi)+\underbrace{\mathbb{E}\_{x\sim d\_{\pi},\,u\sim\pi(\cdot\mid x)}\!\big[\omega(x,u)\,\tilde{A}\_{\pi}^{(\alpha)}(x,u)\big]}\_{\text{importance (first-order) term}}+\underbrace{R(\Delta)}\_{\text{state-distribution remainder}}, |  | (28) |

where œâ‚Äã(x,u)=œÄ‚Ä≤‚Äã(u‚à£x)/œÄ‚Äã(u‚à£x)\omega(x,u)=\pi^{\prime}(u\mid x)/\pi(u\mid x) and |R‚Äã(Œî)|‚â§C‚Ä≤‚Äã‚ÄñdœÄ‚Ä≤‚àídœÄ‚Äñ1|R(\Delta)|\leq C^{\prime}\|d\_{\pi^{\prime}}-d\_{\pi}\|\_{1} for a constant C‚Ä≤C^{\prime} depending on the Lipschitz/boundedness constants of the one-step surrogate (Assumption¬†[7](https://arxiv.org/html/2510.04555v1#Thmassumption7 "Assumption 7 (Per-state KL constraint and smoothness). ‚Ä£ Theorem 4 (trust-region improvement inequality for CVaR with KL limits) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")).
By Lemma¬†[5](https://arxiv.org/html/2510.04555v1#Thmlemma5 "Lemma 5 (Occupancy shift under per-state KL). ‚Ä£ A.5 Proof of Theorem 4 (CVaR trust-region improvement) ‚Ä£ Appendix A Proofs for Section 4 ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"), if KL(œÄ‚Ä≤(‚ãÖ‚à£x)‚à•œÄ(‚ãÖ‚à£x))‚â§Œ≤\mathrm{KL}\!\big(\pi^{\prime}(\cdot\mid x)\|\pi(\cdot\mid x)\big)\leq\beta for all xx, then

|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚ÄñdœÄ‚Ä≤‚àídœÄ‚Äñ1‚â§CT‚ÄãŒ≤,CT=12‚Äã(T‚àí1)(or¬†O‚Äã(T)¬†with mixing-aware refinement).\|d\_{\pi^{\prime}}-d\_{\pi}\|\_{1}\;\leq\;C\_{T}\sqrt{\beta},\qquad C\_{T}=\sqrt{\tfrac{1}{2}}\,(T-1)\quad\text{(or $O(T)$ with mixing-aware refinement).} |  | (29) |

Combining ([28](https://arxiv.org/html/2510.04555v1#A1.E28 "In A.5 Proof of Theorem 4 (CVaR trust-region improvement) ‚Ä£ Appendix A Proofs for Section 4 ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")) and ([29](https://arxiv.org/html/2510.04555v1#A1.E29 "In A.5 Proof of Theorem 4 (CVaR trust-region improvement) ‚Ä£ Appendix A Proofs for Section 4 ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")) yields

|  |  |  |  |
| --- | --- | --- | --- |
|  | JŒ±‚Äã(œÄ‚Ä≤)‚â§JŒ±‚Äã(œÄ)+ùîºx‚àºdœÄ,u‚àºœÄ‚Äã[œâ‚ÄãA~œÄ(Œ±)]+C‚Ä≤‚ÄãCT‚ÄãŒ≤.J\_{\alpha}(\pi^{\prime})\leq J\_{\alpha}(\pi)+\mathbb{E}\_{x\sim d\_{\pi},\,u\sim\pi}\!\big[\omega\,\tilde{A}\_{\pi}^{(\alpha)}\big]+C^{\prime}C\_{T}\sqrt{\beta}. |  | (30) |

It remains to justify that higher-order terms in the Taylor expansion of the CVaR surrogate w.r.t. policy parameters are o‚Äã(‚ÄñŒî‚Äñ)o(\|\Delta\|), and that the first-order term is well-defined under the per-state KL constraint.

##### Control of higher-order terms.

Let Œ∏\theta parametrize œÄŒ∏\pi\_{\theta} and Œ∏‚Ä≤=Œ∏+Œî\theta^{\prime}=\theta+\Delta parametrize œÄ‚Ä≤\pi^{\prime}.
Under Assumption¬†[7](https://arxiv.org/html/2510.04555v1#Thmassumption7 "Assumption 7 (Per-state KL constraint and smoothness). ‚Ä£ Theorem 4 (trust-region improvement inequality for CVaR with KL limits) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") (bounded/smooth one-step loss, bounded horizon), the Rockafellar‚ÄìUryasev surrogate
t+Œ±‚àí1‚ÄãùîºœÄŒ∏‚Äã[(LT‚àít)+]t+\alpha^{-1}\mathbb{E}\_{\pi\_{\theta}}[(L\_{T}-t)\_{+}] is locally twice differentiable in Œ∏\theta (Clarke subdifferential reduces to gradient almost everywhere because the hinge is averaged over bounded losses). Thus a second-order Taylor expansion around Œ∏\theta yields

|  |  |  |
| --- | --- | --- |
|  | JŒ±‚Äã(œÄŒ∏‚Ä≤)=JŒ±‚Äã(œÄŒ∏)+‚ü®‚àáŒ∏JŒ±‚Äã(œÄŒ∏),Œî‚ü©+12‚ÄãŒî‚ä§‚ÄãHŒ∏‚ÄãŒî+r‚Äã(Œî),J\_{\alpha}(\pi\_{\theta^{\prime}})=J\_{\alpha}(\pi\_{\theta})+\langle\nabla\_{\theta}J\_{\alpha}(\pi\_{\theta}),\,\Delta\rangle+\frac{1}{2}\,\Delta^{\top}H\_{\theta}\Delta+r(\Delta), |  |

with ‚ÄñHŒ∏‚Äñ‚â§CH\|H\_{\theta}\|\leq C\_{H} locally and r‚Äã(Œî)=o‚Äã(‚ÄñŒî‚Äñ2)r(\Delta)=o(\|\Delta\|^{2}).
The first-order term equals the importance-weighted advantage in¬†([28](https://arxiv.org/html/2510.04555v1#A1.E28 "In A.5 Proof of Theorem 4 (CVaR trust-region improvement) ‚Ä£ Appendix A Proofs for Section 4 ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")), while the quadratic term is O‚Äã(‚ÄñŒî‚Äñ2)O(\|\Delta\|^{2}).
Since the per-state KL bound Œ≤\beta implies a total-variation bound TV‚Äã(œÄ‚Ä≤,œÄ)‚â§Œ≤/2\mathrm{TV}(\pi^{\prime},\pi)\leq\sqrt{\beta/2} pointwise (Pinsker), standard inequalities (e.g., Bretagnolle‚ÄìHuber) give ‚ÄñŒî‚Äñ=O‚Äã(Œ≤)\|\Delta\|=O(\sqrt{\beta}) in a local chart, hence
Œî‚ä§‚ÄãHŒ∏‚ÄãŒî=O‚Äã(Œ≤)\Delta^{\top}H\_{\theta}\Delta=O(\beta) and is dominated by the O‚Äã(Œ≤)O(\sqrt{\beta}) term in ([30](https://arxiv.org/html/2510.04555v1#A1.E30 "In A.5 Proof of Theorem 4 (CVaR trust-region improvement) ‚Ä£ Appendix A Proofs for Section 4 ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")).
Therefore the collected higher-order contribution is o‚Äã(‚ÄñŒî‚Äñ)=o‚Äã(Œ≤)o(\|\Delta\|)=o(\sqrt{\beta}).

##### Well-posedness of the first-order term.

For each xx, the ratio œâ(‚ãÖ)=œÄ‚Ä≤(‚ãÖ‚à£x)/œÄ(‚ãÖ‚à£x)\omega(\cdot)=\pi^{\prime}(\cdot\mid x)/\pi(\cdot\mid x) satisfies ùîºu‚àºœÄ(‚ãÖ‚à£x)‚Äã[œâ]=1\mathbb{E}\_{u\sim\pi(\cdot\mid x)}[\omega]=1 and

|  |  |  |
| --- | --- | --- |
|  | ùîºu‚àºœÄ(‚ãÖ‚à£x)[|œâ‚àí1|]‚â§‚ÄÑ2TV(œÄ‚Ä≤(‚ãÖ‚à£x),œÄ(‚ãÖ‚à£x))‚â§2‚ÄãŒ≤,\mathbb{E}\_{u\sim\pi(\cdot\mid x)}[|\omega-1|]\;\leq\;2\,\mathrm{TV}\!\big(\pi^{\prime}(\cdot\mid x),\pi(\cdot\mid x)\big)\;\leq\;\sqrt{2\beta}, |  |

by Pinsker. If |A~œÄ(Œ±)‚Äã(x,u)|‚â§G|\tilde{A}\_{\pi}^{(\alpha)}(x,u)|\leq G uniformly (as in Lemma¬†[6](https://arxiv.org/html/2510.04555v1#Thmlemma6 "Lemma 6 (CVaR performance expansion under importance reweighting). ‚Ä£ A.5 Proof of Theorem 4 (CVaR trust-region improvement) ‚Ä£ Appendix A Proofs for Section 4 ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")), then

|  |  |  |
| --- | --- | --- |
|  | |ùîºu‚àºœÄ(‚ãÖ‚à£x)‚Äã[(œâ‚àí1)‚ÄãA~œÄ(Œ±)‚Äã(x,u)]|‚â§G‚Äãùîºu‚àºœÄ‚Äã[|œâ‚àí1|]‚â§G‚Äã2‚ÄãŒ≤.\Big|\mathbb{E}\_{u\sim\pi(\cdot\mid x)}\big[(\omega-1)\tilde{A}\_{\pi}^{(\alpha)}(x,u)\big]\Big|\leq G\,\mathbb{E}\_{u\sim\pi}[|\omega-1|]\leq G\sqrt{2\beta}. |  |

Averaging over x‚àºdœÄx\sim d\_{\pi} preserves the bound, so the first-order term is finite and Lipschitz in Œ≤\sqrt{\beta}.

##### Conclusion.

Set CŒ±:=C‚Ä≤‚ÄãCTC\_{\alpha}:=C^{\prime}C\_{T}; it depends on the horizon TT, the one-step Lipschitz constant, and the envelope GG of the CVaR surrogate, but is independent of Œ≤\beta. Incorporating the o‚Äã(‚ÄñŒî‚Äñ)=o‚Äã(Œ≤)o(\|\Delta\|)=o(\sqrt{\beta}) residual into the right-hand side of ([30](https://arxiv.org/html/2510.04555v1#A1.E30 "In A.5 Proof of Theorem 4 (CVaR trust-region improvement) ‚Ä£ Appendix A Proofs for Section 4 ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")) yields

|  |  |  |
| --- | --- | --- |
|  | JŒ±‚Äã(œÄ‚Ä≤)‚â§JŒ±‚Äã(œÄ)+ùîºx‚àºdœÄ,u‚àºœÄ‚Äã[œâ‚ÄãA~œÄ(Œ±)‚Äã(x,u)]+CŒ±‚ÄãŒ≤+o‚Äã(Œ≤),J\_{\alpha}(\pi^{\prime})\;\leq\;J\_{\alpha}(\pi)+\mathbb{E}\_{x\sim d\_{\pi},\,u\sim\pi}\!\big[\omega\,\tilde{A}\_{\pi}^{(\alpha)}(x,u)\big]+C\_{\alpha}\sqrt{\beta}+o(\sqrt{\beta}), |  |

which is the claimed CVaR trust-region improvement inequality.
‚àé

### A.6 Proof of Theorem¬†[5](https://arxiv.org/html/2510.04555v1#Thmtheorem5 "Theorem 5 (Persistence via NTB shrinkage and rate tightening). ‚Ä£ Theorem 5 (feasibility persistence under tail guards) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") (feasibility persistence)

We show that shrinking the NTB radius and tightening the rate cap can maintain feasibility at the next step under Lipschitz dynamics.

###### Lemma 7 (Lipschitz variation of exposure error).

Under Assumption¬†[8](https://arxiv.org/html/2510.04555v1#Thmassumption8 "Assumption 8 (Lipschitz constraints and affine exposure map). ‚Ä£ Theorem 5 (feasibility persistence under tail guards) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"), suppose e:ùí≥√óùí∞‚Üí‚Ñùqe:\mathcal{X}\times\mathcal{U}\to\mathbb{R}^{q} is given by
e‚Äã(x,u)=A‚Äã(x)‚Äãu‚àíd‚Äã(x)e(x,u)=A(x)u-d(x), where A:ùí≥‚Üí‚Ñùq√ómA:\mathcal{X}\to\mathbb{R}^{q\times m} and d:ùí≥‚Üí‚Ñùqd:\mathcal{X}\to\mathbb{R}^{q}
are locally Lipschitz on a neighborhood ùí©x\mathcal{N}\_{x} of xtx\_{t}. Assume the action set is compact (e.g., by box/rate limits), so that u,ut‚ààùí∞u,u\_{t}\in\mathcal{U} with ‚Äñu‚Äñ‚â§Umax\|u\|\leq U\_{\max} and ‚Äñut‚Äñ‚â§Umax\|u\_{t}\|\leq U\_{\max}.
Then there exist finite constants Lex,Leu>0L\_{e}^{x},L\_{e}^{u}>0 (depending on ùí©x\mathcal{N}\_{x} and ùí∞\mathcal{U}) such that for all (x,u)(x,u) in a neighborhood of (xt,ut)(x\_{t},u\_{t}),

|  |  |  |
| --- | --- | --- |
|  | ‚Äñe‚Äã(x,u)‚àíe‚Äã(xt,ut)‚Äñ‚â§Lex‚Äã‚Äñx‚àíxt‚Äñ+Leu‚Äã‚Äñu‚àíut‚Äñ.\|e(x,u)-e(x\_{t},u\_{t})\|\;\leq\;L\_{e}^{x}\,\|x-x\_{t}\|\;+\;L\_{e}^{u}\,\|u-u\_{t}\|. |  |

###### Proof.

Fix a compact neighborhood ùí¶x‚äÇùí©x\mathcal{K}\_{x}\subset\mathcal{N}\_{x} of xtx\_{t} and a compact set ùí¶u‚äÇùí∞\mathcal{K}\_{u}\subset\mathcal{U} containing utu\_{t} and all feasible uu in a local tube around utu\_{t} (the existence is guaranteed by box/rate constraints). Because AA and dd are locally Lipschitz on ùí©x\mathcal{N}\_{x}, there exist finite moduli LA,Ld>0L\_{A},L\_{d}>0 such that

|  |  |  |
| --- | --- | --- |
|  | ‚ÄñA‚Äã(x)‚àíA‚Äã(y)‚Äñ‚â§LA‚Äã‚Äñx‚àíy‚Äñ,‚Äñd‚Äã(x)‚àíd‚Äã(y)‚Äñ‚â§Ld‚Äã‚Äñx‚àíy‚Äñ,‚àÄx,y‚ààùí¶x,\|A(x)-A(y)\|\leq L\_{A}\|x-y\|,\qquad\|d(x)-d(y)\|\leq L\_{d}\|x-y\|,\qquad\forall x,y\in\mathcal{K}\_{x}, |  |

where ‚à•‚ãÖ‚à•\|\cdot\| on matrices denotes the operator norm induced by the Euclidean vector norm.
Furthermore, since AA is continuous on the compact set ùí¶x\mathcal{K}\_{x}, the bound

|  |  |  |
| --- | --- | --- |
|  | MA:=supx‚ààùí¶x‚ÄñA‚Äã(x)‚Äñ<‚àûM\_{A}\;:=\;\sup\_{x\in\mathcal{K}\_{x}}\|A(x)\|\;<\;\infty |  |

holds. Now decompose

|  |  |  |
| --- | --- | --- |
|  | e‚Äã(x,u)‚àíe‚Äã(xt,ut)=A‚Äã(x)‚Äã(u‚àíut)+(A‚Äã(x)‚àíA‚Äã(xt))‚Äãut‚àí(d‚Äã(x)‚àíd‚Äã(xt)).e(x,u)-e(x\_{t},u\_{t})=A(x)(u-u\_{t})\;+\;\big(A(x)-A(x\_{t})\big)u\_{t}\;-\;\big(d(x)-d(x\_{t})\big). |  |

Taking norms and using the triangle inequality together with the bounds above yields

|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚Äñe‚Äã(x,u)‚àíe‚Äã(xt,ut)‚Äñ\displaystyle\|e(x,u)-e(x\_{t},u\_{t})\| | ‚â§‚ÄñA‚Äã(x)‚Äñ‚Äã‚Äñu‚àíut‚Äñ+‚ÄñA‚Äã(x)‚àíA‚Äã(xt)‚Äñ‚Äã‚Äñut‚Äñ+‚Äñd‚Äã(x)‚àíd‚Äã(xt)‚Äñ\displaystyle\leq\|A(x)\|\,\|u-u\_{t}\|\;+\;\|A(x)-A(x\_{t})\|\,\|u\_{t}\|\;+\;\|d(x)-d(x\_{t})\| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§MA‚Äã‚Äñu‚àíut‚Äñ+LA‚Äã‚Äñx‚àíxt‚Äñ‚Äã‚Äñut‚Äñ+Ld‚Äã‚Äñx‚àíxt‚Äñ\displaystyle\leq M\_{A}\,\|u-u\_{t}\|\;+\;L\_{A}\,\|x-x\_{t}\|\,\|u\_{t}\|\;+\;L\_{d}\,\|x-x\_{t}\| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§MA‚Äã‚Äñu‚àíut‚Äñ+(LA‚ÄãUmax+Ld)‚Äã‚Äñx‚àíxt‚Äñ.\displaystyle\leq M\_{A}\,\|u-u\_{t}\|\;+\;(L\_{A}U\_{\max}+L\_{d})\,\|x-x\_{t}\|. |  |

Hence the claim holds with

|  |  |  |
| --- | --- | --- |
|  | Leu:=MA,Lex:=LA‚ÄãUmax+Ld,L\_{e}^{u}:=M\_{A},\qquad L\_{e}^{x}:=L\_{A}U\_{\max}+L\_{d}, |  |

which depend only on the chosen compact neighborhoods ùí¶x\mathcal{K}\_{x} and ùí¶u\mathcal{K}\_{u} (and thus are uniform locally around (xt,ut)(x\_{t},u\_{t})).
‚àé

###### Lemma 8 (State drift bound).

Under Assumption¬†[1](https://arxiv.org/html/2510.04555v1#Thmassumption1 "Assumption 1 (Local dynamics and mismatch). ‚Ä£ Theorem 1 (robust forward invariance of the safety set) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"), there exists Lx>0L\_{x}>0 such that, for all feasible xtx\_{t} in a compact tube ùí¶x\mathcal{K}\_{x} and actions utu\_{t} in a compact set ùí¶u\mathcal{K}\_{u},

|  |  |  |
| --- | --- | --- |
|  | ‚Äñxt+1‚àíxt‚Äñ‚â§Lx‚Äã(‚Äñut‚Äñ+1)+w¬Ø.\|x\_{t+1}-x\_{t}\|\;\leq\;L\_{x}\big(\|u\_{t}\|+1\big)\;+\;\bar{w}. |  |

###### Proof.

The dynamics are xt+1=f‚Äã(xt)+g‚Äã(xt)‚Äãut+wtx\_{t+1}=f(x\_{t})+g(x\_{t})u\_{t}+w\_{t}, with ‚Äñwt‚Äñ‚â§w¬Ø\|w\_{t}\|\leq\bar{w}. Fix compact sets ùí¶x‚àãxt\mathcal{K}\_{x}\ni x\_{t} and ùí¶u‚àãut\mathcal{K}\_{u}\ni u\_{t} that contain the closed-loop trajectory locally (box/rate constraints and the CBF conditions ensure such compactness). Since ff and gg are locally Lipschitz (hence continuous), the suprema

|  |  |  |
| --- | --- | --- |
|  | Mf:=supx‚ààùí¶x‚Äñf‚Äã(x)‚àíx‚Äñ<‚àû,Mg:=supx‚ààùí¶x‚Äñg‚Äã(x)‚Äñ<‚àûM\_{f}\;:=\;\sup\_{x\in\mathcal{K}\_{x}}\|f(x)-x\|\;<\;\infty,\qquad M\_{g}\;:=\;\sup\_{x\in\mathcal{K}\_{x}}\|g(x)\|\;<\;\infty |  |

are finite. Then

|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚Äñxt+1‚àíxt‚Äñ\displaystyle\|x\_{t+1}-x\_{t}\| | =‚Äñf‚Äã(xt)‚àíxt+g‚Äã(xt)‚Äãut+wt‚Äñ\displaystyle=\|f(x\_{t})-x\_{t}+g(x\_{t})u\_{t}+w\_{t}\| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§‚Äñf‚Äã(xt)‚àíxt‚Äñ+‚Äñg‚Äã(xt)‚Äñ‚Äã‚Äñut‚Äñ+‚Äñwt‚Äñ\displaystyle\leq\|f(x\_{t})-x\_{t}\|+\|g(x\_{t})\|\,\|u\_{t}\|+\|w\_{t}\| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§Mf+Mg‚Äã‚Äñut‚Äñ+w¬Ø.\displaystyle\leq M\_{f}+M\_{g}\,\|u\_{t}\|+\bar{w}. |  |

Finally, let Lx:=max‚Å°{Mf,Mg}L\_{x}:=\max\{M\_{f},M\_{g}\}. Then Mf+Mg‚Äã‚Äñut‚Äñ‚â§Lx‚Äã(1+‚Äñut‚Äñ)M\_{f}+M\_{g}\,\|u\_{t}\|\leq L\_{x}(1+\|u\_{t}\|), and the displayed inequality becomes

|  |  |  |
| --- | --- | --- |
|  | ‚Äñxt+1‚àíxt‚Äñ‚â§Lx‚Äã(‚Äñut‚Äñ+1)+w¬Ø,\|x\_{t+1}-x\_{t}\|\leq L\_{x}(\|u\_{t}\|+1)+\bar{w}, |  |

as claimed. The constant LxL\_{x} depends only on the local compact tube ùí¶x\mathcal{K}\_{x} and thus is uniform along any trajectory that remains in ùí¶x\mathcal{K}\_{x}.
‚àé

##### Remarks.

(i) Lemma¬†[7](https://arxiv.org/html/2510.04555v1#Thmlemma7 "Lemma 7 (Lipschitz variation of exposure error). ‚Ä£ A.6 Proof of Theorem 5 (feasibility persistence) ‚Ä£ Appendix A Proofs for Section 4 ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") makes explicit how the exposure map‚Äôs sensitivity to state and action splits into a term controlled by a uniform operator-norm bound on A‚Äã(‚ãÖ)A(\cdot) and a term proportional to the Lipschitz modulus of A‚Äã(‚ãÖ)A(\cdot) times a local action bound UmaxU\_{\max}, plus the Lipschitz modulus of d‚Äã(‚ãÖ)d(\cdot).
(ii) Lemma¬†[8](https://arxiv.org/html/2510.04555v1#Thmlemma8 "Lemma 8 (State drift bound). ‚Ä£ A.6 Proof of Theorem 5 (feasibility persistence) ‚Ä£ Appendix A Proofs for Section 4 ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") uses only *boundedness on compact sets* for f‚Äã(‚ãÖ)‚àíidf(\cdot)-\mathrm{id} and g‚Äã(‚ãÖ)g(\cdot); stronger bounds (e.g., contraction or Jacobian bounds) would refine LxL\_{x} but are not needed for feasibility persistence in Theorem¬†[5](https://arxiv.org/html/2510.04555v1#Thmtheorem5 "Theorem 5 (Persistence via NTB shrinkage and rate tightening). ‚Ä£ Theorem 5 (feasibility persistence under tail guards) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets").

###### Proof of Theorem¬†[5](https://arxiv.org/html/2510.04555v1#Thmtheorem5 "Theorem 5 (Persistence via NTB shrinkage and rate tightening). ‚Ä£ Theorem 5 (feasibility persistence under tail guards) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets").

Fix a time tt and suppose the QP at tt is solved with Œ∂=0\zeta=0 (strict feasibility) and with the stated margins in Assumption¬†[9](https://arxiv.org/html/2510.04555v1#Thmassumption9 "Assumption 9 (Margins at time ùë°). ‚Ä£ Theorem 5 (feasibility persistence under tail guards) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"):

|  |  |  |
| --- | --- | --- |
|  | (NTB)e‚Äã(xt,ut)‚ä§‚ÄãM‚Äãe‚Äã(xt,ut)‚â§bmax‚àíŒ¥b,(rate)‚Äñut‚àíut‚àí1‚Äñ2‚â§rmax‚àíŒ¥r,\text{(NTB)}\quad e(x\_{t},u\_{t})^{\top}Me(x\_{t},u\_{t})\;\leq\;b\_{\max}-\delta\_{b},\qquad\text{(rate)}\quad\|u\_{t}-u\_{t-1}\|\_{2}\;\leq\;r\_{\max}-\delta\_{r}, |  |

for some Œ¥b,Œ¥r>0\delta\_{b},\delta\_{r}>0. Throughout, we work on a compact tube ùí¶x√óùí¶u\mathcal{K}\_{x}\times\mathcal{K}\_{u} containing (xt,ut)(x\_{t},u\_{t}) (guaranteed by box/rate constraints), so local Lipschitz and boundedness moduli are finite.

We will exhibit a *feasible witness* for the QP at time t+1t{+}1 under the tightened parameters

|  |  |  |
| --- | --- | --- |
|  | bmax‚Ä≤:=Œ∑b‚Äãbmax,rmax‚Ä≤:=Œ∑r‚Äãrmax,Œ∑b,Œ∑r‚àà(0,1),b\_{\max}^{\prime}:=\eta\_{b}\,b\_{\max},\qquad r\_{\max}^{\prime}:=\eta\_{r}\,r\_{\max},\qquad\eta\_{b},\eta\_{r}\in(0,1), |  |

and then appeal to convexity to conclude nonemptiness of the feasible set. Our candidate is the *zero-adjustment action*

|  |  |  |
| --- | --- | --- |
|  | u~t+1:=ut.\tilde{u}\_{t+1}:=u\_{t}. |  |

##### Step 1: box and rate constraints.

Box constraints are unchanged in the theorem statement; since utu\_{t} satisfied them at time tt, the same utu\_{t} satisfies them at time t+1t{+}1. For the tightened rate cap we have

|  |  |  |
| --- | --- | --- |
|  | ‚Äñu~t+1‚àíut‚Äñ2=0‚â§rmax‚Ä≤=Œ∑r‚Äãrmax‚àÄŒ∑r‚àà(0,1),\|\tilde{u}\_{t+1}-u\_{t}\|\_{2}=0\;\leq\;r\_{\max}^{\prime}=\eta\_{r}r\_{\max}\qquad\forall\,\eta\_{r}\in(0,1), |  |

so the rate constraint holds *trivially* for any Œ∑r‚àà(0,1)\eta\_{r}\in(0,1). (No extra condition such as Œ∑r‚â§1‚àíŒ¥r/rmax\eta\_{r}\leq 1-\delta\_{r}/r\_{\max} is needed when u~t+1=ut\tilde{u}\_{t+1}=u\_{t}.)

##### Step 2: NTB constraint under shrinkage.

Let vt:=e‚Äã(xt,ut)v\_{t}:=e(x\_{t},u\_{t}) and vt+1:=e‚Äã(xt+1,ut)v\_{t+1}:=e(x\_{t+1},u\_{t}). By Lemma¬†[7](https://arxiv.org/html/2510.04555v1#Thmlemma7 "Lemma 7 (Lipschitz variation of exposure error). ‚Ä£ A.6 Proof of Theorem 5 (feasibility persistence) ‚Ä£ Appendix A Proofs for Section 4 ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") and Lemma¬†[8](https://arxiv.org/html/2510.04555v1#Thmlemma8 "Lemma 8 (State drift bound). ‚Ä£ A.6 Proof of Theorem 5 (feasibility persistence) ‚Ä£ Appendix A Proofs for Section 4 ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"),

|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚à•vt+1‚àívt‚à•‚â§Lex‚à•xt+1‚àíxt‚à•‚â§Lex(Lx(‚à•ut‚à•+1)+w¬Ø)=:Œîe.\|v\_{t+1}-v\_{t}\|\;\leq\;L\_{e}^{x}\|x\_{t+1}-x\_{t}\|\;\leq\;L\_{e}^{x}\Big(L\_{x}(\|u\_{t}\|+1)+\bar{w}\Big)\;=:\Delta\_{e}. |  | (31) |

Using eigenvalue bounds for quadratic forms,

|  |  |  |
| --- | --- | --- |
|  | vt+1‚ä§‚ÄãM‚Äãvt+1‚â§Œªmax‚Äã(M)‚Äã‚Äñvt+1‚Äñ2‚â§Œªmax‚Äã(M)‚Äã(‚Äñvt‚Äñ+‚Äñvt+1‚àívt‚Äñ)2‚â§Œªmax‚Äã(M)‚Äã(bmax‚àíŒ¥bŒªmin‚Äã(M)+Œîe)2,v\_{t+1}^{\top}Mv\_{t+1}\;\leq\;\lambda\_{\max}(M)\,\|v\_{t+1}\|^{2}\;\leq\;\lambda\_{\max}(M)\,\big(\|v\_{t}\|+\|v\_{t+1}-v\_{t}\|\big)^{2}\;\leq\;\lambda\_{\max}(M)\,\Big(\sqrt{\tfrac{b\_{\max}-\delta\_{b}}{\lambda\_{\min}(M)}}+\Delta\_{e}\Big)^{2}, |  |

where we used ‚Äñvt‚Äñ2‚â§(bmax‚àíŒ¥b)/Œªmin‚Äã(M)\|v\_{t}\|^{2}\leq(b\_{\max}-\delta\_{b})/\lambda\_{\min}(M), since vt‚ä§‚ÄãM‚Äãvt‚â§bmax‚àíŒ¥bv\_{t}^{\top}Mv\_{t}\leq b\_{\max}-\delta\_{b}. Therefore a sufficient condition for the tightened NTB,

|  |  |  |
| --- | --- | --- |
|  | vt+1‚ä§‚ÄãM‚Äãvt+1‚â§bmax‚Ä≤=Œ∑b‚Äãbmax,v\_{t+1}^{\top}Mv\_{t+1}\;\leq\;b\_{\max}^{\prime}=\eta\_{b}b\_{\max}, |  |

is

|  |  |  |  |
| --- | --- | --- | --- |
|  | Œ∑b‚â•Œ∑b‚ãÜ:=Œªmax‚Äã(M)bmax‚Äã(bmax‚àíŒ¥bŒªmin‚Äã(M)+Œîe)2.\eta\_{b}\;\geq\;\eta\_{b}^{\star}\;:=\;\frac{\lambda\_{\max}(M)}{b\_{\max}}\,\Big(\sqrt{\tfrac{b\_{\max}-\delta\_{b}}{\lambda\_{\min}(M)}}+\Delta\_{e}\Big)^{2}. |  | (32) |

This is feasible (i.e., Œ∑b‚ãÜ<1\eta\_{b}^{\star}<1) provided

|  |  |  |  |
| --- | --- | --- | --- |
|  | Œîe<bmaxŒªmax‚Äã(M)‚àíbmax‚àíŒ¥bŒªmin‚Äã(M).\Delta\_{e}\;<\;\sqrt{\frac{b\_{\max}}{\lambda\_{\max}(M)}}\;-\;\sqrt{\frac{b\_{\max}-\delta\_{b}}{\lambda\_{\min}(M)}}. |  | (33) |

The right-hand side is positive as soon as Œ¥b>0\delta\_{b}>0. Inequality ([33](https://arxiv.org/html/2510.04555v1#A1.E33 "In Step 2: NTB constraint under shrinkage. ‚Ä£ A.6 Proof of Theorem 5 (feasibility persistence) ‚Ä£ Appendix A Proofs for Section 4 ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")) holds by choosing the local tube small enough (reducing the bound on ‚Äñut‚Äñ\|u\_{t}\| and using the given w¬Ø\bar{w}) so that the state drift bound in ([31](https://arxiv.org/html/2510.04555v1#A1.E31 "In Step 2: NTB constraint under shrinkage. ‚Ä£ A.6 Proof of Theorem 5 (feasibility persistence) ‚Ä£ Appendix A Proofs for Section 4 ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")) is sufficiently small. Under ([33](https://arxiv.org/html/2510.04555v1#A1.E33 "In Step 2: NTB constraint under shrinkage. ‚Ä£ A.6 Proof of Theorem 5 (feasibility persistence) ‚Ä£ Appendix A Proofs for Section 4 ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")), pick any Œ∑b‚àà(Œ∑b‚ãÜ,1)\eta\_{b}\in(\eta\_{b}^{\star},1) to satisfy the tightened NTB.

##### Step 3: CBF constraints.

Let us denote the discrete-time CBF map at state xx by

|  |  |  |
| --- | --- | --- |
|  | ùíûi‚Äã(x,u):=hi‚Äã(f‚Äã(x)+g‚Äã(x)‚Äãu)‚àí(1‚àíŒ∫i‚ÄãŒî‚Äãt)‚Äãhi‚Äã(x).\mathcal{C}\_{i}(x,u):=h\_{i}(f(x)+g(x)u)-(1-\kappa\_{i}\Delta t)\,h\_{i}(x). |  |

At time tt, strict feasibility with Œ∂=0\zeta=0 and the margin construction (Assumption¬†[2](https://arxiv.org/html/2510.04555v1#Thmassumption2 "Assumption 2 (Discrete-time CBF constraint with margin). ‚Ä£ Theorem 1 (robust forward invariance of the safety set) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") in Theorem¬†[1](https://arxiv.org/html/2510.04555v1#Thmtheorem1 "Theorem 1 (Robust forward invariance). ‚Ä£ Theorem 1 (robust forward invariance of the safety set) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")) give

|  |  |  |
| --- | --- | --- |
|  | ùíûi‚Äã(xt,ut)‚â•ŒµiwithŒµi‚â•Li‚Äãw¬Ø>‚ÄÑ0.\mathcal{C}\_{i}(x\_{t},u\_{t})\;\geq\;\varepsilon\_{i}\quad\text{with}\quad\varepsilon\_{i}\geq L\_{i}\bar{w}\;>\;0. |  |

By local Lipschitzness of hih\_{i}, ff, and gg on ùí¶x√óùí¶u\mathcal{K}\_{x}\times\mathcal{K}\_{u}, there exists a constant Lùíû,i>0L\_{\mathcal{C},i}>0 such that

|  |  |  |
| --- | --- | --- |
|  | |ùíûi‚Äã(x‚Ä≤,u)‚àíùíûi‚Äã(x,u)|‚â§Lùíû,i‚Äã‚Äñx‚Ä≤‚àíx‚Äñ‚àÄ(x‚Ä≤,x,u)‚ààùí¶x√óùí¶x√óùí¶u.\big|\mathcal{C}\_{i}(x^{\prime},u)-\mathcal{C}\_{i}(x,u)\big|\;\leq\;L\_{\mathcal{C},i}\,\|x^{\prime}-x\|\qquad\forall(x^{\prime},x,u)\in\mathcal{K}\_{x}\times\mathcal{K}\_{x}\times\mathcal{K}\_{u}. |  |

Therefore, using Lemma¬†[8](https://arxiv.org/html/2510.04555v1#Thmlemma8 "Lemma 8 (State drift bound). ‚Ä£ A.6 Proof of Theorem 5 (feasibility persistence) ‚Ä£ Appendix A Proofs for Section 4 ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"),

|  |  |  |
| --- | --- | --- |
|  | ùíûi‚Äã(xt+1,ut)‚â•ùíûi‚Äã(xt,ut)‚àíLùíû,i‚Äã‚Äñxt+1‚àíxt‚Äñ‚â•Œµi‚àíLùíû,i‚Äã(Lx‚Äã(‚Äñut‚Äñ+1)+w¬Ø).\mathcal{C}\_{i}(x\_{t+1},u\_{t})\;\geq\;\mathcal{C}\_{i}(x\_{t},u\_{t})-L\_{\mathcal{C},i}\|x\_{t+1}-x\_{t}\|\;\geq\;\varepsilon\_{i}-L\_{\mathcal{C},i}\big(L\_{x}(\|u\_{t}\|+1)+\bar{w}\big). |  |

Define the *CBF residual*

|  |  |  |
| --- | --- | --- |
|  | œÅi:=Œµi‚àíLùíû,i‚Äã(Lx‚Äã(‚Äñut‚Äñ+1)+w¬Ø).\rho\_{i}\;:=\;\varepsilon\_{i}-L\_{\mathcal{C},i}\big(L\_{x}(\|u\_{t}\|+1)+\bar{w}\big). |  |

By shrinking the local tube (equivalently, bounding ‚Äñut‚Äñ\|u\_{t}\| more tightly) we can ensure œÅi>0\rho\_{i}>0 for all ii, hence

|  |  |  |
| --- | --- | --- |
|  | ùíûi‚Äã(xt+1,ut)‚â•œÅi>‚ÄÑ0,\mathcal{C}\_{i}(x\_{t+1},u\_{t})\;\geq\;\rho\_{i}\;>\;0, |  |

i.e., the CBF inequalities remain strictly feasible at t+1t{+}1 for the witness u~t+1=ut\tilde{u}\_{t+1}=u\_{t}.

##### Step 4: sign-consistency gate.

Assume the gate map gcons‚Äã(x,u)g\_{\mathrm{cons}}(x,u) is locally Lipschitz in xx uniformly in u‚ààùí¶uu\in\mathcal{K}\_{u} with modulus LgL\_{g}, and that at time tt the margin

|  |  |  |
| --- | --- | --- |
|  | gcons‚Äã(xt,ut)‚â•Œ¥g>‚ÄÑ0g\_{\mathrm{cons}}(x\_{t},u\_{t})\;\geq\;\delta\_{g}\;>\;0 |  |

holds (this is typical since the solver returns positive gate scores when the constraint is inactive). Then

|  |  |  |
| --- | --- | --- |
|  | gcons‚Äã(xt+1,ut)‚â•gcons‚Äã(xt,ut)‚àíLg‚Äã‚Äñxt+1‚àíxt‚Äñ‚â•Œ¥g‚àíLg‚Äã(Lx‚Äã(‚Äñut‚Äñ+1)+w¬Ø),g\_{\mathrm{cons}}(x\_{t+1},u\_{t})\;\geq\;g\_{\mathrm{cons}}(x\_{t},u\_{t})-L\_{g}\,\|x\_{t+1}-x\_{t}\|\;\geq\;\delta\_{g}-L\_{g}\big(L\_{x}(\|u\_{t}\|+1)+\bar{w}\big), |  |

which remains nonnegative once the local tube is chosen so that Œ¥g>Lg‚Äã(Lx‚Äã(‚Äñut‚Äñ+1)+w¬Ø)\delta\_{g}>L\_{g}(L\_{x}(\|u\_{t}\|+1)+\bar{w}).

##### Step 5: conclusion.

Collecting the four constraint families:
(i) box and (tightened) rate constraints hold for u~t+1=ut\tilde{u}\_{t+1}=u\_{t};
(ii) the tightened NTB holds provided ([32](https://arxiv.org/html/2510.04555v1#A1.E32 "In Step 2: NTB constraint under shrinkage. ‚Ä£ A.6 Proof of Theorem 5 (feasibility persistence) ‚Ä£ Appendix A Proofs for Section 4 ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"))‚Äì([33](https://arxiv.org/html/2510.04555v1#A1.E33 "In Step 2: NTB constraint under shrinkage. ‚Ä£ A.6 Proof of Theorem 5 (feasibility persistence) ‚Ä£ Appendix A Proofs for Section 4 ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")) and the choice Œ∑b‚àà(Œ∑b‚ãÜ,1)\eta\_{b}\in(\eta\_{b}^{\star},1);
(iii) the CBF inequalities remain strictly feasible thanks to the residual œÅi>0\rho\_{i}>0;
(iv) the gate remains nonnegative by continuity and the margin Œ¥g>0\delta\_{g}>0.

Therefore u~t+1\tilde{u}\_{t+1} is a feasible point of the *tightened* constraint set at state xt+1x\_{t+1} with Œ∂=0\zeta=0. Since all constraints are convex in uu (affine CBF surrogates, ellipsoidal NTB, box/rate, and a convex gate), the feasible set at t+1t{+}1 is nonempty and closed, and thus the QP at t+1t{+}1 is feasible with Œ∂=0\zeta=0. This establishes *feasibility persistence* under the stated tail guards.
‚àé

### A.7 Proof of Proposition¬†[2](https://arxiv.org/html/2510.04555v1#Thmproposition2 "Proposition 2 (Gate-induced lower bound). ‚Ä£ Proposition 2 (negative-advantage suppression by sign-consistency) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") (negative-advantage suppression)

###### Lemma 9 (Alignment inequality).

Let v,g‚àà‚Ñùmv,g\in\mathbb{R}^{m} be unit vectors with angle ‚à†‚Äã(v,g)=Œ∏‚àà[0,œÄ]\angle(v,g)=\theta\in[0,\pi], i.e., ‚ü®v,g‚ü©=cos‚Å°Œ∏\langle v,g\rangle=\cos\theta. Then for any u‚àà‚Ñùmu\in\mathbb{R}^{m},

|  |  |  |
| --- | --- | --- |
|  | ‚ü®u,g‚ü©‚â•‚ü®u,v‚ü©‚Äãcos‚Å°Œ∏‚àí‚Äñu‚Äñ‚Äãsin‚Å°Œ∏.\langle u,g\rangle\;\geq\;\langle u,v\rangle\cos\theta\;-\;\|u\|\,\sin\theta. |  |

Moreover, the bound is tight: equality holds whenever uu lies in the plane spanned by {v,g}\{v,g\} and has a component orthogonal to vv aligned with the orthogonal component of gg.

###### Proof.

Complete vv to an orthonormal basis {v,v‚üÇ(1),‚Ä¶,v‚üÇ(m‚àí1)}\{v,v\_{\perp}^{(1)},\dots,v\_{\perp}^{(m-1)}\} with v‚üÇ(1)v\_{\perp}^{(1)} chosen in the plane span‚Äã{v,g}\mathrm{span}\{v,g\} such that

|  |  |  |
| --- | --- | --- |
|  | g=(cos‚Å°Œ∏)‚Äãv+(sin‚Å°Œ∏)‚Äãv‚üÇ(1).g=(\cos\theta)\,v+(\sin\theta)\,v\_{\perp}^{(1)}. |  |

Decompose u=Œ±‚Äãv+‚àëi=1m‚àí1Œ≤i‚Äãv‚üÇ(i)u=\alpha v+\sum\_{i=1}^{m-1}\beta\_{i}v\_{\perp}^{(i)}. Then

|  |  |  |
| --- | --- | --- |
|  | ‚ü®u,g‚ü©=Œ±‚Äãcos‚Å°Œ∏+Œ≤1‚Äãsin‚Å°Œ∏.\langle u,g\rangle\;=\;\alpha\cos\theta\;+\;\beta\_{1}\sin\theta. |  |

Using |Œ≤1|‚â§‚àëi=1m‚àí1Œ≤i2‚â§‚Äñu‚Äñ|\beta\_{1}|\leq\sqrt{\sum\_{i=1}^{m-1}\beta\_{i}^{2}}\leq\|u\| and Œ±=‚ü®u,v‚ü©\alpha=\langle u,v\rangle, we obtain

|  |  |  |
| --- | --- | --- |
|  | ‚ü®u,g‚ü©‚â•‚ü®u,v‚ü©‚Äãcos‚Å°Œ∏‚àí‚Äñu‚Äñ‚Äãsin‚Å°Œ∏.\langle u,g\rangle\;\geq\;\langle u,v\rangle\cos\theta\;-\;\|u\|\,\sin\theta. |  |

Tightness: if uu lies in span‚Äã{v,v‚üÇ(1)}\mathrm{span}\{v,v\_{\perp}^{(1)}\} and Œ≤1=‚àí‚Äñu‚Äñ\beta\_{1}=-\|u\| (i.e., the entire orthogonal component of uu is opposite to v‚üÇ(1)v\_{\perp}^{(1)}), the inequality is attained with equality.
‚àé

###### Proof of Proposition¬†[2](https://arxiv.org/html/2510.04555v1#Thmproposition2 "Proposition 2 (Gate-induced lower bound). ‚Ä£ Proposition 2 (negative-advantage suppression by sign-consistency) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets").

Fix a state xx and write, for brevity, v:=v‚Äã(x)v:=v(x) (the consensus direction of the ensemble signals) and gŒ±:=‚àáA~œÄ(Œ±)‚Äã(x)/‚Äñ‚àáA~œÄ(Œ±)‚Äã(x)‚Äñg\_{\alpha}:=\nabla\tilde{A}\_{\pi}^{(\alpha)}(x)/\|\nabla\tilde{A}\_{\pi}^{(\alpha)}(x)\| (the unit CVaR-advantage direction). We proceed in three steps.

##### Step 1: from gate feasibility to a lower bound along vv.

By Assumption¬†[10](https://arxiv.org/html/2510.04555v1#Thmassumption10 "Assumption 10 (Gate alignment and mismatch). ‚Ä£ Proposition 2 (negative-advantage suppression by sign-consistency) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"), for all j‚àà{1,‚Ä¶,J}j\in\{1,\ldots,J\} one has

|  |  |  |
| --- | --- | --- |
|  | ‚Äñ‚àá^‚ÄãŒ†(j)‚Äã(x)‚àív‚Äñ‚â§œµg.\|\widehat{\nabla}\Pi^{(j)}(x)-v\|\;\leq\;\epsilon\_{g}. |  |

For any feasible uu with gcons‚Äã(x,u)‚â•0g\_{\mathrm{cons}}(x,u)\geq 0, we have

|  |  |  |
| --- | --- | --- |
|  | minj‚Å°‚ü®u,‚àá^‚ÄãŒ†(j)‚Äã(x)‚ü©‚â•Œ¥adv.\min\_{j}\langle u,\widehat{\nabla}\Pi^{(j)}(x)\rangle\;\geq\;\delta\_{\mathrm{adv}}. |  |

For each jj, write

|  |  |  |
| --- | --- | --- |
|  | ‚ü®u,v‚ü©=‚ü®u,‚àá^‚ÄãŒ†(j)‚Äã(x)‚ü©+‚ü®u,v‚àí‚àá^‚ÄãŒ†(j)‚Äã(x)‚ü©‚â•Œ¥adv‚àí‚Äñu‚Äñ‚Äã‚Äñv‚àí‚àá^‚ÄãŒ†(j)‚Äã(x)‚Äñ‚â•Œ¥adv‚àí‚Äñu‚Äñ‚Äãœµg.\langle u,v\rangle\;=\;\langle u,\widehat{\nabla}\Pi^{(j)}(x)\rangle\;+\;\langle u,\,v-\widehat{\nabla}\Pi^{(j)}(x)\rangle\;\geq\;\delta\_{\mathrm{adv}}\;-\;\|u\|\,\|v-\widehat{\nabla}\Pi^{(j)}(x)\|\;\geq\;\delta\_{\mathrm{adv}}\;-\;\|u\|\,\epsilon\_{g}. |  |

Taking the minimum over jj preserves the inequality, hence

|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚ü®u,v‚ü©‚â•Œ¥adv‚àí‚Äñu‚Äñ‚Äãœµg.\langle u,v\rangle\;\geq\;\delta\_{\mathrm{adv}}\;-\;\|u\|\,\epsilon\_{g}. |  | (34) |

##### Step 2: project the lower bound from vv onto the CVaR-advantage direction gŒ±g\_{\alpha}.

Assumption¬†[10](https://arxiv.org/html/2510.04555v1#Thmassumption10 "Assumption 10 (Gate alignment and mismatch). ‚Ä£ Proposition 2 (negative-advantage suppression by sign-consistency) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") further states that the angle between vv and gŒ±g\_{\alpha} is bounded: ‚à†‚Äã(v,gŒ±)‚â§œµŒ∏\angle(v,g\_{\alpha})\leq\epsilon\_{\theta}. Applying Lemma¬†[9](https://arxiv.org/html/2510.04555v1#Thmlemma9 "Lemma 9 (Alignment inequality). ‚Ä£ A.7 Proof of Proposition 2 (negative-advantage suppression) ‚Ä£ Appendix A Proofs for Section 4 ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") with g=gŒ±g=g\_{\alpha} yields

|  |  |  |
| --- | --- | --- |
|  | ‚ü®u,gŒ±‚ü©‚â•‚ü®u,v‚ü©‚Äãcos‚Å°œµŒ∏‚àí‚Äñu‚Äñ‚Äãsin‚Å°œµŒ∏.\langle u,g\_{\alpha}\rangle\;\geq\;\langle u,v\rangle\cos\epsilon\_{\theta}\;-\;\|u\|\sin\epsilon\_{\theta}. |  |

Combining with ([34](https://arxiv.org/html/2510.04555v1#A1.E34 "In Step 1: from gate feasibility to a lower bound along ùë£. ‚Ä£ A.7 Proof of Proposition 2 (negative-advantage suppression) ‚Ä£ Appendix A Proofs for Section 4 ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")),

|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚ü®u,gŒ±‚ü©‚â•(Œ¥adv‚àí‚Äñu‚Äñ‚Äãœµg)‚Äãcos‚Å°œµŒ∏‚àí‚Äñu‚Äñ‚Äãsin‚Å°œµŒ∏.\langle u,g\_{\alpha}\rangle\;\geq\;\big(\delta\_{\mathrm{adv}}-\|u\|\,\epsilon\_{g}\big)\cos\epsilon\_{\theta}\;-\;\|u\|\sin\epsilon\_{\theta}. |  | (35) |

##### Step 3: first-order lower bound for the CVaR-advantage and remainder control.

Let GŒ±‚Äã(x):=‚Äñ‚àáA~œÄ(Œ±)‚Äã(x)‚ÄñG\_{\alpha}(x):=\|\nabla\tilde{A}\_{\pi}^{(\alpha)}(x)\|. A first-order Taylor expansion of the scalar map u‚Ü¶A~œÄ(Œ±)‚Äã(x,u)u\mapsto\tilde{A}\_{\pi}^{(\alpha)}(x,u) at u=0u=0 gives

|  |  |  |
| --- | --- | --- |
|  | A~œÄ(Œ±)‚Äã(x,u)=GŒ±‚Äã(x)‚Äã‚ü®u,gŒ±‚ü©+RŒ±‚Äã(x,u),\tilde{A}\_{\pi}^{(\alpha)}(x,u)\;=\;G\_{\alpha}(x)\,\langle u,g\_{\alpha}\rangle\;+\;R\_{\alpha}(x,u), |  |

where RŒ±‚Äã(x,u)R\_{\alpha}(x,u) is the Taylor remainder. If ‚àáA~œÄ(Œ±)‚Äã(x,‚ãÖ)\nabla\tilde{A}\_{\pi}^{(\alpha)}(x,\cdot) is LŒ±L\_{\alpha}-Lipschitz in uu locally (a standard assumption inherited from the bounded/smooth one-step losses and the bounded horizon), then

|  |  |  |
| --- | --- | --- |
|  | |RŒ±‚Äã(x,u)|‚â§LŒ±2‚Äã‚Äñu‚Äñ2for¬†‚Äñu‚Äñ¬†in a local tube.|R\_{\alpha}(x,u)|\;\leq\;\tfrac{L\_{\alpha}}{2}\,\|u\|^{2}\quad\text{for $\|u\|$ in a local tube.} |  |

Combining with ([35](https://arxiv.org/html/2510.04555v1#A1.E35 "In Step 2: project the lower bound from ùë£ onto the CVaR-advantage direction ùëî_ùõº. ‚Ä£ A.7 Proof of Proposition 2 (negative-advantage suppression) ‚Ä£ Appendix A Proofs for Section 4 ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")) yields the pointwise lower bound

|  |  |  |
| --- | --- | --- |
|  | A~œÄ(Œ±)‚Äã(x,u)‚â•GŒ±‚Äã(x)‚Äã[(Œ¥adv‚àí‚Äñu‚Äñ‚Äãœµg)‚Äãcos‚Å°œµŒ∏‚àí‚Äñu‚Äñ‚Äãsin‚Å°œµŒ∏]‚àíLŒ±2‚Äã‚Äñu‚Äñ2.\tilde{A}\_{\pi}^{(\alpha)}(x,u)\;\geq\;G\_{\alpha}(x)\Big[\big(\delta\_{\mathrm{adv}}-\|u\|\,\epsilon\_{g}\big)\cos\epsilon\_{\theta}-\|u\|\sin\epsilon\_{\theta}\Big]\;-\;\tfrac{L\_{\alpha}}{2}\,\|u\|^{2}. |  |

Taking conditional expectation given xx (which leaves the deterministic right-hand side unchanged if uu is deterministic given xx, or replaces ‚Äñu‚Äñ\|u\| by ùîº‚Äã[‚Äñu‚Äñ‚à£x]\mathbb{E}[\|u\|\mid x] in the stochastic case and then uses Jensen/triangle inequalities) gives

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Äã[A~œÄ(Œ±)‚Äã(x,u)‚à£x]‚â•‚àíŒæ‚Äã(œµg,œµŒ∏,Œ¥adv,‚Äñu‚Äñ;GŒ±,LŒ±),\mathbb{E}\!\left[\tilde{A}\_{\pi}^{(\alpha)}(x,u)\mid x\right]\;\geq\;-\,\xi(\epsilon\_{g},\epsilon\_{\theta},\delta\_{\mathrm{adv}},\|u\|;G\_{\alpha},L\_{\alpha}), |  |

with the explicit tolerance function

|  |  |  |
| --- | --- | --- |
|  | Œæ‚Äã(‚ãÖ):=(GŒ±‚Äã(x)‚Äã[‚Äñu‚Äñ‚Äãœµg‚Äãcos‚Å°œµŒ∏+‚Äñu‚Äñ‚Äãsin‚Å°œµŒ∏‚àíŒ¥adv‚Äãcos‚Å°œµŒ∏]+LŒ±2‚Äã‚Äñu‚Äñ2)+,\xi(\cdot)\;:=\;\left(G\_{\alpha}(x)\Big[\|u\|\,\epsilon\_{g}\cos\epsilon\_{\theta}+\|u\|\sin\epsilon\_{\theta}-\delta\_{\mathrm{adv}}\cos\epsilon\_{\theta}\Big]+\tfrac{L\_{\alpha}}{2}\,\|u\|^{2}\right)\_{+}, |  |

where (‚ãÖ)+(\cdot)\_{+} denotes positive part. In particular, as (œµg,œµŒ∏)‚Üí0(\epsilon\_{g},\epsilon\_{\theta})\to 0 and for fixed Œ¥adv>0\delta\_{\mathrm{adv}}>0, Œæ‚Äã(‚ãÖ)‚Üí(LŒ±2‚Äã‚Äñu‚Äñ2‚àíGŒ±‚Äã(x)‚ÄãŒ¥adv)+\xi(\cdot)\to\big(\tfrac{L\_{\alpha}}{2}\|u\|^{2}-G\_{\alpha}(x)\delta\_{\mathrm{adv}}\big)\_{+}, i.e., the gate suppresses negative CVaR-advantage up to the second-order Taylor remainder. For sufficiently small ‚Äñu‚Äñ\|u\| or larger Œ¥adv\delta\_{\mathrm{adv}}, the negative-advantage region is eliminated.
‚àé

### Auxiliary notes on constants and domains

All Lipschitz and boundedness constants are understood to hold on a compact subset containing the closed-loop trajectories (guaranteed during training/evaluation by box/rate constraints and statewise CBF conditions). The ‚ÄúO‚Äã(T)O(T)‚Äù dependence in Lemma¬†[5](https://arxiv.org/html/2510.04555v1#Thmlemma5 "Lemma 5 (Occupancy shift under per-state KL). ‚Ä£ A.5 Proof of Theorem 4 (CVaR trust-region improvement) ‚Ä£ Appendix A Proofs for Section 4 ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") can be refined under mixing assumptions but suffices for trust-region purposes. The SNIS bound in Lemma¬†[3](https://arxiv.org/html/2510.04555v1#Thmlemma3 "Lemma 3 (Concentration for SNIS ratios with bounded weights). ‚Ä£ A.4 Proof of Theorem 3 (temperature-tilted CVaR concentration) ‚Ä£ Appendix A Proofs for Section 4 ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") can be strengthened via empirical Bernstein inequalities or self-normalized martingale bounds; we use a Hoeffding-style presentation for clarity, which already gives the O~‚Äã((K‚ÄãŒ±eff)‚àí1/2)\tilde{O}\!\big((K\alpha\_{\mathrm{eff}})^{-1/2}\big) rate stated in Theorem¬†[3](https://arxiv.org/html/2510.04555v1#Thmtheorem3 "Theorem 3 (Concentration of the temperature-tilted CVaR estimator). ‚Ä£ Theorem 3 (bias/variance and sample complexity of the CVaR estimator) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets").

Cross-references.

* ‚Ä¢

  Section¬†[4](https://arxiv.org/html/2510.04555v1#S4 "4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"): statements of Theorems¬†[1](https://arxiv.org/html/2510.04555v1#Thmtheorem1 "Theorem 1 (Robust forward invariance). ‚Ä£ Theorem 1 (robust forward invariance of the safety set) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"), [2](https://arxiv.org/html/2510.04555v1#Thmtheorem2 "Theorem 2 (Donsker‚ÄìVaradhan bound and per-state KL conservatism). ‚Ä£ Theorem 2 (KL‚ÄìDRO upper bound and conservatism of per-state KL) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"), [3](https://arxiv.org/html/2510.04555v1#Thmtheorem3 "Theorem 3 (Concentration of the temperature-tilted CVaR estimator). ‚Ä£ Theorem 3 (bias/variance and sample complexity of the CVaR estimator) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"), [4](https://arxiv.org/html/2510.04555v1#Thmtheorem4 "Theorem 4 (CVaR trust-region improvement). ‚Ä£ Theorem 4 (trust-region improvement inequality for CVaR with KL limits) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"), [5](https://arxiv.org/html/2510.04555v1#Thmtheorem5 "Theorem 5 (Persistence via NTB shrinkage and rate tightening). ‚Ä£ Theorem 5 (feasibility persistence under tail guards) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") and Propositions¬†[1](https://arxiv.org/html/2510.04555v1#Thmproposition1 "Proposition 1 (Shifted projection). ‚Ä£ Proposition 1 (minimal-deviation ùêª-metric projection) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"), [2](https://arxiv.org/html/2510.04555v1#Thmproposition2 "Proposition 2 (Gate-induced lower bound). ‚Ä£ Proposition 2 (negative-advantage suppression by sign-consistency) ‚Ä£ 4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets").
* ‚Ä¢

  Appendix¬†C (Reproducibility & Governance): data/seed pairing for CIs; telemetry storage schema; dashboard triggers referenced in Sec.¬†[6](https://arxiv.org/html/2510.04555v1#S6 "6 Explainability & Governance ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets").

## Appendix B Implementation Notes and Additional Materials

This appendix complements the main text and the proofs in Appendix¬†[A](https://arxiv.org/html/2510.04555v1#A1 "Appendix A Proofs for Section 4 ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") with implementation and reproducibility details.
We avoid introducing new figures or numeric results; instead, we provide concrete procedures, schemas, and checklists that allow faithful reproduction of the experiments and governance flows referenced in the main paper.

### B.1 Implementation Notes: Market, Execution, and Safety Layer

#### B.1¬†SSVI calibration (no-arbitrage)

We fit the SSVI surface w‚Äã(k,œÑ)w(k,\tau) (Eq.¬†([1](https://arxiv.org/html/2510.04555v1#S2.E1 "In 2.1 Arbitrage-Free Volatility Surfaces via SSVI ‚Ä£ 2 Preliminaries & Problem Setting ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets"))) to option mid quotes under static no-arbitrage constraints.

* ‚Ä¢

  Objective. Minimize a robust loss
  ‚Ñí‚Äã(Œ∏,œÜ,œÅ)=‚àë(k,œÑ)‚Ñì‚Äã(œÉ^imp‚Äã(k,œÑ;Œ∏,œÜ,œÅ)‚àíœÉimpmkt‚Äã(k,œÑ))\mathcal{L}(\theta,\varphi,\rho)=\sum\_{(k,\tau)}\ell\!\big(\hat{\sigma}\_{\mathrm{imp}}(k,\tau;\theta,\varphi,\rho)-\sigma\_{\mathrm{imp}}^{\mathrm{mkt}}(k,\tau)\big)
  where ‚Ñì\ell is Huber or Tukey; œÉ^imp\hat{\sigma}\_{\mathrm{imp}} derives from ww.
* ‚Ä¢

  Constraints. Enforce Gatheral‚ÄìJacquier sufficient conditions across maturities to preclude butterfly/calendar arbitrage¬†[[3](https://arxiv.org/html/2510.04555v1#bib.bib3)]. We implement as bound and inequality constraints in a constrained optimizer (e.g., interior-point).
* ‚Ä¢

  Smoothing. Penalize roughness along (k,œÑ)(k,\tau):
  Œªk‚Äã‚Äñ‚àÇk2w‚Äñ22+ŒªœÑ‚Äã‚Äñ‚àÇœÑ2w‚Äñ22\lambda\_{k}\|\partial\_{k}^{2}w\|\_{2}^{2}+\lambda\_{\tau}\|\partial\_{\tau}^{2}w\|\_{2}^{2} (finite differences); this aids Dupire stability.
* ‚Ä¢

  Diagnostics. Reject fits where convexity (in KK) of call prices implied by the surface is violated beyond tolerance.

#### B.2¬†Dupire local volatility (numerics)

* ‚Ä¢

  Call grid. Build a call surface C‚Äã(t,K)C(t,K) from SSVI via Black‚ÄìScholes inversion and put-call parity; interpolate with a monotone C2C^{2} spline in KK and a C1C^{1} spline in tt.
* ‚Ä¢

  Derivatives. Compute ‚àÇtC\partial\_{t}C and ‚àÇK‚ÄãKC\partial\_{KK}C by finite differences with Tikhonov regularization: minimize
  ‚ÄñDt‚ÄãC‚àí‚àÇtC‚Äñ22+Œ∑t‚Äã‚Äñ‚àÇt2C‚Äñ22\|D\_{t}C-\partial\_{t}C\|\_{2}^{2}+\eta\_{t}\|\partial\_{t}^{2}C\|\_{2}^{2} and analogously for ‚àÇK‚ÄãKC\partial\_{KK}C to stabilize Eq.¬†([2](https://arxiv.org/html/2510.04555v1#S2.E2 "In 2.2 Local Volatility via Dupire ‚Ä£ 2 Preliminaries & Problem Setting ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")).
* ‚Ä¢

  Positivity. Enforce ‚àÇK‚ÄãKC‚â•œµK‚ÄãK>0\partial\_{KK}C\geq\epsilon\_{KK}>0; if violated locally, project to the nearest positive value to avoid singularities in Eq.¬†([2](https://arxiv.org/html/2510.04555v1#S2.E2 "In 2.2 Local Volatility via Dupire ‚Ä£ 2 Preliminaries & Problem Setting ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")).
* ‚Ä¢

  Simulator. Evolve StS\_{t} with Euler‚ÄìMaruyama on a nonuniform time grid refined near expiry; clamp œÉloc\sigma\_{\mathrm{loc}} to a bounded interval to avoid stiffness.

#### B.3¬†VIX computation (OTM integral)

* ‚Ä¢

  Discretization. Approximate Eq.¬†([3](https://arxiv.org/html/2510.04555v1#S2.E3 "In 2.3 VIX Leg from Surface-Consistent Variance ‚Ä£ 2 Preliminaries & Problem Setting ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")) by midpoint or Simpson quadrature over available strikes; extrapolate tails using power-law decay anchored at the last observed OTM points.
* ‚Ä¢

  Parity & OTM selection. Use put-call parity to synthesize missing sides and include only OTM quotes: puts for K<FK<F, calls for K>FK>F.
* ‚Ä¢

  Non-negativity. Truncate negative integrand contributions (due to noise) at zero to preserve variance interpretation.

#### B.4¬†Execution adapter (ABIDES/MockLOB)

* ‚Ä¢

  Fills. Submit marketable or limit orders to the LOB; record partial fills, cancellations, and realized slippage.
* ‚Ä¢

  Impact. Implement Eq.¬†([4](https://arxiv.org/html/2510.04555v1#S2.E4 "In 2.4 Execution, Microstructure, and Impact ‚Ä£ 2 Preliminaries & Problem Setting ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")) with a linear temporary component Œ∑‚Äãut\eta u\_{t} and a decaying transient kernel G‚Äã(j)G(j); ensure no-dynamic-arbitrage conditions¬†[[46](https://arxiv.org/html/2510.04555v1#bib.bib46)].
* ‚Ä¢

  Latency. Add fixed or random latency; batch orders to emulate smart routing without venue modeling.

#### B.5¬†CBF‚ÄìQP layer (assembly & numerics)

* ‚Ä¢

  Linearization. Where hi‚Äã(f‚Äã(x)+g‚Äã(x)‚Äãu)h\_{i}(f(x)+g(x)u) is nonlinear in uu, use a first-order expansion around (xt,utnom)(x\_{t},u\_{t}^{\mathrm{nom}}) for the QP and re-solve with warm-start (one SQP step per environment step).
* ‚Ä¢

  Scaling. Diagonal-scale the QP (row/col) to improve conditioning; choose HH as a scaled identity or Fisher-like metric from policy covariance.
* ‚Ä¢

  Warm-start. Initialize with u=ut‚àí1u=u\_{t-1} and duals from the previous solve; set OSQP tolerances to meet per-step latency budgets.
* ‚Ä¢

  Slack policy. Penalize ‚ÄñŒ∂‚Äñ1\|\zeta\|\_{1} with large œÅ\rho; audit any Œ∂>0\zeta>0 events (Sec.¬†[6](https://arxiv.org/html/2510.04555v1#S6 "6 Explainability & Governance ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")).

### B.2 Reproducibility & Governance Checklist (referenced as Appendix¬†C)

#### C.1¬†Environment and determinism

* ‚Ä¢

  Record random seeds for Python/NumPy/PyTorch/ABIDES; fix CuDNN determinism flags where applicable.
* ‚Ä¢

  Log package versions and OS/hardware details; pin dependency hashes.

#### C.2¬†Artifact manifest

* ‚Ä¢

  Configs: YAML/JSON files for market generator (SSVI/Dupire/VIX), execution, RL hyperparameters, and CBF‚ÄìQP settings.
* ‚Ä¢

  Outputs: per-episode CSV of P&L, per-step telemetry logs (Sec.¬†[6](https://arxiv.org/html/2510.04555v1#S6 "6 Explainability & Governance ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")), and summary JSONs for metrics.
* ‚Ä¢

  Provenance: Git commit hash and config fingerprint embedded in each artifact.

#### C.3¬†Paired evaluation protocol

* ‚Ä¢

  Use the same scenario seeds and path indices across methods; when legacy runs have different nn, downsample to a common effective nn *per seed*.
* ‚Ä¢

  Compute 95%95\% paired bootstrap CIs with stratification by seed (Sec.¬†[5](https://arxiv.org/html/2510.04555v1#S5 "5 Experiments in Arbitrage-Free Synthetic Markets ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")).

#### C.4¬†Governance triggers and storage

* ‚Ä¢

  Implement the online triggers in Sec.¬†[6](https://arxiv.org/html/2510.04555v1#S6 "6 Explainability & Governance ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") as rules bound to telemetry streams with persistent queues.
* ‚Ä¢

  Store *Explanation Records* with immutable IDs, timestamps, and hashes; ensure read-only access for internal audit.

### B.3 Statistical Protocols (referenced as Appendix¬†B)

#### B.6¬†Paired bootstrap for metrics

We use stratified paired bootstrap across seeds. For each bootstrap replicate: sample seeds with replacement, then sample paths (indices) with replacement *within* each seed; compute per-method statistics; record differences. Confidence intervals are empirical quantiles in the main text already outlines a variant for Œî‚ÄãES\Delta\mathrm{ES}).

#### B.7¬†Multiple testing & effect sizes

* ‚Ä¢

  Adjust pp-values across metrics by Benjamini‚ÄìHochberg (FDR control).
* ‚Ä¢

  Report Vargha‚ÄìDelaney A^12\hat{A}\_{12} for effect sizes; interpret A^12‚àà[0,1]\hat{A}\_{12}\in[0,1] with 0.50.5 as no effect.

#### B.8¬†Ratio metrics

For Sharpe/Sortino/Œ©\Omega, prefer *paired* computation (same path pairing) and report CIs via bootstrap on the *ratio* directly; avoid delta-method linearization unless moments are well-behaved.

### B.4 Hyperparameters & Search Spaces

We list *recommended ranges* (not the specific values used in any run) to guide replication. These ranges are standard for PPO-style training and convex safety layers; they are *not* new numerical results.

Table 6: Recommended hyperparameter ranges (to be tuned per hardware/time budget).

| Component | Range / Notes |
| --- | --- |
| PPO clip œµ\epsilon | small to moderate (e.g., [0.05,0.3][0.05,0.3]) |
| KL coeff ŒªKL\lambda\_{\mathrm{KL}} | increase as Œ±\alpha tightens; grid over [10‚àí3,10‚àí1][10^{-3},10^{-1}] |
| Entropy coeff Œªent\lambda\_{\mathrm{ent}} | decay schedule; start in [10‚àí3,10‚àí2][10^{-3},10^{-2}] |
| IQN quantiles per update KK | increase with smaller Œ±\alpha; e.g., [64,512][64,512] |
| Temperature TT | controller-managed in [Tmin,Tmax][T\_{\min},T\_{\max}] with Tmin>0T\_{\min}>0 |
| Tail-boost Œ≥tail\gamma\_{\mathrm{tail}} | controller-managed in [1,Œ≥max][1,\gamma\_{\max}] |
| Œ±\alpha schedule | from ‚âà0.10\approx 0.10 to target (e.g., 0.0250.025) in stages |
| CBF gains Œ∫i\kappa\_{i} | choose for desired contraction; constant or state-dependent |
| QP metric HH | scaled identity or diag of action covariance |
| Rate cap rmaxr\_{\max} | align with execution latency and LOB depth |
| NTB (M,bmax)(M,b\_{\max}) | M‚âª0M\succ 0; bmaxb\_{\max} shrinks near expiry |
| Sign gate Œ¥adv\delta\_{\mathrm{adv}} | small positive threshold; calibrated via telemetry |

### B.5 Safety Telemetry Schema (extended)

We provide a machine-readable schema for *Explanation Records* and stepwise telemetry (human-readable templates are in Sec.¬†[6](https://arxiv.org/html/2510.04555v1#S6 "6 Explainability & Governance ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")). The schema is language-agnostic; below is a JSON-like sketch.

```
Record {
  run_id: string, episode_id: int, step: int, timestamp: iso8601,
  state_hash: string, action_nominal: vector[m], action_safe: vector[m],
  H_norm_deviation: float,
  active_set: array[int], tightest_id: int,
  multipliers: array[float],         // KKT duals if exposed
  rate_util: float, gate_score: float,
  slack_sum: float, solver_status: string, solver_time_ms: float,
  rule_names: array[string],         // human-friendly names for active_set
  rationale_text: string,            // filled from template
  kl_step: float, tail_coverage: float, alpha: float
}
```

### B.6 Failure Modes & Debugging Playbook

##### High variance in CVaR estimate.

Symptoms: oscillatory w^\widehat{w}, noisy gradients, stalled improvements. Actions: widen batch size; increase TT (less tilt); increase Œ≥tail\gamma\_{\mathrm{tail}} cautiously; strengthen KL; enable gradient clipping.

##### Frequent QP infeasibility.

Symptoms: positive slack\_sum, solver fallbacks. Actions: relax NTB or box bounds; add expiry-aware shrinkage earlier; reduce rate cap; improve linearization (two SQP inner steps); check scaling.

##### Gate over-blocking.

Symptoms: low pass-rate, missed opportunities. Actions: reduce Œ¥adv\delta\_{\mathrm{adv}}; increase ensemble diversity; use moving-average signals; add confidence thresholds.

##### Latency spikes.

Symptoms: P95 solver time breaches. Actions: pre-factor HH; prune inactive constraints; warm-start duals; increase OSQP ADMM iterations budget only when needed.

### B.7 Extended Notation and Acronyms

Table 7: Extended notation and acronyms (complements Table¬†[1](https://arxiv.org/html/2510.04555v1#S2.T1 "Table 1 ‚Ä£ 2.8 Notation Summary ‚Ä£ 2 Preliminaries & Problem Setting ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")).

|  |  |
| --- | --- |
| Symbol/Acronym | Description |
| IQN | Implicit Quantile Network (distributional critic) |
| CBF | Control Barrier Function |
| QP | Quadratic Program |
| NTB | No-Trade Band (ellipsoidal exposure tolerance) |
| SNIS | Self-Normalized Importance Sampling |
| EMA | Exponential Moving Average (reference policy) |
| PPO | Proximal Policy Optimization |
| DRO | Distributionally Robust Optimization |
| BH-FDR | Benjamini‚ÄìHochberg False Discovery Rate |
| Œ¥adv\delta\_{\mathrm{adv}} | Advantage threshold in sign-consistency gate |
| Œ∫i\kappa\_{i} | CBF contraction gains |
| œÅ\rho | Slack penalty coefficient in QP |

### B.8 Ethical Use and Risk Notices

This research concerns algorithmic decision-making in financial contexts. The safety guarantees discussed in Sec.¬†[4](https://arxiv.org/html/2510.04555v1#S4 "4 Theoretical Results ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") are *operational* (state-wise constraints under feasibility) and do not constitute guarantees of fairness, market integrity, or cybersecurity. Live deployment requires additional controls (surveillance, abuse detection, red-team adversarial tests), appropriate disclosures, and adherence to applicable regulations and firm-specific policies.

Pointers back to the main text.
Appendix¬†[B.1](https://arxiv.org/html/2510.04555v1#A2.SS1 "B.1 Implementation Notes: Market, Execution, and Safety Layer ‚Ä£ Appendix B Implementation Notes and Additional Materials ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") supports Sections¬†[2.1](https://arxiv.org/html/2510.04555v1#S2.SS1 "2.1 Arbitrage-Free Volatility Surfaces via SSVI ‚Ä£ 2 Preliminaries & Problem Setting ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets")‚Äì[2.4](https://arxiv.org/html/2510.04555v1#S2.SS4 "2.4 Execution, Microstructure, and Impact ‚Ä£ 2 Preliminaries & Problem Setting ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") and [3.3](https://arxiv.org/html/2510.04555v1#S3.SS3 "3.3 White-Box CBF‚ÄìQP Safety Layer ‚Ä£ 3 Method: Tail-Safe Hedging Framework ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets");
Appendix¬†[B.2](https://arxiv.org/html/2510.04555v1#A2.SS2 "B.2 Reproducibility & Governance Checklist (referenced as Appendix C) ‚Ä£ Appendix B Implementation Notes and Additional Materials ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") implements the reproducibility and governance references in Sections¬†[5](https://arxiv.org/html/2510.04555v1#S5 "5 Experiments in Arbitrage-Free Synthetic Markets ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") and [6](https://arxiv.org/html/2510.04555v1#S6 "6 Explainability & Governance ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets");
Appendix¬†[B.3](https://arxiv.org/html/2510.04555v1#A2.SS3 "B.3 Statistical Protocols (referenced as Appendix B) ‚Ä£ Appendix B Implementation Notes and Additional Materials ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") details the statistical procedures referenced in Section¬†[5](https://arxiv.org/html/2510.04555v1#S5 "5 Experiments in Arbitrage-Free Synthetic Markets ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets");
Appendix¬†[B.4](https://arxiv.org/html/2510.04555v1#A2.SS4 "B.4 Hyperparameters & Search Spaces ‚Ä£ Appendix B Implementation Notes and Additional Materials ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") summarizes tunable ranges without adding numerical results;
Appendix¬†[B.5](https://arxiv.org/html/2510.04555v1#A2.SS5 "B.5 Safety Telemetry Schema (extended) ‚Ä£ Appendix B Implementation Notes and Additional Materials ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") and [B.6](https://arxiv.org/html/2510.04555v1#A2.SS6 "B.6 Failure Modes & Debugging Playbook ‚Ä£ Appendix B Implementation Notes and Additional Materials ‚Ä£ Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF‚ÄìQP Safety Layer in Arbitrage-Free Markets") provide operational scaffolding for audits and incident response.

## References

* [1]

  Robert Almgren and Neil Chriss.
  Optimal Execution of Portfolio Transactions.
  *Journal of Risk*, 3:5‚Äì39, 2001.
* [2]

  Albert¬†S. Kyle.
  Continuous Auctions and Insider Trading.
  *Econometrica*, 53(6):1315‚Äì1335, 1985.
* [3]

  Jim Gatheral and Antoine Jacquier.
  Arbitrage-free SVI volatility surfaces.
  *Quantitative Finance*, 14(1):59‚Äì71, 2014.
* [4]

  Bruno Dupire.
  Pricing with a Smile.
  *Risk*, 7:18‚Äì20, 1994.
* [5]

  Anna Obizhaeva and Jiang Wang.
  Optimal trading strategy and supply/demand dynamics.
  *Journal of Financial Markets*, 16(1):1‚Äì32, 2013.
* [6]

  Hans B√ºhler, Lukas Gonon, Josef Teichmann, and Ben Wood.
  Deep Hedging.
  *Quantitative Finance*, 19(8):1271‚Äì1291, 2019.
* [7]

  David Byrd, Maria Hybinette, and Tucker Balch.
  ABIDES: Towards High-Fidelity Multi-Agent Market Simulation.
  In *Proceedings of the 2020 ACM SIGSIM Conference on Principles of Advanced Discrete Simulation (PADS)*, 2020.
* [8]

  David Byrd, Maria Hybinette, and Tucker Balch.
  ABIDES: Towards High-Fidelity Market Simulation for AI Research.
  arXiv:1904.12066, 2019.
* [9]

  Bryan¬†T. Kelly and Dacheng Xiu.
  Financial Machine Learning.
  *Foundations and Trends in Finance*, 13(3‚Äì4):205‚Äì363, 2023.
  doi:10.1561/0500000064.
* [10]

  Ben Hambly, Xunyu Xu, and Haotian Yang.
  Recent advances in reinforcement learning in finance.
  *Mathematical Finance*, 33(3):437‚Äì503, 2023.
* [11]

  Shuo Sun, Rundong Wang, and Bo An.
  Reinforcement Learning for Quantitative Trading: A Survey.
  *ACM Transactions on Intelligent Systems and Technology*, 14(3):1‚Äì29, 2023.
* [12]

  Nikolaos Pippas, √áagatay Turkay, and Elliot¬†A. Ludvig.
  The Evolution of Reinforcement Learning in Quantitative Finance: A Survey.
  *ACM Computing Surveys*, 2025.
  Early access. doi:10.1145/3733714.
* [13]

  Marc¬†G. Bellemare, Will Dabney, and R√©mi Munos.
  A Distributional Perspective on Reinforcement Learning.
  In *Proceedings of ICML*, 2017.
* [14]

  Will Dabney, Georg Ostrovski, David Silver, and R√©mi Munos.
  Implicit Quantile Networks for Distributional Reinforcement Learning.
  In *Proceedings of ICML*, pages 1096‚Äì1105, 2018.
* [15]

  John Schulman, Sergey Levine, Philipp Moritz, Michael¬†I. Jordan, and Pieter Abbeel.
  Trust Region Policy Optimization.
  In *Proceedings of ICML*, 2015.
* [16]

  John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
  Proximal Policy Optimization Algorithms.
  arXiv:1707.06347, 2017.
* [17]

  Yinlam Chow and Mohammad Ghavamzadeh.
  Algorithms for CVaR Optimization in MDPs.
  In *NeurIPS*, 2014.
* [18]

  Yinlam Chow, Aviv Tamar, Shie Mannor, and Marco Pavone.
  Risk-Constrained Reinforcement Learning with Percentile Risk Criteria.
  *Journal of Machine Learning Research*, 18(167):1‚Äì51, 2018.
* [19]

  Aviv Tamar, Yinlam Chow, Mohammad Ghavamzadeh, and Shie Mannor.
  Policy Gradient for Coherent Risk Measures.
  In *NeurIPS*, 2015.
* [20]

  Alex Ray, Joshua Achiam, and Dario Amodei.
  Benchmarking Safe Exploration in Deep Reinforcement Learning.
  OpenAI Technical Report, 2019.
* [21]

  Jiawei Ji et¬†al.
  Safety-Gymnasium: A Unified Safe Reinforcement Learning Benchmark.
  In *NeurIPS Datasets and Benchmarks*, 2023.
* [22]

  Weiye Zhao, Tairan He, Rui Chen, Tianhao Wei, and Changliu Liu.
  State-wise Safe Reinforcement Learning: A Survey.
  In *IJCAI*, pages 6882‚Äì6890, 2023.
* [23]

  Aaron¬†D. Ames, Xiangru Xu, Jessy¬†W. Grizzle, and Paulo Tabuada.
  Control Barrier Function Based Quadratic Programs for Safety Critical Systems.
  *IEEE Transactions on Automatic Control*, 62(8):3861‚Äì3876, 2017.
* [24]

  Aaron¬†D. Ames et¬†al.
  Control Barrier Functions: Theory and Applications.
  arXiv:1903.11199, 2019.
* [25]

  Ayush Garg, Shishir Kolathaya, Ian¬†R. Manchester, Aaron¬†D. Ames, Sertac Karaman, et¬†al.
  Advances in the Theory of Control Barrier Functions.
  *Annual Reviews in Control*, 2024.
* [26]

  Qinlin Ma et¬†al.
  Differentiable Control Barrier Functions for Learning of Safe Robot Control.
  In *European Control Conference (ECC)*, 2022.
* [27]

  Zengyi Yang, Shishir Kolathaya, and Aaron¬†D. Ames.
  Differentiable Safe Controller Design through Control Barrier Functions.
  *IEEE Control Systems Letters*, 6:1024‚Äì1029, 2022.
* [28]

  Wenceslao Shaw-Cortez and Dimos¬†V. Dimarogonas.
  Designing Performance-Critical Controllers with Control Barrier Functions.
  arXiv:2208.02319, 2022.
* [29]

  Jason¬†J. Choi, Donggun Lee, Koushil Sreenath, Claire¬†J. Tomlin, and Sylvia¬†L. Herbert.
  Robust Control Barrier‚ÄìValue Functions for Safety-Critical Control.
  arXiv:2107.01188, 2021.
* [30]

  Miodrag Jankovic.
  Robust Control Barrier Functions for Constrained Control of Nonlinear Systems.
  *Automatica*, 96:359‚Äì367, 2018.
* [31]

  Xiangru Xu, Paulo Tabuada, Jessy¬†W. Grizzle, and Aaron¬†D. Ames.
  Robustness of Control Barrier Functions for Safety Critical Control.
  *IFAC-PapersOnLine*, 48(27):54‚Äì61, 2015.
* [32]

  Yujie Wang and Xiangru Xu.
  Disturbance-Observer-based Robust Control Barrier Functions.
  Technical report, 2024.
  URL: <https://xu.me.wisc.edu/wp-content/uploads/sites/1196/2024/04/Disturbance-Observer-based-Robust-Control-Barrier-Functions.pdf>.
* [33]

  Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.
  Conservative Q-Learning for Offline Reinforcement Learning.
  In *NeurIPS*, 2020.
* [34]

  Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta.
  Robust Adversarial Reinforcement Learning.
  In *Proceedings of ICML*, 2017.
* [35]

  Ce Lu, Hao Wang, and Lin Chen.
  On Distributionally Robust Reinforcement Learning with Total Variation Distance.
  In *NeurIPS*, 2024.
* [36]

  Karthik Sundhar¬†Ramesh and Sanjiban Choudhury.
  Distributionally Robust Model-Based Reinforcement Learning.
  In *Proceedings of the 41st International Conference on Machine Learning (ICML)*, 2024.
* [37]

  Pu Shi, Chris Russell, Pietro Li√≥, and Dmitry Kazhdan.
  Worst-Case Performance and Fairness in Offline Reinforcement Learning.
  *Journal of Machine Learning Research*, 25(151):1‚Äì61, 2024.
* [38]

  Shuo Liu, Yingying Ma, Pan Li, Liwei Wang, and Zhi-Hua Zhou.
  Learning Distributionally Robust Linear Mixture MDPs.
  arXiv:2505.18044, 2025.
* [39]

  David Byrd et¬†al.
  ABIDES-Gym: Gym Environments for Multi-Agent Discrete Event Simulation and Application to Financial Markets.
  arXiv:2110.14771, 2021.
* [40]

  Selim Amrouni, Aymeric Moulin, Jared Vann, Svitlana Vyetrenko, Tucker Balch, and Manuela Veloso.
  ABIDES-Gym: Gym Environments for Multi-Agent Discrete Event Simulation and Application to Financial Markets.
  In *Proceedings of the Second ACM International Conference on AI in Finance (ICAIF)*, 2021.
* [41]

  Kresimir Demeterfi, Emanuel Derman, Michael Kamal, and Joseph Zou.
  A Guide to Volatility and Variance Swaps.
  Goldman Sachs Quantitative Strategies Research, Technical Report, 1999.
* [42]

  Cboe Global Markets.
  White Paper: The Cboe Volatility Index (VIX¬Æ).
  Cboe, 2019.
  URL: <https://www.cboe.com/tradable_products/vix/>.
* [43]

  R.¬†Tyrrell Rockafellar and Stanislav Uryasev.
  Optimization of Conditional Value-at-Risk.
  *Journal of Risk*, 2(3):21‚Äì41, 2000.
* [44]

  Carlo Acerbi and Dirk Tasche.
  On the coherence of Expected Shortfall.
  *Journal of Banking & Finance*, 26(7):1487‚Äì1503, 2002.
* [45]

  Solomon Kullback and Richard¬†A. Leibler.
  On Information and Sufficiency.
  *The Annals of Mathematical Statistics*, 22(1):79‚Äì86, 1951.
* [46]

  Jim Gatheral.
  No-dynamic-arbitrage and market impact.
  *Quantitative Finance*, 10(7):749‚Äì759, 2010.
* [47]

  John Schulman, Philipp Moritz, Sergey Levine, Michael¬†I. Jordan, and Pieter Abbeel.
  High-Dimensional Continuous Control Using Generalized Advantage Estimation.
  In *Proceedings of the International Conference on Learning Representations (ICLR)*, 2016.
* [48]

  Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel.
  Constrained Policy Optimization.
  In *Proceedings of the 34th International Conference on Machine Learning (ICML)*, pages 22‚Äì31, 2017.
* [49]

  Stephen Boyd and Lieven Vandenberghe.
  *Convex Optimization*.
  Cambridge University Press, 2004.
* [50]

  Art¬†B. Owen.
  *Monte Carlo Theory, Methods and Examples*.
  Stanford University, 2013. Available online.
* [51]

  Karl¬†J. √Östr√∂m and Tore H√§gglund.
  *PID Controllers: Theory, Design, and Tuning*.
  Instrument Society of America, 1995.
* [52]

  Bartolomeo Stellato, Goran Banjac, Paul Goulart, Alberto Bemporad, and Stephen Boyd.
  OSQP: An Operator Splitting Solver for Quadratic Programs.
  *Mathematical Programming Computation*, 12(4):637‚Äì672, 2020.
* [53]

  Richard¬†S. Sutton and Andrew¬†G. Barto.
  *Reinforcement Learning: An Introduction (2nd Edition)*.
  MIT Press, 2018.
* [54]

  Monroe¬†D. Donsker and S.¬†R. Srinivasa Varadhan.
  Asymptotic Evaluation of Certain Markov Process Expectations for Large Time, I.
  *Communications on Pure and Applied Mathematics*, 28(1):1‚Äì47, 1975.
* [55]

  Imre Csisz√°r and J√°nos K√∂rner.
  *Information Theory: Coding Theorems for Discrete Memoryless Systems*.
  Cambridge University Press, Second edition, 2011.
* [56]

  Hongseok Namkoong and John¬†C. Duchi.
  Stochastic Gradient Methods for Distributionally Robust Optimization with ff-Divergences.
  In *NeurIPS*, 2017.
* [57]

  Sham Kakade and John Langford.
  Approximately Optimal Approximate Reinforcement Learning.
  In *Proceedings of the 19th International Conference on Machine Learning (ICML)*, 2002.
* [58]

  Bradley Efron.
  Bootstrap Methods: Another Look at the Jackknife.
  *The Annals of Statistics*, 7(1):1‚Äì26, 1979.
* [59]

  A.¬†C. Davison and D.¬†V. Hinkley.
  *Bootstrap Methods and their Application*.
  Cambridge University Press, 1997.
* [60]

  Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger.
  Deep Reinforcement Learning that Matters.
  In *Proceedings of the AAAI Conference on Artificial Intelligence*, 2018.
* [61]

  William¬†F. Sharpe.
  The Sharpe Ratio.
  *The Journal of Portfolio Management*, 21(1):49‚Äì58, 1994.
* [62]

  Frank¬†A. Sortino and Lee¬†N. Price.
  Performance Measurement in a Downside Risk Framework.
  *Journal of Investing*, 3(3):59‚Äì64, 1994.
* [63]

  Con Keating and William¬†F. Shadwick.
  A Universal Performance Measure.
  Working Paper, The Finance Development Centre, 2002.
* [64]

  Yoav Benjamini and Yosef Hochberg.
  Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing.
  *Journal of the Royal Statistical Society: Series B (Methodological)*, 57(1):289‚Äì300, 1995.
* [65]

  Andr√°s Vargha and Harold¬†D. Delaney.
  A Critique and Improvement of the CL Common Language Effect Size Statistics of McGraw and Wong.
  *Journal of Educational and Behavioral Statistics*, 25(2):101‚Äì132, 2000.
* [66]

  Board of Governors of the Federal Reserve System.
  SR 11-7: Guidance on Model Risk Management.
  2011.
  URL: <https://www.federalreserve.gov/supervisionreg/srletters/sr1107.htm>.
* [67]

  Basel Committee on Banking Supervision.
  Principles for effective risk data aggregation and risk reporting.
  January 2013.
* [68]

  National Institute of Standards and Technology.
  Artificial Intelligence Risk Management Framework (AI RMF 1.0).
  NIST AI 100-1, 2023.
* [69]

  Javier Garc√≠a and Fernando Fern√°ndez.
  A Comprehensive Survey on Safe Reinforcement Learning.
  *Journal of Machine Learning Research*, 16(1):1437‚Äì1480, 2015.
* [70]

  Mohammed Alshiekh, Roderick Bloem, R√ºdiger Ehlers, Bettina K√°m√°n, Jan Kret√≠nsk√Ω, Scott Niekum, and Ufuk Topcu.
  Safe Reinforcement Learning via Shielding.
  In *AAAI Conference on Artificial Intelligence*, 2018.
* [71]

  Gal Dalal, Elad Gilboa, Shie Mannor, and Nahum Shimkin.
  Safe Exploration in Continuous Action Spaces.
  In *Proceedings of the 35th International Conference on Machine Learning (ICML)*, 2018.
* [72]

  Brandon Amos and J.¬†Zico Kolter.
  OptNet: Differentiable Optimization as a Layer in Neural Networks.
  In *Proceedings of the 34th International Conference on Machine Learning (ICML)*, 2017.
* [73]

  Akshay Agrawal, Brandon Amos, Shane Barratt, Stephen Boyd, Steven Diamond, and J.¬†Zico Kolter.
  Differentiable Convex Optimization Layers.
  In *Advances in Neural Information Processing Systems (NeurIPS)*, 2019.
* [74]

  Garud¬†N. Iyengar.
  Robust Dynamic Programming.
  *Mathematics of Operations Research*, 30(2):257‚Äì280, 2005.
* [75]

  Arnab Nilim and Laurent El¬†Ghaoui.
  Robust Control of Markov Decision Processes with Uncertain Transition Matrices.
  *Operations Research*, 53(5):780‚Äì798, 2005.
* [76]

  Esther Derman, Huan Xu, and Shie Mannor.
  Distributionally Robust Policy Evaluation and Optimization.
  In *Proceedings of the 35th International Conference on Machine Learning (ICML)*, 2018.
* [77]

  √Ålvaro Cartea, Sebastian Jaimungal, and Jos√© Penalva.
  *Algorithmic and High-Frequency Trading*.
  Cambridge University Press, 2015.
* [78]

  Finale Doshi-Velez and Been Kim.
  Towards a Rigorous Science of Interpretable Machine Learning.
  arXiv:1702.08608, 2017.
* [79]

  Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti, and Dino Pedreschi.
  A Survey of Methods for Explaining Black Box Models.
  *ACM Computing Surveys*, 51(5):93, 2018.
* [80]

  Cynthia Rudin.
  Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead.
  *Nature Machine Intelligence*, 1(5):206‚Äì215, 2019.