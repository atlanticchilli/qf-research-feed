---
authors:
- Yimeng Qiu
doc_id: arxiv:2510.24607v1
family_id: arxiv:2510.24607
is_current: true
taxonomy:
  alpha_families: []
  asset_classes: []
  horizons: []
  themes: []
title: 'Entropy-Guided Multiplicative Updates: KL Projections for Multi-Factor Target
  Exposures'
url_abs: http://arxiv.org/abs/2510.24607v1
url_html: https://arxiv.org/html/2510.24607v1
venue: arXiv q-fin
version: 1
year: 2025
---


Yimeng Qiu

(2025/06/18)

###### Abstract

We develop *Entropy-Guided Multiplicative Updates* (EGMU), a convex optimization framework for constructing multi-factor target-exposure portfolios by minimizing Kullbackâ€“Leibler (KL) divergence from a benchmark subject to linear factor constraints.
Our contributions are theoretical and algorithmic.
(*i*) We formalize feasibility and uniqueness: with strictly positive benchmark and feasible targets in the convex hull of exposures, the solution is unique and strictly positive.
(*ii*) We derive the dual concave program with gradient tâˆ’ğ”¼wâ€‹(Î¸)â€‹[x]t-\mathbb{E}\_{w(\theta)}[x] and Hessian âˆ’Covwâ€‹(Î¸)â€‹(x)-\mathrm{Cov}\_{w(\theta)}(x), and give a precise sensitivity formula âˆ‚Î¸âˆ—/âˆ‚t=Covwâˆ—â€‹(x)âˆ’1\partial\theta^{\*}/\partial t=\mathrm{Cov}\_{w^{\*}}(x)^{-1} and âˆ‚wâˆ—/âˆ‚t=diagâ€‹(wâˆ—)â€‹(Xâˆ’ğŸâ€‹Î¼âŠ¤)â€‹Covwâˆ—â€‹(x)âˆ’1\partial w^{\*}/\partial t=\mathrm{diag}(w^{\*})(X-\mathbf{1}\mu^{\top})\mathrm{Cov}\_{w^{\*}}(x)^{-1}.
(*iii*) We present two provably convergent solvers: a damped *dual Newton* method with global convergence and local quadratic rate, and a *KL-projection* scheme based on IPF/Bregmanâ€“Dykstra for equalities and inequalities.
(*iv*) We further generalize EGMU with *elastic targets* (strongly concave dual) and *robust target sets* (support-function dual), and introduce a *path-following ODE* for solution trajectories, all reusing the same dual-moment structure and solved via Newton or proximal-gradient schemes.
(*v*) We detail numerically stable and scalable implementations (LogSumExp, covariance regularization, half-space KL-projections).
We emphasize theory and reproducible algorithms; empirical benchmarking is optional.

Keywords: KL divergence, information projection, entropy pooling, factor exposures, Bregman projections, convex optimization.

## 1 Introduction

Rules-based multi-factor portfolios seek specified exposures (Value, Momentum, Quality, Low Volatility, etc.).
Heuristic sequential â€œtiltsâ€ lack a single global objective and are order-dependent.
Quadratic exposure-matching solves a different closeness metric and often needs explicit regularization and a risk model.

We pose *Entropy-Guided Multiplicative Updates* (EGMU): minimize DKL(ğ°âˆ¥ğ›)D\_{\mathrm{KL}}\!\left(\mathbf{w}\middle\|\mathbf{b}\right) over the simplex under linear exposure constraints. This information projection is classical and yields exponential-family solutions and convex duality structure [[6](https://arxiv.org/html/2510.24607v1#bib.bib6), [5](https://arxiv.org/html/2510.24607v1#bib.bib5)]. In portfolio engineering it parallels Entropy Pooling [[8](https://arxiv.org/html/2510.24607v1#bib.bib8)]. Our focus is to provide a rigorous, self-contained treatment tailored to target-exposure construction: feasibility/uniqueness, sensitivity, and provably convergent algorithms for equality and inequality constraints. We also give implementable pseudo-code with stability safeguards. Our generalized variantsâ€”elastic/robust targets and solution pathsâ€”remain within the same dual-moment framework.

#### Notation.

Let NN be the number of assets, KK factors. Benchmark ğ›âˆˆÎ”N:={ğ°âˆˆâ„â‰¥0N:ğŸâŠ¤â€‹ğ°=1}\mathbf{b}\in\Delta^{N}:=\{\mathbf{w}\!\in\!\mathbb{R}\_{\geq 0}^{N}:\mathbf{1}^{\top}\mathbf{w}=1\} has strictly positive entries (bi>0b\_{i}>0). Exposure matrix ğ—âˆˆâ„NÃ—K\mathbf{X}\in\mathbb{R}^{N\times K} has rows ğ±iâŠ¤\mathbf{x}\_{i}^{\top}. Targets tâˆˆâ„Kt\in\mathbb{R}^{K}. Expectations ğ”¼wâ€‹[â‹…]\mathbb{E}\_{w}[\cdot] are under the discrete distribution ww on {1,â€¦,N}\{1,\dots,N\}.

#### Operators and shorthand.

For a vector vv, normalizeâ€‹(v):=v/(ğŸâŠ¤â€‹v)\mathrm{normalize}(v):=v/(\mathbf{1}^{\top}v) projects vv onto the simplex ray.
Elementwise product/division are denoted by âŠ™\odot and âŠ˜\oslash.
We write Î ğ’KLâ€‹(u)\Pi^{\mathrm{KL}}\_{\mathcal{C}}(u) for the (unique) KL projection of uu onto a closed convex set ğ’âŠ†Î”N\mathcal{C}\subseteq\Delta^{N}.
We use the LogSumExp trick logâ€‹âˆ‘ibiâ€‹esi=logâ€‹âˆ‘ibiâ€‹esiâˆ’m+m\log\!\sum\_{i}b\_{i}e^{s\_{i}}=\log\!\sum\_{i}b\_{i}e^{s\_{i}-m}+m with m=maxiâ¡sim=\max\_{i}s\_{i}.
We adopt ğ”¼wâ€‹[x]=âˆ‘iwiâ€‹xi\mathbb{E}\_{w}[x]=\sum\_{i}w\_{i}x\_{i} and Covwâ€‹(x)=âˆ‘iwiâ€‹(xiâˆ’Î¼)â€‹(xiâˆ’Î¼)âŠ¤\mathrm{Cov}\_{w}(x)=\sum\_{i}w\_{i}(x\_{i}-\mu)(x\_{i}-\mu)^{\top} with Î¼=ğ”¼wâ€‹[x]\mu=\mathbb{E}\_{w}[x].

#### Contributions at a glance.

* â€¢

  KL-based target-exposure construction: existence/uniqueness and the exponential-family solution with covariance Hessian.
* â€¢

  Algorithms with guarantees: a damped dual Newton method and Bregman projection schemes (IPF/Dykstra) for equalities and inequalities.
* â€¢

  Generalizations with shared dual moments: elastic targets (strongly concave dual) and robust target sets (support-function dual) with a proximal-gradient solver.
* â€¢

  Sensitivity and paths: closed-form sensitivities and a homotopy ODE to trace optimal solutions along target directions.

## 2 Problem, Feasibility, and Geometry

### 2.1 KL-Minimization with Linear Constraints

We study

|  |  |  |  |
| --- | --- | --- | --- |
|  | minğ°âˆˆÎ”NDKL(ğ°âˆ¥ğ›)s.t.ğ—âŠ¤ğ°=t,Ağ°â‰¤c.\min\_{\mathbf{w}\in\Delta^{N}}\ D\_{\mathrm{KL}}\!\left(\mathbf{w}\middle\|\mathbf{b}\right)\quad\text{s.t.}\quad\mathbf{X}^{\top}\mathbf{w}=t,\qquad A\mathbf{w}\leq c. |  | (1) |

The objective is strictly convex on the relative interior of Î”N\Delta^{N}; the feasible set is convex.

### 2.2 Feasibility and Strict Positivity

###### Proposition 1 (Feasibility and strict positivity).

If only equality constraints ğ—âŠ¤â€‹ğ°=t\mathbf{X}^{\top}\mathbf{w}=t are present, feasibility holds iff tâˆˆconvâ€‹{ğ±i}i=1Nt\in\mathrm{conv}\{\mathbf{x}\_{i}\}\_{i=1}^{N}. If tt lies in the relative interior of convâ€‹{ğ±i}\mathrm{conv}\{\mathbf{x}\_{i}\}, the unique optimizer satisfies wiâ‹†>0w\_{i}^{\star}>0 for all ii. With additional linear inequalities Aâ€‹ğ°â‰¤cA\mathbf{w}\leq c, feasibility remains a convex polytope; infeasibility admits a Farkas-type certificate.

###### Remark 1 (Intercept factor, linear dependence, and gauge fixing).

If ğ—\mathbf{X} contains a constant (intercept) column ğŸ\mathbf{1}, then the budget constraint ğŸâŠ¤â€‹ğ°=1\mathbf{1}^{\top}\mathbf{w}=1 is linearly redundant with that column, making the dual variable non-unique and the covariance Covwâ€‹(Î¸)â€‹(x)\mathrm{Cov}\_{w(\theta)}(x) singular along the intercept direction.
In practice, *remove the intercept column and keep the budget*, or equivalently keep the intercept but *fix its dual component to zero* (gauge fixing).
Numerically, this avoids singular normal equations and yields a well-posed Newton step on the Kâˆ’1K{-}1 dimensional exposure subspace.

###### Lemma 1 (Column-shift (translation) invariance).

For any dâˆˆâ„Kd\in\mathbb{R}^{K}, define Xâ€²:=Xâˆ’ğŸâ€‹dâŠ¤X^{\prime}:=X-\mathbf{1}d^{\top} and tâ€²:=tâˆ’dt^{\prime}:=t-d. Then the equality-feasible sets coincide:

|  |  |  |
| --- | --- | --- |
|  | {wâˆˆÎ”N:XâŠ¤â€‹w=t}={wâˆˆÎ”N:Xâ€²â£âŠ¤â€‹w=tâ€²}.\{w\in\Delta^{N}:\ X^{\top}w=t\}\;=\;\{w\in\Delta^{N}:\ X^{\prime\top}w=t^{\prime}\}. |  |

*Proof.* One line: Xâ€²â£âŠ¤â€‹w=(Xâˆ’ğŸâ€‹dâŠ¤)âŠ¤â€‹w=XâŠ¤â€‹wâˆ’dâ€‹(ğŸâŠ¤â€‹w)=tâˆ’dX^{\prime\top}w=(X-\mathbf{1}d^{\top})^{\top}w=X^{\top}w-d(\mathbf{1}^{\top}w)=t-d since ğŸâŠ¤â€‹w=1\mathbf{1}^{\top}w=1.

## 3 Duality and Exponential-Family Form

### 3.1 Exponential Tilt (Equality Case)

With only ğ—âŠ¤â€‹ğ°=t\mathbf{X}^{\top}\mathbf{w}=t, the KKT conditions give the exponential-family solution

|  |  |  |  |
| --- | --- | --- | --- |
|  | wiâ€‹(Î¸)=biâ€‹expâ¡(Î¸âŠ¤â€‹ğ±i)âˆ‘jbjâ€‹expâ¡(Î¸âŠ¤â€‹ğ±j).w\_{i}(\theta)=\frac{b\_{i}\exp(\theta^{\top}\mathbf{x}\_{i})}{\sum\_{j}b\_{j}\exp(\theta^{\top}\mathbf{x}\_{j})}. |  | (2) |

The dual concave objective reads

|  |  |  |  |
| --- | --- | --- | --- |
|  | Lâ€‹(Î¸)=Î¸âŠ¤â€‹tâˆ’logâ¡(âˆ‘ibiâ€‹eÎ¸âŠ¤â€‹ğ±i),L(\theta)=\theta^{\top}t-\log\!\Big(\sum\_{i}b\_{i}e^{\theta^{\top}\mathbf{x}\_{i}}\Big), |  | (3) |

with

|  |  |  |
| --- | --- | --- |
|  | âˆ‡Lâ€‹(Î¸)=tâˆ’ğ”¼wâ€‹(Î¸)â€‹[ğ±],âˆ‡2Lâ€‹(Î¸)=âˆ’Covwâ€‹(Î¸)â€‹(ğ±).\nabla L(\theta)=t-\mathbb{E}\_{w(\theta)}[\mathbf{x}],\qquad\nabla^{2}L(\theta)=-\mathrm{Cov}\_{w(\theta)}(\mathbf{x}). |  |

Strict concavity holds on the span where Covwâ€‹(Î¸)â€‹(ğ±)â‰»0\mathrm{Cov}\_{w(\theta)}(\mathbf{x})\succ 0 [[9](https://arxiv.org/html/2510.24607v1#bib.bib9)].

### 3.2 Sensitivity to Targets

Let Î¸â‹†\theta^{\star} maximize LL, Î¼=ğ”¼wâ‹†â€‹[ğ±]\mu=\mathbb{E}\_{w^{\star}}[\mathbf{x}], and Î£=Covwâ‹†â€‹(ğ±)\Sigma=\mathrm{Cov}\_{w^{\star}}(\mathbf{x}). Then

|  |  |  |  |
| --- | --- | --- | --- |
|  | âˆ‚Î¸â‹†âˆ‚t=Î£âˆ’1,âˆ‚wiâ‹†âˆ‚t=wiâ‹†â€‹(ğ±iâˆ’Î¼)âŠ¤â€‹Î£âˆ’1.\frac{\partial\theta^{\star}}{\partial t}=\Sigma^{-1},\qquad\frac{\partial w\_{i}^{\star}}{\partial t}=w\_{i}^{\star}(\mathbf{x}\_{i}-\mu)^{\top}\Sigma^{-1}. |  | (4) |

### 3.3 Elastic Targets (Soft Penalty): Dual, Uniqueness, and Sensitivity

Consider the elastic objective

|  |  |  |
| --- | --- | --- |
|  | minwâˆˆÎ”NDKL(wâˆ¥b)+Î»soft2âˆ¥XâŠ¤wâˆ’tâˆ¥22.\min\_{w\in\Delta^{N}}\ D\_{\mathrm{KL}}\!\left(w\middle\|b\right)+\frac{\lambda\_{\mathrm{soft}}}{2}\,\|X^{\top}w-t\|\_{2}^{2}. |  |

Its Fenchel dual is

|  |  |  |
| --- | --- | --- |
|  | maxÎ¸âˆˆâ„KLelâ€‹(Î¸):=Î¸âŠ¤â€‹tâˆ’logâ€‹âˆ‘ibiâ€‹eÎ¸âŠ¤â€‹xiâˆ’12â€‹Î»softâ€‹â€–Î¸â€–22,\max\_{\theta\in\mathbb{R}^{K}}\quad L\_{\mathrm{el}}(\theta):=\theta^{\top}t-\log\!\sum\_{i}b\_{i}e^{\theta^{\top}x\_{i}}-\frac{1}{2\lambda\_{\mathrm{soft}}}\|\theta\|\_{2}^{2}, |  |

so the optimizer is unique and the primal solution remains the exponential tilt wiâˆbiâ€‹eÎ¸â‹†âŠ¤â€‹xiw\_{i}\propto b\_{i}e^{\theta^{\star\top}x\_{i}}. The gradient/Hessian of LelL\_{\mathrm{el}} are

|  |  |  |
| --- | --- | --- |
|  | âˆ‡Lelâ€‹(Î¸)=tâˆ’ğ”¼wâ€‹(Î¸)â€‹[x]âˆ’1Î»softâ€‹Î¸,âˆ‡2Lelâ€‹(Î¸)=âˆ’Covwâ€‹(Î¸)â€‹(x)âˆ’1Î»softâ€‹I.\nabla L\_{\mathrm{el}}(\theta)=t-\mathbb{E}\_{w(\theta)}[x]-\tfrac{1}{\lambda\_{\mathrm{soft}}}\theta,\qquad\nabla^{2}L\_{\mathrm{el}}(\theta)=-\mathrm{Cov}\_{w(\theta)}(x)-\tfrac{1}{\lambda\_{\mathrm{soft}}}I. |  |

###### Theorem 1 (Elastic sensitivity).

At Î¸elâ‹†\theta^{\star}\_{\mathrm{el}}, we have

|  |  |  |
| --- | --- | --- |
|  | âˆ‚Î¸elâ‹†âˆ‚t=(Î£+1Î»softâ€‹I)âˆ’1,âˆ‚wâ‹†âˆ‚t=diagâ€‹(wâ‹†)â€‹(Xâˆ’ğŸâ€‹Î¼âŠ¤)â€‹(Î£+1Î»softâ€‹I)âˆ’1.\frac{\partial\theta^{\star}\_{\mathrm{el}}}{\partial t}=\big(\Sigma+\tfrac{1}{\lambda\_{\mathrm{soft}}}I\big)^{-1},\qquad\frac{\partial w^{\star}}{\partial t}=\mathrm{diag}(w^{\star})\,(X-\mathbf{1}\mu^{\top})\,\big(\Sigma+\tfrac{1}{\lambda\_{\mathrm{soft}}}I\big)^{-1}. |  |

### 3.4 Robust Target Sets via Support Functions

Relax the equality to a convex set: XâŠ¤â€‹wâˆˆt0+ğ’°X^{\top}w\in t\_{0}+\mathcal{U} for a closed, convex, centrally-symmetric set ğ’°âŠ‚â„K\mathcal{U}\subset\mathbb{R}^{K}. Then

|  |  |  |
| --- | --- | --- |
|  | minwâˆˆÎ”NDKL(wâˆ¥b)+Î¹t0+ğ’°(XâŠ¤w)âŸºmaxÎ¸âˆˆâ„KLrob(Î¸):=Ïƒt0+ğ’°(Î¸)âˆ’logâˆ‘ibieÎ¸âŠ¤â€‹xi,\min\_{w\in\Delta^{N}}\ D\_{\mathrm{KL}}\!\left(w\middle\|b\right)+\iota\_{\,t\_{0}+\mathcal{U}}(X^{\top}w)\quad\Longleftrightarrow\quad\max\_{\theta\in\mathbb{R}^{K}}\ L\_{\mathrm{rob}}(\theta):=\sigma\_{t\_{0}+\mathcal{U}}(\theta)-\log\!\sum\_{i}b\_{i}e^{\theta^{\top}x\_{i}}, |  |

where ÏƒSâ€‹(Î¸)\sigma\_{S}(\theta) is the support function. In particular,

|  |  |  |
| --- | --- | --- |
|  | ğ’°={u:â€–uâ€–2â‰¤Ï}â‡’Ïƒt0+ğ’°â€‹(Î¸)=Î¸âŠ¤â€‹t0+Ïâ€‹â€–Î¸â€–2;ğ’°={u:â€–uâ€–âˆâ‰¤Ï}â‡’Ïƒt0+ğ’°â€‹(Î¸)=Î¸âŠ¤â€‹t0+Ïâ€‹â€–Î¸â€–1.\mathcal{U}=\{u:\|u\|\_{2}\leq\rho\}\Rightarrow\sigma\_{t\_{0}+\mathcal{U}}(\theta)=\theta^{\top}t\_{0}+\rho\|\theta\|\_{2};\quad\mathcal{U}=\{u:\|u\|\_{\infty}\leq\rho\}\Rightarrow\sigma\_{t\_{0}+\mathcal{U}}(\theta)=\theta^{\top}t\_{0}+\rho\|\theta\|\_{1}. |  |

The primal optimizer keeps the exponential tilt wiâˆbiâ€‹eÎ¸â‹†âŠ¤â€‹xiw\_{i}\propto b\_{i}e^{\theta^{\star\top}x\_{i}}.

## 4 Algorithms

### 4.1 EGMU-Newton: Damped Dual Newton Ascent (Equality Core)

We solve ([3](https://arxiv.org/html/2510.24607v1#S3.E3 "In 3.1 Exponential Tilt (Equality Case) â€£ 3 Duality and Exponential-Family Form â€£ Entropy-Guided Multiplicative Updates: KL Projections for Multi-Factor Target Exposures")) via Newton steps with backtracking. Each iteration forms Î¼=ğ”¼wâ€‹(Î¸)â€‹[ğ±]\mu=\mathbb{E}\_{w(\theta)}[\mathbf{x}] and Î£=Covwâ€‹(Î¸)â€‹(ğ±)\Sigma=\mathrm{Cov}\_{w(\theta)}(\mathbf{x}) in Oâ€‹(Nâ€‹K)O(NK) and Oâ€‹(Nâ€‹K2)O(NK^{2}), and solves Î£â€‹Î”=g\Sigma\,\Delta=g with g=tâˆ’Î¼g=t-\mu.

Algorithm 1  EGMU-Newton (Equality Case, LogSumExp-stable)

1:Input: bâˆˆÎ”Nb\in\Delta^{N}, Xâˆˆâ„NÃ—KX\in\mathbb{R}^{N\times K}, target tt, tol Îµ\varepsilon, ridge Î´â‰¥0\delta\geq 0

2:Initialize Î¸â†0\theta\leftarrow 0

3:while â€–âˆ‡Lâ€‹(Î¸)â€–2>Îµ\|\nabla L(\theta)\|\_{2}>\varepsilon do

4:â€ƒâ€‚Scores: siâ†Î¸âŠ¤â€‹xis\_{i}\leftarrow\theta^{\top}x\_{i};â€ƒmâ†maxiâ¡sim\leftarrow\max\_{i}s\_{i}

5:â€ƒâ€‚Log-sum-exp: logâ¡Zâ†logâ€‹âˆ‘ibiâ€‹expâ¡(siâˆ’m)+m\log Z\leftarrow\log\!\sum\_{i}b\_{i}\exp(s\_{i}-m)+m

6:â€ƒâ€‚Weights: wiâ†biâ€‹expâ¡(siâˆ’logâ¡Z)w\_{i}\leftarrow b\_{i}\exp(s\_{i}-\log Z)

7:â€ƒâ€‚Moments: Î¼â†XâŠ¤â€‹w\mu\leftarrow X^{\top}w;â€ƒgâ†tâˆ’Î¼g\leftarrow t-\mu

8:â€ƒâ€‚Covariance: Î£â†âˆ‘iwiâ€‹(xiâˆ’Î¼)â€‹(xiâˆ’Î¼)âŠ¤\Sigma\leftarrow\sum\_{i}w\_{i}(x\_{i}-\mu)(x\_{i}-\mu)^{\top}

9:â€ƒâ€‚Solve: (Î£+Î´â€‹I)â€‹Î”=g(\Sigma+\delta I)\Delta=g âŠ³\triangleright Cholesky; Î´\delta only if needed

10:â€ƒâ€‚Line search: Armijo backtracking with parameters (c,Î²)(c,\beta)

11:â€ƒâ€‚Î¸â†Î¸+Î±â€‹Î”\theta\leftarrow\theta+\alpha\Delta

12:end while

13:Return wâ€‹(Î¸)w(\theta) via ([2](https://arxiv.org/html/2510.24607v1#S3.E2 "In 3.1 Exponential Tilt (Equality Case) â€£ 3 Duality and Exponential-Family Form â€£ Entropy-Guided Multiplicative Updates: KL Projections for Multi-Factor Target Exposures"))

#### Line-search parameters.

Choose câˆˆ(10âˆ’6,10âˆ’1)c\in(10^{-6},10^{-1}) and Î²âˆˆ(0,1)\beta\in(0,1) (e.g., Î²=0.5\beta=0.5); pick the largest Î±=Î²m\alpha=\beta^{m} such that
Lâ€‹(Î¸+Î±â€‹Î”)â‰¥Lâ€‹(Î¸)+câ€‹Î±â€‹gâŠ¤â€‹Î”L(\theta+\alpha\Delta)\geq L(\theta)+c\,\alpha\,g^{\top}\Delta.

#### Elastic variant (R1).

For Lelâ€‹(Î¸)L\_{\mathrm{el}}(\theta), reuse AlgorithmÂ [1](https://arxiv.org/html/2510.24607v1#alg1 "Algorithm 1 â€£ 4.1 EGMU-Newton: Damped Dual Newton Ascent (Equality Core) â€£ 4 Algorithms â€£ Entropy-Guided Multiplicative Updates: KL Projections for Multi-Factor Target Exposures") with

|  |  |  |
| --- | --- | --- |
|  | gâ†tâˆ’Î¼âˆ’1Î»softâ€‹Î¸,Î£â†Î£+1Î»softâ€‹I.g\leftarrow t-\mu-\tfrac{1}{\lambda\_{\mathrm{soft}}}\theta,\qquad\Sigma\leftarrow\Sigma+\tfrac{1}{\lambda\_{\mathrm{soft}}}I. |  |

This preserves global convergence and improves conditioning via the I/Î»softI/\lambda\_{\mathrm{soft}} term.

### 4.2 KL-Projections for Equalities: IPF / One-Dimensional Solves

For a single equality aâŠ¤â€‹w=Ï„a^{\top}w=\tau, the KL projection of uu onto that hyperplane has closed form

|  |  |  |
| --- | --- | --- |
|  | wâ€‹(Î±)âˆuâŠ™expâ¡(Î±â€‹a),withÂ â€‹Ï•â€‹(Î±):=aâŠ¤â€‹wâ€‹(Î±)âˆ’Ï„=0,w(\alpha)\ \propto\ u\odot\exp(\alpha a),\quad\text{with }\ \phi(\alpha):=a^{\top}w(\alpha)-\tau=0, |  |

where Ï•\phi is strictly monotone since Ï•â€²â€‹(Î±)=Varwâ€‹(Î±)â€‹(a)>0\phi^{\prime}(\alpha)=\mathrm{Var}\_{w(\alpha)}(a)>0 unless aa is degenerate. Root Î±\alpha is found by bisection/Brent in Oâ€‹(N)O(N).
Cycling over k=1,â€¦,Kk=1,\dots,K yields IPF/GIS; it converges to the KL minimizer under feasibility [[6](https://arxiv.org/html/2510.24607v1#bib.bib6), [7](https://arxiv.org/html/2510.24607v1#bib.bib7)].

Algorithm 2  EGMU-IPF (Equalities via KL One-Dimensional Projections)

1:Input: prior uâˆˆÎ”Nu\in\Delta^{N}, constraints {(ak,Ï„k)}k=1K\{(a\_{k},\tau\_{k})\}\_{k=1}^{K}, tol Îµ\varepsilon

2:wâ†uw\leftarrow u

3:repeat

4:â€ƒâ€‚for k=1k=1 to KK do

5:â€ƒâ€ƒâ€ƒFind Î±\alpha s.t. akâŠ¤â€‹(normalizeâ€‹(wâŠ™eÎ±â€‹ak))=Ï„ka\_{k}^{\top}\big(\mathrm{normalize}(w\odot e^{\alpha a\_{k}})\big)=\tau\_{k} âŠ³\triangleright bisection/Brent

6:â€ƒâ€ƒâ€ƒwâ†normalizeâ€‹(wâŠ™eÎ±â€‹ak)w\leftarrow\mathrm{normalize}(w\odot e^{\alpha a\_{k}})

7:â€ƒâ€‚end for

8:until maxkâ¡|akâŠ¤â€‹wâˆ’Ï„k|â‰¤Îµ\max\_{k}|a\_{k}^{\top}w-\tau\_{k}|\leq\varepsilon

9:Return ww

### 4.3 KL-Projections for Inequalities: Bregmanâ€“Dykstra

For a half-space â„‹={w:aâŠ¤â€‹wâ‰¤Ï„}\mathcal{H}=\{w:a^{\top}w\leq\tau\}, the KL projection of uu onto â„‹\mathcal{H} is either uu (if feasible) or wâ€‹(Î»)âˆuâŠ™eâˆ’Î»â€‹aw(\lambda)\propto u\odot e^{-\lambda a} with Î»â‰¥0\lambda\geq 0 chosen so that aâŠ¤â€‹wâ€‹(Î»)=Ï„a^{\top}w(\lambda)=\tau.
Bregmanâ€“Dykstra cycles projections onto {ğ’j}\{\mathcal{C}\_{j}\} with correction terms {qj}\{q\_{j}\} and converges to the KL-projection onto âˆ©jğ’j\cap\_{j}\mathcal{C}\_{j} [[3](https://arxiv.org/html/2510.24607v1#bib.bib3)].
Moreover, since ddâ€‹Î»â€‹aâŠ¤â€‹wâ€‹(Î»)=âˆ’Varwâ€‹(Î»)â€‹(a)â‰¤0\tfrac{d}{d\lambda}\,a^{\top}w(\lambda)=-\mathrm{Var}\_{w(\lambda)}(a)\leq 0, the residual aâŠ¤â€‹wâ€‹(Î»)âˆ’Ï„a^{\top}w(\lambda)-\tau is strictly decreasing in Î»\lambda (unless aa is degenerate), so the one-dimensional root-finding is robust and unimodal.

Algorithm 3  EGMU-Projection (Inequalities via KL Bregmanâ€“Dykstra)

1:Input: prior uâˆˆÎ”Nu\in\Delta^{N}, sets {ğ’j}j=1J\{\mathcal{C}\_{j}\}\_{j=1}^{J} (equalities/half-spaces), tol Îµ\varepsilon

2:wâ†uw\leftarrow u;â€ƒqjâ†ğŸq\_{j}\leftarrow\mathbf{1} for all jj

3:repeat

4:â€ƒâ€‚for j=1j=1 to JJ do

5:â€ƒâ€ƒâ€ƒyâ†normalizeâ€‹(wâŠ™qj)y\leftarrow\mathrm{normalize}(w\odot q\_{j})

6:â€ƒâ€ƒâ€ƒzâ†Î ğ’jKLâ€‹(y)z\leftarrow\Pi^{\mathrm{KL}}\_{\mathcal{C}\_{j}}(y) âŠ³\triangleright closed-form or 1-D solve as above

7:â€ƒâ€ƒâ€ƒqjâ†(wâŠ™qj)âŠ˜zq\_{j}\leftarrow(w\odot q\_{j})\oslash z âŠ³\triangleright elementwise

8:â€ƒâ€ƒâ€ƒwâ†zw\leftarrow z

9:â€ƒâ€‚end for

10:until constraint violations â‰¤Îµ\leq\varepsilon

11:Return ww

### 4.4 EGMU-ProxGrad (Robust Dual, R2)

For Lrobâ€‹(Î¸)=Î¸âŠ¤â€‹t0âˆ’logâ€‹âˆ‘ibiâ€‹eÎ¸âŠ¤â€‹xiâŸsmooth concaveÂ â€‹fâ€‹(Î¸)+Ïƒğ’°â€‹(Î¸)âŸconvex,L\_{\mathrm{rob}}(\theta)=\underbrace{\theta^{\top}t\_{0}-\log\!\sum\_{i}b\_{i}e^{\theta^{\top}x\_{i}}}\_{\text{smooth concave }f(\theta)}\ +\ \underbrace{\sigma\_{\mathcal{U}}(\theta)}\_{\text{convex}}, apply proximal gradient ascent

|  |  |  |
| --- | --- | --- |
|  | Î¸+=proxÎ·â€‹Ïƒğ’°â€‹(Î¸+Î·â€‹âˆ‡fâ€‹(Î¸)),withâˆ‡fâ€‹(Î¸)=t0âˆ’ğ”¼wâ€‹(Î¸)â€‹[x].\theta^{+}\;=\;\mathrm{prox}\_{\eta\,\sigma\_{\mathcal{U}}}\big(\theta+\eta\,\nabla f(\theta)\big),\quad\text{with}\quad\nabla f(\theta)=t\_{0}-\mathbb{E}\_{w(\theta)}[x]. |  |

By Moreauâ€™s identity, proxÎ·â€‹Ïƒğ’°â€‹(z)=zâˆ’Î·â€‹Î ğ’°â€‹(z/Î·)\mathrm{prox}\_{\eta\,\sigma\_{\mathcal{U}}}(z)=z-\eta\,\Pi\_{\mathcal{U}}(z/\eta) (see, e.g., [2](https://arxiv.org/html/2510.24607v1#bib.bib2)), where Î ğ’°\Pi\_{\mathcal{U}} is the Euclidean projection onto ğ’°\mathcal{U} (closed forms: â„“2\ell\_{2} ball â‡’\Rightarrow radial shrink; â„“âˆ\ell\_{\infty} box â‡’\Rightarrow coordinatewise clip).

Algorithm 4  EGMU-ProxGrad (Robust Dual with â„“2/â„“âˆ\ell\_{2}/\ell\_{\infty} target sets)

1:Input: b,X,t0b,X,t\_{0}, convex ğ’°\mathcal{U} (e.g., â„“2\ell\_{2} ball radius Ï\rho or â„“âˆ\ell\_{\infty} box), step Î·>0\eta>0, tol Îµ\varepsilon

2:Initialize Î¸â†0\theta\leftarrow 0

3:repeat

4:â€ƒâ€‚wiâˆbiâ€‹eÎ¸âŠ¤â€‹xiw\_{i}\propto b\_{i}e^{\theta^{\top}x\_{i}}; normalize ww

5:â€ƒâ€‚gâ†t0âˆ’XâŠ¤â€‹wg\leftarrow t\_{0}-X^{\top}w âŠ³\triangleright =âˆ‡fâ€‹(Î¸)=\nabla f(\theta)

6:â€ƒâ€‚zâ†Î¸+Î·â€‹gz\leftarrow\theta+\eta g

7:â€ƒâ€‚Prox:â€ƒÎ¸â†zâˆ’Î·â€‹Î ğ’°â€‹(z/Î·)\theta\leftarrow z-\eta\,\Pi\_{\mathcal{U}}(z/\eta)

8:until â€–âˆ‡fâ€‹(Î¸)âˆ’uâ€–2â‰¤Îµ\|\nabla f(\theta)-u\|\_{2}\leq\varepsilon for some uâˆˆâˆ‚Ïƒğ’°â€‹(Î¸)u\in\partial\sigma\_{\mathcal{U}}(\theta)

9:Return wâ€‹(Î¸)w(\theta)

#### When to use which solver.

Use AlgorithmÂ [1](https://arxiv.org/html/2510.24607v1#alg1 "Algorithm 1 â€£ 4.1 EGMU-Newton: Damped Dual Newton Ascent (Equality Core) â€£ 4 Algorithms â€£ Entropy-Guided Multiplicative Updates: KL Projections for Multi-Factor Target Exposures") for fast equality matching (small KK, large NN). Use the elastic variant in Â§[3.3](https://arxiv.org/html/2510.24607v1#S3.SS3 "3.3 Elastic Targets (Soft Penalty): Dual, Uniqueness, and Sensitivity â€£ 3 Duality and Exponential-Family Form â€£ Entropy-Guided Multiplicative Updates: KL Projections for Multi-Factor Target Exposures") when exact feasibility is difficult or undesirable. Use AlgorithmÂ [4](https://arxiv.org/html/2510.24607v1#alg4 "Algorithm 4 â€£ 4.4 EGMU-ProxGrad (Robust Dual, R2) â€£ 4 Algorithms â€£ Entropy-Guided Multiplicative Updates: KL Projections for Multi-Factor Target Exposures") for robust target sets (â„“2/â„“âˆ\ell\_{2}/\ell\_{\infty}) or when you want feasibility-by-construction via projections.

### 4.5 Path-Following via Sensitivity ODE (Module C)

For a target path tâ€‹(Î»)=t0+Î»â€‹Î”t(\lambda)=t\_{0}+\lambda\Delta, the optimal dual parameter satisfies the ODE

|  |  |  |
| --- | --- | --- |
|  | dâ€‹Î¸â€‹(Î»)dâ€‹Î»=(Î£â€‹(Î¸â€‹(Î»))+1Î»softâ€‹I)âˆ’1â€‹Î”,Î¸â€‹(0)=Î¸â‹†â€‹(t0),Î»âˆˆ[0,1],\frac{d\theta(\lambda)}{d\lambda}=\Big(\Sigma(\theta(\lambda))+\tfrac{1}{\lambda\_{\mathrm{soft}}}I\Big)^{-1}\Delta,\qquad\theta(0)=\theta^{\star}(t\_{0}),\ \lambda\in[0,1], |  |

with Î»soft=âˆ\lambda\_{\mathrm{soft}}=\infty for the equality case. The path is unique under Î£âª°mâ€‹I\Sigma\succeq mI and locally Lipschitz Hessian; for robust sets it is piecewise smooth (kinks when the active face of ğ’°\mathcal{U} changes).

Algorithm 5  EGMU-Path (Homotopy Integrator)

1:Input: b,X,t0,Î”b,X,t\_{0},\Delta, (optional) Î»soft\lambda\_{\mathrm{soft}}, step h>0h>0

2:Initialize Î¸â†Î¸â‹†â€‹(t0)\theta\leftarrow\theta^{\star}(t\_{0}) (or 0)

3:for Î»=0\lambda=0 to 11 step hh do

4:â€ƒâ€‚wiâˆbiâ€‹eÎ¸âŠ¤â€‹xiw\_{i}\propto b\_{i}e^{\theta^{\top}x\_{i}}; normalize ww

5:â€ƒâ€‚Î¼â†XâŠ¤â€‹w\mu\leftarrow X^{\top}w;â€ƒÎ£â†âˆ‘iwiâ€‹(xiâˆ’Î¼)â€‹(xiâˆ’Î¼)âŠ¤\Sigma\leftarrow\sum\_{i}w\_{i}(x\_{i}-\mu)(x\_{i}-\mu)^{\top}

6:â€ƒâ€‚Mâ†Î£+1Î»softâ€‹IM\leftarrow\Sigma+\tfrac{1}{\lambda\_{\mathrm{soft}}}I (take 1/Î»soft=01/\lambda\_{\mathrm{soft}}=0 if equality)

7:â€ƒâ€‚Euler/RK2:â€ƒÎ¸â†Î¸+hâ€‹Mâˆ’1â€‹Î”\theta\leftarrow\theta+h\,M^{-1}\Delta (or a second-order variant)

8:end for

9:Return the path {Î¸â€‹(Î»),wâ€‹(Î»)}\{\theta(\lambda),w(\lambda)\}

## 5 Theoretical Guarantees

###### Theorem 2 (Existence and uniqueness).

Under feasibility (Slater) and strictly positive bb, problem ([1](https://arxiv.org/html/2510.24607v1#S2.E1 "In 2.1 KL-Minimization with Linear Constraints â€£ 2 Problem, Feasibility, and Geometry â€£ Entropy-Guided Multiplicative Updates: KL Projections for Multi-Factor Target Exposures")) admits a unique optimizer. If tâˆˆrelintâ€‹convâ€‹{ğ±i}t\in\mathrm{relint}\,\mathrm{conv}\{\mathbf{x}\_{i}\} and no inequality is active at the boundary, the optimizer is strictly positive.

###### Theorem 3 (Dual structure and strict concavity).

Lâ€‹(Î¸)L(\theta) in ([3](https://arxiv.org/html/2510.24607v1#S3.E3 "In 3.1 Exponential Tilt (Equality Case) â€£ 3 Duality and Exponential-Family Form â€£ Entropy-Guided Multiplicative Updates: KL Projections for Multi-Factor Target Exposures")) is concave with âˆ‡Lâ€‹(Î¸)=tâˆ’ğ”¼wâ€‹(Î¸)â€‹[ğ±]\nabla L(\theta)=t-\mathbb{E}\_{w(\theta)}[\mathbf{x}] and âˆ‡2Lâ€‹(Î¸)=âˆ’Covwâ€‹(Î¸)â€‹(ğ±)\nabla^{2}L(\theta)=-\mathrm{Cov}\_{w(\theta)}(\mathbf{x}). On the subspace where Covwâ€‹(Î¸)â€‹(ğ±)â‰»0\mathrm{Cov}\_{w(\theta)}(\mathbf{x})\succ 0, LL is strictly concave, hence Î¸â‹†\theta^{\star} is unique and ([2](https://arxiv.org/html/2510.24607v1#S3.E2 "In 3.1 Exponential Tilt (Equality Case) â€£ 3 Duality and Exponential-Family Form â€£ Entropy-Guided Multiplicative Updates: KL Projections for Multi-Factor Target Exposures")) yields the unique primal optimizer.

###### Theorem 4 (Sensitivity).

At the optimum, âˆ‚Î¸â‹†âˆ‚t=Covwâ‹†â€‹(ğ±)âˆ’1\dfrac{\partial\theta^{\star}}{\partial t}=\mathrm{Cov}\_{w^{\star}}(\mathbf{x})^{-1} and âˆ‚wâ‹†âˆ‚t=diagâ€‹(wâ‹†)â€‹(Xâˆ’ğŸâ€‹Î¼âŠ¤)â€‹Covwâ‹†â€‹(ğ±)âˆ’1\dfrac{\partial w^{\star}}{\partial t}=\mathrm{diag}(w^{\star})\,(X-\mathbf{1}\mu^{\top})\,\mathrm{Cov}\_{w^{\star}}(\mathbf{x})^{-1} with Î¼=ğ”¼wâ‹†â€‹[ğ±]\mu=\mathbb{E}\_{w^{\star}}[\mathbf{x}].

###### Theorem 5 (Elastic dual: strong concavity and sensitivity).

Lelâ€‹(Î¸)L\_{\mathrm{el}}(\theta) is strongly concave with parameter 1/Î»soft1/\lambda\_{\mathrm{soft}}; the maximizer is unique and TheoremÂ [1](https://arxiv.org/html/2510.24607v1#Thmtheorem1 "Theorem 1 (Elastic sensitivity). â€£ 3.3 Elastic Targets (Soft Penalty): Dual, Uniqueness, and Sensitivity â€£ 3 Duality and Exponential-Family Form â€£ Entropy-Guided Multiplicative Updates: KL Projections for Multi-Factor Target Exposures") holds.

###### Proposition 2 (Robust dual: concavity and optimality).

Lrobâ€‹(Î¸)=Ïƒt0+ğ’°â€‹(Î¸)âˆ’logâ€‹âˆ‘ibiâ€‹eÎ¸âŠ¤â€‹xiL\_{\mathrm{rob}}(\theta)=\sigma\_{t\_{0}+\mathcal{U}}(\theta)-\log\sum\_{i}b\_{i}e^{\theta^{\top}x\_{i}} is concave. Any maximizer Î¸â‹†\theta^{\star} yields the exponential tilt wiâ‹†âˆbiâ€‹eÎ¸â‹†âŠ¤â€‹xiw\_{i}^{\star}\propto b\_{i}e^{\theta^{\star\top}x\_{i}}. For ğ’°\mathcal{U} an â„“2\ell\_{2} ball or â„“âˆ\ell\_{\infty} box, AlgorithmÂ [4](https://arxiv.org/html/2510.24607v1#alg4 "Algorithm 4 â€£ 4.4 EGMU-ProxGrad (Robust Dual, R2) â€£ 4 Algorithms â€£ Entropy-Guided Multiplicative Updates: KL Projections for Multi-Factor Target Exposures") converges to a maximizer under standard step-size/backtracking rules (Lipschitz gradient of ff).

###### Theorem 6 (Convergence of EGMU-Newton).

With standard backtracking/damping, Newton ascent on LL is globally convergent; if Covwâ€‹(Î¸)â€‹(ğ±)âª°mâ€‹I\mathrm{Cov}\_{w(\theta)}(\mathbf{x})\succeq mI and âˆ‡2L\nabla^{2}L is Lipschitz in a neighborhood of Î¸â‹†\theta^{\star}, the rate is locally quadratic.

###### Theorem 7 (Convergence of projection schemes).

(*i*) IPF/one-dimensional KL projections cycling over equalities converge to the unique KL minimizer when feasible.
(*ii*) Bregmanâ€“Dykstra with KL distance over finitely many closed convex sets (equalities and half-spaces) converges to the KL projection onto their intersection.

###### Remark 2 (Complexity).

Per Newton step: Oâ€‹(Nâ€‹K)O(NK) + Oâ€‹(Nâ€‹K2)O(NK^{2}) to form moments and covariance, and Oâ€‹(K3)O(K^{3}) to solve the KÃ—KK\times K system.
Each 1-D projection is Oâ€‹(N)O(N) per function/derivative evaluation (bisection/Brent). Memory footprint is Oâ€‹(Nâ€‹K)O(NK).

## 6 Implementation Notes (Stability and Scaling)

* â€¢

  Stability: always use LogSumExp for partition functions; center exposures to reduce conditioning; add small ridge Î´â€‹I\delta I when Î£\Sigma is nearly singular.
* â€¢

  Elastic targets (R1): the I/Î»softI/\lambda\_{\mathrm{soft}} term improves conditioning and ensures strong concavity in the dual; recommended defaults Î»softâˆˆ[10,103]\lambda\_{\mathrm{soft}}\in[10,10^{3}] when feasibility is uncertain.
* â€¢

  Robust sets (R2): for â„“2/â„“âˆ\ell\_{2}/\ell\_{\infty} sets, use AlgorithmÂ [4](https://arxiv.org/html/2510.24607v1#alg4 "Algorithm 4 â€£ 4.4 EGMU-ProxGrad (Robust Dual, R2) â€£ 4 Algorithms â€£ Entropy-Guided Multiplicative Updates: KL Projections for Multi-Factor Target Exposures"); for general ğ’°\mathcal{U}, combine projection oracles (or Bregmanâ€“Dykstra in tt-space) with Moreau identity.
* â€¢

  Cap/box constraints in ww: half-space KL projections have 1-D solves with monotone residuals (ddâ€‹Î»â€‹aâŠ¤â€‹wâ€‹(Î»)=âˆ’Varwâ€‹(Î»)â€‹(a)â‰¤0\frac{d}{d\lambda}a^{\top}w(\lambda)=-\mathrm{Var}\_{w(\lambda)}(a)\leq 0), hence root-finding is unimodal/robust.
* â€¢

  Default solver parameters: Îµ=10âˆ’8\varepsilon=10^{-8}, c=10âˆ’4c=10^{-4}, Î²=0.5\beta=0.5, Î´=maxâ¡(10âˆ’10,10âˆ’6â€‹trâ€‹(Î£)/K)\delta=\max(10^{-10},10^{-6}\,\mathrm{tr}(\Sigma)/K).

## 7 Extension: Multi-Period and Turnover Regularization (Brief)

At time tt, given previous weights ptâˆ’1p\_{t-1}, consider

|  |  |  |
| --- | --- | --- |
|  | minwtâˆˆÎ”NDKL(wtâˆ¥b)+Î³DKL(wtâˆ¥ptâˆ’1)s.t.XâŠ¤wt=Ï„t,Awtâ‰¤c.\min\_{w\_{t}\in\Delta^{N}}\ D\_{\mathrm{KL}}\!\left(w\_{t}\middle\|b\right)\;+\;\gamma\,D\_{\mathrm{KL}}\!\left(w\_{t}\middle\|p\_{t-1}\right)\quad\text{s.t.}\quad X^{\top}w\_{t}=\tau\_{t},\;Aw\_{t}\leq c. |  |

This is equivalent (up to an additive constant) to (1+Î³)DKL(wtâˆ¥b~t)(1+\gamma)\,D\_{\mathrm{KL}}\!\left(w\_{t}\middle\|\tilde{b}\_{t}\right) with the *effective prior*

|  |  |  |
| --- | --- | --- |
|  | b~t,iâˆbi11+Î³â€‹ptâˆ’1,iÎ³1+Î³,\tilde{b}\_{t,i}\ \propto\ b\_{i}^{\frac{1}{1+\gamma}}\;p\_{t-1,i}^{\frac{\gamma}{1+\gamma}}, |  |

hence the solution remains an exponential tilt wt,iâˆb~t,iâ€‹expâ¡(Î¸tâŠ¤â€‹xi)w\_{t,i}\propto\tilde{b}\_{t,i}\exp(\theta\_{t}^{\top}x\_{i}) and all dual/algorithmic machinery is unchanged after bâ†b~tb\leftarrow\tilde{b}\_{t}.
If an explicit turnover budget is desired, one may add linearized constraints or standard split variables to encode â„“1\ell\_{1}-type variation limits, which fit directly into the KL-projection (Bregmanâ€“Dykstra) framework.

## 8 Related Work

#### Information projection and exponential families.

Our formulation is a classical II-projection (minimization of KL under linear moment constraints),
which yields exponential-family solutions and a concave dual with covariance Hessian; see
CsiszÃ¡r [[6](https://arxiv.org/html/2510.24607v1#bib.bib6)] for the geometry of II-divergence, Cover and Thomas [[5](https://arxiv.org/html/2510.24607v1#bib.bib5)] for an information-theoretic treatment,
and Wainwright and Jordan [[9](https://arxiv.org/html/2510.24607v1#bib.bib9)] for the exponential-family viewpoint connecting gradients/Hessians with moments/covariances.

#### Iterative proportional fitting and Bregman projections.

For equality constraints, iterative proportional fitting / generalized iterative scaling (IPF/GIS)
provides a coordinate-wise Bregman projection method with convergence guarantees [[7](https://arxiv.org/html/2510.24607v1#bib.bib7), [6](https://arxiv.org/html/2510.24607v1#bib.bib6)].
For intersections of convex sets (equalities and half-spaces), Bregmanâ€“Dykstra cycles converge to the unique Bregman projection
onto the intersection [[3](https://arxiv.org/html/2510.24607v1#bib.bib3)].

#### Entropy pooling and portfolio engineering.

In portfolio applications, our setup parallels Entropy Pooling (EP), which applies cross-entropy
updating to scenario probabilities under linear â€œviewsâ€ [[8](https://arxiv.org/html/2510.24607v1#bib.bib8)]. EGMU adapts the same KL geometry to
*asset weights on the simplex* with *factor exposure* constraints, and makes the dual structure and sensitivity
explicitly operational for target-exposure construction.

#### Convex duality, support functions, and robustness.

The elastic and robust variants we study are standard Fenchelâ€“Rockafellar constructs: adding a squared penalty in the primal corresponds to
a Tikhonov (strongly concave) term in the dual; relaxing equalities to a convex target set yields a dual support function. These follow from
textbook convex analysis and duality [[4](https://arxiv.org/html/2510.24607v1#bib.bib4), Ch.Â 3â€“5], and integrate seamlessly with the exponential-family moment
structure reviewed by Wainwright and Jordan [[9](https://arxiv.org/html/2510.24607v1#bib.bib9)].

#### Optimization and numerical stability.

Our damped Newton method with backtracking and ridge regularization follows standard convex-optimization practice [[4](https://arxiv.org/html/2510.24607v1#bib.bib4)].
Implementation details (LogSumExp stabilization, covariance centering/ridge, and moment reuse) are tailored to large-NN, small-KK regimes
typical in factor construction.

## 9 Conclusion

EGMU frames target-exposure construction as KL minimization on the simplex with rigorous feasibility, uniqueness, dual structure, and sensitivity. We provide provably convergent solversâ€”dual Newton and KL projection (IPF/Bregmanâ€“Dykstra)â€”and *extend* the framework to elastic/robust targets with a shared dual-moment core and furnish a path-following ODE. This yields a principled, reproducible baseline requiring minimal empirical work.

## Aâ€ƒProofs and Technical Details

### A.1â€ƒProof of PropositionÂ [1](https://arxiv.org/html/2510.24607v1#Thmproposition1 "Proposition 1 (Feasibility and strict positivity). â€£ 2.2 Feasibility and Strict Positivity â€£ 2 Problem, Feasibility, and Geometry â€£ Entropy-Guided Multiplicative Updates: KL Projections for Multi-Factor Target Exposures")

Let ğ’³={ğ±i}i=1N\mathcal{X}=\{\mathbf{x}\_{i}\}\_{i=1}^{N}. Since wâˆˆÎ”Nw\in\Delta^{N} implies XâŠ¤â€‹w=âˆ‘iwiâ€‹ğ±iX^{\top}w=\sum\_{i}w\_{i}\mathbf{x}\_{i}, feasibility of XâŠ¤â€‹w=tX^{\top}w=t is equivalent to tâˆˆconvâ€‹(ğ’³)t\in\mathrm{conv}(\mathcal{X}). If tâˆˆrelintâ€‹convâ€‹(ğ’³)t\in\mathrm{relint}\,\mathrm{conv}(\mathcal{X}) and bi>0b\_{i}>0, the KL objective is essentially smooth and strictly convex on the relative interior of the simplex, so the unique minimizer satisfies wiâ‹†>0w\_{i}^{\star}>0 by standard Lagrange multiplier/KKT arguments. With inequalities Aâ€‹wâ‰¤cAw\leq c, feasibility is a convex polytope; infeasibility admits a Farkas certificate (see, e.g., [4](https://arxiv.org/html/2510.24607v1#bib.bib4), Ch.Â 5). â–¡\square

### A.2â€ƒExponential Family and Dual Structure

Consider the Lagrangian (equalities only)

|  |  |  |
| --- | --- | --- |
|  | â„’â€‹(w,Î»,Î½)=âˆ‘iwiâ€‹logâ¡wibi+Î»âŠ¤â€‹(XâŠ¤â€‹wâˆ’t)+Î½â€‹(ğŸâŠ¤â€‹wâˆ’1).\mathcal{L}(w,\lambda,\nu)=\sum\_{i}w\_{i}\log\frac{w\_{i}}{b\_{i}}+\lambda^{\top}(X^{\top}w-t)+\nu(\mathbf{1}^{\top}w-1). |  |

Stationarity in wiw\_{i} gives logâ¡wiâˆ’logâ¡bi+Î»âŠ¤â€‹xi+Î½+1=0\log w\_{i}-\log b\_{i}+\lambda^{\top}x\_{i}+\nu+1=0, hence

|  |  |  |
| --- | --- | --- |
|  | wiâ€‹(Î¸)=biâ€‹eÎ¸âŠ¤â€‹xiâˆ‘jbjâ€‹eÎ¸âŠ¤â€‹xj,Î¸:=âˆ’Î».w\_{i}(\theta)=\frac{b\_{i}e^{\theta^{\top}x\_{i}}}{\sum\_{j}b\_{j}e^{\theta^{\top}x\_{j}}},\quad\theta:=-\lambda. |  |

Substituting into the Lagrangian yields the dual Lâ€‹(Î¸)=Î¸âŠ¤â€‹tâˆ’logâ€‹âˆ‘ibiâ€‹eÎ¸âŠ¤â€‹xiL(\theta)=\theta^{\top}t-\log\sum\_{i}b\_{i}e^{\theta^{\top}x\_{i}}. Differentiating under the softmax,

|  |  |  |
| --- | --- | --- |
|  | âˆ‡Lâ€‹(Î¸)=tâˆ’âˆ‘iwiâ€‹(Î¸)â€‹xi,âˆ‡2Lâ€‹(Î¸)=âˆ’âˆ‘iwiâ€‹(Î¸)â€‹(xiâˆ’Î¼)â€‹(xiâˆ’Î¼)âŠ¤=âˆ’Covwâ€‹(Î¸)â€‹(x).\nabla L(\theta)=t-\sum\_{i}w\_{i}(\theta)x\_{i},\qquad\nabla^{2}L(\theta)=-\sum\_{i}w\_{i}(\theta)(x\_{i}-\mu)(x\_{i}-\mu)^{\top}=-\mathrm{Cov}\_{w(\theta)}(x). |  |

Strict concavity holds where Covwâ€‹(Î¸)â€‹(x)â‰»0\mathrm{Cov}\_{w(\theta)}(x)\succ 0 (see [9](https://arxiv.org/html/2510.24607v1#bib.bib9)). â–¡\square

### A.3â€ƒProof of TheoremÂ [2](https://arxiv.org/html/2510.24607v1#Thmtheorem2 "Theorem 2 (Existence and uniqueness). â€£ 5 Theoretical Guarantees â€£ Entropy-Guided Multiplicative Updates: KL Projections for Multi-Factor Target Exposures")

DKL(â‹…âˆ¥b)D\_{\mathrm{KL}}\!\left(\cdot\middle\|b\right) is strictly convex and lower semi-continuous on the simplex; the feasible set is convex and, under Slater, nonempty with nonempty relative interior. Hence a unique minimizer exists. Strict positivity follows from the fact that bi>0b\_{i}>0 and tâˆˆrelintt\in\mathrm{relint} enforce finite Lagrange multipliers and thus wiâ‹†âˆbiâ€‹eÎ¸â‹†âŠ¤â€‹xi>0w\_{i}^{\star}\propto b\_{i}e^{\theta^{\star\top}x\_{i}}>0. â–¡\square

### A.4â€ƒProof of TheoremÂ [4](https://arxiv.org/html/2510.24607v1#Thmtheorem4 "Theorem 4 (Sensitivity). â€£ 5 Theoretical Guarantees â€£ Entropy-Guided Multiplicative Updates: KL Projections for Multi-Factor Target Exposures")

At optimum, âˆ‡Lâ€‹(Î¸â‹†)=0â‡”ğ”¼wâ€‹(Î¸â‹†)â€‹[x]=t\nabla L(\theta^{\star})=0\iff\mathbb{E}\_{w(\theta^{\star})}[x]=t. Differentiate both sides w.r.t. tt:
âˆ‚âˆ‚tâ€‹ğ”¼wâ€‹(Î¸â‹†)â€‹[x]=I.\frac{\partial}{\partial t}\mathbb{E}\_{w(\theta^{\star})}[x]=I.
Using the exponential-family identity
âˆ‚âˆ‚Î¸â€‹ğ”¼wâ€‹(Î¸)â€‹[x]=Covwâ€‹(Î¸)â€‹(x),\frac{\partial}{\partial\theta}\mathbb{E}\_{w(\theta)}[x]=\mathrm{Cov}\_{w(\theta)}(x),
apply the chain rule to get
Covwâ‹†â€‹(x)â‹…âˆ‚Î¸â‹†âˆ‚t=Iâ‡’âˆ‚Î¸â‹†âˆ‚t=Covwâ‹†â€‹(x)âˆ’1.\mathrm{Cov}\_{w^{\star}}(x)\cdot\frac{\partial\theta^{\star}}{\partial t}=I\Rightarrow\frac{\partial\theta^{\star}}{\partial t}=\mathrm{Cov}\_{w^{\star}}(x)^{-1}.
For wiâ‹†=biâ€‹expâ¡(Î¸â‹†âŠ¤â€‹xiâˆ’logâ¡Z)w\_{i}^{\star}=b\_{i}\exp(\theta^{\star\top}x\_{i}-\log Z),

|  |  |  |
| --- | --- | --- |
|  | âˆ‚wiâ‹†âˆ‚Î¸=wiâ‹†â€‹(xiâˆ’Î¼)âŠ¤,Î¼=ğ”¼wâ‹†â€‹[x].\frac{\partial w\_{i}^{\star}}{\partial\theta}=w\_{i}^{\star}(x\_{i}-\mu)^{\top},\quad\mu=\mathbb{E}\_{w^{\star}}[x]. |  |

Thus âˆ‚wiâ‹†âˆ‚t=wiâ‹†â€‹(xiâˆ’Î¼)âŠ¤â€‹Covwâ‹†â€‹(x)âˆ’1\dfrac{\partial w\_{i}^{\star}}{\partial t}=w\_{i}^{\star}(x\_{i}-\mu)^{\top}\mathrm{Cov}\_{w^{\star}}(x)^{-1}, yielding the matrix form in the main text. â–¡\square

### A.5â€ƒElastic Dual and Sensitivity (Proof of Thm.Â [5](https://arxiv.org/html/2510.24607v1#Thmtheorem5 "Theorem 5 (Elastic dual: strong concavity and sensitivity). â€£ 5 Theoretical Guarantees â€£ Entropy-Guided Multiplicative Updates: KL Projections for Multi-Factor Target Exposures"))

The dual reads Lelâ€‹(Î¸)=Lâ€‹(Î¸)âˆ’12â€‹Î»softâ€‹â€–Î¸â€–2L\_{\mathrm{el}}(\theta)=L(\theta)-\tfrac{1}{2\lambda\_{\mathrm{soft}}}\|\theta\|^{2}. Hence âˆ‡Lel=âˆ‡Lâˆ’1Î»softâ€‹Î¸\nabla L\_{\mathrm{el}}=\nabla L-\tfrac{1}{\lambda\_{\mathrm{soft}}}\theta and âˆ‡2Lel=âˆ‡2Lâˆ’1Î»softâ€‹I\nabla^{2}L\_{\mathrm{el}}=\nabla^{2}L-\tfrac{1}{\lambda\_{\mathrm{soft}}}I, proving strong concavity. At the maximizer, tâˆ’ğ”¼wâ€‹(Î¸)â€‹[x]âˆ’1Î»softâ€‹Î¸=0t-\mathbb{E}\_{w(\theta)}[x]-\tfrac{1}{\lambda\_{\mathrm{soft}}}\theta=0. Differentiating w.r.t. tt and using âˆ‚ğ”¼wâ€‹(Î¸)â€‹[x]/âˆ‚Î¸=Î£\partial\mathbb{E}\_{w(\theta)}[x]/\partial\theta=\Sigma gives (Î£+1Î»softâ€‹I)â€‹âˆ‚Î¸/âˆ‚t=I(\Sigma+\tfrac{1}{\lambda\_{\mathrm{soft}}}I)\,\partial\theta/\partial t=I, establishing the stated sensitivities. â–¡\square

### A.6â€ƒKL Projection onto a Single Equality (IPF step)

Fix uâˆˆÎ”Nu\in\Delta^{N} and the set â„‹={w:aâŠ¤â€‹w=Ï„}\mathcal{H}=\{w:a^{\top}w=\tau\}. Minimize DKLâ€‹(wâˆ¥u)D\_{\mathrm{KL}}(w\|u) subject to aâŠ¤â€‹w=Ï„a^{\top}w=\tau and ğŸâŠ¤â€‹w=1\mathbf{1}^{\top}w=1. Stationarity: logâ¡(wi/ui)+1+Î±â€‹ai+Î½=0\log(w\_{i}/u\_{i})+1+\alpha a\_{i}+\nu=0, so wiâˆuiâ€‹eÎ±â€‹aiw\_{i}\propto u\_{i}e^{\alpha a\_{i}}. The normalization ensures wâ€‹(Î±)âˆˆÎ”Nw(\alpha)\in\Delta^{N}. Define Ï•â€‹(Î±)=aâŠ¤â€‹wâ€‹(Î±)âˆ’Ï„\phi(\alpha)=a^{\top}w(\alpha)-\tau. One computes Ï•â€²â€‹(Î±)=Varwâ€‹(Î±)â€‹(a)>0\phi^{\prime}(\alpha)=\mathrm{Var}\_{w(\alpha)}(a)>0 unless aa is degenerate, hence a unique root exists and can be found by bisection. â–¡\square

### A.7â€ƒKL Projection onto a Half-space (Inequality step)

For â„‹={w:aâŠ¤â€‹wâ‰¤Ï„}\mathcal{H}=\{w:a^{\top}w\leq\tau\}, if uu is feasible, the projection is uu. Otherwise, the KKT conditions yield wâ€‹(Î»)âˆuâŠ™eâˆ’Î»â€‹aw(\lambda)\propto u\odot e^{-\lambda a} with Î»â‰¥0\lambda\geq 0 chosen so that aâŠ¤â€‹wâ€‹(Î»)=Ï„a^{\top}w(\lambda)=\tau. Monotonicity follows from ddâ€‹Î»â€‹aâŠ¤â€‹wâ€‹(Î»)=âˆ’Varwâ€‹(Î»)â€‹(a)â‰¤0\frac{d}{d\lambda}a^{\top}w(\lambda)=-\mathrm{Var}\_{w(\lambda)}(a)\leq 0. â–¡\square

### A.8â€ƒConvergence of EGMU-Newton (Refinement of Thm.Â [6](https://arxiv.org/html/2510.24607v1#Thmtheorem6 "Theorem 6 (Convergence of EGMU-Newton). â€£ 5 Theoretical Guarantees â€£ Entropy-Guided Multiplicative Updates: KL Projections for Multi-Factor Target Exposures"))

The objective Lâ€‹(Î¸)=Î¸âŠ¤â€‹tâˆ’logâ€‹âˆ‘ibiâ€‹eÎ¸âŠ¤â€‹xiL(\theta)=\theta^{\top}t-\log\sum\_{i}b\_{i}e^{\theta^{\top}x\_{i}} is twice continuously differentiable and concave, with âˆ‡Lâ€‹(Î¸)=tâˆ’ğ”¼wâ€‹(Î¸)â€‹[x]\nabla L(\theta)=t-\mathbb{E}\_{w(\theta)}[x] and âˆ‡2Lâ€‹(Î¸)=âˆ’Covwâ€‹(Î¸)â€‹(x)\nabla^{2}L(\theta)=-\mathrm{Cov}\_{w(\theta)}(x). If â€–xiâ€–2â‰¤R\|x\_{i}\|\_{2}\leq R for all ii, then â€–âˆ‡2Lâ€‹(Î¸)â€–â‰¤R2\|\nabla^{2}L(\theta)\|\leq R^{2} for all Î¸\theta, and âˆ‡2L\nabla^{2}L is locally Lipschitz (with constant depending on RR and the third centered moment). Under these mild smoothness conditions, damped Newton with Armijo backtracking is globally convergent and locally quadratically convergent in a neighborhood of Î¸â‹†\theta^{\star} for strongly concave LL on the relevant subspace (see [4](https://arxiv.org/html/2510.24607v1#bib.bib4), Ch.Â 9). Ridge regularization (Î£+Î´â€‹I)(\Sigma+\delta I) stabilizes solves when Î£\Sigma is ill-conditioned; as Î´â†“0\delta\downarrow 0 the step approaches the exact Newton direction.

### A.9â€ƒConvergence of IPF and Bregmanâ€“Dykstra (Proof of Thm.Â [7](https://arxiv.org/html/2510.24607v1#Thmtheorem7 "Theorem 7 (Convergence of projection schemes). â€£ 5 Theoretical Guarantees â€£ Entropy-Guided Multiplicative Updates: KL Projections for Multi-Factor Target Exposures"))

Part (i) follows from CsiszÃ¡râ€™s II-projection theory and the Darrochâ€“Ratcliff analysis of generalized iterative scaling for log-linear models [[6](https://arxiv.org/html/2510.24607v1#bib.bib6), [7](https://arxiv.org/html/2510.24607v1#bib.bib7)]. Part (ii) is a special case of Dykstraâ€™s algorithm with Bregman divergences: for finitely many closed convex sets and a Legendre-type Bregman generator (negative entropy here), the cyclic projections converge to the unique Bregman projection onto the intersection [[3](https://arxiv.org/html/2510.24607v1#bib.bib3)]. â–¡\square

### A.10â€ƒCarathÃ©odory support bound (remark)

Any tâˆˆconvâ€‹{ğ±i}t\in\mathrm{conv}\{\mathbf{x}\_{i}\} admits a representation using at most K+1K+1 points. See, e.g., Barvinok [[1](https://arxiv.org/html/2510.24607v1#bib.bib1)]. This yields a sparsity upper bound for exact feasibility, though KL minimization under strictly positive prior typically produces dense solutions unless boundary constraints are active.

### A.11â€ƒRobust dual and proximal map (details)

Let gâ€‹(y)=Î¹t0+ğ’°â€‹(y)g(y)=\iota\_{\,t\_{0}+\mathcal{U}}(y). Its Fenchel conjugate is
gâˆ—â€‹(Î¸)=supy{Î¸âŠ¤â€‹yâˆ’gâ€‹(y)}=supuâˆˆğ’°Î¸âŠ¤â€‹(t0+u)=Î¸âŠ¤â€‹t0+Ïƒğ’°â€‹(Î¸)g^{\*}(\theta)=\sup\_{y}\{\theta^{\top}y-g(y)\}=\sup\_{u\in\mathcal{U}}\theta^{\top}(t\_{0}+u)=\theta^{\top}t\_{0}+\sigma\_{\mathcal{U}}(\theta),
hence the robust dual in Â§[3.4](https://arxiv.org/html/2510.24607v1#S3.SS4 "3.4 Robust Target Sets via Support Functions â€£ 3 Duality and Exponential-Family Form â€£ Entropy-Guided Multiplicative Updates: KL Projections for Multi-Factor Target Exposures"). For the proximal step, use Moreauâ€™s identity for conjugates:
proxÎ·â€‹gâˆ—â€‹(z)=zâˆ’Î·â€‹proxg/Î·â€‹(z/Î·)\mathrm{prox}\_{\eta g^{\*}}(z)=z-\eta\,\mathrm{prox}\_{g/\eta}(z/\eta).
Since g/Î·g/\eta is the indicator of t0+ğ’°t\_{0}+\mathcal{U}, proxg/Î·â€‹(z/Î·)=Î t0+ğ’°â€‹(z/Î·)\mathrm{prox}\_{g/\eta}(z/\eta)=\Pi\_{\,t\_{0}+\mathcal{U}}(z/\eta).
With the translation yâ†¦yâˆ’t0y\mapsto y-t\_{0}, this yields
proxÎ·â€‹Ïƒğ’°â€‹(z)=zâˆ’Î·â€‹Î ğ’°â€‹(z/Î·)\mathrm{prox}\_{\eta\,\sigma\_{\mathcal{U}}}(z)=z-\eta\,\Pi\_{\mathcal{U}}(z/\eta) used in AlgorithmÂ [4](https://arxiv.org/html/2510.24607v1#alg4 "Algorithm 4 â€£ 4.4 EGMU-ProxGrad (Robust Dual, R2) â€£ 4 Algorithms â€£ Entropy-Guided Multiplicative Updates: KL Projections for Multi-Factor Target Exposures"). â–¡\square

### A.12â€ƒExistence and uniqueness of the solution path ODE

For tâ€‹(Î»)=t0+Î»â€‹Î”t(\lambda)=t\_{0}+\lambda\Delta, the optimal Î¸â€‹(Î»)\theta(\lambda) satisfies
Fâ€‹(Î¸,Î»):=tâ€‹(Î»)âˆ’ğ”¼wâ€‹(Î¸)â€‹[x]âˆ’1Î»softâ€‹Î¸=0F(\theta,\lambda):=t(\lambda)-\mathbb{E}\_{w(\theta)}[x]-\tfrac{1}{\lambda\_{\mathrm{soft}}}\theta=0.
Then âˆ‚Î¸Fâ€‹(Î¸,Î»)=Î£â€‹(Î¸)+1Î»softâ€‹Iâª°mâ€‹I\partial\_{\theta}F(\theta,\lambda)=\Sigma(\theta)+\tfrac{1}{\lambda\_{\mathrm{soft}}}I\succeq mI on a neighborhood where
Î£\Sigma is bounded below. By the implicit function theorem, there exists a unique C1C^{1} path Î¸â€‹(Î»)\theta(\lambda) with
dâ€‹Î¸dâ€‹Î»=(Î£â€‹(Î¸)+1Î»softâ€‹I)âˆ’1â€‹Î”\dfrac{d\theta}{d\lambda}=\big(\Sigma(\theta)+\tfrac{1}{\lambda\_{\mathrm{soft}}}I\big)^{-1}\Delta.
Under locally Lipschitz âˆ‡2L\nabla^{2}L, Euler and RK2 integrators achieve Oâ€‹(h)O(h) and Oâ€‹(h2)O(h^{2}) global errors respectively. â–¡\square

## Classification and availability

JEL: G11, C61, C63, C58.â€ƒMSC 2020: 90C25, 90C90, 62F10, 94A17.â€ƒReproducibility: Minimal synthetic scripts (Newton/IPF/ProxGrad/Path) to reproduce algorithms and figures are provided in the supplementary material; no proprietary data are used.

## References

* Barvinok [2002]

  A.Â Barvinok.
  *A Course in Convexity*, volumeÂ 54 of *Graduate Studies in Mathematics*.
  American Mathematical Society, Providence, RI, 2002.
* Bauschke and Combettes [2011]

  H.Â H. Bauschke and P.Â L. Combettes.
  *Convex Analysis and Monotone Operator Theory in Hilbert Spaces*.
  Springer, New York, 2011.
* Bauschke and Lewis [2000]

  H.Â H. Bauschke and A.Â S. Lewis.
  Dykstraâ€™s algorithm with bregman projections: A convergence proof.
  *Optimization*, 48(4):409â€“427, 2000.
* Boyd and Vandenberghe [2004]

  S.Â Boyd and L.Â Vandenberghe.
  *Convex Optimization*.
  Cambridge University Press, Cambridge, 2004.
* Cover and Thomas [2006]

  T.Â M. Cover and J.Â A. Thomas.
  *Elements of Information Theory*.
  Wiley-Interscience, Hoboken, NJ, 2 edition, 2006.
* CsiszÃ¡r [1975]

  I.Â CsiszÃ¡r.
  I-divergence geometry of probability distributions and minimization problems.
  *The Annals of Probability*, 3(1):146â€“158, 1975.
* Darroch and Ratcliff [1972]

  J.Â N. Darroch and D.Â Ratcliff.
  Generalized iterative scaling for log-linear models.
  *The Annals of Mathematical Statistics*, 43(5):1470â€“1480, 1972.
* Meucci [2008]

  A.Â Meucci.
  Fully flexible views: Theory and practice.
  *Risk*, 21(10):97â€“102, 2008.
* Wainwright and Jordan [2008]

  M.Â J. Wainwright and M.Â I. Jordan.
  Graphical models, exponential families, and variational inference.
  *Foundations and Trends in Machine Learning*, 1(1-2):1â€“305, 2008.