---
authors:
- Ahmet Umur Ã–zsoy
doc_id: arxiv:2511.14980v1
family_id: arxiv:2511.14980
is_current: true
taxonomy:
  alpha_families: []
  asset_classes: []
  horizons: []
  themes: []
title: 'Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton
  Framework'
url_abs: http://arxiv.org/abs/2511.14980v1
url_html: https://arxiv.org/html/2511.14980v1
venue: arXiv q-fin
version: 1
year: 2025
---


[umurozsoy@gmail.com](mailto:umurozsoy@gmail.com)
[

###### Abstract

Calibration of option pricing models is routinely repeated as markets evolve, yet modern systems lack an operator for removing data from a calibrated model without full retraining. When quotes become stale, corrupted, or subject to deletion requirements, existing calibration pipelines must rebuild the entire nonlinear least-squares problem, even if only a small subset of data must be excluded. In this work, we introduce a principled framework for selective forgetting (machine unlearning) in parametric option calibration. We provide stability guarantees, perturbation bounds, and show that the proposed operators satisfy local exactness under standard regularity assumptions.

###### keywords:

selective forgetting, option calibration, Gaussâ€“Newton methods,
sufficient statistics, numerical unlearning, Heston model

## 1 Introduction

Modern financial models are not static; they are recalibrated as market conditions change.
Therefore calibrating parametric asset-pricing models to market data has always been an ongoing interest for both practitioners and academics in the field of mathematical finance.
Risk management systems along with trading desks rely heavily on the repeated solutions of inverse problems aimed at calibrating and adjusting parameters Î¸\theta so that the model-based prices mâ€‹(x;Î¸)m(x;\theta) reproduce observed quotes to some extent of accuracy.
Option-implied volatility surfaces evolve minute by minute, and model parameters such as
mean reversion, volatility of volatility, or correlation etc. are adapted to new market information.
Formally, calibration seeks parameters Î¸\theta minimizing a discrepancy between model generated prices
mâ€‹(x;Î¸)m(x;\theta) and observed quotes, typically through nonlinear least squares or
maximum-likelihood estimation.
This sort of inverse problem is present at models such as Heston and SABR
to structural credit, interest rate, and hybrid models and lies at almost all the operational core of
risk engines and trading platforms.

While long-standing research have refined the *estimating* or *learning* side of calibration of financial derivatives, options in our case, no interest has been shown to its conceptual dual, *unlearning*.
When certain data becomes corrupted, obsolete, or subject to possible deletion requests,
the model should exclude its influence without a full recalibration from scratch.
Therefore deletion and data retraction could become necessities as quotes expire, bad ticks or outlier surfaces are purged, and perhaps regulatory obligations (e.g. GDPR, audit requests, or data licensing constraints) require that specific records be removed from already-calibrated models.
The question is immediate yet unsolved, therefore the question becomes how a calibrated model retract the influence of certain data without a full recalibration from scratch.
The removal (i.e., deletion) of certain subsets of available data from an already trained models has emerged recently in machine learning as *Machine Unlearning* or *Selective Forgetting*.
The essence of such approaches casts a simple question whether or not the exclusion of a subset of data requires retraining on the remaining data,Â [ginart2019making, bourtoule2021machine, sekhari2021remember, qu2024learn, guo2019certified],
and has become central to privacy-aware and data-efficient algorithm design.

In financial modeling, however, unlearning has not been formalized.
To address this, we introduce a principled framework for *machine unlearning in option calibration*.
Our goal is to update calibrated parameters as if certain quotes had never been observed,
without re-accessing or reprocessing the entire dataset.
We cast calibration as a nonlinear least-squares problem solved by Gaussâ€“Newton iterations
and show that the normal-equation structure naturally supports machine unlearning.

The term *machine unlearning* has traditionally referred to privacy-motivated deletion in which the goal is to modify a trained model so that it becomes indistinguishable from one that
was never exposed to the deleted dataÂ [ginart2019making].
That definition requires a probabilistic or differential-privacy guarantee, an epistemic statement that an observer cannot tell whether the forgotten data ever influenced the model.

Our work adopts the unlearning viewpoint in a different, numerical sense that is particularly natural for calibration.
We are not concerned with information-theoretic indistinguishability, but with the *computational removal of numerical influence*.
Given a parametric model mâ€‹(x;Î¸)m(x;\theta) calibrated on market data DD, and a subset FâŠ‚DF\subset D of quotes to be discarded (e.g., due to stale prices or data errors), our
objective is to obtain parameters

|  |  |  |
| --- | --- | --- |
|  | Î¸â€²=argâ¡minÎ¸â¡Jâ€‹(Î¸;Dâˆ–F),\theta^{\prime}\;=\;\arg\min\_{\theta}J(\theta;D\setminus F), |  |

*without reprocessing all of DD*.
In this setting, unlearning means reproducing the same parameter update that full retraining on Dâˆ–FD\setminus F would yield, up to machine precision.
This provides a strict advantage as retraining on large option datasets could be expensive, especially for models with higher complexity.
Hence the *forgetting* is not about privacy or randomness in our point of view, but about *efficiently erasing the numerical footprint* of specific data in parameter space. We refer toÂ [zhang2023review] andÂ [nguyen2025survey] for recent and well articulated examples of review of *Machine Unlearning* as the literature is quite vast to provide deep holistic view.

Even though we touch upon the fact that no interest has been shown to the removal of data that compels a recalibration, there are studies that are directionally *forward in time* in which the models are refit periodically conditional on the accumulation of *new* data, for instance Â [date2011linear],Â [broto2004estimation],Â [bakshi1997empirical],Â [broadie2007model].
Such studies perform incremental steps on new samples, by the very nature of it.
By construction, however, they do not address the inverse problem of properly removing the informational contribution of a specific subset of observations while reproducing the solution that would have been obtained had those observations never been used.

Calibration could be repeated thousands of times as market conditions evolve, often under strict latency and consistency constraints.
This increases our interest in *Machine Unlearning* as a strong reason of removal bulk amount of data might be on cleaning corrupted data.
Even in stable markets, bad ticks, stale or misrecorded quotes could exist and distort calibration.
Therefore unlearning such contaminated shards of data could improve calibration precision without retraining the model from scratch.
Consider that a pricing engine or feed producing several days of quotes with a decimal-shift bug, instead of full retrain; unlearning those days might realign the parameters with the clean market.
Therefore, especially inspired by the SISA (Sharded Isolated Sliced Aggregation) paradigm fromÂ [bourtoule2021machine], we suggest two unlearning operators by showing that in nonlinear calibration, the Gaussâ€“Newton equations can be reorganized into algebraically additive terms that admit exact deletion operators as the apparent simplicity hides a structural insight.

This work introduces a principled framework for *selective forgetting* (machine unlearning) in parametric option calibration, formulated under the standard Gaussâ€“Newton (GN) least-squares setting used in Heston-type models.
Given an initially calibrated parameter vector Î¸\theta fitted on a dataset of option quotes D={(xi,yi)}D=\{(x\_{i},y\_{i})\}, the goal is to efficiently update Î¸\theta to the parameter that would result from retraining on the retained subset Dâˆ–FD\setminus F, without accessing the full dataset again.
Our contributions are both algorithmic and theoretical.
We first design a shard-aware decomposition of the GN normal equations and then we prove that this system coincides exactly with the one obtained by retraining on Dâˆ–FD\setminus F at a fixed linearization. We term this approach the sharded recompute as it enables machine-precision unlearning with partial data access and provides a scalable, shard-local recalibration architecture for option models.
We then develop a data-free *refactor* operator that realizes exact forgetting without reopening any raw quotes making this the first exact forgetting operator for nonlinear least-squares calibration models, to our knowledge.
The approach achieves retraining-level accuracy while remaining completely data-free once the sufficient statistics (ui,Ïˆi)(u\_{i},\psi\_{i}) are cached.

We further show in synthetic option datasets that our framework achieves near-zero degradation in calibration accuracy compared to full retraining, while reducing computational cost by an order of magnitude or more.
The proposed framework supports operational scenarios in which data must be removed due to regulatory, contractual, or quality-control reasons, providing a principled and efficient alternative to discarding and recalibrating the entire dataset.
By applying selective forgetting to targeted subsets (e.g., quotes from specific dates or sources), we measure their influence on calibrated parameters and pricing accuracy, enabling a form of leave-one-shard-out sensitivity analysis for option pricing models.
Overall, this study bridges the emerging field of machine unlearning with the long-standing problem of derivative model calibration, introducing both a novel theoretical framework and an immediately applicable methodology for real-world financial modeling.
Beyond computational efficiency, the proposed framework positions unlearning as a fundamental complement to calibration in model management.
In large-scale pricing and risk systems, models must not only learn from new data but also *forget* obsolete or restricted information.
The machine unlearning operators developed here establish an analytical bridge between machine unlearning and quantitative finance, enabling the first operator-theoretic treatment of data deletion in stochastic-volatility model calibration.

## 2 Formulation of the unlearning problem

The unlearning procedure proposed in this study operates purely on the normal equations of the Gaussâ€“Newton method.
As such, it does not modify the underlying option pricing model, the risk-neutral pricing map, or any no-arbitrage structure inherent to the parametric family.
While the numerical experiments use European calls under the Heston model for analytical clarity,
the proposed forgetting framework extends to any differentiable pricing map,
including exotic or path-dependent contracts evaluated by Monte Carlo or adjoint methods.
The machine unlearning calibration framework developed here does respect all classical no-arbitrage and mathematical finance principles (i.e., monotonicity, convexity, existence of the equivalent martingale measures etc.) to the same extent as the underlying model calibrated.
In this section, we first discuss preliminaries and then mathematically develop the machine unlearning operators.

### 2.1 Preliminaries

We consider the problem of calibration of the Heston model to European call option prices.
Under the riskâ€“neutral measure, the Heston dynamics for the asset price StS\_{t} and its variance vtv\_{t} are

|  |  |  |
| --- | --- | --- |
|  | dâ€‹St=râ€‹Stâ€‹dâ€‹t+vtâ€‹Stâ€‹dâ€‹WtS,dâ€‹vt=Îºâ€‹(Î¸vâˆ’vt)â€‹dâ€‹t+Ïƒvâ€‹vtâ€‹dâ€‹Wtv,\mathrm{d}S\_{t}=rS\_{t}\,\mathrm{d}t+\sqrt{v\_{t}}S\_{t}\,\mathrm{d}W\_{t}^{S},\qquad\mathrm{d}v\_{t}=\kappa(\theta\_{v}-v\_{t})\,\mathrm{d}t+\sigma\_{v}\sqrt{v\_{t}}\,\mathrm{d}W\_{t}^{v}, |  |

with dâ€‹âŸ¨WS,WvâŸ©t=Ïâ€‹dâ€‹t\mathrm{d}\langle W^{S},W^{v}\rangle\_{t}=\rho\,\mathrm{d}t.
The parameter vector is
Î¸=(Îº,Î¸v,Ïƒv,Ï,v0)\theta=(\kappa,\theta\_{v},\sigma\_{v},\rho,v\_{0}).
Call option prices mâ€‹(x;Î¸)m(x;\theta) are given in semi-closed form via the characteristic function of logâ¡ST\log S\_{T}Â [heston1993closed].
European call prices under the Heston model admit the semi-analytical representation

|  |  |  |  |
| --- | --- | --- | --- |
|  | Câ€‹(S0,K,T;Î¸)=S0â€‹P1âˆ’Kâ€‹eâˆ’râ€‹Tâ€‹P2,C(S\_{0},K,T;\theta)=S\_{0}P\_{1}-Ke^{-rT}P\_{2}, |  | (1) |

where the riskâ€“neutral probabilities, PjP\_{j}, are given by

|  |  |  |  |
| --- | --- | --- | --- |
|  | Pj=12+1Ï€â€‹âˆ«0âˆâ„œâ¡(eâˆ’iâ€‹uâ€‹lnâ¡Kâ€‹Ï•jâ€‹(u)iâ€‹u)â€‹du,jâˆˆ{1,2}.P\_{j}=\tfrac{1}{2}+\tfrac{1}{\pi}\int\_{0}^{\infty}\Re\!\left(\frac{e^{-iu\ln K}\,\phi\_{j}(u)}{iu}\right)\mathrm{d}u,\qquad j\in\{1,2\}. |  | (2) |

The characteristic function Ï•jâ€‹(u)\phi\_{j}(u) follows the standard form ofÂ [heston1993closed], depending on parameters Î¸=(Îº,Î¸v,Ïƒv,Ï,v0)\theta=(\kappa,\theta\_{v},\sigma\_{v},\rho,v\_{0}).
We evaluate ([2](https://arxiv.org/html/2511.14980v1#S2.E2 "In 2.1 Preliminaries â€£ 2 Formulation of the unlearning problem â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework")) numerically via Simpson integration with an upper bound UmaxU\_{\max} and NN sub-intervals, which ensures differentiability of Câ€‹(â‹…;Î¸)C(\cdot;\theta) with respect to each parameter which ensures that the Jacobian Jiâ€‹(Î¸)=âˆ‡Î¸mâ€‹(xi;Î¸)J\_{i}(\theta)=\nabla\_{\theta}m(x\_{i};\theta) exists for each quote given that xix\_{i} mâ€‹(x;Î¸)m(x;\theta) the parametric pricing map (e.g., Heston) with Î¸âˆˆÎ˜âŠ‚â„p\theta\in\Theta\subset\mathbb{R}^{p} with D={(xi,yi)}i=1ND=\{(x\_{i},y\_{i})\}\_{i=1}^{N} denoting the dataset of option quotes (features xix\_{i} and responses yiy\_{i}).
Further, let x=(S,K,T,r)x=(S,K,T,r) denote the market features of a European call option quote,
and let Î¸=(Îº,Î¸v,Ïƒv,Ï,v0)\theta=(\kappa,\theta\_{v},\sigma\_{v},\rho,v\_{0}) be the Heston parameter vector.
We define the parametric pricing map

|  |  |  |
| --- | --- | --- |
|  | mâ€‹(x;Î¸)=Sâ€‹P1â€‹(S,K,T,r;Î¸)âˆ’Kâ€‹eâˆ’râ€‹Tâ€‹P2â€‹(S,K,T,r;Î¸),m(x;\theta)=S\,P\_{1}(S,K,T,r;\theta)-Ke^{-rT}P\_{2}(S,K,T,r;\theta), |  |

where P1P\_{1} and P2P\_{2} are the riskâ€“neutral probabilities given by the
Fourierâ€“Laplace integrals of ([2](https://arxiv.org/html/2511.14980v1#S2.E2 "In 2.1 Preliminaries â€£ 2 Formulation of the unlearning problem â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework")).
The calibration problem then seeks

|  |  |  |
| --- | --- | --- |
|  | Î¸â‹†=argâ¡minÎ¸â¡Jâ€‹(Î¸;D),Jâ€‹(Î¸;D)=âˆ‘iâˆˆD(yiâˆ’mâ€‹(xi;Î¸))2.\theta^{\star}=\arg\min\_{\theta}J(\theta;D),\qquad J(\theta;D)=\sum\_{i\in D}\big(y\_{i}-m(x\_{i};\theta)\big)^{2}. |  |

With riâ€‹(Î¸)=yiâˆ’mâ€‹(xi;Î¸)r\_{i}(\theta)=y\_{i}-m(x\_{i};\theta), linearizing each residual around a reference Î¸ref\theta^{\mathrm{ref}} gives riâ€‹(Î¸ref+Î”â€‹Î¸)â‰ˆriâ€‹(Î¸ref)âˆ’Jiâ€‹(Î¸ref)â€‹Î”â€‹Î¸r\_{i}(\theta^{\mathrm{ref}}+\Delta\theta)\approx r\_{i}(\theta^{\mathrm{ref}})-J\_{i}(\theta^{\mathrm{ref}})\Delta\theta, where Jiâ€‹(Î¸)=âˆ‡Î¸mâ€‹(xi;Î¸)J\_{i}(\theta)=\nabla\_{\theta}m(x\_{i};\theta) is the sensitivity (Jacobian) of the model output with respect to parameters.
Substituting into Jâ€‹(Î¸)J(\theta) and minimizing the quadratic approximation yields
the *Gaussâ€“Newton normal equations*

|  |  |  |
| --- | --- | --- |
|  | Hâ€‹(Î¸ref)â€‹Î”â€‹Î¸=Gâ€‹(Î¸ref),H=âˆ‘iJiâŠ¤â€‹Ji,G=âˆ‘iJiâŠ¤â€‹ri.H(\theta^{\mathrm{ref}})\,\Delta\theta=G(\theta^{\mathrm{ref}}),\qquad H=\sum\_{i}J\_{i}^{\top}J\_{i},\quad G=\sum\_{i}J\_{i}^{\top}r\_{i}. |  |

Solving for Î”â€‹Î¸\Delta\theta provides the parameter correction that minimizes the
local linearized loss:
Î¸â€²=Î¸ref+Î”â€‹Î¸\theta^{\prime}=\theta^{\mathrm{ref}}+\Delta\theta.
Because HH and GG are additive across data points, any subset of quotes can be
removed or updated by simple algebraic subtraction of their local contributions.
This additive structure underpins the exactness of the proposed forgetting
operator.
Therefore, we build upon this simplistic yet structural insightful observation of additivity in designing the unlearning operators.

### 2.2 Sharded recompute operator

The idea of dividing a dataset into shards for efficient unlearning has appeared in the
machine-learning literature, notably in the â€œSISAâ€ framework of [bourtoule2021machine], which trains isolated submodels that can be retrained independently upon deletion requests.
Our sharded design which we now present is mathematically different; rather than training independent submodels, we partition the Gaussâ€“Newton normal equations themselves into additive
shard contributions (Hk,Gk)(H\_{k},G\_{k}), allowing exact recomputation of the global system after a shard-level deletion.

Let xix\_{i} be features (e.g., moneyness, maturity etc) and yiy\_{i} be the observed price (or implied volatility); together constituting the set option quotes (xi,yi)(x\_{i},y\_{i}) with iâˆˆDi\in D (finite index set).
We define mâ€‹(x;Î¸)m(x;\theta) as the parametric pricing map (e.g., Heston) with parameters Î¸âˆˆÎ˜âŠ‚â„p\theta\in\Theta\subset\mathbb{R}^{p} and Loss as â„“â€‹(y,y^)\ell(y,\hat{y}), typically squared error on prices.
And calibration minimizes the empirical loss of the form

|  |  |  |  |
| --- | --- | --- | --- |
|  | Jâ€‹(Î¸;D)=âˆ‘iâˆˆDâ„“â€‹(yi,mâ€‹(xi;Î¸)).J(\theta;D)=\sum\_{i\in D}\ell(y\_{i},m(x\_{i};\theta)). |  | (3) |

At a reference Î¸\theta, We use Gaussâ€“Newton to solve

|  |  |  |  |
| --- | --- | --- | --- |
|  | Hâ€‹(Î¸)â€‹Î”â€‹Î¸=gâ€‹(Î¸),\quad H(\theta)\Delta\theta=g(\theta), |  | (4) |

where gâ€‹(Î¸)=âˆ‘iJiâ€‹(Î¸)âŠ¤â€‹riâ€‹(Î¸)g(\theta)=\sum\_{i}J\_{i}(\theta)^{\top}r\_{i}(\theta), Hâ€‹(Î¸)=âˆ‘iJiâ€‹(Î¸)âŠ¤â€‹Jiâ€‹(Î¸)H(\theta)=\sum\_{i}J\_{i}(\theta)^{\top}J\_{i}(\theta), riâ€‹(Î¸)=yiâˆ’mâ€‹(xi;Î¸)r\_{i}(\theta)=y\_{i}-m(x\_{i};\theta), Jiâ€‹(Î¸)=âˆ‡Î¸mâ€‹(xi;Î¸)J\_{i}(\theta)=\nabla\_{\theta}m(x\_{i};\theta).
Given a trained model on DD, and a subset FâŠ‚DF\subset D to "forget", update Î¸\theta so the new parameter equals (or closely matches) the parameter you would obtain by retraining on Dâˆ–FD\setminus F, without sweeping the entire DD again.
Sharded recomputation first partitions the data set into KK shards D=â‹ƒk=1KDkD=\bigcup\_{k=1}^{K}D\_{k}.
Sharding could be by time (e.g., month).We remark an important suggestion on the shard sizes.
Given the assumption that some parts of data will be unlearned, keeping shards moderately sized so that removing a subset touches few shards .
For any reference Î¸\theta, we define per-shard Gaussâ€“Newton aggregates:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Gkâ€‹(Î¸)=âˆ‘iâˆˆDkJiâ€‹(Î¸)âŠ¤â€‹riâ€‹(Î¸),Hkâ€‹(Î¸)=âˆ‘iâˆˆDkJiâ€‹(Î¸)âŠ¤â€‹Jiâ€‹(Î¸).G\_{k}(\theta)=\sum\_{i\in D\_{k}}J\_{i}(\theta)^{\top}r\_{i}(\theta),\quad H\_{k}(\theta)=\sum\_{i\in D\_{k}}J\_{i}(\theta)^{\top}J\_{i}(\theta). |  | (5) |

Global aggregates are sums across shards:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Gâ€‹(Î¸)=âˆ‘k=1KGkâ€‹(Î¸),Hâ€‹(Î¸)=âˆ‘k=1KHkâ€‹(Î¸).G(\theta)=\sum\_{k=1}^{K}G\_{k}(\theta),\quad H(\theta)=\sum\_{k=1}^{K}H\_{k}(\theta). |  | (6) |

The methodology with reference Î¸ref\theta^{\text{ref}}.
First compute Gkâ€‹(Î¸ref)G\_{k}(\theta^{\text{ref}}), Hkâ€‹(Î¸ref)H\_{k}(\theta^{\text{ref}}) for each shard kk, sum to GG, HH; solve Hâ€‹Î”â€‹Î¸=GH\Delta\theta=G and finally update Î¸â†Î¸ref+Î”â€‹Î¸\theta\leftarrow\theta^{\text{ref}}+\Delta\theta.
One can optionally relinearize once or twice (update reference and re-compute shard stats).
This, rather, describes the baseline calibration procedure when no data has been forgotten.
For FâŠ‚DF\subset D, unlearning phase includes identifying the set of affected shards ğ’¦â€‹(F)={k:Dkâˆ©Fâ‰ âˆ…}\mathcal{K}(F)=\{k:D\_{k}\cap F\neq\emptyset\} first, then recomputing only those shards such that
kâˆˆğ’¦â€‹(F)k\in\mathcal{K}(F) on Dkâˆ–FD\_{k}\setminus F to obtain Gkâ€²G\_{k}^{\prime}, Hkâ€²H\_{k}^{\prime}.
We note that unaffected shards keep their statistics GkG\_{k}, HkH\_{k} while new global stats become

|  |  |  |  |
| --- | --- | --- | --- |
|  | Gâ€²=âˆ‘kâˆ‰ğ’¦â€‹(F)Gk+âˆ‘kâˆˆğ’¦â€‹(F)Gkâ€²,G^{\prime}=\sum\_{k\notin\mathcal{K}(F)}G\_{k}+\sum\_{k\in\mathcal{K}(F)}G\_{k}^{\prime}, |  | (7) |

|  |  |  |  |
| --- | --- | --- | --- |
|  | Hâ€²=âˆ‘kâˆ‰ğ’¦â€‹(F)Hk+âˆ‘kâˆˆğ’¦â€‹(F)Hkâ€²,H^{\prime}=\sum\_{k\notin\mathcal{K}(F)}H\_{k}+\sum\_{k\in\mathcal{K}(F)}H\_{k}^{\prime}, |  | (8) |

with ([7](https://arxiv.org/html/2511.14980v1#S2.E7 "In 2.2 Sharded recompute operator â€£ 2 Formulation of the unlearning problem â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework")) and ([8](https://arxiv.org/html/2511.14980v1#S2.E8 "In 2.2 Sharded recompute operator â€£ 2 Formulation of the unlearning problem â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework")), we solve for Hâ€²â€‹Î”â€‹Î¸â€²=Gâ€²H^{\prime}\Delta\theta^{\prime}=G^{\prime} and update accordingly.
Although the discussion above is straightforward, presenting the intuition of sharding is timely.
The idea of grouping the option (panel) data into subgroups before calibration is already well known in the literature; for instance we refer to [dumas1998implied], [ulrich2023implied], [homescu2011implied] and [friedman2014some].
As suggested, bucketing option data is already a common practice and we repurpose it in our analogy to share the same sharding narrative presented in [bourtoule2021machine].
On the broader terms, there are possible alternatives in sharding which will ultimately depend on the unlearning requests or necessities.
In our study, we simply do it by time as it is simple and stable.
This approach is particularly useful if the quotes arrive over days and forgetting targets a *date range*.
Another possibility is through product structure, for instance one shard including ATM with fewer than 30 days to expiration and another for OTMs with fewer than 60 days to expiration.
Possible scenario might include forgetting particular short-dated options from the dataset or removing bad surface are around 1M tenor.
This could provide better locality than time-sharding and perhaps the one that could contribute to model validation procedures given sharding targets after specific questions raised.
Finally we could suggest a hybrid version of what we discussed so far yet we leave such possibilities for further studies.

To ensure validity, we have standard stability and regularity assumptions.
We assume that mâ€‹(x;Î¸)m(x;\theta) is twice continuously differentiable in Î¸\theta on Î˜\Theta so that the modelâ€™s pricing map is smooth enough.
Further, the Jacobian Jâ€‹(Î¸)J(\theta) should exist and local Taylor expansion, râ€‹(Î¸+Î”â€‹Î¸)â‰ˆrâ€‹(Î¸)+Jâ€‹Î”â€‹Î¸r(\theta+\Delta\theta)\approx r(\theta)+J\Delta\theta, does not lead to instability in residuals.
Secondly we assume that per-shard sums Hkâ€‹(Î¸ref)H\_{k}(\theta^{\text{ref}}) are positive semidefinite so that local curvature remains nonnegative; i.e. shards contribute non-negative information.
And lastly we assume that global Hâ€‹(Î¸ref)H(\theta^{\text{ref}}) is positive definite with minimal eigenvalue Î»min>0\lambda\_{\min}>0 so that local strong convexity is established at Î¸râ€‹eâ€‹f\theta^{ref} and the global normal equation has a unique solution.

###### Proposition 1.

(Shard-level exactness at a fixed linearization)
Fix the reference Î¸ref\theta^{\text{ref}}. Consider the Gaussâ€“Newton normal equations at that reference. If we recompute exactly Gkâ€²G^{\prime}\_{k}, Hkâ€²H^{\prime}\_{k} for all affected shards on Dkâˆ–FD\_{k}\setminus F and keep (Gk,Hk)(G\_{k},H\_{k}) for unaffected shards, then the global system

|  |  |  |
| --- | --- | --- |
|  | (âˆ‘kâˆ‰ğ’¦â€‹(F)Hk+âˆ‘kâˆˆğ’¦â€‹(F)Hkâ€²)â€‹Î”â€‹Î¸â€²=âˆ‘kâˆ‰ğ’¦â€‹(F)Gk+âˆ‘kâˆˆğ’¦â€‹(F)Gkâ€²\left(\sum\_{k\notin\mathcal{K}(F)}H\_{k}+\sum\_{k\in\mathcal{K}(F)}H^{\prime}\_{k}\right)\Delta\theta^{\prime}=\sum\_{k\notin\mathcal{K}(F)}G\_{k}+\sum\_{k\in\mathcal{K}(F)}G^{\prime}\_{k} |  |

is identical to the Gaussâ€“Newton system built by running over the full retained set Dâˆ–FD\setminus F at Î¸ref\theta^{\text{ref}}. Consequently, the update Î¸â€²=Î¸ref+Î”â€‹Î¸â€²\theta^{\prime}=\theta^{\text{ref}}+\Delta\theta^{\prime} matches full retraining under the same linearization.

Proof is trivial as both sides are linear sums over iâˆˆDâˆ–Fi\in D\setminus F; sharding is just a partition.
The reason we put forward Proposition [1](https://arxiv.org/html/2511.14980v1#Thmtheorem1 "Proposition 1. â€£ 2.2 Sharded recompute operator â€£ 2 Formulation of the unlearning problem â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework") is straightforward.
Linearization, in our context, refers to the first-order approximation of the residual between model-generated prices and observed quotes, i.e., the loss surface around the current parameter estimate.
With this, Proposition [1](https://arxiv.org/html/2511.14980v1#Thmtheorem1 "Proposition 1. â€£ 2.2 Sharded recompute operator â€£ 2 Formulation of the unlearning problem â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework") simply formalizes that under a fixed linearization, sharding recomputation is *exactly equivalent* to full recalibration on the retained data (the data left upon removal of some data).
Formally, the unlearning operation becomes linear in the data as it is the bridge from nonlinear calibration to unlearning operator and justifies our sharded recomputation unlearning operator as an analytically consistent replacement for full retraining.
The following result adapts the classical local error bound of Gaussâ€“Newton iterations to our sharded-unlearning framework.
Here, the bound quantifies the accuracy gained after one local relinearization on affected shards, rather than the asymptotic convergence of a full iterative scheme.

###### Proposition 2 (Accuracy after one relinearization on affected shards).

Under the same smoothness and strong-convexity assumptions as before,
and additionally assuming that the Jacobian Jâ€‹(Î¸)J(\theta) is Lipschitz
continuous in a neighborhood of Î¸ref\theta^{\text{ref}}
Let Î¸â€²=Î¸ref+Î”â€‹Î¸â€²\theta^{\prime}=\theta^{\text{ref}}+\Delta\theta^{\prime} be the parameter produced by the shard-level update of PropositionÂ [1](https://arxiv.org/html/2511.14980v1#Thmtheorem1 "Proposition 1. â€£ 2.2 Sharded recompute operator â€£ 2 Formulation of the unlearning problem â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework") at a fixed reference Î¸ref\theta^{\text{ref}} on the retained dataset Dâˆ–FD\setminus F.
Let Î¸^\hat{\theta} denote the parameter obtained by performing one relinearization at Î¸â€²\theta^{\prime} and resolving the Gaussâ€“Newton system on Dâˆ–FD\setminus F.
Then there exist constants C1,C2>0C\_{1},C\_{2}>0 depending on LJL\_{J}, RmaxR\_{\max}, and the conditioning of Hâ€²â€‹(Î¸ref)H^{\prime}(\theta^{\text{ref}}) such that

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–Î¸^âˆ’Î¸â€²â€–â‰¤C1â€‹â€–râ€‹(Î¸ref)â€–â€‹â€–Î”â€‹Î¸â€²â€–+C2â€‹â€–Î”â€‹Î¸â€²â€–2.\|\hat{\theta}-\theta^{\prime}\|\;\leq\;C\_{1}\,\|r(\theta^{\text{ref}})\|\,\|\Delta\theta^{\prime}\|\;+\;C\_{2}\,\|\Delta\theta^{\prime}\|^{2}. |  | (9) |

###### Remark 1 (Quadratic accuracy under small residuals).

If, in addition, the residual norm at the reference satisfies
â€–râ€‹(Î¸ref)â€–â‰¤câ€‹â€–Î”â€‹Î¸â€²â€–\|r(\theta^{\text{ref}})\|\leq c\,\|\Delta\theta^{\prime}\| for some c>0c>0,
then inequalityÂ ([9](https://arxiv.org/html/2511.14980v1#S2.E9 "In Proposition 2 (Accuracy after one relinearization on affected shards). â€£ 2.2 Sharded recompute operator â€£ 2 Formulation of the unlearning problem â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework")) reduces to

|  |  |  |
| --- | --- | --- |
|  | â€–Î¸^âˆ’Î¸â€²â€–â‰¤Câ€‹â€–Î”â€‹Î¸â€²â€–2,C:=C1â€‹c+C2.\|\hat{\theta}-\theta^{\prime}\|\;\leq\;C\,\|\Delta\theta^{\prime}\|^{2},\quad C:=C\_{1}c+C\_{2}. |  |

Hence, a single relinearization on the affected shards yields a second-order accurate refinement of the fixed-linearization update.

###### Proof of PropositionÂ [2](https://arxiv.org/html/2511.14980v1#Thmtheorem2 "Proposition 2 (Accuracy after one relinearization on affected shards). â€£ 2.2 Sharded recompute operator â€£ 2 Formulation of the unlearning problem â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework").

Write aggregated quantities on Dâˆ–FD\setminus F as

|  |  |  |
| --- | --- | --- |
|  | Hâ€‹(Î¸)=Jâ€‹(Î¸)âŠ¤â€‹Jâ€‹(Î¸),Gâ€‹(Î¸)=Jâ€‹(Î¸)âŠ¤â€‹râ€‹(Î¸).H(\theta)=J(\theta)^{\top}J(\theta),\qquad G(\theta)=J(\theta)^{\top}r(\theta). |  |

By the Lipschitz property and twice differentiability of mm, for Î”=Î”â€‹Î¸â€²\Delta=\Delta\theta^{\prime} we have

|  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- |
|  | Jâ€‹(Î¸â€²)\displaystyle J(\theta^{\prime}) | =Jâ€‹(Î¸ref)+EJ,\displaystyle=J(\theta^{\text{ref}})+E\_{J},\qquad |  | â€–EJâ€–â‰¤LJâ€‹â€–Î”â€–,\displaystyle\|E\_{J}\|\leq L\_{J}\|\Delta\|, |  |
|  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- |
|  | râ€‹(Î¸â€²)\displaystyle r(\theta^{\prime}) | =râ€‹(Î¸ref)+Jâ€‹(Î¸ref)â€‹Î”+Rr,\displaystyle=r(\theta^{\text{ref}})+J(\theta^{\text{ref}})\Delta+R\_{r},\qquad |  | â€–Rrâ€–â‰¤Crâ€‹â€–Î”â€–2,\displaystyle\|R\_{r}\|\leq C\_{r}\|\Delta\|^{2}, |  |

for some constant Cr=Oâ€‹(LJ)C\_{r}=O(L\_{J}).
Expanding Gâ€‹(Î¸â€²)G(\theta^{\prime}) and Hâ€‹(Î¸â€²)H(\theta^{\prime}) gives

|  |  |  |  |
| --- | --- | --- | --- |
|  | Gâ€‹(Î¸â€²)\displaystyle G(\theta^{\prime}) | =Jâ€‹(Î¸â€²)âŠ¤â€‹râ€‹(Î¸â€²)=JâŠ¤â€‹r+JâŠ¤â€‹Jâ€‹Î”+EJâŠ¤â€‹r+Oâ€‹(â€–Î”â€–2),\displaystyle=J(\theta^{\prime})^{\top}r(\theta^{\prime})=J^{\top}r+J^{\top}J\Delta+E\_{J}^{\top}r+O(\|\Delta\|^{2}), |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | Hâ€‹(Î¸â€²)\displaystyle H(\theta^{\prime}) | =Jâ€‹(Î¸â€²)âŠ¤â€‹Jâ€‹(Î¸â€²)=Hâ€‹(Î¸ref)+Î”â€‹H,â€–Î”â€‹Hâ€–â‰¤CHâ€‹â€–Î”â€–,\displaystyle=J(\theta^{\prime})^{\top}J(\theta^{\prime})=H(\theta^{\text{ref}})+\Delta H,\qquad\|\Delta H\|\leq C\_{H}\|\Delta\|, |  |

with CH=Oâ€‹(â€–Jâ€‹(Î¸ref)â€–â€‹LJ+LJ2)C\_{H}=O(\|J(\theta^{\text{ref}})\|L\_{J}+L\_{J}^{2}).
Let Î”+\Delta^{+} solve the relinearized system Hâ€‹(Î¸â€²)â€‹Î”+=Gâ€‹(Î¸â€²)H(\theta^{\prime})\Delta^{+}=G(\theta^{\prime}).
Subtracting the fixed-linearization equation Hâ€‹(Î¸ref)â€‹Î”=Gâ€‹(Î¸ref)H(\theta^{\text{ref}})\Delta=G(\theta^{\text{ref}}) yields

|  |  |  |
| --- | --- | --- |
|  | Hâ€‹(Î¸â€²)â€‹(Î”+âˆ’Î”)=EJâŠ¤â€‹r+Oâ€‹(â€–Î”â€–2)+Î”â€‹Hâ€‹Î”.H(\theta^{\prime})(\Delta^{+}-\Delta)=E\_{J}^{\top}r+O(\|\Delta\|^{2})+\Delta H\,\Delta. |  |

Taking norms and using â€–Hâ€‹(Î¸â€²)âˆ’1â€–â‰¤2/Î»min\|H(\theta^{\prime})^{-1}\|\leq 2/\lambda\_{\min} for â€–Î”â€–\|\Delta\| small gives

|  |  |  |
| --- | --- | --- |
|  | â€–Î”+âˆ’Î”â€–â‰¤2â€‹LJÎ»minâ€‹â€–râ€‹(Î¸ref)â€–â€‹â€–Î”â€–+C2â€²â€‹â€–Î”â€–2,\|\Delta^{+}-\Delta\|\;\leq\;\frac{2L\_{J}}{\lambda\_{\min}}\,\|r(\theta^{\text{ref}})\|\,\|\Delta\|\;+\;C\_{2}^{\prime}\|\Delta\|^{2}, |  |

for a constant C2â€²C\_{2}^{\prime} depending on LJL\_{J} and Î»minâˆ’1\lambda\_{\min}^{-1}. Since
Î¸^âˆ’Î¸â€²=(Î¸ref+Î”+)âˆ’(Î¸ref+Î”)=Î”+âˆ’Î”\hat{\theta}-\theta^{\prime}=(\theta^{\text{ref}}+\Delta^{+})-(\theta^{\text{ref}}+\Delta)=\Delta^{+}-\Delta,
this provesÂ ([9](https://arxiv.org/html/2511.14980v1#S2.E9 "In Proposition 2 (Accuracy after one relinearization on affected shards). â€£ 2.2 Sharded recompute operator â€£ 2 Formulation of the unlearning problem â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework")) with C1=2â€‹LJ/Î»minC\_{1}=2L\_{J}/\lambda\_{\min} and C2=C2â€²C\_{2}=C\_{2}^{\prime}.
The corollary follows by substituting â€–râ€‹(Î¸ref)â€–â‰¤câ€‹â€–Î”â€–\|r(\theta^{\text{ref}})\|\leq c\|\Delta\| and absorbing constants. âˆ

The quadratic accuracy bound derived above is structurally related to the classical local error analysis of Gaussâ€“Newton and Newtonâ€“Kantorovich iterations.
Here, however, the theorem is not invoked to study asymptotic convergence of an iterative solver, but to establish the *fidelity of machine unlearning* within a sharded calibration framework.
In our setting, the relinearization step is applied only to the affected shards after a data-deletion event, and the resulting bound quantifies how closely this partial update reproduces the fully retrained Gaussâ€“Newton solution on the retained data set.
The adaptation of a classical local error argument to the context of selective unlearning therefore provides new insight into the stability and precision of unlearning operations in financial model
calibration.
The following adapts a standard perturbation bound for linear systems to our Gaussâ€“Newton unlearning update.
It provides an upper limit on the parameter deviation induced by downdating the curvature and gradient terms.

###### Proposition 3 (Stability of the unlearning update).

Let Î¸=Î¸ref+Î”â€‹Î¸\theta=\theta^{\mathrm{ref}}+\Delta\theta solve the fixed linearization system
Hâ€‹Î”â€‹Î¸=GH\Delta\theta=G on DD, and let Î¸â€²=Î¸ref+Î”â€‹Î¸â€²\theta^{\prime}=\theta^{\mathrm{ref}}+\Delta\theta^{\prime} solve
Hâ€²â€‹Î”â€‹Î¸â€²=Gâ€²H^{\prime}\Delta\theta^{\prime}=G^{\prime} on Dâˆ–FD\setminus F, where

|  |  |  |
| --- | --- | --- |
|  | Hâ€²=H+Î”â€‹H,Gâ€²=G+Î”â€‹G.H^{\prime}=H+\Delta H,\qquad G^{\prime}=G+\Delta G. |  |

Assume Hâ€²H^{\prime} is invertible. Then

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–Î¸â€²âˆ’Î¸â€–=â€–Î”â€‹Î¸â€²âˆ’Î”â€‹Î¸â€–â‰¤â€–Hâ€²â£âˆ’1â€–â€‹(â€–Î”â€‹Gâ€–+â€–Î”â€‹Hâ€–â€‹â€–Î”â€‹Î¸â€–).\|\theta^{\prime}-\theta\|\;=\;\|\Delta\theta^{\prime}-\Delta\theta\|\;\leq\;\|H^{\prime-1}\|\,\big(\,\|\Delta G\|+\|\Delta H\|\,\|\Delta\theta\|\,\big). |  | (10) |

Moreover, if â€–Hâˆ’1â€–â€‹â€–Î”â€‹Hâ€–<1\|H^{-1}\|\,\|\Delta H\|<1, then

|  |  |  |
| --- | --- | --- |
|  | â€–Hâ€²â£âˆ’1â€–â‰¤â€–Hâˆ’1â€–1âˆ’â€–Hâˆ’1â€–â€‹â€–Î”â€‹Hâ€–.\|H^{\prime-1}\|\;\leq\;\frac{\|H^{-1}\|}{1-\|H^{-1}\|\,\|\Delta H\|}. |  |

###### Proof.

From Hâ€‹Î”â€‹Î¸=GH\Delta\theta=G and (H+Î”â€‹H)â€‹Î”â€‹Î¸â€²=G+Î”â€‹G(H+\Delta H)\Delta\theta^{\prime}=G+\Delta G,

|  |  |  |
| --- | --- | --- |
|  | Hâ€²â€‹(Î”â€‹Î¸â€²âˆ’Î”â€‹Î¸)=Î”â€‹Gâˆ’Î”â€‹Hâ€‹Î”â€‹Î¸.H^{\prime}(\Delta\theta^{\prime}-\Delta\theta)\;=\;\Delta G-\Delta H\,\Delta\theta. |  |

Multiply by Hâ€²â£âˆ’1H^{\prime-1} and take norms; the Neumann bound follows from
Hâ€²â£âˆ’1=(I+Hâˆ’1â€‹Î”â€‹H)âˆ’1â€‹Hâˆ’1H^{\prime-1}=(I+H^{-1}\Delta H)^{-1}H^{-1} whenever â€–Hâˆ’1â€‹Î”â€‹Hâ€–<1\|H^{-1}\Delta H\|<1.
âˆ

###### Remark 2 (Robust loss control).

Suppose the per-quote loss is Huber with threshold c>0c>0 and residuals are
locally bounded. Then each quoteâ€™s influence function is bounded by cc,
so there exist constants CJ,CJâ€‹JC\_{J},C\_{JJ} (depending on Jacobian norms) such that,
for forgetting FF,

|  |  |  |
| --- | --- | --- |
|  | â€–Î”â€‹Gâ€–â‰¤CJâ€‹câ€‹|F|,â€–Î”â€‹Hâ€–â‰¤CJâ€‹Jâ€‹|F|.\|\Delta G\|\;\leq\;C\_{J}\,c\,|F|,\qquad\|\Delta H\|\;\leq\;C\_{JJ}\,|F|. |  |

Consequently, if Hâ€²H^{\prime} is well conditioned,

|  |  |  |
| --- | --- | --- |
|  | â€–Î¸â€²âˆ’Î¸â€–â‰²Îºâ€‹(Hâ€²)â€‹(câ€‹|F|+|F|â€‹â€–Î”â€‹Î¸â€–),\|\theta^{\prime}-\theta\|\;\lesssim\;\kappa(H^{\prime})\big(c\,|F|+|F|\,\|\Delta\theta\|\big), |  |

i.e., the change scales linearly with the forgotten mass and the conditioning.

###### Remark 3 (Conditioning links: eigenvalues and Neumann bound).

Assume throughout the spectral (2-)norm and that H,Hâ€²H,H^{\prime} are symmetric positive definite.
Then â€–Hâ€²â£âˆ’1â€–2=1/Î»minâ€‹(Hâ€²)\|H^{\prime-1}\|\_{2}=1/\lambda\_{\min}(H^{\prime}), and the stability estimate

|  |  |  |
| --- | --- | --- |
|  | â€–Î”â€‹Î¸â€²âˆ’Î”â€‹Î¸â€–â‰¤â€–Î”â€‹Gâ€–2+â€–Î”â€‹Hâ€–2â€‹â€–Î”â€‹Î¸â€–2Î»minâ€‹(Hâ€²)\|\Delta\theta^{\prime}-\Delta\theta\|\;\leq\;\frac{\|\Delta G\|\_{2}+\|\Delta H\|\_{2}\,\|\Delta\theta\|\_{2}}{\lambda\_{\min}(H^{\prime})} |  |

shows that larger Î»minâ€‹(Hâ€²)\lambda\_{\min}(H^{\prime}) (better conditioning) improves robustness.
Moreover, if â€–Hâˆ’1â€‹Î”â€‹Hâ€–2<1\|H^{-1}\Delta H\|\_{2}<1, the Neumann expansion yields

|  |  |  |
| --- | --- | --- |
|  | â€–Hâ€²â£âˆ’1â€–2â‰¤â€–Hâˆ’1â€–2â€‰1âˆ’â€–Hâˆ’1â€‹Î”â€‹Hâ€–2=1Î»minâ€‹(H)â€‹(1âˆ’â€–Hâˆ’1â€‹Î”â€‹Hâ€–2),\|H^{\prime-1}\|\_{2}\;\leq\;\frac{\|H^{-1}\|\_{2}}{\,1-\|H^{-1}\Delta H\|\_{2}\,}\;=\;\frac{1}{\,\lambda\_{\min}(H)\,\big(1-\|H^{-1}\Delta H\|\_{2}\big)}\,, |  |

and consequently

|  |  |  |
| --- | --- | --- |
|  | â€–Î”â€‹Î¸â€²âˆ’Î”â€‹Î¸â€–â‰¤â€–Î”â€‹Gâ€–2+â€–Î”â€‹Hâ€–2â€‹â€–Î”â€‹Î¸â€–2Î»minâ€‹(H)â€‹(1âˆ’â€–Hâˆ’1â€‹Î”â€‹Hâ€–2).\|\Delta\theta^{\prime}-\Delta\theta\|\;\leq\;\frac{\|\Delta G\|\_{2}+\|\Delta H\|\_{2}\,\|\Delta\theta\|\_{2}}{\lambda\_{\min}(H)\,\big(1-\|H^{-1}\Delta H\|\_{2}\big)}. |  |

Finally, by Weylâ€™s inequality,
Î»minâ€‹(Hâ€²)â‰¥Î»minâ€‹(H)âˆ’â€–Î”â€‹Hâ€–2\lambda\_{\min}(H^{\prime})\geq\lambda\_{\min}(H)-\|\Delta H\|\_{2},
so Hâ€²H^{\prime} remains positive definite whenever â€–Î”â€‹Hâ€–2<Î»minâ€‹(H)\|\Delta H\|\_{2}<\lambda\_{\min}(H).

The above results collectively ensure that the curvature downdate remains numerically stable and the Gaussâ€“Newton step is well defined under moderate forgetting.
Having established the local stability and conditioning properties, we next turn to the fast refactorization approach.

### 2.3 Fast refactor operator

Machine unlearning is not defined by the speed of recomputation, but rather by the semantics of the data removal.
We could formalize it such that an algorithm carries an unlearning spirit after deleting subset FâŠ‚DF\subset D and the resulting model parameters are *indistinguishable* from those obtained by retraining on Dâˆ–FD\setminus F.
Therefore, in the sense of [bourtoule2021machine], the sharded recomputation remains a legitimate unlearning operator, laying the conceptual definition of unlearning in our framework.
Given our points of concern embark on computational capability rather than on issues related to the well articulated purposes of machine unlearning (eg. privacy), we remark the necessity of offering an efficient implementation of that same operator for reasons we discuss shortly.

We now introduce a faster data-free forgetting operator that yields the same Gaussâ€“Newton (GN) update as retraining on the retained set, without accessing raw quotes once a cache is built.
Throughout, we fix a reference parameter Î¸refâˆˆÎ˜\theta^{\mathrm{ref}}\in\Theta and work with the GN normal equations at this reference.
Let FâŠ‚DF\subset D denote a subset of quotes to be forgotten and (Hâ€²,Gâ€²)(H^{\prime},G^{\prime}) denote the post-forgetting aggregates obtained by subtracting the contributions of FâŠ‚DF\subset D.
Given the Gaussâ€“Newton aggregates are linear in {ui,Ïˆi}\{u\_{i},\psi\_{i}\}, the effect of removing
FF can be represented exactly by subtraction:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Hâ€²=Hâˆ’âˆ‘iâˆˆFÏˆi,Gâ€²=Gâˆ’âˆ‘iâˆˆFui.H^{\prime}\;=\;H-\sum\_{i\in F}\psi\_{i},\qquad G^{\prime}\;=\;G-\sum\_{i\in F}u\_{i}. |  | (11) |

The updated parameter is then obtained by solving once

|  |  |  |  |
| --- | --- | --- | --- |
|  | (Hâ€²+Î»â€‹I)â€‹Î”â€‹Î¸â€²=Gâ€²,Î¸â€²=Î¸ref+Î”â€‹Î¸â€².(H^{\prime}+\lambda I)\,\Delta\theta^{\prime}=G^{\prime},\qquad\theta^{\prime}=\theta^{\mathrm{ref}}+\Delta\theta^{\prime}. |  | (12) |

Equations ([12](https://arxiv.org/html/2511.14980v1#S2.E12 "In 2.3 Fast refactor operator â€£ 2 Formulation of the unlearning problem â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework")) require only the cached statistics (ui,Ïˆi)(u\_{i},\psi\_{i}), not the raw market data (xi,yi)(x\_{i},y\_{i}), and thus implement a *data-free forgetting operator*.
At the fixed linearization Î¸ref\theta^{\mathrm{ref}}, this refactoring exactly removes the influence of the
forgotten subset from the calibration system.
The procedure achieves the same solution as a full retraining on Dâˆ–FD\setminus F, up to machine precision, while
avoiding all repricing and re-differentiation.
Fast refactorization operates under the same local regularity conditions introduced in SectionÂ [2.2](https://arxiv.org/html/2511.14980v1#S2.SS2 "2.2 Sharded recompute operator â€£ 2 Formulation of the unlearning problem â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework"), namely smoothness, local strong convexity (possibly enforced via a small ridge term Î»â€‹I\lambda I), Lipschitz continuity of the Jacobian, and bounded residuals.
These ensure that the refactorized system (Hâ€²+Î»â€‹I)â€‹Î”â€‹Î¸â€²=Gâ€²(H^{\prime}+\lambda I)\Delta\theta^{\prime}=G^{\prime} remains well-posed and that all prior analytical results remain valid.

Each option quote ii contributes via its residual
riâ€‹(Î¸ref)=yiâˆ’mâ€‹(xi;Î¸ref)r\_{i}(\theta^{\mathrm{ref}})=y\_{i}-m(x\_{i};\theta^{\mathrm{ref}}) and local sensitivity Jiâ€‹(Î¸ref)=âˆ‡Î¸mâ€‹(xi;Î¸ref)J\_{i}(\theta^{\mathrm{ref}})=\nabla\_{\theta}m(x\_{i};\theta^{\mathrm{ref}}).
Define ui:=JiâŠ¤â€‹riu\_{i}:=J\_{i}^{\top}r\_{i} and Ïˆi:=JiâŠ¤â€‹Ji\psi\_{i}:=J\_{i}^{\top}J\_{i}.
Then G=âˆ‘iuiG=\sum\_{i}u\_{i} and H=âˆ‘iÏˆiH=\sum\_{i}\psi\_{i} are the Gaussâ€“Newton aggregates at Î¸ref\theta^{\mathrm{ref}}, and the update solves Hâ€‹Î”â€‹Î¸=GH\,\Delta\theta=G.
Thus the collection {(ui,Ïˆi)}i\{(u\_{i},\psi\_{i})\}\_{i} is *algebraically sufficient for the linearized calibration at Î¸ref\theta^{\mathrm{ref}}*: once stored, the influence of any subset FF can be removed exactly by subtraction, Hâ€²=Hâˆ’âˆ‘iâˆˆFÏˆiH^{\prime}=H-\sum\_{i\in F}\psi\_{i} and Gâ€²=Gâˆ’âˆ‘iâˆˆFuiG^{\prime}=G-\sum\_{i\in F}u\_{i},
without revisiting raw data.
If a robust loss is used, the same identities hold with per-quote weights (ui=wiâ€‹JiâŠ¤â€‹riu\_{i}=w\_{i}J\_{i}^{\top}r\_{i}, Ïˆi=wiâ€‹JiâŠ¤â€‹Ji\psi\_{i}=w\_{i}J\_{i}^{\top}J\_{i}).
These statistics are tied to the chosen reference; upon relinearization (Î¸refâ†¦Î¸new\theta^{\mathrm{ref}}\mapsto\theta^{\mathrm{new}}), the pairs (ui,Ïˆi)(u\_{i},\psi\_{i}) should be recomputed at the new reference.

Although we build fast refactor operator on the foundations of the sharded recomputation, the latter does not actually need shards.
During the initial training we build caches, (H,G)(H,G), and per-shard aggregates, (Hk,Gk)(H\_{k},G\_{k}).
In the sharded recomputation, with some quotes removed, we reopen only the shards that contain them, then recompute (Hk,Gk)(H\_{k},G\_{k}) for those shards and sum up with others.
In fast refactor we go one step further and directly subtract each forgotten quoteâ€™s contribution from the cached global (H,K)(H,K) so that there no longer exists the need to reopen or recompute the shards, making it completely data-free and instantaneous.
Inclusion of shards, then, in fast refactor might seem contradictory.
However, we remark that the shards play important roles in categorizing the forgetting set (although removal is not driven by shards) and more importantly it provides security in the case some of quotes lacking cached Jacobians, JiJ\_{i}.

Technically speaking, in the fast refactor variant, unlearning operates at quote granularity as once per-quote GN statistics (JiTâ€‹Ji,JiTâ€‹ri)(J\_{i}^{T}J\_{i},J\_{i}^{T}r\_{i}) are cashed at the reference point, removing any subset FâŠ‚DF\subset D amounts to simplistic algebraic operations of the global normal equations, unrelated to how the data were originally sharded.
In our implementation, the fast refactorization step forms Hâ€²=Hâˆ’âˆ‘iâˆˆFJiâŠ¤â€‹JiH^{\prime}=H-\sum\_{i\in F}J\_{i}^{\top}J\_{i} and Gâ€²=Gâˆ’âˆ‘iâˆˆFJiâŠ¤â€‹riG^{\prime}=G-\sum\_{i\in F}J\_{i}^{\top}r\_{i} explicitly, followed by a fresh Cholesky factorization of Hâ€²H^{\prime}.
This retains exactness under the fixed linearization while avoiding any recomputation over the retained dataset.
Although a true rank-1 Cholesky downdate would further reduce cost to Oâ€‹(p2â€‹|F|)O(p^{2}|F|), we found the explicit rebuild to be numerically safer and sufficiently fast for moderate pp.

While the concept of subtracting per-sample statistics is trivial for linear models,
it becomes nontrivial for nonlinear calibration because the residuals and Jacobians
depend on the current parameter estimate.
Naively removing quotes invalidates the current linearization, so retraining from scratch remains the default.
Yet, in practice, calibration pipelines already store large intermediate structuresâ€™
per-quote sensitivities, residuals, and curvature estimates for diagnostic or
parallel-computation purposes.
This suggests the possibility of an *operator-level unlearning* mechanism: removing data by algebraic downdating of the cached normal equations, without reprocessing the raw option surface.
In our framework, this takes the form of Gaussâ€“Newton updates on refactored (Hâ€²,Gâ€²)(H^{\prime},G^{\prime}), achieving exact unlearning at a fixed linearization.

The subtraction step removes the statistical influence of each forgotten quote because,
under Gaussâ€“Newton linearization, the normal equations decompose additively across data points.
Each quote ii contributes (Ïˆi,ui)=(JiâŠ¤â€‹Ji,JiâŠ¤â€‹ri)(\psi\_{i},u\_{i})=(J\_{i}^{\top}J\_{i},\,J\_{i}^{\top}r\_{i}) to the global
system (H,G)(H,G). Solving (H+Î»â€‹I)â€‹Î”â€‹Î¸=G(H+\lambda I)\Delta\theta=G thus depends on the data only
through these linear aggregates. Forgetting a subset FF corresponds to replacing

|  |  |  |  |
| --- | --- | --- | --- |
|  | (Hâ€²,Gâ€²)=(Hâˆ’âˆ‘iâˆˆFÏˆi,Gâˆ’âˆ‘iâˆˆFui),(H^{\prime},G^{\prime})=\big(H-\sum\_{i\in F}\psi\_{i},\;G-\sum\_{i\in F}u\_{i}\big), |  | (13) |

which is identical to the system built on Dâˆ–FD\setminus F.
Consequently, the updated parameter Î¸â€²=Î¸ref+Î”â€‹Î¸â€²\theta^{\prime}=\theta^{\mathrm{ref}}+\Delta\theta^{\prime}
matches the retraining result under the same linearization, with no residual dependence on FF.
This equality Hâ€²=Hâ‹†H^{\prime}=H^{\star}, Gâ€²=Gâ‹†G^{\prime}=G^{\star} is formalized in such as:

###### Proposition 4 (Exactness under fixed linearization).

Let Hâ‹†,Gâ‹†H^{\star},G^{\star} denote the Gaussâ€“Newton aggregates constructed directly on
the retained set Dâˆ–FD\setminus F at Î¸ref\theta^{\mathrm{ref}}:

|  |  |  |
| --- | --- | --- |
|  | Hâ‹†=âˆ‘iâˆˆDâˆ–FJiâ€‹(Î¸ref)âŠ¤â€‹Jiâ€‹(Î¸ref),Gâ‹†=âˆ‘iâˆˆDâˆ–FJiâ€‹(Î¸ref)âŠ¤â€‹riâ€‹(Î¸ref).H^{\star}=\sum\_{i\in D\setminus F}J\_{i}(\theta^{\mathrm{ref}})^{\top}J\_{i}(\theta^{\mathrm{ref}}),\qquad G^{\star}=\sum\_{i\in D\setminus F}J\_{i}(\theta^{\mathrm{ref}})^{\top}r\_{i}(\theta^{\mathrm{ref}}). |  |

Then Hâ‹†=Hâ€²H^{\star}=H^{\prime} and Gâ‹†=Gâ€²G^{\star}=G^{\prime}, where
(Hâ€²,Gâ€²)=(Hâˆ’âˆ‘iâˆˆFÏˆi,Gâˆ’âˆ‘iâˆˆFui)(H^{\prime},G^{\prime})=(H-\sum\_{i\in F}\psi\_{i},\,G-\sum\_{i\in F}u\_{i}) are the refactored
aggregates at Î¸ref\theta^{\mathrm{ref}}.
Consequently, for the same Î»â‰¥0\lambda\geq 0, the update Î¸â€²\theta^{\prime} produced by
(Hâ€²+Î»â€‹I)â€‹Î”â€‹Î¸â€²=Gâ€²(H^{\prime}+\lambda I)\Delta\theta^{\prime}=G^{\prime} coincides with the parameter obtained by
retraining the Gaussâ€“Newton system on Dâˆ–FD\setminus F at Î¸ref\theta^{\mathrm{ref}}.

###### Proof.

By the additive decompositions at Î¸ref\theta^{\mathrm{ref}},
H=âˆ‘iâˆˆDÏˆi=âˆ‘iâˆˆDJiâŠ¤â€‹JiH=\sum\_{i\in D}\psi\_{i}=\sum\_{i\in D}J\_{i}^{\top}J\_{i} and
G=âˆ‘iâˆˆDui=âˆ‘iâˆˆDJiâŠ¤â€‹riG=\sum\_{i\in D}u\_{i}=\sum\_{i\in D}J\_{i}^{\top}r\_{i}.
Subtracting forgotten contributions gives
Hâ€²=Hâˆ’âˆ‘iâˆˆFÏˆi=âˆ‘iâˆˆDâˆ–FJiâŠ¤â€‹Ji=Hâ‹†H^{\prime}=H-\sum\_{i\in F}\psi\_{i}=\sum\_{i\in D\setminus F}J\_{i}^{\top}J\_{i}=H^{\star}
and similarly Gâ€²=âˆ‘iâˆˆDâˆ–FJiâŠ¤â€‹ri=Gâ‹†G^{\prime}=\sum\_{i\in D\setminus F}J\_{i}^{\top}r\_{i}=G^{\star}.
Thus the regularized systems (Hâ€²+Î»â€‹I)â€‹Î”â€‹Î¸â€²=Gâ€²(H^{\prime}+\lambda I)\Delta\theta^{\prime}=G^{\prime} and
(Hâ‹†+Î»â€‹I)â€‹Î”â€‹Î¸â‹†=Gâ‹†(H^{\star}+\lambda I)\Delta\theta^{\star}=G^{\star} are identical, yielding
Î”â€‹Î¸â€²=Î”â€‹Î¸â‹†\Delta\theta^{\prime}=\Delta\theta^{\star} and hence the same Î¸â€²\theta^{\prime}.
âˆ

###### Remark 4.

The operator we suggest in this part acts solely on the cached per-quote statistics (ui,Ïˆi)(u\_{i},\psi\_{i}) and the precomputed global aggregates (H,G)(H,G) at Î¸ref\theta^{\mathrm{ref}}.
It therefore removes the influence of the forgotten set FF exactly under the Gaussâ€“Newton linearization without accessing any raw market quotes or re-evaluating model prices.

Let Î”â€‹H:=Hâ€²âˆ’H\Delta H:=H^{\prime}-H and Î”â€‹G:=Gâ€²âˆ’G\Delta G:=G^{\prime}-G. Let Î”â€‹Î¸\Delta\theta and Î”â€‹Î¸â€²\Delta\theta^{\prime} be the GN steps at Î¸ref\theta^{\mathrm{ref}} on DD and Dâˆ–FD\setminus F, respectively, both with the same Î»\lambda.
We use the vector 2âˆ’2-norm and the induced operator norm for matrices.

###### Proposition 5 (Linearized stability).

With the same Î»\lambda (so Hâ€²+Î»â€‹IH^{\prime}+\lambda I is invertible),

|  |  |  |
| --- | --- | --- |
|  | â€–Î”â€‹Î¸â€²âˆ’Î”â€‹Î¸â€–â‰¤â€–(Hâ€²+Î»â€‹I)âˆ’1â€–â€‹(â€–Î”â€‹Gâ€–+â€–Î”â€‹Hâ€–â€‹â€–Î”â€‹Î¸â€–).\|\Delta\theta^{\prime}-\Delta\theta\|\;\leq\;\|(H^{\prime}+\lambda I)^{-1}\|\;\Big(\|\Delta G\|+\|\Delta H\|\,\|\Delta\theta\|\Big). |  |

In particular, if |F|/|D||F|/|D| is small and Hâ€²+Î»â€‹IH^{\prime}+\lambda I is well conditioned, then â€–Î”â€‹Î¸â€²âˆ’Î”â€‹Î¸â€–\|\Delta\theta^{\prime}-\Delta\theta\| is small.

###### Sketch.

Write (H+Î»â€‹I)â€‹Î”â€‹Î¸=G(H+\lambda I)\Delta\theta=G and (Hâ€²+Î»â€‹I)â€‹Î”â€‹Î¸â€²=Gâ€²(H^{\prime}+\lambda I)\Delta\theta^{\prime}=G^{\prime}. Subtract to obtain

|  |  |  |
| --- | --- | --- |
|  | (Hâ€²+Î»â€‹I)â€‹(Î”â€‹Î¸â€²âˆ’Î”â€‹Î¸)=Î”â€‹Gâˆ’Î”â€‹Hâ€‹Î”â€‹Î¸,(H^{\prime}+\lambda I)(\Delta\theta^{\prime}-\Delta\theta)\;=\;\Delta G-\Delta H\,\Delta\theta, |  |

then multiply by (Hâ€²+Î»â€‹I)âˆ’1(H^{\prime}+\lambda I)^{-1} and take norms.
âˆ

###### Proposition 6 (Accuracy after one relinearization).

Let Î¸â‹†\theta^{\star} denote the (local) least-squares solution on Dâˆ–â„±D\setminus\mathcal{F}.
Assume: (i) Jâ€‹(Î¸)J(\theta) is Lipschitz in a neighborhood of Î¸â‹†\theta^{\star} with constant LJL\_{J},
(ii) Jâ€‹(Î¸â‹†)J(\theta^{\star}) has full column rank, and
(iii) the residual at the solution is small, â€–râ€‹(Î¸â‹†)â€–â‰¤Îµ\|r(\theta^{\star})\|\leq\varepsilon.
Let Î¸^\widehat{\theta} be the GN/LM (Levenbergâ€“Marquardt) solution obtained on Dâˆ–â„±D\setminus\mathcal{F} after
one relinearization at Î¸ref+Î”â€‹Î¸â€²\theta^{\mathrm{ref}}+\Delta\theta^{\prime} (same Î»\lambda).
Then there exist constants C1,C2>0C\_{1},C\_{2}>0 (depending on LJL\_{J}, local bounds, and â€–(Hâ€²+Î»â€‹I)âˆ’1â€–\|(H^{\prime}+\lambda I)^{-1}\|) such that

|  |  |  |
| --- | --- | --- |
|  | â€–Î¸^âˆ’Î¸â‹†â€–â‰¤C1â€‹â€–Î”â€‹Î¸â€²â€–2+C2â€‹Îµâ€‹â€–Î”â€‹Î¸â€²â€–.\|\widehat{\theta}-\theta^{\star}\|\;\leq\;C\_{1}\,\|\Delta\theta^{\prime}\|^{2}\;+\;C\_{2}\,\varepsilon\,\|\Delta\theta^{\prime}\|. |  |

In particular, in the small-residual regime (Îµâ‰ˆ0)(\varepsilon\approx 0),
â€–Î¸^âˆ’Î¸â‹†â€–=ğ’ªâ€‹(â€–Î”â€‹Î¸â€²â€–2)\|\widehat{\theta}-\theta^{\star}\|=\mathcal{O}(\|\Delta\theta^{\prime}\|^{2}).

###### Sketch.

Standard Gaussâ€“Newton local analysis (Newtonâ€“Kantorovich style):
the model error from relinearization is ğ’ªâ€‹(â€–Î”â€‹Î¸â€²â€–2)\mathcal{O}(\|\Delta\theta^{\prime}\|^{2}) by Lipschitz JJ.
Mapping this through the normal equations introduces â€–(Hâ€²+Î»â€‹I)âˆ’1â€–\|(H^{\prime}+\lambda I)^{-1}\|.
The residual term yields the mixed Îµâ€‹â€–Î”â€‹Î¸â€²â€–\varepsilon\,\|\Delta\theta^{\prime}\| contribution.
âˆ

Shard-level recompute is exact at a fixed linearization by linearity of sums yet it still requires opening the affected shards.
The refactor operator strengthens this to a *data-free* update by using cached per-quote statistics.
In linearity, forgetting is exact by subtraction Our result lifts this idea to *nonlinear* parametric models via Gaussâ€“Newton linearization, providing (to our knowledge) the first data-free exact forgetting operator for nonlinear least squares in financial calibration.
Once (ui,Ïˆi)(u\_{i},\psi\_{i}) are retained and raw quotes purged, subsequent unlearning requests are executed algebraically.
If (ui,Ïˆi)(u\_{i},\psi\_{i}) are deemed sensitive, they can be encrypted or perturbed; bounds in PropositionÂ [5](https://arxiv.org/html/2511.14980v1#Thmtheorem5 "Proposition 5 (Linearized stability). â€£ 2.3 Fast refactor operator â€£ 2 Formulation of the unlearning problem â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework") quantify the resulting parameter drift.
We remark that the cache does not contain raw market quotes or strikeâ€“maturity grids.
Instead it stores derived Jacobian vectors ui=Jiu\_{i}=J\_{i} and Ïˆi=Jiâ€‹ri\psi\_{i}=J\_{i}r\_{i}, together with the global normal equations H=âˆ‘iJiâŠ¤â€‹JiH=\sum\_{i}J\_{i}^{\top}J\_{i}, G=âˆ‘iJiâ€‹riG=\sum\_{i}J\_{i}r\_{i}
evaluated at the calibrated parameter Î¸â‹†\theta^{\star}.
These quantities are sufficient for a local Gaussâ€“Newton update but
contain no reconstructive information about individual data points.

###### Remark 5.

Machine unlearning is effected by removing the contributions of
forgotten quotes from the cached statistics:

|  |  |  |
| --- | --- | --- |
|  | Hâ€²=Hâˆ’âˆ‘iâˆˆâ„±JiâŠ¤â€‹Ji,Gâ€²=Gâˆ’âˆ‘iâˆˆâ„±Jiâ€‹ri.H^{\prime}=H-\sum\_{i\in\mathcal{F}}J\_{i}^{\top}J\_{i},\qquad G^{\prime}=G-\sum\_{i\in\mathcal{F}}J\_{i}r\_{i}. |  |

The updated parameters
Î¸fast=(Hâ€²+Î»â€‹I)âˆ’1â€‹Gâ€²\theta\_{\text{fast}}=(H^{\prime}+\lambda I)^{-1}G^{\prime}
coincide with those obtained by full retraining on the retained set
(up to numerical precision).
Hence the method satisfies the formal definition of *machine unlearning*
as the ability to expunge a subsetâ€™s influence from the trained model
without re-accessing the original data, [guo2019certified, bourtoule2021machine].

We have several remarks that we believe is timely.
While the proposed unlearning operators reproduce the Gaussâ€“Newton update on the retained dataset, occasional increases in performance metrics (RMSE, in our case) may still be observed when unlearning is applied to very small subsets of option quotes.
Two conceptually distinct mechanisms may explain this behavior.
A single Gaussâ€“Newton step is locally exact only in a neighborhood of a previously converged solution.
When the forget set is small, the displacement of the optimum is also small, and one correction typically recovers the new minimizer to machine precision.
However, when the retained dataset becomes extremely small, the linearization may no longer be valid, and the residuals may appear unstable.
Another one is, independently of the Gaussâ€“Newton linearization error, numerical instability
could arise if the curvature matrix Hâ€²H^{\prime} becomes ill-conditioned after forgetting111This could especially amplify if you remove a few influential quotes (e.g. some maturities or deep-OTMs that strongly shape volatility) which leaves the new curvature matrix with extremely small eigenvalues (large condition number) and perhaps with directions in parameter space that are almost unconstrained by the retained data..
This occurs when the retained quotes provide insufficient informational content, leading to exceedingly small eigenvalues and nearly unconstrained parameter directions.
In such cases, even correct cached gradients could produce large parameter excursions and elevated RMSE.
Such optimization refinements fall outside the scope of our unlearning study as it is sampling related rather than the methodology.
In summary, the occasional RMSE deviations observed for extremely small retained
datasets arise from the standard numerical behavior of Gaussâ€“Newton and Levenbergâ€“Marquardt schemes, rather than from the unlearning operators themselves, [ait1998nonparametric].

Before closing the section, we include another proposition on the computational complexity of the calibration and both unlearning operators.
We defer this result to the end of the section so as not to interrupt the flow of the theoretical development in the previous subsections as the developed approaches are not related to the Heston model that we use for exemplary purposes.
Given our framework is fully model-agnostic and applies to any differentiable option pricing map, the complexity statement becomes most transparent when expressed for the Heston model, where each price evaluation is performed via a Fourierâ€“Simpson integral with NuN\_{u} frequency nodes.
The following proposition therefore specializes the analysis to this setting, which is also the one used in our numerical experiments.

###### Proposition 7 (Computational complexity of calibration and unlearning operators).

Let NN denote the number of option quotes, NuN\_{u} the number of Fourierâ€“Simpson integration nodes used in the Heston pricer, and pp the dimension of the parameter vector (e.g. p=5p=5 for the Heston model). Assume pp is fixed and small. Then, under a Gaussâ€“Newton calibration scheme at a fixed reference Î¸ref\theta^{\mathrm{ref}}, the following complexity bounds hold:

1. [(i)]
2. 1.

   Full recalibration. A single Gaussâ€“Newton iteration on a dataset of size NN has time complexity

   |  |  |  |
   | --- | --- | --- |
   |  | Tretrain=ğ’ªâ€‹(Nâ€‹Nu)+ğ’ªâ€‹(p3),T\_{\mathrm{retrain}}=\mathcal{O}(N\,N\_{u})+\mathcal{O}(p^{3}), |  |

   where the dominant cost is the evaluation of NN Heston prices via Fourierâ€“Simpson quadrature. The ğ’ªâ€‹(p3)\mathcal{O}(p^{3}) term arises from assembling and solving the pÃ—pp\times p normal equations.
3. 2.

   Sharded recomputation. Let D=â‹ƒk=1KDkD=\bigcup\_{k=1}^{K}D\_{k} be a partition of the data into KK shards and let Kâ€‹(F)âŠ†{1,â€¦,K}K(F)\subseteq\{1,\dots,K\} denote the set of shards affected by a forget set FâŠ‚DF\subset D. Denote by Neffâ€‹(F)N\_{\mathrm{eff}}(F) the number of quotes in â‹ƒkâˆˆKâ€‹(F)Dk\bigcup\_{k\in K(F)}D\_{k}. Then a sharded recomputation step has time complexity

   |  |  |  |
   | --- | --- | --- |
   |  | Trecompâ€‹(F)=ğ’ªâ€‹(Neffâ€‹(F)â€‹Nu)+ğ’ªâ€‹(p3),T\_{\mathrm{recomp}}(F)=\mathcal{O}\big(N\_{\mathrm{eff}}(F)\,N\_{u}\big)+\mathcal{O}(p^{3}), |  |

   i.e. it is equivalent to a full Gaussâ€“Newton step restricted to the affected shards. In the worst case Neffâ€‹(F)â‰ˆNN\_{\mathrm{eff}}(F)\approx N, and Trecompâ€‹(F)T\_{\mathrm{recomp}}(F) degenerates to TretrainT\_{\mathrm{retrain}}.
4. 3.

   Fast refactorization. Suppose that, at Î¸ref\theta^{\mathrm{ref}}, per-quote Gaussâ€“Newton statistics

   |  |  |  |
   | --- | --- | --- |
   |  | ui=Jiâ€‹(Î¸ref)âŠ¤â€‹riâ€‹(Î¸ref),Ïˆi=Jiâ€‹(Î¸ref)âŠ¤â€‹Jiâ€‹(Î¸ref)u\_{i}=J\_{i}(\theta^{\mathrm{ref}})^{\top}r\_{i}(\theta^{\mathrm{ref}}),\qquad\psi\_{i}=J\_{i}(\theta^{\mathrm{ref}})^{\top}J\_{i}(\theta^{\mathrm{ref}}) |  |

   and the global aggregates H=âˆ‘iÏˆiH=\sum\_{i}\psi\_{i}, G=âˆ‘iuiG=\sum\_{i}u\_{i} have been cached. Then a fast refactorization unlearning request for a forget set FF can be carried out in

   |  |  |  |
   | --- | --- | --- |
   |  | Tfastâ€‹(F)=ğ’ªâ€‹(|F|â€‹p2)+ğ’ªâ€‹(p3)T\_{\mathrm{fast}}(F)=\mathcal{O}(|F|\,p^{2})+\mathcal{O}(p^{3}) |  |

   time, corresponding to subtracting {Ïˆi,ui}iâˆˆF\{\psi\_{i},u\_{i}\}\_{i\in F} from (H,G)(H,G) and solving the refactored pÃ—pp\times p linear system. For fixed pp, this is ğ’ªâ€‹(|F|)+ğ’ªâ€‹(1)\mathcal{O}(|F|)+\mathcal{O}(1), independent of NN and NuN\_{u}.

In particular, for fixed pp, both full recalibration and sharded recomputation scale linearly in NN and approximately linearly in NuN\_{u}, i.e. ğ’ªâ€‹(Nâ€‹Nu)\mathcal{O}(N\,N\_{u}) in the dominant term, whereas the fast refactorization operator has per-request complexity independent of NN and NuN\_{u} and grows only with the size of the forget set FF.

###### Proof.

For each quote ii, evaluation of the Heston price mâ€‹(xi;Î¸)m(x\_{i};\theta) via Fourierâ€“Simpson quadrature requires ğ’ªâ€‹(Nu)\mathcal{O}(N\_{u}) floating point operations, as the characteristic function Ï†â€‹(u;Î¸)\varphi(u;\theta) and the integrand are evaluated at NuN\_{u} frequency nodes and combined by a weighted summation. Thus, pricing all NN quotes at a fixed parameter vector costs ğ’ªâ€‹(Nâ€‹Nu)\mathcal{O}(N\,N\_{u}) operations. The Gaussâ€“Newton step additionally forms residuals rir\_{i} and Jacobians JiJ\_{i}, and accumulates

|  |  |  |
| --- | --- | --- |
|  | H=âˆ‘i=1NJiâŠ¤â€‹Ji,G=âˆ‘i=1NJiâŠ¤â€‹ri,H=\sum\_{i=1}^{N}J\_{i}^{\top}J\_{i},\qquad G=\sum\_{i=1}^{N}J\_{i}^{\top}r\_{i}, |  |

which require at most a constant factor overhead per quote when pp is fixed. Solving the normal equations (H+Î»â€‹I)â€‹Î”â€‹Î¸=G(H+\lambda I)\Delta\theta=G by, e.g., Cholesky factorization, has cost ğ’ªâ€‹(p3)\mathcal{O}(p^{3}). This proves (i).

For sharded recomputation, only quotes in shards kâˆˆKâ€‹(F)k\in K(F) are repriced and their Jacobians recomputed, while unaffected shards reuse their cached (Hk,Gk)(H\_{k},G\_{k}). If Neffâ€‹(F)N\_{\mathrm{eff}}(F) denotes the total number of quotes in the affected shards, then the cost of recomputing their contributions is ğ’ªâ€‹(Neffâ€‹(F)â€‹Nu)\mathcal{O}(N\_{\mathrm{eff}}(F)\,N\_{u}), followed by the same ğ’ªâ€‹(p3)\mathcal{O}(p^{3}) solve on the updated global system. In the worst case, if Kâ€‹(F)={1,â€¦,K}K(F)=\{1,\dots,K\}, then Neffâ€‹(F)â‰ˆNN\_{\mathrm{eff}}(F)\approx N and the complexity coincides with full recalibration, establishing (ii).

For fast refactorization, no repricing or Jacobian evaluation is performed once the cache is built. Each forgotten quote iâˆˆFi\in F contributes a rank-one downdate222Each forgotten quote ii contributes a small pÃ—pp\times p matrix
Ïˆi=JiâŠ¤â€‹Ji\psi\_{i}=J\_{i}^{\top}J\_{i} and vector ui=JiâŠ¤â€‹riu\_{i}=J\_{i}^{\top}r\_{i} to the downdate
of (H,G)(H,G).
 Ïˆiâˆˆâ„pÃ—p\psi\_{i}\in\mathbb{R}^{p\times p} and a vector downdate uiâˆˆâ„pu\_{i}\in\mathbb{R}^{p} to (H,G)(H,G). Updating

|  |  |  |
| --- | --- | --- |
|  | Hâ€²=Hâˆ’âˆ‘iâˆˆFÏˆi,Gâ€²=Gâˆ’âˆ‘iâˆˆFuiH^{\prime}=H-\sum\_{i\in F}\psi\_{i},\qquad G^{\prime}=G-\sum\_{i\in F}u\_{i} |  |

requires ğ’ªâ€‹(|F|â€‹p2)\mathcal{O}(|F|\,p^{2}) operations. A fresh Cholesky factorization of Hâ€²H^{\prime} and back-substitution then cost ğ’ªâ€‹(p3)\mathcal{O}(p^{3}). Since pp is fixed and small, these costs are independent of NN and NuN\_{u}, and the total complexity is ğ’ªâ€‹(|F|â€‹p2)+ğ’ªâ€‹(p3)\mathcal{O}(|F|\,p^{2})+\mathcal{O}(p^{3}), proving (iii).
âˆ

We finally note that our exactness statements are always with respect to the Gaussâ€“Newton linearization at a reference point, not claiming global equivalence of fully iterated nonlinear solvers.
In practice, however, calibration is often near an optimum and a single GN/LM step is used as a local adjustment, which is precisely the regime our operators are designed for.

## 3 Illustrations

First333All scripts are in Python and all associated numerical illustrations presented in this manuscript are carried out on a system with i7 Core with 2.20 GHz and 16 GB RAM., we remark that in Figures of this section, unless stated otherwise, report the median over 3-5 random unlearning realizations per fraction, ensuring robustness to randomness in forgotten subsets.
We synthetically generate a surface of European call prices under the Heston model with known ground-truth parameters such as Î¸true=(Îº,Î¸v,Ïƒv,Ï,v0)=(2.0,â€‰0.06,â€‰0.30,âˆ’0.6,â€‰0.06),\theta\_{\mathrm{true}}=(\kappa,\theta\_{v},\sigma\_{v},\rho,v\_{0})=(2.0,\,0.06,\,0.30,\,-0.6,\,0.06), with r=0.01,S0=100.r=0.01,S\_{0}=100.
We generate a path of either 90 trading days for a small sample experiment or 180 trading days for a large sample experiment (Eulerâ€“Maruyama with correlated Brownian shocks; with Î”â€‹t=1/252\Delta t=1/252 throughout this section).
For each day in the path we form European call quotes at maturities Tâˆˆ{30,60}T\in\{30,60\} days (i.e., {30,60}/252\{30,60\}/252 years) and strikes Xâˆˆ{90,100,110}X\in\{90,100,110\} for small sample experiment and maturities Tâˆˆ{30,60,90}T\in\{30,60,90\} days and strikes Xâˆˆ{80,90,100,110,120}X\in\{80,90,100,110,120\} for large sample experiment.
Option prices mâ€‹(xi;Î¸)m(x\_{i};\theta) are computed with the semi-analytic Heston formula via Fourier inversion and Simpsonâ€™s rule with either Umax=50U\_{\max}=50 and Nu=180N\_{u}=180 nodes or Umax=120,Nu=800U\_{\max}=120,\;N\_{u}=800 again depending on the sample size.
To emulate measurement noise, we perturb each price either by Îµiâˆ¼ğ’©â€‹(0,Ïƒ2)\varepsilon\_{i}\sim\mathcal{N}(0,\sigma^{2}) with Ïƒ=10âˆ’3\sigma=10^{-3} or Îµiâˆ¼ğ’©â€‹(0,(5Ã—10âˆ’4)2)\varepsilon\_{i}\sim\mathcal{N}(0,(5\times 10^{-4})^{2}) for the small and large sample size, respectively.
We remark that the total number of quotes, NN, depend on the path horizon and coverage and is thus not fixed a priori.
We partition the quotes by calendar time into contiguous shards of either 10 or 30 days, similarly.
Starting from Î¸ref=(1.0,â€‰0.04,â€‰0.20,âˆ’0.3,â€‰0.04)\theta^{\text{ref}}=(1.0,\,0.04,\,0.20,\,-0.3,\,0.04), we run a short Levenbergâ€“Marquardt loop (Gaussâ€“Newton with adaptive damping) to obtain Î¸â‹†\theta^{\star}.
At Î¸â‹†\theta^{\star}, we compute central finite-difference Jacobians Ji=âˆ‡Î¸mâ€‹(xi;Î¸â‹†)J\_{i}=\nabla\_{\theta}m(x\_{i};\theta^{\star}) and residuals ri=yiâˆ’mâ€‹(xi;Î¸â‹†)r\_{i}=y\_{i}-m(x\_{i};\theta^{\star}).
We cache, for each quote ii, ui,Ïˆiâˆˆâ„5u\_{i},\psi\_{i}\in\mathbb{R}^{5} and the global normal equations, (H,G)(H,G) together with a small Tikhonov term Î»=10âˆ’6\lambda=10^{-6} (i.e., we solve with H+Î»â€‹IH+\lambda I).
We also store per-shard, (Hk,Gk)(H\_{k},G\_{k}) given a forget set â„±âŠ‚{1,â€¦,N}\mathcal{F}\subset\{1,\dots,N\}.

Speaking of the cache, we remind that it is not a replica of the training data.
It stores only derivative-based sufficient statistics that summarize the modelâ€™s local curvature at the calibration optimum.
Forgetting operates by algebraic removal of those statistics associated with the forgotten samples, which is exactly what the unlearning literature defines as *data deletion at the parameter level*.
No raw strikes, maturities, or prices are revisited once the cache is built.

Our baseline, as we mentioned before, is the recalibration, full retraining in which we recompute all Jacobians and residuals on the retained subset from the raw quotes, thereby rebuilding the normal equations (Hâ€²,Gâ€²)(H^{\prime},G^{\prime})
and take one Gaussâ€“Newton step to obtain Î¸ret=(Î¸ref+(Hâ€²+Î»â€‹I)âˆ’1â€‹Gâ€²)\theta\_{\text{ret}}=(\theta^{\text{ref}}+(H^{\prime}+\lambda I)^{-1}G^{\prime}).
This represents a full recalibration from scratch and serves as the groundâ€“truth
baseline for evaluating the two unlearning operators.

All timings are on a single laptop core (NumPy/BLAS pinned to one thread).
Because each calibration step involves only a few thousand Heston price evaluations and a single 5Ã—55\times 5 linear solve, wall-clock runtimes are on the order of seconds even in the full configuration (Umax=120,Nu=800U\_{\max}=120,\,N\_{u}=800).
Subsequent unlearning operations reuse cached Jacobians and require no re-pricing, yielding sub-second updates.
This behavior is consistent with the ğ’ªâ€‹(p3)\mathcal{O}(p^{3}) cost of the
Gaussâ€“Newton linear system and the modest number of Fourier nodes per price evaluation.
More intuitively speaking, it is as we are not performing a full market-scale optimization but a one-step linearized Gaussâ€“Newton update on a small synthetic grid.
We also stress that every plotted point is a median across independent random forget sets so that the figures already represents typical behavior, not a single lucky case.

We start by showing equivalence between retraining and our proposed machine unlearning approach, the fast factorization in this case.
By equivalence, we mean that up to machine precision (mostly on the order of 10âˆ’1310^{-13}); both approaches provide identical results.
We observe an exemplary comparison in FigureÂ [1](https://arxiv.org/html/2511.14980v1#S3.F1 "Figure 1 â€£ 3 Illustrations â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework") with the Heston variables separately shown, and the yy-axis shows the difference of Î¸fastâˆ’Î¸retrain\theta\_{\text{fast}}-\theta\_{\text{retrain}}.
We remark again that we reserve Î¸v\theta\_{v} for a parameter of the Heston model, and Î¸\theta for the parameter space of the Heston model.
SubfigureÂ [1(a)](https://arxiv.org/html/2511.14980v1#S3.F1.sf1 "In Figure 1 â€£ 3 Illustrations â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework") shows per-parameter distributions of the same differences across all runs. The difference distribution of each parameter collapses around zero, and even that Îº\kappa has broader variance it still is on the order of 10âˆ’1310^{-13}; meaning purely numerical floating-point variation.
Another note is that the *vase* in Îº\kappa shows slightly more spread around zero, but still zero bias.

![Refer to caption](prec1.png)


(a) Per-parameter (all runs)

![Refer to caption](prec2.png)


(b) Îº\kappa

![Refer to caption](prec3.png)


(c)  Î¸v\theta\_{v}

![Refer to caption](prec4.png)


(d)  Ïƒv\sigma\_{v}

![Refer to caption](prec5.png)


(e)  Ï\rho

![Refer to caption](prec6.png)


(f)  v0v\_{0}

Figure 1: An exemplary comparison of equivalence of retraining and fast factorization with a smaller sample

SubfiguresÂ [1(b)](https://arxiv.org/html/2511.14980v1#S3.F1.sf2 "In Figure 1 â€£ 3 Illustrations â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework")â€“[1(f)](https://arxiv.org/html/2511.14980v1#S3.F1.sf6 "In Figure 1 â€£ 3 Illustrations â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework") compare parameter wise, while xx-axis indicates the mean of the two estimates (of retraining and fast factorization) for each parameter across runs (i.e., trials or experiments).
We remark that the vertical scale is around 10âˆ’1310^{-13}, and expectedly Îº\kappa has higher variation due to pure round-off noise (i.e., the mean reversion speed is the most sensitive numerically).
We therefore conclude that the recalibration of the fast factorization is statistically indistinguishable from retraining fully, indicating perfect numerical agreement and no systematic bias across the parameters.
While we are aware of the fact that we provide an exemplary comparison, we remark that in several hundreds of trials based on different sources of randomness; we failed to see different behavior than that of in FigureÂ [1](https://arxiv.org/html/2511.14980v1#S3.F1 "Figure 1 â€£ 3 Illustrations â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework")444We present both estimates with the same color dots given that the dispersion is almost non-existent under the equivalence of machine precision..
Unlike the results presented in FigureÂ [1](https://arxiv.org/html/2511.14980v1#S3.F1 "Figure 1 â€£ 3 Illustrations â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework")â€“[2](https://arxiv.org/html/2511.14980v1#S3.F2 "Figure 2 â€£ 3 Illustrations â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework") describes an exemplary comparison of all parameters and Îº\kappa for a larger sample.
Especially, in SubfigureÂ [2(a)](https://arxiv.org/html/2511.14980v1#S3.F2.sf1 "In Figure 2 â€£ 3 Illustrations â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework"), we see more regular behavior due to the possible reasons we discussed in an earlier section,Â [2.3](https://arxiv.org/html/2511.14980v1#S2.SS3 "2.3 Fast refactor operator â€£ 2 Formulation of the unlearning problem â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework").
Even though we, for the time being, exclude the sharded recomputation for brevity in FiguresÂ [1](https://arxiv.org/html/2511.14980v1#S3.F1 "Figure 1 â€£ 3 Illustrations â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework")â€“[2](https://arxiv.org/html/2511.14980v1#S3.F2 "Figure 2 â€£ 3 Illustrations â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework"); similar visualizations could be constructed on the exactness of the sharded recomputation.
We remark, however, that there is no difference between all three approaches up to machine precision, i.e., â€–Î¸fastâˆ’Î¸retrainâ€–2<10âˆ’8\|\theta\_{\text{fast}}-\theta\_{\text{retrain}}\|\_{2}<10^{-8} and â€–Î¸recompâˆ’Î¸retrainâ€–2<10âˆ’8\|\theta\_{\text{recomp}}-\theta\_{\text{retrain}}\|\_{2}<10^{-8} in all instances we observed in large sample experiments along with.
Another reason for the exclusion of the sharded computation is timely, as it is highly sensitive to the forgetting set which we now discuss in FigureÂ [3](https://arxiv.org/html/2511.14980v1#S3.F3 "Figure 3 â€£ 3 Illustrations â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework").

![Refer to caption](prec_b_1.png)


(a)  Per parameter (all runs)

![Refer to caption](prec_b_2.png)


(b) Îº\kappa

Figure 2: An exemplary comparison of equivalence of retraining and fast factorization with a larger sample

Full recalibration scales with the data set size since each quote requires multiple finite-difference Heston evaluations.
In contrast, the proposed fast refactor requires no re-pricing and runs in sub-millisecond time, as it merely updates the cached curvature system, unlike the sharded recomputation approach for reason we discuss now.
Recall that FF refers to the forget set, i.e. the subset of quotes the user asked to *unlearn*, ğ’¦â€‹(F)\mathcal{K}(F) refers to the set of affected shards with KK being the total number of shards.
The sharded recomputation should be faster than retraining only if the number of affected shard is smaller than number of shards, âˆ£ğ’¦â€‹(F)âˆ£â‰ªK\mid\mathcal{K}(F)\mid\ll K.
Therefore, two cases become interesting that either there exists a small number of shards, or with higher likelihood that the forget set is spread *roughly uniformly* across all shards.
The second case suggests that almost every shard is affected.
In small experiments (few data per shard), shard recomputation skips a few shards and is faster.
In full-scale runs, when almost every shard contains forgotten quotes, recomputation is likely to degenerates to full retraining.
The recomputation cost approaches full retraining when the forget set is evenly distributed across shards.
Only the fast refactorization operator retains sub-millisecond latency regardless of shard coverage.

![Refer to caption](recomp_equal_almost_1_seed_42.png)


(a) Larger-sample configuration

![Refer to caption](recomp_equal_almost_1_seed_60.png)


(b) Larger-sample configuration

![Refer to caption](recomp_equal_almost_2_seed_42_debug.png)


(c) Smaller-sample configuration

![Refer to caption](recomp_equal_almost_2_seed_60_debug.png)


(d) Smaller-sample configuration

Figure 3: Example on the importance of the forgetting set for the sharded recomputation

We show this aspect in FigureÂ [3](https://arxiv.org/html/2511.14980v1#S3.F3 "Figure 3 â€£ 3 Illustrations â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework"), in which SubfiguresÂ [3(a)](https://arxiv.org/html/2511.14980v1#S3.F3.sf1 "In Figure 3 â€£ 3 Illustrations â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework")â€“[3(b)](https://arxiv.org/html/2511.14980v1#S3.F3.sf2 "In Figure 3 â€£ 3 Illustrations â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework") are based on a larger sample with the fixed random forgetting but different sources of randomness in the underlying paths leading to different quotes, the ones on the lower panel are from a smaller sample with the same characteristics; hence much lower computational time given in SubfiguresÂ [3(a)](https://arxiv.org/html/2511.14980v1#S3.F3.sf1 "In Figure 3 â€£ 3 Illustrations â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework")â€“[3(b)](https://arxiv.org/html/2511.14980v1#S3.F3.sf2 "In Figure 3 â€£ 3 Illustrations â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework").
In each example observable in FigureÂ [3](https://arxiv.org/html/2511.14980v1#S3.F3 "Figure 3 â€£ 3 Illustrations â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework"), the sharded recomputation tends to be in a co-movement with the retraining fully, especially after higher percentage of unlearned quotes.
So that in case of worst-case dispersion of deletions, i.e. too many shards being affected, the sharded recomputation is no longer cheap in computational cost.

![Refer to caption](when1.png)


(a) Effect of earlier shards

![Refer to caption](when2.png)


(b) Effect of no earlier shards

Figure 4: Exemplary demonstration of the sharding positions in sharded recomputation

In FigureÂ [4](https://arxiv.org/html/2511.14980v1#S3.F4 "Figure 4 â€£ 3 Illustrations â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework"), we demonstrate different forgetting parts and its effect on the relative computation time of the sharded recomputation by retraining.
Time ration on the yy-axis shows that if the time ratio is lower than one, the sharded recomputation is faster than retraining on the retained data.
By construction, xx-axis shows how many shards contain at least one forgotten quote.
While the second shard affected in SubfigureÂ [4(a)](https://arxiv.org/html/2511.14980v1#S3.F4.sf1 "In Figure 4 â€£ 3 Illustrations â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework"), SubfigureÂ [4(b)](https://arxiv.org/html/2511.14980v1#S3.F4.sf2 "In Figure 4 â€£ 3 Illustrations â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework") illustrates the first three shard is not affected.
If we were to decide not to unlearn some of the quotes in the experiments, this could overall be efficient in the first example, and with likely none to little effect on the second example in SubfigureÂ [4(b)](https://arxiv.org/html/2511.14980v1#S3.F4.sf2 "In Figure 4 â€£ 3 Illustrations â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework") as the current set-up is saving so little time given the affected shards.
Therefore, we stress strongly that the sharded recomputation methodâ€™s efficiency is governed by the locality of the forgotten data.

Across multiple independent runs, the relative parameter error consistently remained at machine precision, confirming the generality of the result.
The surface is fixed intentionally to ensure that performance differences stem solely from the unlearning mechanism, not from new random draws.
Additional seeds produced qualitatively identical behavior
The reported numbers in TablesÂ [1](https://arxiv.org/html/2511.14980v1#S3.T1 "Table 1 â€£ 3 Illustrations â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework") andÂ [2](https://arxiv.org/html/2511.14980v1#S3.T2 "Table 2 â€£ 3 Illustrations â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework") are medians across 10 runs and variability was negligible (IQR555IQR stands for interquartile range. below 1%\%).
While ss refers to seconds, mâ€‹sms refers to milliseconds (10âˆ’3â€‹s10^{-3}s) and Î¼â€‹s\mu s refers to microseconds (10âˆ’6â€‹s10^{-6}s).

Table 1: Benchmark results across forgetting fractions.
Median runtimes and parameter deviations are reported.
The fast-refactor approach achieves identical accuracy to full retraining
with several orders of magnitude speedup.

| F(%) | Retrain | Recompute | Fast | RMSE kept (fast/retr) | Speedup |
| --- | --- | --- | --- | --- | --- |
| 1%\% | 21.06 s | 20.44 s | 174.2 Î¼â€‹s\mu s | 0.00049 / 0.00049 | Ã—\times120,886.6 |
| 2%\% | 18.36 s | 18.05 s | 242.8 Î¼â€‹s\mu s | 0.00049 / 0.00049 | Ã—\times75,639.3 |
| 5%\% | 17.71 s | 17.53 s | 436.9 Î¼â€‹s\mu s | 0.00049 / 0.00049 | Ã—\times40,414.3 |
| 10%\% | 16.79 s | 16.61 s | 882.0 Î¼â€‹s\mu s | 0.00049 / 0.00049 | Ã—\times19,268.7 |
| 25%\% | 14.31 s | 13.93 s | 2.12 mâ€‹sms | 0.00050 / 0.00050 | Ã—\times6,565.9 |

Although in TablesÂ [1](https://arxiv.org/html/2511.14980v1#S3.T1 "Table 1 â€£ 3 Illustrations â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework")â€“[2](https://arxiv.org/html/2511.14980v1#S3.T2 "Table 2 â€£ 3 Illustrations â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework"), we employ different underlying paths so that different quotes could be generated, yet the forgetting sets are fixed across runs.
The first columns refer the fraction of data forgotten, and throughout the study we never control the forgetting set; we simply randomize it.
Reported values show that, on general, even in the worst case scenario the fast factorization speeds up the calibration by six thousand times, roughly four to five orders of magnitude speedup remarking strong evidence of the efficiency of the unlearning operator.
We remark that the speedup is measured as the ratio of median computation time taken via retraining by the median computation time taken via the fast factorization operator.
The fifth columns in TablesÂ [1](https://arxiv.org/html/2511.14980v1#S3.T1 "Table 1 â€£ 3 Illustrations â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework")â€“[2](https://arxiv.org/html/2511.14980v1#S3.T2 "Table 2 â€£ 3 Illustrations â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework") measure the validation error on retained quotes,

Table 2: Benchmark results across forgetting fractions.
Median runtimes and parameter deviations are reported.
The fast-refactor approach achieves identical accuracy to full retraining
with several orders of magnitude speedup.

| F(%) | Retrain | Recompute | Fast | RMSE kept (fast/retr) | Speedup |
| --- | --- | --- | --- | --- | --- |
| 1%\% | 18.16 s | 17.63 s | 154.0 Î¼â€‹s\mu s | 0.00049 / 0.00049 | Ã—\times120,110.6 |
| 2%\% | 18.01 s | 17.42 s | 230.0 Î¼â€‹s\mu s | 0.00049 / 0.00049 | Ã—\times78,171.0 |
| 5%\% | 17.13 s | 17.08 s | 444.2 Î¼â€‹s\mu s | 0.00049 / 0.00049 | Ã—\times38,550.8 |
| 10%\% | 16.19 s | 16.17 s | 816.2 Î¼â€‹s\mu s | 0.00049 / 0.00049 | Ã—\times20,276.0 |
| 25%\% | 13.39 s | 13.37 s | 1.90 ms | 0.00050 / 0.00050 | Ã—\times7,059.7 |

TablesÂ [1](https://arxiv.org/html/2511.14980v1#S3.T1 "Table 1 â€£ 3 Illustrations â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework")â€“[2](https://arxiv.org/html/2511.14980v1#S3.T2 "Table 2 â€£ 3 Illustrations â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework") illustrate retraining time does not scale perfectly linearly (since GN step cost flattens with fewer quotes).
Therefore, denominator decreases slightly faster than numerator.
We also observe that the effective speedup decreases monotonically with the forgotten fraction since a larger fraction of the cached structure must be updated or recomputed.
This scaling is consistent with the theoretical expectation that the cost advantage of refactorization diminishes as the retained set shrinks.
All timings were measured as median wall-clock durations over 10 runs using identical random seeds and quote subsets.
We now provide visual presentation of the experiments reported in TablesÂ [1](https://arxiv.org/html/2511.14980v1#S3.T1 "Table 1 â€£ 3 Illustrations â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework")â€“[2](https://arxiv.org/html/2511.14980v1#S3.T2 "Table 2 â€£ 3 Illustrations â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework").

![Refer to caption](relative_error_1a_42.png)


(a) Typical behavior

![Refer to caption](relative_error_1a_60.png)


(b) Slight differencing at machine precision

Figure 5: Exemplary comparison of relative parameter error and runtime across forgetting fractions

FigureÂ [5](https://arxiv.org/html/2511.14980v1#S3.F5 "Figure 5 â€£ 3 Illustrations â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework") demonstrates relative parameter error against runtime across various forgetting fractions.
In SubfiguresÂ [5(a)](https://arxiv.org/html/2511.14980v1#S3.F5.sf1 "In Figure 5 â€£ 3 Illustrations â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework")â€“[5(b)](https://arxiv.org/html/2511.14980v1#S3.F5.sf2 "In Figure 5 â€£ 3 Illustrations â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework"), the deviation is calculated by â€–Î¸fâ€‹aâ€‹sâ€‹tâˆ’Î¸râ€‹eâ€‹tâ€‹râ€‹aâ€‹iâ€‹nâ€–2/â€–Î¸râ€‹eâ€‹tâ€‹râ€‹aâ€‹iâ€‹nâ€–2\|\theta\_{fast}-\theta\_{retrain}\|\_{2}/\|\theta\_{retrain}\|\_{2} given Dâˆ–FD\setminus F.
In SubfigureÂ [5(a)](https://arxiv.org/html/2511.14980v1#S3.F5.sf1 "In Figure 5 â€£ 3 Illustrations â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework"), we observe a perfectly flat blue line, all around 10âˆ’1510^{-15}, whereas SubfigureÂ [5(b)](https://arxiv.org/html/2511.14980v1#S3.F5.sf2 "In Figure 5 â€£ 3 Illustrations â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework") demonstrates that the blue line jumps upward near 25%25\% around 10âˆ’1210^{-12}.
That single-point rise indicates that, for one run at the highest forgetting fraction fast and retrain parameters differed slightly more, probably due to numerical conditioning or cache subtraction noise.
Although this is evident that the unlearning operator behaves stably as relative parameter deviation remains near machine precision; an exemplary case such as this requires several remarks.
Higher percentage removals are actually a loss of too many informative points on the surface, carrying the risk of Hâ€²H^{\prime} becoming poorly conditioned and amplifying small floating-point noise in Gâ€²G^{\prime}.
Even with that, we remark that fast refactor method reproduces the retraining solution to floating-point accuracy.

A crucial remark is that we empirically observe that the positive-definiteness condition Î»minâ€‹(Hâ€²)>0\lambda\_{\min}(H^{\prime})>0 remains satisfied well beyond typical forgetting levels.
In particular, the theoretical bound, â€–âˆ‘iâˆˆFÏˆiâ€–2<Î»minâ€‹(H)\|\sum\_{i\in F}\psi\_{i}\|\_{2}<\lambda\_{\min}(H), ensures that Hâ€²H^{\prime} remains positive definite, is rarely active until more than approximately 70%70\% of quotes are removed.
This indicates a strong numerical robustness of the downdate procedure and supports the stability of the fast-refactor updates under realistic unlearning scenarios.
We deem a further examination of the eigenvalue structure of the curvature matrix Hâ€²H^{\prime}
after forgetting to be of deeper interest to optimization algorithms rather than to the methodological structure of our unlearning framework.
Therefore, we keep this discussion brief and do not pursue an extensive numerical analysis beyond stability verification.

Empirically, calibration runtimes scale linearly in the number of quotes NN
and approximately quadratically in the number of Fourierâ€“Simpson integration
nodes NuN\_{u}, in line with the overall ğ’ªâ€‹(Nâ€‹Nu)\mathcal{O}(N\,N\_{u}) cost of evaluating
the Heston pricing integral.
Since the parameter dimension pp is fixed and relatively small, the memory and computational cost of assembling and solving the Gaussâ€“Newton normal equations is negligible.
The reported wallâ€“clock times therefore match the analytic complexity of the pricing integral and the minimal number of nonlinear iterations typically required for Heston calibration.

The goal of our benchmark is not largeâ€“scale industrial calibration but a controlled and reproducible comparison of unlearning operators.
In this setting, second-level runtimes are representative and analytically consistent. The key
question is whether the proposed operators reproduce the calibration update on
the reduced dataset. Both unlearning operators achieve numerical agreement up to
machine precision, and the fast refactorization operator does so at a small
fraction of the computational cost; see PropositionÂ [7](https://arxiv.org/html/2511.14980v1#Thmtheorem7 "Proposition 7 (Computational complexity of calibration and unlearning operators). â€£ 2.3 Fast refactor operator â€£ 2 Formulation of the unlearning problem â€£ Selective Forgetting in Option Calibration: An Operator-Theoretic Gaussâ€“Newton Framework").

## 4 Conclusion

We have shown that the proposed unlearning operators admit rigorous guarantees; local exactness under fixed linearization and stability under curvature perturbations
Numerical experiments further confirm that the fast refactorization operator matches full retraining to floating-point precision and achieves several orders of magnitude speedup, even for substantial forgetting fractions.
Although algebraically simple, these operators rely on the observation that the Gaussâ€“Newton normal equations encode a sufficient-statistics structure for nonlinear calibration.
Identifying the exact additive quantities whose removal preserves the optimality conditions under fixed linearization is, in our view, a nontrivial contribution and appears not to have been articulated previously in either the calibration or unlearning literature.

From a computational perspective, recalibration of the Heston model involves the evaluation of semi-analytic Fourierâ€“Simpson integrals and accumulation of the pÃ—pp\times p curvature matrix H=âˆ‘iJiâŠ¤â€‹JiH=\sum\_{i}J\_{i}^{\top}J\_{i}.
Even in the full configuration (Umax=120U\_{\max}=120, Nu=800N\_{u}=800), this entails only NÃ—Nuâ‰ˆ106N\times N\_{u}\approx 10^{6} function evaluations for datasets of typical size (Nâ‰ˆ103âˆ’104N\approx 10^{3}{-}10^{4}).
Such runtimes are modest in isolation, but financial institutions routinely process thousands of option books or parameter updates per day.
Recomputing all normal equations after each deletion therefore becomes costly, whereas the proposed unlearning operators perform mathematically exact deletions using only cached curvature statistics. The contribution is thus not raw speed alone, but the ability to *delete data deterministically without retraining*, enabling reversible and auditable calibration updates at negligible incremental cost.

Beyond computational gains, the framework reframes recalibration as an additive-subtractive operator calculus, enabling principled deletion of corrupted, stale, or restricted data.
This expands calibration from a purely forward-learning procedure into a bidirectional model-management process, useful for regulatory compliance, data-quality control, and influence diagnostics.
Although it is not in our interest yet our framework could also be used to quantify the influence of the subsets of data.
Questions such as which period affecting the calibration most, or what happens in case of exclusion of a data source might also be asked.

We focus on one representative semi-analytic model to isolate the operator behavior; extension to other models and real data is left for future work.
Future work may explore whether the operator perspective developed here extends beyond the static Gaussâ€“Newton setting.
One natural question is how additive-subtractive updates interact with models that contain latent or filtered state variables, such as stochastic-volatility or regime-switching specifications, where forgetting would couple to the underlying filter-smoother structure.
Another possible direction concerns the use of the resulting sufficient-statistics calculus for influence diagnostics, for example to quantify the leverage of particular maturities, regimes, or data sources on the calibrated parameters.
The same viewpoint also suggests potential analogues for rolling-window or online calibration procedures, where the removal of stale information must be carried out without repeatedly rebuilding the normal equations from scratch.
Finally, a more ambitious line of inquiry is whether analogous operator rules exist for quasi-Newton or higher-order curvature representations.
Overall, these directions emphasize that machine unlearning should be regarded not only as a computational device, but as part of a broader operator-theoretic framework for interpretable, auditable, and dynamically maintainable calibration pipelines.

## Compliance with Ethical Standards

\bmhead

\*Competing Interests
There are no financial or non-financial interests directly or indirectly related to the work submitted for publication.
\bmhead\*Funding
There is no funding received in the making of this manuscript.