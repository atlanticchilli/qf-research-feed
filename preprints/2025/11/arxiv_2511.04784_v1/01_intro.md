---
authors:
- Hamidreza Maleki Almani
doc_id: arxiv:2511.04784v1
family_id: arxiv:2511.04784
is_current: true
taxonomy:
  alpha_families: []
  asset_classes: []
  horizons: []
  themes: []
title: Insights into Tail-Based and Order Statistics
url_abs: http://arxiv.org/abs/2511.04784v1
url_html: https://arxiv.org/html/2511.04784v1
venue: arXiv q-fin
version: 1
year: 2025
---

Insights into Tail-Based and Order Statistics

![[Uncaptioned image]](IMG_7763_Final.jpg)

Hamidreza Maleki Almani

ORCID: <https://orcid.org/0000-0002-3071-4982>

Web: <https://www.uwasa.fi/en/person/2169161>

This article is an independent work by the author. He is a Postdoctoral Researcher in the Department of Mathematics and Statistics and the Department of Energy Technology at the University of Vaasa, Finland. He independently conducted and completed all aspects of this study.

November 6, 2025

Vaasa, FINLAND

Insights into Tail-Based and Order Statistics

###### Abstract.

Heavy-tailed phenomena appear across diverse domainsâ€”from wealth and firm sizes in economics to network traffic, biological systems, and physical processesâ€”characterized by the disproportionate influence of extreme values. These distributions challenge classical statistical models, as their tails decay too slowly for conventional approximations to hold. Among their key descriptive measures are quantile contributions, which quantify the proportion of a total quantity (such as income, energy, or risk) attributed to observations above a given quantile threshold.
This paper presents a theoretical study of the quantile contribution statistic and its relationship with order statistics. We derive a closed-form expression for the joint cumulative distribution function (CDF) of order statistics and, based on it, obtain an explicit CDF for quantile contributions applicable to small samples. We then investigate the asymptotic behavior of these contributions as the sample size increases, establishing the asymptotic normality of the numerator and characterizing the limiting distribution of the quantile contribution. Finally, simulation studies illustrate the convergence properties and empirical accuracy of the theoretical results, providing a foundation for applying quantile contributions in the analysis of heavy-tailed data.

###### Key words and phrases:

Heavy-tailed distributions,
Quantile contributions,
Order statistics,
Asymptotic distribution,
Ratio distribution,
Convergence analysis,
Extreme value theory,
Empirical simulation,

###### 2020 Mathematics Subject Classification:

60E05, 62E20, 60F05, 60G15, 60G70, 62G30, 62G32, 62M10, 62P20.

## 1. Introduction

In 1906, Pareto, in his first well-known work [[46](https://arxiv.org/html/2511.04784v1#bib.bib46)], showed that approximately 80% of the land in the Kingdom of Italy was owned by only 20% of the population at that time. This became known as Paretoâ€™s 80/20 principle.
Sturgeonâ€™s publications in the 1950s [[55](https://arxiv.org/html/2511.04784v1#bib.bib55), [56](https://arxiv.org/html/2511.04784v1#bib.bib56), [57](https://arxiv.org/html/2511.04784v1#bib.bib57), [58](https://arxiv.org/html/2511.04784v1#bib.bib58)] highlighted the observation that the majority of everything is of low quality. However, the prevalence of low-quality content across all genres disproves the notion that any single genre is inherently inferior. This idea is now known as Sturgeonâ€™s adage: â€Ninety percent of everything is crud!â€
Computer programmers are familiar with this in another form [[10](https://arxiv.org/html/2511.04784v1#bib.bib10), [41](https://arxiv.org/html/2511.04784v1#bib.bib41)]: in computer programming and software engineering, the Ninetyâ€“Ninety Rule is a humorous aphorism that states, â€œThe first 90% of the code accounts for the first 90% of the development time, and the remaining 10% of the code accounts for the other 90% of the development time!â€ This adds up to 180%, making a wry allusion to the notorious tendency of software development projects to significantly overrun their schedules.
In global health care, as a seriouse issue, the 10/90 gap is a term adopted by the Global Forum for Health Research to highlight the finding by the Commission on Health Research for Development in 1990 that less than 10% of worldwide resources devoted to health research were allocated to developing countriesâ€”where over 90% of all preventable deaths worldwide occur (see [[62](https://arxiv.org/html/2511.04784v1#bib.bib62), [22](https://arxiv.org/html/2511.04784v1#bib.bib22), [1](https://arxiv.org/html/2511.04784v1#bib.bib1)]). This disparity is a major concern of the World Health Organization (WHO) [[19](https://arxiv.org/html/2511.04784v1#bib.bib19), [20](https://arxiv.org/html/2511.04784v1#bib.bib20)].
This is observed even more sharply in internet culture [[13](https://arxiv.org/html/2511.04784v1#bib.bib13), [61](https://arxiv.org/html/2511.04784v1#bib.bib61)]. The 1% rule is a general rule of thumb regarding participation in an online community, stating that only 1% of a websiteâ€™s users actively create new content, while the other 99% simply lurk.

The observations mentioned above relate to a deeper fact beyond mere statistical inference. Informally, to estimate the probability of an event, it is often sufficient to focus on the concentration region of its distributionâ€”provided we have a large enough sample and the distributionâ€™s tails â€œvanish rapidly enough.â€ However, this assumption does not hold if the tails are thicker than negligible. In such cases, infrequent events have a significant probability, meaning that the usual â€œwell-behavedâ€ statistical models fail to accurately represent them. This is when the tails of the distribution must be taken into account, leading to what are called heavy-tailed processes. The historical evolution of heavy-tailed phenomena, some of which we have mentioned, reveals the following setup:

1. âˆ™\bullet

   Vanishing rapidly enough means a negligible tail, typically vanishing exponentially,
2. âˆ™\bullet

   Well-behaved also refers to distributions with exponentially vanishing tails.

So, a distribution FF is heavy-tailed [[50](https://arxiv.org/html/2511.04784v1#bib.bib50), [28](https://arxiv.org/html/2511.04784v1#bib.bib28)] if 1âˆ’Fâ€‹(x)=â„™â€‹[X>x]â‰«eâˆ’sâ€‹x1-F(x)=\mathbb{P}[X>x]\gg e^{-sx} for xâ†’âˆx\to\infty and s>0s>0, i.e.,

|  |  |  |
| --- | --- | --- |
|  | limxâ†’âˆesâ€‹xâ€‹(1âˆ’Fâ€‹(x))=âˆ.\lim\_{x\to\infty}e^{sx}(1-F(x))\,=\,\infty. |  |

Three well-known sub-classes of the heavy-tailed distributions are

1. (i)

   Fat-tailed distributions [[44](https://arxiv.org/html/2511.04784v1#bib.bib44), [40](https://arxiv.org/html/2511.04784v1#bib.bib40)] with index 0<Î±<20<\alpha<2 that

   |  |  |  |
   | --- | --- | --- |
   |  | 1âˆ’Fâ€‹(x)âˆ¼xâˆ’Î±â€‹Â forÂ â€‹xâ†’âˆ,1-F(x)\sim x^{-\alpha}\text{ for }x\to\infty, |  |
2. (ii)

   Long-tailed distribution [[4](https://arxiv.org/html/2511.04784v1#bib.bib4)] that for all t>0t>0 we have

   |  |  |  |
   | --- | --- | --- |
   |  | 1âˆ’Fâ€‹(x+t)âˆ¼1âˆ’Fâ€‹(x)â€‹Â forÂ â€‹xâ†’âˆ,1-F(x+t)\sim 1-F(x)\text{ for }x\to\infty, |  |
3. (iii)

   Subexponential distributions [[23](https://arxiv.org/html/2511.04784v1#bib.bib23), [14](https://arxiv.org/html/2511.04784v1#bib.bib14)] that for all independent processes X1,â€¦,Xnâˆ¼FX\_{1},\ldots,X\_{n}\sim F we have

   |  |  |  |
   | --- | --- | --- |
   |  | â„™â€‹[X1+â‹¯+Xn>x]âˆ¼â„™â€‹[maxâ¡(X1,â€¦,Xn)>x]â€‹Â forÂ â€‹xâ†’âˆ.\mathbb{P}[X\_{1}+\cdots+X\_{n}>x]\sim\mathbb{P}[\max(X\_{1},\ldots,X\_{n})>x]\text{ for }x\to\infty. |  |

Heavy-tailed distributions are crucial in numerous scientific fields due to their ability to model rare, high-impact events and skewed distributions. In economy, finance, and business, they capture extreme asset returns [[39](https://arxiv.org/html/2511.04784v1#bib.bib39), [26](https://arxiv.org/html/2511.04784v1#bib.bib26)], volatility clustering [[16](https://arxiv.org/html/2511.04784v1#bib.bib16)], and market shocks [[53](https://arxiv.org/html/2511.04784v1#bib.bib53)], enhancing risk modeling and forecasting. Wealth and firm size distributions often follow power laws, aiding economic analysis [[29](https://arxiv.org/html/2511.04784v1#bib.bib29), [5](https://arxiv.org/html/2511.04784v1#bib.bib5)]. These models also inform business strategies in sales, resource allocation, and resilience to demand shocks [[59](https://arxiv.org/html/2511.04784v1#bib.bib59), [23](https://arxiv.org/html/2511.04784v1#bib.bib23), [38](https://arxiv.org/html/2511.04784v1#bib.bib38)]. In computer science, heavy-tailed patterns appear in internet traffic [[35](https://arxiv.org/html/2511.04784v1#bib.bib35)], file sizes [[21](https://arxiv.org/html/2511.04784v1#bib.bib21)], and server loads, affecting network protocols and performance [[18](https://arxiv.org/html/2511.04784v1#bib.bib18), [47](https://arxiv.org/html/2511.04784v1#bib.bib47)]. They also underpin job scheduling in distributed systems [[31](https://arxiv.org/html/2511.04784v1#bib.bib31), [36](https://arxiv.org/html/2511.04784v1#bib.bib36)] and anomaly detection in cybersecurity [[8](https://arxiv.org/html/2511.04784v1#bib.bib8)].

In physics and engineering, heavy-tailed distributions describe anomalous diffusion [[52](https://arxiv.org/html/2511.04784v1#bib.bib52)], turbulent transport [[42](https://arxiv.org/html/2511.04784v1#bib.bib42)], and structural failure in materials [[9](https://arxiv.org/html/2511.04784v1#bib.bib9), [12](https://arxiv.org/html/2511.04784v1#bib.bib12)]. They are used in modeling impulsive noise and signal degradation in communication systems [[45](https://arxiv.org/html/2511.04784v1#bib.bib45), [43](https://arxiv.org/html/2511.04784v1#bib.bib43)], as well as robust signal processing under uncertainty.In biology and health sciences, these distributions explain superspreading in epidemics [[37](https://arxiv.org/html/2511.04784v1#bib.bib37), [24](https://arxiv.org/html/2511.04784v1#bib.bib24)], scale-free gene and protein networks [[7](https://arxiv.org/html/2511.04784v1#bib.bib7), [33](https://arxiv.org/html/2511.04784v1#bib.bib33)], and variability in neural dynamics [[11](https://arxiv.org/html/2511.04784v1#bib.bib11)]. They also capture skewed healthcare metrics such as drug response and hospital stays [[15](https://arxiv.org/html/2511.04784v1#bib.bib15)]. Across disciplines, heavy-tailed distributions support more realistic, data-driven modeling of complex systems, improving prediction, design, and decision-making.

The most important statistics of a heavy-tailed distribution are its quantiles. This is because we aim to identify a precise split of the distribution into two parts: the head and the tail. Specifically, we look for the point below which a considerable percentage pp of the population falls (the pth quantile), while the accumulated value of the remaining (1âˆ’p)(1-p) percent above that point constitutes a significant portion of the total value in the population. This measure is known as the quantile contribution [[60](https://arxiv.org/html/2511.04784v1#bib.bib60)]. It refers to the proportion of a total quantityâ€”such as income, risk, energy, or emissionsâ€”attributed to elements above (or sometimes below) a certain quantile threshold within a statistical distribution. In simple terms, it shows how much of a total amount is accounted for by a particular subset of units ranked by size (e.g., income, energy, or emissions).
The â€œnaturalâ€ estimator for the quantile contribution is calculated as the ratio of the sum of values above the exceedance threshold (the value above a specific quantile) to the total sum. That is

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î›nâ€‹(p)=âˆ‘jâˆˆğ’¥nâ€‹(p)Xjâˆ‘j=1nXj,\Lambda\_{n}(p)=\frac{\sum\_{j\in\mathscr{J}\_{n}(p)}X\_{j}}{\sum\_{j=1}^{n}X\_{j}}, |  | (1.1) |

where pâˆˆ[0,1]p\in[0,1] is a constant number, Xj,j=1,â€¦,nX\_{j},j=1,\ldots,n are independently identically distributed (i.i.d), and

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ’¥nâ€‹(p)={j|Xjâˆˆ100â€‹p%â€‹largest observations amongâ€‹X1,â€¦,Xn},\mathscr{J}\_{n}(p)=\{j\,|\,X\_{j}\in 100p\%\,\text{largest observations among}\,X\_{1},\ldots,X\_{n}\}, |  | (1.2) |

we note that for all pâˆˆ[0,1],nâ‰¥1p\in[0,1],n\geq 1 we have |Î»nâ€‹(p)|â‰¤1|\lambda\_{n}(p)|\leq 1.

In this article, we study the connection between quantile contributions and order statistics, focusing on their distributions and convergence. In Section [2](https://arxiv.org/html/2511.04784v1#S2 "2. Ordered and Tail-Based Statistics"), we derive a closed-form expression for the joint cumulative distribution function (CDF) of order statistics. Building on this, Section [3](https://arxiv.org/html/2511.04784v1#S3 "3. Exact Distribution of Tail-Based Statistics") presents an explicit form of the CDF for quantile contributions, applicable to a small number of variables. Section [4](https://arxiv.org/html/2511.04784v1#S4 "4. Convergence of Tail-Based Statistics") explores the convergence of quantile contributions as the number of variables grows large. Section [5](https://arxiv.org/html/2511.04784v1#S5 "5. Asymptotic Distribution of Numerator") presents the asymptotic normality of the numerator, and Section [6](https://arxiv.org/html/2511.04784v1#S6 "6. Asymptotic Distribution of Tail-Based Statistic") applies this result to characterize the asymptotic distribution of quantile contributions for a large number of variables. Finally, in Section [7](https://arxiv.org/html/2511.04784v1#S7 "7. Simulation and Callibration"), we present simulations of important cases and cumulative errors to illustrate the empirical performance and accuracy of our results.

## 2. Ordered and Tail-Based Statistics

To investigate the convergence and distribution of the Î›n\Lambda\_{n} given in ([1.1](https://arxiv.org/html/2511.04784v1#S1.E1 "In 1. Introduction")), first we must consider its close relationship to the order statistics. For the random variables X1,X2,â€¦,XnX\_{1},X\_{2},\ldots,X\_{n}, the associated order statistics are the random variables X(1)n,X(2)n,â€¦,X(n)nX\_{(1)}^{n},X\_{(2)}^{n},\ldots,X\_{(n)}^{n} defined by ascending resorting of the X1,X2,â€¦,XnX\_{1},X\_{2},\ldots,X\_{n}. Then we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î›nâ€‹(p)=âˆ‘i=âŒˆnâ€‹pâŒ‰nX(i)nâˆ‘i=1nXi=âˆ‘i=âŒˆnâ€‹pâŒ‰nX(i)nâˆ‘i=1nX(i)n.\Lambda\_{n}(p)=\frac{\sum\_{i=\left\lceil np\right\rceil}^{n}X\_{(i)}^{n}}{\sum\_{i=1}^{n}X\_{i}}=\frac{\sum\_{i=\left\lceil np\right\rceil}^{n}X\_{(i)}^{n}}{\sum\_{i=1}^{n}X\_{(i)}^{n}}. |  | (2.1) |

In other word, to investigate the probability distribution of Î›n\Lambda\_{n}, it is sufficient to know the joint distribution of the order statistics X(1)n,X(2)n,â€¦,X(n)nX\_{(1)}^{n},X\_{(2)}^{n},\ldots,X\_{(n)}^{n}.
The following proposition for the distribution of each X(i)nX\_{(i)}^{n} is explained in [[17](https://arxiv.org/html/2511.04784v1#bib.bib17), [49](https://arxiv.org/html/2511.04784v1#bib.bib49), [3](https://arxiv.org/html/2511.04784v1#bib.bib3)]. Here I just rewrite the proof with a quantitative formulation of its combinatorics.

###### Proposition 2.1.

The probability distribution F(i)nF\_{(i)}^{n} and density function f(i)nf\_{(i)}^{n} of the order statistic X(i)nX\_{(i)}^{n} are

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | F(i)nâ€‹(x)\displaystyle F\_{(i)}^{n}(x) | =Iâ€‹(Fâ€‹(x);i,nâˆ’i+1)=âˆ‘J=in(nJ)â€‹(Fâ€‹(x))Jâ€‹(1âˆ’Fâ€‹(x))nâˆ’J,\displaystyle=I\big(F(x);\,i,n-i+1\big)=\sum\_{J=i}^{n}\binom{n}{J}\Big(F(x)\Big)^{J}\Big(1-F(x)\Big)^{n-J}, |  | (2.2) |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | f(i)nâ€‹(x)\displaystyle f\_{(i)}^{n}(x) | =fâ€‹(x)Bâ€‹(i,nâˆ’i+1)â€‹(Fâ€‹(x))iâˆ’1â€‹(1âˆ’Fâ€‹(x))nâˆ’i,\displaystyle=\frac{f(x)}{B(i,n-i+1)}\Big(F(x)\Big)^{i-1}\Big(1-F(x)\Big)^{n-i}, |  | (2.3) |

where FF and ff are respectively the probability distribution and density function of the variable X1X\_{1}, and B,IB,I are respectively the beta function and the regularized inclomplete beta function, i.e., for all â„œâ€‹ğ”¢â€‹(p),â„œâ€‹ğ”¢â€‹(q)>0\mathfrak{Re}(p),\mathfrak{Re}(q)>0

|  |  |  |
| --- | --- | --- |
|  | Bâ€‹(p,q)=âˆ«01tpâˆ’1â€‹(1âˆ’t)qâˆ’1â€‹ğ‘‘t,\displaystyle B(p,q)=\int\_{0}^{1}t^{p-1}(1-t)^{q-1}\,dt, |  |
|  |  |  |
| --- | --- | --- |
|  | Iâ€‹(x;p,q)=1Bâ€‹(p,q)â€‹âˆ«0xtpâˆ’1â€‹(1âˆ’t)qâˆ’1â€‹ğ‘‘t.\displaystyle I(x;\,p,q)=\frac{1}{B(p,q)}\int\_{0}^{x}t^{p-1}(1-t)^{q-1}\,dt. |  |

###### Proof.

As X(i)nX\_{(i)}^{n} is the (i/n)(i/n)-quantile variable of X1,â€¦,XnX\_{1},\ldots,X\_{n}, for all xâˆˆâ„x\in\mathbb{R}

|  |  |  |
| --- | --- | --- |
|  | X(i)n=Qnâ€‹(i/n)â‰¤xâ‡”iâ‰¤âˆ‘j=1nğŸ™(âˆ’âˆ,x]â€‹(Xj).X\_{(i)}^{n}=Q\_{n}(i/n)\leq x\iff i\leq\sum\_{j=1}^{n}\mathds{1}\_{(-\infty,x]}(X\_{j}). |  |

So,

|  |  |  |  |
| --- | --- | --- | --- |
|  | F(j)nâ€‹(x)\displaystyle F\_{(j)}^{n}(x) | =â„™â€‹[X(i)nâ‰¤x]\displaystyle=\mathbb{P}[X\_{(i)}^{n}\leq x] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =â„™â€‹[iâ‰¤âˆ‘j=1nğŸ™(âˆ’âˆ,x]â€‹(Xj)]\displaystyle=\mathbb{P}\left[i\leq\sum\_{j=1}^{n}\mathds{1}\_{(-\infty,x]}(X\_{j})\right] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =âˆ‘J=inâ„™â€‹[J=âˆ‘j=1nğŸ™(âˆ’âˆ,x]â€‹(Xj)]\displaystyle=\sum\_{J=i}^{n}\mathbb{P}\left[J=\sum\_{j=1}^{n}\mathds{1}\_{(-\infty,x]}(X\_{j})\right] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =âˆ‘J=in(nJ)â€‹(â„™â€‹[X1â‰¤x])Jâ€‹(â„™â€‹[X1>x])nâˆ’J\displaystyle=\sum\_{J=i}^{n}\binom{n}{J}\Big(\mathbb{P}[X\_{1}\leq x]\Big)^{J}\Big(\mathbb{P}[X\_{1}>x]\Big)^{n-J} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =âˆ‘J=in(nJ)â€‹(Fâ€‹(x))Jâ€‹(1âˆ’Fâ€‹(x))nâˆ’J.\displaystyle=\sum\_{J=i}^{n}\binom{n}{J}\Big(F(x)\Big)^{J}\Big(1-F(x)\Big)^{n-J}. |  |

Now, we note

|  |  |  |
| --- | --- | --- |
|  | âˆ‘J=in(nJ)â€‹yJâ€‹(1âˆ’y)nâˆ’J\displaystyle\sum\_{J=i}^{n}\binom{n}{J}y^{J}(1-y)^{n-J} |  |
|  |  |  |
| --- | --- | --- |
|  | =âˆ«0ytiâ€‹(1âˆ’t)nâˆ’i+1â€‹ğ‘‘tBâ€‹(i,nâˆ’i+1)\displaystyle=\frac{\displaystyle\int\_{0}^{y}t^{i}(1-t)^{n-i+1}\,dt}{B(i,n-i+1)} |  |
|  |  |  |
| --- | --- | --- |
|  | =Iâ€‹(y;i,nâˆ’i+1),\displaystyle=I\big(y;\,i,n-i+1\big), |  |

and these prove ([2.2](https://arxiv.org/html/2511.04784v1#S2.E2 "In Proposition 2.1. â€£ 2. Ordered and Tail-Based Statistics")). The proof of ([2.3](https://arxiv.org/html/2511.04784v1#S2.E3 "In Proposition 2.1. â€£ 2. Ordered and Tail-Based Statistics")) is straight forward as follows.

|  |  |  |  |
| --- | --- | --- | --- |
|  | f(j)nâ€‹(x)\displaystyle f\_{(j)}^{n}(x) | =dâ€‹F(j)nâ€‹(x)dâ€‹x\displaystyle=\frac{dF\_{(j)}^{n}(x)}{dx} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =dâ€‹Iâ€‹(Fâ€‹(x);i,nâˆ’i+1)dâ€‹x\displaystyle=\frac{dI\big(F(x);\,i,n-i+1\big)}{dx} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =fâ€‹(x)â€‹(Fâ€‹(x))iâˆ’1â€‹(1âˆ’Fâ€‹(x))nâˆ’iBâ€‹(i,nâˆ’i+1).\displaystyle=\frac{f(x)\big(F(x)\big)^{i-1}\big(1-F(x)\big)^{n-i}}{B(i,n-i+1)}. |  |

âˆ

The joint density function of the order statistics X(1)n,X(2)n,â€¦,X(n)nX\_{(1)}^{n},X\_{(2)}^{n},\ldots,X\_{(n)}^{n} is given by [[49](https://arxiv.org/html/2511.04784v1#bib.bib49), [2](https://arxiv.org/html/2511.04784v1#bib.bib2)] as following theorem and corollary.

###### Theorem 2.2.

Let 1â‰¤kâ‰¤n1\leq k\leq n and 0=r0<r1<â‹¯<rk<rk+1=n+10=r\_{0}<r\_{1}<\cdots<r\_{k}<r\_{k+1}=n+1. If the random variables X1,X2,â€¦,XnX\_{1},X\_{2},\ldots,X\_{n} are i.i.d with common absolutely continuous distribution FF and density function ff, then the joint density function of X(r1)n,X(r2)n,â€¦,X(rk)nX\_{(r\_{1})}^{n},X\_{(r\_{2})}^{n},\ldots,X\_{(r\_{k})}^{n} is

|  |  |  |  |
| --- | --- | --- | --- |
|  | f(r1,â€¦,rk)nâ€‹(x1,â€¦,xk)=n!â€‹(âˆi=1kfâ€‹(xi))â€‹âˆi=1k+1(Fâ€‹(xi)âˆ’Fâ€‹(xiâˆ’1))riâˆ’riâˆ’1âˆ’1(riâˆ’riâˆ’1âˆ’1)!,f\_{(r\_{1},\ldots,r\_{k})}^{n}(x\_{1},\ldots,x\_{k})=n!\left(\prod\_{i=1}^{k}f(x\_{i})\right)\prod\_{i=1}^{k+1}\frac{\big(F(x\_{i})-F(x\_{i-1})\big)^{r\_{i}-r\_{i-1}-1}}{(r\_{i}-r\_{i-1}-1)!}, |  | (2.4) |

if x1<x2<â‹¯<xkx\_{1}<x\_{2}<\cdots<x\_{k}, and it is 0 otherwise. Here Fâ€‹(x0)=0F(x\_{0})=0 and Fâ€‹(xk+1)=1F(x\_{k+1})=1.

###### Corollary 2.3.

If the random variables X1,X2,â€¦,XnX\_{1},X\_{2},\ldots,X\_{n} are i.i.d with common absolutely continuous distribution FF and density function ff, then the joint density function of X(1)n,X(2)n,â€¦,X(n)nX\_{(1)}^{n},X\_{(2)}^{n},\ldots,X\_{(n)}^{n} is

|  |  |  |  |
| --- | --- | --- | --- |
|  | f(1,â€¦,n)nâ€‹(x1,â€¦,xn)=n!â€‹âˆi=1nfâ€‹(xi),f\_{(1,\ldots,n)}^{n}(x\_{1},\ldots,x\_{n})=n!\prod\_{i=1}^{n}f(x\_{i}), |  | (2.5) |

if x1<x2<â‹¯<xnx\_{1}<x\_{2}<\cdots<x\_{n}, and it is 0 otherwise.

Next, we evaluate the cumulative distribution function of the order statistics. However, this requires some insight into the relationship between the Binomial distribution and the regularized incomplete Beta function, as presented in the following lemma.

###### Lemma 2.4.

For all positive integers p,qâ‰¥1p,q\geq 1, and all a,bâˆˆâ„a,b\in\mathbb{R}

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ia,bâ€‹(y;p,q)\displaystyle I\_{a,b}(y;\,p,q) | :=1Bâ€‹(p,q)â€‹âˆ«ay(xâˆ’a)pâˆ’1â€‹(bâˆ’x)qâˆ’1â€‹ğ‘‘x\displaystyle:=\frac{1}{B(p,q)}\int\_{a}^{y}(x-a)^{p-1}(b-x)^{q-1}\,dx |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =âˆ‘j=pp+qâˆ’1(p+qâˆ’1j)â€‹(yâˆ’a)jâ€‹(bâˆ’y)p+qâˆ’1âˆ’j\displaystyle=\sum\_{j=p}^{p+q-1}\binom{p+q-1}{j}(y-a)^{j}(b-y)^{p+q-1-j} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =âˆ‘j=0qâˆ’1(p+qâˆ’1j)â€‹(yâˆ’a)p+qâˆ’1âˆ’jâ€‹(bâˆ’y)j.\displaystyle=\sum\_{j=0}^{q-1}\binom{p+q-1}{j}(y-a)^{p+q-1-j}(b-y)^{j}. |  |

###### Proof.

By changing the variable t=xâˆ’abâˆ’at=\frac{x-a}{b-a}, we have

|  |  |  |
| --- | --- | --- |
|  | Ia,bâ€‹(y;p,q)=(bâˆ’a)p+qâˆ’1â€‹Iâ€‹(yâˆ’abâˆ’a;p,q),I\_{a,b}(y;p,q)=(b-a)^{p+q-1}I\left(\frac{y-a}{b-a};p,q\right), |  |

and

|  |  |  |  |
| --- | --- | --- | --- |
|  | Iâ€‹(u;p,q)\displaystyle I\left(u;p,q\right) | =â„™â€‹[Jâ‰¥p]\displaystyle=\mathbb{P}[J\geq p] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =âˆ‘j=pp+qâˆ’1(p+qâˆ’1j)â€‹ujâ€‹(1âˆ’u)p+qâˆ’1âˆ’j\displaystyle=\sum\_{j=p}^{p+q-1}\binom{p+q-1}{j}u^{j}(1-u)^{p+q-1-j} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =âˆ‘j=0qâˆ’1(p+qâˆ’1j)â€‹up+qâˆ’1âˆ’jâ€‹(1âˆ’u)j,\displaystyle=\sum\_{j=0}^{q-1}\binom{p+q-1}{j}u^{p+q-1-j}(1-u)^{j}, |  |

where Jâˆ¼â„¬â€‹iâ€‹nâ€‹oâ€‹mâ€‹iâ€‹aâ€‹lâ€‹(u;p+qâˆ’1)J\sim\mathscr{B}inomial(u;p+q-1). Now, substituting u=yâˆ’abâˆ’au=\frac{y-a}{b-a} proves the claim.
âˆ

###### Theorem 2.5.

Let 1â‰¤kâ‰¤n1\leq k\leq n and 0<r1<â‹¯<rk<n+10<r\_{1}<\cdots<r\_{k}<n+1 are integers. If the random variables X1,X2,â€¦,XnX\_{1},X\_{2},\ldots,X\_{n} are i.i.d with common absolutely continuous distribution FF and density function ff, then the cumulative distribution function of X(r1)n,X(r2)n,â€¦,X(rk)nX\_{(r\_{1})}^{n},X\_{(r\_{2})}^{n},\ldots,X\_{(r\_{k})}^{n} is

|  |  |  |
| --- | --- | --- |
|  | F(r1,â€¦,rk)nâ€‹(x1,â€¦,xk)\displaystyle F\_{(r\_{1},\ldots,r\_{k})}^{n}(x\_{1},\ldots,x\_{k}) |  |
|  |  |  |
| --- | --- | --- |
|  | =âˆ‘Jk=0nâˆ’rkâˆ‘Jkâˆ’1=0nâˆ’rkâˆ’1âˆ’Jkâˆ‘Jkâˆ’2=0nâˆ’rkâˆ’2âˆ’Jkâˆ’Jkâˆ’1â‹¯â€‹âˆ‘J1=0nâˆ’r1âˆ’âˆ‘i=2kJi\displaystyle=\sum\_{J\_{k}=0}^{n-r\_{k}}\quad\sum\_{J\_{k-1}=0}^{n-r\_{k-1}-J\_{k}}\quad\sum\_{J\_{k-2}=0}^{n-r\_{k-2}-J\_{k}-J\_{k-1}}\cdots\sum\_{J\_{1}=0}^{n-r\_{1}-\sum\_{i=2}^{k}J\_{i}} |  |
|  |  |  |
| --- | --- | --- |
|  | (nJ0,J1,â€¦,Jk)â€‹âˆi=0k(Fâ€‹(x(i+1)k)âˆ’Fâ€‹(x(i)k))Ji,\displaystyle\quad\binom{n}{J\_{0},J\_{1},\ldots,J\_{k}}\prod\_{i=0}^{k}\Big(F\left(x\_{(i+1)}^{k}\right)-F\left(x\_{(i)}^{k}\right)\Big)^{J\_{i}}, |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | s.t.âˆ‘i=0kJi=n,\displaystyle s.t.\quad\sum\_{i=0}^{k}J\_{i}=n, |  | (2.6) |

if x1<x2<â‹¯<xkx\_{1}<x\_{2}<\cdots<x\_{k}, and it is 0 otherwise. Here Fâ€‹(x0)=0F(x\_{0})=0, and Fâ€‹(xk+1)=1F(x\_{k+1})=1. (Note: J0=nâˆ’âˆ‘i=1kJiJ\_{0}=n-\sum\_{i=1}^{k}J\_{i})

###### Proof 1: Calculus.

Applying the density function from Theorem [2.2](https://arxiv.org/html/2511.04784v1#S2.Thmdfn2 "Theorem 2.2. â€£ 2. Ordered and Tail-Based Statistics"), for y1â‰¤â‹¯â‰¤yky\_{1}\leq\cdots\leq y\_{k} we have

|  |  |  |
| --- | --- | --- |
|  | F(r1,â€¦,rk)nâ€‹(y1,â€¦,yk)\displaystyle F\_{(r\_{1},\ldots,r\_{k})}^{n}(y\_{1},\ldots,y\_{k}) |  |
|  |  |  |
| --- | --- | --- |
|  | =n!â€‹âˆ«âˆ’âˆy1âˆ«x1y2â‹¯â€‹âˆ«xkâˆ’2ykâˆ’1âˆ«xkâˆ’1ykğ‘‘xkâ€‹â‹¯â€‹ğ‘‘x1\displaystyle=n!\int\_{-\infty}^{y\_{1}}\int\_{x\_{1}}^{y\_{2}}\cdots\int\_{x\_{k-2}}^{y\_{k-1}}\int\_{x\_{k-1}}^{y\_{k}}dx\_{k}\cdots dx\_{1} |  |
|  |  |  |
| --- | --- | --- |
|  | Ã—(âˆi=1kfâ€‹(xi))â€‹âˆi=1k+1(Fâ€‹(xi)âˆ’Fâ€‹(xiâˆ’1))riâˆ’riâˆ’1âˆ’1(riâˆ’riâˆ’1âˆ’1)!,\displaystyle\quad\times\left(\prod\_{i=1}^{k}f(x\_{i})\right)\prod\_{i=1}^{k+1}\frac{\big(F(x\_{i})-F(x\_{i-1})\big)^{r\_{i}-r\_{i-1}-1}}{(r\_{i}-r\_{i-1}-1)!}, |  |

where r0=0r\_{0}=0 and rk+1=n+1r\_{k+1}=n+1. By changing the variables ui=Fâ€‹(xi),i=1,â€¦â€‹k+1u\_{i}=F(x\_{i}),\,i=1,\ldots k+1, we have

|  |  |  |
| --- | --- | --- |
|  | =n!â€‹âˆ«0Fâ€‹(y1)âˆ«u1Fâ€‹(y2)â‹¯â€‹âˆ«ukâˆ’2Fâ€‹(ykâˆ’1)âˆ«ukâˆ’1Fâ€‹(yk)âˆi=1k+1(uiâˆ’uiâˆ’1)riâˆ’riâˆ’1âˆ’1(riâˆ’riâˆ’1âˆ’1)!â€‹dâ€‹ukâ€‹â‹¯â€‹dâ€‹u1\displaystyle=n!\int\_{0}^{F(y\_{1})}\int\_{u\_{1}}^{F(y\_{2})}\cdots\int\_{u\_{k-2}}^{F(y\_{k-1})}\int\_{u\_{k-1}}^{F(y\_{k})}\prod\_{i=1}^{k+1}\frac{(u\_{i}-u\_{i-1})^{r\_{i}-r\_{i-1}-1}}{(r\_{i}-r\_{i-1}-1)!}\,du\_{k}\cdots du\_{1} |  |
|  |  |  |
| --- | --- | --- |
|  | =n!â€‹âˆ«0Fâ€‹(y1)âˆ«u1Fâ€‹(y2)â‹¯â€‹âˆ«ukâˆ’2Fâ€‹(ykâˆ’1)âˆi=1kâˆ’1(uiâˆ’uiâˆ’1)riâˆ’riâˆ’1âˆ’1(riâˆ’riâˆ’1âˆ’1)!â€‹dâ€‹ukâˆ’1â€‹â‹¯â€‹dâ€‹u1âŸÏ€kâˆ’1\displaystyle=n!\underbrace{\int\_{0}^{F(y\_{1})}\int\_{u\_{1}}^{F(y\_{2})}\cdots\int\_{u\_{k-2}}^{F(y\_{k-1})}\prod\_{i=1}^{k-1}\frac{(u\_{i}-u\_{i-1})^{r\_{i}-r\_{i-1}-1}}{(r\_{i}-r\_{i-1}-1)!}\,du\_{k-1}\cdots du\_{1}}\_{\pi\_{k-1}} |  |
|  |  |  |
| --- | --- | --- |
|  | Ã—âˆ«ukâˆ’1Fâ€‹(yk)(uk+1âˆ’uk)rk+1âˆ’rkâˆ’1â€‹(ukâˆ’ukâˆ’1)rkâˆ’rkâˆ’1âˆ’1(rk+1âˆ’rkâˆ’1)!â€‹(rkâˆ’rkâˆ’1âˆ’1)!duk\displaystyle\quad\times\int\_{u\_{k-1}}^{F(y\_{k})}\frac{(u\_{k+1}-u\_{k})^{r\_{k+1}-r\_{k}-1}(u\_{k}-u\_{k-1})^{r\_{k}-r\_{k-1}-1}}{(r\_{k+1}-r\_{k}-1)!(r\_{k}-r\_{k-1}-1)!}\,du\_{k} |  |
|  |  |  |
| --- | --- | --- |
|  | =n!â€‹Ï€kâˆ’1â€‹âˆ«ukâˆ’1Fâ€‹(yk)(uk+1âˆ’uk)rk+1âˆ’rkâˆ’1â€‹(ukâˆ’ukâˆ’1)rkâˆ’rkâˆ’1âˆ’1Bâ€‹(rk+1âˆ’rk,rkâˆ’rkâˆ’1âˆ’1)â€‹(rk+1âˆ’rkâˆ’1âˆ’1)!â€‹ğ‘‘uk.\displaystyle=n!\,\pi\_{k-1}\int\_{u\_{k-1}}^{F(y\_{k})}\frac{(u\_{k+1}-u\_{k})^{r\_{k+1}-r\_{k}-1}(u\_{k}-u\_{k-1})^{r\_{k}-r\_{k-1}-1}}{B(r\_{k+1}-r\_{k},r\_{k}-r\_{k-1}-1)(r\_{k+1}-r\_{k-1}-1)!}\,du\_{k}. |  |

Taking pk=rkâˆ’rkâˆ’1,qk=rk+1âˆ’rkp\_{k}=r\_{k}-r\_{k-1},q\_{k}=r\_{k+1}-r\_{k}, then by Lemma [2.4](https://arxiv.org/html/2511.04784v1#S2.Thmdfn4 "Lemma 2.4. â€£ 2. Ordered and Tail-Based Statistics")

|  |  |  |
| --- | --- | --- |
|  | =n!â€‹Ï€kâˆ’1(rk+1âˆ’rkâˆ’1âˆ’1)!â€‹Iukâˆ’1,uk+1â€‹(Fâ€‹(yk);rkâˆ’rkâˆ’1,rk+1âˆ’rk)\displaystyle=\frac{n!\,\pi\_{k-1}}{(r\_{k+1}-r\_{k-1}-1)!}\,I\_{u\_{k-1},u\_{k+1}}\Big(F(y\_{k});r\_{k}-r\_{k-1},r\_{k+1}-r\_{k}\Big) |  |
|  |  |  |
| --- | --- | --- |
|  | =n!â€‹Ï€kâˆ’1(rk+1âˆ’rkâˆ’1âˆ’1)!â€‹Iukâˆ’1,uk+1â€‹(Fâ€‹(yk);pk,qk)\displaystyle=\frac{n!\,\pi\_{k-1}}{(r\_{k+1}-r\_{k-1}-1)!}\,I\_{u\_{k-1},u\_{k+1}}\big(F(y\_{k});p\_{k},q\_{k}\big) |  |
|  |  |  |
| --- | --- | --- |
|  | =n!â€‹Ï€kâˆ’1(rk+1âˆ’rkâˆ’1âˆ’1)!\displaystyle=\frac{n!\,\pi\_{k-1}}{(r\_{k+1}-r\_{k-1}-1)!} |  |
|  |  |  |
| --- | --- | --- |
|  | Ã—âˆ‘Jk=0qkâˆ’1(pk+qkâˆ’1Jk)(F(yk)âˆ’ukâˆ’1)pk+qkâˆ’1âˆ’Jk(uk+1âˆ’F(yk))Jk\displaystyle\times\sum\_{J\_{k}=0}^{q\_{k}-1}\binom{p\_{k}+q\_{k}-1}{J\_{k}}\big(F(y\_{k})-u\_{k-1}\big)^{p\_{k}+q\_{k}-1-J\_{k}}\big(u\_{k+1}-F(y\_{k})\big)^{J\_{k}} |  |
|  |  |  |
| --- | --- | --- |
|  | =n!â€‹Ï€kâˆ’1(rk+1âˆ’rkâˆ’1âˆ’1)!\displaystyle=\frac{n!\,\pi\_{k-1}}{(r\_{k+1}-r\_{k-1}-1)!} |  |
|  |  |  |
| --- | --- | --- |
|  | Ã—âˆ‘Jk=0rk+1âˆ’rkâˆ’1(rk+1âˆ’rkâˆ’1âˆ’1Jk)(F(yk)âˆ’ukâˆ’1)rk+1âˆ’rkâˆ’1âˆ’1âˆ’Jk(uk+1âˆ’F(yk))Jk\displaystyle\times\sum\_{J\_{k}=0}^{r\_{k+1}-r\_{k}-1}\binom{r\_{k+1}-r\_{k-1}-1}{J\_{k}}\big(F(y\_{k})-u\_{k-1}\big)^{r\_{k+1}-r\_{k-1}-1-J\_{k}}\big(u\_{k+1}-F(y\_{k})\big)^{J\_{k}} |  |
|  |  |  |
| --- | --- | --- |
|  | =n!â€‹Ï€kâˆ’2â€‹âˆ‘Jk=0rk+1âˆ’rkâˆ’1(uk+1âˆ’Fâ€‹(yk))JkJk!âŸÎ£Jk\displaystyle=n!\,\pi\_{k-2}\underbrace{\sum\_{J\_{k}=0}^{r\_{k+1}-r\_{k}-1}\frac{\big(u\_{k+1}-F(y\_{k})\big)^{J\_{k}}}{J\_{k}!}}\_{\Sigma\_{J\_{k}}} |  |
|  |  |  |
| --- | --- | --- |
|  | Ã—âˆ«ukâˆ’2Fâ€‹(ykâˆ’1)(ukâˆ’1âˆ’ukâˆ’2)rkâˆ’1âˆ’rkâˆ’2âˆ’1(rkâˆ’1âˆ’rkâˆ’2âˆ’1)!â‹…(Fâ€‹(yk)âˆ’ukâˆ’1)rk+1âˆ’rkâˆ’1âˆ’1âˆ’Jk(rk+1âˆ’rkâˆ’1âˆ’1âˆ’Jk)!dukâˆ’1\displaystyle\times\int\_{u\_{k-2}}^{F(y\_{k-1})}\frac{(u\_{k-1}-u\_{k-2})^{r\_{k-1}-r\_{k-2}-1}}{(r\_{k-1}-r\_{k-2}-1)!}\cdot\frac{(F(y\_{k})-u\_{k-1})^{r\_{k+1}-r\_{k-1}-1-J\_{k}}}{(r\_{k+1}-r\_{k-1}-1-J\_{k})!}\,du\_{k-1} |  |
|  |  |  |
| --- | --- | --- |
|  | =n!â€‹Ï€kâˆ’2â€‹Î£Jk\displaystyle=n!\,\pi\_{k-2}\Sigma\_{J\_{k}} |  |
|  |  |  |
| --- | --- | --- |
|  | Ã—âˆ«ukâˆ’2Fâ€‹(ykâˆ’1)(ukâˆ’1âˆ’ukâˆ’2)rkâˆ’1âˆ’rkâˆ’2âˆ’1â‹…(Fâ€‹(yk)âˆ’ukâˆ’1)rk+1âˆ’rkâˆ’1âˆ’1âˆ’JkBâ€‹(rkâˆ’1âˆ’rkâˆ’2,rk+1âˆ’rkâˆ’1âˆ’Jk)â€‹Î“â€‹(rk+1âˆ’rkâˆ’2âˆ’Jk)dukâˆ’1.\displaystyle\times\int\_{u\_{k-2}}^{F(y\_{k-1})}\frac{(u\_{k-1}-u\_{k-2})^{r\_{k-1}-r\_{k-2}-1}\cdot(F(y\_{k})-u\_{k-1})^{r\_{k+1}-r\_{k-1}-1-J\_{k}}}{B(r\_{k-1}-r\_{k-2},r\_{k+1}-r\_{k-1}-J\_{k})\Gamma(r\_{k+1}-r\_{k-2}-J\_{k})}\,du\_{k-1}. |  |

Again, by taking pkâˆ’1=rkâˆ’1âˆ’rkâˆ’2,qkâˆ’1=rk+1âˆ’rkâˆ’1âˆ’Jkp\_{k-1}=r\_{k-1}-r\_{k-2},\,q\_{k-1}=r\_{k+1}-r\_{k-1}-J\_{k}, then from Lemma [2.4](https://arxiv.org/html/2511.04784v1#S2.Thmdfn4 "Lemma 2.4. â€£ 2. Ordered and Tail-Based Statistics")

|  |  |  |
| --- | --- | --- |
|  | =n!â€‹Ï€kâˆ’2â€‹Î£Jk(rk+1âˆ’rkâˆ’2âˆ’Jkâˆ’1)!â€‹Iukâˆ’2,Fâ€‹(yk)â€‹(Fâ€‹(ykâˆ’1);pkâˆ’1,qkâˆ’1)\displaystyle=\frac{n!\,\pi\_{k-2}\Sigma\_{J\_{k}}}{(r\_{k+1}-r\_{k-2}-J\_{k}-1)!}\,I\_{u\_{k-2},F(y\_{k})}\Big(F(y\_{k-1});p\_{k-1},q\_{k-1}\Big) |  |
|  |  |  |
| --- | --- | --- |
|  | =n!â€‹Ï€kâˆ’2â€‹Î£Jk(rk+1âˆ’rkâˆ’2âˆ’Jkâˆ’1)!â€‹âˆ‘Jkâˆ’1=0qkâˆ’1âˆ’1(pkâˆ’1+qkâˆ’1âˆ’1Jkâˆ’1)\displaystyle=\frac{n!\,\pi\_{k-2}\Sigma\_{J\_{k}}}{(r\_{k+1}-r\_{k-2}-J\_{k}-1)!}\sum\_{J\_{k-1}=0}^{q\_{k-1}-1}\binom{p\_{k-1}+q\_{k-1}-1}{J\_{k-1}} |  |
|  |  |  |
| --- | --- | --- |
|  | Ã—(Fâ€‹(ykâˆ’1)âˆ’ukâˆ’2)pkâˆ’1+qkâˆ’1âˆ’1âˆ’Jkâˆ’1â€‹(ukâˆ’Fâ€‹(ykâˆ’1))Jkâˆ’1\displaystyle\times\big(F(y\_{k-1})-u\_{k-2}\big)^{p\_{k-1}+q\_{k-1}-1-J\_{k-1}}\big(u\_{k}-F(y\_{k-1})\big)^{J\_{k-1}} |  |
|  |  |  |
| --- | --- | --- |
|  | =n!â€‹Ï€kâˆ’2â€‹Î£Jk(rk+1âˆ’rkâˆ’2âˆ’Jkâˆ’1)!â€‹âˆ‘Jkâˆ’1=0rk+1âˆ’rkâˆ’1âˆ’Jkâˆ’1(rk+1âˆ’rkâˆ’2âˆ’Jkâˆ’1Jkâˆ’1)\displaystyle=\frac{n!\,\pi\_{k-2}\Sigma\_{J\_{k}}}{(r\_{k+1}-r\_{k-2}-J\_{k}-1)!}\sum\_{J\_{k-1}=0}^{r\_{k+1}-r\_{k-1}-J\_{k}-1}\binom{r\_{k+1}-r\_{k-2}-J\_{k}-1}{J\_{k-1}} |  |
|  |  |  |
| --- | --- | --- |
|  | Ã—(Fâ€‹(ykâˆ’1)âˆ’ukâˆ’2)rk+1âˆ’rkâˆ’2âˆ’Jkâˆ’Jkâˆ’1âˆ’1â€‹(ukâˆ’Fâ€‹(ykâˆ’1))Jkâˆ’1\displaystyle\times\big(F(y\_{k-1})-u\_{k-2}\big)^{r\_{k+1}-r\_{k-2}-J\_{k}-J\_{k-1}-1}\big(u\_{k}-F(y\_{k-1})\big)^{J\_{k-1}} |  |
|  |  |  |
| --- | --- | --- |
|  | =n!â€‹Ï€kâˆ’3â€‹Î£Jkâ€‹âˆ‘Jkâˆ’1=0rk+1âˆ’rkâˆ’1âˆ’Jkâˆ’1(Fâ€‹(yk)âˆ’Fâ€‹(ykâˆ’1))Jkâˆ’1Jkâˆ’1!âŸÎ£Jkâˆ’1\displaystyle=n!\,\pi\_{k-3}\Sigma\_{J\_{k}}\underbrace{\sum\_{J\_{k-1}=0}^{r\_{k+1}-r\_{k-1}-J\_{k}-1}\frac{\big(F(y\_{k})-F(y\_{k-1})\big)^{J\_{k-1}}}{J\_{k-1}!}}\_{\Sigma\_{J\_{k-1}}} |  |
|  |  |  |
| --- | --- | --- |
|  | Ã—âˆ«ukâˆ’3Fâ€‹(ykâˆ’2)(ukâˆ’2âˆ’ukâˆ’3)rkâˆ’2âˆ’rkâˆ’3âˆ’1(rkâˆ’2âˆ’rkâˆ’3âˆ’1)!\displaystyle\hskip 56.9055pt\times\int\_{u\_{k-3}}^{F(y\_{k-2})}\frac{(u\_{k-2}-u\_{k-3})^{r\_{k-2}-r\_{k-3}-1}}{(r\_{k-2}-r\_{k-3}-1)!} |  |
|  |  |  |
| --- | --- | --- |
|  | â‹…(Fâ€‹(ykâˆ’1)âˆ’ukâˆ’2)rk+1âˆ’rkâˆ’2âˆ’Jkâˆ’Jkâˆ’1âˆ’1(rk+1âˆ’rkâˆ’2âˆ’Jkâˆ’Jkâˆ’1âˆ’1)!â€‹dâ€‹ukâˆ’2\displaystyle\hskip 56.9055pt\cdot\frac{(F(y\_{k-1})-u\_{k-2})^{r\_{k+1}-r\_{k-2}-J\_{k}-J\_{k-1}-1}}{(r\_{k+1}-r\_{k-2}-J\_{k}-J\_{k-1}-1)!}\,du\_{k-2} |  |
|  |  |  |
| --- | --- | --- |
|  | =n!â€‹Ï€kâˆ’3â€‹Î£Jkâ€‹Î£Jkâˆ’1â€‹âˆ«ukâˆ’3Fâ€‹(ykâˆ’2)(ukâˆ’2âˆ’ukâˆ’3)rkâˆ’2âˆ’rkâˆ’3âˆ’1Î“â€‹(rk+1âˆ’rkâˆ’3âˆ’Jkâˆ’Jkâˆ’1)\displaystyle=n!\,\pi\_{k-3}\Sigma\_{J\_{k}}\Sigma\_{J\_{k-1}}\int\_{u\_{k-3}}^{F(y\_{k-2})}\frac{(u\_{k-2}-u\_{k-3})^{r\_{k-2}-r\_{k-3}-1}}{\Gamma(r\_{k+1}-r\_{k-3}-J\_{k}-J\_{k-1})} |  |
|  |  |  |
| --- | --- | --- |
|  | Ã—(Fâ€‹(ykâˆ’1)âˆ’ukâˆ’2)rk+1âˆ’rkâˆ’2âˆ’Jkâˆ’Jkâˆ’1âˆ’1Bâ€‹(rkâˆ’2âˆ’rkâˆ’3,rk+1âˆ’rkâˆ’2âˆ’Jkâˆ’Jkâˆ’1)â€‹dâ€‹ukâˆ’2,\displaystyle\hskip 85.35826pt\times\frac{(F(y\_{k-1})-u\_{k-2})^{r\_{k+1}-r\_{k-2}-J\_{k}-J\_{k-1}-1}}{B(r\_{k-2}-r\_{k-3},r\_{k+1}-r\_{k-2}-J\_{k}-J\_{k-1})}\,du\_{k-2}, |  |

To identify the limits of the summations, we proceed one step further, and again by taking pkâˆ’2=rkâˆ’2âˆ’rkâˆ’3,qkâˆ’2=rk+1âˆ’rkâˆ’2âˆ’Jkâˆ’Jkâˆ’1p\_{k-2}=r\_{k-2}-r\_{k-3},\,q\_{k-2}=r\_{k+1}-r\_{k-2}-J\_{k}-J\_{k-1}, from Lemma [2.4](https://arxiv.org/html/2511.04784v1#S2.Thmdfn4 "Lemma 2.4. â€£ 2. Ordered and Tail-Based Statistics"), we have

|  |  |  |
| --- | --- | --- |
|  | =n!â€‹Ï€kâˆ’3â€‹Î£Jkâ€‹Î£Jkâˆ’1(rk+1âˆ’rkâˆ’3âˆ’Jkâˆ’Jk+1âˆ’1)!â€‹Iukâˆ’3,Fâ€‹(ykâˆ’1)â€‹(Fâ€‹(ykâˆ’2);pkâˆ’2,qkâˆ’2)\displaystyle=\frac{n!\,\pi\_{k-3}\Sigma\_{J\_{k}}\Sigma\_{J\_{k-1}}}{(r\_{k+1}-r\_{k-3}-J\_{k}-J\_{k+1}-1)!}\,I\_{u\_{k-3},F(y\_{k-1})}\Big(F(y\_{k-2});p\_{k-2},q\_{k-2}\Big) |  |
|  |  |  |
| --- | --- | --- |
|  | =n!â€‹Ï€kâˆ’3â€‹Î£Jkâ€‹Î£Jkâˆ’1(rk+1âˆ’rkâˆ’3âˆ’Jkâˆ’Jkâˆ’1âˆ’1)!â€‹âˆ‘Jkâˆ’2=0qkâˆ’2âˆ’1(pkâˆ’2+qkâˆ’2âˆ’1Jkâˆ’2)\displaystyle=\frac{n!\,\pi\_{k-3}\Sigma\_{J\_{k}}\Sigma\_{J\_{k-1}}}{(r\_{k+1}-r\_{k-3}-J\_{k}-J\_{k-1}-1)!}\sum\_{J\_{k-2}=0}^{q\_{k-2}-1}\binom{p\_{k-2}+q\_{k-2}-1}{J\_{k-2}} |  |
|  |  |  |
| --- | --- | --- |
|  | Ã—(Fâ€‹(ykâˆ’2)âˆ’ukâˆ’3)pkâˆ’2+qkâˆ’2âˆ’1âˆ’Jkâˆ’2â€‹(ukâˆ’Fâ€‹(ykâˆ’1))Jkâˆ’2\displaystyle\times\big(F(y\_{k-2})-u\_{k-3}\big)^{p\_{k-2}+q\_{k-2}-1-J\_{k-2}}\big(u\_{k}-F(y\_{k-1})\big)^{J\_{k-2}} |  |
|  |  |  |
| --- | --- | --- |
|  | =n!â€‹Ï€kâˆ’3â€‹Î£Jkâ€‹Î£Jkâˆ’1(rk+1âˆ’rkâˆ’3âˆ’Jkâˆ’Jkâˆ’1âˆ’1)!\displaystyle=\frac{n!\,\pi\_{k-3}\Sigma\_{J\_{k}}\Sigma\_{J\_{k-1}}}{(r\_{k+1}-r\_{k-3}-J\_{k}-J\_{k-1}-1)!} |  |
|  |  |  |
| --- | --- | --- |
|  | Ã—âˆ‘Jkâˆ’2=0rk+1âˆ’rkâˆ’2âˆ’Jkâˆ’Jkâˆ’1âˆ’1(rk+1âˆ’rkâˆ’3âˆ’Jkâˆ’Jkâˆ’1âˆ’1Jkâˆ’2)\displaystyle\times\sum\_{J\_{k-2}=0}^{r\_{k+1}-r\_{k-2}-J\_{k}-J\_{k-1}-1}\binom{r\_{k+1}-r\_{k-3}-J\_{k}-J\_{k-1}-1}{J\_{k-2}} |  |
|  |  |  |
| --- | --- | --- |
|  | Ã—(F(ykâˆ’2)âˆ’ukâˆ’3)rk+1âˆ’rkâˆ’3âˆ’Jkâˆ’Jkâˆ’1âˆ’Jkâˆ’2âˆ’1(F(ykâˆ’1âˆ’F(ykâˆ’1))Jkâˆ’2\displaystyle\times\big(F(y\_{k-2})-u\_{k-3}\big)^{r\_{k+1}-r\_{k-3}-J\_{k}-J\_{k-1}-J\_{k-2}-1}\big(F(y\_{k-1}-F(y\_{k-1})\big)^{J\_{k-2}} |  |
|  |  |  |
| --- | --- | --- |
|  | =n!â€‹Ï€kâˆ’4â€‹Î£Jkâ€‹Î£Jkâˆ’1â€‹âˆ‘Jkâˆ’2=0rk+1âˆ’rkâˆ’2âˆ’Jkâˆ’Jkâˆ’1âˆ’1(Fâ€‹(ykâˆ’1)âˆ’Fâ€‹(ykâˆ’2))Jkâˆ’2Jkâˆ’2!âŸÎ£Jkâˆ’2\displaystyle=n!\,\pi\_{k-4}\Sigma\_{J\_{k}}\Sigma\_{J\_{k-1}}\underbrace{\sum\_{J\_{k-2}=0}^{r\_{k+1}-r\_{k-2}-J\_{k}-J\_{k-1}-1}\frac{\big(F(y\_{k-1})-F(y\_{k-2})\big)^{J\_{k-2}}}{J\_{k-2}!}}\_{\Sigma\_{J\_{k-2}}} |  |
|  |  |  |
| --- | --- | --- |
|  | Ã—âˆ«ukâˆ’4Fâ€‹(ykâˆ’3)(ukâˆ’3âˆ’ukâˆ’4)rkâˆ’3âˆ’rkâˆ’4âˆ’1(rkâˆ’3âˆ’rkâˆ’4âˆ’1)!\displaystyle\hskip 85.35826pt\times\int\_{u\_{k-4}}^{F(y\_{k-3})}\frac{(u\_{k-3}-u\_{k-4})^{r\_{k-3}-r\_{k-4}-1}}{(r\_{k-3}-r\_{k-4}-1)!} |  |
|  |  |  |
| --- | --- | --- |
|  | â‹…(Fâ€‹(ykâˆ’2)âˆ’ukâˆ’3)rk+1âˆ’rkâˆ’3âˆ’Jkâˆ’Jkâˆ’1âˆ’Jkâˆ’2âˆ’1(rk+1âˆ’rkâˆ’3âˆ’Jkâˆ’Jkâˆ’1âˆ’Jkâˆ’2âˆ’1)!â€‹dâ€‹ukâˆ’3.\displaystyle\hskip 85.35826pt\cdot\frac{(F(y\_{k-2})-u\_{k-3})^{r\_{k+1}-r\_{k-3}-J\_{k}-J\_{k-1}-J\_{k-2}-1}}{(r\_{k+1}-r\_{k-3}-J\_{k}-J\_{k-1}-J\_{k-2}-1)!}\,du\_{k-3}. |  |

Continuing this calculation recursively, we obtain

|  |  |  |
| --- | --- | --- |
|  | =n!â€‹Î£Jkâ€‹â‹¯â€‹Î£J2â€‹âˆ«u0Fâ€‹(y1)(u1âˆ’u0)r1âˆ’r0âˆ’1(r1âˆ’r0âˆ’1)!â‹…(Fâ€‹(y2)âˆ’u1)rk+1âˆ’r1âˆ’1âˆ’âˆ‘i=2kJi(rk+1âˆ’r1âˆ’1âˆ’âˆ‘i=2kJi)!â€‹ğ‘‘u1\displaystyle=n!\Sigma\_{J\_{k}}\cdots\Sigma\_{J\_{2}}\int\_{u\_{0}}^{F(y\_{1})}\frac{(u\_{1}-u\_{0})^{r\_{1}-r\_{0}-1}}{(r\_{1}-r\_{0}-1)!}\cdot\frac{(F(y\_{2})-u\_{1})^{r\_{k+1}-r\_{1}-1-\sum\_{i=2}^{k}J\_{i}}}{(r\_{k+1}-r\_{1}-1-\sum\_{i=2}^{k}J\_{i})!}\,du\_{1} |  |
|  |  |  |
| --- | --- | --- |
|  | =n!â€‹Î£Jkâ€‹â‹¯â€‹Î£J2â€‹âˆ«0Fâ€‹(y1)u1r1âˆ’1â€‹(Fâ€‹(y2)âˆ’u1)rk+1âˆ’r1âˆ’1âˆ’âˆ‘i=2kJiBâ€‹(r1,rk+1âˆ’r1âˆ’âˆ‘j=2kJi)â€‹(rk+1âˆ’1âˆ’âˆ‘i=2kJi)!â€‹ğ‘‘u1.\displaystyle=n!\Sigma\_{J\_{k}}\cdots\Sigma\_{J\_{2}}\int\_{0}^{F(y\_{1})}\frac{u\_{1}^{r\_{1}-1}(F(y\_{2})-u\_{1})^{r\_{k+1}-r\_{1}-1-\sum\_{i=2}^{k}J\_{i}}}{B(r\_{1},r\_{k+1}-r\_{1}-\sum\_{j=2}^{k}J\_{i})(r\_{k+1}-1-\sum\_{i=2}^{k}J\_{i})!}\,du\_{1}. |  |

By taking p1=r1,q1=rk+1âˆ’r1âˆ’âˆ‘i=2kJip\_{1}=r\_{1},q\_{1}=r\_{k+1}-r\_{1}-\sum\_{i=2}^{k}J\_{i}, we have

|  |  |  |
| --- | --- | --- |
|  | =n!â€‹Î£Jkâ€‹â‹¯â€‹Î£J2â€‹I0,Fâ€‹(y2)â€‹(Fâ€‹(yâˆ’1);p1,q1)(rk+1âˆ’1âˆ’âˆ‘i=2kJi)!\displaystyle=n!\Sigma\_{J\_{k}}\cdots\Sigma\_{J\_{2}}\frac{I\_{0,F(y\_{2})}\big(F(y-1);p\_{1},q\_{1}\big)}{(r\_{k+1}-1-\sum\_{i=2}^{k}J\_{i})!} |  |
|  |  |  |
| --- | --- | --- |
|  | =n!â€‹Î£Jkâ€‹â‹¯â€‹Î£J2â€‹1(rk+1âˆ’1âˆ’âˆ‘i=2kJi)!\displaystyle=n!\Sigma\_{J\_{k}}\cdots\Sigma\_{J\_{2}}\frac{1}{(r\_{k+1}-1-\sum\_{i=2}^{k}J\_{i})!} |  |
|  |  |  |
| --- | --- | --- |
|  | Ã—âˆ‘J1=0q1âˆ’1(p1+q1âˆ’1J1)(F(y1))p1+q1âˆ’1âˆ’J1(F(y2)âˆ’F(y1))J1\displaystyle\times\sum\_{J\_{1}=0}^{q\_{1}-1}\binom{p\_{1}+q\_{1}-1}{J\_{1}}\Big(F(y\_{1})\Big)^{p\_{1}+q\_{1}-1-J\_{1}}\Big(F(y\_{2})-F(y\_{1})\Big)^{J\_{1}} |  |
|  |  |  |
| --- | --- | --- |
|  | =n!â€‹Î£Jkâ€‹â‹¯â€‹Î£J2â€‹1(rk+1âˆ’1âˆ’âˆ‘i=2kJi)!â€‹âˆ‘J1=0rk+1âˆ’r1âˆ’1âˆ’âˆ‘i=2kJi(rk+1âˆ’1âˆ’âˆ‘i=2kJiJ1)\displaystyle=n!\Sigma\_{J\_{k}}\cdots\Sigma\_{J\_{2}}\frac{1}{(r\_{k+1}-1-\sum\_{i=2}^{k}J\_{i})!}\sum\_{J\_{1}=0}^{r\_{k+1}-r\_{1}-1-\sum\_{i=2}^{k}J\_{i}}\binom{r\_{k+1}-1-\sum\_{i=2}^{k}J\_{i}}{J\_{1}} |  |
|  |  |  |
| --- | --- | --- |
|  | Ã—(Fâ€‹(y1))rk+1âˆ’1âˆ’âˆ‘i=2kJiâ€‹(Fâ€‹(y2)âˆ’Fâ€‹(y1))J1\displaystyle\times\Big(F(y\_{1})\Big)^{r\_{k+1}-1-\sum\_{i=2}^{k}J\_{i}}\Big(F(y\_{2})-F(y\_{1})\Big)^{J\_{1}} |  |
|  |  |  |
| --- | --- | --- |
|  | =n!â€‹Î£Jkâ€‹â‹¯â€‹Î£J2â€‹âˆ‘J1=0rk+1âˆ’r1âˆ’1âˆ’âˆ‘i=2kJi(Fâ€‹(y2)âˆ’Fâ€‹(y1))J1J1!â‹…(Fâ€‹(y1))rk+1âˆ’1âˆ’âˆ‘i=1kJi(rk+1âˆ’1âˆ’âˆ‘i=1kJi)!\displaystyle=n!\Sigma\_{J\_{k}}\cdots\Sigma\_{J\_{2}}\sum\_{J\_{1}=0}^{r\_{k+1}-r\_{1}-1-\sum\_{i=2}^{k}J\_{i}}\frac{\big(F(y\_{2})-F(y\_{1})\big)^{J\_{1}}}{J\_{1}!}\cdot\frac{\big(F(y\_{1})\big)^{r\_{k+1}-1-\sum\_{i=1}^{k}J\_{i}}}{(r\_{k+1}-1-\sum\_{i=1}^{k}J\_{i})!} |  |
|  |  |  |
| --- | --- | --- |
|  | =âˆ‘Jk=0rk+1âˆ’rkâˆ’1âˆ‘Jkâˆ’1=0rk+2âˆ’rkâˆ’1âˆ’Jkâˆ’1âˆ‘Jkâˆ’2=0rk+1âˆ’rkâˆ’2âˆ’Jkâˆ’Jkâˆ’1âˆ’1â‹¯â€‹âˆ‘J1=0rk+1âˆ’r1âˆ’1âˆ’âˆ‘i=2kJi\displaystyle=\sum\_{J\_{k}=0}^{r\_{k+1}-r\_{k}-1}\quad\sum\_{J\_{k-1}=0}^{r\_{k+2}-r\_{k-1}-J\_{k}-1}\quad\sum\_{J\_{k-2}=0}^{r\_{k+1}-r\_{k-2}-J\_{k}-J\_{k-1}-1}\cdots\sum\_{J\_{1}=0}^{r\_{k+1}-r\_{1}-1-\sum\_{i=2}^{k}J\_{i}} |  |
|  |  |  |
| --- | --- | --- |
|  | n!(rk+1âˆ’1âˆ’âˆ‘i=1kJi)!â€‹J1!â€‹â‹¯â€‹Jk!\displaystyle\frac{n!}{(r\_{k+1}-1-\sum\_{i=1}^{k}J\_{i})!J\_{1}!\cdots J\_{k}!} |  |
|  |  |  |
| --- | --- | --- |
|  | Ã—(Fâ€‹(yk+1)âˆ’Fâ€‹(yk))Jkâ€‹(Fâ€‹(yk)âˆ’Fâ€‹(ykâˆ’1))Jkâˆ’1â€‹â‹¯â€‹(Fâ€‹(y1)âˆ’Fâ€‹(y0))rk+1âˆ’1âˆ’âˆ‘i=1kJi\displaystyle\times\big(F(y\_{k+1})-F(y\_{k})\big)^{J\_{k}}\big(F(y\_{k})-F(y\_{k-1})\big)^{J\_{k-1}}\cdots\big(F(y\_{1})-F(y\_{0})\big)^{r\_{k+1}-1-\sum\_{i=1}^{k}J\_{i}} |  |

where Fâ€‹(y0)=0F(y\_{0})=0 and Fâ€‹(yk+1)=1F(y\_{k+1})=1. Then, taking J0=rk+1âˆ’1âˆ’âˆ‘i=1kJi=nâˆ’âˆ‘i=1kJiJ\_{0}=r\_{k+1}-1-\sum\_{i=1}^{k}J\_{i}=n-\sum\_{i=1}^{k}J\_{i}, we have

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | =âˆ‘Jk=0nâˆ’rkâˆ‘Jkâˆ’1=0nâˆ’rkâˆ’1âˆ’Jkâˆ‘Jkâˆ’2=0nâˆ’rkâˆ’2âˆ’Jkâˆ’Jkâˆ’1â‹¯â€‹âˆ‘J1=0nâˆ’r1âˆ’âˆ‘i=2kJi\displaystyle=\sum\_{J\_{k}=0}^{n-r\_{k}}\quad\sum\_{J\_{k-1}=0}^{n-r\_{k-1}-J\_{k}}\quad\sum\_{J\_{k-2}=0}^{n-r\_{k-2}-J\_{k}-J\_{k-1}}\cdots\sum\_{J\_{1}=0}^{n-r\_{1}-\sum\_{i=2}^{k}J\_{i}} |  | (2.7) |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | (nJ0,J1,â€¦,Jk)â€‹âˆi=0k(Fâ€‹(yi+1)âˆ’Fâ€‹(yi))Ji,\displaystyle\quad\binom{n}{J\_{0},J\_{1},\ldots,J\_{k}}\prod\_{i=0}^{k}\Big(F\left(y\_{i+1}\right)-F\left(y\_{i}\right)\Big)^{J\_{i}}, |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | s.t.âˆ‘i=0kJi=n.\displaystyle s.t.\quad\sum\_{i=0}^{k}J\_{i}=n. |  |

Now, for arbitrary y1,â€¦,yky\_{1},\ldots,y\_{k}, we have

|  |  |  |
| --- | --- | --- |
|  | F(r1,â€¦,rk)nâ€‹(y1,â€¦,yk)\displaystyle F\_{(r\_{1},\ldots,r\_{k})}^{n}(y\_{1},\ldots,y\_{k}) |  |
|  |  |  |
| --- | --- | --- |
|  | =â„™â€‹[X(1)nâ‰¤y1,â€¦,X(k)nâ‰¤yk]\displaystyle=\mathbb{P}\left[X\_{(1)}^{n}\leq y\_{1},\ldots,X\_{(k)}^{n}\leq y\_{k}\right] |  |
|  |  |  |
| --- | --- | --- |
|  | =â„™â€‹[X(1)nâ‰¤y(1)k,â€¦,X(k)nâ‰¤y(k)k]\displaystyle=\mathbb{P}\left[X\_{(1)}^{n}\leq y\_{(1)}^{k},\ldots,X\_{(k)}^{n}\leq y\_{(k)}^{k}\right] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | =F(r1,â€¦,rk)nâ€‹(y(1)k,â€¦,y(k)k),\displaystyle=F\_{(r\_{1},\ldots,r\_{k})}^{n}\left(y\_{(1)}^{k},\ldots,y\_{(k)}^{k}\right), |  | (2.8) |

and as y(1)kâ‰¤â‹¯â‰¤y(k)ky\_{(1)}^{k}\leq\cdots\leq y\_{(k)}^{k}, applying ([2.7](https://arxiv.org/html/2511.04784v1#S2.E7 "In 2. Ordered and Tail-Based Statistics")) to ([2.8](https://arxiv.org/html/2511.04784v1#S2.E8 "In 2. Ordered and Tail-Based Statistics")) returns

|  |  |  |
| --- | --- | --- |
|  | F(r1,â€¦,rk)nâ€‹(y1,â€¦,yk)\displaystyle F\_{(r\_{1},\ldots,r\_{k})}^{n}(y\_{1},\ldots,y\_{k}) |  |
|  |  |  |
| --- | --- | --- |
|  | =âˆ‘Jk=0nâˆ’rkâˆ‘Jkâˆ’1=0nâˆ’rkâˆ’1âˆ’Jkâˆ‘Jkâˆ’2=0nâˆ’rkâˆ’2âˆ’Jkâˆ’Jkâˆ’1â‹¯â€‹âˆ‘J1=0nâˆ’r1âˆ’âˆ‘i=2kJi\displaystyle=\sum\_{J\_{k}=0}^{n-r\_{k}}\quad\sum\_{J\_{k-1}=0}^{n-r\_{k-1}-J\_{k}}\quad\sum\_{J\_{k-2}=0}^{n-r\_{k-2}-J\_{k}-J\_{k-1}}\cdots\sum\_{J\_{1}=0}^{n-r\_{1}-\sum\_{i=2}^{k}J\_{i}} |  |
|  |  |  |
| --- | --- | --- |
|  | (nJ0,J1,â€¦,Jk)â€‹âˆi=0k(Fâ€‹(y(i+1)k)âˆ’Fâ€‹(y(i)k))Ji,\displaystyle\quad\binom{n}{J\_{0},J\_{1},\ldots,J\_{k}}\prod\_{i=0}^{k}\Big(F\left(y\_{(i+1)}^{k}\right)-F\left(y\_{(i)}^{k}\right)\Big)^{J\_{i}}, |  |
|  |  |  |
| --- | --- | --- |
|  | s.t.âˆ‘i=0kJi=n.\displaystyle s.t.\quad\sum\_{i=0}^{k}J\_{i}=n. |  |

âˆ

###### Proof 2: Combinatorics.

First, we note

|  |  |  |
| --- | --- | --- |
|  | X(r)nâ‰¤yâ‡”râ‰¤âˆ‘i=1nğŸ™(âˆ’âˆ,y]â€‹(Xi),X\_{(r)}^{n}\leq y\iff r\leq\sum\_{i=1}^{n}\mathds{1}\_{(-\infty,y]}(X\_{i}), |  |

and so,

|  |  |  |
| --- | --- | --- |
|  | F(r1,â€¦,rk)nâ€‹(y1,â€¦,yk)\displaystyle F\_{(r\_{1},\ldots,r\_{k})}^{n}(y\_{1},\ldots,y\_{k}) |  |
|  |  |  |
| --- | --- | --- |
|  | =â„™â€‹[X(1)nâ‰¤y1,â€¦,X(k)nâ‰¤yk]\displaystyle=\mathbb{P}\left[X\_{(1)}^{n}\leq y\_{1},\ldots,X\_{(k)}^{n}\leq y\_{k}\right] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | =â„™â€‹[r1â‰¤âˆ‘i=1nğŸ™(âˆ’âˆ,y1]â€‹(Xi),â‹¯,rkâ‰¤âˆ‘i=1nğŸ™(âˆ’âˆ,yk]â€‹(Xi)].\displaystyle=\mathbb{P}\left[r\_{1}\leq\sum\_{i=1}^{n}\mathds{1}\_{(-\infty,y\_{1}]}(X\_{i})\,,\,\cdots\,,\,r\_{k}\leq\sum\_{i=1}^{n}\mathds{1}\_{(-\infty,y\_{k}]}(X\_{i})\right]. |  | (2.9) |

Here, we consider the following intervals

U0U1â‹¯Ukâˆ’1Uk\;U\_{0}\hskip 42.67912ptU\_{1}\hskip 39.83368pt\cdots\hskip 42.67912ptU\_{k-1}\hskip 45.52458ptU\_{k}

âˆ’âˆ=y0-\infty=y\_{0}]â€”â€”â€”[
y1y\_{1}]â€”â€”â€”[y2â‹¯ykâˆ’1y\_{2}\quad\cdots\quad y\_{k-1}]â€”â€”â€”[
yky\_{k}]â€”â€”â€”[yk+1=âˆy\_{k+1}=\infty,

and denote

|  |  |  |
| --- | --- | --- |
|  | #k:=#â€‹{i|XiâˆˆUk}=âˆ‘i=1nğŸ™]yk,yk+1[â€‹(Xi),\#\_{k}\;:=\;\#\{i|X\_{i}\in U\_{k}\}\,=\,\sum\_{i=1}^{n}\mathds{1}\_{]y\_{k},y\_{k+1}[}(X\_{i}), |  |

where #\# denotes the cardinality of the set. Then, we have

|  |  |  |
| --- | --- | --- |
|  | rkâ‰¤âˆ‘i=1nğŸ™(âˆ’âˆ,yk]â€‹(Xi)â‡”0â‰¤#kâ‰¤nâˆ’rk.r\_{k}\leq\sum\_{i=1}^{n}\mathds{1}\_{(-\infty,y\_{k}]}(X\_{i})\iff 0\leq\#\_{k}\leq n-r\_{k}. |  |

Thus, equation ([2.9](https://arxiv.org/html/2511.04784v1#S2.E9 "In 2. Ordered and Tail-Based Statistics")) can be rewritten as follows.

|  |  |  |
| --- | --- | --- |
|  | =â„™â€‹[r1â‰¤âˆ‘i=1nğŸ™(âˆ’âˆ,y1]â€‹(Xi),â‹¯,rkâˆ’1â‰¤âˆ‘i=1nğŸ™(âˆ’âˆ,ykâˆ’1]â€‹(Xi),â€‰0â‰¤#kâ‰¤nâˆ’rk]\displaystyle=\mathbb{P}\left[r\_{1}\leq\sum\_{i=1}^{n}\mathds{1}\_{(-\infty,y\_{1}]}(X\_{i})\,,\,\cdots\,,\,r\_{k-1}\leq\sum\_{i=1}^{n}\mathds{1}\_{(-\infty,y\_{k-1}]}(X\_{i}),\,0\leq\#\_{k}\leq n-r\_{k}\right] |  |
|  |  |  |
| --- | --- | --- |
|  | =âˆ‘Jk=0nâˆ’rkâ„™â€‹[r1â‰¤âˆ‘i=1nğŸ™(âˆ’âˆ,y1]â€‹(Xi),â‹¯,rkâˆ’1â‰¤âˆ‘i=1nğŸ™(âˆ’âˆ,ykâˆ’1]â€‹(Xi)|#k=Jk]\displaystyle=\sum\_{J\_{k}=0}^{n-r\_{k}}\mathbb{P}\left[r\_{1}\leq\sum\_{i=1}^{n}\mathds{1}\_{(-\infty,y\_{1}]}(X\_{i})\,,\,\cdots\,,\,r\_{k-1}\leq\sum\_{i=1}^{n}\mathds{1}\_{(-\infty,y\_{k-1}]}(X\_{i})\Big|\#\_{k}=J\_{k}\right] |  |
|  |  |  |
| --- | --- | --- |
|  | Ã—â„™â€‹[#k=Jk]\displaystyle\times\mathbb{P}[\#\_{k}=J\_{k}] |  |
|  |  |  |
| --- | --- | --- |
|  | =âˆ‘Jk=0nâˆ’rkâ„™â€‹[r1â‰¤âˆ‘i=1nğŸ™(âˆ’âˆ,y1]â€‹(Xi),â‹¯,rkâˆ’1â‰¤âˆ‘i=1nğŸ™(âˆ’âˆ,ykâˆ’1]â€‹(Xi)|#k=Jk]\displaystyle=\sum\_{J\_{k}=0}^{n-r\_{k}}\mathbb{P}\left[r\_{1}\leq\sum\_{i=1}^{n}\mathds{1}\_{(-\infty,y\_{1}]}(X\_{i})\,,\,\cdots\,,\,r\_{k-1}\leq\sum\_{i=1}^{n}\mathds{1}\_{(-\infty,y\_{k-1}]}(X\_{i})\Big|\#\_{k}=J\_{k}\right] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | Ã—(nJk)â€‹(Fâ€‹(yk+1)âˆ’Fâ€‹(yk))Jk.\displaystyle\times\binom{n}{J\_{k}}\Big(F\left(y\_{k+1}\right)-F\left(y\_{k}\right)\Big)^{J\_{k}}. |  | (2.10) |

By denoting

|  |  |  |
| --- | --- | --- |
|  | #kâˆ’1:=#â€‹{i|XiâˆˆUkâˆ’1}=âˆ‘i=1nğŸ™]ykâˆ’1,yk[â€‹(Xi),\#\_{k-1}:=\#\{i|X\_{i}\in U\_{k-1}\}=\sum\_{i=1}^{n}\mathds{1}\_{]y\_{k-1},y\_{k}[}(X\_{i}), |  |

we can continue ([2.10](https://arxiv.org/html/2511.04784v1#S2.E10 "In 2. Ordered and Tail-Based Statistics")) as follows.

|  |  |  |
| --- | --- | --- |
|  | =âˆ‘Jk=0nâˆ’rkâˆ‘Jkâˆ’1=0nâˆ’Jkâˆ’rkâˆ’1\displaystyle=\sum\_{J\_{k}=0}^{n-r\_{k}}\quad\sum\_{J\_{k-1}=0}^{n-J\_{k}-r\_{k-1}} |  |
|  |  |  |
| --- | --- | --- |
|  | â„™[r1â‰¤âˆ‘i=1nğŸ™(âˆ’âˆ,y1](Xi),â‹¯,rkâˆ’2â‰¤âˆ‘i=1nğŸ™(âˆ’âˆ,ykâˆ’2](Xi)|#kâˆ’1=Jkâˆ’1,#k=Jk]\displaystyle\mathbb{P}\left[r\_{1}\leq\sum\_{i=1}^{n}\mathds{1}\_{(-\infty,y\_{1}]}(X\_{i})\,,\,\cdots\,,\,r\_{k-2}\leq\sum\_{i=1}^{n}\mathds{1}\_{(-\infty,y\_{k-2}]}(X\_{i})\Big|\#\_{k-1}=J\_{k-1},\#\_{k}=J\_{k}\right] |  |
|  |  |  |
| --- | --- | --- |
|  | Ã—â„™â€‹[#kâˆ’1=Jkâˆ’1|#k=Jk]â‹…â„™â€‹[#k=Jk]\displaystyle\times\mathbb{P}[\#\_{k-1}=J\_{k-1}\,|\,\#\_{k}=J\_{k}]\cdot\mathbb{P}[\#\_{k}=J\_{k}] |  |
|  |  |  |
| --- | --- | --- |
|  | =âˆ‘Jk=0nâˆ’rkâˆ‘Jkâˆ’1=0nâˆ’Jkâˆ’rkâˆ’1\displaystyle=\sum\_{J\_{k}=0}^{n-r\_{k}}\sum\_{J\_{k-1}=0}^{n-J\_{k}-r\_{k-1}} |  |
|  |  |  |
| --- | --- | --- |
|  | â„™[r1â‰¤âˆ‘i=1nğŸ™(âˆ’âˆ,y1](Xi),â‹¯,rkâˆ’2â‰¤âˆ‘i=1nğŸ™(âˆ’âˆ,ykâˆ’2](Xi)|#kâˆ’1=Jkâˆ’1,#k=Jk]\displaystyle\mathbb{P}\left[r\_{1}\leq\sum\_{i=1}^{n}\mathds{1}\_{(-\infty,y\_{1}]}(X\_{i})\,,\,\cdots\,,\,r\_{k-2}\leq\sum\_{i=1}^{n}\mathds{1}\_{(-\infty,y\_{k-2}]}(X\_{i})\Big|\#\_{k-1}=J\_{k-1},\#\_{k}=J\_{k}\right] |  |
|  |  |  |
| --- | --- | --- |
|  | Ã—(nJk)â€‹(nâˆ’JkJkâˆ’1)â€‹(Fâ€‹(yk+1)âˆ’Fâ€‹(yk))Jkâ€‹(Fâ€‹(yk)âˆ’Fâ€‹(ykâˆ’1))Jkâˆ’1.\displaystyle\times\binom{n}{J\_{k}}\binom{n-J\_{k}}{J\_{k-1}}\Big(F\left(y\_{k+1}\right)-F\left(y\_{k}\right)\Big)^{J\_{k}}\Big(F\left(y\_{k}\right)-F\left(y\_{k-1}\right)\Big)^{J\_{k-1}}. |  |

By continuing this process, we obtain

|  |  |  |  |
| --- | --- | --- | --- |
|  | =\displaystyle= | âˆ‘Jk=0nâˆ’rkâˆ‘Jkâˆ’1=0nâˆ’rkâˆ’1âˆ’Jkâˆ‘Jkâˆ’2=0nâˆ’rkâˆ’2âˆ’Jkâˆ’Jkâˆ’1â‹¯â€‹âˆ‘J1=0nâˆ’r1âˆ’âˆ‘i=2kJi\displaystyle\sum\_{J\_{k}=0}^{n-r\_{k}}\quad\sum\_{J\_{k-1}=0}^{n-r\_{k-1}-J\_{k}}\quad\sum\_{J\_{k-2}=0}^{n-r\_{k-2}-J\_{k}-J\_{k-1}}\cdots\sum\_{J\_{1}=0}^{n-r\_{1}-\sum\_{i=2}^{k}J\_{i}} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â„™[#1=J1|#2=J2,â€¦,#k=Jk]\displaystyle\mathbb{P}[\#\_{1}=J\_{1}\,|\,\#\_{2}=J\_{2},\ldots,\#\_{k}=J\_{k}] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â„™[#2=J2|#3=J3,â€¦,#k=Jk]\displaystyle\mathbb{P}[\#\_{2}=J\_{2}\,|\,\#\_{3}=J\_{3},\ldots,\#\_{k}=J\_{k}] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‹¯\displaystyle\cdots |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â„™â€‹[#kâˆ’1=Jkâˆ’1|#k=Jk]\displaystyle\mathbb{P}[\#\_{k-1}=J\_{k-1}\,|\,\#\_{k}=J\_{k}] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â„™â€‹[#k=Jk]\displaystyle\mathbb{P}[\#\_{k}=J\_{k}] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | =\displaystyle= | âˆ‘Jk=0nâˆ’rkâˆ‘Jkâˆ’1=0nâˆ’rkâˆ’1âˆ’Jkâˆ‘Jkâˆ’2=0nâˆ’rkâˆ’2âˆ’Jkâˆ’Jkâˆ’1â‹¯â€‹âˆ‘J1=0nâˆ’r1âˆ’âˆ‘i=2kJi\displaystyle\sum\_{J\_{k}=0}^{n-r\_{k}}\quad\sum\_{J\_{k-1}=0}^{n-r\_{k-1}-J\_{k}}\quad\sum\_{J\_{k-2}=0}^{n-r\_{k-2}-J\_{k}-J\_{k-1}}\cdots\sum\_{J\_{1}=0}^{n-r\_{1}-\sum\_{i=2}^{k}J\_{i}} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | (nJk)â€‹(nâˆ’JkJkâˆ’1)â€‹â‹¯â€‹(nâˆ’âˆ‘i=2kJiJ1)â‹…(Fâ€‹(y1)âˆ’Fâ€‹(y0))nâˆ’âˆ‘i=1kJi\displaystyle\binom{n}{J\_{k}}\binom{n-J\_{k}}{J\_{k-1}}\cdots\binom{n-\sum\_{i=2}^{k}J\_{i}}{J\_{1}}\cdot\Big(F\left(y\_{1}\right)-F\left(y\_{0}\right)\Big)^{n-\sum\_{i=1}^{k}J\_{i}} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | Ã—(Fâ€‹(y2)âˆ’Fâ€‹(y1))J1â€‹â‹¯â€‹(Fâ€‹(yk+1)âˆ’Fâ€‹(yk))Jk,\displaystyle\times\Big(F\left(y\_{2}\right)-F\left(y\_{1}\right)\Big)^{J\_{1}}\cdots\Big(F\left(y\_{k+1}\right)-F\left(y\_{k}\right)\Big)^{J\_{k}}, |  |

and by taking J0=nâˆ’âˆ‘i=1kJiJ\_{0}=n-\sum\_{i=1}^{k}J\_{i}, this returns ([2.6](https://arxiv.org/html/2511.04784v1#S2.E6 "In Theorem 2.5. â€£ 2. Ordered and Tail-Based Statistics")).
âˆ

###### Corollary 2.6.

Given the assumptions and notations in Theorem [2.5](https://arxiv.org/html/2511.04784v1#S2.Thmdfn5 "Theorem 2.5. â€£ 2. Ordered and Tail-Based Statistics"), if x1â‰¤â‹¯â‰¤xkx\_{1}\leq\cdots\leq x\_{k}, then

|  |  |  |  |
| --- | --- | --- | --- |
|  | F(r1,â€¦,rk)nâ€‹(x1,â€¦,xk)\displaystyle F\_{(r\_{1},\ldots,r\_{k})}^{n}(x\_{1},\ldots,x\_{k}) |  | (2.11) |
|  |  |  |
| --- | --- | --- |
|  | =âˆ‘Jk=0nâˆ’rkâˆ‘Jkâˆ’1=0nâˆ’rkâˆ’1âˆ’Jkâˆ‘Jkâˆ’2=0nâˆ’rkâˆ’2âˆ’Jkâˆ’Jkâˆ’1â‹¯â€‹âˆ‘J1=0nâˆ’r1âˆ’âˆ‘i=2kJi\displaystyle=\sum\_{J\_{k}=0}^{n-r\_{k}}\quad\sum\_{J\_{k-1}=0}^{n-r\_{k-1}-J\_{k}}\quad\sum\_{J\_{k-2}=0}^{n-r\_{k-2}-J\_{k}-J\_{k-1}}\cdots\sum\_{J\_{1}=0}^{n-r\_{1}-\sum\_{i=2}^{k}J\_{i}} |  |
|  |  |  |
| --- | --- | --- |
|  | (nJ0,J1,â€¦,Jk)â€‹âˆi=0k(Fâ€‹(xi+1)âˆ’Fâ€‹(xi))Ji,\displaystyle\quad\binom{n}{J\_{0},J\_{1},\ldots,J\_{k}}\prod\_{i=0}^{k}\big(F\left(x\_{i+1}\right)-F\left(x\_{i}\right)\big)^{J\_{i}}, |  |
|  |  |  |
| --- | --- | --- |
|  | s.t.âˆ‘i=0kJi=n.\displaystyle s.t.\quad\sum\_{i=0}^{k}J\_{i}=n. |  |

## 3. Exact Distribution of Tail-Based Statistics

Here, we apply the Corollary [2.3](https://arxiv.org/html/2511.04784v1#S2.Thmdfn3 "Corollary 2.3. â€£ 2. Ordered and Tail-Based Statistics") to investigate the exact cumulative distribution of Î›nâ€‹(p)\Lambda\_{n}(p). We use a.s. to denote almost sure convergence.

###### Proposition 3.1.

Let pâˆˆ(0,1)p\in(0,1) and 0<|Î»|<10<|\lambda|<1. If the random variables X1,X2,â€¦,XnX\_{1},X\_{2},\ldots,X\_{n} are i.i.d with common absolutely continuous distribution FF, and the almost everywhere positive density function ff, then for some N0â‰¥1N\_{0}\geq 1, the cumulative distribution function of Î›nâ€‹(p),nâ‰¥N0\Lambda\_{n}(p),n\geq N\_{0} is

* (i)

  If Î»â€‹ğ”¼â€‹[X]>0\lambda\mathbb{E}[X]>0, then

  |  |  |  |  |
  | --- | --- | --- | --- |
  |  |  | FÎ›nâ€‹(p)â€‹(Î»)=1âˆ’n!â€‹âˆ«01âˆ«0unâ‹¯â€‹âˆ«0u3\displaystyle F\_{\Lambda\_{n}(p)}(\lambda)=1-n!\int\_{0}^{1}\int\_{0}^{u\_{n}}\cdots\int\_{0}^{u\_{3}} |  |
  |  |  |  |  |  |
  | --- | --- | --- | --- | --- |
  |  |  | Fâ€‹[(1âˆ’Î»Î»)â€‹âˆ‘i=âŒˆnâ€‹pâŒ‰nFâˆ’1â€‹(ui)âˆ’âˆ‘i=2âŒˆnâ€‹pâŒ‰âˆ’1Fâˆ’1â€‹(ui)]â€‹dâ€‹u2â€‹â‹¯â€‹dâ€‹un.\displaystyle F\left[\left(\frac{1-\lambda}{\lambda}\right)\sum\_{i=\left\lceil np\right\rceil}^{n}F^{-1}(u\_{i})-\sum\_{i=2}^{\left\lceil np\right\rceil-1}F^{-1}(u\_{i})\right]\,du\_{2}\cdots du\_{n}. |  | (3.1) |
* (ii)

  If Î»â€‹ğ”¼â€‹[X]<0\lambda\mathbb{E}[X]<0, then

  |  |  |  |  |
  | --- | --- | --- | --- |
  |  |  | FÎ›nâ€‹(p)â€‹(Î»)=n!â€‹âˆ«01âˆ«0unâ‹¯â€‹âˆ«0u3\displaystyle F\_{\Lambda\_{n}(p)}(\lambda)=n!\int\_{0}^{1}\int\_{0}^{u\_{n}}\cdots\int\_{0}^{u\_{3}} |  |
  |  |  |  |  |  |
  | --- | --- | --- | --- | --- |
  |  |  | Fâ€‹[(1âˆ’Î»Î»)â€‹âˆ‘i=âŒˆnâ€‹pâŒ‰nFâˆ’1â€‹(ui)âˆ’âˆ‘i=2âŒˆnâ€‹pâŒ‰âˆ’1Fâˆ’1â€‹(ui)]â€‹dâ€‹u2â€‹â‹¯â€‹dâ€‹un.\displaystyle F\left[\left(\frac{1-\lambda}{\lambda}\right)\sum\_{i=\left\lceil np\right\rceil}^{n}F^{-1}(u\_{i})-\sum\_{i=2}^{\left\lceil np\right\rceil-1}F^{-1}(u\_{i})\right]\,du\_{2}\cdots du\_{n}. |  | (3.2) |

###### Proof.

For (i), let 0<Î»,ğ”¼â€‹[X]<10<\lambda,\mathbb{E}[X]<1. Then by the strong low of larg numbers (LLN) âˆ‘i=1nXi/nâ€‹âŸ¶a.s.â€‹ğ”¼â€‹[X]\sum\_{i=1}^{n}X\_{i}/n\overset{a.s.}{\longrightarrow}\mathbb{E}[X] and so, there are some N0â‰¥1N\_{0}\geq 1 that for all nâ‰¥N0n\geq N\_{0} we have âˆ‘i=1nXi>0\sum\_{i=1}^{n}X\_{i}>0. Next we have

|  |  |  |
| --- | --- | --- |
|  | Î›nâ€‹(p)â‰¤Î»\displaystyle\Lambda\_{n}(p)\leq\lambda |  |
|  |  |  |
| --- | --- | --- |
|  | â‡”âˆ‘i=âŒˆnâ€‹pâŒ‰nX(i)nâˆ‘i=1nXiâ‰¤Î»\displaystyle\iff\frac{\sum\_{i=\left\lceil np\right\rceil}^{n}X\_{(i)}^{n}}{\sum\_{i=1}^{n}X\_{i}}\leq\lambda |  |
|  |  |  |
| --- | --- | --- |
|  | â‡”âˆ‘i=âŒˆnâ€‹pâŒ‰nX(i)nâ‰¤Î»â€‹âˆ‘i=1nXi\displaystyle\iff\sum\_{i=\left\lceil np\right\rceil}^{n}X\_{(i)}^{n}\leq\lambda\sum\_{i=1}^{n}X\_{i} |  |
|  |  |  |
| --- | --- | --- |
|  | â‡”Î»â€‹âˆ‘i=1âŒˆnâ€‹pâŒ‰âˆ’1X(i)nâˆ’(1âˆ’Î»)â€‹âˆ‘i=âŒˆnâ€‹pâŒ‰nX(i)nâ‰¥â€‰0\displaystyle\iff\lambda\sum\_{i=1}^{\left\lceil np\right\rceil-1}X\_{(i)}^{n}\,-\,(1-\lambda)\sum\_{i=\left\lceil np\right\rceil}^{n}X\_{(i)}^{n}\,\geq\,0 |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‡”X(1)nâ‰¥(1âˆ’Î»Î»)â€‹âˆ‘i=âŒˆnâ€‹pâŒ‰nX(i)nâˆ’âˆ‘i=2âŒˆnâ€‹pâŒ‰âˆ’1X(i)n.\displaystyle\iff X\_{(1)}^{n}\geq\left(\frac{1-\lambda}{\lambda}\right)\sum\_{i=\left\lceil np\right\rceil}^{n}X\_{(i)}^{n}-\sum\_{i=2}^{\left\lceil np\right\rceil-1}X\_{(i)}^{n}. |  | (3.3) |

Now, by denoting

|  |  |  |
| --- | --- | --- |
|  | Dnâ€‹(Î»,p):={ğ’™=(x1,â€¦,xn)|(1âˆ’Î»Î»)â€‹âˆ‘i=âŒˆnâ€‹pâŒ‰nxiâˆ’âˆ‘i=2âŒˆnâ€‹pâŒ‰âˆ’1xiâ‰¤x1,x1<x2<â‹¯<xn}âŠ‚â„n,D\_{n}(\lambda,p):=\left\{\bm{x}=(x\_{1},\ldots,x\_{n})\,\Bigg|\,\begin{matrix}\left(\frac{1-\lambda}{\lambda}\right)\sum\_{i=\left\lceil np\right\rceil}^{n}x\_{i}-\sum\_{i=2}^{\left\lceil np\right\rceil-1}x\_{i}\leq x\_{1},\\ x\_{1}<x\_{2}<\cdots<x\_{n}\end{matrix}\right\}\subset\mathbb{R}^{n}, |  |

from Corollary [2.3](https://arxiv.org/html/2511.04784v1#S2.Thmdfn3 "Corollary 2.3. â€£ 2. Ordered and Tail-Based Statistics"), one can write

|  |  |  |  |
| --- | --- | --- | --- |
|  | FÎ›nâ€‹(p)â€‹(Î»)\displaystyle F\_{\Lambda\_{n}(p)}(\lambda) | =â„™â€‹[Î›nâ€‹(p)â‰¤Î»]\displaystyle=\mathbb{P}[\Lambda\_{n}(p)\leq\lambda] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =â„™â€‹[Î»â€‹âˆ‘i=1âŒˆnâ€‹pâŒ‰âˆ’1X(i)nâˆ’(1âˆ’Î»)â€‹âˆ‘i=âŒˆnâ€‹pâŒ‰nX(i)nâ‰¥â€‰0]\displaystyle=\mathbb{P}\left[\lambda\sum\_{i=1}^{\left\lceil np\right\rceil-1}X\_{(i)}^{n}\,-\,(1-\lambda)\sum\_{i=\left\lceil np\right\rceil}^{n}X\_{(i)}^{n}\,\geq\,0\right] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =âˆ«Dnâ€‹(Î»,p)f(1,â€¦,n)nâ€‹(ğ’™)â€‹ğ‘‘ğ’™\displaystyle=\int\_{D\_{n}(\lambda,p)}f^{n}\_{(1,\ldots,n)}(\bm{x})\,d\bm{x} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =n!â€‹âˆ«â‹¯âˆ«Dnâ€‹(Î»,p)â€‹(âˆi=1nfâ€‹(xi))â€‹dâ€‹x1â€‹â‹¯â€‹dâ€‹xn\displaystyle=n!\underset{D\_{n}(\lambda,p)}{\idotsint}\left(\prod\_{i=1}^{n}f(x\_{i})\right)\,dx\_{1}\cdots dx\_{n} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =n!â€‹âˆ«âˆ’âˆâˆâˆ«âˆ’âˆxnâ‹¯â€‹âˆ«âˆ’âˆx3âˆ«(1âˆ’Î»Î»)â€‹âˆ‘i=âŒˆnâ€‹pâŒ‰nxiâˆ’âˆ‘i=2âŒˆnâ€‹pâŒ‰âˆ’1xix2\displaystyle=n!\int\_{-\infty}^{\infty}\int\_{-\infty}^{x\_{n}}\cdots\int\_{-\infty}^{x\_{3}}\int\_{\left(\frac{1-\lambda}{\lambda}\right)\sum\_{i=\left\lceil np\right\rceil}^{n}x\_{i}-\sum\_{i=2}^{\left\lceil np\right\rceil-1}x\_{i}}^{x\_{2}} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | (âˆi=1nfâ€‹(xi))â€‹dâ€‹x1â€‹â‹¯â€‹dâ€‹xn.\displaystyle\qquad\left(\prod\_{i=1}^{n}f(x\_{i})\right)\,dx\_{1}\cdots dx\_{n}. |  |

Then, since FF is almost everywhere differentiable and invertible, by changing the variables ui=Fâ€‹(xi)u\_{i}=F(x\_{i}) or xi=Fâˆ’1â€‹(ui)x\_{i}=F^{-1}(u\_{i}), for every i=1,â€¦,ni=1,\ldots,n we have

|  |  |  |
| --- | --- | --- |
|  | =n!â€‹âˆ«01âˆ«0unâ‹¯â€‹âˆ«0u3âˆ«Fâ€‹[(1âˆ’Î»Î»)â€‹âˆ‘i=âŒˆnâ€‹pâŒ‰nFâˆ’1â€‹(ui)âˆ’âˆ‘i=2âŒˆnâ€‹pâŒ‰âˆ’1Fâˆ’1â€‹(ui)]u2ğ‘‘u1â€‹â‹¯â€‹ğ‘‘un\displaystyle=n!\int\_{0}^{1}\int\_{0}^{u\_{n}}\cdots\int\_{0}^{u\_{3}}\int\_{F\left[\left(\frac{1-\lambda}{\lambda}\right)\sum\_{i=\left\lceil np\right\rceil}^{n}F^{-1}(u\_{i})-\sum\_{i=2}^{\left\lceil np\right\rceil-1}F^{-1}(u\_{i})\right]}^{u\_{2}}\,du\_{1}\cdots du\_{n} |  |
|  |  |  |
| --- | --- | --- |
|  | =n!â€‹âˆ«01âˆ«0unâ‹¯â€‹âˆ«0u3âˆ«0u2ğ‘‘u1â€‹â‹¯â€‹ğ‘‘un\displaystyle=n!\int\_{0}^{1}\int\_{0}^{u\_{n}}\cdots\int\_{0}^{u\_{3}}\int\_{0}^{u\_{2}}\,du\_{1}\cdots du\_{n} |  |
|  |  |  |
| --- | --- | --- |
|  | âˆ’n!â€‹âˆ«01âˆ«0unâ‹¯â€‹âˆ«0u3âˆ«0Fâ€‹[(1âˆ’Î»Î»)â€‹âˆ‘i=âŒˆnâ€‹pâŒ‰nFâˆ’1â€‹(ui)âˆ’âˆ‘i=2âŒˆnâ€‹pâŒ‰âˆ’1Fâˆ’1â€‹(ui)]ğ‘‘u1â€‹â‹¯â€‹ğ‘‘un,\displaystyle-n!\int\_{0}^{1}\int\_{0}^{u\_{n}}\cdots\int\_{0}^{u\_{3}}\int\_{0}^{F\left[\left(\frac{1-\lambda}{\lambda}\right)\sum\_{i=\left\lceil np\right\rceil}^{n}F^{-1}(u\_{i})-\sum\_{i=2}^{\left\lceil np\right\rceil-1}F^{-1}(u\_{i})\right]}\,du\_{1}\cdots du\_{n}, |  |

and this yields ([(i)](https://arxiv.org/html/2511.04784v1#S3.Ex1 "item (i) â€£ Proposition 3.1. â€£ 3. Exact Distribution of Tail-Based Statistics")). If âˆ’1<Î»,ğ”¼â€‹[X]<0-1<\lambda,\mathbb{E}[X]<0, then similarly ([3.3](https://arxiv.org/html/2511.04784v1#S3.E3 "In 3. Exact Distribution of Tail-Based Statistics")) is valid and so we have the same result.

For (ii), let Î»>0,ğ”¼â€‹[X]<0\lambda>0,\mathbb{E}[X]<0. Similar to the proof of (i), there are some N0â‰¥1N\_{0}\geq 1 that for all nâ‰¥N0n\geq N\_{0} we have âˆ‘i=1nXi<0\sum\_{i=1}^{n}X\_{i}<0. Next, for this case, one can see

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î›nâ€‹(p)â‰¥Î»â‡”X(1)nâ‰¥(1âˆ’Î»Î»)â€‹âˆ‘i=âŒˆnâ€‹pâŒ‰nX(i)nâˆ’âˆ‘i=2âŒˆnâ€‹pâŒ‰âˆ’1X(i)n,\Lambda\_{n}(p)\geq\lambda\iff X\_{(1)}^{n}\geq\left(\frac{1-\lambda}{\lambda}\right)\sum\_{i=\left\lceil np\right\rceil}^{n}X\_{(i)}^{n}-\sum\_{i=2}^{\left\lceil np\right\rceil-1}X\_{(i)}^{n}, |  | (3.4) |

and so, in this case we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | FÎ›nâ€‹(p)â€‹(Î»)\displaystyle F\_{\Lambda\_{n}(p)}(\lambda) | =â„™â€‹[Î›nâ€‹(p)â‰¤Î»]\displaystyle=\mathbb{P}[\Lambda\_{n}(p)\leq\lambda] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =1âˆ’â„™â€‹[Î›nâ€‹(p)â‰¥Î»]\displaystyle=1-\mathbb{P}[\Lambda\_{n}(p)\geq\lambda] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =1âˆ’â„™â€‹[X(1)nâ‰¥(1âˆ’Î»Î»)â€‹âˆ‘i=âŒˆnâ€‹pâŒ‰nX(i)nâˆ’âˆ‘i=2âŒˆnâ€‹pâŒ‰âˆ’1X(i)n]\displaystyle=1-\mathbb{P}\left[X\_{(1)}^{n}\geq\left(\frac{1-\lambda}{\lambda}\right)\sum\_{i=\left\lceil np\right\rceil}^{n}X\_{(i)}^{n}-\sum\_{i=2}^{\left\lceil np\right\rceil-1}X\_{(i)}^{n}\right] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =1âˆ’âˆ«Dnâ€‹(Î»,p)f(1,â€¦,n)nâ€‹(ğ’™)â€‹ğ‘‘ğ’™.\displaystyle=1-\int\_{D\_{n}(\lambda,p)}f^{n}\_{(1,\ldots,n)}(\bm{x})\,d\bm{x}. |  |

Now, proceeding with similar calculation of part (i), from this final integral we have ([(ii)](https://arxiv.org/html/2511.04784v1#S3.Ex2 "item (ii) â€£ Proposition 3.1. â€£ 3. Exact Distribution of Tail-Based Statistics")). If Î»<0,ğ”¼â€‹[X]>0\lambda<0,\mathbb{E}[X]>0, then similarly ([3.4](https://arxiv.org/html/2511.04784v1#S3.E4 "In 3. Exact Distribution of Tail-Based Statistics")) is valid, and so, we have the same result.
âˆ

## 4. Convergence of Tail-Based Statistics

As the explicit form of the exact distribution functions ([(i)](https://arxiv.org/html/2511.04784v1#S3.Ex1 "item (i) â€£ Proposition 3.1. â€£ 3. Exact Distribution of Tail-Based Statistics")) and ([(ii)](https://arxiv.org/html/2511.04784v1#S3.Ex2 "item (ii) â€£ Proposition 3.1. â€£ 3. Exact Distribution of Tail-Based Statistics")) include multiple integrals, they are not computationally suitable for larg nambers of nn. So, we need to investigate further for the asymptotic behavior and distribution here.

###### Lemma 4.1.

Let X1,X2,â€¦,XnX\_{1},X\_{2},\ldots,X\_{n} be i.i.d random variables with common absolutely continuous distribution FF. Then, for all pâˆˆ(0,1)p\in(0,1) that FF is continuous at its ppth quantile qpq\_{p}, we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | 1nâ€‹âˆ‘i=1nXiâ€‹ğŸ™{Xiâ‰¥Qnâ€‹(p)}â€‹âŸ¶a.s.â€‹ğ”¼â€‹[X1â€‹ğŸ™{X1â‰¥qp}],\frac{1}{n}\sum\_{i=1}^{n}X\_{i}\mathds{1}\_{\{X\_{i}\geq Q\_{n}(p)\}}\overset{a.s.}{\longrightarrow}\mathbb{E}\left[X\_{1}\mathds{1}\_{\{X\_{1}\geq q\_{p}\}}\right], |  | (4.1) |

where Qnâ€‹(p)Q\_{n}(p) is the ppth quantile of {Xi}i=1n\{X\_{i}\}\_{i=1}^{n}.

###### Proof.

Let

|  |  |  |  |
| --- | --- | --- | --- |
|  | Unâ€‹(p)\displaystyle U\_{n}(p) | =1nâ€‹âˆ‘i=1nXiâ€‹ğŸ™{Xiâ‰¥Qnâ€‹(p)},\displaystyle=\frac{1}{n}\sum\_{i=1}^{n}X\_{i}\mathds{1}\_{\{X\_{i}\geq Q\_{n}(p)\}}, |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | Vnâ€‹(p)\displaystyle V\_{n}(p) | =1nâ€‹âˆ‘i=1nXiâ€‹ğŸ™{Xiâ‰¥qp}.\displaystyle=\frac{1}{n}\sum\_{i=1}^{n}X\_{i}\mathds{1}\_{\{X\_{i}\geq q\_{p}\}}. |  |

By the strong LLN we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | Vnâ€‹(p)â€‹âŸ¶a.s.â€‹ğ”¼â€‹[X1â€‹ğŸ™{X1â‰¥qp}].V\_{n}(p)\overset{a.s.}{\longrightarrow}\mathbb{E}\left[X\_{1}\mathds{1}\_{\{X\_{1}\geq q\_{p}\}}\right]. |  | (4.2) |

On the other hand, it is shown in [[25](https://arxiv.org/html/2511.04784v1#bib.bib25)] that, given the continuity of FF on qpq\_{p}, we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | Qnâ€‹(p)â€‹âŸ¶a.s.â€‹qp.Q\_{n}(p)\overset{a.s.}{\longrightarrow}q\_{p}. |  | (4.3) |

Next, we can write

|  |  |  |  |
| --- | --- | --- | --- |
|  | |Unâ€‹(p)âˆ’Vnâ€‹(p)|\displaystyle|U\_{n}(p)-V\_{n}(p)| | â‰¤1nâ€‹âˆ‘i=1n|Xi|â‹…|ğŸ™{Xiâ‰¥Qnâ€‹(p)}âˆ’ğŸ™{Xiâ‰¥qp}|\displaystyle\leq\frac{1}{n}\sum\_{i=1}^{n}|X\_{i}|\cdot|\mathds{1}\_{\{X\_{i}\geq Q\_{n}(p)\}}-\mathds{1}\_{\{X\_{i}\geq q\_{p}\}}| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =1nâ€‹âˆ‘i=1n|Xi|â€‹â€‰1{Qnâ€‹(p)âˆ§qpâ‰¤Xi<Qnâ€‹(p)âˆ¨qp}\displaystyle=\frac{1}{n}\sum\_{i=1}^{n}|X\_{i}|\,\mathds{1}\_{\{Q\_{n}(p)\wedge q\_{p}\leq X\_{i}<Q\_{n}(p)\vee q\_{p}\}} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =1nâ€‹âˆ‘i=1n|Xi|â€‹â€‰1{anâ€‹(p)â‰¤Xi<bnâ€‹(p)},\displaystyle=\frac{1}{n}\sum\_{i=1}^{n}|X\_{i}|\,\mathds{1}\_{\{a\_{n}(p)\leq X\_{i}<b\_{n}(p)\}}, |  |

where âˆ§\wedge and âˆ¨\vee are respectively minimum and maximum. Here, from ([4.3](https://arxiv.org/html/2511.04784v1#S4.E3 "In 4. Convergence of Tail-Based Statistics")), for both anâ€‹(p)=Qnâ€‹(p)âˆ§qpa\_{n}(p)=Q\_{n}(p)\wedge q\_{p} and bnâ€‹(p)=Qnâ€‹(p)âˆ¨qpb\_{n}(p)=Q\_{n}(p)\vee q\_{p} we have

|  |  |  |
| --- | --- | --- |
|  | anâ€‹(p),bnâ€‹(p)â€‹âŸ¶a.s.â€‹qp.a\_{n}(p),b\_{n}(p)\overset{a.s.}{\longrightarrow}q\_{p}. |  |

So, almost surely, for all arbitrary Îµ>0\varepsilon>0, there are some N1Îµâ€‹(p)>0N\_{1}^{\varepsilon}(p)>0 that for all nâ‰¥N1Îµâ€‹(p)n\geq N\_{1}^{\varepsilon}(p), we have

|  |  |  |
| --- | --- | --- |
|  | ğŸ™[anâ€‹(p),bnâ€‹(p))â€‹(x)=0,âˆ€xâˆˆâ„âˆ–(qpâˆ’Îµ,qp+Îµ),\mathds{1}\_{[a\_{n}(p),b\_{n}(p))}(x)=0,\qquad\forall x\in\mathbb{R}\setminus(q\_{p}-\varepsilon,q\_{p}+\varepsilon), |  |

and so,

|  |  |  |
| --- | --- | --- |
|  | ğŸ™[anâ€‹(p),bnâ€‹(p))â€‹(x)â‰¤ğŸ™(qpâˆ’Îµ,qp+Îµ)â€‹(x).\mathds{1}\_{[a\_{n}(p),b\_{n}(p))}(x)\leq\mathds{1}\_{(q\_{p}-\varepsilon\,,\,q\_{p}+\varepsilon)}(x). |  |

Hence, for iâ‰¥1i\geq 1

|  |  |  |
| --- | --- | --- |
|  | ğŸ™[anâ€‹(p),bnâ€‹(p))â€‹(Xi)â‰¤ğŸ™(qpâˆ’Îµ,qp+Îµ)â€‹(Xi).\mathds{1}\_{[a\_{n}(p),b\_{n}(p))}(X\_{i})\leq\mathds{1}\_{(q\_{p}-\varepsilon\,,\,q\_{p}+\varepsilon)}(X\_{i}). |  |

So, for nâ‰¥N1Îµâ€‹(p)n\geq N\_{1}^{\varepsilon}(p), we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | |Unâ€‹(p)âˆ’Vnâ€‹(p)|\displaystyle|U\_{n}(p)-V\_{n}(p)| | â‰¤1nâ€‹âˆ‘i=1n|Xi|â€‹â€‰1{[anâ€‹(p),bnâ€‹(p))}â€‹(Xi)\displaystyle\leq\frac{1}{n}\sum\_{i=1}^{n}|X\_{i}|\,\mathds{1}\_{\{[a\_{n}(p),b\_{n}(p))\}}(X\_{i}) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤1nâ€‹âˆ‘i=1n|Xi|â€‹â€‰1(qpâˆ’Îµ,qp+Îµ)â€‹(Xi).\displaystyle\leq\frac{1}{n}\sum\_{i=1}^{n}|X\_{i}|\,\mathds{1}\_{(q\_{p}-\varepsilon\,,\,q\_{p}+\varepsilon)}(X\_{i}). |  |

Again, by the strong LLN

|  |  |  |
| --- | --- | --- |
|  | 1nâ€‹âˆ‘i=1n|Xi|â€‹â€‰1(qpâˆ’Îµ,qp+Îµ)â€‹(Xi)â€‹âŸ¶a.s.â€‹ğ”¼â€‹[|X1|â€‹â€‰1(qpâˆ’Îµ,qp+Îµ)â€‹(X1)].\frac{1}{n}\sum\_{i=1}^{n}|X\_{i}|\,\mathds{1}\_{(q\_{p}-\varepsilon\,,\,q\_{p}+\varepsilon)}(X\_{i})\overset{a.s.}{\longrightarrow}\mathbb{E}\left[|X\_{1}|\,\mathds{1}\_{(q\_{p}-\varepsilon\,,\,q\_{p}+\varepsilon)}(X\_{1})\right]. |  |

That is, almost surely, for all Îµ>0\varepsilon>0, there are some N2Îµâ€‹(p)>0N\_{2}^{\varepsilon}(p)>0 that for all nâ‰¥N2Îµâ€‹(p)n\geq N\_{2}^{\varepsilon}(p), we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | |Unâ€‹(p)âˆ’Vnâ€‹(p)|\displaystyle|U\_{n}(p)-V\_{n}(p)| | â‰¤ğ”¼â€‹[|X1|â€‹â€‰1(qpâˆ’Îµ,qp+Îµ)â€‹(X1)]\displaystyle\leq\mathbb{E}\left[|X\_{1}|\,\mathds{1}\_{(q\_{p}-\varepsilon\,,\,q\_{p}+\varepsilon)}(X\_{1})\right] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =âˆ«âˆ’ÎµÎµ|qpâˆ’x|â‹…fâ€‹(qpâˆ’x)â€‹ğ‘‘x\displaystyle=\int\_{-\varepsilon}^{\varepsilon}|q\_{p}-x|\cdot f(q\_{p}-x)\,dx |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤2â€‹Îµâ€‹max(âˆ’Îµ,Îµ)â¡|qpâˆ’x|â‹…fâ€‹(qpâˆ’x)\displaystyle\leq 2\varepsilon\max\_{(-\varepsilon,\varepsilon)}|q\_{p}-x|\cdot f(q\_{p}-x) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤2â€‹Îµâ€‹M,\displaystyle\leq 2\varepsilon M, |  |

where ff is the Radon-Nikodym derivative of FF, i.e., the probability density function of X1X\_{1}. We note, as ff is integrable in an interval (qpâˆ’â„“,qp+â„“)(q\_{p}-\ell,q\_{p}+\ell) around x=qpx=q\_{p}, we have M=max(âˆ’â„“,â„“)â¡|qpâˆ’x|â‹…fâ€‹(qpâˆ’x)<âˆM=\max\_{(-\ell,\ell)}|q\_{p}-x|\cdot f(q\_{p}-x)<\infty. So,

|  |  |  |  |
| --- | --- | --- | --- |
|  | |Unâ€‹(p)âˆ’Vnâ€‹(p)|â€‹âŸ¶a.s.â€‹0.|U\_{n}(p)-V\_{n}(p)|\overset{a.s.}{\longrightarrow}0. |  | (4.4) |

Now, ([4.2](https://arxiv.org/html/2511.04784v1#S4.E2 "In 4. Convergence of Tail-Based Statistics")) and ([4.4](https://arxiv.org/html/2511.04784v1#S4.E4 "In 4. Convergence of Tail-Based Statistics")) prove the Theorem since the intersection of two events, each with probability 1, also has probability 1.
âˆ

###### Corollary 4.2.

By the assumptions and notations of the Lemma [4.1](https://arxiv.org/html/2511.04784v1#S4.Thmdfn1 "Lemma 4.1. â€£ 4. Convergence of Tail-Based Statistics") and its proof, for nâ†’âˆn\to\infty we have
Unâ€‹(p)=1nâ€‹âˆ‘i=1nXiâ€‹ğŸ™{Xiâ‰¥Qnâ€‹(p)}U\_{n}(p)=\frac{1}{n}\sum\_{i=1}^{n}X\_{i}\mathds{1}\_{\{X\_{i}\geq Q\_{n}(p)\}}
is almost surely (a.s.) close to the process
Vnâ€‹(p)=1nâ€‹âˆ‘i=1nXiâ€‹ğŸ™{Xiâ‰¥qp}V\_{n}(p)=\frac{1}{n}\sum\_{i=1}^{n}X\_{i}\mathds{1}\_{\{X\_{i}\geq q\_{p}\}}.

Next, we note

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î›nâ€‹(p)=1nâ€‹âˆ‘i=1nXiâ€‹ğŸ™{Xiâ‰¥Qnâ€‹(p)}1nâ€‹âˆ‘i=1nXi,\Lambda\_{n}(p)=\frac{\frac{1}{n}\sum\_{i=1}^{n}X\_{i}\mathds{1}\_{\{X\_{i}\geq Q\_{n}(p)\}}}{\frac{1}{n}\sum\_{i=1}^{n}X\_{i}}, |  | (4.5) |

and so, we have the following theorem as a straightforward consequence of the Lemma [4.1](https://arxiv.org/html/2511.04784v1#S4.Thmdfn1 "Lemma 4.1. â€£ 4. Convergence of Tail-Based Statistics") and the strong LLN result that
1nâ€‹âˆ‘i=1nXiâ€‹âŸ¶a.s.â€‹Î¼\frac{1}{n}\sum\_{i=1}^{n}X\_{i}\overset{a.s.}{\longrightarrow}\mu
.

###### Theorem 4.3.

Let X1,X2,â€¦,XnX\_{1},X\_{2},\ldots,X\_{n} be i.i.d random variables with common absolutely continuous distribution FF, and Î¼=ğ”¼â€‹[X1]â‰ 0\mu=\mathbb{E}[X\_{1}]\neq 0. Then, for all pâˆˆ(0,1)p\in(0,1) that FF is continuous at its ppth quantile qpq\_{p}, we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î›nâ€‹(p)â€‹âŸ¶a.s.â€‹aqpÎ¼,\Lambda\_{n}(p)\overset{a.s.}{\longrightarrow}\frac{a\_{q\_{p}}}{\mu}, |  | (4.6) |

where aqp=ğ”¼â€‹[X1â€‹â€‰1{X1â‰¥qp}]a\_{q\_{p}}=\mathbb{E}[X\_{1}\,\mathds{1}\_{\{X\_{1}\geq q\_{p}\}}].

## 5. Asymptotic Distribution of Numerator

Considering the explicit form of Î›nâ€‹(p)\Lambda\_{n}(p) given by equation ([4.5](https://arxiv.org/html/2511.04784v1#S4.E5 "In 4. Convergence of Tail-Based Statistics")), it has a ratio distribution for large nâ†’âˆn\to\infty. If {Xi}iâ‰¥1\{X\_{i}\}\_{i\geq 1} are i.i.d with ğ”¼â€‹[Xi]=Î¼,ğ•â€‹arâ€‹[Xi]=Ïƒ2\mathbb{E}[X\_{i}]=\mu,\mathbb{V}\mathrm{ar}[X\_{i}]=\sigma^{2}, Then, by the central limit theorem (CLT), the denominator of the fraction converges to a normally distributed random variable. That is, for nâ†’âˆn\to\infty

|  |  |  |
| --- | --- | --- |
|  | Zn=1nâ€‹âˆ‘i=1nXiâˆ¼ğ’©â€‹(Î¼,Ïƒ2/n).Z\_{n}=\frac{1}{n}\sum\_{i=1}^{n}X\_{i}\sim\mathscr{N}(\mu,\sigma^{2}/n). |  |

On the other hand, as {Xiâ€‹â€‰1{Xiâ‰¥Qnâ€‹(p)}}iâ‰¥1\{X\_{i}\,\mathds{1}\_{\{X\_{i}\geq Q\_{n}(p)\}}\}\_{i\geq 1} are not independent random variables, the CLT is not applicable for the asymptotic distribution of the numerator of the fraction
Unâ€‹(p)U\_{n}(p),
even though it converges almost surely by the Lemma [4.1](https://arxiv.org/html/2511.04784v1#S4.Thmdfn1 "Lemma 4.1. â€£ 4. Convergence of Tail-Based Statistics"). Considering the literature on ratio distributions and the multiple integral involved in the explicit form of the exact distribution functions, ([(i)](https://arxiv.org/html/2511.04784v1#S3.Ex1 "item (i) â€£ Proposition 3.1. â€£ 3. Exact Distribution of Tail-Based Statistics")) and ([(ii)](https://arxiv.org/html/2511.04784v1#S3.Ex2 "item (ii) â€£ Proposition 3.1. â€£ 3. Exact Distribution of Tail-Based Statistics")), a close form of the Î›nâ€‹(p)\Lambda\_{n}(p) distribution is so complicated (case-dependent) to characterize in general for large nâ†’âˆn\to\infty.

To overcome these difficulties, the asymptotic distribution of Unâ€‹(p)U\_{n}(p) is required. To this, we apply the asymptotic distribution of Qnâ€‹(p)Q\_{n}(p) and the law of total probability. The asymptotic normality of the distribution of Qnâ€‹(p)Q\_{n}(p) for nâ†’âˆn\to\infty was investigated by [[51](https://arxiv.org/html/2511.04784v1#bib.bib51), [6](https://arxiv.org/html/2511.04784v1#bib.bib6)]

|  |  |  |  |
| --- | --- | --- | --- |
|  | Qnâ€‹(p)âˆ¼ğ’©â€‹(qp,pâ€‹(1âˆ’p)nâ€‹f2â€‹(qp)),nâ†’âˆ.Q\_{n}(p)\sim\mathscr{N}\left(q\_{p}\,,\,\frac{p(1-p)}{nf^{2}(q\_{p})}\right),\quad n\to\infty. |  | (5.1) |

So, applying this distribution, one can see

|  |  |  |
| --- | --- | --- |
|  | fQnâ€‹(p)â€‹(q)=eâˆ’nâ€‹f2â€‹(qp)2â€‹pâ€‹(1âˆ’p)â€‹(qâˆ’qp)22â€‹Ï€â€‹pâ€‹(1âˆ’p)nâ€‹f2â€‹(qp)â€‹âŸ¶nâ†’âˆâ€‹Î´qpâ€‹(q).f\_{Q\_{n}(p)}(q)\;=\;\frac{e^{-\frac{nf^{2}(q\_{p})}{2p(1-p)}(q-q\_{p})^{2}}}{\sqrt{\frac{2\pi p(1-p)}{nf^{2}(q\_{p})}}}\;\underset{n\to\infty}{\longrightarrow}\;\delta\_{q\_{p}}(q). |  |

While there are plenty studies for the ratio distributions of two Gaussian processes, the literatures for those ratios that numerator or denumerator are non-Gaussian are not that rich and also show sevear difficulties to have an explicit form of those ratio distribution. Then, a very straight forward question one may ask that is:

â€œDoes UnU\_{n} have an asymptotic normality in distribution?â€

The following theorem reveals a positive response, and the fact behind it.

###### Theorem 5.1.

Let X1,X2,â€¦,XnX\_{1},X\_{2},\ldots,X\_{n} be i.i.d square integrable random variables, i.e., ğ”¼â€‹[X12]<âˆ\mathbb{E}[X\_{1}^{2}]<\infty, with common distribution FF continuous at qpq\_{p}. Then, for nâ†’âˆn\to\infty, the process UnU\_{n} admits the asymptotic normal distribution

|  |  |  |  |
| --- | --- | --- | --- |
|  | Unâ€‹(p)âˆ¼ğ’©â€‹(aqp,((bqp+)2+(bqpâˆ’)2+2â€‹aqp+â€‹aqpâˆ’)/n),U\_{n}(p)\,\sim\,\mathscr{N}\Big(a\_{q\_{p}}\,,\,\Big((b^{+}\_{q\_{p}})^{2}+(b^{-}\_{q\_{p}})^{2}+2a^{+}\_{q\_{p}}a^{-}\_{q\_{p}}\Big)\Big/n\Big), |  | (5.2) |

where aqp+,bqp+a^{+}\_{q\_{p}},b^{+}\_{q\_{p}} are the expectation and standard deviation of Xi+â€‹ğŸ™{Xiâ‰¥qp}X\_{i}^{+}\mathds{1}\_{\{X\_{i}\geq q\_{p}\}}, and aqpâˆ’,bqpâˆ’a^{-}\_{q\_{p}},b^{-}\_{q\_{p}} are the expectation and standard deviation of Xiâˆ’â€‹ğŸ™{Xiâ‰¥qp}X\_{i}^{-}\mathds{1}\_{\{X\_{i}\geq q\_{p}\}}.

###### Proof.

First, considering ([5.1](https://arxiv.org/html/2511.04784v1#S5.E1 "In 5. Asymptotic Distribution of Numerator")), for nâ†’âˆn\to\infty there are some rn>0r\_{n}>0 that rnâ†’0r\_{n}\to 0 and

|  |  |  |
| --- | --- | --- |
|  | â„™â€‹[QnâˆˆBrnâ€‹(qp)]â‰ˆ1.\mathbb{P}[Q\_{n}\in B\_{r\_{n}}(q\_{p})]\approx 1. |  |

So, for nâ†’âˆn\to\infty almost surely

|  |  |  |
| --- | --- | --- |
|  | qpâˆ’rnâ‰¤Qnâ‰¤qp+rn,q\_{p}-r\_{n}\leq Q\_{n}\leq q\_{p}+r\_{n}, |  |

and so,

|  |  |  |
| --- | --- | --- |
|  | ğŸ™{Xiâ‰¥qp+rn}â‰¤ğŸ™{Xiâ‰¥Qn}â‰¤ğŸ™{Xiâ‰¥qpâˆ’rn}.\mathds{1}\_{\{X\_{i}\geq q\_{p}+r\_{n}\}}\leq\mathds{1}\_{\{X\_{i}\geq Q\_{n}\}}\leq\mathds{1}\_{\{X\_{i}\geq q\_{p}-r\_{n}\}}. |  |

Next, we have Xi=Xi+âˆ’Xiâˆ’X\_{i}=X\_{i}^{+}-X\_{i}^{-} where Xi+=maxâ¡{Xi,0}X\_{i}^{+}=\max\{X\_{i},0\} and Xiâˆ’=maxâ¡{âˆ’Xi,0}X\_{i}^{-}=\max\{-X\_{i},0\}, and also

|  |  |  |
| --- | --- | --- |
|  | Xi+â€‹ğŸ™{Xiâ‰¥qp+rn}â‰¤Xi+â€‹ğŸ™{Xiâ‰¥Qn}â‰¤Xi+â€‹ğŸ™{Xiâ‰¥qpâˆ’rn},\displaystyle X\_{i}^{+}\mathds{1}\_{\{X\_{i}\geq q\_{p}+r\_{n}\}}\leq X\_{i}^{+}\mathds{1}\_{\{X\_{i}\geq Q\_{n}\}}\leq X\_{i}^{+}\mathds{1}\_{\{X\_{i}\geq q\_{p}-r\_{n}\}}, |  |
|  |  |  |
| --- | --- | --- |
|  | Xiâˆ’â€‹ğŸ™{Xiâ‰¥qp+rn}â‰¤Xiâˆ’â€‹ğŸ™{Xiâ‰¥Qn}â‰¤Xiâˆ’â€‹ğŸ™{Xiâ‰¥qpâˆ’rn}.\displaystyle X\_{i}^{-}\mathds{1}\_{\{X\_{i}\geq q\_{p}+r\_{n}\}}\leq X\_{i}^{-}\mathds{1}\_{\{X\_{i}\geq Q\_{n}\}}\leq X\_{i}^{-}\mathds{1}\_{\{X\_{i}\geq q\_{p}-r\_{n}\}}. |  |

Hence,

|  |  |  |
| --- | --- | --- |
|  | Xi+â€‹ğŸ™{Xiâ‰¥qp+rn}âˆ’Xiâˆ’â€‹ğŸ™{Xiâ‰¥qpâˆ’rn}\displaystyle X\_{i}^{+}\mathds{1}\_{\{X\_{i}\geq q\_{p}+r\_{n}\}}-X\_{i}^{-}\mathds{1}\_{\{X\_{i}\geq q\_{p}-r\_{n}\}} |  |
|  |  |  |
| --- | --- | --- |
|  | â‰¤Xiâ€‹ğŸ™{Xiâ‰¥Qn}=(Xi+âˆ’Xiâˆ’)â€‹ğŸ™{Xiâ‰¥Qn}\displaystyle\leq X\_{i}\mathds{1}\_{\{X\_{i}\geq Q\_{n}\}}=(X\_{i}^{+}-X\_{i}^{-})\mathds{1}\_{\{X\_{i}\geq Q\_{n}\}} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤Xi+â€‹ğŸ™{Xiâ‰¥qpâˆ’rn}âˆ’Xiâˆ’â€‹ğŸ™{Xiâ‰¥qp+rn},\displaystyle\leq X\_{i}^{+}\mathds{1}\_{\{X\_{i}\geq q\_{p}-r\_{n}\}}-X\_{i}^{-}\mathds{1}\_{\{X\_{i}\geq q\_{p}+r\_{n}\}}, |  | (5.3) |

and so,

|  |  |  |
| --- | --- | --- |
|  | W++â€‹(n)âˆ’Wâˆ’âˆ’â€‹(n)â‰¤Unâ‰¤Wâˆ’+â€‹(n)âˆ’W+âˆ’â€‹(n),W^{+}\_{+}(n)-W^{-}\_{-}(n)\leq U\_{n}\leq W^{+}\_{-}(n)-W^{-}\_{+}(n), |  |

where

|  |  |  |  |
| --- | --- | --- | --- |
|  | W++â€‹(n)\displaystyle W^{+}\_{+}(n) | =1nâ€‹âˆ‘i=1nXi+â€‹ğŸ™{Xiâ‰¥qp+rn},\displaystyle=\frac{1}{n}\sum\_{i=1}^{n}X\_{i}^{+}\mathds{1}\_{\{X\_{i}\geq q\_{p}+r\_{n}\}}, |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | Wâˆ’+â€‹(n)\displaystyle W^{+}\_{-}(n) | =1nâ€‹âˆ‘i=1nXi+â€‹ğŸ™{Xiâ‰¥qpâˆ’rn},\displaystyle=\frac{1}{n}\sum\_{i=1}^{n}X\_{i}^{+}\mathds{1}\_{\{X\_{i}\geq q\_{p}-r\_{n}\}}, |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | W+âˆ’â€‹(n)\displaystyle W^{-}\_{+}(n) | =1nâ€‹âˆ‘i=1nXiâˆ’â€‹ğŸ™{Xiâ‰¥qp+rn},\displaystyle=\frac{1}{n}\sum\_{i=1}^{n}X\_{i}^{-}\mathds{1}\_{\{X\_{i}\geq q\_{p}+r\_{n}\}}, |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | Wâˆ’âˆ’â€‹(n)\displaystyle W^{-}\_{-}(n) | =1nâ€‹âˆ‘i=1nXiâˆ’â€‹ğŸ™{Xiâ‰¥qpâˆ’rn},\displaystyle=\frac{1}{n}\sum\_{i=1}^{n}X\_{i}^{-}\mathds{1}\_{\{X\_{i}\geq q\_{p}-r\_{n}\}}, |  |

are all Gaussian processes. So, the processes Wn:=W++â€‹(n)âˆ’Wâˆ’âˆ’â€‹(n)W\_{n}:=W^{+}\_{+}(n)-W^{-}\_{-}(n) and Vn:=Wâˆ’+â€‹(n)âˆ’W+âˆ’â€‹(n)V\_{n}:=W^{+}\_{-}(n)-W^{-}\_{+}(n) are also Gaussian and

|  |  |  |  |
| --- | --- | --- | --- |
|  | Wnâ‰¤Unâ‰¤Vn.W\_{n}\leq U\_{n}\leq V\_{n}. |  | (5.4) |

Now,

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â€‹[Wn]\displaystyle\mathbb{E}[W\_{n}] | =ğ”¼[X1+ğŸ™{X1â‰¥qp+rn}]âˆ’ğ”¼[X1âˆ’ğŸ™{X1â‰¥qpâˆ’rn}]=:a++(n,qp)âˆ’aâˆ’âˆ’(n,qp),\displaystyle=\mathbb{E}[X\_{1}^{+}\mathds{1}\_{\{X\_{1}\geq q\_{p}+r\_{n}\}}]-\mathbb{E}[X\_{1}^{-}\mathds{1}\_{\{X\_{1}\geq q\_{p}-r\_{n}\}}]=:a^{+}\_{+}(n,q\_{p})-a^{-}\_{-}(n,q\_{p}), |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â€‹[Vn]\displaystyle\mathbb{E}[V\_{n}] | =ğ”¼[X1+ğŸ™{X1â‰¥qpâˆ’rn}]âˆ’ğ”¼[X1âˆ’ğŸ™{X1â‰¥qp+rn}]=:aâˆ’+(n,qp)âˆ’a+âˆ’(n,qp),\displaystyle=\mathbb{E}[X\_{1}^{+}\mathds{1}\_{\{X\_{1}\geq q\_{p}-r\_{n}\}}]-\mathbb{E}[X\_{1}^{-}\mathds{1}\_{\{X\_{1}\geq q\_{p}+r\_{n}\}}]=:a^{+}\_{-}(n,q\_{p})-a^{-}\_{+}(n,q\_{p}), |  |

and

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ•â€‹arâ€‹[Wn]\displaystyle\mathbb{V}\mathrm{ar}[W\_{n}] | =1n(ğ•ar[X1+ğŸ™{X1â‰¥qp+rn}]+ğ•ar[X1âˆ’ğŸ™{X1â‰¥qpâˆ’rn}]\displaystyle=\frac{1}{n}\Big(\mathbb{V}\mathrm{ar}[X\_{1}^{+}\mathds{1}\_{\{X\_{1}\geq q\_{p}+r\_{n}\}}]+\mathbb{V}\mathrm{ar}[X\_{1}^{-}\mathds{1}\_{\{X\_{1}\geq q\_{p}-r\_{n}\}}] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | âˆ’2â„‚ov[X1+ğŸ™{X1â‰¥qp+rn},X1âˆ’ğŸ™{X1â‰¥qpâˆ’rn}])\displaystyle\qquad\quad-2\mathbb{C}\mathrm{ov}[X\_{1}^{+}\mathds{1}\_{\{X\_{1}\geq q\_{p}+r\_{n}\}},X\_{1}^{-}\mathds{1}\_{\{X\_{1}\geq q\_{p}-r\_{n}\}}]\Big) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =1nâ€‹((b++)2â€‹(n,qp)+(bâˆ’âˆ’)2â€‹(n,qp)+2â€‹a++â€‹(n,qp)â€‹aâˆ’âˆ’â€‹(n,qp)).\displaystyle=\frac{1}{n}\Big((b^{+}\_{+})^{2}(n,q\_{p})+(b^{-}\_{-})^{2}(n,q\_{p})+2a^{+}\_{+}(n,q\_{p})a^{-}\_{-}(n,q\_{p})\Big). |  |

Similarly,

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ•â€‹arâ€‹[Vn]\displaystyle\mathbb{V}\mathrm{ar}[V\_{n}] | =1n(ğ•ar[X1+ğŸ™{X1â‰¥qpâˆ’rn}]+ğ•ar[X1âˆ’ğŸ™{X1â‰¥qp+rn}]\displaystyle=\frac{1}{n}\Big(\mathbb{V}\mathrm{ar}[X\_{1}^{+}\mathds{1}\_{\{X\_{1}\geq q\_{p}-r\_{n}\}}]+\mathbb{V}\mathrm{ar}[X\_{1}^{-}\mathds{1}\_{\{X\_{1}\geq q\_{p}+r\_{n}\}}] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | âˆ’2â„‚ov[X1+ğŸ™{X1â‰¥qpâˆ’rn},X1âˆ’ğŸ™{X1â‰¥qp+rn}])\displaystyle\qquad\quad-2\mathbb{C}\mathrm{ov}[X\_{1}^{+}\mathds{1}\_{\{X\_{1}\geq q\_{p}-r\_{n}\}},X\_{1}^{-}\mathds{1}\_{\{X\_{1}\geq q\_{p}+r\_{n}\}}]\Big) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =1nâ€‹((bâˆ’+)2â€‹(n,qp)+(b+âˆ’)2â€‹(n,qp)+2â€‹aâˆ’+â€‹(n,qp)â€‹a+âˆ’â€‹(n,qp)).\displaystyle=\frac{1}{n}\Big((b^{+}\_{-})^{2}(n,q\_{p})+(b^{-}\_{+})^{2}(n,q\_{p})+2a^{+}\_{-}(n,q\_{p})a^{-}\_{+}(n,q\_{p})\Big). |  |

Then, for nâ†’âˆn\to\infty

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â€‹[Wn],ğ”¼â€‹[Vn]\displaystyle\mathbb{E}[W\_{n}],\mathbb{E}[V\_{n}] | â‰ˆaqp,\displaystyle\approx a\_{q\_{p}}, |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ•â€‹arâ€‹[Wn],ğ•â€‹arâ€‹[Vn]\displaystyle\mathbb{V}\mathrm{ar}[W\_{n}],\mathbb{V}\mathrm{ar}[V\_{n}] | â‰ˆ1nâ€‹((bqp+)2+(bqpâˆ’)2+2â€‹aqp+â€‹aqpâˆ’).\displaystyle\approx\frac{1}{n}\Big((b^{+}\_{q\_{p}})^{2}+(b^{-}\_{q\_{p}})^{2}+2a^{+}\_{q\_{p}}a^{-}\_{q\_{p}}\Big). |  |

These and ([5.4](https://arxiv.org/html/2511.04784v1#S5.E4 "In 5. Asymptotic Distribution of Numerator")) proves this Theorem.
âˆ

## 6. Asymptotic Distribution of Tail-Based Statistic

The previous section discussed how UnU\_{n} is asymptotically a normal process ğ’©â€‹(Î¼nâ€‹(p),Ïƒn2â€‹(p)/n)\mathscr{N}(\mu\_{n}(p),\sigma^{2}\_{n}(p)/n), that for known large-size sample distributions we have Î¼nâ€‹(p)â‰ˆaqp\mu\_{n}(p)\approx a\_{q\_{p}} and Ïƒn2â€‹(p)â‰ˆ(bqp+)2+(bqpâˆ’)2+2â€‹aqp+â€‹aqpâˆ’\sigma^{2}\_{n}(p)\approx(b^{+}\_{q\_{p}})^{2}+(b^{-}\_{q\_{p}})^{2}+2a^{+}\_{q\_{p}}a^{-}\_{q\_{p}}. Thus, the process Î›n\Lambda\_{n} has a ratio distribution of two correlated, noncentral, normally distributed processes, UnU\_{n} and ZnZ\_{n}. Here, we aim to identify this ratio distribution.
The method used in the following lemma was initiated by [[32](https://arxiv.org/html/2511.04784v1#bib.bib32)] and further developed by [[54](https://arxiv.org/html/2511.04784v1#bib.bib54)], who transformed the variables into a ratio of two uncorrelated normal processes with a constant offset. [[30](https://arxiv.org/html/2511.04784v1#bib.bib30)] showed that these ratios can be â€œalmost Gaussianâ€ under certain restrictions, [[27](https://arxiv.org/html/2511.04784v1#bib.bib27)] provided an exact analysis, and [[48](https://arxiv.org/html/2511.04784v1#bib.bib48)] examined them comprehensively. However, their computational combinations and associated complexities must be taken into account. [[32](https://arxiv.org/html/2511.04784v1#bib.bib32)] also developed exact results for the correlated case. By transforming the variables to be uncorrelated, however, one can simply apply Hinkleyâ€™s formula rather than resorting to more complicated expressions.

###### Lemma 6.1.

Let X1,X2,â€¦,XnX\_{1},X\_{2},\ldots,X\_{n} be i.i.d square integrable random variables, i.e., ğ”¼â€‹[X12]<âˆ\mathbb{E}[X\_{1}^{2}]<\infty, with common absolutely continuous distribution FF. Then, for nâ†’âˆn\to\infty,

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | Cnâ€‹(p)\displaystyle C\_{n}(p) | =â„‚â€‹ovâ€‹[Un,Zn]=cnâ€‹(p)/n,\displaystyle=\mathbb{C}\mathrm{ov}[U\_{n},Z\_{n}]=c\_{n}(p)/n, |  | (6.1) |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | Ïnâ€‹(p)\displaystyle\rho\_{n}(p) | =â„‚â€‹orâ€‹[Un,Zn]=cnâ€‹(p)Ïƒâ‹…Ïƒnâ€‹(p),\displaystyle=\mathbb{C}\mathrm{or}[U\_{n},Z\_{n}]=\frac{c\_{n}(p)}{\sigma\cdot\sigma\_{n}(p)}, |  | (6.2) |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | cnâ€‹(p)\displaystyle c\_{n}(p) | â‰ˆaqp(2)âˆ’Î¼â€‹aqp=bqp2âˆ’a~qpâ€‹aqp,\displaystyle\approx a^{(2)}\_{q\_{p}}-\mu a\_{q\_{p}}=b^{2}\_{q\_{p}}-\tilde{a}\_{q\_{p}}a\_{q\_{p}}, |  | (6.3) |

where aqp(2)=ğ”¼â€‹[Xi2â€‹ğŸ™{Xiâ‰¥qp}]a^{(2)}\_{q\_{p}}=\mathbb{E}[X\_{i}^{2}\mathds{1}\_{\{X\_{i}\geq q\_{p}\}}] and a~qp=Î¼âˆ’aqp=ğ”¼â€‹[Xiâ€‹ğŸ™{Xiâ‰¤qp}]\tilde{a}\_{q\_{p}}=\mu-a\_{q\_{p}}=\mathbb{E}[X\_{i}\mathds{1}\_{\{X\_{i}\leq q\_{p}\}}].

###### Proof.

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„‚â€‹ovâ€‹[Un,Zn]\displaystyle\mathbb{C}\mathrm{ov}[U\_{n},Z\_{n}] | =1n2â€‹â„‚â€‹ovâ€‹[âˆ‘i=1nXiâ€‹ğŸ™{Xiâ‰¥Qn},âˆ‘j=1nXj]\displaystyle=\frac{1}{n^{2}}\mathbb{C}\mathrm{ov}\left[\sum\_{i=1}^{n}X\_{i}\mathds{1}\_{\{X\_{i}\geq Q\_{n}\}},\sum\_{j=1}^{n}X\_{j}\right] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =1n2â€‹âˆ‘i,j=1nâ„‚â€‹ovâ€‹[Xiâ€‹ğŸ™{Xiâ‰¥Qn},Xj].\displaystyle=\frac{1}{n^{2}}\sum\_{i,j=1}^{n}\mathbb{C}\mathrm{ov}[X\_{i}\mathds{1}\_{\{X\_{i}\geq Q\_{n}\}},X\_{j}]. |  |

From ([5.3](https://arxiv.org/html/2511.04784v1#S5.E3 "In 5. Asymptotic Distribution of Numerator")) we have

|  |  |  |
| --- | --- | --- |
|  | Xj+â€‹Xi+â€‹ğŸ™{Xiâ‰¥qp+rn}âˆ’Xj+â€‹Xiâˆ’â€‹ğŸ™{Xiâ‰¥qpâˆ’rn}\displaystyle X\_{j}^{+}X\_{i}^{+}\mathds{1}\_{\{X\_{i}\geq q\_{p}+r\_{n}\}}-X\_{j}^{+}X\_{i}^{-}\mathds{1}\_{\{X\_{i}\geq q\_{p}-r\_{n}\}} |  |
|  |  |  |
| --- | --- | --- |
|  | â‰¤Xj+â€‹Xiâ€‹ğŸ™{Xiâ‰¥Qn}\displaystyle\leq X\_{j}^{+}X\_{i}\mathds{1}\_{\{X\_{i}\geq Q\_{n}\}} |  |
|  |  |  |
| --- | --- | --- |
|  | â‰¤Xj+â€‹Xi+â€‹ğŸ™{Xiâ‰¥qpâˆ’rn}âˆ’Xj+â€‹Xiâˆ’â€‹ğŸ™{Xiâ‰¥qp+rn},\displaystyle\leq X\_{j}^{+}X\_{i}^{+}\mathds{1}\_{\{X\_{i}\geq q\_{p}-r\_{n}\}}-X\_{j}^{+}X\_{i}^{-}\mathds{1}\_{\{X\_{i}\geq q\_{p}+r\_{n}\}}, |  |

and also,

|  |  |  |
| --- | --- | --- |
|  | Xjâˆ’â€‹Xi+â€‹ğŸ™{Xiâ‰¥qp+rn}âˆ’Xjâˆ’â€‹Xiâˆ’â€‹ğŸ™{Xiâ‰¥qpâˆ’rn}\displaystyle X\_{j}^{-}X\_{i}^{+}\mathds{1}\_{\{X\_{i}\geq q\_{p}+r\_{n}\}}-X\_{j}^{-}X\_{i}^{-}\mathds{1}\_{\{X\_{i}\geq q\_{p}-r\_{n}\}} |  |
|  |  |  |
| --- | --- | --- |
|  | â‰¤Xjâˆ’â€‹Xiâ€‹ğŸ™{Xiâ‰¥Qn}\displaystyle\leq X\_{j}^{-}X\_{i}\mathds{1}\_{\{X\_{i}\geq Q\_{n}\}} |  |
|  |  |  |
| --- | --- | --- |
|  | â‰¤Xjâˆ’â€‹Xi+â€‹ğŸ™{Xiâ‰¥qpâˆ’rn}âˆ’Xjâˆ’â€‹Xiâˆ’â€‹ğŸ™{Xiâ‰¥qp+rn}.\displaystyle\leq X\_{j}^{-}X\_{i}^{+}\mathds{1}\_{\{X\_{i}\geq q\_{p}-r\_{n}\}}-X\_{j}^{-}X\_{i}^{-}\mathds{1}\_{\{X\_{i}\geq q\_{p}+r\_{n}\}}. |  |

So,

|  |  |  |
| --- | --- | --- |
|  | Xj+â€‹Xi+â€‹ğŸ™{Xiâ‰¥qp+rn}âˆ’Xj+â€‹Xiâˆ’â€‹ğŸ™{Xiâ‰¥qpâˆ’rn}\displaystyle X\_{j}^{+}X\_{i}^{+}\mathds{1}\_{\{X\_{i}\geq q\_{p}+r\_{n}\}}-X\_{j}^{+}X\_{i}^{-}\mathds{1}\_{\{X\_{i}\geq q\_{p}-r\_{n}\}} |  |
|  |  |  |
| --- | --- | --- |
|  | âˆ’Xjâˆ’â€‹Xi+â€‹ğŸ™{Xiâ‰¥qpâˆ’rn}+Xjâˆ’â€‹Xiâˆ’â€‹ğŸ™{Xiâ‰¥qp+rn}\displaystyle-X\_{j}^{-}X\_{i}^{+}\mathds{1}\_{\{X\_{i}\geq q\_{p}-r\_{n}\}}+X\_{j}^{-}X\_{i}^{-}\mathds{1}\_{\{X\_{i}\geq q\_{p}+r\_{n}\}} |  |
|  |  |  |
| --- | --- | --- |
|  | â‰¤Xjâ€‹Xiâ€‹ğŸ™{Xiâ‰¥Qn}=(Xj+âˆ’Xjâˆ’)â€‹Xiâ€‹ğŸ™{Xiâ‰¥Qn}\displaystyle\leq X\_{j}X\_{i}\mathds{1}\_{\{X\_{i}\geq Q\_{n}\}}=(X\_{j}^{+}-X\_{j}^{-})X\_{i}\mathds{1}\_{\{X\_{i}\geq Q\_{n}\}} |  |
|  |  |  |
| --- | --- | --- |
|  | â‰¤Xj+â€‹Xi+â€‹ğŸ™{Xiâ‰¥qpâˆ’rn}âˆ’Xj+â€‹Xiâˆ’â€‹ğŸ™{Xiâ‰¥qp+rn}\displaystyle\leq X\_{j}^{+}X\_{i}^{+}\mathds{1}\_{\{X\_{i}\geq q\_{p}-r\_{n}\}}-X\_{j}^{+}X\_{i}^{-}\mathds{1}\_{\{X\_{i}\geq q\_{p}+r\_{n}\}} |  |
|  |  |  |
| --- | --- | --- |
|  | âˆ’Xjâˆ’â€‹Xi+â€‹ğŸ™{Xiâ‰¥qp+rn}+Xjâˆ’â€‹Xiâˆ’â€‹ğŸ™{Xiâ‰¥qpâˆ’rn},\displaystyle-X\_{j}^{-}X\_{i}^{+}\mathds{1}\_{\{X\_{i}\geq q\_{p}+r\_{n}\}}+X\_{j}^{-}X\_{i}^{-}\mathds{1}\_{\{X\_{i}\geq q\_{p}-r\_{n}\}}, |  |

and for nâ†’âˆn\to\infty we have

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[Xjâ€‹Xiâ€‹ğŸ™{Xiâ‰¥Qn}]â‰ˆğ”¼â€‹[Xjâ€‹Xiâ€‹ğŸ™{Xiâ‰¥qp}]\displaystyle\mathbb{E}[X\_{j}X\_{i}\mathds{1}\_{\{X\_{i}\geq Q\_{n}\}}]\approx\mathbb{E}[X\_{j}X\_{i}\mathds{1}\_{\{X\_{i}\geq q\_{p}\}}] |  |
|  |  |  |
| --- | --- | --- |
|  | ={ğ”¼â€‹[Xi2â€‹ğŸ™{Xiâ‰¥qp}]=aqp(2)i=jğ”¼â€‹[Xj]â€‹ğ”¼â€‹[Xiâ€‹ğŸ™{Xiâ‰¥qp}]=Î¼â€‹aqpiâ‰¤j.\displaystyle=\begin{cases}\mathbb{E}[X\_{i}^{2}\mathds{1}\_{\{X\_{i}\geq q\_{p}\}}]=a^{(2)}\_{q\_{p}}&i=j\\ \mathbb{E}[X\_{j}]\mathbb{E}[X\_{i}\mathds{1}\_{\{X\_{i}\geq q\_{p}\}}]=\mu a\_{q\_{p}}&i\leq j.\\ \end{cases} |  |

Hence,

|  |  |  |
| --- | --- | --- |
|  | â„‚â€‹ovâ€‹[Xiâ€‹ğŸ™{Xiâ‰¥Qn},Xj]â‰ˆ{aqp(2)âˆ’Î¼â€‹aqpi=j0iâ‰ j,\displaystyle\mathbb{C}\mathrm{ov}[X\_{i}\mathds{1}\_{\{X\_{i}\geq Q\_{n}\}},X\_{j}]\approx\begin{cases}a^{(2)}\_{q\_{p}}-\mu a\_{q\_{p}}&i=j\\ 0&i\neq j,\\ \end{cases} |  |

and so,

|  |  |  |
| --- | --- | --- |
|  | â„‚â€‹ovâ€‹[Un,Zn]=(aqp(2)âˆ’Î¼â€‹aqp)/n.\mathbb{C}\mathrm{ov}[U\_{n},Z\_{n}]=\left(a^{(2)}\_{q\_{p}}-\mu a\_{q\_{p}}\right)\Big/n. |  |

âˆ

###### Lemma 6.2.

For Î¼â‰ 0\mu\neq 0, the process

|  |  |  |
| --- | --- | --- |
|  | Î›~n=Î›nâˆ’cnÏƒ2\widetilde{\Lambda}\_{n}=\Lambda\_{n}-\frac{c\_{n}}{\sigma^{2}} |  |

is a ratio of uncorrelated noncentral Gaussian processes, and has the probability density function

|  |  |  |  |
| --- | --- | --- | --- |
|  | fÎ›~nâ€‹(t)=\displaystyle f\_{\widetilde{\Lambda}\_{n}}(t)= | n2â€‹Ï€â€‹Ïƒ2â€‹Ïƒ~n2â‹…B~â€‹(t)A~3â€‹(t)â‹…expâ¡[âˆ’n2â‹…Î¼2Ïƒ2â‹…(tâˆ’Î¼~n/Î¼)2t2+Ïƒ~n2/Ïƒ2]â€‹erfâ€‹(B~â€‹(t)A~â€‹(t)â€‹n2)\displaystyle\;\sqrt{\frac{n}{2\pi\sigma^{2}\tilde{\sigma}^{2}\_{n}}}\cdot\frac{\widetilde{B}(t)}{\widetilde{A}^{3}(t)}\cdot\exp\left[{-\frac{n}{2}\cdot\frac{\mu^{2}}{\sigma^{2}}\cdot\frac{\left(t-{\tilde{\mu}\_{n}}/{\mu}\right)^{2}}{t^{2}+{\tilde{\sigma}^{2}\_{n}}/{\sigma^{2}}}}\right]\mathrm{erf}\left(\frac{\widetilde{B}(t)}{\widetilde{A}(t)}\sqrt{\frac{n}{2}}\right) |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | +eâˆ’n2â€‹rn2Ï€â€‹Ïƒâ€‹Ïƒ~nâ€‹A~2â€‹(t),\displaystyle+\frac{e^{-\frac{n}{2}r^{2}\_{n}}}{{\pi\sigma\tilde{\sigma}\_{n}}\widetilde{A}^{2}(t)}, |  | (6.4) |

where

|  |  |  |
| --- | --- | --- |
|  | Î¼~n=Î¼nâˆ’Î¼â€‹cn/Ïƒ2,Ïƒ~n=Ïƒn2âˆ’cn2/Ïƒ2,rn2=Î¼~n2/Ïƒ~n2+Î¼2/Ïƒ2\displaystyle\tilde{\mu}\_{n}=\mu\_{n}-\mu c\_{n}/\sigma^{2},\quad\tilde{\sigma}\_{n}=\sqrt{\sigma^{2}\_{n}-c^{2}\_{n}/\sigma^{2}},\quad r^{2}\_{n}=\tilde{\mu}^{2}\_{n}/\tilde{\sigma}^{2}\_{n}+\mu^{2}/\sigma^{2} |  |
|  |  |  |
| --- | --- | --- |
|  | A~â€‹(t)=t2Ïƒ~n2+1Ïƒ2,B~â€‹(t)=Î¼~nÏƒ~n2â€‹t+Î¼Ïƒ2.\displaystyle\widetilde{A}(t)=\sqrt{\frac{t^{2}}{\tilde{\sigma}^{2}\_{n}}+\frac{1}{\sigma^{2}}},\quad\widetilde{B}(t)=\frac{\tilde{\mu}\_{n}}{\tilde{\sigma}\_{n}^{2}}\,t+\frac{\mu}{\sigma^{2}}. |  |

###### Proof.

We have

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î›n=Î¼n+unÎ¼+zn,\Lambda\_{n}=\frac{\mu\_{n}+u\_{n}}{\mu+z\_{n}}, |  | (6.5) |

where unâ€‹(p)u\_{n}(p) and znz\_{n} are corellated central Gaussian variables with variances Ïƒn2/n\sigma\_{n}^{2}/n and Ïƒ2/n\sigma^{2}/n respectively, and correlation Ïn\rho\_{n}. Now, we apply the Geary-Hinkley transformation. Let

|  |  |  |  |
| --- | --- | --- | --- |
|  | u~n\displaystyle\tilde{u}\_{n} | =unâˆ’Ïu,zâ€‹ÏƒuÏƒzâ€‹zn\displaystyle=u\_{n}-\rho\_{u,z}\frac{\sigma\_{u}}{\sigma\_{z}}z\_{n} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =unâˆ’Ïnâ€‹ÏƒnÏƒâ€‹zn\displaystyle=u\_{n}-\rho\_{n}\frac{\sigma\_{n}}{\sigma}z\_{n} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =unâˆ’cnÏƒ2â€‹zn,\displaystyle=u\_{n}-\frac{c\_{n}}{\sigma^{2}}z\_{n}, |  |

then u~n\tilde{u}\_{n} and znz\_{n} are uncorrelated, and by

|  |  |  |
| --- | --- | --- |
|  | ğ”¼[u~n]=Î¼nâˆ’Î¼â‹…cnÏƒ2=:Î¼~n,\displaystyle\mathbb{E}[\tilde{u}\_{n}]=\mu\_{n}-\mu\cdot\frac{c\_{n}}{\sigma^{2}}=:\tilde{\mu}\_{n}, |  |
|  |  |  |
| --- | --- | --- |
|  | ğ•ar[u~n]=(Ïƒn2âˆ’cn2/Ïƒ2)/n=:Ïƒ~n2/n,\displaystyle\mathbb{V}\mathrm{ar}[\tilde{u}\_{n}]=(\sigma^{2}\_{n}-c^{2}\_{n}/\sigma^{2})/n=:\tilde{\sigma}^{2}\_{n}/n, |  |

we have

|  |  |  |
| --- | --- | --- |
|  | Î›n=Î¼~n+u~nÎ¼+zn+cnÏƒ2.\Lambda\_{n}=\frac{\tilde{\mu}\_{n}+\tilde{u}\_{n}}{\mu+z\_{n}}+\frac{c\_{n}}{\sigma^{2}}. |  |

Hence,

|  |  |  |
| --- | --- | --- |
|  | Î›~n=Î›nâˆ’cnÏƒ2=Î¼~n+u~nÎ¼+zn,\widetilde{\Lambda}\_{n}=\Lambda\_{n}-\frac{c\_{n}}{\sigma^{2}}=\frac{\tilde{\mu}\_{n}+\tilde{u}\_{n}}{\mu+z\_{n}}, |  |

is a ratio of uncorrelated noncenteral Gaussian process, and so by [[32](https://arxiv.org/html/2511.04784v1#bib.bib32)] has the probability density function

|  |  |  |  |
| --- | --- | --- | --- |
|  | fÎ›~nâ€‹(t)=nâ€‹expâ¡(âˆ’R2/2)2â€‹Ï€â€‹Ïƒnâ€‹Ïƒâ€‹a2â€‹(t)â€‹[2â€‹Ï€â€‹bâ€‹(t)aâ€‹(t)â€‹expâ¡(b2â€‹(t)2â€‹a2â€‹(t))â€‹erfâ€‹(bâ€‹(t)2â€‹aâ€‹(t))+2],f\_{\widetilde{\Lambda}\_{n}}(t)\;=n\,\frac{\exp(-R^{2}/2)}{2\pi\sigma\_{n}\sigma\,a^{2}(t)}\,\left[\sqrt{2\pi}\,\frac{b(t)}{a(t)}\,\exp\left(\frac{b^{2}(t)}{2a^{2}(t)}\right)\mathrm{erf}\left(\frac{b(t)}{\sqrt{2}a(t)}\right)+2\,\right], |  | (6.6) |

where

|  |  |  |  |
| --- | --- | --- | --- |
|  | R2\displaystyle R^{2} | =nâ€‹(Î¼~n2Ïƒ~n2+Î¼2Ïƒ2)=nâ€‹rn2,\displaystyle=n\left(\frac{\tilde{\mu}^{2}\_{n}}{\tilde{\sigma}^{2}\_{n}}+\frac{\mu^{2}}{\sigma^{2}}\right)=nr^{2}\_{n}, |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | aâ€‹(t)\displaystyle a(t) | =nâ€‹(t2Ïƒ~n2+1Ïƒ2)=nâ€‹A~â€‹(t),\displaystyle=\sqrt{n\left(\frac{t^{2}}{\tilde{\sigma}^{2}\_{n}}+\frac{1}{\sigma^{2}}\right)}=\sqrt{n}\widetilde{A}(t), |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | bâ€‹(t)\displaystyle b(t) | =nâ€‹(Î¼~nÏƒ~n2â€‹t+Î¼Ïƒ2)=nâ€‹B~â€‹(t).\displaystyle=n\left(\frac{\tilde{\mu}\_{n}}{\tilde{\sigma}^{2}\_{n}}\,t+\frac{\mu}{\sigma^{2}}\right)=n\widetilde{B}(t). |  |

Finally, since

|  |  |  |
| --- | --- | --- |
|  | 12â€‹(R2âˆ’b2â€‹(t)a2â€‹(t))=n2â€‹(R2âˆ’B2â€‹(t)A2â€‹(t))=n2â‹…Î¼2Ïƒ2â‹…(tâˆ’Î¼~n/Î¼)2t2+Ïƒ~n2/Ïƒ2,\frac{1}{2}\left(R^{2}-\frac{b^{2}(t)}{a^{2}(t)}\right)=\frac{n}{2}\left(R^{2}-\frac{B^{2}(t)}{A^{2}(t)}\right)=\frac{n}{2}\cdot\frac{\mu^{2}}{\sigma^{2}}\cdot\frac{\left(t-\tilde{\mu}\_{n}/{\mu}\right)^{2}}{t^{2}+{\tilde{\sigma}^{2}\_{n}}/{\sigma^{2}}}, |  |

([6.6](https://arxiv.org/html/2511.04784v1#S6.E6 "In 6. Asymptotic Distribution of Tail-Based Statistic")) results ([6.2](https://arxiv.org/html/2511.04784v1#S6.Ex19 "Lemma 6.2. â€£ 6. Asymptotic Distribution of Tail-Based Statistic")).
âˆ

Compensating the constant offset âˆ’cn/Ïƒ2-c\_{n}/\sigma^{2} of the Lemma [6.2](https://arxiv.org/html/2511.04784v1#S6.Thmdfn2 "Lemma 6.2. â€£ 6. Asymptotic Distribution of Tail-Based Statistic"), by changing variable tâ†¦tâˆ’cn/Ïƒ2t\mapsto t-c\_{n}/\sigma^{2} we have the following theorem.

###### Theorem 6.3.

For Î¼â‰ 0\mu\neq 0 and nâ†’âˆn\to\infty, the (asymptotic) probability density function of Î›n\Lambda\_{n} is

|  |  |  |  |
| --- | --- | --- | --- |
|  | fÎ›nâ€‹(t)=\displaystyle f\_{\Lambda\_{n}}(t)= | n2â€‹Ï€â€‹(Ïƒ2â€‹Ïƒn2âˆ’cn2)â‹…Bâ€‹(t)A3â€‹(t)â€‹erfâ€‹(Bâ€‹(t)Aâ€‹(t)â€‹n2)\displaystyle\;\sqrt{\frac{n}{2\pi(\sigma^{2}\sigma^{2}\_{n}-c\_{n}^{2})}}\cdot\frac{B(t)}{A^{3}(t)}\;\mathrm{erf}\left(\frac{B(t)}{A(t)}\sqrt{\frac{n}{2}}\right) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | Ã—expâ¡[âˆ’n2â‹…Î¼2Ïƒ2â‹…(tâˆ’Î¼n/Î¼)2(tâˆ’Ïƒn/Ïƒ)2+2â€‹tâ€‹(1âˆ’Ïn)â€‹Ïƒn/Ïƒ]\displaystyle\times\exp\left[{-\frac{n}{2}\cdot\frac{\mu^{2}}{\sigma^{2}}\cdot\frac{(t-\mu\_{n}/\mu)^{2}}{(t-\sigma\_{n}/\sigma)^{2}+2t(1-\rho\_{n})\sigma\_{n}/\sigma}}\right] |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | +eâˆ’n2â€‹rn2Ï€â€‹A2â€‹(t)â€‹Ïƒ2â€‹Ïƒn2âˆ’cn2,\displaystyle+\;\frac{e^{-\frac{n}{2}r^{2}\_{n}}}{\pi A^{2}(t)\sqrt{\sigma^{2}\sigma^{2}\_{n}-c^{2}\_{n}}}, |  | (6.7) |

where

|  |  |  |  |
| --- | --- | --- | --- |
|  | Aâ€‹(t)\displaystyle A(t) | =(tâˆ’cn/Ïƒ2)2Ïƒn2âˆ’cn2/Ïƒ2+1Ïƒ2,\displaystyle=\sqrt{\frac{(t-c\_{n}/\sigma^{2})^{2}}{\sigma\_{n}^{2}-c\_{n}^{2}/\sigma^{2}}+\frac{1}{\sigma^{2}}}, |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | Bâ€‹(t)\displaystyle B(t) | =(Î¼nâˆ’cnâ€‹Î¼/Ïƒ2Ïƒn2âˆ’cn2/Ïƒ2)â€‹(tâˆ’cn/Ïƒ2)+Î¼Ïƒ2,\displaystyle=\left(\frac{\mu\_{n}-c\_{n}\mu/\sigma^{2}}{\sigma\_{n}^{2}-c\_{n}^{2}/\sigma^{2}}\right)(t-c\_{n}/\sigma^{2})\,+\,\frac{\mu}{\sigma^{2}}, |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | rn2\displaystyle r^{2}\_{n} | =(Î¼nâˆ’cnâ€‹Î¼/Ïƒ2)2Ïƒn2âˆ’cn2/Ïƒ2+Î¼2Ïƒ2.\displaystyle=\frac{(\mu\_{n}-c\_{n}\mu/\sigma^{2})^{2}}{\sigma\_{n}^{2}-c\_{n}^{2}/\sigma^{2}}+\frac{\mu^{2}}{\sigma^{2}}. |  |

###### Proof.

For all Tâˆˆâ„T\in\mathbb{R}

|  |  |  |  |
| --- | --- | --- | --- |
|  | FÎ›nâ€‹(T)\displaystyle F\_{\Lambda\_{n}}(T) | =â„™â€‹[Î›nâ‰¤T]\displaystyle=\mathbb{P}[\Lambda\_{n}\leq T] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =â„™â€‹[Î›~nâ‰¤Tâˆ’cn/Ïƒ2]\displaystyle=\mathbb{P}[\widetilde{\Lambda}\_{n}\leq T-c\_{n}/\sigma^{2}] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =âˆ«âˆ’âˆTâˆ’cn/Ïƒ2fÎ›~nâ€‹(u)â€‹ğ‘‘u\displaystyle=\int\_{-\infty}^{T-c\_{n}/\sigma^{2}}f\_{\widetilde{\Lambda}\_{n}}(u)\,du |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =âˆ«âˆ’âˆTfÎ›~nâ€‹(tâˆ’cn/Ïƒ2)â€‹ğ‘‘t,\displaystyle=\int\_{-\infty}^{T}f\_{\widetilde{\Lambda}\_{n}}(t-c\_{n}/\sigma^{2})\,dt, |  |

and so,

|  |  |  |
| --- | --- | --- |
|  | fÎ›nâ€‹(t)=fÎ›~nâ€‹(tâˆ’cn/Ïƒ2),f\_{\Lambda\_{n}}(t)=f\_{\widetilde{\Lambda}\_{n}}(t-c\_{n}/\sigma^{2}), |  |

where fÎ›~nf\_{\widetilde{\Lambda}\_{n}} is given by Lemma [6.2](https://arxiv.org/html/2511.04784v1#S6.Thmdfn2 "Lemma 6.2. â€£ 6. Asymptotic Distribution of Tail-Based Statistic"). Now, one may note

|  |  |  |
| --- | --- | --- |
|  | (tâˆ’cn/Ïƒ2âˆ’Î¼~n/Î¼)2(tâˆ’cn/Ïƒ2)2+Ïƒ~n2/Ïƒ2=(tâˆ’Î¼n/Î¼)2(tâˆ’Ïƒn/Ïƒ)2+2â€‹tâ€‹(1âˆ’Ïn)â€‹Ïƒn/Ïƒ.\frac{(t-c\_{n}/\sigma^{2}-\tilde{\mu}\_{n}/\mu)^{2}}{(t-c\_{n}/\sigma^{2})^{2}+\tilde{\sigma}^{2}\_{n}/\sigma^{2}}=\frac{(t-\mu\_{n}/\mu)^{2}}{(t-\sigma\_{n}/\sigma)^{2}+2t(1-\rho\_{n})\sigma\_{n}/\sigma}. |  |

âˆ

There is another approach to investigate the ratio distribution provided by Katz (1978) distribution approximation [[34](https://arxiv.org/html/2511.04784v1#bib.bib34)]. Here, we formulate it for Î›n\Lambda\_{n} by the following proposition.

###### Proposition 6.4.

For Î¼â‰ 0\mu\neq 0 and nâ†’âˆn\to\infty, the process Î›n\Lambda\_{n} admits the approximatly logarithmic Gaussian distribution

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î›nâˆ¼Î¼nÎ¼â‹…â„’â€‹oâ€‹gâ€‹ğ’©â€‹oâ€‹râ€‹mâ€‹aâ€‹lâ€‹(0,Ïƒ2Î¼2+Ïƒn2Î¼n2âˆ’2â€‹cnÎ¼nâ€‹Î¼n).\Lambda\_{n}\sim\frac{\mu\_{n}}{\mu}\cdot\mathscr{L}\!{og}\mathscr{N}\!{ormal}\Bigg(0,\frac{\frac{\sigma^{2}}{\mu^{2}}+\frac{\sigma\_{n}^{2}}{\mu\_{n}^{2}}-\frac{2c\_{n}}{\mu\_{n}\mu}}{n}\Bigg). |  | (6.8) |

###### Proof.

From ([6.5](https://arxiv.org/html/2511.04784v1#S6.E5 "In 6. Asymptotic Distribution of Tail-Based Statistic")) we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î›n=Î¼nÎ¼â‹…1+un/Î¼n1+zn/Î¼,\Lambda\_{n}=\frac{\mu\_{n}}{\mu}\cdot\frac{1+u\_{n}/\mu\_{n}}{1+z\_{n}/\mu}, |  | (6.9) |

where unâˆ¼ğ’©â€‹(0,Ïƒn2/n)u\_{n}\sim\mathscr{N}(0,\sigma\_{n}^{2}/n) and znâˆ¼ğ’©â€‹(0,Ïƒ2/n)z\_{n}\sim\mathscr{N}(0,\sigma^{2}/n). Taking the logarithm of ([6.9](https://arxiv.org/html/2511.04784v1#S6.E9 "In 6. Asymptotic Distribution of Tail-Based Statistic")), we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | logâ¡Î›n=logâ¡(Î¼nÎ¼)+logâ¡(1+unÎ¼n)âˆ’logâ¡(1+znÎ¼).\log\Lambda\_{n}=\log\left(\frac{\mu\_{n}}{\mu}\right)+\log\left(1+\frac{u\_{n}}{\mu\_{n}}\right)-\log\left(1+\frac{z\_{n}}{\mu}\right). |  | (6.10) |

Here, we apply the logarithmic power series, covergent on |x|<1|x|<1,

|  |  |  |
| --- | --- | --- |
|  | logâ¡(1+x)=âˆ‘k=0âˆ(âˆ’1)kâ€‹xk+1k+1=xâˆ’x22+x33âˆ’â‹¯\log(1+x)=\sum\_{k=0}^{\infty}(-1)^{k}\frac{x^{k+1}}{k+1}=x-\frac{x^{2}}{2}+\frac{x^{3}}{3}-\cdots |  |

to approximate the two final part of the right hand side of ([6.10](https://arxiv.org/html/2511.04784v1#S6.E10 "In 6. Asymptotic Distribution of Tail-Based Statistic")). By the first power k=1k=1, for nâ†’âˆn\to\infty we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | logâ¡Î›n\displaystyle\log\Lambda\_{n} | â‰ˆlogâ¡(Î¼nÎ¼)+unÎ¼nâˆ’znÎ¼\displaystyle\approx\log\left(\frac{\mu\_{n}}{\mu}\right)+\frac{u\_{n}}{\mu\_{n}}-\frac{z\_{n}}{\mu} |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | âˆ¼logâ¡(Î¼nÎ¼)+ğ’©â€‹(0,Ïƒ2Î¼2+Ïƒn2Î¼n2âˆ’2â€‹cnÎ¼nâ€‹Î¼n),\displaystyle\sim\log\left(\frac{\mu\_{n}}{\mu}\right)+\mathscr{N}\Bigg(0,\frac{\frac{\sigma^{2}}{\mu^{2}}+\frac{\sigma\_{n}^{2}}{\mu\_{n}^{2}}-\frac{2c\_{n}}{\mu\_{n}\mu}}{n}\Bigg), |  | (6.11) |

or equivalently

|  |  |  |
| --- | --- | --- |
|  | Î›nâ‰ˆÎ¼nÎ¼â‹…expâ¡(unÎ¼nâˆ’znÎ¼),\Lambda\_{n}\approx\frac{\mu\_{n}}{\mu}\cdot\exp\left(\frac{u\_{n}}{\mu\_{n}}-\frac{z\_{n}}{\mu}\right), |  |

and this proves ([6.8](https://arxiv.org/html/2511.04784v1#S6.E8 "In Proposition 6.4. â€£ 6. Asymptotic Distribution of Tail-Based Statistic")).
âˆ

###### Remark 6.5.

The asymptotic ratio distributions ([6.7](https://arxiv.org/html/2511.04784v1#S6.E7 "In Theorem 6.3. â€£ 6. Asymptotic Distribution of Tail-Based Statistic")) and ([6.8](https://arxiv.org/html/2511.04784v1#S6.E8 "In Proposition 6.4. â€£ 6. Asymptotic Distribution of Tail-Based Statistic")) are indeed usefull when the sample distribution is unknown. However, if the sample distribution is known, then by Theorem [4.3](https://arxiv.org/html/2511.04784v1#S4.Thmdfn3 "Theorem 4.3. â€£ 4. Convergence of Tail-Based Statistics"), Theorem [5.1](https://arxiv.org/html/2511.04784v1#S5.Thmdfn1 "Theorem 5.1. â€£ 5. Asymptotic Distribution of Numerator"), and Lemma [6.1](https://arxiv.org/html/2511.04784v1#S6.Thmdfn1 "Lemma 6.1. â€£ 6. Asymptotic Distribution of Tail-Based Statistic") for nâ†’âˆn\to\infty we can apply

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î¼n\displaystyle\mu\_{n} | â‰ˆaqp,\displaystyle\approx a\_{q\_{p}}, |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | Ïƒn2\displaystyle\sigma^{2}\_{n} | â‰ˆ(bqp+)2+(bqpâˆ’)2+2â€‹aqp+â€‹aqpâˆ’,\displaystyle\approx(b^{+}\_{q\_{p}})^{2}+(b^{-}\_{q\_{p}})^{2}+2a^{+}\_{q\_{p}}a^{-}\_{q\_{p}}, |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | cn\displaystyle c\_{n} | â‰ˆbqp2âˆ’a~qpâ€‹aqp,\displaystyle\approx b^{2}\_{q\_{p}}-\tilde{a}\_{q\_{p}}a\_{q\_{p}}, |  |

in ([6.7](https://arxiv.org/html/2511.04784v1#S6.E7 "In Theorem 6.3. â€£ 6. Asymptotic Distribution of Tail-Based Statistic")), and also ([6.8](https://arxiv.org/html/2511.04784v1#S6.E8 "In Proposition 6.4. â€£ 6. Asymptotic Distribution of Tail-Based Statistic")) returns

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î›nâˆ¼aqpÎ¼â‹…â„’â€‹oâ€‹gâ€‹ğ’©â€‹oâ€‹râ€‹mâ€‹aâ€‹lâ€‹(0,Ïƒ2Î¼2+(bqp+)2+(bqpâˆ’)2+2â€‹aqp+â€‹aqpâˆ’aqp2âˆ’2â€‹bqp2âˆ’2â€‹a~qpâ€‹aqpÎ¼â€‹aqpn).\displaystyle\Lambda\_{n}\sim\frac{a\_{q\_{p}}}{\mu}\cdot\mathscr{L}\!{og}\mathscr{N}\!{ormal}\left(0,\frac{\frac{\sigma^{2}}{\mu^{2}}+\frac{(b^{+}\_{q\_{p}})^{2}+(b^{-}\_{q\_{p}})^{2}+2a^{+}\_{q\_{p}}a^{-}\_{q\_{p}}}{a^{2}\_{q\_{p}}}-\frac{2b^{2}\_{q\_{p}}-2\tilde{a}\_{q\_{p}}a\_{q\_{p}}}{\mu a\_{q\_{p}}}}{n}\right). |  | (6.12) |

## 7. Simulation and Callibration

In this section, we conduct Monte Carlo simulations of the distribution ([6.8](https://arxiv.org/html/2511.04784v1#S6.E8 "In Proposition 6.4. â€£ 6. Asymptotic Distribution of Tail-Based Statistic")) with p=80%p=80\% for n=1000n=1000 i.i.d variables, and N=105N=10^{5} replications, with variablesâ€™ common (continuous) distributions

|  |  |  |
| --- | --- | --- |
|  | ğ™½ğš˜ğš›ğš–ğšŠğš•â€‹(Î¼,ÏƒğŸ¸),ğ™»ğš˜ğšğš—ğš˜ğš›ğš–ğšŠğš•â€‹(Î¼,ÏƒğŸ¸),ğ™´ğš¡ğš™ğš˜ğš—ğšğš—ğšğš’ğšŠğš•â€‹(Î¼),\displaystyle\mathtt{Normal(\mu,\sigma^{2}),Lognormal(\mu,\sigma^{2}),Exponential(\mu),} |  |
|  |  |  |
| --- | --- | --- |
|  | ğšğšŠğš’ğš•ğšğš’ğšğš‘â€‹(ğš‹),ğ™¶ğšğš—ğšğš›ğšŠğš•ğš’ğš£ğšğšğ™¿ğšŠğš›ğšğšğš˜â€‹(ğš”,ğšœ,Î¸),ğ™¶ğšŠğš–ğš–ğšŠâ€‹(Î±,Î¸),\displaystyle\mathtt{Raileigh(b),GeneralizedPareto(k,s,\theta),Gamma(\alpha,\theta),} |  |

where Î¼=Î¸=1,Ïƒ=k=b=s=0.25,Î±=3\mu=\theta=1,\sigma=k=b=s=0.25,\alpha=3, and their practical estimated distribution. To enable a clear comparison between the log-normal formulated density function and the empirically estimated density function, both are plotted on the histogram of the Î›n\Lambda\_{n} statistics in Figure [1](https://arxiv.org/html/2511.04784v1#S7.F1 "Figure 1 â€£ 7. Simulation and Callibration"). One can easily observe how closely they approximate the actual density of this statistic, although, due to the sample size and the numbers nn and NN, there are always some differences between the formulated and estimated distributions. The accumulated area between the two curves (which represents the difference in cumulative probability) is reported in Table [1](https://arxiv.org/html/2511.04784v1#S7.T1 "Table 1 â€£ 7. Simulation and Callibration").

![Refer to caption](x1.png)


Figure 1. The histogram, analytical log-normal probability density function, and estimated probability density function of Î›n\Lambda\_{n} for i.i.d variables from different continuous distributions.



| Distribution | Area between PDFs |
| --- | --- |
| Normal | 0.0713 |
| LogNormal | 0.0708 |
| Exponential | 0.0662 |
| Rayleigh | 0.0687 |
| Generalized Pareto | 0.0949 |
| Gamma | 0.0709 |

Table 1. Cumulative area between analytic and estimated PDFs for different variable distributions.

## Acknowledgement

The concept of quantile contributions was originally introduced to me by Tommi Sottinen and Klaus Grobys in the context of another study on applied statistics in economics, sincerely grateful to them for their valuable insights and inspiration. Nevertheless, the present work is an independent analytical study, and all parts of the research, analysis, and writing have been carried out solely by the author.

## References

* [1]

  10/90 gap, vol.Â from the original on January 21, 2021. Retrieved April
  16, 2015., Global Forum for Health Research (Organization), Archive of Global
  Forum for Health Research, 2011.
* [2]

  B.Â C. Arnold and N.Â Balakrishnan, Relations, bounds and
  approximations for order statistics, vol.Â 53, Springer Science & Business
  Media, 2012.
* [3]

  B.Â C. Arnold, N.Â Balakrishnan, and H.Â N. Nagaraja, A first course in
  order statistics, SIAM, 2008.
* [4]

  S.Â Asmussen, Steady-state properties of GI/G/1, Applied
  probability and Queues, (2003), pp.Â 266â€“301.
* [5]

  R.Â L. Axtell, Zipf distribution of us firm sizes, science, 293
  (2001), pp.Â 1818â€“1820.
* [6]

  R.Â R. Bahadur, A note on quantiles in large samples, The Annals of
  Mathematical Statistics, 37 (1966), pp.Â 577â€“580.
* [7]

  A.-L. Barabasi and Z.Â N. Oltvai, Network biology: understanding the
  cellâ€™s functional organization, Nature reviews genetics, 5 (2004),
  pp.Â 101â€“113.
* [8]

  P.Â Barford, J.Â Kline, D.Â Plonka, and A.Â Ron, A signal analysis of
  network traffic anomalies, in Proceedings of the 2nd ACM SIGCOMM Workshop on
  Internet measurment, 2002, pp.Â 71â€“82.
* [9]

  Z.Â P. BaÅ¾ant, Scaling theory for quasibrittle structural
  failure, Proceedings of the National Academy of Sciences, 101 (2004),
  pp.Â 13400â€“13407.
* [10]

  J.Â Bentley, Programmimg pearls, Communications of the ACM, 28
  (1985), pp.Â 896â€“901.
* [11]

  G.Â BuzsÃ¡ki and K.Â Mizuseki, The log-dynamic brain: how skewed
  distributions affect network operations, Nature Reviews Neuroscience, 15
  (2014), pp.Â 264â€“278.
* [12]

  E.Â Castillo and A.Â FernÃ¡ndez-Canteli, A general regression model
  for lifetime evaluation and prediction, International Journal of Fracture,
  107 (2001), pp.Â 117â€“137.
* [13]

  A.Â Charles, What is the 1% rule?, The Guardian, July 2006.
* [14]

  V.Â Chistyakov, A theorem on sums of independent positive random
  variables and its applications to branching random processes, Theory of
  Probability & Its Applications, 9 (1964), pp.Â 640â€“648.
* [15]

  A.Â Clauset, C.Â R. Shalizi, and M.Â E. Newman, Power-law distributions
  in empirical data, SIAM review, 51 (2009), pp.Â 661â€“703.
* [16]

  R.Â Cont, Empirical properties of asset returns: stylized facts and
  statistical issues, Quantitative finance, 1 (2001), p.Â 223.
* [17]

  R.Â M. Cooke, D.Â Nieboer, and J.Â Misiewicz, Fat-Tailed Distributions:
  Data, Diagnostics and Dependence, Volume 1, vol.Â 1, John Wiley & Sons,
  2014.
* [18]

  M.Â E. Crovella and A.Â Bestavros, Self-similarity in world wide web
  traffic: Evidence and possible causes, IEEE/ACM Transactions on networking,
  5 (2002), pp.Â 835â€“846.
* [19]

  L.Â Currat, A.Â Francisco, S.Â Al-Tuwaijri, A.Â Ghaffar, and S.Â Jupp, 10/90 report on health research 2003-2004, vol.Â Archived 2015-04-16 at the
  Wayback Machine, WHO Drug Information, 2004.
* [20]

  S.Â Davey, The 10/90 report on health research 2003-2004., 2004.
* [21]

  A.Â B. Downey, The structural cause of file size distributions, in
  Proceedings of the 2001 ACM SIGMETRICS international conference on
  Measurement and modeling of computer systems, 2001, pp.Â 328â€“329.
* [22]

  L.Â Doyal, Gender and the 10/90 gap in health research, 2004.
* [23]

  P.Â Embrechts, C.Â KlÃ¼ppelberg, and T.Â Mikosch, Modelling extremal
  events: for insurance and finance, vol.Â 33, Springer Science & Business
  Media, 2013.
* [24]

  A.Â Endo, S.Â Abbott, A.Â J. Kucharski, S.Â Funk, etÂ al., Estimating the
  overdispersion in covid-19 transmission using outbreak sizes outside china,
  Wellcome open research, 5 (2020), p.Â 67.
* [25]

  V.Â Fabian and J.Â Hannan, Introduction to probability and
  mathematical statistics, John Wiley & Sons, 1985.
* [26]

  E.Â F. Fama, Mandelbrot and the stable paretian hypothesis, The
  journal of business, 36 (1963), pp.Â 420â€“429.
* [27]

  E.Â C. Fieller, The distribution of the index in a normal bivariate
  population, Biometrika, 24 (1932), pp.Â 428â€“440.
* [28]

  S.Â Foss, D.Â Korshunov, S.Â Zachary, etÂ al., An introduction to
  heavy-tailed and subexponential distributions, vol.Â 6, Springer, 2011.
* [29]

  X.Â Gabaix, Power laws in economics and finance, Annu. Rev. Econ., 1
  (2009), pp.Â 255â€“294.
* [30]

  R.Â C. Geary, The frequency distribution of the quotient of two
  normal variates, Journal of the Royal Statistical Society, 93 (1930),
  pp.Â 442â€“446.
* [31]

  M.Â Harchol-Balter, B.Â Schroeder, N.Â Bansal, and M.Â Agrawal, Size-based scheduling to improve web performance, ACM Transactions on
  Computer Systems (TOCS), 21 (2003), pp.Â 207â€“233.
* [32]

  D.Â V. Hinkley, On the ratio of two correlated normal random
  variables, Biometrika, 56 (1969), pp.Â 635â€“639.
* [33]

  H.Â Jeong, S.Â P. Mason, A.-L. BarabÃ¡si, and Z.Â N. Oltvai, Lethality and centrality in protein networks, Nature, 411 (2001),
  pp.Â 41â€“42.
* [34]

  D.Â Katz, J.Â Baptista, S.Â Azen, and M.Â Pike, Obtaining confidence
  intervals for the risk ratio in cohort studies, Biometrics, (1978),
  pp.Â 469â€“474.
* [35]

  W.Â E. Leland, M.Â S. Taqqu, W.Â Willinger, and D.Â V. Wilson, On the
  self-similar nature of ethernet traffic (extended version), IEEE/ACM
  Transactions on networking, 2 (2002), pp.Â 1â€“15.
* [36]

  M.Â Lin, B.Â Fan, J.Â C. Lui, and D.-M. Chiu, Stochastic analysis of
  file-swarming systems, Performance Evaluation, 64 (2007), pp.Â 856â€“875.
* [37]

  J.Â O. Lloyd-Smith, S.Â J. Schreiber, P.Â E. Kopp, and W.Â M. Getz, Superspreading and the effect of individual variation on disease emergence,
  Nature, 438 (2005), pp.Â 355â€“359.
* [38]

  T.Â Lux and M.Â Marchesi, Scaling and criticality in a stochastic
  multi-agent model of a financial market, Nature, 397 (1999), pp.Â 498â€“500.
* [39]

  B.Â Mandelbrot, The variation of certain speculative prices, The
  Journal of Business, 39 (1963), p.Â 394â€“419.
* [40]

  B.Â B. Mandelbrot and R.Â L. Hudson, The (mis) behaviour of markets: a
  fractal view of risk, ruin and reward, Profile books, 2010.
* [41]

  M.Â W. Mantle and R.Â Lichty, Managing the unmanageable: rules, tools,
  and insights for managing software people and teams, Addison-Wesley, 2012.
* [42]

  R.Â Metzler and J.Â Klafter, The random walkâ€™s guide to anomalous
  diffusion: a fractional dynamics approach, Physics reports, 339 (2000),
  pp.Â 1â€“77.
* [43]

  D.Â Middleton, Non-gaussian noise models in signal processing for
  telecommunications: new methods an results for class a and class b noise
  models, IEEE Transactions on information theory, 45 (2002), pp.Â 1129â€“1149.
* [44]

  T.Â Mikosch, Regular variation, subexponentiality and their
  applications in probability theory, (1999).
* [45]

  C.Â L. Nikias and M.Â Shao, Signal processing with alpha-stable
  distributions and applications, Wiley-Interscience, 1995.
* [46]

  V.Â Pareto, Cours dâ€™Ã©conomie politique, vol.Â 1, Librairie Droz,
  1964.
* [47]

  V.Â Paxson and S.Â Floyd, Wide area traffic: the failure of poisson
  modeling, IEEE/ACM Transactions on networking, 3 (1995), pp.Â 226â€“244.
* [48]

  T.Â Pham-Gia, N.Â Turkkan, and E.Â Marchand, Density of the ratio of
  two normal random variables and applications, Communications in
  Statistics-Theory and Methods, 35 (2006), pp.Â 1569â€“1591.
* [49]

  R.-D. Reiss, Approximate distributions of order statistics: with
  applications to nonparametric statistics, Springer science & business
  media, 2012.
* [50]

  T.Â Rolski, H.Â Schmidli, V.Â Schmidt, and J.Â L. Teugels, Stochastic
  processes for insurance and finance, John Wiley & Sons, 2009.
* [51]

  R.Â J. Serfling, Approximation theorems of mathematical statistics,
  John Wiley & Sons, 2009.
* [52]

  M.Â F. Shlesinger, G.Â M. Zaslavsky, and J.Â Klafter, Strange
  kinetics, Nature, 363 (1993), pp.Â 31â€“37.
* [53]

  D.Â Sornette, Why stock markets crash: critical events in complex
  financial systems, in Why stock markets crash, Princeton university press,
  2009.
* [54]

  M.Â D. Springer, The algebra of random variables, (1979).
* [55]

  T.Â Sturgeon, The Claustrophile, Venture: Science fiction
  Magazine, (1956).
* [56]

  Â , On Hand: A
  book, Venture: Science fiction Magazine, 1 (1957).
* [57]

  Â , On
  Handâ€¦Offhand: Books, Venture: Science fiction Magazine, 1 (1957).
* [58]

  Â , Sturgeonâ€™s law,
  Venture Science Fiction, 66 (1958), pp.Â 2â€“8.
* [59]

  N.Â N. Taleb, The Black Swan: The Impact of the Highly Improbable,
  vol.Â 2, Random house trade paperbacks, 2010.
* [60]

  N.Â N. Taleb and R.Â Douady, On the super-additivity and estimation
  biases of quantile contributions, Physica A: Statistical Mechanics and its
  Applications, 429 (2015), pp.Â 252â€“260.
* [61]

  T.Â VanÂ Mierlo etÂ al., The 1% rule in four digital health social
  networks: an observational study, Journal of medical Internet research, 16
  (2014), p.Â e2966.
* [62]

  D.Â Vidyasagar, Global notes: the 10/90 gap disparities in global
  health research, Journal of Perinatology, 26 (2006), pp.Â 55â€“56.