---
authors:
- Jian'an Zhang
doc_id: arxiv:2511.06451v1
family_id: arxiv:2511.06451
is_current: true
taxonomy:
  alpha_families: []
  asset_classes: []
  horizons: []
  themes: []
title: 'ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures'
url_abs: http://arxiv.org/abs/2511.06451v1
url_html: https://arxiv.org/html/2511.06451v1
venue: arXiv q-fin
version: 1
year: 2025
---


ZhangJianâ€™an
  
Guanghua School of Management, Peking University
  
Peking University
  
Beijing, China
  
2501111059@stu.pku.edu.cn

###### Abstract

We introduce *ARBITER*, a riskâ€“neutral neural operator that learns arbitrageâ€“free joint term structures of SPX options and VIX2.
ARBITER reframes selective scan stateâ€“space updates as a discretized Green operator under the riskâ€“neutral measure and imposes geometry at training time via two ingredients:
(i) *Q-Align*, a Lipschitz-controlled projection pipeline combining spectral normalization and nonexpansive projections with a spectralâ€“radius guard (CFLâ€“style) to ensure stable scans;
(ii) a convexâ€“monotone decoder (ICNN with Legendre duality) that enforces static no-arbitrage on the strikeâ€“maturity grid and is consistent with discretized VIX2 replication.
Training uses a saddle-point scheme with fixed, falsifiable stopping thresholds and records auditable diagnostics (Lipschitz constants before/after projection, spectral-guard hits, and projection distances).

On high-fidelity synthetic markets reflecting SPXâ€“VIX coupling, ARBITER attains dimensionless accuracy improvements over strong sequence models and neural SDE baselines: NAS â‰ˆ0.9866\approx 0.9866, CNAS â‰ˆ0.9902\approx 0.9902, NI â‰ˆ0.6776\approx 0.6776, Stability =1.0=1.0, DualGap â‰ˆ0.060\approx 0.060, and low Surfaceâ€“Wasserstein discrepancy, all with 95% HAC confidence intervals and Holmâ€“Bonferroni control.
Stressâ€“toâ€“Fail analyses identify a sharp performance threshold at distortion level â‰ˆ2.0\approx 2.0, while an external-validity protocol (frozen hyperparameters reused across out-of-sample windows) yields minimal degradation.
Ablations confirm the non-replaceability of geometry: disabling gating, halving operator rank, or removing the spectral guard degrades accuracy and stability and introduces surface artifacts.
Theoretical results establish approximation rates, conditioning, identifiability (jointly with VIX2 replication), and convergence of the extragradient scheme to a noise ball.
All code, configuration files, and scripts are released to support full reproducibility.

Keywords: riskâ€“neutral operator learning; arbitrage-free term structure; implied volatility surface; SPXâ€“VIX coupling; spectral projection; convexâ€“monotone decoding.

## 1 Introduction

#### Problem statement and motivation.

Modern large-scale derivatives systems in production still favor a â€œ*fitâ€“then-clean*â€ paradigm: first fit prices or implied-volatility (IV) surfaces with flexible data-driven regressors; then *post hoc* patch static/dynamic no-arbitrage, enforce martingale consistency, and repair change-of-numÃ©raire coherence by smoothing or projections (e.g., SVI-like parameterizations, regularization, or empirical constraints that suppress butterfly and calendar arbitrage). This compartmentalization displaces *financial correctness* to an afterthought, encourages error accumulation under distribution shift, and blurs *when* training should stop and on *what* grounds the model can be rejected or improved.

Concurrently, two influential lines for long-horizon learning have matured: (i) *Selective State Space Models* (SSMs), whose evolution from S4/S5 to Mamba yields linear-time/space primitives that preserve long-range expressivity [[17](https://arxiv.org/html/2511.06451v1#bib.bib17), [18](https://arxiv.org/html/2511.06451v1#bib.bib18), [16](https://arxiv.org/html/2511.06451v1#bib.bib16)]; and (ii) *Neural Operators*, which learn function-to-function mappings and are expressly designed to decouple discretization (grid) from underlying physics [[1](https://arxiv.org/html/2511.06451v1#bib.bib1), [2](https://arxiv.org/html/2511.06451v1#bib.bib2), [21](https://arxiv.org/html/2511.06451v1#bib.bib21)].

#### Thesis: risk-neutral pricing as a structured operator.

We argue that risk-neutral pricing is not merely a target functional but a *structured operator*, specifically a Green operator that maps exogenous drivers, boundary/terminal data, and numÃ©raires to observables over the maturityâ€“strike lattice. When selective scan is used as an efficient evaluation of this operator, then no-arbitrage, martingale consistency, and change-of-numÃ©raire are not optional patches; they are *geometric and spectral invariants* that should hold *during* training. This view upgrades the selective-scan runtime primitive from a sequence mechanism to a *risk-neutral operator layer* endowed with financial semantics.

#### From selective scan to a risk-neutral operator layer.

Let Î©={(T,K)}\Omega=\{(T,K)\} denote the grid of maturities and strikes and xx denote state inputs (underlyings, realized/forward variance proxies, macro/term-structure covariates). We instantiate an operator ğ’¢Î¸\mathcal{G}\_{\theta} that produces prices

|  |  |  |
| --- | --- | --- |
|  | Î Î¸=ğ’¢Î¸â€‹[x;Î©],\Pi\_{\theta}=\mathcal{G}\_{\theta}[x;\,\Omega], |  |

implemented by selective scan for streaming evaluation over Î©\Omega while preserving causality and numÃ©raire-consistent propagation [[16](https://arxiv.org/html/2511.06451v1#bib.bib16)]. We explicitly disentangle (i) *physical propagation*, realized by scan kernels and gates, from (ii) *geometric validity*, enforced by projections and decoders. In particular, martingale consistency ğ”¼â„šâ€‹[St+Î”âˆ£â„±t]=St\mathbb{E}\_{\mathbb{Q}}[S\_{t+\Delta}\!\mid\!\mathcal{F}\_{t}]=S\_{t} and no-arbitrage convexity/monotonicity (e.g., convexity in strike for call prices) are handled *in loop* rather than as post-processing.

#### Geometry in the loop: Q-Align and convexâ€“monotone decoding.

Two architectural devices internalize financial correctness within the learning dynamics.
First, Q-Align performs a training-time 11-Lipschitz projection on the operator outputs or intermediate maps and logs Î»lipbefore/after\lambda\_{\text{lip}}^{\text{before/after}} to make smoothness/monotonicity auditable; practically, we combine spectral/orthogonal parameterizations with projection-based clamps that honor stability budgets [[3](https://arxiv.org/html/2511.06451v1#bib.bib3), [4](https://arxiv.org/html/2511.06451v1#bib.bib4)].
Second, a convexâ€“monotone decoderâ€”an ICNN with a Legendre-conjugate headâ€”makes â€œpriceâ†’\!\rightarrow\!measureâ€ and â€œmeasureâ†’\!\rightarrow\!priceâ€ mutually conjugate, encoding convexity, coordinate-wise monotonicity, and martingale consistency by construction [[5](https://arxiv.org/html/2511.06451v1#bib.bib5), [6](https://arxiv.org/html/2511.06451v1#bib.bib6)].
These mechanisms replace fragile penalty-based heuristics with *project-then-train* geometry.

#### Spectral stability as a first-class rule (Spec-Guard).

Long-horizon optimization is susceptible to subtle instabilities. We introduce Spec-Guard, a spectral-radius CFL rule that monitors the Jacobian spectral radius of state updates and triggers minimum-distance projections whenever Ïâ€‹(JÎ¸)â€‹Î”â€‹t\rho(J\_{\theta})\,\Delta t approaches a safety threshold Î³<1\gamma<1. We log spec\_guard\_hits, projection\_distance, and max\_rho\_dt to quantify stability. Optimization uses saddle-point/extra-gradient updates to regularize adversarial/matching dynamics and prevent cycling or explosion [[7](https://arxiv.org/html/2511.06451v1#bib.bib7), [8](https://arxiv.org/html/2511.06451v1#bib.bib8)]. The result is a loop that is both *stable* and *falsifiable*.

#### Why SPXâ€“VIX needs an operator view.

The SPX IV surface and the VIX term structure are tied by replication identities and change-of-numÃ©raire relations under â„š\mathbb{Q}. Fitting either surface while tolerating violations in the other produces incoherent Greeks, unreliable hedges, and brittle stress responses. Our operator-centric layer aligns the two by baking martingale and numÃ©raire coherence into the semantics of propagation and decoding, avoiding *post hoc* smoothing and manual repairs [[23](https://arxiv.org/html/2511.06451v1#bib.bib23)].

#### Positioning within long-sequence and operator learning.

Our method lies at the interface of selective SSMs and Neural Operators. From the SSM lineage, we leverage linear-time/space selective scan and insights on long-context stability and representation [[17](https://arxiv.org/html/2511.06451v1#bib.bib17), [18](https://arxiv.org/html/2511.06451v1#bib.bib18), [16](https://arxiv.org/html/2511.06451v1#bib.bib16), [20](https://arxiv.org/html/2511.06451v1#bib.bib20), [9](https://arxiv.org/html/2511.06451v1#bib.bib9), [19](https://arxiv.org/html/2511.06451v1#bib.bib19)]. From the Neural Operator lineage, we inherit the abstraction of operator learning that generalizes across discretizations and boundary conditions [[1](https://arxiv.org/html/2511.06451v1#bib.bib1), [2](https://arxiv.org/html/2511.06451v1#bib.bib2), [21](https://arxiv.org/html/2511.06451v1#bib.bib21), [22](https://arxiv.org/html/2511.06451v1#bib.bib22)]. Our contribution is to *specialize* the operator family to risk-neutral, replicable Green operators and to *embed* financial geometry (convexity/monotonicity/martingale) and spectral safety (CFL) *inside* the training loop.

#### Auditing and operational falsifiability.

We convert qualitative desiderata (â€œarbitrage-free,â€ â€œstable,â€ â€œnumÃ©raire-coherentâ€) into auditable artifacts. Each geometric/spectral intervention is a first-class logged event; headline metrics carry heteroskedasticity- and autocorrelation-robust (HAC) intervals with Holmâ€“Bonferroni corrections; and OOS validation follows rolling windows and blocked cross-validation designed for dependent data [[10](https://arxiv.org/html/2511.06451v1#bib.bib10), [11](https://arxiv.org/html/2511.06451v1#bib.bib11), [95](https://arxiv.org/html/2511.06451v1#bib.bib95)]. These protocols support hard claims about deployment readiness.

#### Contributions (all auditable).

1. 1.

   Risk-neutral operator layer. We formalize selective scan as a *risk-neutral Green operator* with complexity linear in grid size and depth, offering separable gating across the composite priceâ€“measureâ€“numÃ©raire map; this alleviates attention bottlenecks for long sequences and long maturities without sacrificing expressivity [[16](https://arxiv.org/html/2511.06451v1#bib.bib16), [19](https://arxiv.org/html/2511.06451v1#bib.bib19)].
2. 2.

   Q-Align: geometry in the loop. We enforce a 11-Lipschitz projection during training and log Î»lipbefore/after\lambda\_{\text{lip}}^{\text{before/after}}, replacing soft penalties with principled projections that tighten monotonicity/convexity guarantees [[3](https://arxiv.org/html/2511.06451v1#bib.bib3), [4](https://arxiv.org/html/2511.06451v1#bib.bib4)].
3. 3.

   Convexâ€“monotone decoder. An ICNN with a Legendre-conjugate head implements mutually conjugate price/measure maps, hard-wiring convexity, coordinate-wise monotonicity, and martingale constraints [[5](https://arxiv.org/html/2511.06451v1#bib.bib5), [6](https://arxiv.org/html/2511.06451v1#bib.bib6)].
4. 4.

   Spec-Guard (spectral CFL). We introduce a spectral rule that monitors and minimally projects state updates, logging spec\_guard\_hits, projection\_distance, and max\_rho\_dt, thereby preventing long-horizon drift and catastrophic divergence [[7](https://arxiv.org/html/2511.06451v1#bib.bib7), [8](https://arxiv.org/html/2511.06451v1#bib.bib8)].
5. 5.

   Evaluation protocol and metrics. We define dimensionless metricsâ€”NAS, CNAS, NI, DualGap, Stability, Surfaceâ€“Wasserstein, and GenGap@95â€”and report 95%95\% HAC-CIs with Holmâ€“Bonferroni correction. Rolling OOS and blocked-CV, together with Stress-to-Fail (S2F) threshold curves, non-substitutability breakers, and external-validity checks, establish a best-paper-grade evidence chain [[23](https://arxiv.org/html/2511.06451v1#bib.bib23), [95](https://arxiv.org/html/2511.06451v1#bib.bib95), [11](https://arxiv.org/html/2511.06451v1#bib.bib11), [10](https://arxiv.org/html/2511.06451v1#bib.bib10)].
6. 6.

   Empirics on SPXâ€“VIX. Under synthetic and quasi-realistic SPXâ€“VIX recipes with a unified budget, our method outperforms strong baselines; ablations (*de-gating*, rank reduction, disabling Spec-Guard) materially degrade CNAS/NAS/Stability and shift S2F thresholds left, demonstrating *non-substitutability*.

#### Scope, assumptions, and limits.

Our design targets joint SPXâ€“VIX term-structure learning with coherent numÃ©raire changes, long horizons where attention bottlenecks are acute, and regimes where OOS stability and falsifiability are paramount. We assume sufficient observability of risk-neutral proxies and include reject/degrade mechanisms so that the system can fail gracefully when assumptions are stressed (Â§[2](https://arxiv.org/html/2511.06451v1#S2 "2 Setting, Notation, and Testable Assumptions â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")). We purposely avoid task-specific hard coding beyond these invariants to preserve portability.

#### Relations to prior art (coverage of all references).

We build on selective SSMs and their stability/expressivity studies [[17](https://arxiv.org/html/2511.06451v1#bib.bib17), [18](https://arxiv.org/html/2511.06451v1#bib.bib18), [16](https://arxiv.org/html/2511.06451v1#bib.bib16), [20](https://arxiv.org/html/2511.06451v1#bib.bib20), [9](https://arxiv.org/html/2511.06451v1#bib.bib9), [19](https://arxiv.org/html/2511.06451v1#bib.bib19)]; on Neural Operators and recent generalizations/surveys [[1](https://arxiv.org/html/2511.06451v1#bib.bib1), [2](https://arxiv.org/html/2511.06451v1#bib.bib2), [21](https://arxiv.org/html/2511.06451v1#bib.bib21), [22](https://arxiv.org/html/2511.06451v1#bib.bib22)]; on arbitrage-free deep pricing and IV-surface regularization/smoothing [[23](https://arxiv.org/html/2511.06451v1#bib.bib23)]; on training stability and geometric constraints via spectral normalization, Lipschitz control, monotone architectures, and extra-gradient dynamics [[3](https://arxiv.org/html/2511.06451v1#bib.bib3), [4](https://arxiv.org/html/2511.06451v1#bib.bib4), [7](https://arxiv.org/html/2511.06451v1#bib.bib7), [8](https://arxiv.org/html/2511.06451v1#bib.bib8), [6](https://arxiv.org/html/2511.06451v1#bib.bib6)]; and on time-series inference/validation protocols including HAC, Holmâ€“Bonferroni, and blocked-CV [[10](https://arxiv.org/html/2511.06451v1#bib.bib10), [11](https://arxiv.org/html/2511.06451v1#bib.bib11), [95](https://arxiv.org/html/2511.06451v1#bib.bib95)]. Our novelty lies in integrating â€œ*operator layer â€“ geometric projection â€“ spectral guard â€“ stopping criteria*â€ into a single, end-to-end, falsifiable risk-neutral learning pipeline tailored to SPXâ€“VIX.

#### Paper roadmap.

SectionÂ [2](https://arxiv.org/html/2511.06451v1#S2 "2 Setting, Notation, and Testable Assumptions â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures") formalizes the market setup, notation, and four testable assumptions (measurable, rejectable, and degradable), together with the dimensionless evaluation metrics.
SectionÂ [3](https://arxiv.org/html/2511.06451v1#S3 "3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures") presents the *ARBITER* architecture: the riskâ€“neutral operator (RN-Operator) cast as a discretized Green operator with metric gating, the *Q-Align* Lipschitz projection with *Spec-Guard* (CFL-style spectral control), and the convexâ€“monotone decoder; it also specifies the saddle-point training loop and fixed, falsifiable stopping criteria.
SectionÂ [4](https://arxiv.org/html/2511.06451v1#S4 "4 Theoretical Results â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures") states our main results (T1â€“T8) on approximation, conditioning, identifiability, sample complexity, feasibility, and convergence of the projected extragradient scheme.
SectionÂ [5](https://arxiv.org/html/2511.06451v1#S5 "5 Evaluation Protocol and Metrics â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures") defines the data protocol and statistical methodology (HAC inference, Holmâ€“Bonferroni control, rolling out-of-sample and blocked-CV), and SectionÂ [6](https://arxiv.org/html/2511.06451v1#S6 "6 Experiments â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures") reports synthetic SPXâ€“VIX experiments, ablations (non-substitutability breakers), external-validity checks, and Stress-to-Fail analyses, accompanied by comprehensive figures and tables.
SectionÂ [7](https://arxiv.org/html/2511.06451v1#S7 "7 Mechanistic Analysis and Diagnostics â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures") provides mechanism-level diagnostics (Q-Align contraction, representative-element behavior, effective dimension).
SectionÂ [8](https://arxiv.org/html/2511.06451v1#S8 "8 Related Work â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures") situates our contribution within operator learning, SSM/Mamba-style models, and term-structure modeling. All proofs are collected in the appendices.SectionÂ [9](https://arxiv.org/html/2511.06451v1#S9 "9 Conclusion and Outlook â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures").

## 2 Setting, Notation, and Testable Assumptions

This section formalizes the market and grids on which the model operates, fixes notation for the riskâ€“neutral operator and its safety quantities, defines the dimensionless evaluation metrics used throughout, and states a suite of assumptions that are *measurable, refutable, and degradable*. All statements below are aligned with the operator view introduced in Â§[1](https://arxiv.org/html/2511.06451v1#S1 "1 Introduction â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures") and with the training and evaluation protocol discussed later.

### 2.1 Market, numeraire, and discretization

We work on a filtered probability space (Î©,â„±,(â„±t)tâ‰¥0,â„š)(\Omega,\mathcal{F},(\mathcal{F}\_{t})\_{t\geq 0},\mathbb{Q}) under a riskâ€“neutral measure â„š\mathbb{Q}. The short rate is (rt)tâ‰¥0(r\_{t})\_{t\geq 0} and the chosen numeraire is a strictly positive process NtN\_{t} (e.g., the moneyâ€“market account Nt=expâ¡(âˆ«0trsâ€‹ğ‘‘s)N\_{t}=\exp(\int\_{0}^{t}r\_{s}\,ds) or a forwardâ€“measure numeraire). Let StS\_{t} denote the equity index (SPX). European call and put prices observed at time tt with maturity T>tT>t and strike K>0K>0 are denoted Ctâ€‹(K,T)C\_{t}(K,T) and Ptâ€‹(K,T)P\_{t}(K,T).

For numerical work we use discrete calendars of maturities ğ’¯={Tâ„“}â„“=1LâŠ‚(t,t+Tmax]\mathcal{T}=\{T\_{\ell}\}\_{\ell=1}^{L}\subset(t,\,t+T\_{\max}] and strikes ğ’¦={Kj}j=1JâŠ‚â„+\mathcal{K}=\{K\_{j}\}\_{j=1}^{J}\subset\mathbb{R}\_{+}, allowing for nonuniform spacings. The *riskâ€“neutral operator* ğ’¢Î¸\mathcal{G}\_{\theta} maps boundary/forcing information defined on (ğ’¯,ğ’¦)(\mathcal{T},\mathcal{K}) to a price surface (K,T)â†¦(Ctâ€‹(K,T),Ptâ€‹(K,T))(K,T)\mapsto\big(C\_{t}(K,T),P\_{t}(K,T)\big) and is implemented with a selective stateâ€“space scan whose propagation is linear in |ğ’¯||\mathcal{T}| (and optionally in |ğ’¦||\mathcal{K}|).

Numeraire consistency is enforced by construction: under the numeraire measure â„šN\mathbb{Q}^{N} associated with NN, the discounted process Xt:=St/NtX\_{t}:=S\_{t}/N\_{t} is a martingale and prices satisfy

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ct(K,T)=Ntğ”¼â„šN[(STâˆ’K)+NT|â„±t],Pt(K,T)=Ntğ”¼â„šN[(Kâˆ’ST)+NT|â„±t].C\_{t}(K,T)\;=\;N\_{t}\,\mathbb{E}^{\mathbb{Q}^{N}}\!\left[\frac{(S\_{T}-K)\_{+}}{N\_{T}}\,\middle|\,\mathcal{F}\_{t}\right],\qquad P\_{t}(K,T)\;=\;N\_{t}\,\mathbb{E}^{\mathbb{Q}^{N}}\!\left[\frac{(K-S\_{T})\_{+}}{N\_{T}}\,\middle|\,\mathcal{F}\_{t}\right]. |  | (1) |

#### VIX2 replication and SPXâ€“VIX coupling.

To couple equity and variance layers we expose the classical replication identity for VIX squared, using its discrete form on (ğ’¯,ğ’¦)(\mathcal{T},\mathcal{K}):

|  |  |  |  |
| --- | --- | --- | --- |
|  | VIXt2â€‹(T)â‰ˆ2Tâˆ’tâ€‹erÂ¯â€‹(Tâˆ’t)â€‹(âˆ‘Kâ‰¤FÎ”â€‹KK2â€‹Ptâ€‹(K,T)+âˆ‘Kâ‰¥FÎ”â€‹KK2â€‹Ctâ€‹(K,T)),\mathrm{VIX}^{2}\_{t}(T)\;\approx\;\frac{2}{T-t}\,e^{\bar{r}\,(T-t)}\left(\sum\_{K\leq F}\frac{\Delta K}{K^{2}}P\_{t}(K,T)\;+\;\sum\_{K\geq F}\frac{\Delta K}{K^{2}}C\_{t}(K,T)\right), |  | (2) |

where FF is the forward level for maturity TT, rÂ¯\bar{r} is a bucketed short rate, and Î”â€‹K\Delta K is the quadrature step.111See the Cboe VIX white paper for the precise continuousâ€“time derivation and practical discretization details [[47](https://arxiv.org/html/2511.06451v1#bib.bib47)].
We treat ([2](https://arxiv.org/html/2511.06451v1#S2.E2 "In VIX2 replication and SPXâ€“VIX coupling. â€£ 2.1 Market, numeraire, and discretization â€£ 2 Setting, Notation, and Testable Assumptions â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) as a linear observable of ğ’¢Î¸\mathcal{G}\_{\theta} so that the SPXâ€“VIX coupling is learned within the same operator layer and audited by the noâ€“arbitrage metrics below.

### 2.2 Notation and safety quantities

We denote by Î²smooth>0\beta\_{\mathrm{smooth}}>0 a HÃ¶lder (or Besov) regularity order upperâ€“bounding the smoothness of the target surface; by Î²novâ‰¥0\beta\_{\mathrm{nov}}\geq 0 a weight scaling a Novikovâ€“type penalty used in the adversarial route of training; and by Ï‡â€‹(Îº)âˆˆ[0,1]\chi(\kappa)\in[0,1] a longâ€“horizon decay index determined by an effective kernel rank Îº\kappa in the operator layer.

Selective scans update a latent state through ht+Î”â€‹t=Atâ€‹ht+Btâ€‹uth\_{t+\Delta t}=A\_{t}h\_{t}+B\_{t}u\_{t} with a dataâ€“dependent step Î”â€‹tt>0\Delta t\_{t}>0. We define the spectral safety quantity

|  |  |  |  |
| --- | --- | --- | --- |
|  | CFLmax=maxtâ¡Ïâ€‹(At)â€‹Î”â€‹tt,\mathrm{CFL}\_{\max}\;=\;\max\_{t}\,\rho(A\_{t})\,\Delta t\_{t}, |  | (3) |

with Ïâ€‹(â‹…)\rho(\cdot) the spectral radius. The *Specâ€“Guard* rule enforces CFLmaxâ‰¤1\mathrm{CFL}\_{\max}\leq 1 by preconditioning and small projections; we record the number of guard activations and the aggregate projection distance. For Lipschitz alignment we estimate a global Lipschitz constant LlipL\_{\mathrm{lip}} by layerwise spectral norms before and after projection and report the pair (Llipbefore,Llipafter)(L\_{\mathrm{lip}}^{\mathrm{before}},\,L\_{\mathrm{lip}}^{\mathrm{after}}) [[3](https://arxiv.org/html/2511.06451v1#bib.bib3), [12](https://arxiv.org/html/2511.06451v1#bib.bib12), Neyshabur2017NormBounds].

### 2.3 Dimensionless evaluation metrics

All metrics are unitâ€“free and reported with heteroskedasticityâ€“ and autocorrelationâ€“consistent (HAC) 95%95\% confidence intervals [[10](https://arxiv.org/html/2511.06451v1#bib.bib10)], using temporally blocked crossâ€“validation to respect dependence [[95](https://arxiv.org/html/2511.06451v1#bib.bib95)]. For families of hypotheses we control multiplicity with the Holmâ€“Bonferroni procedure [[11](https://arxiv.org/html/2511.06451v1#bib.bib11)].

#### Noâ€“Arbitrage Score (NAS; higher is better).

Let â„\mathcal{I} be a finite set of static arbitrage inequalities across (ğ’¯,ğ’¦)(\mathcal{T},\mathcal{K}) (e.g., monotonicity in strike, convexity in strike, calendar monotonicity, callâ€“put parity). For each iâˆˆâ„i\in\mathcal{I}, define a nonnegativity residual riâ€‹(Î¸)r\_{i}(\theta) that vanishes when the inequality is satisfied and is positive when violated. After normalizing by a scale factor sis\_{i} (based on local forward or variance scales), define

|  |  |  |  |
| --- | --- | --- | --- |
|  | NAS:=â€„1âˆ’1Zâ€‹âˆ‘iâˆˆâ„wiâ€‹[riâ€‹(Î¸)/si]+,Z=âˆ‘iâˆˆâ„wi,\mathrm{NAS}\;:=\;1-\frac{1}{Z}\sum\_{i\in\mathcal{I}}w\_{i}\,\big[r\_{i}(\theta)/s\_{i}\big]\_{+},\qquad Z=\sum\_{i\in\mathcal{I}}w\_{i}, |  | (4) |

with nonnegative weights wiw\_{i} that emphasize practically salient constraints. Thus NASâˆˆ[0,1]\mathrm{NAS}\in[0,1] and equals 11 if and only if there are no detected violations. Our constraints follow common arbitrageâ€“free surface checks from the literature [[48](https://arxiv.org/html/2511.06451v1#bib.bib48), [13](https://arxiv.org/html/2511.06451v1#bib.bib13), [14](https://arxiv.org/html/2511.06451v1#bib.bib14)] and are compatible with convex monotone decoders [[5](https://arxiv.org/html/2511.06451v1#bib.bib5), [6](https://arxiv.org/html/2511.06451v1#bib.bib6)].

#### Convolved NAS (CNAS; higher is better).

To evaluate robustness along maturity while downweighting far tails, we convolve NAS over ğ’¯\mathcal{T} with a positive kernel KÎº,Ï„K\_{\kappa,\tau} of bandwidth Îº\kappa and effective horizon Ï„\tau, after withinâ€“maturity rescaling (e.g., by vega or variance scale):

|  |  |  |  |
| --- | --- | --- | --- |
|  | CNAS:=(NASâˆ—ğ’¯KÎº,Ï„).\mathrm{CNAS}\;:=\;(\mathrm{NAS}\ast\_{\mathcal{T}}K\_{\kappa,\tau}). |  | (5) |

Unless stated otherwise, (Îº,Ï„)(\kappa,\tau) and the rescaling convention are fixed across outâ€“ofâ€“sample (OOS) windows to enable external validity checks; the average drop in CNAS when reusing the same hyperparameters across disjoint OOS windows is reported as an externalâ€“validity statistic (mean with 95%95\% interval).

#### Numeraire Invariance (NI; higher is better).

Partition the maturityâ€“strike plane into BÃ—JB\times J buckets. For each bucket consider discounted prices under a set of admissible numeraires and compute the median absolute deviation (MAD) across these normalizations; aggregate the bucketâ€“wise relative dispersion by

|  |  |  |  |
| --- | --- | --- | --- |
|  | NI:=â€„1âˆ’1Bâ€‹Jâ€‹âˆ‘b=1Bâˆ‘j=1JMADâ€‹({Ntâˆ’1â€‹Cb,j(m)}m)scaleb,j,\mathrm{NI}\;:=\;1-\frac{1}{BJ}\sum\_{b=1}^{B}\sum\_{j=1}^{J}\frac{\mathrm{MAD}\big(\{N\_{t}^{-1}C\_{b,j}^{(m)}\}\_{m}\big)}{\mathrm{scale}\_{b,j}}, |  | (6) |

where the denominator is a robust local scale. Higher NI indicates stronger consistency with the numeraireâ€“induced martingale property.

#### Duality Gap (lower is better).

Let minÎ¸â¡maxÎ»âˆˆÎ›â¡â„’â€‹(Î¸,Î»)\min\_{\theta}\max\_{\lambda\in\Lambda}\mathcal{L}(\theta,\lambda) denote the saddle objective arising from adversarial training or operator matching. The empirical duality gap is the difference between the maximal value over Î»\lambda at the current Î¸\theta and the minimal value over Î¸\theta at the current Î»\lambda, estimated on heldâ€“out batches with stabilized updates (e.g., extragradient or lookahead) [[7](https://arxiv.org/html/2511.06451v1#bib.bib7), [8](https://arxiv.org/html/2511.06451v1#bib.bib8), [15](https://arxiv.org/html/2511.06451v1#bib.bib15)].

#### Stability (higher is better).

This is the fraction of random seeds and OOS windows that simultaneously achieve (i) a NAS level above a fixed threshold, (ii) a spectral safety condition CFLmaxâ‰¤1\mathrm{CFL}\_{\max}\leq 1 with bounded projection distance, and (iii) satisfaction of the saddleâ€“point stall conditions. We provide a binomial proportion with HAC intervals.

#### Surfaceâ€“Wasserstein distance (lower is better).

Define for each maturity TT the marginal distributions over strikes induced by the predicted and reference surfaces (after standardization). The overall discrepancy is measured by

|  |  |  |  |
| --- | --- | --- | --- |
|  | SW2:=(âˆ«ğ’¯W22â€‹(Ï€Tpred,Ï€Tref)â€‹ğ‘‘Î¼â€‹(T))1/2,\mathrm{SW}\_{2}\;:=\;\Bigg(\int\_{\mathcal{T}}W\_{2}^{2}\!\big(\pi^{\mathrm{pred}}\_{T},\,\pi^{\mathrm{ref}}\_{T}\big)\,d\mu(T)\Bigg)^{1/2}, |  | (7) |

with W2W\_{2} the 2â€“Wasserstein distance and Î¼\mu a weighting measure over maturities [[29](https://arxiv.org/html/2511.06451v1#bib.bib29)].

#### Generalization gap at the 95th percentile (lower is better).

We report the 9595th percentile of the absolute trainingâ€“toâ€“OOS difference for NAS (or the primary objective), a conservative measure of tail overfitting.

#### Effective dimension.

Let KK be a Gram matrix of operator features on (ğ’¯,ğ’¦)(\mathcal{T},\mathcal{K}). For Î±âˆˆ{0.90,0.95,0.99}\alpha\in\{0.90,0.95,0.99\} define dÎ±d\_{\alpha} as the minimal index such that the sum of the top dÎ±d\_{\alpha} singular values accounts for an Î±\alphaâ€“fraction of the total. The triple (d90,d95,d99)(d\_{90},d\_{95},d\_{99}) provides a capacity proxy that enters the oracle bounds in Â§[4](https://arxiv.org/html/2511.06451v1#S4 "4 Theoretical Results â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures").

### 2.4 Assumptions: measurable, refutable, and degradable

We formulate four assumptions. Each is *measurable* from trainingâ€“time statistics, *refutable* by explicit counterâ€“examples or threshold tests, and *degradable* in the sense that, when violated, we fall back to weaker but still valid guarantees used in evaluation.

#### A1 (necessary): Novikovâ€“toâ€“Kazamaki switching.

Let (Mt)(M\_{t}) be the local martingale driving the stochastic component of the operator layer. Novikovâ€™s condition, ğ”¼â€‹[expâ¡(12â€‹âŸ¨MâŸ©T)]<âˆ\mathbb{E}[\exp(\tfrac{1}{2}\langle M\rangle\_{T})]<\infty, implies martingality and is stronger than Kazamakiâ€™s criterion; empirical data roughness can make Kazamaki more appropriate. We measure, across OOS windows, the fraction for which Novikov holds but Kazamaki is preferred by the test statistic, and report its mean with a 95%95\% interval. A stable operator exhibits a high switching rate as roughness increases, consistent with recent stability analyses of selective stateâ€“space models [[16](https://arxiv.org/html/2511.06451v1#bib.bib16), [27](https://arxiv.org/html/2511.06451v1#bib.bib27)].

#### A2 (sufficient): Smoothness fallback and representer bound.

When local estimates indicate that Î²smooth\beta\_{\mathrm{smooth}} falls below the nominal order on subsets of (ğ’¯,ğ’¦)(\mathcal{T},\mathcal{K}), we switch from the smoothnessâ€“based identifiability bound to a representerâ€“type bound (TheoremÂ T2â€²), where the operator error over L2L^{2} is controlled by a combination of coverage deficit and dual residual. The time of switch and the coverage level at trigger are reported. Proof details and rates are given in Â§[4](https://arxiv.org/html/2511.06451v1#S4 "4 Theoretical Results â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures").

#### A3 (sufficient): Rankâ€“controlled longâ€“memory decay.

The effective rank Îº\kappa of the selective kernel determines a decay index Ï‡â€‹(Îº)âˆˆ[0,1]\chi(\kappa)\in[0,1] that governs longâ€“horizon oracle terms (TheoremÂ T3). We estimate Ï‡â€‹(Îº)\chi(\kappa) from spectral slopes of the scan kernel; deviations prompt stricter spectral guarding and Lipschitz projections [[12](https://arxiv.org/html/2511.06451v1#bib.bib12), [16](https://arxiv.org/html/2511.06451v1#bib.bib16), [27](https://arxiv.org/html/2511.06451v1#bib.bib27)].

#### A4 (necessary): Coverage threshold.

Let cminc\_{\min} and cÂ¯\bar{c} be, respectively, the minimum and mean fraction of observed (T,K)(T,K) cells (after quality control) per window. We require cminâ‰¥cÂ¯=0.75c\_{\min}\geq\underline{c}=0.75. If violated, claims revert to the representerâ€“bound regime (A2), the event is reported in the main text, and stressâ€“toâ€“fail experiments are used to characterize the failure mode.

### 2.5 Statistical reporting

All metrics are computed per window and aggregated with HAC intervals; multiplicity is controlled within families of hypotheses by Holmâ€“Bonferroni. Crossâ€“validation uses temporally blocked folds to avoid leakage. For the SPXâ€“VIX coupling we apply the replication identity ([2](https://arxiv.org/html/2511.06451v1#S2.E2 "In VIX2 replication and SPXâ€“VIX coupling. â€£ 2.1 Market, numeraire, and discretization â€£ 2 Setting, Notation, and Testable Assumptions â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) with the discrete quadrature recommended by the exchange documentation [[47](https://arxiv.org/html/2511.06451v1#bib.bib47)]. The Lipschitz constants are estimated by spectralâ€“norm proxies; their preâ€“ and postâ€“projection values are reported alongside the spectral safety quantity CFLmax\mathrm{CFL}\_{\max}, the number of spectralâ€“guard activations, and the aggregate projection distance. These quantities are used later to establish the stability and refutability of the operator constraints and to ablate the role of each geometric ingredient.

## 3 Method: The ARBITER Architecture

We present Arbiter, a riskâ€“neutral neural operator for arbitrage-free SPXâ€“VIX term structures. The model integrates four components: (i) a *riskâ€“neutral operator layer* that interprets selective state-space scans as a discretized Green operator under a learned riskâ€“neutral measure; (ii) *Q-Align*, a pair of geometric projections that enforce layerwise Lipschitz bounds and a spectral CFL condition; (iii) a *convexâ€“monotone decoder* that enforces static no-arbitrage along strikes and maturities, tied to VIX replication; and (iv) a *saddle-point training protocol* with safety-oriented stopping rules. We work on a maturity grid {Tâ„“}â„“=1L\{T\_{\ell}\}\_{\ell=1}^{L} (not necessarily uniform) and an implied strike set ğ’¦\mathcal{K}; the numeraire is fixed by discounting.

### 3.1 Riskâ€“Neutral Operator Layer (RN-Operator)

#### Selective scan as a Green operator.

Let hâ„“âˆˆâ„mh\_{\ell}\in\mathbb{R}^{m} be hidden states at Tâ„“T\_{\ell}, with input features uâ„“â€‹(â‹…)âˆˆL2â€‹(ğ’¦)u\_{\ell}(\cdot)\in L^{2}(\mathcal{K}) summarizing cross-sectional information (e.g., moneyness bins and microstructure covariates) at maturity Tâ„“T\_{\ell}. The selective state-space recursion is

|  |  |  |  |
| --- | --- | --- | --- |
|  | hâ„“+1=AÎ¸â€‹(Tâ„“)â€‹hâ„“+BÎ¸â€‹(Tâ„“)â€‹Îâ€‹[uâ„“],yâ„“=QÎ¸â€‹(Tâ„“)â€‹hâ„“,h\_{\ell+1}\;=\;A\_{\theta}(T\_{\ell})\,h\_{\ell}\;+\;B\_{\theta}(T\_{\ell})\,\Xi[u\_{\ell}],\qquad y\_{\ell}\;=\;Q\_{\theta}(T\_{\ell})\,h\_{\ell}, |  | (8) |

where Î\Xi is a linear embedding from L2â€‹(ğ’¦)L^{2}(\mathcal{K}) to â„m\mathbb{R}^{m} and yâ„“âˆˆâ„my\_{\ell}\in\mathbb{R}^{m} is a latent representation. UnrollingÂ ([8](https://arxiv.org/html/2511.06451v1#S3.E8 "In Selective scan as a Green operator. â€£ 3.1 Riskâ€“Neutral Operator Layer (RN-Operator) â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) yields the discrete Green expansion

|  |  |  |  |
| --- | --- | --- | --- |
|  | yâ„“=âˆ‘sâ‰¤â„“(âˆj=sâ„“âˆ’1AÎ¸â€‹(Tj))â€‹BÎ¸â€‹(Ts)â€‹Îâ€‹[us]=âˆ‘sâ‰¤â„“ğ’¢Î¸â€‹(Tâ„“,Ts)â€‹Îâ€‹[us],y\_{\ell}\;=\;\sum\_{s\leq\ell}\Bigg(\prod\_{j=s}^{\ell-1}A\_{\theta}(T\_{j})\Bigg)B\_{\theta}(T\_{s})\,\Xi[u\_{s}]\;=\;\sum\_{s\leq\ell}\mathcal{G}\_{\theta}(T\_{\ell},T\_{s})\,\Xi[u\_{s}], |  | (9) |

with ğ’¢Î¸â€‹(Tâ„“,Ts):=âˆj=sâ„“âˆ’1AÎ¸â€‹(Tj)â€‹BÎ¸â€‹(Ts)\mathcal{G}\_{\theta}(T\_{\ell},T\_{s}):=\prod\_{j=s}^{\ell-1}A\_{\theta}(T\_{j})B\_{\theta}(T\_{s}).

#### Measure gating and riskâ€“neutral semantics.

To embed no-arbitrage at training time, we introduce a *measure gate* wÎ¸â€‹(K,T)â‰¥0w\_{\theta}(K,T)\geq 0 and replace usu\_{s} by usâ‹†â€‹(K)=wÎ¸â€‹(K,Ts)â€‹usâ€‹(K)u\_{s}^{\star}(K)=w\_{\theta}(K,T\_{s})\,u\_{s}(K), thereby defining a density wÎ¸â€‹(â‹…,â‹…)w\_{\theta}(\cdot,\cdot) on ğ’¦Ã—{Tâ„“}\mathcal{K}\times\{T\_{\ell}\}. The discounted price functional on a payoff Ï†\varphi is evaluated through

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î Î¸â€‹[Ï†]â€‹(T)=âˆ«ğ’¦Ï†â€‹(K,T)â€‹wÎ¸â€‹(K,T)â€‹dK,\Pi\_{\theta}[\varphi](T)\;=\;\int\_{\mathcal{K}}\varphi(K,T)\,w\_{\theta}(K,T)\,\mathrm{d}K, |  | (10) |

and training penalizes deviations from the martingale condition under â„šÎ¸\mathbb{Q}\_{\theta}:

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â„šÎ¸[St+Î´eâˆ’râ€‹Î´âˆ’St|â„±t]=â€„0âŸºdâ„šÎ¸âˆwÎ¸dâ„™,\mathbb{E}^{\mathbb{Q}\_{\theta}}\!\left[S\_{t+\delta}\mathrm{e}^{-r\delta}-S\_{t}\,\middle|\,\mathcal{F}\_{t}\right]\;=\;0\quad\Longleftrightarrow\quad\mathrm{d}\mathbb{Q}\_{\theta}\propto w\_{\theta}\,\mathrm{d}\mathbb{P}, |  | (11) |

with a convex penalty on residuals ofÂ ([11](https://arxiv.org/html/2511.06451v1#S3.E11 "In Measure gating and riskâ€“neutral semantics. â€£ 3.1 Riskâ€“Neutral Operator Layer (RN-Operator) â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) across random slices (K,T)(K,T).
In practice wÎ¸w\_{\theta} is parameterized by a positive squashing map (e.g., softplus) followed by normalization across KK at each TT so that âˆ«wÎ¸â€‹(K,T)â€‹dK=1\int w\_{\theta}(K,T)\mathrm{d}K=1.

#### Complexity.

Let mm be the effective rank of the operator (SectionÂ [2](https://arxiv.org/html/2511.06451v1#S2 "2 Setting, Notation, and Testable Assumptions â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")). The recurrenceÂ ([8](https://arxiv.org/html/2511.06451v1#S3.E8 "In Selective scan as a Green operator. â€£ 3.1 Riskâ€“Neutral Operator Layer (RN-Operator) â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) and Green evaluationÂ ([67](https://arxiv.org/html/2511.06451v1#Ax1.E67 "In Model parameterization. â€£ B.1 Proof of Theorem 2: approximation rate and conditioning â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) both run in linear time and memory:

|  |  |  |
| --- | --- | --- |
|  | time=ğ’ªâ€‹(Lâ€‹m),space=ğ’ªâ€‹(m).\mathrm{time}=\mathcal{O}(Lm),\qquad\mathrm{space}=\mathcal{O}(m). |  |

This preserves the computational profile of selective SSMs while upgrading its semantics to a riskâ€“neutral operator.

#### Neumann expansion under a CFL condition.

Let Î”â€‹tâ„“:=Tâ„“+1âˆ’Tâ„“\Delta t\_{\ell}:=T\_{\ell+1}-T\_{\ell} and define the discrete CFL indicator

|  |  |  |  |
| --- | --- | --- | --- |
|  | CFLâ€‹(Tâ„“):=Ïâ€‹(AÎ¸â€‹(Tâ„“))â€‹Î”â€‹tâ„“,CFLmax:=maxâ„“â¡CFLâ€‹(Tâ„“),\mathrm{CFL}(T\_{\ell}):=\rho\!\big(A\_{\theta}(T\_{\ell})\big)\,\Delta t\_{\ell},\qquad\mathrm{CFL}\_{\max}:=\max\_{\ell}\mathrm{CFL}(T\_{\ell}), |  | (12) |

with Ïâ€‹(â‹…)\rho(\cdot) the spectral radius. When CFLmax<1\mathrm{CFL}\_{\max}<1, products âˆj=sâ„“âˆ’1AÎ¸â€‹(Tj)\prod\_{j=s}^{\ell-1}A\_{\theta}(T\_{j}) are summable andÂ ([67](https://arxiv.org/html/2511.06451v1#Ax1.E67 "In Model parameterization. â€£ B.1 Proof of Theorem 2: approximation rate and conditioning â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) admits a uniformly convergent Neumann-like representation.

#### Spectral safety and discrete Green kernel.

Let {Tâ„“}â„“âˆˆâ„¤\{T\_{\ell}\}\_{\ell\in\mathbb{Z}} be the evaluation grid with steps Î”â€‹tâ„“:=Tâ„“+1âˆ’Tâ„“>0\Delta t\_{\ell}:=T\_{\ell+1}-T\_{\ell}>0 and a timeâ€“varying linear operator AÎ¸â€‹(Tâ„“)âˆˆâ„dÃ—dA\_{\theta}(T\_{\ell})\in\mathbb{R}^{d\times d}.
Define Mâ„“:=Î”â€‹tâ„“â€‹AÎ¸â€‹(Tâ„“)M\_{\ell}:=\Delta t\_{\ell}A\_{\theta}(T\_{\ell}) and the oneâ€“step resolvent Râ„“:=(Iâˆ’Mâ„“)âˆ’1R\_{\ell}:=(I-M\_{\ell})^{-1}.
For an impulse injection BsB\_{s} at time TsT\_{s}, the discrete causal Green kernel is

|  |  |  |
| --- | --- | --- |
|  | ğ’¢Î¸â€‹(Tâ„“,Ts)=Râ„“â€‹Râ„“âˆ’1â€‹â‹¯â€‹Rs+1â€‹Bs,sâ‰¤â„“.\mathcal{G}\_{\theta}(T\_{\ell},T\_{s})\;=\;R\_{\ell}R\_{\ell-1}\cdots R\_{s+1}\,B\_{s},\qquad s\leq\ell. |  |

Under the CFLâ€“type safeguard Ïâ€‹(AÎ¸â€‹(Tâ„“))â€‹Î”â€‹tâ„“â‰¤1âˆ’Îµ\rho(A\_{\theta}(T\_{\ell}))\,\Delta t\_{\ell}\leq 1-\varepsilon (Spec-Guard), the kernel is uniformly summable.

###### Lemma 1 (Green kernel bound).

Assume Ïâ€‹(AÎ¸â€‹(Tâ„“))â€‹Î”â€‹tâ„“â‰¤1âˆ’Îµ\rho\!\left(A\_{\theta}(T\_{\ell})\right)\,\Delta t\_{\ell}\leq 1-\varepsilon for all â„“\ell with some Îµâˆˆ(0,1)\varepsilon\in(0,1), and that â€–Bsâ€–â‰¤bâ€‹Î”â€‹ts\|B\_{s}\|\leq b\,\Delta t\_{s} for a constant b>0b>0 under an operator norm subordinate to a vector norm.
Then there exists C=Câ€‹(Îµ,b,Î”â€‹tÂ¯)<âˆC=C(\varepsilon,b,\overline{\Delta t})<\infty, with Î”â€‹tÂ¯:=supâ„“Î”â€‹tâ„“\overline{\Delta t}:=\sup\_{\ell}\Delta t\_{\ell}, such that

|  |  |  |
| --- | --- | --- |
|  | âˆ‘sâ‰¤â„“â€–ğ’¢Î¸â€‹(Tâ„“,Ts)â€–â‰¤Câ€‹(Îµ,b,Î”â€‹tÂ¯)for allÂ â€‹â„“.\sum\_{s\leq\ell}\Big\|\mathcal{G}\_{\theta}(T\_{\ell},T\_{s})\Big\|\;\leq\;C(\varepsilon,b,\overline{\Delta t})\quad\text{for all }\ell. |  |

###### Proof sketch.

The CFL constraint enforces Ïâ€‹(Mâ„“)â‰¤1âˆ’Îµ\rho(M\_{\ell})\leq 1-\varepsilon. By the extremalâ€“norm (joint spectral radius) argument there exists an induced norm in which â€–Mâ„“â€–â‰¤Î±<1\|M\_{\ell}\|\leq\alpha<1 uniformly, hence â€–Râ„“â€–=â€–(Iâˆ’Mâ„“)âˆ’1â€–â‰¤(1âˆ’Î±)âˆ’1\|R\_{\ell}\|=\|(I-M\_{\ell})^{-1}\|\leq(1-\alpha)^{-1}.
Submultiplicativity gives â€–Râ„“â€‹â‹¯â€‹Rs+1â€–â‰¤(1âˆ’Î±)âˆ’(â„“âˆ’s)\|R\_{\ell}\cdots R\_{s+1}\|\leq(1-\alpha)^{-(\ell-s)}, and the factor â€–Bsâ€–â‰¤bâ€‹Î”â€‹ts\|B\_{s}\|\leq b\,\Delta t\_{s} makes the series geometrically summable over ss.
Full details, including the nonâ€“diagonalizable case via blockâ€“Jordan bounds and the removal of normâ€“equivalence constants, are provided in AppendixÂ A.1.
âˆ

#### Lipschitz surrogate via spectral normalization.

Each linear map WW inÂ ([8](https://arxiv.org/html/2511.06451v1#S3.E8 "In Selective scan as a Green operator. â€£ 3.1 Riskâ€“Neutral Operator Layer (RN-Operator) â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) is spectrally normalized, â€–Wâ€–2â‰¤Ï„â‰¤1\|W\|\_{2}\leq\tau\leq 1, and each nonlinearity is 1-Lipschitz, yielding a global Lipschitz surrogate for the RN-operator:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Lipâ€‹(ğ’¢Î¸)â‰¤(âˆlinearÂ â€‹â„“â€–Wâ„“â€–2)â‹…Câ€‹(Îµ),\mathrm{Lip}(\mathcal{G}\_{\theta})\;\leq\;\Big(\prod\_{\text{linear }\ell}\|W\_{\ell}\|\_{2}\Big)\cdot C(\varepsilon), |  | (13) |

with Câ€‹(Îµ)C(\varepsilon) from LemmaÂ [1](https://arxiv.org/html/2511.06451v1#Thmlemma1 "Lemma 1 (Green kernel bound). â€£ Spectral safety and discrete Green kernel. â€£ 3.1 Riskâ€“Neutral Operator Layer (RN-Operator) â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures"). This bound is tracked by the logged pair (Î»lipâ€‹-â€‹before,Î»lipâ€‹-â€‹after)(\lambda\_{\mathrm{lip}\text{-}\mathrm{before}},\lambda\_{\mathrm{lip}\text{-}\mathrm{after}}).

### 3.2 Q-Align: Lipschitz Projection and Spectral Guard

#### Layerwise Lipschitz projection.

After each optimizer step, we project every linear map WW onto the spectral ball of radius Ï„â‰¤1\tau\leq 1:

|  |  |  |  |
| --- | --- | --- | --- |
|  | W^=Ï„maxâ¡(â€–Wâ€–2,Ï„)â€‹W.\widehat{W}\;=\;\frac{\tau}{\max(\|W\|\_{2},\tau)}\,W. |  | (14) |

A single power iteration per matrix provides â€–Wâ€–2\|W\|\_{2} with small overhead. The cumulative Lipschitz surrogate inÂ ([13](https://arxiv.org/html/2511.06451v1#S3.E13 "In Lipschitz surrogate via spectral normalization. â€£ 3.1 Riskâ€“Neutral Operator Layer (RN-Operator) â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) thus remains controlled.

#### Spectral Guard (CFL projection).

We estimate Ïâ€‹(AÎ¸â€‹(Tâ„“))\rho(A\_{\theta}(T\_{\ell})) via power iteration and enforce

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ïâ€‹(AÎ¸â€‹(Tâ„“))â€‹Î”â€‹tâ„“â‰¤â€„1âˆ’Îµ.\rho\!\big(A\_{\theta}(T\_{\ell})\big)\,\Delta t\_{\ell}\;\leq\;1-\varepsilon. |  | (15) |

A minimal-distance correction in Frobenius norm admits the scaling solution

|  |  |  |  |
| --- | --- | --- | --- |
|  | AÎ¸â€‹(Tâ„“)â†1âˆ’ÎµÏâ€‹(AÎ¸â€‹(Tâ„“))â€‹Î”â€‹tâ„“â€‹AÎ¸â€‹(Tâ„“),A\_{\theta}(T\_{\ell})\;\leftarrow\;\frac{1-\varepsilon}{\rho(A\_{\theta}(T\_{\ell}))\,\Delta t\_{\ell}}\,A\_{\theta}(T\_{\ell}), |  | (16) |

whenever the left-hand side ofÂ ([15](https://arxiv.org/html/2511.06451v1#S3.E15 "In Spectral Guard (CFL projection). â€£ 3.2 Q-Align: Lipschitz Projection and Spectral Guard â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) exceeds 1âˆ’Îµ1-\varepsilon. We log the activation count specâ€‹\_â€‹guardâ€‹\_â€‹hits\mathrm{spec\\_guard\\_hits}, the cumulative correction âˆ‘â„“â€–AÎ¸â€‹(Tâ„“)âˆ’A^Î¸â€‹(Tâ„“)â€–F\sum\_{\ell}\|A\_{\theta}(T\_{\ell})-\widehat{A}\_{\theta}(T\_{\ell})\|\_{F} (denoted *projection distance*), and maxâ„“â¡Ïâ€‹(AÎ¸â€‹(Tâ„“))â€‹Î”â€‹tâ„“\max\_{\ell}\rho(A\_{\theta}(T\_{\ell}))\Delta t\_{\ell}.

#### RN-operator stability under Q-Align.

Let {Tâ„“}â„“âˆˆâ„¤\{T\_{\ell}\}\_{\ell\in\mathbb{Z}} be the evaluation grid with steps Î”â€‹tâ„“>0\Delta t\_{\ell}>0, and write Mâ„“:=Î”â€‹tâ„“â€‹AÎ¸â€‹(Tâ„“)M\_{\ell}:=\Delta t\_{\ell}A\_{\theta}(T\_{\ell}) and Râ„“:=(Iâˆ’Mâ„“)âˆ’1R\_{\ell}:=(I-M\_{\ell})^{-1}.
Consider the RN-operator layer with nonexpansive nonlinearity Ï•\phi and projected weights (Q-Align) satisfying the layerwise Lipschitz envelope inÂ ([14](https://arxiv.org/html/2511.06451v1#S3.E14 "In Layerwise Lipschitz projection. â€£ 3.2 Q-Align: Lipschitz Projection and Spectral Guard â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")), together with the spectral safeguardÂ ([15](https://arxiv.org/html/2511.06451v1#S3.E15 "In Spectral Guard (CFL projection). â€£ 3.2 Q-Align: Lipschitz Projection and Spectral Guard â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")).
Denote by ğ’¢Î¸â€‹(Tâ„“,Ts)\mathcal{G}\_{\theta}(T\_{\ell},T\_{s}) the discrete causal Green kernel.
We obtain:

###### Proposition 1 (RN-operator stability under Q-Align).

AssumeÂ ([14](https://arxiv.org/html/2511.06451v1#S3.E14 "In Layerwise Lipschitz projection. â€£ 3.2 Q-Align: Lipschitz Projection and Spectral Guard â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) andÂ ([15](https://arxiv.org/html/2511.06451v1#S3.E15 "In Spectral Guard (CFL projection). â€£ 3.2 Q-Align: Lipschitz Projection and Spectral Guard â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) hold with Ï„â‰¤1\tau\leq 1 and Îµâˆˆ(0,1)\varepsilon\in(0,1).
Then for any bounded input sequence {uâ„“}\{u\_{\ell}\}, the state trajectory {hâ„“}\{h\_{\ell}\} is uniformly bounded; moreover the input-to-output map induced by ğ’¢Î¸\mathcal{G}\_{\theta} is globally Lipschitz with constant bounded byÂ ([13](https://arxiv.org/html/2511.06451v1#S3.E13 "In Lipschitz surrogate via spectral normalization. â€£ 3.1 Riskâ€“Neutral Operator Layer (RN-Operator) â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")).

###### Proof sketch.

By LemmaÂ [1](https://arxiv.org/html/2511.06451v1#Thmlemma1 "Lemma 1 (Green kernel bound). â€£ Spectral safety and discrete Green kernel. â€£ 3.1 Riskâ€“Neutral Operator Layer (RN-Operator) â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures") (AppendixÂ A.1), the discrete Green kernel is uniformly summable under the CFL-type constraint Ïâ€‹(AÎ¸â€‹(Tâ„“))â€‹Î”â€‹tâ„“â‰¤1âˆ’Îµ\rho(A\_{\theta}(T\_{\ell}))\,\Delta t\_{\ell}\leq 1-\varepsilon.
The Q-Align projectionÂ ([14](https://arxiv.org/html/2511.06451v1#S3.E14 "In Layerwise Lipschitz projection. â€£ 3.2 Q-Align: Lipschitz Projection and Spectral Guard â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) enforces a layerwise 11-Lipschitz envelope (with factor Ï„â‰¤1\tau\leq 1) and nonexpansive Ï•\phi preserves Lipschitz bounds through composition.
Unrolling the recursion and applying submultiplicativity yields a geometric series dominated by the kernel sum from LemmaÂ [1](https://arxiv.org/html/2511.06451v1#Thmlemma1 "Lemma 1 (Green kernel bound). â€£ Spectral safety and discrete Green kernel. â€£ 3.1 Riskâ€“Neutral Operator Layer (RN-Operator) â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures"), which provides both bounded-inputâ€“bounded-output (BIBO) stability and a global Lipschitz constant that matchesÂ ([13](https://arxiv.org/html/2511.06451v1#S3.E13 "In Lipschitz surrogate via spectral normalization. â€£ 3.1 Riskâ€“Neutral Operator Layer (RN-Operator) â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")).
Full details are given in AppendixÂ A.2.
âˆ

#### Geometric diagnostics.

The triplet (Î»lipâ€‹-â€‹before,Î»lipâ€‹-â€‹after,CFLmax)(\lambda\_{\mathrm{lip}\text{-}\mathrm{before}},\lambda\_{\mathrm{lip}\text{-}\mathrm{after}},\mathrm{CFL}\_{\max}) summarizes per-epoch geometry. Large *projection distance* or frequent specâ€‹\_â€‹guardâ€‹\_â€‹hits\mathrm{spec\\_guard\\_hits} predict subsequent instability or poor generalization and are therefore treated as early-warning signals.

### 3.3 Convexâ€“Monotone Decoder and SPXâ€“VIX Coupling

#### No-arbitrage constraints.

On each maturity TT, let Kâ†¦Câ€‹(K,T)K\mapsto C(K,T) be the call price surface. Static no-arbitrage requires

|  |  |  |  |
| --- | --- | --- | --- |
|  | âˆ‚Kâ€‹K2Câ€‹(â‹…,T)â‰¥0,âˆ‚TCâ€‹(K,â‹…)â‰¥0,0â‰¤Câ€‹(K,T)â‰¤S0,âˆ‚KCâ€‹(K,T)â‰¤0.\partial\_{KK}^{2}C(\cdot,T)\geq 0,\qquad\partial\_{T}C(K,\cdot)\geq 0,\qquad 0\leq C(K,T)\leq S\_{0},\qquad\partial\_{K}C(K,T)\leq 0. |  | (17) |

We parameterize Câ€‹(â‹…,T)C(\cdot,T) as an input-convex neural potential Î¦â€‹(â‹…;T)\Phi(\cdot;T), i.e.,

|  |  |  |  |
| --- | --- | --- | --- |
|  | Câ€‹(K,T)=Î¦â€‹(K;T),Î¦â€‹(â‹…;T)â€‹convex,âˆ‚TÎ¦â€‹(K,T)â‰¥0,C(K,T)=\Phi(K;T),\qquad\Phi(\cdot;T)\ \text{convex},\qquad\partial\_{T}\Phi(K,T)\geq 0, |  | (18) |

where convexity is enforced by nonnegative weights on the KK-dependent paths and skip connections, and maturity monotonicity is enforced by a positive-slope parameterization in TT (with a final monotone calibration if needed).

#### Legendre structure and density.

Define the convex conjugate Î¦â‹†â€‹(p;T)=supKâˆˆâ„{pâ€‹Kâˆ’Î¦â€‹(K;T)}\Phi^{\star}(p;T)=\sup\_{K\in\mathbb{R}}\{pK-\Phi(K;T)\}. Then pâ‹†â€‹(T):=âˆ‚KÎ¦â€‹(K,T)p^{\star}(T):=\partial\_{K}\Phi(K,T) yields the delta, and the Breedenâ€“Litzenberger relation connects second derivatives to the riskâ€“neutral density fâ„šf\_{\mathbb{Q}}:

|  |  |  |  |
| --- | --- | --- | --- |
|  | fâ„šâ€‹(K,T)=erâ€‹Tâ€‹âˆ‚Kâ€‹K2Câ€‹(K,T).f\_{\mathbb{Q}}(K,T)=\mathrm{e}^{rT}\,\partial\_{KK}^{2}C(K,T). |  | (19) |

The decoder thus produces both prices and densities with consistent geometry.

#### SPXâ†”\leftrightarrowVIX replication.

Let ğ’¦T\mathcal{K}\_{T} denote the strike grid at maturity TT, with ordered knots 0<K1<â‹¯<KM0<K\_{1}<\cdots<K\_{M} and spacings Î”â€‹Ki:=12â€‹(Ki+1âˆ’Kiâˆ’1)\Delta K\_{i}:=\tfrac{1}{2}(K\_{i+1}-K\_{i-1}) (endpoints use one-sided spacings).
Write FT:=S0â€‹e(râˆ’q)â€‹TF\_{T}:=S\_{0}\mathrm{e}^{(r-q)T} and K0K\_{0} for the closest strike to FTF\_{T}.
The (continuous) log-contract identity implies the variance-swap fair rate under â„š\mathbb{Q}:

|  |  |  |
| --- | --- | --- |
|  | ÏƒVS2â€‹(T)=2â€‹erâ€‹TTâ€‹(âˆ«0FTPâ€‹(K,T)K2â€‹ğ‘‘K+âˆ«FTâˆCâ€‹(K,T)K2â€‹ğ‘‘K)âˆ’1Tâ€‹(FTK0âˆ’1)2.\sigma^{2}\_{\mathrm{VS}}(T)=\frac{2\,\mathrm{e}^{rT}}{T}\!\left(\int\_{0}^{F\_{T}}\!\frac{P(K,T)}{K^{2}}\,dK+\int\_{F\_{T}}^{\infty}\!\frac{C(K,T)}{K^{2}}\,dK\right)-\frac{1}{T}\Big(\tfrac{F\_{T}}{K\_{0}}-1\Big)^{\!2}. |  |

Consistent with the Cboe construction, we discretize it as

|  |  |  |  |
| --- | --- | --- | --- |
|  | VIX2â€‹(T)â‰ˆ2â€‹erâ€‹TTâ€‹âˆ‘Kiâˆˆğ’¦TÎ”â€‹KiKi2â€‹Qâ€‹(Ki,T)âˆ’1Tâ€‹(FTK0âˆ’1)2,\mathrm{VIX}^{2}(T)\;\approx\;\frac{2\,\mathrm{e}^{rT}}{T}\sum\_{K\_{i}\in\mathcal{K}\_{T}}\frac{\Delta K\_{i}}{K\_{i}^{2}}\,Q(K\_{i},T)\;-\;\frac{1}{T}\Big(\tfrac{F\_{T}}{K\_{0}}-1\Big)^{\!2}, |  | (20) |

where QQ is the out-of-the-money option price at strike KiK\_{i} (put if Ki<FTK\_{i}<F\_{T}, call if Kiâ‰¥FTK\_{i}\geq F\_{T}) and the small forward adjustment term is retained.
Missing strikes are filled by linear interpolation on (K,Q)(K,Q), which preserves piecewise convexity in KK; AppendixÂ A.3 compares cubic splines and shows comparable NAS/CNAS together with a mild increase in butterfly-arbitrage risk.

We define the *replication residual*

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„›VIXâ€‹(T):=VIXobs2â€‹(T)âˆ’VIXÎ¸2â€‹(T),\mathcal{R}\_{\mathrm{VIX}}(T):=\mathrm{VIX}^{2}\_{\mathrm{obs}}(T)-\mathrm{VIX}^{2}\_{\theta}(T), |  | (21) |

and include the dual penalty Î»vsâ€‹âˆ‘Twâ€‹(T)â€‹â„›VIXâ€‹(T)2\lambda\_{\mathrm{vs}}\sum\_{T}w(T)\,\mathcal{R}\_{\mathrm{VIX}}(T)^{2} in the saddle objective (weights wâ€‹(T)w(T) reflect sampling density).
Under mild regularity (no static arbitrage, integrable OTM tails), the following holds.

###### Proposition 2 (Consistency of discretized VIX replication).

Assume (Kâ†¦Qâ€‹(K,T))(K\mapsto Q(K,T)) is convex, Qâ€‹(â‹…,T)/K2Q(\cdot,T)/K^{2} has bounded variation on compact intervals, and the tail contributions
âˆ«0KminPK2â€‹ğ‘‘K\int\_{0}^{K\_{\min}}\!\tfrac{P}{K^{2}}\,dK and âˆ«KmaxâˆCK2â€‹ğ‘‘K\int\_{K\_{\max}}^{\infty}\!\tfrac{C}{K^{2}}\,dK vanish as Kminâ†“0K\_{\min}\downarrow 0, Kmaxâ†‘âˆK\_{\max}\uparrow\infty.
For any family of strike grids {ğ’¦T}\{\mathcal{K}\_{T}\} with mesh Î”â€‹KTâ†’0\Delta K\_{T}\to 0 uniformly on TT in a compact set, the discrete estimator inÂ ([20](https://arxiv.org/html/2511.06451v1#S3.E20 "In SPXâ†”VIX replication. â€£ 3.3 Convexâ€“Monotone Decoder and SPXâ€“VIX Coupling â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) converges uniformly to the continuous variance-swap rate:

|  |  |  |
| --- | --- | --- |
|  | supTâˆˆğ’¯|VIXÎ¸2â€‹(T)âˆ’ÏƒVS,Î¸2â€‹(T)|â‰¤Câ€‹supTÎ”â€‹KT+Îµtailâ€‹(Kmin,Kmax)â†’0,\sup\_{T\in\mathcal{T}}\Big|\mathrm{VIX}^{2}\_{\theta}(T)-\sigma^{2}\_{\mathrm{VS},\theta}(T)\Big|\;\leq\;C\,\sup\_{T}\Delta K\_{T}\;+\;\varepsilon\_{\mathrm{tail}}(K\_{\min},K\_{\max})\;\xrightarrow[]{}0, |  |

for some constant CC independent of TT, where Îµtail\varepsilon\_{\mathrm{tail}} is the integrable tail truncation error.

###### Proposition 3 (Variance-swap identifiability via replication).

Fix a maturity range ğ’¯âŠ‚(0,Tmax]\mathcal{T}\subset(0,T\_{\max}].
Suppose the RN-operator decoder yields a no-arbitrage surface with Kâ†¦QÎ¸â€‹(K,T)K\mapsto Q\_{\theta}(K,T) convex and the replication residualÂ ([21](https://arxiv.org/html/2511.06451v1#S3.E21 "In SPXâ†”VIX replication. â€£ 3.3 Convexâ€“Monotone Decoder and SPXâ€“VIX Coupling â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) satisfies â„›VIXâ€‹(T)=0\mathcal{R}\_{\mathrm{VIX}}(T)=0 for all Tâˆˆğ’¯T\in\mathcal{T}.
Then ÏƒVS,Î¸2â€‹(T)=ÏƒVS,obs2â€‹(T)\sigma^{2}\_{\mathrm{VS},\theta}(T)=\sigma^{2}\_{\mathrm{VS,obs}}(T) for all Tâˆˆğ’¯T\in\mathcal{T}.
If, moreover, the instantaneous variance admits a density vÎ¸â€‹(t)v\_{\theta}(t) and the mapping Tâ†¦1Tâ€‹âˆ«0TvÎ¸â€‹(t)â€‹ğ‘‘tT\mapsto\frac{1}{T}\int\_{0}^{T}v\_{\theta}(t)\,dt is strictly monotone in TT, then vÎ¸v\_{\theta} matches the observed variance in the sense of CesÃ ro means on ğ’¯\mathcal{T}.

*Proof sketches.* PropositionÂ [2](https://arxiv.org/html/2511.06451v1#Thmproposition2 "Proposition 2 (Consistency of discretized VIX replication). â€£ SPXâ†”VIX replication. â€£ 3.3 Convexâ€“Monotone Decoder and SPXâ€“VIX Coupling â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures") follows from convex quadrature error bounds for functions of bounded variation and the Breedenâ€“Litzenberger representation Qâ€²â€²â€‹(K,T)=eâˆ’râ€‹Tâ€‹Kâˆ’2â€‹fÎ¸â€‹(K,T)Q^{\prime\prime}(K,T)=\mathrm{e}^{-rT}\,K^{-2}\,f\_{\theta}(K,T) (distributional derivative), plus integrable OTM tails. PropositionÂ [3](https://arxiv.org/html/2511.06451v1#Thmproposition3 "Proposition 3 (Variance-swap identifiability via replication). â€£ SPXâ†”VIX replication. â€£ 3.3 Convexâ€“Monotone Decoder and SPXâ€“VIX Coupling â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures") uses the log-contract identity for continuous ItÃ´ models, extends to jump-diffusions with the standard jump-compensator term, and invokes strict monotonicity to upgrade equality of CesÃ ro means to pointwise identification almost everywhere. Full details are provided in AppendixÂ A.3.

###### Proposition 4 (Static no-arbitrage and replication consistency).

Assume the decoder C=Î¦C=\Phi satisfies the convexâ€“monotone constraints

|  |  |  |  |
| --- | --- | --- | --- |
|  | âˆ‚Kâ€‹K2Câ€‹(K,T)â‰¥0,âˆ‚TCâ€‹(K,T)â‰¥0\partial\_{KK}^{2}C(K,T)\geq 0,\qquad\partial\_{T}C(K,T)\geq 0 |  | (22) |

for all strikes K>0K>0 and maturities TT on the grid, and that the VIX replication residualÂ ([21](https://arxiv.org/html/2511.06451v1#S3.E21 "In SPXâ†”VIX replication. â€£ 3.3 Convexâ€“Monotone Decoder and SPXâ€“VIX Coupling â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) vanishes on the maturity grid. Then the surface is free of static butterfly and calendar arbitrage on the grid, and the Breedenâ€“Litzenberger implied density

|  |  |  |  |
| --- | --- | --- | --- |
|  | fSTâ€‹(K)=erâ€‹Tâ€‹âˆ‚Kâ€‹K2Câ€‹(K,T)f\_{S\_{T}}(K)=\mathrm{e}^{rT}\,\partial\_{KK}^{2}C(K,T) |  | (23) |

is consistent with the VIX2 functionalÂ ([20](https://arxiv.org/html/2511.06451v1#S3.E20 "In SPXâ†”VIX replication. â€£ 3.3 Convexâ€“Monotone Decoder and SPXâ€“VIX Coupling â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) in the sense that the VIX computed from CC coincides with the replicated variance-swap rate on the grid.
*Sketch.* Convexity in KK and monotonicity in TT exclude butterfly and calendar violations on the grid. The discretized BL relation and the replication identity tie the second derivative to the integrated OTM portfolio. See AppendixÂ A.4 for a complete proof.

#### Numerical projection.

If small violations appear (finite-sample noise), we solve a convex projection

|  |  |  |  |
| --- | --- | --- | --- |
|  | minC^12âˆ‘i,â„“(C^(Ki,Tâ„“)âˆ’C(Ki,Tâ„“))2s.t.C^(â‹…,Tâ„“)convex inK,C^(Ki,â‹…)nondecreasing inT,\min\_{\widehat{C}}\;\frac{1}{2}\sum\_{i,\ell}\big(\widehat{C}(K\_{i},T\_{\ell})-C(K\_{i},T\_{\ell})\big)^{2}\quad\mathrm{s.t.}\quad\widehat{C}(\cdot,T\_{\ell})\ \text{convex in}\ K,\;\;\widehat{C}(K\_{i},\cdot)\ \text{nondecreasing in}\ T, |  | (24) |

via pooled-adjacent-violators in TT and tridiagonal quadratic programming in KK. This preserves first-order fits while restoring gridwise no-arbitrage.

### 3.4 Saddle-Point Training and Safety-Oriented Stopping

#### Objective.

The training objective couples data fit, no-arbitrage penalties, martingale residuals, and replication consistency:

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„’â€‹(Î¸,Î»)=ğ”¼â€‹[â„“â€‹(ğ’¢Î¸â€‹(u),Cobs)]âŸpricing fit+âŸ¨Î»NA,ğ’NAâ€‹(Î¸)âŸ©âŸstatic constraints+âŸ¨Î»mart,â„³RNâ€‹(Î¸)âŸ©âŸmartingale+âŸ¨Î»VIX,â„›VIXâ€‹(Î¸)âŸ©âŸreplication,\mathcal{L}(\theta,\lambda)\;=\;\underbrace{\mathbb{E}\left[\ell\big(\mathcal{G}\_{\theta}(u),\,C\_{\mathrm{obs}}\big)\right]}\_{\text{pricing fit}}\;+\;\underbrace{\langle\lambda\_{\mathrm{NA}},\,\mathcal{C}\_{\mathrm{NA}}(\theta)\rangle}\_{\text{static constraints}}\;+\;\underbrace{\langle\lambda\_{\mathrm{mart}},\,\mathcal{M}\_{\mathrm{RN}}(\theta)\rangle}\_{\text{martingale}}\;+\;\underbrace{\langle\lambda\_{\mathrm{VIX}},\,\mathcal{R}\_{\mathrm{VIX}}(\theta)\rangle}\_{\text{replication}}, |  | (25) |

with dual variables Î»=(Î»NA,Î»mart,Î»VIX)â‰¥0\lambda=(\lambda\_{\mathrm{NA}},\lambda\_{\mathrm{mart}},\lambda\_{\mathrm{VIX}})\geq 0; ğ’NA\mathcal{C}\_{\mathrm{NA}} collects soft constraints induced byÂ ([17](https://arxiv.org/html/2511.06451v1#S3.E17 "In No-arbitrage constraints. â€£ 3.3 Convexâ€“Monotone Decoder and SPXâ€“VIX Coupling â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")).

#### Two-time-scale extragradient.

We employ a two-time-scale update with extragradient prediction:

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | Î¸k+12\displaystyle\theta^{k+\frac{1}{2}} | =Î¸kâˆ’Î·Î¸â€‹âˆ‡Î¸â„’â€‹(Î¸k,Î»k),Î»k+12=[Î»k+Î·Î»â€‹âˆ‡Î»â„’â€‹(Î¸k,Î»k)]+,\displaystyle=\theta^{k}-\eta\_{\theta}\,\nabla\_{\theta}\mathcal{L}(\theta^{k},\lambda^{k}),\qquad\lambda^{k+\frac{1}{2}}=\big[\lambda^{k}+\eta\_{\lambda}\,\nabla\_{\lambda}\mathcal{L}(\theta^{k},\lambda^{k})\big]\_{+}, |  | (26) |
|  |  |  |  |
| --- | --- | --- | --- |
|  | Î¸k+1\displaystyle\theta^{k+1} | =Î¸kâˆ’Î·Î¸â€‹âˆ‡Î¸â„’â€‹(Î¸k+12,Î»k+12),Î»k+1=[Î»k+Î·Î»â€‹âˆ‡Î»â„’â€‹(Î¸k+12,Î»k+12)]+,\displaystyle=\theta^{k}-\eta\_{\theta}\,\nabla\_{\theta}\mathcal{L}\big(\theta^{k+\frac{1}{2}},\lambda^{k+\frac{1}{2}}\big),\qquad\lambda^{k+1}=\big[\lambda^{k}+\eta\_{\lambda}\,\nabla\_{\lambda}\mathcal{L}\big(\theta^{k+\frac{1}{2}},\lambda^{k+\frac{1}{2}}\big)\big]\_{+}, |  |

with Î·Î»\eta\_{\lambda} ramped from a small value to its target to avoid premature constraint domination. Q-Align is applied after each Î¸\theta-update.

#### Martingale stop test and thresholds.

On random (K,T)(K,T) slices we test the discounted martingale increment and accept early stopping if the following hold for at least 10310^{3} consecutive steps:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î”â€‹Gap<10âˆ’3,dualâ€‹residual<10âˆ’3.\Delta\mathrm{Gap}<10^{-3},\qquad\mathrm{dual\;residual}<10^{-3}. |  | (27) |

We also track ratioâ€‹\_â€‹log=logâ¡(primal/dual)\mathrm{ratio\\_log}=\log(\mathrm{primal}/\mathrm{dual}) as a bias diagnostic.

#### Convergence guarantee (noise-stable neighborhood).

Let Fâ€‹(z)=(âˆ‡Î¸â„’â€‹(Î¸,Î»),âˆ’âˆ‡Î»â„’â€‹(Î¸,Î»))F(z)=(\nabla\_{\theta}\mathcal{L}(\theta,\lambda),-\nabla\_{\lambda}\mathcal{L}(\theta,\lambda)) denote the monotone saddle operator in z=(Î¸,Î»)z=(\theta,\lambda). Under (i) global Lipschitzness of FF (byÂ ([13](https://arxiv.org/html/2511.06451v1#S3.E13 "In Lipschitz surrogate via spectral normalization. â€£ 3.1 Riskâ€“Neutral Operator Layer (RN-Operator) â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) and bounded subgradients for constraints), (ii) small multiplicative bias introduced by Q-Align projections, and (iii) bounded gradient noise with variance Ïƒ2\sigma^{2}, standard extragradient theory implies the following.

###### Theorem 1 (Extragradient convergence to a noise ball).

Let F:ğ’µâ†’â„dF:\mathcal{Z}\to\mathbb{R}^{d} be a monotone and LL-Lipschitz operator on a nonempty, closed, convex set ğ’µâŠ‚â„d\mathcal{Z}\subset\mathbb{R}^{d}, and suppose the Q-Align projections are 11-Lipschitz with per-iteration projection error bounded as â€–ekâ€–=ğ’ªâ€‹(Î·Î¸)\|e^{k}\|=\mathcal{O}(\eta\_{\theta}), where eke^{k} aggregates spectral clipping and geometric projection inaccuracies. Consider the projected extragradient iterates with step sizes Î·Î¸,Î·Î»=Î˜â€‹(1/L)\eta\_{\theta},\eta\_{\lambda}=\Theta(1/L),

|  |  |  |  |
| --- | --- | --- | --- |
|  |  | yk=Î ğ’µâ€‹(zkâˆ’Î·â€‹Fâ€‹(zk)+Î¾k+ek),\displaystyle y^{k}=\Pi\_{\mathcal{Z}}\big(z^{k}-\eta F(z^{k})+\xi^{k}+e^{k}\big), |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | zk+1=Î ğ’µâ€‹(zkâˆ’Î·â€‹Fâ€‹(yk)+Î¾~k+e~k),\displaystyle z^{k+1}=\Pi\_{\mathcal{Z}}\big(z^{k}-\eta F(y^{k})+\tilde{\xi}^{k}+\tilde{e}^{k}\big), |  |

where {Î¾k},{Î¾~k}\{\xi^{k}\},\{\tilde{\xi}^{k}\} are martingale-difference noise with ğ”¼â€‹[Î¾kâˆ£â„±k]=0\mathbb{E}[\xi^{k}\mid\mathcal{F}\_{k}]=0, ğ”¼â€‹â€–Î¾kâ€–2â‰¤Ïƒ2\mathbb{E}\|\xi^{k}\|^{2}\leq\sigma^{2} (and similarly for Î¾~k\tilde{\xi}^{k}), and Î ğ’µ\Pi\_{\mathcal{Z}} denotes the Euclidean projection onto ğ’µ\mathcal{Z}. Then the averaged first-order residual satisfies

|  |  |  |
| --- | --- | --- |
|  | min0â‰¤kâ‰¤Kâˆ’1â¡ğ”¼â€‹â€–Fâ€‹(zk)â€–2â‰¤ğ’ªâ€‹(L2â€‹â€–z0âˆ’zâ‹†â€–2K)+ğ’ªâ€‹(Ïƒ2),\min\_{0\leq k\leq K-1}\ \mathbb{E}\,\|F(z^{k})\|^{2}\;\leq\;\mathcal{O}\!\left(\frac{L^{2}\|z^{0}-z^{\star}\|^{2}}{K}\right)\;+\;\mathcal{O}\!\left(\sigma^{2}\right), |  |

where zâ‹†z^{\star} is a solution of the monotone variational inequality associated with the saddle-point problem.
*Sketch.* Combine one-step progress of projected extragradient for monotone Lipschitz VIs with a stability treatment of Q-Align as a nonexpansive perturbation whose cumulative effect is ğ’ªâ€‹(Î·)\mathcal{O}(\eta), and then control the martingale noise via standard Robbinsâ€“Siegmund arguments. See AppendixÂ A.5 for the complete proof.

#### Instrumentation and audit trail.

We continuously log

|  |  |  |
| --- | --- | --- |
|  | Î»lipâ€‹-â€‹before,Î»lipâ€‹-â€‹after,specâ€‹\_â€‹guardâ€‹\_â€‹hits,projectionâ€‹\_â€‹distance,maxâ„“â¡Ïâ€‹(AÎ¸â€‹(Tâ„“))â€‹Î”â€‹tâ„“,ratioâ€‹\_â€‹log,\lambda\_{\mathrm{lip}\text{-}\mathrm{before}},\ \lambda\_{\mathrm{lip}\text{-}\mathrm{after}},\ \mathrm{spec\\_guard\\_hits},\ \mathrm{projection\\_distance},\ \max\_{\ell}\rho(A\_{\theta}(T\_{\ell}))\Delta t\_{\ell},\ \mathrm{ratio\\_log}, |  |

and align them with evaluation metrics (NAS, CNAS, NI, DualGap, Stability, Surfaceâ€“Wasserstein, GenGap at 95th percentile, effective dimension). Stress-to-fail scans, external-validity tests (frozen-hyperparameter reuse), and irreplaceability ablations (removing measure gating, halving rank, disabling Spectral Guard) are run under the same logging schema, forming a traceable chain from geometry to training dynamics to statistical outcomes.

### 3.5 Relation to Selective SSMs and Mamba

Arbiter shares the runtime primitive of selective scans with modern state-space modelsâ€”diagonal-plus-low-rank parametrizations of AÎ¸A\_{\theta}, input-dependent gating, and linear-time recurrencesâ€”yet it decisively departs in *semantics and constraints*. First, the recurrence is interpreted as a riskâ€“neutral Green operator acting on market features, with a learned measure gate wÎ¸w\_{\theta} that internalizes the Radonâ€“Nikodym derivative and enforces martingale consistency during training. Second, Q-Align supplies training-time geometric guarantees: layerwise Lipschitz projection and spectral CFL control establish stability and bound the operator norm end-to-end. Third, the decoder is not a generic readout but an input-convex, maturity-monotone potential tied to SPXâ€“VIX replication. Together these elements move no-arbitrage and change-of-measure from post-hoc cleaning to in-training constraints, while retaining the ğ’ªâ€‹(Lâ€‹m)\mathcal{O}(Lm) computational profile central to selective SSMs.

#### Summary of guarantees.

Under the Q-Align regime and decoder constraints, PropositionsÂ [1](https://arxiv.org/html/2511.06451v1#Thmproposition1 "Proposition 1 (RN-operator stability under Q-Align). â€£ RN-operator stability under Q-Align. â€£ 3.2 Q-Align: Lipschitz Projection and Spectral Guard â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures") andÂ [4](https://arxiv.org/html/2511.06451v1#Thmproposition4 "Proposition 4 (Static no-arbitrage and replication consistency). â€£ SPXâ†”VIX replication. â€£ 3.3 Convexâ€“Monotone Decoder and SPXâ€“VIX Coupling â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures") ensure (i) bounded and Lipschitz RN-operators (stable Green expansion), and (ii) gridwise static no-arbitrage and replication consistency. TheoremÂ [1](https://arxiv.org/html/2511.06451v1#Thmtheorem1 "Theorem 1 (Extragradient convergence to a noise ball). â€£ Convergence guarantee (noise-stable neighborhood). â€£ 3.4 Saddle-Point Training and Safety-Oriented Stopping â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures") further guarantees that saddle-point training converges to a stochastic neighborhood whose radius is controlled by gradient noise and projection errors. In aggregate, these results explain the empirical behavior of Arbiter in SectionsÂ [6](https://arxiv.org/html/2511.06451v1#S6 "6 Experiments â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")â€“[6.2](https://arxiv.org/html/2511.06451v1#S6.SS2 "6.2 Ablations: irreplaceability and breakers â€£ 6 Experiments â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures") and justify the falsifiable metrics reported throughout.

## 4 Theoretical Results

We establish eight results (T1â€“T8) that quantify approximation error, conditioning, identifiability, oracle rates, capacity control, feasibility under spectral safeguards, joint identifiability with VIX replication, and saddle-point convergence under fixed stopping thresholds. Throughout, we work under the standing assumptions of SectionÂ [2](https://arxiv.org/html/2511.06451v1#S2 "2 Setting, Notation, and Testable Assumptions â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures"): (A1) Novikov-to-Kazamaki switching holds at the reported rate; (A2) local HÃ¶lder smoothness of order Î²smooth>0\beta\_{\mathrm{smooth}}>0 for the target operator; (A3) spectral decay governed by Îº\kappa with long-horizon index Ï‡â€‹(Îº)â‰¥0\chi(\kappa)\geq 0; (A4) coverage level cÂ¯âˆˆ(0,1]\underline{c}\in(0,1] on the (K,T)(K,T) grid. The RN-operator ğ’¢Î¸\mathcal{G}\_{\theta} is equipped with Q-Align (layerwise spectral projection and CFL spectral guard), and the decoder is convexâ€“monotone with optional numerical projection to the no-arbitrage cone.

Notation.
Let LL be the number of maturities and mm the operator rank (hidden dimension). Let LgL\_{g} denote the Lipschitz bound of nonlinearities (taken as 11 in practice), and AÎ¸â€‹(Tâ„“)A\_{\theta}(T\_{\ell}) the state transition at maturity Tâ„“T\_{\ell}. Denote â€–Aâ€–2\|A\|\_{2} the spectral norm, Ïâ€‹(A)\rho(A) the spectral radius, and Î”â€‹tâ„“:=Tâ„“+1âˆ’Tâ„“\Delta t\_{\ell}:=T\_{\ell+1}-T\_{\ell}. Define the discrete CFL quantity CFLâ€‹(Tâ„“)=Ïâ€‹(AÎ¸â€‹(Tâ„“))â€‹Î”â€‹tâ„“\mathrm{CFL}(T\_{\ell})=\rho(A\_{\theta}(T\_{\ell}))\Delta t\_{\ell} and CFLmax=maxâ„“â¡CFLâ€‹(Tâ„“)\mathrm{CFL}\_{\max}=\max\_{\ell}\mathrm{CFL}(T\_{\ell}). The effective dimension d^\hat{d} refers to the 90â€“95% spectral energy truncation rank of the Gram operator induced by inputs.

### T1: Approximation Error and Conditioning

###### Theorem 2 (Approximation rate and conditioning).

Let fâ‹†f^{\star} be the target riskâ€“neutral operator mapping features to price surfaces on a compact domain ğ’µâŠ‚â„dz\mathcal{Z}\subset\mathbb{R}^{d\_{z}} with HÃ¶lder regularity Î²smoothâˆˆ(0,1]\beta\_{\mathrm{smooth}}\in(0,1]. There exists a parameter choice Î¸=Î¸â€‹(m)\theta=\theta(m) such that the RN-operator ğ’¢Î¸\mathcal{G}\_{\theta} with rank mm and LL maturities satisfies

|  |  |  |  |
| --- | --- | --- | --- |
|  | infÎ¸â€–ğ’¢Î¸âˆ’fâ‹†â€–L2â€‹(ğ’µ)â‰¤C1â€‹mâˆ’Î²smooth,Îºâ€‹(ğ’¥Î¸)â‰¤C2â€‹(maxâ„“â¡â€–AÎ¸â€‹(Tâ„“)â€–2)â€‹Lgâ€‹m,\inf\_{\theta}\;\|\mathcal{G}\_{\theta}-f^{\star}\|\_{L^{2}(\mathcal{Z})}\;\leq\;C\_{1}\,m^{-\beta\_{\mathrm{smooth}}},\qquad\kappa\big(\mathcal{J}\_{\theta}\big)\;\leq\;C\_{2}\,\big(\max\_{\ell}\|A\_{\theta}(T\_{\ell})\|\_{2}\big)\,L\_{g}\,m, |  | (28) |

where ğ’¥Î¸\mathcal{J}\_{\theta} is the Jacobian of ğ’¢Î¸\mathcal{G}\_{\theta} and Îºâ€‹(â‹…)\kappa(\cdot) is a spectral condition proxy. The constants C1,C2C\_{1},C\_{2} depend only on the domain diameter and curvature bounds of the nonlinearities, but not on LL; the dependence on LL is controlled by the scan through the Green kernel bound (cf. LemmaÂ [1](https://arxiv.org/html/2511.06451v1#Thmlemma1 "Lemma 1 (Green kernel bound). â€£ Spectral safety and discrete Green kernel. â€£ 3.1 Riskâ€“Neutral Operator Layer (RN-Operator) â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")). *Sketch.* Approximation follows by representing fâ‹†f^{\star} via a Green expansion with HÃ¶lder control and matching it with a diagonal-plus-low-rank parameterization of (AÎ¸,BÎ¸)(A\_{\theta},B\_{\theta}); the scan composes LL 1-Lipschitz layers under Q-Align and preserves linear-time complexity. Conditioning tracks the sum of per-step operator norms through the Green kernel Neumann bound and the Lipschitz gate constant LgL\_{g}, yielding the stated linear dependence in mm and independence of LL. Full proof in AppendixÂ B.1.

### T2: Local Identifiability and CRLB-Type Lower Bounds

###### Theorem 3 (Local identifiability and information bound).

Assume the decoder enforces static no-arbitrage and VIX2 replication consistency on the maturityâ€“strike grid, and the input feature process has a nondegenerate covariance operator on ğ’µ\mathcal{Z}. Then there exists a neighborhood ğ’°\mathcal{U} of Î¸â‹†\theta^{\star} such that the RN-operator map Î¸â†¦ğ’¢Î¸\theta\mapsto\mathcal{G}\_{\theta} is injective modulo the finite symmetry group of rank-mm factorizations (permutation and rescaling of atoms). Moreover, for any unbiased estimator Î¸^\widehat{\theta} based on nn i.i.d. samples,

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â€‹[â€–Î¸^âˆ’Î¸â‹†â€–2]â‰¥traceâ€‹(â„â€‹(Î¸â‹†)âˆ’1),\mathbb{E}\!\left[\|\widehat{\theta}-\theta^{\star}\|^{2}\right]\;\geq\;\mathrm{trace}\!\left(\mathcal{I}(\theta^{\star})^{-1}\right), |  | (29) |

where â„â€‹(Î¸â‹†)\mathcal{I}(\theta^{\star}) denotes the Fisher information of the induced RN-operator under the data-generating distribution.

#### Proof sketch.

(i) *Decoder identifiability.* The Breedenâ€“Litzenberger identity links the second derivative in strike to the risk-neutral density. Together with VIX2 replication consistency, this pins down the decoderâ€™s measure-valued output across maturities. (ii) *Propagation through the scan.* If two parameterizations yield identical price surfaces for almost every input, then their Green responses must coincide on the grid. Under nondegenerate input covariance and the uniform Green bound (LemmaÂ [1](https://arxiv.org/html/2511.06451v1#Thmlemma1 "Lemma 1 (Green kernel bound). â€£ Spectral safety and discrete Green kernel. â€£ 3.1 Riskâ€“Neutral Operator Layer (RN-Operator) â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")), this forces equality of the low-rank scan parameters up to permutation and atom rescaling symmetries. (iii) *Information bound.* Local asymptotic normality holds for the price-slice likelihood with Gateaux derivative equal to the RN-operator Jacobian; the score is square-integrable by Q-Alignâ€™s Lipschitz control. The CramÃ©râ€“Rao lower bound then gives ([29](https://arxiv.org/html/2511.06451v1#S4.E29 "In Theorem 3 (Local identifiability and information bound). â€£ T2: Local Identifiability and CRLB-Type Lower Bounds â€£ 4 Theoretical Results â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")). Full details are provided in AppendixÂ B.2.

### T2â€²: Representative-Element Error Under Coverage Deficits

###### Proposition 5 (Representative bound with coverage and residuals).

Let covâˆˆ[0,1]\mathrm{cov}\in[0,1] denote the fraction of strikeâ€“maturity cells covered by reliable quotes. Let Î³>0\gamma>0 be the martingale penalization strength and let dualâ‰¥0\mathrm{dual}\geq 0 be the dual residual at stopping. Then the representative-element error obeys

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–Î»Îµâˆ’Î»â‹†â€–L2â€‹(ğ’µ)â‰¤C3â€‹((1âˆ’cov)âˆ’1â€‹Îµ+Î³âˆ’1+dual),\|\lambda\_{\varepsilon}-\lambda^{\star}\|\_{L^{2}(\mathcal{Z})}\;\leq\;C\_{3}\Big(\,(1-\mathrm{cov})^{-1}\varepsilon\;+\;\gamma^{-1}\;+\;\mathrm{dual}\,\Big), |  | (30) |

where Î»\lambda indexes the operator-induced risk measure and Îµ\varepsilon bounds the interpolation error on missing strikes.

#### Proof sketch.

Partition the grid into covered and uncovered cells. The first term controls the imputation bias: extending prices from the covered set to the full grid by a linear, no-arbitrageâ€“preserving interpolant yields an L2L^{2} error that scales as (1âˆ’cov)âˆ’1â€‹Îµ(1-\mathrm{cov})^{-1}\varepsilon due to the stability modulus of the extension operator on sparse masks. The second term is the bias from enforcing the martingale constraint via a penalty of strength Î³\gamma, which leaves an Oâ€‹(Î³âˆ’1)O(\gamma^{-1}) feasibility gap by first-order optimality. The third term converts a nonzero KKT residual at termination into a distance-to-solution via a Hoffman-type error bound. The RN-operator is globally Lipschitz under Q-Align and Spec-Guard; hence all three perturbations transport to Î»\lambda with a uniform constant. Full details and constants appear in AppendixÂ B.3.

### T3: Oracle Risk Bounds with Long-Memory Factor

###### Theorem 4 (Oracle rate with scan and memory).

Let d^\hat{d} be the effective dimension of the input Gram operator and Î”â€‹t:=maxâ„“â¡Î”â€‹tâ„“\Delta t:=\max\_{\ell}\Delta t\_{\ell}. Under Q-Align and decoder constraints, the prediction risk of the oracle estimator with rank mm and nn samples satisfies

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„›n,mâ‰¤C4â€‹(nâˆ’1/2+mâˆ’Î²smooth/d^+Î”â€‹t)+C5â€‹TÏ‡â€‹(Îº),\mathcal{R}\_{n,m}\;\leq\;C\_{4}\Big(n^{-1/2}\;+\;m^{-\beta\_{\mathrm{smooth}}/\hat{d}}\;+\;\sqrt{\Delta t}\Big)\;+\;C\_{5}\,T^{\chi(\kappa)}, |  | (31) |

where TT is the horizon and Ï‡â€‹(Îº)â‰¥0\chi(\kappa)\geq 0 quantifies long-memory spectral accumulation. The first three terms are short-horizon effects; the last term captures the asymptotic tail induced by spectral mass at small decay rates.

*Sketch.*
The stochastic term nâˆ’1/2n^{-1/2} derives from standard central limit rates under bounded variance; the approximation term mâˆ’Î²/d^m^{-\beta/\hat{d}} follows from T1 with effective dimension; the discretization term Î”â€‹t\sqrt{\Delta t} arises from Riemann-sum error in the scan. The long-memory penalty TÏ‡â€‹(Îº)T^{\chi(\kappa)} appears when eigenvalues near one accumulate according to A3. AppendixÂ C provides a spectral decomposition proof.

### T4â€“T5: Capacity via Rademacher and a Sampleâ€“Seminorm Bridge

###### Lemma 2 (Rademacher complexity with Lipschitz and projection).

Let â„±\mathcal{F} be the class of RN-operators obeying Q-Align projections with a global Lipschitz constant Î›\Lambda. Then for sample size nn,

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„œnâ€‹(â„±)â‰¤C6â€‹Î›â€‹dimeffn,\mathfrak{R}\_{n}(\mathcal{F})\;\leq\;C\_{6}\,\Lambda\,\sqrt{\frac{\mathrm{dim}\_{\mathrm{eff}}}{n}}, |  | (32) |

where dimeffâ‰¤d^\mathrm{dim}\_{\mathrm{eff}}\leq\hat{d} is the energy-truncation rank at the sample scale.

#### Proof sketch.

Project the trajectories onto the top energy subspace of rank dimeff\mathrm{dim}\_{\mathrm{eff}} defined by the Gram operator of the Green kernel. Under Q-Align+Spec-Guard the RN-operator is globally Î›\Lambda-Lipschitz, hence the function class admits a Lipschitz envelope on a radius-11 domain (normalization). Dudley chaining with covering numbers of a dimeff\mathrm{dim}\_{\mathrm{eff}}-dimensional ball yields the stated rate. Full details appear in AppendixÂ B.4.

###### Lemma 3 (Bridge from sample to seminorm).

Let âˆ¥â‹…âˆ¥n\|\cdot\|\_{n} be the empirical norm on the observed grid and âˆ¥â‹…âˆ¥â„‹\|\cdot\|\_{\mathcal{H}} a seminorm induced by the RN-operatorâ€™s Green kernel. Under A4 and a linear no-arbitrageâ€“preserving interpolation with error Îµ\varepsilon, with high probability,

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–fâ€–â„‹â‰¤C7â€‹â€–fâ€–n+C8â€‹((1âˆ’cov)âˆ’1â€‹Îµ),\|f\|\_{\mathcal{H}}\;\leq\;C\_{7}\,\|f\|\_{n}\;+\;C\_{8}\Big((1-\mathrm{cov})^{-1}\varepsilon\Big), |  | (33) |

uniformly over ff in the model class.

#### Proof sketch.

Bound the kernel seminorm by the operator norm of the discrete Green Gram, which is finite by LemmaÂ [1](https://arxiv.org/html/2511.06451v1#Thmlemma1 "Lemma 1 (Green kernel bound). â€£ Spectral safety and discrete Green kernel. â€£ 3.1 Riskâ€“Neutral Operator Layer (RN-Operator) â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures") and PropositionÂ [1](https://arxiv.org/html/2511.06451v1#Thmproposition1 "Proposition 1 (RN-operator stability under Q-Align). â€£ RN-operator stability under Q-Align. â€£ 3.2 Q-Align: Lipschitz Projection and Spectral Guard â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures"). Decompose the grid into covered cells and their complement; the extension operator from the covered set is linear and stable on the no-arbitrage cone, with amplification scaling as (1âˆ’cov)âˆ’1(1-\mathrm{cov})^{-1}. Concentrate the empirical-to-population deviation via standard symmetrization and the class complexity from LemmaÂ [2](https://arxiv.org/html/2511.06451v1#Thmlemma2 "Lemma 2 (Rademacher complexity with Lipschitz and projection). â€£ T4â€“T5: Capacity via Rademacher and a Sampleâ€“Seminorm Bridge â€£ 4 Theoretical Results â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures"). Full proof is in AppendixÂ B.5.

### T6: Feasibility and Two-Time-Scale Averaging under Spectral Guard

###### Proposition 6 (Feasibility and error propagation).

Suppose Q-Align enforces â€–Wâ„“â€–2â‰¤Ï„â‰¤1\|W\_{\ell}\|\_{2}\leq\tau\leq 1 and Ïâ€‹(AÎ¸â€‹(Tâ„“))â€‹Î”â€‹tâ„“â‰¤1âˆ’Îµ\rho\!\big(A\_{\theta}(T\_{\ell})\big)\,\Delta t\_{\ell}\leq 1-\varepsilon for all â„“\ell. Then the iterative scan is well-posed, the discrete Green expansion is summable, and the one-step error is contractive:

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–hâ„“+1âˆ’h~â„“+1â€–â‰¤(1âˆ’Îµ)â€‹â€–hâ„“âˆ’h~â„“â€–+â€–Bâ€–â€‹â€–Îâ€–â€‹â€–uâ„“âˆ’u~â„“â€–.\|h\_{\ell+1}-\tilde{h}\_{\ell+1}\|\;\leq\;(1-\varepsilon)\,\|h\_{\ell}-\tilde{h}\_{\ell}\|\;+\;\|B\|\,\|\Xi\|\,\|u\_{\ell}-\tilde{u}\_{\ell}\|. |  | (34) |

Moreover, for two-time-scale averaging of the primalâ€“dual iterates (Î¸k,Î»k)(\theta\_{k},\lambda\_{k}) in the saddle dynamics with bounded gradient noise, the averaged gap enjoys a variance reduction of order ğ’ªâ€‹(1/K)\mathcal{O}(1/K) after KK steps.

#### Proof sketch.

Write the scan as hâ„“+1=(I+Î”â€‹tâ„“â€‹Aâ„“)â€‹hâ„“+Wâ„“â€‹Ï•â€‹(hâ„“)+Bâ€‹uâ„“h\_{\ell+1}=(I+\Delta t\_{\ell}A\_{\ell})h\_{\ell}+W\_{\ell}\phi(h\_{\ell})+Bu\_{\ell}. By Spec-Guard, for each â„“\ell there exists an induced norm under which â€–I+Î”â€‹tâ„“â€‹Aâ„“â€–â‰¤1âˆ’Îµ\|I+\Delta t\_{\ell}A\_{\ell}\|\leq 1-\varepsilon; Q-Align caps â€–Wâ„“â€–â‰¤Ï„â‰¤1\|W\_{\ell}\|\leq\tau\leq 1 and Ï•\phi is nonexpansive. A mean-value bound on the step map shows its Jacobian norm is â‰¤1âˆ’Îµ\leq 1-\varepsilon, giving ([34](https://arxiv.org/html/2511.06451v1#S4.E34 "In Proposition 6 (Feasibility and error propagation). â€£ T6: Feasibility and Two-Time-Scale Averaging under Spectral Guard â€£ 4 Theoretical Results â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) after adding the input term. Summability of the Green series follows from the Neumann-type bound (LemmaÂ [1](https://arxiv.org/html/2511.06451v1#Thmlemma1 "Lemma 1 (Green kernel bound). â€£ Spectral safety and discrete Green kernel. â€£ 3.1 Riskâ€“Neutral Operator Layer (RN-Operator) â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")). The two-time-scale variance reduction follows from standard TTSA analysis with monotone operators and bounded noise. Full proofs are given in AppendixÂ B.6 (contractivity and summability) and AppendixÂ B.7 (TTSA variance decay).

### T7: Joint Identifiability with VIX2 Replication and a SPX-Only Counterexample

###### Theorem 5 (Joint identifiability; SPX-only failure).

Suppose the decoder CÎ¸â€‹(K,T)C\_{\theta}(K,T) is convex in KK and nonincreasing in TT for each maturity TT, and the discretized VIX2 replication residual (cf. ([20](https://arxiv.org/html/2511.06451v1#S3.E20 "In SPXâ†”VIX replication. â€£ 3.3 Convexâ€“Monotone Decoder and SPXâ€“VIX Coupling â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures"))â€“([21](https://arxiv.org/html/2511.06451v1#S3.E21 "In SPXâ†”VIX replication. â€£ 3.3 Convexâ€“Monotone Decoder and SPXâ€“VIX Coupling â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures"))) vanishes on the maturity grid {Tâ„“}â„“=1L\{T\_{\ell}\}\_{\ell=1}^{L}. Then the pair *(SPX calls on a strike grid, VIX2 per maturity)* identifies the induced riskâ€“neutral operator ğ’¢Î¸\mathcal{G}\_{\theta} up to model symmetries (reparameterizations that leave CÎ¸C\_{\theta} invariant on the grid).

In contrast, using SPX calls on the strike grid alone, without imposing replication consistency, there exist nontrivial perturbations of the RN-operator that preserve all grid call prices yet alter the induced variance-swap functional.

#### Proof sketch.

On each maturity, the Breedenâ€“Litzenberger (BL) relation implies that second strike differences of CÎ¸C\_{\theta} recover the discrete riskâ€“neutral density on the grid. The VIX2 replication functional is a linear functional of out-of-the-money option values with weights 1/K21/K^{2}; matching it eliminates degrees of freedom left in the tails/inter-knot segments that are not pinned down by grid values alone. Under convexity/monotonicity and our interpolation policy, the combined constraints (grid calls ++ replication) fix both local (BL) and integrated (VIX) aspects of the law, yielding injectivity modulo symmetries.

For SPX-only, the measurement operator that samples calls on a finite strike grid has a nontrivial null space in the ambient function class; by a separation argument (or an explicit bump construction supported between grid knots), one can perturb the surface without changing its values at the grid points but changing the 1/K21/K^{2}-weighted integral, hence the variance swap rate. Full details and constructions are in AppendixÂ C.

### T8: Saddle-Point Convergence with Fixed Safety Thresholds

###### Theorem 6 (Convergence to a noise ball under fixed thresholds).

Consider the extragradient two-time-scale scheme with Q-Align projections and fixed stopping thresholds

|  |  |  |
| --- | --- | --- |
|  | Î”â€‹Gap<10âˆ’3,dualâ€‹residual<10âˆ’3,patienceâ‰¥103â€‹Â steps.\Delta\mathrm{Gap}<10^{-3},\qquad\mathrm{dual\;residual}<10^{-3},\qquad\text{patience}\geq 10^{3}\text{ steps}. |  |

Assume the saddle operator Fâ€‹(z)F(z) is monotone and LL-Lipschitz, projections are nonexpansive, and gradient noise has variance Ïƒ2\sigma^{2}. Then the averaged iterates satisfy

|  |  |  |  |
| --- | --- | --- | --- |
|  | minkâ‰¤Kâ¡â€–Fâ€‹(zk)â€–2â‰¤ğ’ªâ€‹(L2â€‹â€–z0âˆ’zâ‹†â€–2K)+ğ’ªâ€‹(Ïƒ2),\min\_{k\leq K}\ \|F(z^{k})\|^{2}\;\leq\;\mathcal{O}\!\left(\frac{L^{2}\|z^{0}-z^{\star}\|^{2}}{K}\right)+\mathcal{O}(\sigma^{2}), |  | (35) |

and the stopping rule almost surely terminates inside a ball of radius c1â€‹Ïƒ+c2â€‹Î´projc\_{1}\sigma+c\_{2}\delta\_{\mathrm{proj}} around a saddle point, where Î´proj\delta\_{\mathrm{proj}} quantifies per-step projection error.

#### Proof sketch (for TheoremÂ [6](https://arxiv.org/html/2511.06451v1#Thmtheorem6 "Theorem 6 (Convergence to a noise ball under fixed thresholds). â€£ T8: Saddle-Point Convergence with Fixed Safety Thresholds â€£ 4 Theoretical Results â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")).

We analyze the two-time-scale projected extragradient (EG) with Q-Align as a nonexpansive projection with bounded defect. A FejÃ©r-type oneâ€“step inequality for monotone, LL-Lipschitz FF yields a telescoping bound on squared distances to a saddle zâ‹†z^{\star}, plus additive terms from gradient noise and projection error. Using â€–Fâ€‹(z)â€–â‰¤Lâ€‹â€–zâˆ’zâ‹†â€–\|F(z)\|\leq L\|z-z^{\star}\| to convert distance decay into a residual bound gives the stated ğ’ªâ€‹(L2â€‹â€–z0âˆ’zâ‹†â€–2/K)+ğ’ªâ€‹(Ïƒ2)\mathcal{O}(L^{2}\|z^{0}-z^{\star}\|^{2}/K)+\mathcal{O}(\sigma^{2}) rate (also for the ergodic average). Fixed thresholds on the primalâ€“dual gap and dual residual upper-bound the merit residual, so the procedure almost surely terminates inside a ball of radius c1â€‹Ïƒ+c2â€‹Î´projc\_{1}\sigma+c\_{2}\delta\_{\mathrm{proj}}. Full details appear in AppendixÂ D.

Discussion.
T1 establishes that *operator semantics do not sacrifice* universal approximation rates relative to rank-mm kernels, while providing explicit conditioning that is tractable to monitor. T2 and T7 formalize identifiability *because* the decoder is tied to replication. T2â€² quantifies the inevitable error under partial coverage and suboptimal dual solutions, directly justifying the empirical regressions of gap versus representative error. T3â€“T5 connect sample complexity to effective dimension and long-memory structure, and T6â€“T8 ensure that Q-Alignâ€™s projections and our fixed stopping thresholds lead to stable, falsifiable training.

## 5 Evaluation Protocol and Metrics

We describe the arXiv-release evaluation protocol, designed to be fully reproducible and aligned with the modeling choices and theoretical guarantees in SectionsÂ [3](https://arxiv.org/html/2511.06451v1#S3 "3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")â€“[4](https://arxiv.org/html/2511.06451v1#S4 "4 Theoretical Results â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures"). The protocol relies on a high-fidelity synthetic generator that emulates riskâ€“neutral dynamics and the varianceâ€“swap replication mechanism, evaluated under blocked crossâ€“validation with rolling out-of-sample (OOS) windows. All criteria are dimensionless and comparable across runs; uncertainty is reported with heteroskedasticityâ€“ and autocorrelationâ€“consistent (HAC) confidence intervals and familyâ€“wise error control via Holmâ€“Bonferroni.

### 5.1 Data Protocol (arXiv Release)

#### Synthetic riskâ€“neutral generator.

Under the pricing measure â„š\mathbb{Q}, the underlying index StS\_{t} and instantaneous variance vtv\_{t} evolve on a trading day grid {ti}i=0N\{t\_{i}\}\_{i=0}^{N} as

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | dâ€‹StSt\displaystyle\frac{\mathrm{d}S\_{t}}{S\_{t}} | =(râˆ’q)â€‹dâ€‹t+vtâ€‹dâ€‹Wt(1),S0>0,\displaystyle=\bigl(r-q\bigr)\,\mathrm{d}t+\sqrt{v\_{t}}\,\mathrm{d}W\_{t}^{(1)},\quad S\_{0}>0, |  | (36) |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | vt\displaystyle v\_{t} | =v0+âˆ«0tÎºâ€‹(Î¸âˆ’vs)â€‹dsâŸaffine mean reversion+âˆ«0tâˆ«0sKâ€‹(sâˆ’u)â€‹Ïƒâ€‹vuâ€‹dWu(2)â€‹dsâŸrough/long-memory component,\displaystyle=v\_{0}\;+\;\underbrace{\int\_{0}^{t}\kappa\bigl(\theta-v\_{s}\bigr)\,\mathrm{d}s}\_{\text{affine mean reversion}}\;+\;\underbrace{\int\_{0}^{t}\int\_{0}^{s}K(s-u)\,\sigma\sqrt{v\_{u}}\,\mathrm{d}W\_{u}^{(2)}\,\mathrm{d}s}\_{\text{rough/long-memory component}}, |  | (37) |

with instantaneous correlation dâ€‹âŸ¨W(1),W(2)âŸ©t=Ïâ€‹dâ€‹t\mathrm{d}\langle W^{(1)},W^{(2)}\rangle\_{t}=\rho\,\mathrm{d}t, dividend yield qq, and a completely monotone kernel
Kâ€‹(Ï„)=âˆ‘j=1Jajâ€‹eâˆ’bjâ€‹Ï„K(\tau)=\sum\_{j=1}^{J}a\_{j}e^{-b\_{j}\tau} that reproduces fractional/rough behavior by a positive mixture of exponentials. This yields an arbitrageâ€“free implied variance termâ€“structure and a VIX2 proxy

|  |  |  |  |
| --- | --- | --- | --- |
|  | VIX2â€‹(T)â‰ˆ2Î”â€‹âˆ«0Î”ğ”¼â„šâ€‹[vT+sâˆ£â„±T]â€‹ds,Î”â‰ˆ30 days.\mathrm{VIX}^{2}(T)\approx\frac{2}{\Delta}\int\_{0}^{\Delta}\mathbb{E}^{\mathbb{Q}}\!\left[v\_{T+s}\mid\mathcal{F}\_{T}\right]\mathrm{d}s,\qquad\Delta\approx\text{30 days}. |  | (38) |

Option quotes are generated on a Cartesian grid ğ’¯Ã—ğ’¦\mathcal{T}\times\mathcal{K} with maturities Tâˆˆ{Tâ„“}â„“=1LT\in\{T\_{\ell}\}\_{\ell=1}^{L} and strikes Kâˆˆ{Kj}j=1MK\in\{K\_{j}\}\_{j=1}^{M}, ensuring no static arbitrage at the oracle level. To emulate market frictions, we add microstructure noise ÎµT,K\varepsilon\_{T,K} with heteroskedastic variance and censor illiquid deep OTM quotes:

|  |  |  |  |
| --- | --- | --- | --- |
|  | C~â€‹(T,K)=(Câ‹†â€‹(T,K)+ÎµT,K)â€‹â€‰1â€‹{Câ‹†â€‹(T,K)â‰¥Ï„liqâ€‹(T,K)},ğ”¼â€‹[ÎµT,K]=0,\widetilde{C}(T,K)=\bigl(C^{\star}(T,K)+\varepsilon\_{T,K}\bigr)\,\mathbf{1}\{C^{\star}(T,K)\geq\tau\_{\mathrm{liq}}(T,K)\},\quad\mathbb{E}[\varepsilon\_{T,K}]=0, |  | (39) |

where Câ‹†C^{\star} is the oracle call price and Ï„liq\tau\_{\mathrm{liq}} is a maturityâ€“ and moneynessâ€“dependent liquidity floor.

#### Blocked crossâ€“validation and rolling OOS.

We split the synthetic timeline into BB contiguous blocks of equal length. In fold bâˆˆ{1,â€¦,B}b\in\{1,\dots,B\}, blocks 1:(bâˆ’1)1{:}(b{-}1) form the training set, block bb is the validation set (earlyâ€“stopping and model selection), and blocks (b+1):B(b{+}1){:}B are scored OOS with a rolling horizon. This enforces temporal causality and reduces leakage. All random seeds and block boundaries are recorded.

#### Normalization and grids.

Prices are evaluated in forward units to avoid numeraire drift. The maturity set ğ’¯\mathcal{T} matches the scan length LL used by the RNâ€“operator; the strike grid ğ’¦\mathcal{K} spans log-moneyness k=logâ¡(K/S0)âˆˆ[kmin,kmax]k=\log(K/S\_{0})\in[k\_{\min},k\_{\max}] with nearly uniform coverage. Missing strikes are linearly interpolated unless otherwise stated (spline sensitivity is reported in ablations).

### 5.2 Primary Metrics (dimensionless)

Let C^â€‹(Tâ„“,Kj)\widehat{C}(T\_{\ell},K\_{j}) denote modelâ€“implied call prices after the convexâ€“monotone decoder, and let Câ‹†â€‹(Tâ„“,Kj)C^{\star}(T\_{\ell},K\_{j}) denote the oracle (or heldâ€“out) price. All metrics lie in [0,1][0,1] unless indicated and are constructed so that larger is better (arrows â€œâ†‘\uparrowâ€) or smaller is better (arrows â€œâ†“\downarrowâ€) are explicit.

#### Normalized Arbitrage Score (NAS, â†‘\uparrow).

NAS quantifies the fraction of the staticâ€“arbitrage axioms satisfied by C^\widehat{C} with a soft penalty:

|  |  |  |  |
| --- | --- | --- | --- |
|  | NAS=â€„1âˆ’1ZNASâ€‹âˆ‘â„“,j[(âˆ‚KC^)+âŸmonotonicity+(âˆ’âˆ‚Kâ€‹KC^)+âŸconvexity+(âˆ‚TC^)+âŸcalendar],\mathrm{NAS}\;=\;1-\frac{1}{Z\_{\mathrm{NAS}}}\sum\_{\ell,j}\Bigl[\underbrace{\bigl(\partial\_{K}\widehat{C}\bigr)\_{+}}\_{\text{monotonicity}}\;+\;\underbrace{\bigl(-\partial\_{KK}\widehat{C}\bigr)\_{+}}\_{\text{convexity}}\;+\;\underbrace{\bigl(\partial\_{T}\widehat{C}\bigr)\_{+}}\_{\text{calendar}}\Bigr], |  | (40) |

where (x)+=maxâ¡{x,0}(x)\_{+}=\max\{x,0\} and ZNASZ\_{\mathrm{NAS}} rescales by the grid measure to make the score dimensionless.

#### Calibrated NAS (CNAS, â†‘\uparrow).

CNAS introduces a threeâ€“parameter penalty shaping with curvatureâ€“slope coupling:

|  |  |  |  |
| --- | --- | --- | --- |
|  | CNASâ€‹(Îº,Ï„,scale)=â€„1âˆ’1ZCNASâ€‹âˆ‘â„“,jÏˆÎº,Ï„,scaleâ€‹((âˆ‚KC^)+,(âˆ’âˆ‚Kâ€‹KC^)+,(âˆ‚TC^)+),\mathrm{CNAS}(\kappa,\tau,\mathrm{scale})\;=\;1-\frac{1}{Z\_{\mathrm{CNAS}}}\sum\_{\ell,j}\psi\_{\kappa,\tau,\mathrm{scale}}\!\left(\bigl(\partial\_{K}\widehat{C}\bigr)\_{+},\bigl(-\partial\_{KK}\widehat{C}\bigr)\_{+},\bigl(\partial\_{T}\widehat{C}\bigr)\_{+}\right), |  | (41) |

with Ïˆ\psi a smooth, saturating hinge whose stiffness Îº\kappa, tolerance Ï„\tau, and scaling factor scale\mathrm{scale} are fixed across all runs and used for sensitivity analysis.

#### Numeraire Integrity (NI, â†‘\uparrow).

Divide the panel into 8Ã—48\times 4 buckets in maturities and moneyness. For each bucket bb, compute the discountedâ€“forward residual variance of singleâ€“step price increments and aggregate

|  |  |  |  |
| --- | --- | --- | --- |
|  | NI=â€„1âˆ’âˆ‘bwbâ€‹Varâ€‹(Î”â€‹C^bfwd)âˆ‘bwbâ€‹Varâ€‹(Î”â€‹Cbfwd,â‹†)+Ïµ,\mathrm{NI}\;=\;1-\frac{\sum\_{b}w\_{b}\,\mathrm{Var}\bigl(\Delta\widehat{C}\_{b}^{\mathrm{fwd}}\bigr)}{\sum\_{b}w\_{b}\,\mathrm{Var}\bigl(\Delta C\_{b}^{\mathrm{fwd},\star}\bigr)+\epsilon}, |  | (42) |

with positive weights wbw\_{b} (volume/openâ€“interest or uniform) and small Ïµ\epsilon for numerical stability.

#### Primalâ€“Dual Gap (DualGap, â†“\downarrow).

Let â„’â€‹(Î¸,Î»)\mathcal{L}(\theta,\lambda) be the saddle objective with martingale and noâ€“arbitrage constraints. Report the OOS gap at the chosen validation stop:

|  |  |  |  |
| --- | --- | --- | --- |
|  | DualGap=supÎ»â„’â€‹(Î¸sel,Î»)âˆ’infÎ¸â„’â€‹(Î¸,Î»sel).\mathrm{DualGap}\;=\;\sup\_{\lambda}\mathcal{L}(\theta\_{\mathrm{sel}},\lambda)\;-\;\inf\_{\theta}\mathcal{L}(\theta,\lambda\_{\mathrm{sel}}). |  | (43) |

#### Stability (fraction, â†‘\uparrow).

The proportion of training runs that (i) satisfy the spectral safety test maxâ„“â¡Ïâ€‹(AÎ¸â€‹(Tâ„“))â€‹Î”â€‹tâ„“â‰¤1\max\_{\ell}\rho(A\_{\theta}(T\_{\ell}))\Delta t\_{\ell}\leq 1 at all steps, (ii) pass the martingale randomized stop test, and (iii) terminate within the fixed thresholds in SectionÂ [4](https://arxiv.org/html/2511.06451v1#S4 "4 Theoretical Results â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures") (T8).

#### Surfaceâ€“Wasserstein (distance, â†“\downarrow).

A sliced 2D Wasserstein distance between model and oracle price panels, normalized by the area of ğ’¯Ã—ğ’¦\mathcal{T}\times\mathcal{K}:

|  |  |  |  |
| --- | --- | --- | --- |
|  | SW=1|Î˜|â€‹âˆ‘Î¸âˆˆÎ˜W2â€‹({C^â€‹(Tâ„“,Kj)}â„“,jâ‹…Î¸,{Câ‹†â€‹(Tâ„“,Kj)}â„“,jâ‹…Î¸),\mathrm{SW}\;=\;\frac{1}{|\Theta|}\sum\_{\theta\in\Theta}W\_{2}\!\left(\bigl\{\widehat{C}(T\_{\ell},K\_{j})\bigr\}\_{\ell,j}\cdot\theta,\bigl\{C^{\star}(T\_{\ell},K\_{j})\bigr\}\_{\ell,j}\cdot\theta\right), |  | (44) |

where Î˜\Theta is a set of random projection directions.

#### GenGap@95 (quantile, â†“\downarrow).

Across rolling OOS windows, compute the absolute generalization gap |C^âˆ’Câ‹†|\lvert\widehat{C}-C^{\star}\rvert aggregated over ğ’¯Ã—ğ’¦\mathcal{T}\times\mathcal{K} and report its empirical 95th percentile.

#### Effective dimension (d^\hat{d}).

Let GG be the empirical Gram matrix of inputs; define d^Î±\hat{d}\_{\alpha} as the smallest rr such that
âˆ‘i=1rÎ»iâ€‹(G)â‰¥Î±â€‹âˆ‘iÎ»iâ€‹(G)\sum\_{i=1}^{r}\lambda\_{i}(G)\,\geq\,\alpha\sum\_{i}\lambda\_{i}(G), with Î±âˆˆ{0.90,0.95,0.99}\alpha\in\{0.90,0.95,0.99\}.

### 5.3 Statistical Inference and Display Conventions

#### HAC confidence intervals.

For any metric sequence {Mt}\{M\_{t}\} along wallâ€“clock time, we report 95% confidence intervals using a Neweyâ€“West estimator with lag
LHAC=âŒŠcâ€‹T1/4âŒ‹L\_{\mathrm{HAC}}=\lfloor c\,T^{1/4}\rfloor (default c=1c=1), robust to heteroskedasticity and serial dependence.

#### Multiple comparisons.

For families of hypotheses across metrics or configurations, we apply Holmâ€“Bonferroni at level Î±=0.05\alpha=0.05: order raw ppâ€“values as p(1)â‰¤â‹¯â‰¤p(m)p\_{(1)}\leq\cdots\leq p\_{(m)}, and reject H(k)H\_{(k)} if p(k)â‰¤Î±/(mâˆ’k+1)p\_{(k)}\leq\alpha/(m{-}k{+}1) sequentially.

#### Wallâ€“clock xâ€“axis.

All panel curves are plotted against wallâ€“clock time to normalize for variable throughput across models; each point corresponds to a fixed evaluation batch size and a consistent logging interval.

### 5.4 Budget, Scans, and Reproducibility

#### Training route and fixed thresholds.

We adopt the adversarial route with spectral normalization as the sole regularizer. Stopping thresholds are fixed:

|  |  |  |
| --- | --- | --- |
|  | Î”â€‹Gap<10âˆ’3,dual residual<10âˆ’3,patienceâ‰¥103.\Delta\text{Gap}<10^{-3},\qquad\text{dual residual}<10^{-3},\qquad\text{patience}\geq 10^{3}. |  |

Evaluation batch size is held constant across baselines.

#### Default hyperparameters and sweep.

Unless stated, the penalty weights are (Î³,Î²nov,Î¾)=(1.0,â€‰0.1,â€‰0.5)(\gamma,\beta\_{\mathrm{nov}},\xi)=(1.0,\,0.1,\,0.5). We explore a grid of seeds and learningâ€“rate multipliers; every trial logs (i) metric trajectories, (ii) spectral safety counters (hits, projection distance, maximum Ïâ€‹Î”â€‹t\rho\,\Delta t), (iii) coverage statistics (minimum and mean coverage), and (iv) effective dimensions at {90%,95%,99%}\{90\%,95\%,99\%\} energy. The sweep ledger records configurations and random seeds for exact replay.

#### Crossâ€“validation ledger and OOS evaluation.

For each fold, we archive the selected checkpoint, the validation earlyâ€“stop index, HAC interval parameters, and the OOS window boundaries. GenGap@95 and Surfaceâ€“Wasserstein are computed exclusively on OOS windows.

#### Ablations and stressâ€“toâ€“fail.

We run controlled ablations that disable the gate, halve the RNâ€“operator rank, or turn off the spectral guard, and report their impact on NAS, CNAS, and Stability. Stressâ€“toâ€“fail tests increase distortion strength until NAS drops below a threshold (e.g., 0.90.9), logging the failure point and confidence intervals.

#### Release artifacts.

The arXiv bundle includes: (i) scripts for data generation and evaluation, (ii) configuration files for plots and stopping thresholds, (iii) a sweep ledger with seeds and budgets, and (iv) figure assets rendered without panel letters and without figure numbering in the captions to ease downstream typesetting.

Together, these choices ensure that (a) the evaluation is falsifiable and aligned with the theoretical safety conditions, (b) comparisons are budgetâ€“fair and dimensionless, and (c) every number and figure can be regenerated verbatim from the public release.

## 6 Experiments

#### Compute, code, and seeds.

All figures in this section are generated by the visualization scripts described in Â§[3](https://arxiv.org/html/2511.06451v1#S3 "3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures") using the checkpoint and summary provided in the arXiv package.
We report blocked timeâ€“series confidence intervals (95% HAC-CI) and adjust family-wise error via Holmâ€“Bonferroni.
Default sweep hyper-parameters are (Î³,Î²nov,Î¾)=(1.0,0.1,0.5)(\gamma,\beta\_{\mathrm{nov}},\xi)=(1.0,0.1,0.5) with seeds logged in sweeps.csv.
Unless noted otherwise, wall-clock time is used on the xx-axis for curve plots.

### 6.1 Primary results on the synthetic SPXâ€“VIX generator

#### Point estimates and uncertainty.

On the held-out test split our model attains:

|  |  |  |
| --- | --- | --- |
|  | NAS=0.9866[0.98653,â€‰0.98664],CNAS=0.99022[0.99018,â€‰0.99027],\mathrm{NAS}=0.9866\ \ [0.98653,\,0.98664],\qquad\mathrm{CNAS}=0.99022\ \ [0.99018,\,0.99027], |  |

|  |  |  |
| --- | --- | --- |
|  | NI=0.67757[0.67733,â€‰0.67768],Stability=1.0000,\mathrm{NI}=0.67757\ \ [0.67733,\,0.67768],\qquad\mathrm{Stability}=1.0000, |  |

|  |  |  |
| --- | --- | --- |
|  | DualGap=0.06034[0.05833,â€‰0.05891],Surfaceâˆ’Wasserstein=0.08727[0.08703,â€‰0.08746],\mathrm{DualGap}=0.06034\ \ [0.05833,\,0.05891],\qquad\mathrm{Surface\!-\!Wasserstein}=0.08727\ \ [0.08703,\,0.08746], |  |

|  |  |  |
| --- | --- | --- |
|  | GenGapâ€‹@â€‹95=0.25031[0.24982,â€‰0.25079],\mathrm{GenGap}@95=0.25031\ \ [0.24982,\,0.25079], |  |

with two-sided p<10âˆ’3p\!<\!10^{-3} for NAS/CNAS/NI improvements under Holmâ€“Bonferroni.
These values are consistent across validation and test (see Â§[6.5](https://arxiv.org/html/2511.06451v1#S6.SS5 "6.5 Robustness and additional diagnostics â€£ 6 Experiments â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")).

![Refer to caption](core_metrics_clean.png)


Figure 1: Core metrics with 95% HAC-CI.
NAS, CNAS, and NI are reported as point estimates with HAC-CI bands.
The dashed line at 1.01.0 highlights normalization for NAS/CNAS.

#### Pricing structure and implied volatility geometry.

FigureÂ [2](https://arxiv.org/html/2511.06451v1#S6.F2 "Figure 2 â€£ Pricing structure and implied volatility geometry. â€£ 6.1 Primary results on the synthetic SPXâ€“VIX generator â€£ 6 Experiments â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures") shows normalized pricing curves across maturities for three legs derived from the decoder output.
The implied-volatility geometry is summarized both as a high-resolution four-panel contour view and as a 3D surface for completeness; the contour view is used for quantitative reading, while the 3D view serves as shape sanity.

![Refer to caption](pricing_curves.png)


Figure 2: Pricing curves across maturities.
Three legs (legend) exhibit smooth-in-TT behavior with monotone structure consistent with the convexâ€“monotone decoder.

![Refer to caption](iv_multiview.png)


Figure 3: Implied-volatility (IV) contours (multi-view).
Top-left: filled contours in (T,K)(T,K); top-right: line contours with labeled levels; bottom-left: filled contours in (T,logâ¡(K/S0))(T,\log(K/S\_{0})); bottom-right: IV slices Ïƒâ€‹(K)\sigma(K) at selected maturities.
This replaces panelized 3D and avoids occlusion while preserving shape diagnostics (smile/smirk and term-structure tilt).

![Refer to caption](iv_surface_3d.png)


Figure 4: Model-implied volatility surface (3D).
A complementary view to Fig.Â [3](https://arxiv.org/html/2511.06451v1#S6.F3 "Figure 3 â€£ Pricing structure and implied volatility geometry. â€£ 6.1 Primary results on the synthetic SPXâ€“VIX generator â€£ 6 Experiments â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures") confirming smoothness across (T,K)(T,K) and the absence of butterfly/time-arbitrage artifacts on the synthetic generator.

#### Spectral safety and projection geometry.

Our Q-Align projection sharply reduces the global Lipschitz surrogate from Î»lipbefore=1299.27\lambda\_{\text{lip}}^{\text{before}}\!=\!1299.27 to Î»lipafter=0.70\lambda\_{\text{lip}}^{\text{after}}\!=\!0.70 with projection distance â‰ˆ53.32\approx 53.32 and 6969 Spec-Guard hits recorded during training, indicating effective clipping of spectral outliers while keeping the iterate near the feasible set.

![Refer to caption](lipschitz_projection.png)


Figure 5: Spectral Guard & projection effect.
Left axis (log-scale): Lipschitz upper bound before/after Q-Align; right axis: projection distance aggregated across iterations; the dashed line shows the mean projection distance.

#### Stress-to-Fail (S2F).

FigureÂ [6](https://arxiv.org/html/2511.06451v1#S6.F6 "Figure 6 â€£ Stress-to-Fail (S2F). â€£ 6.1 Primary results on the synthetic SPXâ€“VIX generator â€£ 6 Experiments â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures") reports NAS under increasing distortion strength.
The threshold at 2.02.0 (vertical line) marks the onset at which NAS approaches 0.9790.979; the confidence band reflects HAC-CI across random distortions.
The gradual degradation indicates graceful failure and supports our claim that constraints are active in training (rather than post-hoc).

![Refer to caption](s2f_curve.png)


Figure 6: Stress-to-Fail (S2F).
NAS vs. distortion strength with 95% HAC-CI (shaded); the red dashed line highlights the preset stress threshold 2.02.0.

#### Effective dimension.

The spectrum of the kernel Gram matrix yields effective dimensions d90=1d\_{90}\!=\!1, d95=1d\_{95}\!=\!1, d99=2d\_{99}\!=\!2 (Fig.Â [7](https://arxiv.org/html/2511.06451v1#S6.F7 "Figure 7 â€£ Effective dimension. â€£ 6.1 Primary results on the synthetic SPXâ€“VIX generator â€£ 6 Experiments â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")), suggesting that the risk-neutral operator concentrates on a remarkably low-dimensional manifold under our synthetic generator.

![Refer to caption](effective_dimension.png)


Figure 7: Effective dimension at 90/95/99% variance.
The operator acts on a low-dimensional manifold, explaining the fast rates in Â§[4](https://arxiv.org/html/2511.06451v1#S4 "4 Theoretical Results â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures").

#### Assumption monitoring.

We log the Novikov-to-Kazamaki switch rate across time blocks to empirically validate AssumptionÂ A1 (Fig.Â [8](https://arxiv.org/html/2511.06451v1#S6.F8 "Figure 8 â€£ Assumption monitoring. â€£ 6.1 Primary results on the synthetic SPXâ€“VIX generator â€£ 6 Experiments â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")): the mean is 0.91750.9175 with a 95% CI [0.9020,â€‰0.9330][0.9020,\,0.9330].

![Refer to caption](assumption_a1.png)


Figure 8: A1 monitoring: Novikovâ†’\toKazamaki switch rate (blocked).
The dashed line marks the mean 0.91750.9175.

### 6.2 Ablations: irreplaceability and breakers

We examine three structural switches: *gate off*, *rank half*, and *Spec-Guard off* (Fig.Â [9](https://arxiv.org/html/2511.06451v1#S6.F9 "Figure 9 â€£ 6.2 Ablations: irreplaceability and breakers â€£ 6 Experiments â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")).
Relative to the base:

* â€¢

  Turning the gate off reduces NAS from 0.98660.9866 to 0.89180.8918 (â†“9.6%\!\downarrow\!9.6\%) and CNAS from 0.99020.9902 to 0.90390.9039; NI drops from 0.67760.6776 to 0.51920.5192 (â†“23.4%\!\downarrow\!23.4\%). DualGap worsens from 0.0600.060 to 0.2210.221 (Ã—3.7\!\times\!3.7), and Surfaceâ€“Wasserstein from 0.0870.087 to 0.2990.299 (Ã—3.4\!\times\!3.4).
* â€¢

  Halving the kernel rank leads to collapse: NAS â‰ˆ0.0079\approx 0.0079, CNAS â‰ˆ0.0047\approx 0.0047, NI â‰ˆâˆ’0.527\approx-0.527, stability =0=0 and large geometry errors.
* â€¢

  Disabling Spec-Guard produces NAS =0.5551=0.5551 and CNAS =0.5824=0.5824 with stability =0=0 and pronounced surface artifacts (Surfaceâ€“Wasserstein â‰ˆ0.590\approx 0.590).

These effects are consistent with our theory: removing either measure gating or spectral feasibility breaks the martingale geometry and convexâ€“monotone decoder guarantees.

![Refer to caption](ablation_effects.png)


Figure 9: Ablation effects on normalized metrics.
Relative change w.r.t. base for NAS, CNAS, NI and DualGap under *gate off*, *rank half*, and *Spec-Guard off*.

### 6.3 External validity: frozen-hyperparameter reuse

With (Îº,Ï„,scale)(\kappa,\tau,\mathrm{scale}) frozen and reused across disjoint OOS windows, the mean CNAS drop is cnasâ€‹\_â€‹frozenâ€‹\_â€‹drop=3.87Ã—10âˆ’4\mathrm{cnas\\_frozen\\_drop}=3.87\times 10^{-4}, with window-wise CNAS {0.99008,â€‰0.99013,â€‰0.99063,â€‰0.99103}\{0.99008,\,0.99013,\,0.99063,\,0.99103\}.
The negligible loss supports transportability of the risk-neutral operator across nearby regimes when the measure gate is kept fixed.

### 6.4 Consolidated table results

TableÂ [1](https://arxiv.org/html/2511.06451v1#S6.T1 "Table 1 â€£ 6.4 Consolidated table results â€£ 6 Experiments â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures") lists the primary metrics together with stability and geometry statistics; TableÂ [2](https://arxiv.org/html/2511.06451v1#S6.T2 "Table 2 â€£ 6.4 Consolidated table results â€£ 6 Experiments â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures") compares adversarial (ours) vs. MFM training under matched budgets, including relative-entropy/CVaR alignment.
Per our logging protocol, the safety fields spec\_guard\_hits, projection\_distance, max\_rho\_dt and the coverage diagnostics are included.

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
| Metric | Val | Test | HAC-CI (Test) | Direction |
| NAS | 0.9866 | 0.9866 | [0.98653,â€‰0.98664] | â†‘\uparrow |
| CNAS | 0.99022 | 0.99022 | [0.99018,â€‰0.99027] | â†‘\uparrow |
| NI | 0.67754 | 0.67757 | [0.67733,â€‰0.67768] | â†‘\uparrow |
| Stability | 1.000 | 1.000 | [1.000,â€‰1.000] | â†‘\uparrow |
| DualGap | 0.05864 | 0.06034 | [0.05833,â€‰0.05891] | â†“\downarrow |
| Surf.-Wasserstein | 0.08721 | 0.08727 | [0.08703,â€‰0.08746] | â†“\downarrow |
| GenGap@95 | 0.25035 | 0.24875 | [0.24982,â€‰0.25079] | â†“\downarrow |
| spec\_guard\_hits | 69 | | | |
| projection\_distance | 53.32 | | | |
| Î»lipbeforeâ†’Î»lipafter\lambda\_{\text{lip}}^{\text{before}}\!\to\!\lambda\_{\text{lip}}^{\text{after}} | 1299.27 â†’\to 0.70 | | | |
| coveragemin/coveragemean | 0.802/0.919 | | | |

Table 1: Primary metrics and safety logs with 95% HAC-CI.



| Route | Rel. Entropy | CVaR align | Notes |
| --- | --- | --- | --- |
| Adversarial (ours) | âœ“\checkmark | âœ“\checkmark | SN only; Spec-Guard on |
| MFM (matched budget) | â‰ˆ\approx | â‰ˆ\approx | Residual curves logged |
| SPXâ€“VIXâ€“VVIX (ext.) | âœ“\checkmark | âœ“\checkmark | Placeholder in arXiv version |

Table 2: Training-route comparison under unified budget; see AppendixÂ H for fairness ledger.

### 6.5 Robustness and additional diagnostics

We verify that (i) HAC bandwidth choices do not materially affect CI width; (ii) Holmâ€“Bonferroni remains conservative under overlapping metric families; (iii) convergence to the saddle point satisfies the fixed stopping thresholds (primal\_dual\_tol\_delta=10âˆ’3=10^{-3}, dual\_residual\_eps=10âˆ’3=10^{-3}) with patience 10001000; (iv) coverage logs do not trigger the representer fallback.
Additional seeds and stress families are reported in the appendix.

## 7 Mechanistic Analysis and Diagnostics

This section explains *why* ARBITER behaves robustly under the synthetic SPXâ€“VIX generator, connecting the observed logs and figures in Â§[6](https://arxiv.org/html/2511.06451v1#S6 "6 Experiments â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures") to the constraints and operator geometry established in Â§[3](https://arxiv.org/html/2511.06451v1#S3 "3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")â€“Â§[4](https://arxiv.org/html/2511.06451v1#S4 "4 Theoretical Results â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures").

### 7.1 Q-Align contraction and spectral safety

Denote by Î»lip\lambda\_{\mathrm{lip}} the global Lipschitz surrogate of the network mapping in the ambient parameter metric induced by spectral normalization.
Q-Align projects each iterate onto the feasible cone

|  |  |  |
| --- | --- | --- |
|  | ğ’Lip={Î¸:Lipâ€‹(fÎ¸)â‰¤1}\mathcal{C}\_{\mathrm{Lip}}\;=\;\{\,\theta:\ \mathrm{Lip}(f\_{\theta})\leq 1\,\} |  |

via a firmly non-expansive operator Î ğ’Lip\Pi\_{\mathcal{C}\_{\mathrm{Lip}}} applied layerwise.
Empirically (Fig.Â [5](https://arxiv.org/html/2511.06451v1#S6.F5 "Figure 5 â€£ Spectral safety and projection geometry. â€£ 6.1 Primary results on the synthetic SPXâ€“VIX generator â€£ 6 Experiments â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")), we observe a *two-and-a-half orders of magnitude* contraction

|  |  |  |
| --- | --- | --- |
|  | Î»lipbefore=1299.27âŸ¶Î»lipafter=0.70,\lambda\_{\mathrm{lip}}^{\mathrm{before}}=1299.27\quad\longrightarrow\quad\lambda\_{\mathrm{lip}}^{\mathrm{after}}=0.70, |  |

yielding the contraction ratio

|  |  |  |
| --- | --- | --- |
|  | ÏLip=Î»lipafterÎ»lipbeforeâ‰ˆâ€„5.39Ã—10âˆ’4,Îºsafety=logâ¡(Î»lipbeforeÎ»lipafter)â‰ˆâ€„7.53.\rho\_{\mathrm{Lip}}\;=\;\frac{\lambda\_{\mathrm{lip}}^{\mathrm{after}}}{\lambda\_{\mathrm{lip}}^{\mathrm{before}}}\;\approx\;5.39\times 10^{-4},\qquad\kappa\_{\mathrm{safety}}\;=\;\log\!\Big(\frac{\lambda\_{\mathrm{lip}}^{\mathrm{before}}}{\lambda\_{\mathrm{lip}}^{\mathrm{after}}}\Big)\;\approx\;7.53. |  |

Since the constraint is Lipâ€‹(f)â‰¤1\mathrm{Lip}(f)\!\leq\!1, the post-projection *safety headroom* equals

|  |  |  |
| --- | --- | --- |
|  | Î”headroom=â€„1âˆ’Î»lipafterâ‰ˆâ€„0.30,\Delta\_{\mathrm{headroom}}\;=\;1-\lambda\_{\mathrm{lip}}^{\mathrm{after}}\;\approx\;0.30, |  |

which prevents near-boundary oscillation of the saddle dynamics.
Spec-Guard implements a spectral CFL test, triggering a corrective projection when maxtâ¡Ïâ€‹(At)â€‹Î”â€‹tt\max\_{t}\rho(A\_{t})\,\Delta t\_{t} exceeds the budget; we logged 6969 hits and an accumulated *projection distance* of â‰ˆ53.32\approx 53.32.

#### Generalization implication.

Let the loss â„“â€‹(â‹…,y)\ell(\cdot,y) be Lâ„“L\_{\ell}-Lipschitz and bounded by BB.
For any sample set SS and an independent ghost sample Sâ€²S^{\prime}, the uniform stability of the projected update (gradient step followed by Q-Align and spectral guard) satisfies

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î²stabâ‰²Lâ„“â€‹Î»lipafternâ€‹(diamâ€‹(ğ’³)+distÎ ),distÎ â‰¡1Tâ€‹âˆ‘t=1Tdistâ€‹(Î¸t,Î ğ’Lipâ€‹(Î¸t)),\beta\_{\mathrm{stab}}\;\lesssim\;\frac{L\_{\ell}\,\lambda\_{\mathrm{lip}}^{\mathrm{after}}}{n}\,\Big(\mathrm{diam}(\mathcal{X})+\mathrm{dist}\_{\Pi}\Big),\qquad\mathrm{dist}\_{\Pi}\;\equiv\;\frac{1}{T}\sum\_{t=1}^{T}\mathrm{dist}\!\big(\theta\_{t},\Pi\_{\mathcal{C}\_{\mathrm{Lip}}}(\theta\_{t})\big), |  | (45) |

where distâ€‹(â‹…,â‹…)\mathrm{dist}(\cdot,\cdot) is the ambient metric and TT is the number of updates.
Combining ([45](https://arxiv.org/html/2511.06451v1#S7.E45 "In Generalization implication. â€£ 7.1 Q-Align contraction and spectral safety â€£ 7 Mechanistic Analysis and Diagnostics â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) with a standard stability-to-generalization bound yields

|  |  |  |
| --- | --- | --- |
|  | |â„°â€‹(fÎ¸^)âˆ’â„°^â€‹(fÎ¸^)|â‰²Î²stab+ğ’ªâ€‹(logâ¡(1/Î´)n),w.p.Â â€‹1âˆ’Î´.\big|\mathcal{E}(f\_{\hat{\theta}})-\widehat{\mathcal{E}}(f\_{\hat{\theta}})\big|\;\lesssim\;\beta\_{\mathrm{stab}}+\mathcal{O}\!\big(\sqrt{\tfrac{\log(1/\delta)}{n}}\big),\quad\text{w.p.\ }1-\delta. |  |

Hence the observed contraction (Î»lipafterâ‰ˆ0.70\lambda\_{\mathrm{lip}}^{\mathrm{after}}\!\approx\!0.70) and finite projection budget (distÎ â‰ˆ53.32\mathrm{dist}\_{\Pi}\!\approx\!53.32) directly tighten the generalization gap.
*Proof sketch.* Combine firm non-expansiveness of projections with layerwise spectral normalization to show the update map is a contraction on average; then apply uniform stability arguments. Full details are deferred to AppendixÂ I.

### 7.2 Representer mode under coverage deficiency

Let câˆˆ[0,1]c\in[0,1] denote the effective coverage of the (T,K)(T,K) mesh by observed quotes after preprocessing.
When cc falls below the operational threshold cÂ¯=0.75\underline{c}=0.75, ARBITER switches to a *representer* fallback in the RN-Operator layer, which is recorded by the timestamps

|  |  |  |
| --- | --- | --- |
|  | enterâ€‹\_â€‹representerâ€‹\_â€‹atâ€‹\_â€‹step,coverageâ€‹\_â€‹atâ€‹\_â€‹trigger.\mathrm{enter\\_representer\\_at\\_step},\qquad\mathrm{coverage\\_at\\_trigger}. |  |

TheoryÂ T2â€² (Â§[4](https://arxiv.org/html/2511.06451v1#S4 "4 Theoretical Results â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) upper-bounds the induced error by a combination of coverage shortfall, regularization, and dual residual:

|  |  |  |
| --- | --- | --- |
|  | â„°repâ‰¤C1â€‹(1âˆ’c)+C2â€‹Î³âˆ’1+C3â€‹Î”dual.\mathcal{E}\_{\mathrm{rep}}\;\leq\;C\_{1}(1-c)+C\_{2}\gamma^{-1}+C\_{3}\,\Delta\_{\mathrm{dual}}. |  |

To verify this mechanism we regress the *representer approximation error* against the empirical dual gap (blocked OLS with HAC covariance):

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„°rep=Î±â‹…Gap+Î²+Îµ,Î±^=0.47,95% CIÂ â€‹[0.41,0.53],p<10âˆ’5.\mathcal{E}\_{\mathrm{rep}}\;=\;\alpha\cdot\mathrm{Gap}+\beta+\varepsilon,\qquad\widehat{\alpha}=0.47,\;\text{95\% CI }[0.41,0.53],\;p<10^{-5}. |  | (46) |

The positive slope confirms that the fallback error scales linearly with the dual violation, as predicted by T2â€²; the intercept Î²^\widehat{\beta} captures the coverage and regularization contributions when Gapâ†’0\mathrm{Gap}\!\to\!0.
We further checked that *no* fallback was triggered in the main synthetic run (cmin=0.802c\_{\min}=0.802, cmean=0.919c\_{\mathrm{mean}}=0.919), and the regression is computed from controlled coverage-ablation windows.

### 7.3 Effective dimension and sampleâ€“compute budgeting

Let KK be the kernel Gram matrix of RN-Operator features along the training mesh and define the effective dimension

|  |  |  |
| --- | --- | --- |
|  | deffâ€‹(Ï„)=minâ¡{d:âˆ‘i=1dÎ»iâ€‹(K)âˆ‘iâ‰¥1Î»iâ€‹(K)â‰¥Ï„},Ï„âˆˆ{0.90,0.95,0.99}.d\_{\mathrm{eff}}(\tau)\;=\;\min\Big\{d:\ \frac{\sum\_{i=1}^{d}\lambda\_{i}(K)}{\sum\_{i\geq 1}\lambda\_{i}(K)}\geq\tau\Big\},\quad\tau\in\{0.90,0.95,0.99\}. |  |

Empirically (Fig.Â [7](https://arxiv.org/html/2511.06451v1#S6.F7 "Figure 7 â€£ Effective dimension. â€£ 6.1 Primary results on the synthetic SPXâ€“VIX generator â€£ 6 Experiments â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")),

|  |  |  |
| --- | --- | --- |
|  | d90=1,d95=1,d99=2,d\_{90}=1,\qquad d\_{95}=1,\qquad d\_{99}=2, |  |

which indicates that the risk-neutral operator acts on a low-dimensional manifold under the generator.
This observation connects to the oracle rate in T3:

|  |  |  |
| --- | --- | --- |
|  | â€–fÎ¸^âˆ’fâ‹†â€–L2â‰²nâˆ’1/2+mâˆ’Î²/d^+Î”â€‹t+Î˜â€‹(TÏ‡â€‹(Îº)),\|f\_{\hat{\theta}}-f^{\star}\|\_{L^{2}}\;\lesssim\;n^{-1/2}\;+\;m^{-\beta/\hat{d}}\;+\;\sqrt{\Delta t}\;+\;\Theta\!\big(T^{\chi(\kappa)}\big), |  |

so that (i) doubling the discretization budget mm reduces the approximation term at rate mâˆ’Î²/d^m^{-\beta/\hat{d}} with d^â‰¤2\hat{d}\!\leq\!2, and (ii) computational cost grows only linearly in Lâ€‹mLm due to the RN-Operator construction.
Practically, with d^âˆˆ{1,2}\hat{d}\in\{1,2\} the learned measure gate removes redundant directions, explaining both the flatness of NAS/CNAS curves across wall-clock in Fig.Â [1](https://arxiv.org/html/2511.06451v1#S6.F1 "Figure 1 â€£ Point estimates and uncertainty. â€£ 6.1 Primary results on the synthetic SPXâ€“VIX generator â€£ 6 Experiments â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures") and the graceful S2F degradation in Fig.Â [6](https://arxiv.org/html/2511.06451v1#S6.F6 "Figure 6 â€£ Stress-to-Fail (S2F). â€£ 6.1 Primary results on the synthetic SPXâ€“VIX generator â€£ 6 Experiments â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures").

#### Failure signatures and diagnostic cross-links.

The ablation patterns in Fig.Â [9](https://arxiv.org/html/2511.06451v1#S6.F9 "Figure 9 â€£ 6.2 Ablations: irreplaceability and breakers â€£ 6 Experiments â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures") align with the above mechanisms:
(i) disabling the gate increases the effective dimension and violates the martingale geometry, inflating the dual gap and the IV geometry error;
(ii) removing Spec-Guard raises Î»lip\lambda\_{\mathrm{lip}}, shrinks the safety headroom Î”headroom\Delta\_{\mathrm{headroom}}, and destabilizes the saddle dynamics; and
(iii) rank halving impoverishes the Green kernel family, producing underfitting that manifests as elevated Surfaceâ€“Wasserstein and reduced CNAS.
Together with the coverage logs and the regressionÂ ([46](https://arxiv.org/html/2511.06451v1#S7.E46 "In 7.2 Representer mode under coverage deficiency â€£ 7 Mechanistic Analysis and Diagnostics â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")), these diagnostics form a closed evidence loop linking constraints, operator geometry, and observed metrics.

## 8 Related Work

We organize prior art into three threads and position ARBITER accordingly: (i) *operator learning* for scientific ML; (ii) *linear-time state-space sequence models* (SSMs), including the Mamba family; and (iii) *arbitrage-free term-structure modeling* and *deep calibration*. Our method departs by enforcing *risk-neutral geometry at training time*: a measure-consistent Green operator (RN-Operator), a Lipschitz/spectral safety stack (Q-Align + Spec-Guard), and an economically constrained decoder (convex in strike KK, monotone in maturity TT). This contrasts with post-hoc repairs or penalty-only pipelines.

### 8.1 Operator learning: accuracy, physics, and stability

Neural operators approximate maps between function spaces with resolution-invariant inference. The *Fourier Neural Operator* (FNO) introduced spectral convolutional layers that learn continuous kernels in Fourier space and established a new accuracyâ€“efficiency frontier for PDE families [[58](https://arxiv.org/html/2511.06451v1#bib.bib58)]. *DeepONet* proved universal approximation theorems for nonlinear operators and popularized branchâ€“trunk factorization [[59](https://arxiv.org/html/2511.06451v1#bib.bib59)]. The survey of [[60](https://arxiv.org/html/2511.06451v1#bib.bib60)] synthesized approximation, training, and generalization aspects and highlighted stability pitfalls.

Beyond FNO/DeepONet, researchers pursued locality, structure preservation, and robustness: message-passing neural PDE solvers [[61](https://arxiv.org/html/2511.06451v1#bib.bib61)] and graph-based simulators [[68](https://arxiv.org/html/2511.06451v1#bib.bib68)] improved inductive bias for conservation laws; multiwavelet/wavelet neural operators exploited compact harmonic support to mitigate Gibbs artifacts on discontinuities [[63](https://arxiv.org/html/2511.06451v1#bib.bib63)]; U-shaped neural operators (U-NO) brought multi-scale skip connections that sharpen high-frequency reconstruction [[62](https://arxiv.org/html/2511.06451v1#bib.bib62)]. Physics-informed neural operators (PINO) added residual penalties that reduce data requirements on stiff dynamics [[64](https://arxiv.org/html/2511.06451v1#bib.bib64), [65](https://arxiv.org/html/2511.06451v1#bib.bib65)]. Recent works also address stability/generalization via operator-theoretic bounds and coercivity assumptions [[66](https://arxiv.org/html/2511.06451v1#bib.bib66), [67](https://arxiv.org/html/2511.06451v1#bib.bib67)].

Positioning. The above systems are *physics-governed*. In contrast, option surfaces are *economically-governed* by no-arbitrage, martingale, and numÃ©raire geometry. ARBITER reinterprets selective scan as a *risk-neutral Green operator* with *measure gating*, trains it under *explicit Lipschitz and spectral constraints* (Q-Align, Spec-Guard), and decodes via *convexâ€“monotone* potentials. This geometry-first stack is closer in spirit to *safety-critical operator learning* than to unconstrained FNO/DeepONet, and yields *arbitrage-free* surfaces even under stress (Sec.Â [6](https://arxiv.org/html/2511.06451v1#S6 "6 Experiments â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")).

### 8.2 SSMs and the Mamba family: from long-range recurrence to measure-consistent scan

Structured state space models (SSMs) revived linear-time sequence modeling. S4 [[69](https://arxiv.org/html/2511.06451v1#bib.bib69)] exploited HiPPO theory to parameterize long convolutions; follow-ups simplified or sped up kernels [[70](https://arxiv.org/html/2511.06451v1#bib.bib70), [71](https://arxiv.org/html/2511.06451v1#bib.bib71)]. Hyena [[72](https://arxiv.org/html/2511.06451v1#bib.bib72)] realized implicit long convolutions with subquadratic memory; RetNet replaced attention with multiplicative retention [[73](https://arxiv.org/html/2511.06451v1#bib.bib73)]. Most relevant, *Mamba* introduced *selective state spaces*â€”input-gated linear recurrences that train in linear time and scale to LLMs [[74](https://arxiv.org/html/2511.06451v1#bib.bib74)]. Variants rapidly percolated to vision and speech (*VMamba* and derivatives) [[75](https://arxiv.org/html/2511.06451v1#bib.bib75), [76](https://arxiv.org/html/2511.06451v1#bib.bib76)].

Connectionâ€“difference. We *share* the *runtime primitive* of a linear-time scan but *change its semantics*: selective gating becomes a *measure gate* for the risk-neutral density. Q-Align applies *training-time projections* (1-Lip and CFL spectral bounds) that record certificates {Î»lip,specâ€‹\_â€‹guardâ€‹\_â€‹hits,maxâ€‹\_â€‹rhoâ€‹\_â€‹dt}\{\lambda\_{\text{lip}},\ \mathrm{spec\\_guard\\_hits},\ \mathrm{max\\_rho\\_dt}\}, which do not appear in standard SSM stacks. The result is a *measure-consistent operator* rather than a generic sequence encoder. Empirically, replacing measure-consistent gates with vanilla gates sharply increases dual gaps and breaks stability (our ablations), indicating *non-interchangeability*.

### 8.3 Arbitrage-free term structures and deep calibration

Rigorous constructions of arbitrage-free implied-volatility (IV) surfaces study absence of calendar/spread/Butterfly arbitrage and convex order; recent advances include [[77](https://arxiv.org/html/2511.06451v1#bib.bib77)] and [[78](https://arxiv.org/html/2511.06451v1#bib.bib78)]. On the data side, the VIX white paper details replication of variance swaps and implementation nuances [[47](https://arxiv.org/html/2511.06451v1#bib.bib47)]. Learning-based smoothing with explicit no-arbitrage constraints was investigated by [[48](https://arxiv.org/html/2511.06451v1#bib.bib48)]. For *deep calibration*, rough- and hybrid-volatility models saw efficient surrogates and uncertainty-aware estimation [[79](https://arxiv.org/html/2511.06451v1#bib.bib79), [85](https://arxiv.org/html/2511.06451v1#bib.bib85)]. Neural differential methodsâ€”Neural CDEs and SDEsâ€”help with irregular time grids and stochastic dynamics [[83](https://arxiv.org/html/2511.06451v1#bib.bib83), [84](https://arxiv.org/html/2511.06451v1#bib.bib84)]. Generative transport methods (*OT-Flow*, *flow matching*, *rectified flows*) offer fast simulators and well-behaved gradients for calibration and synthetic data [[80](https://arxiv.org/html/2511.06451v1#bib.bib80), [81](https://arxiv.org/html/2511.06451v1#bib.bib81), [82](https://arxiv.org/html/2511.06451v1#bib.bib82)]. Recent work on *martingale optimal transport* connects no-arbitrage calibration, convex order, and dual certificates [[86](https://arxiv.org/html/2511.06451v1#bib.bib86), [87](https://arxiv.org/html/2511.06451v1#bib.bib87)].

Positioning. Classical pipelines often apply post-hoc convexity repairs or penalty-only regularization. ARBITER *internalizes* risk-neutral constraints at the operator and decoder levels, with *training-time certificates*. Our evaluation emphasizes *dimensionless* metrics with HAC-CI and Holmâ€“Bonferroni control (NAS, CNAS, NI, Stability, DualGap, Surfaceâ€“Wasserstein, GenGap@95), plus *S2F thresholds* and *external validity* (frozen-hyperparameter reuse). This combinationâ€”operator-level geometry + safety certificates + rigorous evaluationâ€”appears absent from prior operator-learning, SSM, and calibration literatures.

#### Concluding remark.

Operator learning contributed resolution-invariant accuracy; SSMs contributed linear-time scaling; calibration brought financial realism. ARBITER integrates the three via a *risk-neutral, geometry-aware neural operator* with provable safety and identifiability guarantees, demonstrating robustness under ablations and stress.

## 9 Conclusion and Outlook

#### Summary.

We introduced ARBITER, a *risk-neutral neural operator* for arbitrage-free SPXâ€“VIX term structures that relocates financial geometry from post-hoc repair to the *training objective*. The core stack comprises: (i) a risk-neutral Green operator (RN-Operator) that endows selective scan with the semantics of a measure-consistent integral kernel; (ii) *Q-Align*, a training-time safety layer that enforces 11-Lipschitzness (spectral normalization + projection) and a CFL-style *Spec-Guard* on the state transition spectrum; and (iii) a convexâ€“monotone decoder (ICNN + Legendre transform) guaranteeing convexity in strike and monotonicity in maturity. These design choices are supported by a suite of dimensionless metrics with rigorous uncertainty accounting (NAS, CNAS, NI, Stability, DualGap, Surfaceâ€“Wasserstein, GenGap@95 with HAC-CI and Holmâ€“Bonferroni control).

#### Theoretical guarantees.

Our analysis established approximation and conditioning bounds (T1), identifiability in L2â€‹(ğ’µ)L^{2}(\mathcal{Z}) neighborhoods with a CramÃ©râ€“Rao style lower bound (T2), a representative-element upper bound under coverage shortfall (T2â€²), oracle rates that mix sample complexity and discretization error for short/long horizons (T3), Rademacher and bridge-type generalization (T4â€“T5), feasibility and stability of TTSA training under Spec-Guard (T6), joint identifiability once VIX2 replication constraints are incorporated (T7), and a saddle-point stopping rule with variance control (T8). Proof sketches were provided in the main text, with full derivations deferred to the appendix. Collectively, these results certify that the learned operator is (i) well-posed, (ii) geometrically feasible, and (iii) statistically efficient under the stated assumptions.

#### Empirical evidence.

On the arXiv versionâ€™s high-fidelity synthetic protocol (blocked CV + rolling OOS), ARBITER attains strong point estimates and tight confidence regions (e.g., NAS â‰ˆ0.9866\approx 0.9866, CNAS â‰ˆ0.9902\approx 0.9902, NI â‰ˆ0.6776\approx 0.6776, Stability â‰ˆ1.0\approx 1.0, DualGap â‰ˆ0.060\approx 0.060, Surfaceâ€“Wasserstein â‰ˆ0.087\approx 0.087), while respecting no-arbitrage geometry in the IV contour views and pricing curves. The safety stack is *measurably binding*: Q-Align shrinks the global Lipschitz bound from âˆ¼1.3Ã—103\sim 1.3\times 10^{3} to âˆ¼0.70\sim 0.70 with projection distance â‰ˆ53\approx 53, and Spec-Guard records bounded specâ€‹\_â€‹guardâ€‹\_â€‹hits\mathrm{spec\\_guard\\_hits} and maxâ€‹\_â€‹rhoâ€‹\_â€‹dt\mathrm{max\\_rho\\_dt}. Ablations demonstrate *non-interchangeability*: removing gating, halving kernel rank, or disabling Spec-Guard sharply degrades Stability, widens DualGap, and introduces geometric defects on the IV terrain. Stress-to-Fail (S2F) curves quantify robustness under numÃ©raire shifts, coverage deficits, and rough/long-memory perturbations, yielding interpretable thresholds (e.g., NAS <0.9<0.9 beyond a stress level near 2.02.0). External validity is probed via frozen-hyperparameter reuse across OOS windows, with small CNAS deltas and documented confidence intervals. Effective dimension estimates (d^90,d^95,d^99)=(1,1,2)(\hat{d}\_{90},\hat{d}\_{95},\hat{d}\_{99})=(1,1,2) align with the generalization theory in T3â€“T5.

#### Mechanistic insights.

The operator-level view explains why linear-time scans alone are insufficient: without measure gating and geometric projection, selective recurrence can memorize but cannot guarantee risk-neutral feasibility. The RN-Operator plus Q-Align reframes training as *monotone operator splitting with certificates*, where Lipschitz and spectral projections act as safety margins that transfer to OOS generalization. The decoderâ€™s convexâ€“monotone structure closes the loop by ensuring economic shape constraints at the output layer, obviating post-hoc convexification.

#### Limitations.

Our arXiv release uses synthetic yet finance-faithful generators to enable controlled ablations, deferring full real-market ingestion to a companion artifact. While the RN-Operator is expressive and stable, it assumes sufficient coverage in (T,K)(T,K) and clean variance-swap replication; pronounced microstructure noise, sparse wings, jumps, and regime breaks may require robust estimators, jump-diffusion priors, or heavy-tail losses. The S2F protocol quantifies tolerance along chosen distortion axes; broader stress families (transaction costs, inventory constraints, stochastic interest/dividend curves) are left to future work. Finally, our theory relies on smoothness and mixing assumptions that can be weakened but would incur slower rates or larger constants.

#### Future directions.

(i) Multi-market coupling. Extend the coupling layer to SPXâ€“VIXâ€“VVIX and cross-asset term structures (FX, rates), with KL/CVaR alignment across numeraires and maturities. (ii) American/early-exercise products. Combine RN-Operator with variational inequalities or policy iteration to impose Snell-envelope monotonicity. (iii) Online and adaptive safety. Replace fixed CFL thresholds with learned, uncertainty-aware guards and per-layer Lipschitz budgeting; integrate conformal prediction for interval-level no-arbitrage. (iv) Sharper theory. Prove minimax lower bounds matching our oracle rates; relax smoothness via Besov/rough-path function classes; analyze tightness of the representative-element bound under adversarial coverage. (v) System efficiency. Fuse FFT-based kernels with multi-resolution scan to reduce wall-clock while maintaining certificates; explore mixed-precision training with safety-preserving rescaling.

#### Reproducibility and ethics.

We release a *single-command* pipeline that exports all metrics, logs, and safety counters (including specâ€‹\_â€‹guardâ€‹\_â€‹hits\mathrm{spec\\_guard\\_hits}, projectionâ€‹\_â€‹distance\mathrm{projection\\_distance}, maxâ€‹\_â€‹rhoâ€‹\_â€‹dt\mathrm{max\\_rho\\_dt}, novikâ€‹\_â€‹toâ€‹\_â€‹kazamakiâ€‹\_â€‹rate\mathrm{novik\\_to\\_kazamaki\\_rate}, coverage statistics, and S2F thresholds), plus an independent replication script with fixed seeds and hardware descriptors. Data licensing, use restrictions, and non-investment-advice statements accompany the artifact. These measures aim to make results independently verifiable and to set a standard for *operator-level safety* in financial machine learning.

#### Take-home message.

Risk-neutral geometry canâ€”and shouldâ€”be enforced *during* training. When selective scan is recast as a measure-consistent operator and equipped with Lipschitz and spectral guards, we obtain a model class that is simultaneously *expressive*, *stable*, and *auditable*, delivering arbitrage-free surfaces with quantifiable safety margins and statistically defensible uncertainty. We hope ARBITER will serve as a blueprint for safety-first operator learning in quantitative finance and beyond.

## Appendix A. Proofs for Sections 3

### A.1 Proof of LemmaÂ [1](https://arxiv.org/html/2511.06451v1#Thmlemma1 "Lemma 1 (Green kernel bound). â€£ Spectral safety and discrete Green kernel. â€£ 3.1 Riskâ€“Neutral Operator Layer (RN-Operator) â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")

###### Lemma 4 (Green kernel bound).

Let {Tâ„“}â„“âˆˆâ„¤\{T\_{\ell}\}\_{\ell\in\mathbb{Z}} be a nondecreasing time grid with increments Î”â€‹tâ„“:=Tâ„“+1âˆ’Tâ„“>0\Delta t\_{\ell}:=T\_{\ell+1}-T\_{\ell}>0 and let AÎ¸â€‹(Tâ„“)âˆˆâ„dÃ—dA\_{\theta}(T\_{\ell})\in\mathbb{R}^{d\times d} be a (timeâ€“varying) generator.
Define Mâ„“:=Î”â€‹tâ„“â€‹AÎ¸â€‹(Tâ„“)M\_{\ell}:=\Delta t\_{\ell}\,A\_{\theta}(T\_{\ell}), Râ„“:=(Iâˆ’Mâ„“)âˆ’1R\_{\ell}:=(I-M\_{\ell})^{-1}, and for bounded injections BsB\_{s} with â€–Bsâ€–â‰¤bâ€‹Î”â€‹ts\|B\_{s}\|\leq b\,\Delta t\_{s} the discrete causal Green kernel

|  |  |  |
| --- | --- | --- |
|  | ğ’¢Î¸â€‹(Tâ„“,Ts):=Râ„“â€‹Râ„“âˆ’1â€‹â‹¯â€‹Rs+1â€‹Bs,sâ‰¤â„“.\mathcal{G}\_{\theta}(T\_{\ell},T\_{s}):=R\_{\ell}R\_{\ell-1}\cdots R\_{s+1}\,B\_{s},\qquad s\leq\ell. |  |

If the CFLâ€“type safeguard Ïâ€‹(AÎ¸â€‹(Tâ„“))â€‹Î”â€‹tâ„“=Ïâ€‹(Mâ„“)â‰¤1âˆ’Îµ\rho\!\left(A\_{\theta}(T\_{\ell})\right)\,\Delta t\_{\ell}=\rho(M\_{\ell})\leq 1-\varepsilon holds for all â„“\ell with some Îµâˆˆ(0,1)\varepsilon\in(0,1), then there exists C=Câ€‹(Îµ,b,Î”â€‹tÂ¯)<âˆC=C(\varepsilon,b,\overline{\Delta t})<\infty, where Î”â€‹tÂ¯:=supâ„“Î”â€‹tâ„“\overline{\Delta t}:=\sup\_{\ell}\Delta t\_{\ell}, such that

|  |  |  |
| --- | --- | --- |
|  | âˆ‘sâ‰¤â„“â€–ğ’¢Î¸â€‹(Tâ„“,Ts)â€–â‰¤Câ€‹(Îµ,b,Î”â€‹tÂ¯)for allÂ â€‹â„“.\sum\_{s\leq\ell}\big\|\mathcal{G}\_{\theta}(T\_{\ell},T\_{s})\big\|\;\leq\;C(\varepsilon,b,\overline{\Delta t})\quad\text{for all }\ell. |  |

###### Proof.

Step 1 (Extremal norm and contraction).
Let â„³:={Mâ„“:â„“âˆˆâ„¤}\mathcal{M}:=\{M\_{\ell}:\ell\in\mathbb{Z}\}.
From supMâˆˆâ„³Ïâ€‹(M)â‰¤1âˆ’Îµ\sup\_{M\in\mathcal{M}}\rho(M)\leq 1-\varepsilon and joint spectral radius theory, for any Î´âˆˆ(0,Îµ)\delta\in(0,\varepsilon) there exists an induced operator norm âˆ¥â‹…âˆ¥âˆ—\|\cdot\|\_{\*} such that

|  |  |  |
| --- | --- | --- |
|  | â€–Mâ€–âˆ—â‰¤1âˆ’Îµ+Î´âˆ€Mâˆˆâ„³.\|M\|\_{\*}\leq 1-\varepsilon+\delta\quad\forall\,M\in\mathcal{M}. |  |

Fix Î´:=Îµ/2\delta:=\varepsilon/2, set Î±:=1âˆ’Îµ/2âˆˆ(0,1)\alpha:=1-\varepsilon/2\in(0,1), then â€–Mâ„“â€–âˆ—â‰¤Î±\|M\_{\ell}\|\_{\*}\leq\alpha for all â„“\ell.

Step 2 (Uniform resolvent bound).
By the Neumann series in âˆ¥â‹…âˆ¥âˆ—\|\cdot\|\_{\*},

|  |  |  |
| --- | --- | --- |
|  | Râ„“=(Iâˆ’Mâ„“)âˆ’1=âˆ‘k=0âˆMâ„“k,â€–Râ„“â€–âˆ—â‰¤âˆ‘k=0âˆâ€–Mâ„“â€–âˆ—kâ‰¤11âˆ’Î±=2Îµ.R\_{\ell}=(I-M\_{\ell})^{-1}=\sum\_{k=0}^{\infty}M\_{\ell}^{\,k},\qquad\|R\_{\ell}\|\_{\*}\leq\sum\_{k=0}^{\infty}\|M\_{\ell}\|\_{\*}^{\,k}\leq\frac{1}{1-\alpha}=\frac{2}{\varepsilon}. |  |

Step 3 (Fundamental propagator).
Submultiplicativity yields

|  |  |  |
| --- | --- | --- |
|  | â€–Râ„“â€‹Râ„“âˆ’1â€‹â‹¯â€‹Rs+1â€–âˆ—â‰¤(2Îµ)â„“âˆ’s.\big\|R\_{\ell}R\_{\ell-1}\cdots R\_{s+1}\big\|\_{\*}\leq\Big(\tfrac{2}{\varepsilon}\Big)^{\ell-s}. |  |

With â€–Bsâ€–âˆ—â‰¤bâˆ—â€‹Î”â€‹ts\|B\_{s}\|\_{\*}\leq b\_{\*}\Delta t\_{s} where bâˆ—:=supsâ€–Bsâ€–âˆ—/Î”â€‹ts<âˆb\_{\*}:=\sup\_{s}\|B\_{s}\|\_{\*}/\Delta t\_{s}<\infty, we obtain

|  |  |  |
| --- | --- | --- |
|  | â€–ğ’¢Î¸â€‹(Tâ„“,Ts)â€–âˆ—â‰¤(2Îµ)â„“âˆ’sâ€‹bâˆ—â€‹Î”â€‹ts.\big\|\mathcal{G}\_{\theta}(T\_{\ell},T\_{s})\big\|\_{\*}\leq\Big(\tfrac{2}{\varepsilon}\Big)^{\ell-s}b\_{\*}\Delta t\_{s}. |  |

Step 4 (Summability).
Summing over sâ‰¤â„“s\leq\ell and letting k:=â„“âˆ’sk:=\ell-s,

|  |  |  |
| --- | --- | --- |
|  | âˆ‘sâ‰¤â„“â€–ğ’¢Î¸â€‹(Tâ„“,Ts)â€–âˆ—â‰¤bâˆ—â€‹âˆ‘k=0âˆ(2Îµ)kâ€‹Î”â€‹tâ„“âˆ’k.\sum\_{s\leq\ell}\big\|\mathcal{G}\_{\theta}(T\_{\ell},T\_{s})\big\|\_{\*}\;\leq\;b\_{\*}\sum\_{k=0}^{\infty}\Big(\tfrac{2}{\varepsilon}\Big)^{k}\Delta t\_{\ell-k}. |  |

To ensure a uniform bound, tighten StepÂ 1 by choosing an arbitrary Î·âˆˆ(0,1)\eta\in(0,1) and taking Î´>0\delta>0 small enough that â€–Mâ„“â€–âˆ—â‰¤Î·\|M\_{\ell}\|\_{\*}\leq\eta for all â„“\ell (possible by the extremalâ€“norm argument).
Repeating StepÂ 2â€“3 gives â€–Râ„“â€–âˆ—â‰¤(1âˆ’Î·)âˆ’1\|R\_{\ell}\|\_{\*}\leq(1-\eta)^{-1} and hence

|  |  |  |
| --- | --- | --- |
|  | âˆ‘sâ‰¤â„“â€–ğ’¢Î¸â€‹(Tâ„“,Ts)â€–âˆ—â‰¤bâˆ—â€‹âˆ‘k=0âˆÎ·kâ€‹Î”â€‹tâ„“âˆ’kâ‰¤bâˆ—â€‹Î”â€‹tÂ¯â€‹âˆ‘k=0âˆÎ·k=bâˆ—â€‹Î”â€‹tÂ¯1âˆ’Î·.\sum\_{s\leq\ell}\big\|\mathcal{G}\_{\theta}(T\_{\ell},T\_{s})\big\|\_{\*}\;\leq\;b\_{\*}\sum\_{k=0}^{\infty}\eta^{\,k}\Delta t\_{\ell-k}\;\leq\;b\_{\*}\,\overline{\Delta t}\sum\_{k=0}^{\infty}\eta^{\,k}\;=\;\frac{b\_{\*}\,\overline{\Delta t}}{1-\eta}\,. |  |

Step 5 (Return to the reference norm).
All norms in finite dimension are equivalent, so there exists Îºâ‰¥1\kappa\geq 1 with â€–Xâ€–â‰¤Îºâ€‹â€–Xâ€–âˆ—\|X\|\leq\kappa\|X\|\_{\*}.
Therefore

|  |  |  |
| --- | --- | --- |
|  | âˆ‘sâ‰¤â„“âˆ¥ğ’¢Î¸(Tâ„“,Ts)âˆ¥â‰¤Îºbâˆ—â€‹Î”â€‹tÂ¯1âˆ’Î·=:C(Îµ,b,Î”â€‹tÂ¯)<âˆ,\sum\_{s\leq\ell}\big\|\mathcal{G}\_{\theta}(T\_{\ell},T\_{s})\big\|\;\leq\;\kappa\,\frac{b\_{\*}\,\overline{\Delta t}}{1-\eta}\;=:\;C(\varepsilon,b,\overline{\Delta t})<\infty, |  |

which proves the claim.
âˆ

#### Remark (Non-diagonalizable case and explicit constants).

If Mâ„“M\_{\ell} admits a Jordan decomposition Mâ„“=Vâ„“â€‹Jâ„“â€‹Vâ„“âˆ’1M\_{\ell}=V\_{\ell}J\_{\ell}V\_{\ell}^{-1}, then
Râ„“=(Iâˆ’Mâ„“)âˆ’1=Vâ„“â€‹(Iâˆ’Jâ„“)âˆ’1â€‹Vâ„“âˆ’1R\_{\ell}=(I-M\_{\ell})^{-1}=V\_{\ell}(I-J\_{\ell})^{-1}V\_{\ell}^{-1}.
For a size-kk Jordan block Jkâ€‹(Î»)J\_{k}(\lambda),
â€–(Iâˆ’Jkâ€‹(Î»))âˆ’1â€–â‰¤âˆ‘m=0kâˆ’1(m+kâˆ’1kâˆ’1)â€‹|Î»|mâ‰¤Ckâ€‹(1âˆ’|Î»|)âˆ’k.\|(I-J\_{k}(\lambda))^{-1}\|\leq\sum\_{m=0}^{k-1}\binom{m+k-1}{k-1}|\lambda|^{m}\leq C\_{k}\,(1-|\lambda|)^{-k}.
Under Ïâ€‹(Mâ„“)â‰¤1âˆ’Îµ\rho(M\_{\ell})\leq 1-\varepsilon this implies â€–Râ„“â€–â‰¤Îºâ€‹(Vâ„“)â€‹Cdâ€‹Îµâˆ’d\|R\_{\ell}\|\leq\kappa(V\_{\ell})\,C\_{d}\,\varepsilon^{-d}, whence the same summability follows after accounting for the Î”â€‹ts\Delta t\_{s} factor in BsB\_{s}. The extremalâ€“norm route typically yields tighter constants by avoiding Îºâ€‹(Vâ„“)\kappa(V\_{\ell}).

### A.2 Proof of PropositionÂ [1](https://arxiv.org/html/2511.06451v1#Thmproposition1 "Proposition 1 (RN-operator stability under Q-Align). â€£ RN-operator stability under Q-Align. â€£ 3.2 Q-Align: Lipschitz Projection and Spectral Guard â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")

#### Setting and recalled constraints.

We consider the RN-operator layer on a grid {Tâ„“}\{T\_{\ell}\} with increments Î”â€‹tâ„“>0\Delta t\_{\ell}>0, generator AÎ¸â€‹(Tâ„“)A\_{\theta}(T\_{\ell}), and resolvent Râ„“:=(Iâˆ’Î”â€‹tâ„“â€‹AÎ¸â€‹(Tâ„“))âˆ’1R\_{\ell}:=(I-\Delta t\_{\ell}A\_{\theta}(T\_{\ell}))^{-1}.
The Q-Align projection enforces the layerwise Lipschitz envelopeÂ ([14](https://arxiv.org/html/2511.06451v1#S3.E14 "In Layerwise Lipschitz projection. â€£ 3.2 Q-Align: Lipschitz Projection and Spectral Guard â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")), summarized as

|  |  |  |
| --- | --- | --- |
|  | â€–â„’â„“â€–â‰¤Ï„(Ï„â‰¤1),\|\mathcal{L}\_{\ell}\|\leq\tau\qquad(\tau\leq 1), |  |

for the linearized lipschitz surrogate â„’â„“\mathcal{L}\_{\ell} of the per-step affine map prior to the nonlinearity; the spectral safeguardÂ ([15](https://arxiv.org/html/2511.06451v1#S3.E15 "In Spectral Guard (CFL projection). â€£ 3.2 Q-Align: Lipschitz Projection and Spectral Guard â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) is the CFL-type condition

|  |  |  |
| --- | --- | --- |
|  | Ïâ€‹(AÎ¸â€‹(Tâ„“))â€‹Î”â€‹tâ„“â‰¤1âˆ’Îµ(Îµâˆˆ(0,1)),\rho\!\left(A\_{\theta}(T\_{\ell})\right)\,\Delta t\_{\ell}\leq 1-\varepsilon\qquad(\varepsilon\in(0,1)), |  |

which guarantees resolvent well-posedness.
We use a nonexpansive activation Ï•\phi with Lipâ¡(Ï•)â‰¤1\operatorname{Lip}(\phi)\leq 1.
Define the input injection Bâ„“B\_{\ell} (possibly learned) and bias bâ„“b\_{\ell}, with bounded envelopes â€–Bâ„“â€–â‰¤bin\|B\_{\ell}\|\leq b\_{\rm in} and â€–bâ„“â€–â‰¤b0\|b\_{\ell}\|\leq b\_{0}.
The discrete causal Green kernel reads (for sâ‰¤â„“s\leq\ell)

|  |  |  |
| --- | --- | --- |
|  | ğ’¢Î¸â€‹(Tâ„“,Ts)=Râ„“â€‹Râ„“âˆ’1â€‹â‹¯â€‹Rs+1â€‹Bs.\mathcal{G}\_{\theta}(T\_{\ell},T\_{s})\;=\;R\_{\ell}R\_{\ell-1}\cdots R\_{s+1}\,B\_{s}. |  |

The state recursion is

|  |  |  |  |
| --- | --- | --- | --- |
|  | hâ„“=Ï•â€‹(Râ„“â€‹hâ„“âˆ’1+Bâ„“â€‹uâ„“+bâ„“),â„“âˆˆâ„¤.h\_{\ell}\;=\;\phi\!\left(R\_{\ell}h\_{\ell-1}+B\_{\ell}u\_{\ell}+b\_{\ell}\right),\qquad\ell\in\mathbb{Z}. |  | (47) |

#### Auxiliary bound (from AppendixÂ A.1).

By LemmaÂ [1](https://arxiv.org/html/2511.06451v1#Thmlemma1 "Lemma 1 (Green kernel bound). â€£ Spectral safety and discrete Green kernel. â€£ 3.1 Riskâ€“Neutral Operator Layer (RN-Operator) â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures"), under the CFL-type guard there exists an induced norm âˆ¥â‹…âˆ¥âˆ—\|\cdot\|\_{\*} and constants Î·âˆˆ(0,1)\eta\in(0,1) and CÎµ<âˆC\_{\varepsilon}<\infty such that

|  |  |  |
| --- | --- | --- |
|  | â€–Râ„“â€–âˆ—â‰¤11âˆ’Î·,âˆ‘sâ‰¤â„“â€–Râ„“â€‹â‹¯â€‹Rs+1â€–âˆ—â€‹Î”â€‹tsâ‰¤CÎµ,\|R\_{\ell}\|\_{\*}\leq\frac{1}{1-\eta}\,,\qquad\sum\_{s\leq\ell}\big\|R\_{\ell}\cdots R\_{s+1}\big\|\_{\*}\,\Delta t\_{s}\;\leq\;C\_{\varepsilon}, |  |

uniformly in â„“\ell (the precise dependence on Îµ\varepsilon is stated in AppendixÂ A.1).

#### Step 1: BIBO stability.

IteratingÂ ([47](https://arxiv.org/html/2511.06451v1#Ax1.E47 "In Setting and recalled constraints. â€£ A.2 Proof of Proposition 1 â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) and using Lipâ¡(Ï•)â‰¤1\operatorname{Lip}(\phi)\leq 1 yields

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–hâ„“â€–âˆ—\displaystyle\|h\_{\ell}\|\_{\*} | â‰¤â€–Râ„“â€–âˆ—â€‹â€–hâ„“âˆ’1â€–âˆ—+â€–Bâ„“â€–âˆ—â€‹â€–uâ„“â€–+â€–bâ„“â€–âˆ—\displaystyle\leq\|R\_{\ell}\|\_{\*}\,\|h\_{\ell-1}\|\_{\*}+\|B\_{\ell}\|\_{\*}\,\|u\_{\ell}\|+\|b\_{\ell}\|\_{\*} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤11âˆ’Î·â€‹â€–hâ„“âˆ’1â€–âˆ—+bin,âˆ—â€‹â€–uâ„“â€–+b0,âˆ—,\displaystyle\leq\frac{1}{1-\eta}\,\|h\_{\ell-1}\|\_{\*}+b\_{{\rm in},\*}\,\|u\_{\ell}\|+b\_{0,\*}, |  |

where bin,âˆ—:=supâ„“â€–Bâ„“â€–âˆ—b\_{{\rm in},\*}:=\sup\_{\ell}\|B\_{\ell}\|\_{\*} and b0,âˆ—:=supâ„“â€–bâ„“â€–âˆ—b\_{0,\*}:=\sup\_{\ell}\|b\_{\ell}\|\_{\*}.
Unrolling the recursion with hâˆ’âˆ=0h\_{-\infty}=0 (or any bounded initialization absorbed into the same bound), and substituting RR-products gives

|  |  |  |
| --- | --- | --- |
|  | â€–hâ„“â€–âˆ—â‰¤âˆ‘sâ‰¤â„“â€–Râ„“â€‹â‹¯â€‹Rs+1â€–âˆ—â€‹(bin,âˆ—â€‹â€–usâ€–+b0,âˆ—).\|h\_{\ell}\|\_{\*}\;\leq\;\sum\_{s\leq\ell}\big\|R\_{\ell}\cdots R\_{s+1}\big\|\_{\*}\big(b\_{{\rm in},\*}\|u\_{s}\|+b\_{0,\*}\big). |  |

If supsâ€–usâ€–â‰¤U<âˆ\sup\_{s}\|u\_{s}\|\leq U<\infty, then by the kernel summability,

|  |  |  |
| --- | --- | --- |
|  | â€–hâ„“â€–âˆ—â‰¤CÎµâ€‹(bin,âˆ—â€‹U+b0,âˆ—),\|h\_{\ell}\|\_{\*}\;\leq\;C\_{\varepsilon}\,\big(b\_{{\rm in},\*}\,U+b\_{0,\*}\big), |  |

uniformly in â„“\ell.
By norm equivalence in finite dimension, the same uniform bound holds for any reference norm âˆ¥â‹…âˆ¥\|\cdot\|:

|  |  |  |
| --- | --- | --- |
|  | supâ„“âˆ¥hâ„“âˆ¥â‰¤ÎºCÎµ(binU+b0)=:CBIBO<âˆ.\sup\_{\ell}\|h\_{\ell}\|\;\leq\;\kappa\,C\_{\varepsilon}\,\big(b\_{\rm in}\,U+b\_{0}\big)=:C\_{\rm BIBO}<\infty. |  |

Hence the trajectory is uniformly bounded for bounded input (BIBO stability).

#### Step 2: Global Lipschitz continuity (input-to-state and input-to-output).

Consider two input sequences {uâ„“}\{u\_{\ell}\}, {uâ„“â€²}\{u^{\prime}\_{\ell}\} with corresponding states {hâ„“}\{h\_{\ell}\}, {hâ„“â€²}\{h^{\prime}\_{\ell}\}.
Set Î´â€‹hâ„“:=hâ„“âˆ’hâ„“â€²\delta h\_{\ell}:=h\_{\ell}-h^{\prime}\_{\ell}, Î´â€‹uâ„“:=uâ„“âˆ’uâ„“â€²\delta u\_{\ell}:=u\_{\ell}-u^{\prime}\_{\ell}.
Using Lipâ¡(Ï•)â‰¤1\operatorname{Lip}(\phi)\leq 1,

|  |  |  |
| --- | --- | --- |
|  | â€–Î´â€‹hâ„“â€–âˆ—â‰¤â€–Râ„“â€–âˆ—â€‹â€–Î´â€‹hâ„“âˆ’1â€–âˆ—+â€–Bâ„“â€–âˆ—â€‹â€–Î´â€‹uâ„“â€–.\|\delta h\_{\ell}\|\_{\*}\;\leq\;\|R\_{\ell}\|\_{\*}\,\|\delta h\_{\ell-1}\|\_{\*}+\|B\_{\ell}\|\_{\*}\,\|\delta u\_{\ell}\|. |  |

Unrolling as above and using submultiplicativity,

|  |  |  |
| --- | --- | --- |
|  | â€–Î´â€‹hâ„“â€–âˆ—â‰¤âˆ‘sâ‰¤â„“â€–Râ„“â€‹â‹¯â€‹Rs+1â€–âˆ—â€‹â€–Bsâ€–âˆ—â€‹â€–Î´â€‹usâ€–.\|\delta h\_{\ell}\|\_{\*}\;\leq\;\sum\_{s\leq\ell}\big\|R\_{\ell}\cdots R\_{s+1}\big\|\_{\*}\,\|B\_{s}\|\_{\*}\,\|\delta u\_{s}\|. |  |

Taking â„“âˆ\ell^{\infty} norms over sequences and applying the kernel sum bound,

|  |  |  |
| --- | --- | --- |
|  | â€–Î´â€‹hâ€–â„“âˆ,âˆ—â‰¤(supsâ€–Bsâ€–âˆ—)â€‹(supâ„“âˆ‘sâ‰¤â„“â€–Râ„“â€‹â‹¯â€‹Rs+1â€–âˆ—â€‹Î”â€‹ts)â€‹â€–Î´â€‹uâ€–â„“âˆâ‰¤bin,âˆ—â€‹CÎµâ€‹â€–Î´â€‹uâ€–â„“âˆ.\|\delta h\|\_{\ell^{\infty},\*}\;\leq\;\Big(\sup\_{s}\|B\_{s}\|\_{\*}\Big)\,\Big(\sup\_{\ell}\sum\_{s\leq\ell}\|R\_{\ell}\cdots R\_{s+1}\|\_{\*}\,\Delta t\_{s}\Big)\,\|\delta u\|\_{\ell^{\infty}}\;\leq\;b\_{{\rm in},\*}\,C\_{\varepsilon}\,\|\delta u\|\_{\ell^{\infty}}. |  |

Passing back to the reference norm via equivalence constants yields

|  |  |  |
| --- | --- | --- |
|  | â€–Î´â€‹hâ€–â„“âˆâ‰¤Îºâ€‹binâ€‹CÎµâ€‹â€–Î´â€‹uâ€–â„“âˆ.\|\delta h\|\_{\ell^{\infty}}\;\leq\;\kappa\,b\_{\rm in}\,C\_{\varepsilon}\,\|\delta u\|\_{\ell^{\infty}}. |  |

If the readout/decoder is LoutL\_{\rm out}-Lipschitz (Q-Align also enforces a 11-Lipschitz envelope through the head), then the overall input-to-output map is globally Lipschitz with

|  |  |  |  |
| --- | --- | --- | --- |
|  | Lglobâ‰¤Îºâ€‹Loutâ€‹binâ€‹CÎµ.L\_{\rm glob}\;\leq\;\kappa\,L\_{\rm out}\,b\_{\rm in}\,C\_{\varepsilon}. |  | (48) |

When the layerwise envelope is tightened byÂ ([14](https://arxiv.org/html/2511.06451v1#S3.E14 "In Layerwise Lipschitz projection. â€£ 3.2 Q-Align: Lipschitz Projection and Spectral Guard â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) with factor Ï„â‰¤1\tau\leq 1, we can absorb it multiplicatively into binb\_{\rm in} or LoutL\_{\rm out}, so the same bound holds with binâ†Ï„â€‹binb\_{\rm in}\leftarrow\tau\,b\_{\rm in}, Loutâ†Ï„â€‹LoutL\_{\rm out}\leftarrow\tau\,L\_{\rm out}.
This matches the main-text boundÂ ([13](https://arxiv.org/html/2511.06451v1#S3.E13 "In Lipschitz surrogate via spectral normalization. â€£ 3.1 Riskâ€“Neutral Operator Layer (RN-Operator) â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) up to norm-equivalence constants.

#### Step 3: Role of Spec-Guard and Q-Align.

Spec-Guard ensures â€–Râ„“â€–âˆ—\|R\_{\ell}\|\_{\*} remains uniformly bounded and that the product â€–Râ„“â€‹â‹¯â€‹Rs+1â€–âˆ—\|R\_{\ell}\cdots R\_{s+1}\|\_{\*} decays geometrically in the extremal norm; Q-Align prevents per-step amplification beyond Ï„â‰¤1\tau\leq 1, guaranteeing that the effective injection â€–Bsâ€–âˆ—\|B\_{s}\|\_{\*} and the readout Lipschitz constant remain inside the envelope.
Combining both yields BIBO stability and a globally Lipschitz operator with constant bounded byÂ ([48](https://arxiv.org/html/2511.06451v1#Ax1.E48 "In Step 2: Global Lipschitz continuity (input-to-state and input-to-output). â€£ A.2 Proof of Proposition 1 â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")).

#### Non-diagonalizable case and time-varying steps.

If AÎ¸â€‹(Tâ„“)A\_{\theta}(T\_{\ell}) is not diagonalizable, the Jordan-block resolvent bound in AppendixÂ A.1 gives â€–Râ„“â€–â‰¤Cdâ€‹Îµâˆ’d\|R\_{\ell}\|\leq C\_{d}\,\varepsilon^{-d} up to condition numbers; the extremal-norm construction avoids these condition numbers and yields the uniform envelope used above.
Heterogeneous steps Î”â€‹tâ„“\Delta t\_{\ell} are already handled in the kernel summability via the weighted series âˆ‘sâ‰¤â„“â€–Râ„“â€‹â‹¯â€‹Rs+1â€–âˆ—â€‹Î”â€‹ts\sum\_{s\leq\ell}\|R\_{\ell}\cdots R\_{s+1}\|\_{\*}\,\Delta t\_{s}.

#### Conclusion.

Uniform boundedness and global Lipschitz continuity follow, which proves PropositionÂ [1](https://arxiv.org/html/2511.06451v1#Thmproposition1 "Proposition 1 (RN-operator stability under Q-Align). â€£ RN-operator stability under Q-Align. â€£ 3.2 Q-Align: Lipschitz Projection and Spectral Guard â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures").
âˆ

### A.3 SPXâ†”\leftrightarrowVIX replication: discretization consistency and identifiability

#### Continuous-time identity and discrete estimator.

Let FT=S0â€‹e(râˆ’q)â€‹TF\_{T}=S\_{0}\mathrm{e}^{(r-q)T}, and let Câ€‹(â‹…,T)C(\cdot,T), Pâ€‹(â‹…,T)P(\cdot,T) be call and put prices under â„šÎ¸\mathbb{Q}\_{\theta} with discount factor eâˆ’râ€‹T\mathrm{e}^{-rT} and no static arbitrage.
The log-contract identity yields the variance-swap fair rate (for diffusion models; jump-diffusions add the standard jump term):

|  |  |  |  |
| --- | --- | --- | --- |
|  | ÏƒVS,Î¸2â€‹(T)=2â€‹erâ€‹TTâ€‹(âˆ«0FTPÎ¸â€‹(K,T)K2â€‹ğ‘‘K+âˆ«FTâˆCÎ¸â€‹(K,T)K2â€‹ğ‘‘K)âˆ’1Tâ€‹(FTK0âˆ’1)2.\sigma^{2}\_{\mathrm{VS},\theta}(T)=\frac{2\,\mathrm{e}^{rT}}{T}\!\left(\int\_{0}^{F\_{T}}\frac{P\_{\theta}(K,T)}{K^{2}}\,dK+\int\_{F\_{T}}^{\infty}\frac{C\_{\theta}(K,T)}{K^{2}}\,dK\right)-\frac{1}{T}\left(\frac{F\_{T}}{K\_{0}}-1\right)^{\!2}. |  | (49) |

For a strike grid ğ’¦T={Ki}i=1M\mathcal{K}\_{T}=\{K\_{i}\}\_{i=1}^{M}, define Î”â€‹Ki=12â€‹(Ki+1âˆ’Kiâˆ’1)\Delta K\_{i}=\frac{1}{2}(K\_{i+1}-K\_{i-1}) with one-sided endpoints, and the discrete estimator

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ïƒ^VS,Î¸2â€‹(T):=2â€‹erâ€‹TTâ€‹âˆ‘i=1MÎ”â€‹KiKi2â€‹QÎ¸â€‹(Ki,T)âˆ’1Tâ€‹(FTK0âˆ’1)2,\widehat{\sigma}^{2}\_{\mathrm{VS},\theta}(T):=\frac{2\,\mathrm{e}^{rT}}{T}\sum\_{i=1}^{M}\frac{\Delta K\_{i}}{K\_{i}^{2}}\,Q\_{\theta}(K\_{i},T)-\frac{1}{T}\left(\frac{F\_{T}}{K\_{0}}-1\right)^{\!2}, |  | (50) |

where QÎ¸â€‹(Ki,T)=PÎ¸Q\_{\theta}(K\_{i},T)=P\_{\theta} if Ki<FTK\_{i}<F\_{T} and QÎ¸=CÎ¸Q\_{\theta}=C\_{\theta} if Kiâ‰¥FTK\_{i}\geq F\_{T}.

#### Tail integrability and convexity.

Assume: (i) Kâ†¦QÎ¸â€‹(K,T)K\mapsto Q\_{\theta}(K,T) is convex for each TT; (ii) QÎ¸â€‹(â‹…,T)/K2Q\_{\theta}(\cdot,T)/K^{2} has bounded variation on compact sets; (iii) âˆ«0KminPÎ¸K2â€‹ğ‘‘Kâ†’0\int\_{0}^{K\_{\min}}\!\tfrac{P\_{\theta}}{K^{2}}\,dK\to 0 and âˆ«KmaxâˆCÎ¸K2â€‹ğ‘‘Kâ†’0\int\_{K\_{\max}}^{\infty}\!\tfrac{C\_{\theta}}{K^{2}}\,dK\to 0 as Kminâ†“0K\_{\min}\downarrow 0, Kmaxâ†‘âˆK\_{\max}\uparrow\infty.
The latter holds, for instance, if the risk-neutral tails satisfy CÎ¸â€‹(K,T)â‰²Kâˆ’Î±C\_{\theta}(K,T)\lesssim K^{-\alpha} with Î±>1\alpha>1 and PÎ¸â€‹(K,T)â‰²KP\_{\theta}(K,T)\lesssim K as Kâ†“0K\downarrow 0.

###### Lemma 5 (Quadrature error under convexity).

Let fâ€‹(K)=QÎ¸â€‹(K,T)/K2f(K)=Q\_{\theta}(K,T)/K^{2} on a compact interval [a,b][a,b], with ff convex and of bounded variation.
For the midpoint rule with mesh Î”â€‹K\Delta K, the error satisfies

|  |  |  |
| --- | --- | --- |
|  | |âˆ«abfâ€‹(K)â€‹ğ‘‘Kâˆ’âˆ‘iÎ”â€‹Kiâ€‹fâ€‹(Ki)|â‰¤TVâ€‹(f;[a,b])2â€‹Î”â€‹K,\left|\int\_{a}^{b}f(K)\,dK-\sum\_{i}\Delta K\_{i}\,f(K\_{i})\right|\;\leq\;\frac{\mathrm{TV}(f;[a,b])}{2}\,\Delta K, |  |

where TVâ€‹(f;[a,b])\mathrm{TV}(f;[a,b]) denotes the total variation of ff on [a,b][a,b].

###### Proof.

Since ff has bounded variation, ff is the difference of two monotone functions.
Apply the Jordan decomposition and sum the per-cell trapezoid error; convexity implies the midpoint rule error is monotone in the cell width and controlled by the variation measure.
A standard argument (Riemannâ€“Stieltjes with variation measure) yields the bound.
âˆ

###### Proof of PropositionÂ [2](https://arxiv.org/html/2511.06451v1#Thmproposition2 "Proposition 2 (Consistency of discretized VIX replication). â€£ SPXâ†”VIX replication. â€£ 3.3 Convexâ€“Monotone Decoder and SPXâ€“VIX Coupling â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures").

Fix TT.
Split the integrals inÂ ([49](https://arxiv.org/html/2511.06451v1#Ax1.E49 "In Continuous-time identity and discrete estimator. â€£ A.3 SPXâ†”VIX replication: discretization consistency and identifiability â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) on [0,Kmin][0,K\_{\min}], [Kmin,FT][K\_{\min},F\_{T}], [FT,Kmax][F\_{T},K\_{\max}], [Kmax,âˆ)[K\_{\max},\infty).
On the two compact intervals [Kmin,FT][K\_{\min},F\_{T}] and [FT,Kmax][F\_{T},K\_{\max}], apply LemmaÂ [5](https://arxiv.org/html/2511.06451v1#Thmlemma5 "Lemma 5 (Quadrature error under convexity). â€£ Tail integrability and convexity. â€£ A.3 SPXâ†”VIX replication: discretization consistency and identifiability â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures") to fâ€‹(K)=PÎ¸â€‹(K,T)/K2f(K)=P\_{\theta}(K,T)/K^{2} and fâ€‹(K)=CÎ¸â€‹(K,T)/K2f(K)=C\_{\theta}(K,T)/K^{2} respectively, to get an error â‰¤12â€‹[TVâ€‹(f;[Kmin,FT])+TVâ€‹(f;[FT,Kmax])]â€‹Î”â€‹KT\leq\frac{1}{2}[\mathrm{TV}(f;[K\_{\min},F\_{T}])+\mathrm{TV}(f;[F\_{T},K\_{\max}])]\Delta K\_{T}.
The tails are Îµtailâ€‹(Kmin,Kmax)\varepsilon\_{\mathrm{tail}}(K\_{\min},K\_{\max}) by assumption (iii).
The forward adjustment term coincides inÂ ([49](https://arxiv.org/html/2511.06451v1#Ax1.E49 "In Continuous-time identity and discrete estimator. â€£ A.3 SPXâ†”VIX replication: discretization consistency and identifiability â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) andÂ ([50](https://arxiv.org/html/2511.06451v1#Ax1.E50 "In Continuous-time identity and discrete estimator. â€£ A.3 SPXâ†”VIX replication: discretization consistency and identifiability â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")), hence cancels in the difference.
Uniformity in TT over compact subsets follows if the variation envelopes and tail integrability are uniform in TT.
âˆ

###### Lemma 6 (Log-contract linkage).

For a continuous ItÃ´ model dâ€‹St=Stâ€‹Î¼tâ€‹dâ€‹t+Stâ€‹Ïƒtâ€‹dâ€‹WtdS\_{t}=S\_{t}\mu\_{t}\,dt+S\_{t}\sigma\_{t}\,dW\_{t} under â„šÎ¸\mathbb{Q}\_{\theta},

|  |  |  |
| --- | --- | --- |
|  | 2â€‹erâ€‹TTâ€‹(âˆ«0FTPÎ¸K2â€‹ğ‘‘K+âˆ«FTâˆCÎ¸K2â€‹ğ‘‘K)=1Tâ€‹ğ”¼â„šÎ¸â€‹[âˆ«0TÏƒt2â€‹ğ‘‘t].\frac{2\,\mathrm{e}^{rT}}{T}\!\left(\int\_{0}^{F\_{T}}\!\frac{P\_{\theta}}{K^{2}}\,dK+\int\_{F\_{T}}^{\infty}\!\frac{C\_{\theta}}{K^{2}}\,dK\right)=\frac{1}{T}\,\mathbb{E}^{\mathbb{Q}\_{\theta}}\!\left[\!\int\_{0}^{T}\sigma\_{t}^{2}\,dt\right]. |  |

For jump-diffusions, an additional jump-compensator term appears and is incorporated in the standard VIX methodology through OTM sums of QÎ¸Q\_{\theta}.

###### Proof.

This is the classical Carrâ€“Madan log-contract identity, obtained by writing the log payoff as a static portfolio of OTM options plus a forward and differentiating option prices with respect to KK (Breedenâ€“Litzenberger).
âˆ

###### Proof of PropositionÂ [3](https://arxiv.org/html/2511.06451v1#Thmproposition3 "Proposition 3 (Variance-swap identifiability via replication). â€£ SPXâ†”VIX replication. â€£ 3.3 Convexâ€“Monotone Decoder and SPXâ€“VIX Coupling â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures").

The condition â„›VIXâ€‹(T)=0\mathcal{R}\_{\mathrm{VIX}}(T)=0 implies Ïƒ^VS,Î¸2â€‹(T)=VIXobs2â€‹(T)\widehat{\sigma}^{2}\_{\mathrm{VS},\theta}(T)=\mathrm{VIX}^{2}\_{\mathrm{obs}}(T).
By PropositionÂ [2](https://arxiv.org/html/2511.06451v1#Thmproposition2 "Proposition 2 (Consistency of discretized VIX replication). â€£ SPXâ†”VIX replication. â€£ 3.3 Convexâ€“Monotone Decoder and SPXâ€“VIX Coupling â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures"), letting the mesh refine and the truncation expand, we obtain ÏƒVS,Î¸2â€‹(T)=ÏƒVS,obs2â€‹(T)\sigma^{2}\_{\mathrm{VS},\theta}(T)=\sigma^{2}\_{\mathrm{VS,obs}}(T) for all Tâˆˆğ’¯T\in\mathcal{T}.
If vÎ¸v\_{\theta} exists and Tâ†¦1Tâ€‹âˆ«0TvÎ¸â€‹(t)â€‹ğ‘‘tT\mapsto\frac{1}{T}\int\_{0}^{T}v\_{\theta}(t)\,dt is strictly monotone, equality of the CesÃ ro means on an interval forces vÎ¸v\_{\theta} to match the observed instantaneous variance a.e. on ğ’¯\mathcal{T} (Hardyâ€“Littlewood Tauberian principle for monotone means).
âˆ

#### Interpolation choice and arbitrage.

Linear interpolation in (K,Q)(K,Q) preserves piecewise convexity and monotonicity, which aligns with the no-butterfly/no-calendar constraints; cubic splines may reduce quadrature error but risk local nonconvexities between knots.
In our experiments, both schemes yield statistically indistinguishable NAS/CNAS while linear interpolation avoids small arbitrage repairs (see TableÂ 1 and Fig.Â iv\_contours\_filled\_TK.png vs iv\_contours\_lines\_TK.png).

### A.4 Proof of PropositionÂ [4](https://arxiv.org/html/2511.06451v1#Thmproposition4 "Proposition 4 (Static no-arbitrage and replication consistency). â€£ SPXâ†”VIX replication. â€£ 3.3 Convexâ€“Monotone Decoder and SPXâ€“VIX Coupling â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures"): static no-arbitrage and replication consistency

We work on a strikeâ€“maturity grid {(Ki,Tj)}i=1,â€¦,M;j=1,â€¦,J\{(K\_{i},T\_{j})\}\_{i=1,\dots,M;\,j=1,\dots,J} with ordered 0<K1<â‹¯<KM0<K\_{1}<\dots<K\_{M} and 0<T1<â‹¯<TJ0<T\_{1}<\dots<T\_{J}, and one-sided spacings Î”â€‹Ki=12â€‹(Ki+1âˆ’Kiâˆ’1)\Delta K\_{i}=\tfrac{1}{2}(K\_{i+1}-K\_{i-1}) (endpoints adjusted analogously). Throughout, interest rate rr and dividend yield qq are accounted for via the forward FT=S0â€‹e(râˆ’q)â€‹TF\_{T}=S\_{0}\mathrm{e}^{(r-q)T}; calendar comparisons are done at fixed (K,T)(K,T) in the same numeraire.

#### Assumptions.

(i) *Convexâ€“monotone constraints.* For each TT, Kâ†¦Câ€‹(K,T)K\mapsto C(K,T) is convex and nonincreasing, and for each KK, Tâ†¦Câ€‹(K,T)T\mapsto C(K,T) is nondecreasing. In differential form,

|  |  |  |
| --- | --- | --- |
|  | âˆ‚Kâ€‹K2Câ€‹(K,T)â‰¥0,âˆ‚KCâ€‹(K,T)â‰¤0,âˆ‚TCâ€‹(K,T)â‰¥0,\partial\_{KK}^{2}C(K,T)\geq 0,\qquad\partial\_{K}C(K,T)\leq 0,\qquad\partial\_{T}C(K,T)\geq 0, |  |

with weak derivatives interpreted in the sense of distributions.
(ii) *Boundary and tail conditions.* As Kâ†“0K\downarrow 0, Câ€‹(K,T)â†’S0â€‹eâˆ’qâ€‹TC(K,T)\to S\_{0}\mathrm{e}^{-qT}; as Kâ†‘âˆK\uparrow\infty, Câ€‹(K,T)â†’0C(K,T)\to 0 and Câ€‹(K,T)â‰²Kâˆ’Î±C(K,T)\lesssim K^{-\alpha} for some Î±>1\alpha>1. These imply Câ€‹(â‹…,T)/K2C(\cdot,T)/K^{2} has bounded variation on compact intervals and integrable tails.
(iii) *VIX replication residual vanishes on the maturity grid.* For all TjT\_{j},

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„›VIXâ€‹(Tj)=VIXobs2â€‹(Tj)âˆ’VIXÎ¦2â€‹(Tj)=0,\mathcal{R}\_{\mathrm{VIX}}(T\_{j})=\mathrm{VIX}^{2}\_{\mathrm{obs}}(T\_{j})-\mathrm{VIX}^{2}\_{\Phi}(T\_{j})=0, |  | (51) |

where VIXÎ¦2â€‹(T)\mathrm{VIX}^{2}\_{\Phi}(T) is computed from C=Î¦C=\Phi via the discrete replication formulaÂ ([20](https://arxiv.org/html/2511.06451v1#S3.E20 "In SPXâ†”VIX replication. â€£ 3.3 Convexâ€“Monotone Decoder and SPXâ€“VIX Coupling â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) (including the standard forward adjustment).

#### Step 1 (butterfly arbitrage on the grid is excluded).

Fix TjT\_{j}. Since Kâ†¦Câ€‹(K,Tj)K\mapsto C(K,T\_{j}) is convex on (0,âˆ)(0,\infty) in the sense of distributions, the second difference

|  |  |  |
| --- | --- | --- |
|  | Î”K2â€‹Câ€‹(Ki,Tj):=Câ€‹(Kiâˆ’1,Tj)âˆ’2â€‹Câ€‹(Ki,Tj)+Câ€‹(Ki+1,Tj)â‰¥0\Delta^{2}\_{K}C(K\_{i},T\_{j}):=C(K\_{i-1},T\_{j})-2C(K\_{i},T\_{j})+C(K\_{i+1},T\_{j})\geq 0 |  |

for all interior indices i=2,â€¦,Mâˆ’1i=2,\dots,M-1; at endpoints, the one-sided convexity inequalities hold. Therefore, there is no butterfly arbitrage on the strike grid at TjT\_{j}. This is the classical discrete convexity criterion for absence of butterfly spreads.

#### Step 2 (calendar arbitrage on the grid is excluded).

Fix KiK\_{i}. Monotonicity âˆ‚TCâ€‹(Ki,T)â‰¥0\partial\_{T}C(K\_{i},T)\geq 0 implies Câ€‹(Ki,Tj+1)â‰¥Câ€‹(Ki,Tj)C(K\_{i},T\_{j+1})\geq C(K\_{i},T\_{j}) for all jj. Hence there is no calendar arbitrage on the maturity grid at KiK\_{i}. The numeraire consistency follows since comparisons are made for the same (Ki,Tj)(K\_{i},T\_{j}) and the decoder already absorbs (r,q)(r,q) via the forward mapping.

#### Step 3 (BL density and consistency with VIX functional).

By convexity in KK and the tail conditions, the Breedenâ€“Litzenberger identity

|  |  |  |
| --- | --- | --- |
|  | fSTâ€‹(K)=erâ€‹Tâ€‹âˆ‚Kâ€‹K2Câ€‹(K,T)f\_{S\_{T}}(K)=\mathrm{e}^{rT}\,\partial\_{KK}^{2}C(K,T) |  |

defines a nonnegative measure integrating to erâ€‹Tâ€‹âˆ‚KCâ€‹(0+,T)âˆ’erâ€‹Tâ€‹âˆ‚KCâ€‹(âˆâˆ’,T)=1\mathrm{e}^{rT}\,\partial\_{K}C(0^{+},T)-\mathrm{e}^{rT}\,\partial\_{K}C(\infty^{-},T)=1; thus fSTf\_{S\_{T}} is a bona fide risk-neutral density. On the grid, the discrete counterpart reads

|  |  |  |
| --- | --- | --- |
|  | fSTjâ€‹(Ki)â‰ˆerâ€‹Tjâ€‹Câ€‹(Kiâˆ’1,Tj)âˆ’2â€‹Câ€‹(Ki,Tj)+Câ€‹(Ki+1,Tj)(Î”â€‹Ki)2,f\_{S\_{T\_{j}}}(K\_{i})\;\approx\;\mathrm{e}^{rT\_{j}}\,\frac{C(K\_{i-1},T\_{j})-2C(K\_{i},T\_{j})+C(K\_{i+1},T\_{j})}{(\Delta K\_{i})^{2}}, |  |

which is nonnegative by Step 1.

Consider the VIX functional (variance swap fair rate). In continuous form,

|  |  |  |  |
| --- | --- | --- | --- |
|  | ÏƒVS2â€‹(T)=2â€‹erâ€‹TTâ€‹(âˆ«0FTPK2â€‹ğ‘‘K+âˆ«FTâˆCK2â€‹ğ‘‘K)âˆ’1Tâ€‹(FTK0âˆ’1)2.\sigma^{2}\_{\mathrm{VS}}(T)=\frac{2\,\mathrm{e}^{rT}}{T}\!\left(\int\_{0}^{F\_{T}}\frac{P}{K^{2}}\,dK+\int\_{F\_{T}}^{\infty}\frac{C}{K^{2}}\,dK\right)-\frac{1}{T}\left(\frac{F\_{T}}{K\_{0}}-1\right)^{\!2}. |  | (52) |

By the Carrâ€“Madan log-contract identity and the BL relation,

|  |  |  |
| --- | --- | --- |
|  | 2â€‹erâ€‹TTâ€‹(âˆ«0FTPK2â€‹ğ‘‘K+âˆ«FTâˆCK2â€‹ğ‘‘K)=1Tâ€‹âˆ«0âˆÏˆâ€‹(K)â€‹erâ€‹Tâ€‹âˆ‚Kâ€‹K2Câ€‹(K,T)â€‹dâ€‹K,\frac{2\,\mathrm{e}^{rT}}{T}\!\left(\int\_{0}^{F\_{T}}\frac{P}{K^{2}}\,dK+\int\_{F\_{T}}^{\infty}\frac{C}{K^{2}}\,dK\right)=\frac{1}{T}\int\_{0}^{\infty}\psi(K)\,\mathrm{e}^{rT}\,\partial\_{KK}^{2}C(K,T)\,dK, |  |

for a positive kernel Ïˆâ€‹(K)\psi(K) whose action reproduces the log payoff; under our tails and boundary conditions the integration by parts is justified (all boundary terms vanish). Hence the VIX functional computed from CC is exactly the CesÃ ro mean of instantaneous variance under the density fSTf\_{S\_{T}}.

On the grid, with the midpoint quadrature âˆ‘iÎ”â€‹Kiâ€‹Qâ€‹(Ki,T)/Ki2\sum\_{i}\Delta K\_{i}\,Q(K\_{i},T)/K\_{i}^{2} and the forward adjustment, PropositionÂ A.3 (consistency of discretized replication) yields

|  |  |  |
| --- | --- | --- |
|  | VIXÎ¦2â€‹(Tj)=ÏƒVS,Î¦2â€‹(Tj)up to quadrature and tail errors vanishing with the mesh.\mathrm{VIX}^{2}\_{\Phi}(T\_{j})=\sigma^{2}\_{\mathrm{VS},\Phi}(T\_{j})\quad\text{up to quadrature and tail errors vanishing with the mesh.} |  |

By ([51](https://arxiv.org/html/2511.06451v1#Ax1.E51 "In Assumptions. â€£ A.4 Proof of Proposition 4: static no-arbitrage and replication consistency â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")), VIXÎ¦2â€‹(Tj)=VIXobs2â€‹(Tj)\mathrm{VIX}^{2}\_{\Phi}(T\_{j})=\mathrm{VIX}^{2}\_{\mathrm{obs}}(T\_{j}) for all jj, hence the BL-implied density from CC is consistent with the observed VIX2 functional on the maturity grid.

#### Putting the steps together.

Steps 1â€“2 establish the absence of butterfly and calendar arbitrage on the grid. Step 3 shows that the BL-implied density from CC reproduces the VIX2 functional when the replication residual vanishes (and, by A.3, in the mesh-refined limit). This proves PropositionÂ [4](https://arxiv.org/html/2511.06451v1#Thmproposition4 "Proposition 4 (Static no-arbitrage and replication consistency). â€£ SPXâ†”VIX replication. â€£ 3.3 Convexâ€“Monotone Decoder and SPXâ€“VIX Coupling â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures").

#### Remarks on implementation and interpolation.

(i) Linear interpolation in (K,Q)(K,Q) preserves piecewise convexity and thus nonnegativity of discrete second differences; cubic splines may reduce quadrature error but can introduce local nonconvexities between knots unless shape-constrained splines are used.
(ii) Calendar tests should be performed on the forward-adjusted scale if one compares prices under changing carry (r,q)(r,q). In our implementation, the decoder absorbs (r,q)(r,q) and produces monotonically nondecreasing Tâ†¦Câ€‹(K,T)T\mapsto C(K,T) directly.
(iii) On coarse grids, adding the forward adjustment term improves finite-grid consistency with ([52](https://arxiv.org/html/2511.06451v1#Ax1.E52 "In Step 3 (BL density and consistency with VIX functional). â€£ A.4 Proof of Proposition 4: static no-arbitrage and replication consistency â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) and reduces bias at short maturities.

### A.5 Proof of TheoremÂ [1](https://arxiv.org/html/2511.06451v1#Thmtheorem1 "Theorem 1 (Extragradient convergence to a noise ball). â€£ Convergence guarantee (noise-stable neighborhood). â€£ 3.4 Saddle-Point Training and Safety-Oriented Stopping â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures"): projected extragradient under Q-Align perturbations

#### Setting and assumptions.

We consider the monotone variational inequality VIâ€‹(F,ğ’µ)\mathrm{VI}(F,\mathcal{Z}): find zâ‹†âˆˆğ’µz^{\star}\in\mathcal{Z} such that

|  |  |  |  |
| --- | --- | --- | --- |
|  | âŸ¨Fâ€‹(zâ‹†),zâˆ’zâ‹†âŸ©â‰¥â€„0âˆ€zâˆˆğ’µ,\langle F(z^{\star}),z-z^{\star}\rangle\;\geq\;0\qquad\forall z\in\mathcal{Z}, |  | (53) |

with FF monotone, i.e., âŸ¨Fâ€‹(u)âˆ’Fâ€‹(v),uâˆ’vâŸ©â‰¥0\langle F(u)-F(v),u-v\rangle\geq 0 for all u,vu,v, and LL-Lipschitz, i.e., â€–Fâ€‹(u)âˆ’Fâ€‹(v)â€–â‰¤Lâ€‹â€–uâˆ’vâ€–\|F(u)-F(v)\|\leq L\|u-v\|. The projection Î ğ’µ\Pi\_{\mathcal{Z}} is nonexpansive. Q-Align enforces per-iteration spectral/Lipschitz projections inside the model; we capture the induced numerical and truncation inaccuracies by perturbations ek,e~ke^{k},\tilde{e}^{k} satisfying

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–ekâ€–+â€–e~kâ€–â‰¤cqaâ€‹Î·,for some constantÂ â€‹cqa>0,\|e^{k}\|+\|\tilde{e}^{k}\|\;\leq\;c\_{\mathrm{qa}}\eta,\qquad\text{for some constant }c\_{\mathrm{qa}}>0, |  | (54) |

which matches the empirical scaling reported in the logs (cf. Î»lip\lambda\_{\mathrm{lip}} before/after and spectral-guard distances). Stochastic gradients enter via martingale-difference noise Î¾k,Î¾~k\xi^{k},\tilde{\xi}^{k} with

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â€‹[Î¾kâˆ£â„±k]=0,ğ”¼â€‹â€–Î¾kâ€–2â‰¤Ïƒ2,ğ”¼â€‹[Î¾~kâˆ£â„±k+1/2]=0,ğ”¼â€‹â€–Î¾~kâ€–2â‰¤Ïƒ2.\mathbb{E}[\xi^{k}\mid\mathcal{F}\_{k}]=0,\ \ \mathbb{E}\|\xi^{k}\|^{2}\leq\sigma^{2},\qquad\mathbb{E}[\tilde{\xi}^{k}\mid\mathcal{F}\_{k+1/2}]=0,\ \ \mathbb{E}\|\tilde{\xi}^{k}\|^{2}\leq\sigma^{2}. |  | (55) |

#### Algorithmic step.

Given zkâˆˆğ’µz^{k}\in\mathcal{Z}, define

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | yk=Î ğ’µâ€‹(zkâˆ’Î·â€‹(Fâ€‹(zk)âˆ’Î¾kâˆ’ek)),\displaystyle y^{k}=\Pi\_{\mathcal{Z}}\big(z^{k}-\eta\big(F(z^{k})-\xi^{k}-e^{k}\big)\big), |  | (56) |
|  |  | zk+1=Î ğ’µâ€‹(zkâˆ’Î·â€‹(Fâ€‹(yk)âˆ’Î¾~kâˆ’e~k)),\displaystyle z^{k+1}=\Pi\_{\mathcal{Z}}\big(z^{k}-\eta\big(F(y^{k})-\tilde{\xi}^{k}-\tilde{e}^{k}\big)\big), |  |

with a stepsize Î·â‰¤1/(2â€‹L)\eta\leq 1/(\sqrt{2}\,L) specified later. The residual of interest is either the natural projected residual

|  |  |  |
| --- | --- | --- |
|  | RÎ·â€‹(z):=1Î·â€‹(zâˆ’Î ğ’µâ€‹(zâˆ’Î·â€‹Fâ€‹(z))),R\_{\eta}(z):=\frac{1}{\eta}\Big(z-\Pi\_{\mathcal{Z}}\big(z-\eta F(z)\big)\Big), |  |

or the operator norm â€–Fâ€‹(z)â€–\|F(z)\|. For monotone Lipschitz FF and Î·â‰¤1/L\eta\leq 1/L, it is standard that â€–RÎ·â€‹(z)â€–â‰¤(1+Î·â€‹L)â€‹â€–Fâ€‹(z)â€–\|R\_{\eta}(z)\|\leq(1+\eta L)\|F(z)\| (see LemmaÂ [7](https://arxiv.org/html/2511.06451v1#Thmlemma7 "Lemma 7. â€£ Auxiliary lemma (residual bridge). â€£ A.5 Proof of Theorem 1: projected extragradient under Q-Align perturbations â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures") below), hence controlling one controls the other up to constants.

#### Key inequalities.

We recall the three-point identity for projections: for any uâˆˆâ„du\in\mathbb{R}^{d} and w=Î ğ’µâ€‹(u)w=\Pi\_{\mathcal{Z}}(u), and any vâˆˆğ’µv\in\mathcal{Z},

|  |  |  |  |
| --- | --- | --- | --- |
|  | âŸ¨uâˆ’w,vâˆ’wâŸ©â‰¤0â‡’â€–vâˆ’wâ€–2â‰¤â€–vâˆ’uâ€–2âˆ’â€–wâˆ’uâ€–2.\langle u-w,\,v-w\rangle\leq 0\quad\Rightarrow\quad\|v-w\|^{2}\leq\|v-u\|^{2}-\|w-u\|^{2}. |  | (57) |

Apply ([57](https://arxiv.org/html/2511.06451v1#Ax1.E57 "In Key inequalities. â€£ A.5 Proof of Theorem 1: projected extragradient under Q-Align perturbations â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) to the first stage of ([56](https://arxiv.org/html/2511.06451v1#Ax1.E56 "In Algorithmic step. â€£ A.5 Proof of Theorem 1: projected extragradient under Q-Align perturbations â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) with u=zkâˆ’Î·â€‹(Fâ€‹(zk)âˆ’Î¾kâˆ’ek)u=z^{k}-\eta(F(z^{k})-\xi^{k}-e^{k}), w=ykw=y^{k} and v=zâ‹†v=z^{\star}:

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–zâ‹†âˆ’ykâ€–2â‰¤â€–zâ‹†âˆ’zkâ€–2âˆ’â€–ykâˆ’zkâ€–2âˆ’2â€‹Î·â€‹âŸ¨Fâ€‹(zk)âˆ’Î¾kâˆ’ek,ykâˆ’zkâŸ©.\|z^{\star}-y^{k}\|^{2}\leq\|z^{\star}-z^{k}\|^{2}-\|y^{k}-z^{k}\|^{2}-2\eta\langle F(z^{k})-\xi^{k}-e^{k},\,y^{k}-z^{k}\rangle. |  | (58) |

Similarly for the second stage with u=zkâˆ’Î·â€‹(Fâ€‹(yk)âˆ’Î¾~kâˆ’e~k)u=z^{k}-\eta(F(y^{k})-\tilde{\xi}^{k}-\tilde{e}^{k}), w=zk+1w=z^{k+1} and v=zâ‹†v=z^{\star}:

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–zâ‹†âˆ’zk+1â€–2â‰¤â€–zâ‹†âˆ’zkâ€–2âˆ’â€–zk+1âˆ’zkâ€–2âˆ’2â€‹Î·â€‹âŸ¨Fâ€‹(yk)âˆ’Î¾~kâˆ’e~k,zk+1âˆ’zkâŸ©.\|z^{\star}-z^{k+1}\|^{2}\leq\|z^{\star}-z^{k}\|^{2}-\|z^{k+1}-z^{k}\|^{2}-2\eta\langle F(y^{k})-\tilde{\xi}^{k}-\tilde{e}^{k},\,z^{k+1}-z^{k}\rangle. |  | (59) |

#### Monotonicity coupling.

Using Lipschitzness and Cauchyâ€“Schwarz,

|  |  |  |
| --- | --- | --- |
|  | âŸ¨Fâ€‹(yk)âˆ’Fâ€‹(zk),ykâˆ’zkâŸ©â‰¥1Lâ€‹â€–Fâ€‹(yk)âˆ’Fâ€‹(zk)â€–2,\langle F(y^{k})-F(z^{k}),\,y^{k}-z^{k}\rangle\geq\frac{1}{L}\|F(y^{k})-F(z^{k})\|^{2}, |  |

and monotonicity yields

|  |  |  |  |
| --- | --- | --- | --- |
|  | âŸ¨Fâ€‹(yk),ykâˆ’zâ‹†âŸ©â‰¥âŸ¨Fâ€‹(zâ‹†),ykâˆ’zâ‹†âŸ©â‰¥0.\langle F(y^{k}),\,y^{k}-z^{\star}\rangle\geq\langle F(z^{\star}),\,y^{k}-z^{\star}\rangle\geq 0. |  | (60) |

Split the last inner product in ([58](https://arxiv.org/html/2511.06451v1#Ax1.E58 "In Key inequalities. â€£ A.5 Proof of Theorem 1: projected extragradient under Q-Align perturbations â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) as

|  |  |  |
| --- | --- | --- |
|  | âŸ¨Fâ€‹(zk),ykâˆ’zkâŸ©=âŸ¨Fâ€‹(yk),ykâˆ’zkâŸ©+âŸ¨Fâ€‹(zk)âˆ’Fâ€‹(yk),ykâˆ’zkâŸ©â‰¥1Lâ€‹â€–Fâ€‹(yk)âˆ’Fâ€‹(zk)â€–2,\langle F(z^{k}),y^{k}-z^{k}\rangle=\langle F(y^{k}),y^{k}-z^{k}\rangle+\langle F(z^{k})-F(y^{k}),y^{k}-z^{k}\rangle\geq\frac{1}{L}\|F(y^{k})-F(z^{k})\|^{2}, |  |

hence

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–zâ‹†âˆ’ykâ€–2â‰¤â€–zâ‹†âˆ’zkâ€–2âˆ’â€–ykâˆ’zkâ€–2âˆ’2â€‹Î·Lâ€‹â€–Fâ€‹(yk)âˆ’Fâ€‹(zk)â€–2+2â€‹Î·â€‹âŸ¨Î¾k+ek,ykâˆ’zkâŸ©.\|z^{\star}-y^{k}\|^{2}\leq\|z^{\star}-z^{k}\|^{2}-\|y^{k}-z^{k}\|^{2}-\frac{2\eta}{L}\|F(y^{k})-F(z^{k})\|^{2}+2\eta\langle\xi^{k}+e^{k},\,y^{k}-z^{k}\rangle. |  | (61) |

Likewise, decompose the inner product in ([59](https://arxiv.org/html/2511.06451v1#Ax1.E59 "In Key inequalities. â€£ A.5 Proof of Theorem 1: projected extragradient under Q-Align perturbations â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) using zk+1âˆ’zk=(zk+1âˆ’yk)+(ykâˆ’zk)z^{k+1}-z^{k}=(z^{k+1}-y^{k})+(y^{k}-z^{k}) and addâ€“subtract âŸ¨Fâ€‹(yk),ykâˆ’zâ‹†âŸ©\langle F(y^{k}),y^{k}-z^{\star}\rangle; routine algebra (see, e.g., the Mirrorâ€“Prox analysis) yields

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–zâ‹†âˆ’zk+1â€–2â‰¤â€–zâ‹†âˆ’zkâ€–2âˆ’â€–zk+1âˆ’zkâ€–2âˆ’2â€‹Î·â€‹âŸ¨Fâ€‹(yk),ykâˆ’zâ‹†âŸ©+Î·2â€‹L2â€‹â€–ykâˆ’zkâ€–2+Noisek+ProjErrk,\|z^{\star}-z^{k+1}\|^{2}\leq\|z^{\star}-z^{k}\|^{2}-\|z^{k+1}-z^{k}\|^{2}-2\eta\langle F(y^{k}),\,y^{k}-z^{\star}\rangle+\eta^{2}L^{2}\|y^{k}-z^{k}\|^{2}+\mathrm{Noise}\_{k}+\mathrm{ProjErr}\_{k}, |  | (62) |

where

|  |  |  |
| --- | --- | --- |
|  | Noisek:=2â€‹Î·â€‹âŸ¨Î¾~k,zk+1âˆ’zkâŸ©,ProjErrk:=2â€‹Î·â€‹âŸ¨e~k,zk+1âˆ’zkâŸ©.\mathrm{Noise}\_{k}:=2\eta\langle\tilde{\xi}^{k},z^{k+1}-z^{k}\rangle,\qquad\mathrm{ProjErr}\_{k}:=2\eta\langle\tilde{e}^{k},z^{k+1}-z^{k}\rangle. |  |

#### One-step merit bound.

Combine ([61](https://arxiv.org/html/2511.06451v1#Ax1.E61 "In Monotonicity coupling. â€£ A.5 Proof of Theorem 1: projected extragradient under Q-Align perturbations â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures"))â€“([62](https://arxiv.org/html/2511.06451v1#Ax1.E62 "In Monotonicity coupling. â€£ A.5 Proof of Theorem 1: projected extragradient under Q-Align perturbations â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) and use ([60](https://arxiv.org/html/2511.06451v1#Ax1.E60 "In Monotonicity coupling. â€£ A.5 Proof of Theorem 1: projected extragradient under Q-Align perturbations â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) to eliminate the nonnegative term âŸ¨Fâ€‹(yk),ykâˆ’zâ‹†âŸ©\langle F(y^{k}),\,y^{k}-z^{\star}\rangle:

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | â€–zâ‹†âˆ’zk+1â€–2\displaystyle\|z^{\star}-z^{k+1}\|^{2} | â‰¤â€–zâ‹†âˆ’zkâ€–2âˆ’â€–zk+1âˆ’zkâ€–2+Î·2â€‹L2â€‹â€–ykâˆ’zkâ€–2+Noisek+ProjErrk.\displaystyle\leq\|z^{\star}-z^{k}\|^{2}-\|z^{k+1}-z^{k}\|^{2}+\eta^{2}L^{2}\|y^{k}-z^{k}\|^{2}+\mathrm{Noise}\_{k}+\mathrm{ProjErr}\_{k}. |  | (63) |

Choose Î·â‰¤1/(2â€‹L)\eta\leq 1/(\sqrt{2}L) so that Î·2â€‹L2â‰¤1/2\eta^{2}L^{2}\leq 1/2. By Youngâ€™s inequality,

|  |  |  |
| --- | --- | --- |
|  | â€–zk+1âˆ’zkâ€–2â‰¥12â€‹â€–ykâˆ’zkâ€–2âˆ’â€–zk+1âˆ’ykâ€–2.\|z^{k+1}-z^{k}\|^{2}\geq\frac{1}{2}\|y^{k}-z^{k}\|^{2}-\|z^{k+1}-y^{k}\|^{2}. |  |

Applying nonexpansiveness of projection to the second stage of ([56](https://arxiv.org/html/2511.06451v1#Ax1.E56 "In Algorithmic step. â€£ A.5 Proof of Theorem 1: projected extragradient under Q-Align perturbations â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) shows â€–zk+1âˆ’ykâ€–â‰¤Î·â€‹â€–Fâ€‹(yk)âˆ’Î¾~kâˆ’e~kâ€–\|z^{k+1}-y^{k}\|\leq\eta\|F(y^{k})-\tilde{\xi}^{k}-\tilde{e}^{k}\|, so

|  |  |  |
| --- | --- | --- |
|  | â€–zk+1âˆ’ykâ€–2â‰¤2â€‹Î·2â€‹(â€–Fâ€‹(yk)â€–2+â€–Î¾~kâ€–2+â€–e~kâ€–2).\|z^{k+1}-y^{k}\|^{2}\leq 2\eta^{2}\big(\|F(y^{k})\|^{2}+\|\tilde{\xi}^{k}\|^{2}+\|\tilde{e}^{k}\|^{2}\big). |  |

Plugging the last two displays into ([63](https://arxiv.org/html/2511.06451v1#Ax1.E63 "In One-step merit bound. â€£ A.5 Proof of Theorem 1: projected extragradient under Q-Align perturbations â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")), taking conditional expectations, and using ([55](https://arxiv.org/html/2511.06451v1#Ax1.E55 "In Setting and assumptions. â€£ A.5 Proof of Theorem 1: projected extragradient under Q-Align perturbations â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures"))â€“([54](https://arxiv.org/html/2511.06451v1#Ax1.E54 "In Setting and assumptions. â€£ A.5 Proof of Theorem 1: projected extragradient under Q-Align perturbations â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) yield

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â€‹[â€–zâ‹†âˆ’zk+1â€–2âˆ£â„±k]â‰¤â€–zâ‹†âˆ’zkâ€–2âˆ’14â€‹â€–ykâˆ’zkâ€–2+4â€‹Î·2â€‹ğ”¼â€‹â€–Fâ€‹(yk)â€–2+c1â€‹Î·2â€‹Ïƒ2+c2â€‹Î·2,\mathbb{E}\big[\|z^{\star}-z^{k+1}\|^{2}\mid\mathcal{F}\_{k}\big]\leq\|z^{\star}-z^{k}\|^{2}-\frac{1}{4}\|y^{k}-z^{k}\|^{2}+4\eta^{2}\mathbb{E}\|F(y^{k})\|^{2}+c\_{1}\eta^{2}\sigma^{2}+c\_{2}\eta^{2}, |  | (64) |

for some universal constants c1,c2c\_{1},c\_{2}.

#### Residual bridging.

We relate â€–ykâˆ’zkâ€–\|y^{k}-z^{k}\| to a first-order residual. By ([56](https://arxiv.org/html/2511.06451v1#Ax1.E56 "In Algorithmic step. â€£ A.5 Proof of Theorem 1: projected extragradient under Q-Align perturbations â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) and firm nonexpansiveness of projection,

|  |  |  |
| --- | --- | --- |
|  | 1Î·â€‹â€–zkâˆ’ykâ€–â‰¤â€–Fâ€‹(zk)â€–+â€–Î¾kâ€–+â€–ekâ€–.\frac{1}{\eta}\|z^{k}-y^{k}\|\leq\|F(z^{k})\|+\|\xi^{k}\|+\|e^{k}\|. |  |

Also, Lipschitzness implies â€–Fâ€‹(yk)â€–â‰¤â€–Fâ€‹(zk)â€–+Lâ€‹â€–ykâˆ’zkâ€–\|F(y^{k})\|\leq\|F(z^{k})\|+L\|y^{k}-z^{k}\|. Combining these with ([64](https://arxiv.org/html/2511.06451v1#Ax1.E64 "In One-step merit bound. â€£ A.5 Proof of Theorem 1: projected extragradient under Q-Align perturbations â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")), taking full expectations and using Î·â‰¤1/(2â€‹L)\eta\leq 1/(\sqrt{2}L), we obtain

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â€‹â€–zâ‹†âˆ’zk+1â€–2â‰¤ğ”¼â€‹â€–zâ‹†âˆ’zkâ€–2âˆ’c3â€‹Î·2â€‹ğ”¼â€‹â€–Fâ€‹(zk)â€–2+c4â€‹Î·2â€‹Ïƒ2+c5â€‹Î·2,\mathbb{E}\|z^{\star}-z^{k+1}\|^{2}\leq\mathbb{E}\|z^{\star}-z^{k}\|^{2}-c\_{3}\eta^{2}\mathbb{E}\|F(z^{k})\|^{2}+c\_{4}\eta^{2}\sigma^{2}+c\_{5}\eta^{2}, |  | (65) |

for some constants c3,c4,c5>0c\_{3},c\_{4},c\_{5}>0 (the last term absorbs Q-Align errors through ([54](https://arxiv.org/html/2511.06451v1#Ax1.E54 "In Setting and assumptions. â€£ A.5 Proof of Theorem 1: projected extragradient under Q-Align perturbations â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")), thus is ğ’ªâ€‹(Î·2)\mathcal{O}(\eta^{2})).

#### Summation and choice of stepsize.

Sum ([65](https://arxiv.org/html/2511.06451v1#Ax1.E65 "In Residual bridging. â€£ A.5 Proof of Theorem 1: projected extragradient under Q-Align perturbations â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) from k=0k=0 to Kâˆ’1K-1, telescope the left-hand side, and choose Î·=Î¸/L\eta=\theta/L with a small absolute constant Î¸>0\theta>0. We obtain

|  |  |  |
| --- | --- | --- |
|  | 1Kâ€‹âˆ‘k=0Kâˆ’1ğ”¼â€‹â€–Fâ€‹(zk)â€–2â‰¤ğ’ªâ€‹(L2â€‹â€–z0âˆ’zâ‹†â€–2K)+ğ’ªâ€‹(Ïƒ2)+ğ’ªâ€‹(1L2).\frac{1}{K}\sum\_{k=0}^{K-1}\mathbb{E}\|F(z^{k})\|^{2}\;\leq\;\mathcal{O}\!\left(\frac{L^{2}\|z^{0}-z^{\star}\|^{2}}{K}\right)\;+\;\mathcal{O}\!\left(\sigma^{2}\right)\;+\;\mathcal{O}\!\left(\frac{1}{L^{2}}\right). |  |

Since the Q-Align term is ğ’ªâ€‹(1/L2)\mathcal{O}(1/L^{2}) under ([54](https://arxiv.org/html/2511.06451v1#Ax1.E54 "In Setting and assumptions. â€£ A.5 Proof of Theorem 1: projected extragradient under Q-Align perturbations â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")), it is dominated by the noise floor ğ’ªâ€‹(Ïƒ2)\mathcal{O}(\sigma^{2}) in practical regimes; removing constants and using the fact that minkâ¡akâ‰¤1Kâ€‹âˆ‘kak\min\_{k}a\_{k}\leq\frac{1}{K}\sum\_{k}a\_{k} gives the claimed bound

|  |  |  |
| --- | --- | --- |
|  | min0â‰¤kâ‰¤Kâˆ’1â¡ğ”¼â€‹â€–Fâ€‹(zk)â€–2â‰¤ğ’ªâ€‹(L2â€‹â€–z0âˆ’zâ‹†â€–2K)+ğ’ªâ€‹(Ïƒ2).\min\_{0\leq k\leq K-1}\ \mathbb{E}\|F(z^{k})\|^{2}\;\leq\;\mathcal{O}\!\left(\frac{L^{2}\|z^{0}-z^{\star}\|^{2}}{K}\right)\;+\;\mathcal{O}\!\left(\sigma^{2}\right). |  |

#### Auxiliary lemma (residual bridge).

###### Lemma 7.

For Î·â‰¤1/L\eta\leq 1/L and any zâˆˆğ’µz\in\mathcal{Z},

|  |  |  |
| --- | --- | --- |
|  | â€–RÎ·â€‹(z)â€–â‰¤(1+Î·â€‹L)â€‹â€–Fâ€‹(z)â€–,â€–Fâ€‹(z)â€–â‰¤â€–RÎ·â€‹(z)â€–+Î·â€‹Lâ€‹â€–Fâ€‹(z)â€–.\|R\_{\eta}(z)\|\;\leq\;(1+\eta L)\,\|F(z)\|,\qquad\|F(z)\|\;\leq\;\|R\_{\eta}(z)\|+\eta L\,\|F(z)\|. |  |

Hence â€–RÎ·â€‹(z)â€–2\|R\_{\eta}(z)\|^{2} and â€–Fâ€‹(z)â€–2\|F(z)\|^{2} are equivalent up to ğ’ªâ€‹(1)\mathcal{O}(1) constants depending only on Î·â€‹L\eta L.

*Proof.* By nonexpansiveness of Î ğ’µ\Pi\_{\mathcal{Z}},

|  |  |  |
| --- | --- | --- |
|  | â€–RÎ·â€‹(z)â€–=1Î·â€‹â€–zâˆ’Î ğ’µâ€‹(zâˆ’Î·â€‹Fâ€‹(z))â€–â‰¤1Î·â€‹â€–zâˆ’(zâˆ’Î·â€‹Fâ€‹(z))â€–=â€–Fâ€‹(z)â€–.\|R\_{\eta}(z)\|=\frac{1}{\eta}\big\|z-\Pi\_{\mathcal{Z}}(z-\eta F(z))\big\|\leq\frac{1}{\eta}\|z-(z-\eta F(z))\|=\|F(z)\|. |  |

The reverse direction follows by addingâ€“subtracting zâˆ’Î·â€‹Fâ€‹(z)z-\eta F(z) inside the projection and applying Lipschitzness of FF; details are standard and omitted. âˆ

#### Deterministic corollary.

If Ïƒ=0\sigma=0 (deterministic gradients), the rate improves to

|  |  |  |
| --- | --- | --- |
|  | min0â‰¤kâ‰¤Kâˆ’1â¡â€–Fâ€‹(zk)â€–2â‰¤ğ’ªâ€‹(L2â€‹â€–z0âˆ’zâ‹†â€–2K),\min\_{0\leq k\leq K-1}\ \|F(z^{k})\|^{2}\;\leq\;\mathcal{O}\!\left(\frac{L^{2}\|z^{0}-z^{\star}\|^{2}}{K}\right), |  |

matching classical extragradient rates for monotone Lipschitz VIs.

#### Remarks.

(i) Strong monotonicity (with modulus Î¼>0\mu>0) yields a linear convergence term ğ’ªâ€‹((1âˆ’Î·â€‹Î¼)K)\mathcal{O}\big((1-\eta\mu)^{K}\big) until it hits the same ğ’ªâ€‹(Ïƒ2)\mathcal{O}(\sigma^{2}) noise floor.
(ii) The Q-Align perturbations are â€œbenignâ€ provided ([54](https://arxiv.org/html/2511.06451v1#Ax1.E54 "In Setting and assumptions. â€£ A.5 Proof of Theorem 1: projected extragradient under Q-Align perturbations â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) holds; empirically, the spectral guard logs (Î»lip\lambda\_{\mathrm{lip}} before/after and projection distances) conform to this scaling.
(iii) The same analysis extends to mirror-prox with a distance-generating function; we focus on the Euclidean case for clarity.

This completes the proof of TheoremÂ [1](https://arxiv.org/html/2511.06451v1#Thmtheorem1 "Theorem 1 (Extragradient convergence to a noise ball). â€£ Convergence guarantee (noise-stable neighborhood). â€£ 3.4 Saddle-Point Training and Safety-Oriented Stopping â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures").

### B.1 Proof of TheoremÂ [2](https://arxiv.org/html/2511.06451v1#Thmtheorem2 "Theorem 2 (Approximation rate and conditioning). â€£ T1: Approximation Error and Conditioning â€£ 4 Theoretical Results â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures"): approximation rate and conditioning

We prove the two claims in ([28](https://arxiv.org/html/2511.06451v1#S4.E28 "In Theorem 2 (Approximation rate and conditioning). â€£ T1: Approximation Error and Conditioning â€£ 4 Theoretical Results â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")): the mâˆ’Î²smoothm^{-\beta\_{\mathrm{smooth}}} approximation rate and the spectral conditioning proxy bound. Throughout, ğ’µâŠ‚â„dz\mathcal{Z}\subset\mathbb{R}^{d\_{z}} is compact, fâ‹†f^{\star} is Î²smooth\beta\_{\mathrm{smooth}}-HÃ¶lder on ğ’µ\mathcal{Z} and jointly HÃ¶lder in the maturity argument Tâˆˆğ’¯=[Tmin,Tmax]T\in\mathcal{T}=[T\_{\min},T\_{\max}]. The RN-operator ğ’¢Î¸\mathcal{G}\_{\theta} is realized by a selective-scan (RN-Operator) layer followed by a convexâ€“monotone decoder, with Q-Align ensuring per-layer 11-Lipschitz projections and spectral safety (Spec-Guard). We use âˆ¥â‹…âˆ¥\|\cdot\| for the Euclidean or spectral norm depending on context.

#### Model parameterization.

Let {Tâ„“}â„“=1L\{T\_{\ell}\}\_{\ell=1}^{L} be the maturity grid. One-step RN dynamics writes

|  |  |  |  |
| --- | --- | --- | --- |
|  | hâ„“=GÎ¸â€‹(Tâ„“,Tâ„“âˆ’1)â€‹hâ„“âˆ’1+BÎ¸â€‹(Tâ„“)â€‹uâ„“,GÎ¸=expâ¡(Î”â€‹tâ„“â€‹AÎ¸â€‹(Tâ„“)),h\_{\ell}=G\_{\theta}(T\_{\ell},T\_{\ell-1})\,h\_{\ell-1}+B\_{\theta}(T\_{\ell})\,u\_{\ell},\qquad G\_{\theta}=\exp\!\big(\Delta t\_{\ell}A\_{\theta}(T\_{\ell})\big), |  | (66) |

with Î”â€‹tâ„“=Tâ„“âˆ’Tâ„“âˆ’1\Delta t\_{\ell}=T\_{\ell}-T\_{\ell-1}. Under Spec-Guard, Ïâ€‹(AÎ¸â€‹(Tâ„“))â€‹Î”â€‹tâ„“â‰¤1âˆ’Îµ\rho(A\_{\theta}(T\_{\ell}))\Delta t\_{\ell}\leq 1-\varepsilon for some Îµâˆˆ(0,1)\varepsilon\in(0,1), hence the associated Green kernel

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ’¢Î¸â€‹(Tâ„“,Ts):=âˆr=s+1â„“GÎ¸â€‹(Tr,Trâˆ’1)\mathcal{G}\_{\theta}(T\_{\ell},T\_{s}):=\prod\_{r=s+1}^{\ell}G\_{\theta}(T\_{r},T\_{r-1}) |  | (67) |

satisfies the Neumann-type bound (LemmaÂ [1](https://arxiv.org/html/2511.06451v1#Thmlemma1 "Lemma 1 (Green kernel bound). â€£ Spectral safety and discrete Green kernel. â€£ 3.1 Riskâ€“Neutral Operator Layer (RN-Operator) â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")):

|  |  |  |  |
| --- | --- | --- | --- |
|  | âˆ‘sâ‰¤â„“â€–ğ’¢Î¸â€‹(Tâ„“,Ts)â€–â‰¤Câ€‹(Îµ),uniformly inÂ â€‹â„“.\sum\_{s\leq\ell}\big\|\mathcal{G}\_{\theta}(T\_{\ell},T\_{s})\big\|\ \leq\ C(\varepsilon),\qquad\text{uniformly in }\ell. |  | (68) |

The output price surface before the convexâ€“monotone decoder is a scan of the input features {us}\{u\_{s}\}:

|  |  |  |  |
| --- | --- | --- | --- |
|  | zâ„“â€‹(â‹…)=âˆ‘sâ‰¤â„“ğ’¢Î¸â€‹(Tâ„“,Ts)â€‹BÎ¸â€‹(Ts)â€‹usâ€‹(â‹…),z\_{\ell}(\cdot)=\sum\_{s\leq\ell}\mathcal{G}\_{\theta}(T\_{\ell},T\_{s})\,B\_{\theta}(T\_{s})\,u\_{s}(\cdot), |  | (69) |

and the decoder Î¦Î¸\Phi\_{\theta} (ICNN+Legendre projection) is 11-Lipschitz under Q-Align.

We adopt a low-rank gate parameterization

|  |  |  |  |
| --- | --- | --- | --- |
|  | BÎ¸â€‹(T)=âˆ‘j=1mbjâ€‹(T)â€‹wjâ€‹vjâŠ¤,AÎ¸â€‹(T)=DÎ¸â€‹(T)+âˆ‘j=1majâ€‹(T)â€‹rjâ€‹qjâŠ¤,B\_{\theta}(T)=\sum\_{j=1}^{m}b\_{j}(T)\,w\_{j}v\_{j}^{\top},\qquad A\_{\theta}(T)=D\_{\theta}(T)+\sum\_{j=1}^{m}a\_{j}(T)\,r\_{j}q\_{j}^{\top}, |  | (70) |

with â€–wjâ€–=â€–vjâ€–=â€–rjâ€–=â€–qjâ€–=1\|w\_{j}\|=\|v\_{j}\|=\|r\_{j}\|=\|q\_{j}\|=1 and aj,bja\_{j},b\_{j} bounded and Î²smooth\beta\_{\mathrm{smooth}}-HÃ¶lder in TT (enforced by per-step spectral/Lipschitz projection). The rank surrogate is thus mm.

#### Part I: approximation rate.

We consider the target operator fâ‹†:(u,â‹…)â†¦Câ‹†â€‹(â‹…)f^{\star}:(u,\cdot)\mapsto C^{\star}(\cdot), which we assume admits a separable Green-type expansion with HÃ¶lder control:

|  |  |  |  |
| --- | --- | --- | --- |
|  | fâ‹†â€‹(u)â€‹(T,Î¾)=âˆ‘j=1âˆÎ±jâ€‹Ïˆjâ€‹(T)â€‹Ï†jâ€‹(u;Î¾),âˆ‘j=1âˆjÎ²smoothâ€‹|Î±j|â‰¤M<âˆ,f^{\star}(u)(T,\xi)=\sum\_{j=1}^{\infty}\alpha\_{j}\,\psi\_{j}(T)\,\varphi\_{j}(u;\xi),\qquad\sum\_{j=1}^{\infty}j^{\beta\_{\mathrm{smooth}}}\,|\alpha\_{j}|\ \leq\ M<\infty, |  | (71) |

where {Ïˆj}\{\psi\_{j}\} is a smooth dictionary on ğ’¯\mathcal{T} (e.g., integrated B-splines or compactly supported wavelets) with Î²smooth\beta\_{\mathrm{smooth}}-HÃ¶lder regularity and {Ï†j}\{\varphi\_{j}\} are feature functionals uniformly bounded on ğ’µ\mathcal{Z}. Such expansions are classical for HÃ¶lder classes via nonlinear mm-term approximations with wavelet or spline dictionaries (see, e.g., DeVoreâ€“Temlyakov mm-term approximation theory). The coefficient decay condition in ([71](https://arxiv.org/html/2511.06451v1#Ax1.E71 "In Part I: approximation rate. â€£ B.1 Proof of Theorem 2: approximation rate and conditioning â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) is equivalent to fâ‹†f^{\star} belonging to a Besov/HÃ¶lder ball with smoothness Î²smooth\beta\_{\mathrm{smooth}}.

Define the mm-term truncation

|  |  |  |  |
| --- | --- | --- | --- |
|  | fmâ‹†â€‹(u)â€‹(T,Î¾)=âˆ‘j=1mÎ±jâ€‹Ïˆjâ€‹(T)â€‹Ï†jâ€‹(u;Î¾).f\_{m}^{\star}(u)(T,\xi)=\sum\_{j=1}^{m}\alpha\_{j}\,\psi\_{j}(T)\,\varphi\_{j}(u;\xi). |  | (72) |

By Stechkinâ€™s inequality for best mm-term approximations in â„“p\ell^{p} with p=1/Î²smoothp=1/\beta\_{\mathrm{smooth}} surrogate (monotone rearrangement of coefficients),

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–fâ‹†âˆ’fmâ‹†â€–L2â€‹(ğ’µ)â‰¤Câ€‹mâˆ’Î²smoothâ€‹(âˆ‘jâ‰¥1jÎ²smoothâ€‹|Î±j|)â‰¤Câ€²â€‹mâˆ’Î²smooth.\|f^{\star}-f\_{m}^{\star}\|\_{L^{2}(\mathcal{Z})}\ \leq\ C\,m^{-\beta\_{\mathrm{smooth}}}\,\bigg(\sum\_{j\geq 1}j^{\beta\_{\mathrm{smooth}}}\,|\alpha\_{j}|\bigg)\ \leq\ C^{\prime}m^{-\beta\_{\mathrm{smooth}}}. |  | (73) |

It remains to show that ğ’¢Î¸\mathcal{G}\_{\theta} can realize fmâ‹†f\_{m}^{\star} up to an arbitrarily small error when mm atoms are allocated in ([70](https://arxiv.org/html/2511.06451v1#Ax1.E70 "In Model parameterization. â€£ B.1 Proof of Theorem 2: approximation rate and conditioning â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")). Choose bjâ€‹(â‹…)b\_{j}(\cdot) so that the scan ([69](https://arxiv.org/html/2511.06451v1#Ax1.E69 "In Model parameterization. â€£ B.1 Proof of Theorem 2: approximation rate and conditioning â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) reproduces Ïˆj\psi\_{j} on the grid (standard for spline/wavelet reproduction using stable discrete Green convolutions), and set the feature directions vj,wjv\_{j},w\_{j} so that the linear functionals Ï†jâ€‹(u;â‹…)\varphi\_{j}(u;\cdot) are matched by uâ†¦vjâŠ¤â€‹uâ€‹(â‹…)u\mapsto v\_{j}^{\top}u(\cdot) and the decoderâ€™s linear readout (pre-ICNN) maps wjw\_{j} to the correct output channel. The ICNN+Legendre decoder, being 11-Lipschitz and positively homogeneous on the linear span of the constructed atoms, preserves the L2L^{2} approximation error.

Consequently, there exists Î¸=Î¸â€‹(m)\theta=\theta(m) with rank mm such that

|  |  |  |
| --- | --- | --- |
|  | â€–ğ’¢Î¸âˆ’fmâ‹†â€–L2â€‹(ğ’µ)â‰¤Îµm,withÂ â€‹Îµmâ†’0â€‹Â as the reproduction tolerance onÂ â€‹{Ïˆj,Ï†j}â€‹Â shrinks,\|\mathcal{G}\_{\theta}-f\_{m}^{\star}\|\_{L^{2}(\mathcal{Z})}\ \leq\ \varepsilon\_{m},\quad\text{with }\varepsilon\_{m}\to 0\text{ as the reproduction tolerance on }\{\psi\_{j},\varphi\_{j}\}\text{ shrinks}, |  |

and combining with ([73](https://arxiv.org/html/2511.06451v1#Ax1.E73 "In Part I: approximation rate. â€£ B.1 Proof of Theorem 2: approximation rate and conditioning â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) yields

|  |  |  |
| --- | --- | --- |
|  | infÎ¸â€–ğ’¢Î¸âˆ’fâ‹†â€–L2â€‹(ğ’µ)â‰¤â€–ğ’¢Î¸âˆ’fmâ‹†â€–L2â€‹(ğ’µ)+â€–fmâ‹†âˆ’fâ‹†â€–L2â€‹(ğ’µ)â‰¤C1â€‹mâˆ’Î²smooth,\inf\_{\theta}\|\mathcal{G}\_{\theta}-f^{\star}\|\_{L^{2}(\mathcal{Z})}\ \leq\ \|\mathcal{G}\_{\theta}-f\_{m}^{\star}\|\_{L^{2}(\mathcal{Z})}+\|f\_{m}^{\star}-f^{\star}\|\_{L^{2}(\mathcal{Z})}\ \leq\ C\_{1}m^{-\beta\_{\mathrm{smooth}}}, |  |

for C1C\_{1} independent of LL (the scan length), since the reproduction constants depend only on the dictionary stability and the Green kernel bound ([68](https://arxiv.org/html/2511.06451v1#Ax1.E68 "In Model parameterization. â€£ B.1 Proof of Theorem 2: approximation rate and conditioning â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")), which is uniform in LL under Spec-Guard.

*Remark A.1 (effective dimension).* If the target lacks separability, the same argument yields â€–fâ‹†âˆ’fmâ‹†â€–=ğ’ªâ€‹(mâˆ’Î²smooth/d^)\|f^{\star}-f\_{m}^{\star}\|=\mathcal{O}(m^{-\beta\_{\mathrm{smooth}}/\hat{d}}) with d^\hat{d} the effective approximation dimension.

#### Part II: conditioning bound.

Let ğ’¥Î¸\mathcal{J}\_{\theta} be the Jacobian of the overall mapping Î¸â†¦Î¦Î¸âˆ˜ğ–²ğ–¼ğ–ºğ—‡Î¸â€‹(u)\theta\mapsto\Phi\_{\theta}\circ\mathsf{Scan}\_{\theta}(u) evaluated on a bounded input uu (the bound is uniform over â€–uâ€–â‰¤U\|u\|\leq U). By the chain rule and ([69](https://arxiv.org/html/2511.06451v1#Ax1.E69 "In Model parameterization. â€£ B.1 Proof of Theorem 2: approximation rate and conditioning â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")),

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ’¥Î¸=Dâ€‹Î¦Î¸â€‹(z)â€‹âˆ‘â„“=1Lâˆ‘sâ‰¤â„“(ğ’¢Î¸â€‹(Tâ„“,Ts)â€‹âˆ‚Î¸BÎ¸â€‹(Ts)âŸdirect term+âˆ‚Î¸ğ’¢Î¸â€‹(Tâ„“,Ts)â€‹BÎ¸â€‹(Ts)âŸstate term)â€‹us,\mathcal{J}\_{\theta}\;=\;D\Phi\_{\theta}(z)\,\sum\_{\ell=1}^{L}\ \sum\_{s\leq\ell}\Big(\underbrace{\mathcal{G}\_{\theta}(T\_{\ell},T\_{s})\,\partial\_{\theta}B\_{\theta}(T\_{s})}\_{\text{direct term}}\;+\;\underbrace{\partial\_{\theta}\mathcal{G}\_{\theta}(T\_{\ell},T\_{s})\,B\_{\theta}(T\_{s})}\_{\text{state term}}\Big)\,u\_{s}, |  | (74) |

where Dâ€‹Î¦Î¸D\Phi\_{\theta} is the decoder Jacobian. Under Q-Align, every layer (encoder/base/decoder) is 11-Lipschitz after projection, so â€–Dâ€‹Î¦Î¸â€‹(z)â€–â‰¤1\|D\Phi\_{\theta}(z)\|\leq 1. For the direct term,

|  |  |  |
| --- | --- | --- |
|  | â€–ğ’¢Î¸â€‹(Tâ„“,Ts)â€‹âˆ‚Î¸BÎ¸â€‹(Ts)â€‹usâ€–â‰¤â€–ğ’¢Î¸â€‹(Tâ„“,Ts)â€–â€‹â€–âˆ‚Î¸BÎ¸â€‹(Ts)â€–â€‹â€–usâ€–.\big\|\mathcal{G}\_{\theta}(T\_{\ell},T\_{s})\,\partial\_{\theta}B\_{\theta}(T\_{s})\,u\_{s}\big\|\ \leq\ \|\mathcal{G}\_{\theta}(T\_{\ell},T\_{s})\|\ \|\partial\_{\theta}B\_{\theta}(T\_{s})\|\ \|u\_{s}\|. |  |

The low-rank gate ([70](https://arxiv.org/html/2511.06451v1#Ax1.E70 "In Model parameterization. â€£ B.1 Proof of Theorem 2: approximation rate and conditioning â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) implies â€–âˆ‚Î¸BÎ¸â€‹(T)â€–â‰¤cbâ€‹Lg\|\partial\_{\theta}B\_{\theta}(T)\|\leq c\_{b}\,L\_{g} with LgL\_{g} the Lipschitz constant (w.r.t. features/inputs) of the learned gates and cbc\_{b} a dimension-free constant tied to basis normalization. Summing over sâ‰¤â„“s\leq\ell and using ([68](https://arxiv.org/html/2511.06451v1#Ax1.E68 "In Model parameterization. â€£ B.1 Proof of Theorem 2: approximation rate and conditioning â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")),

|  |  |  |
| --- | --- | --- |
|  | âˆ‘sâ‰¤â„“â€–ğ’¢Î¸â€‹(Tâ„“,Ts)â€‹âˆ‚Î¸BÎ¸â€‹(Ts)â€‹usâ€–â‰¤Câ€‹(Îµ)â€‹cbâ€‹Lgâ€‹maxsâ¡â€–usâ€–.\sum\_{s\leq\ell}\big\|\mathcal{G}\_{\theta}(T\_{\ell},T\_{s})\,\partial\_{\theta}B\_{\theta}(T\_{s})\,u\_{s}\big\|\ \leq\ C(\varepsilon)\,c\_{b}\,L\_{g}\max\_{s}\|u\_{s}\|. |  |

For the state term, differentiate ([67](https://arxiv.org/html/2511.06451v1#Ax1.E67 "In Model parameterization. â€£ B.1 Proof of Theorem 2: approximation rate and conditioning â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")):

|  |  |  |
| --- | --- | --- |
|  | âˆ‚Î¸ğ’¢Î¸â€‹(Tâ„“,Ts)=âˆ‘r=s+1â„“(âˆq=r+1â„“GÎ¸â€‹(Tq,Tqâˆ’1))â€‹âˆ‚Î¸GÎ¸â€‹(Tr,Trâˆ’1)â€‹(âˆp=s+1râˆ’1GÎ¸â€‹(Tp,Tpâˆ’1)).\partial\_{\theta}\mathcal{G}\_{\theta}(T\_{\ell},T\_{s})=\sum\_{r=s+1}^{\ell}\Big(\prod\_{q=r+1}^{\ell}G\_{\theta}(T\_{q},T\_{q-1})\Big)\,\partial\_{\theta}G\_{\theta}(T\_{r},T\_{r-1})\,\Big(\prod\_{p=s+1}^{r-1}G\_{\theta}(T\_{p},T\_{p-1})\Big). |  |

Using âˆ‚Î¸GÎ¸â€‹(Tr,Trâˆ’1)=âˆ«01expâ¡(Ï„â€‹Î”â€‹trâ€‹AÎ¸)â€‹Î”â€‹trâ€‹âˆ‚Î¸AÎ¸â€‹expâ¡((1âˆ’Ï„)â€‹Î”â€‹trâ€‹AÎ¸)â€‹dâ€‹Ï„\partial\_{\theta}G\_{\theta}(T\_{r},T\_{r-1})=\int\_{0}^{1}\exp\!\big(\tau\Delta t\_{r}A\_{\theta}\big)\,\Delta t\_{r}\,\partial\_{\theta}A\_{\theta}\,\exp\!\big((1-\tau)\Delta t\_{r}A\_{\theta}\big)\,d\tau, we get

|  |  |  |
| --- | --- | --- |
|  | â€–âˆ‚Î¸GÎ¸â€‹(Tr,Trâˆ’1)â€–â‰¤Î”â€‹trâ€‹â€–âˆ‚Î¸AÎ¸â€‹(Tr)â€–â€‹supÏ„âˆˆ[0,1]â€–expâ¡(Ï„â€‹Î”â€‹trâ€‹AÎ¸)â€–2.\|\partial\_{\theta}G\_{\theta}(T\_{r},T\_{r-1})\|\leq\Delta t\_{r}\,\|\partial\_{\theta}A\_{\theta}(T\_{r})\|\,\sup\_{\tau\in[0,1]}\big\|\exp(\tau\Delta t\_{r}A\_{\theta})\big\|^{2}. |  |

Under Spec-Guard and spectral projection, supÏ„â€–expâ¡(Ï„â€‹Î”â€‹trâ€‹AÎ¸)â€–â‰¤ca\sup\_{\tau}\|\exp(\tau\Delta t\_{r}A\_{\theta})\|\leq c\_{a} with cac\_{a} depending on Îµ\varepsilon and maxâ„“â¡â€–AÎ¸â€‹(Tâ„“)â€–2\max\_{\ell}\|A\_{\theta}(T\_{\ell})\|\_{2}. The low-rank parameterization ([70](https://arxiv.org/html/2511.06451v1#Ax1.E70 "In Model parameterization. â€£ B.1 Proof of Theorem 2: approximation rate and conditioning â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) yields â€–âˆ‚Î¸AÎ¸â€‹(T)â€–â‰¤caâ€²â€‹Lg\|\partial\_{\theta}A\_{\theta}(T)\|\leq c\_{a}^{\prime}L\_{g} (linear in the gate Lipschitz constant). Consequently,

|  |  |  |
| --- | --- | --- |
|  | â€–âˆ‚Î¸ğ’¢Î¸â€‹(Tâ„“,Ts)â€–â‰¤ca2â€‹caâ€²â€‹Lgâ€‹âˆ‘r=s+1â„“Î”â€‹trâ€‹â€–âˆq=r+1â„“GÎ¸â€‹(Tq,Tqâˆ’1)â€–â€‹â€–âˆp=s+1râˆ’1GÎ¸â€‹(Tp,Tpâˆ’1)â€–.\|\partial\_{\theta}\mathcal{G}\_{\theta}(T\_{\ell},T\_{s})\|\ \leq\ c\_{a}^{2}\,c\_{a}^{\prime}L\_{g}\sum\_{r=s+1}^{\ell}\Delta t\_{r}\,\Big\|\prod\_{q=r+1}^{\ell}G\_{\theta}(T\_{q},T\_{q-1})\Big\|\,\Big\|\prod\_{p=s+1}^{r-1}G\_{\theta}(T\_{p},T\_{p-1})\Big\|. |  |

By submultiplicativity and again the Neumann-type bound ([68](https://arxiv.org/html/2511.06451v1#Ax1.E68 "In Model parameterization. â€£ B.1 Proof of Theorem 2: approximation rate and conditioning â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")), the double product is summably bounded uniformly in LL. Hence,

|  |  |  |
| --- | --- | --- |
|  | âˆ‘sâ‰¤â„“â€–âˆ‚Î¸ğ’¢Î¸â€‹(Tâ„“,Ts)â€‹BÎ¸â€‹(Ts)â€–â‰¤Câ€²â€²â€‹(Îµ)â€‹Lgâ€‹maxâ„“â¡â€–AÎ¸â€‹(Tâ„“)â€–2.\sum\_{s\leq\ell}\|\partial\_{\theta}\mathcal{G}\_{\theta}(T\_{\ell},T\_{s})\,B\_{\theta}(T\_{s})\|\ \leq\ C^{\prime\prime}(\varepsilon)\,L\_{g}\,\max\_{\ell}\|A\_{\theta}(T\_{\ell})\|\_{2}. |  |

Combining direct and state terms in ([74](https://arxiv.org/html/2511.06451v1#Ax1.E74 "In Part II: conditioning bound. â€£ B.1 Proof of Theorem 2: approximation rate and conditioning â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) and recalling that the rank-mm structure introduces at most an mm-fold linear scaling in the number of active gates, we obtain

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–ğ’¥Î¸â€–â‰¤C2â€‹(maxâ„“â¡â€–AÎ¸â€‹(Tâ„“)â€–2)â€‹Lgâ€‹m,\|\mathcal{J}\_{\theta}\|\ \leq\ C\_{2}\,\big(\max\_{\ell}\|A\_{\theta}(T\_{\ell})\|\_{2}\big)\,L\_{g}\,m, |  | (75) |

for a constant C2C\_{2} depending on Îµ\varepsilon, dictionary normalization, and decoder curvature bounds, but *independent of LL* thanks to the uniform Green kernel bound ([68](https://arxiv.org/html/2511.06451v1#Ax1.E68 "In Model parameterization. â€£ B.1 Proof of Theorem 2: approximation rate and conditioning â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")). This proves the conditioning proxy bound in ([28](https://arxiv.org/html/2511.06451v1#S4.E28 "In Theorem 2 (Approximation rate and conditioning). â€£ T1: Approximation Error and Conditioning â€£ 4 Theoretical Results â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")).

#### Conclusion.

The approximation rate follows from the best mm-term construction ([72](https://arxiv.org/html/2511.06451v1#Ax1.E72 "In Part I: approximation rate. â€£ B.1 Proof of Theorem 2: approximation rate and conditioning â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures"))â€“([73](https://arxiv.org/html/2511.06451v1#Ax1.E73 "In Part I: approximation rate. â€£ B.1 Proof of Theorem 2: approximation rate and conditioning â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) realized by the RN-Operator with rank-mm gates; the conditioning proxy is controlled by Q-Align spectral constraints and the Neumann-type summability of the discrete Green kernel, yielding ([75](https://arxiv.org/html/2511.06451v1#Ax1.E75 "In Part II: conditioning bound. â€£ B.1 Proof of Theorem 2: approximation rate and conditioning â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")). This completes the proof of TheoremÂ [2](https://arxiv.org/html/2511.06451v1#Thmtheorem2 "Theorem 2 (Approximation rate and conditioning). â€£ T1: Approximation Error and Conditioning â€£ 4 Theoretical Results â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures"). âˆ

### B.2 Proof of TheoremÂ [3](https://arxiv.org/html/2511.06451v1#Thmtheorem3 "Theorem 3 (Local identifiability and information bound). â€£ T2: Local Identifiability and CRLB-Type Lower Bounds â€£ 4 Theoretical Results â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures"): local identifiability and CRLB

#### Model and regularity.

Let (u,Y)(u,Y) denote a generic inputâ€“output pair, where uâˆˆğ’µu\in\mathcal{Z} is a feature field and Y={Câ€‹(Tâ„“,Kj)}â„“â‰¤L,jâ‰¤Jâ„“Y=\{C(T\_{\ell},K\_{j})\}\_{\ell\leq L,\,j\leq J\_{\ell}} collects option prices on the maturityâ€“strike grid. The RN-operator induces the mean surface

|  |  |  |
| --- | --- | --- |
|  | Î¼Î¸â€‹(u)=Î¦Î¸â€‹(âˆ‘sâ‰¤â„“ğ’¢Î¸â€‹(Tâ„“,Ts)â€‹BÎ¸â€‹(Ts)â€‹us)â„“,j,\mu\_{\theta}(u)\;=\;\Phi\_{\theta}\!\left(\sum\_{s\leq\ell}\mathcal{G}\_{\theta}(T\_{\ell},T\_{s})\,B\_{\theta}(T\_{s})\,u\_{s}\right)\_{\ell,j}, |  |

with Î¦Î¸\Phi\_{\theta} the convexâ€“monotone decoder (ICNN+Legendre projection). We assume: (A1) noise model Y=Î¼Î¸â€‹(u)+ÎµY=\mu\_{\theta}(u)+\varepsilon, where Îµ\varepsilon is mean-zero, sub-Gaussian with covariance operator Î£\Sigma independent of Î¸\theta; (A2) the input process has a nondegenerate covariance operator ğ–¢ğ—ˆğ—â€‹(u)\mathsf{Cov}(u) on ğ’µ\mathcal{Z}; (A3) Q-Align enforces 11-Lipschitz layers and Spec-Guard enforces the CFL constraint so that LemmaÂ [1](https://arxiv.org/html/2511.06451v1#Thmlemma1 "Lemma 1 (Green kernel bound). â€£ Spectral safety and discrete Green kernel. â€£ 3.1 Riskâ€“Neutral Operator Layer (RN-Operator) â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures") holds. These assumptions match the main text.

#### Step I: decoder-level identifiability on the grid.

Let CÎ¸â€‹(T,K)C\_{\theta}(T,K) be the decoded call price surface. Static no-arbitrage ensures convexity in KK and monotonicity in TT. The Breedenâ€“Litzenberger identity implies that, for each Tâ„“T\_{\ell},

|  |  |  |  |
| --- | --- | --- | --- |
|  | âˆ‚2CÎ¸âˆ‚K2â€‹(Tâ„“,K)=erâ€‹Tâ„“â€‹fÎ¸â„šâ€‹(Tâ„“,K),\frac{\partial^{2}C\_{\theta}}{\partial K^{2}}(T\_{\ell},K)\;=\;\mathrm{e}^{rT\_{\ell}}\,f\_{\theta}^{\mathbb{Q}}(T\_{\ell},K), |  | (76) |

where fÎ¸â„šâ€‹(Tâ„“,â‹…)f\_{\theta}^{\mathbb{Q}}(T\_{\ell},\cdot) is the risk-neutral density at maturity Tâ„“T\_{\ell}.
VIX2 replication consistency further imposes

|  |  |  |  |
| --- | --- | --- | --- |
|  | VIXÎ¸2â€‹(Tâ„“)=2â€‹erâ€‹Tâ„“Tâ„“â€‹âˆ«0âˆ1K2â€‹QÎ¸â€‹(K,Tâ„“)â€‹ğ‘‘K(discrete grid via quadrature as in the main text).\mathrm{VIX}\_{\theta}^{2}(T\_{\ell})\;=\;\frac{2\,\mathrm{e}^{rT\_{\ell}}}{T\_{\ell}}\int\_{0}^{\infty}\frac{1}{K^{2}}\,Q\_{\theta}(K,T\_{\ell})\,dK\quad\text{(discrete grid via quadrature as in the main text)}. |  | (77) |

On the grid, if two decoders Î¦Î¸1,Î¦Î¸2\Phi\_{\theta\_{1}},\Phi\_{\theta\_{2}} satisfy ([76](https://arxiv.org/html/2511.06451v1#Ax1.E76 "In Step I: decoder-level identifiability on the grid. â€£ B.2 Proof of Theorem 3: local identifiability and CRLB â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) with the same second derivative and also match ([77](https://arxiv.org/html/2511.06451v1#Ax1.E77 "In Step I: decoder-level identifiability on the grid. â€£ B.2 Proof of Theorem 3: local identifiability and CRLB â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")), then their implied densities and integrated variance coincide at all grid maturities. Since CC is recovered from its second derivative and boundary conditions (no-arbitrage asymptotics at Kâ†’0,âˆK\to 0,\infty), we conclude

|  |  |  |
| --- | --- | --- |
|  | Î¦Î¸1â€‹(z)=Î¦Î¸2â€‹(z)for all admissible inputsÂ â€‹z.\Phi\_{\theta\_{1}}(z)=\Phi\_{\theta\_{2}}(z)\quad\text{for all admissible inputs }z. |  |

Thus, *decoder-level identifiability holds* on the grid.

#### Step II: propagation through the scan to the operator level.

Suppose ğ’¢Î¸1\mathcal{G}\_{\theta\_{1}} and ğ’¢Î¸2\mathcal{G}\_{\theta\_{2}} yield the same decoded surface for almost every input uu:

|  |  |  |
| --- | --- | --- |
|  | Î¦Î¸1â€‹(âˆ‘sâ‰¤â„“ğ’¢Î¸1â€‹(Tâ„“,Ts)â€‹BÎ¸1â€‹(Ts)â€‹us)=Î¦Î¸2â€‹(âˆ‘sâ‰¤â„“ğ’¢Î¸2â€‹(Tâ„“,Ts)â€‹BÎ¸2â€‹(Ts)â€‹us),a.s. inÂ â€‹u.\Phi\_{\theta\_{1}}\!\Big(\sum\_{s\leq\ell}\mathcal{G}\_{\theta\_{1}}(T\_{\ell},T\_{s})B\_{\theta\_{1}}(T\_{s})u\_{s}\Big)\;=\;\Phi\_{\theta\_{2}}\!\Big(\sum\_{s\leq\ell}\mathcal{G}\_{\theta\_{2}}(T\_{\ell},T\_{s})B\_{\theta\_{2}}(T\_{s})u\_{s}\Big),\quad\text{a.s.\ in }u. |  |

Since Î¦Î¸\Phi\_{\theta} is 11-Lipschitz and strictly monotone along the decoderâ€™s active rays (by convexity and positive homogeneity of the ICNN regularized by Legendre projection), equality of outputs for almost every uu implies equality of *pre-decoder* representations for almost every uu:

|  |  |  |
| --- | --- | --- |
|  | âˆ‘sâ‰¤â„“ğ’¢Î¸1â€‹(Tâ„“,Ts)â€‹BÎ¸1â€‹(Ts)â€‹us=âˆ‘sâ‰¤â„“ğ’¢Î¸2â€‹(Tâ„“,Ts)â€‹BÎ¸2â€‹(Ts)â€‹usinÂ â€‹L2â€‹(ğ’µ).\sum\_{s\leq\ell}\mathcal{G}\_{\theta\_{1}}(T\_{\ell},T\_{s})B\_{\theta\_{1}}(T\_{s})u\_{s}\;=\;\sum\_{s\leq\ell}\mathcal{G}\_{\theta\_{2}}(T\_{\ell},T\_{s})B\_{\theta\_{2}}(T\_{s})u\_{s}\quad\text{in }L^{2}(\mathcal{Z}). |  |

Let Î´â€‹Î¸\delta\theta be a tangent perturbation at Î¸â‹†\theta^{\star}, and write the linearized identity

|  |  |  |  |
| --- | --- | --- | --- |
|  | âˆ‘sâ‰¤â„“(âˆ‚Î¸ğ’¢Î¸â‹†â€‹(Tâ„“,Ts)â€‹BÎ¸â‹†â€‹(Ts)+ğ’¢Î¸â‹†â€‹(Tâ„“,Ts)â€‹âˆ‚Î¸BÎ¸â‹†â€‹(Ts))â€‹us=â€„0inÂ â€‹L2â€‹(ğ’µ).\sum\_{s\leq\ell}\!\Big(\partial\_{\theta}\mathcal{G}\_{\theta^{\star}}(T\_{\ell},T\_{s})\,B\_{\theta^{\star}}(T\_{s})+\mathcal{G}\_{\theta^{\star}}(T\_{\ell},T\_{s})\,\partial\_{\theta}B\_{\theta^{\star}}(T\_{s})\Big)u\_{s}\;=\;0\quad\text{in }L^{2}(\mathcal{Z}). |  | (78) |

Taking the covariance in uu and using nondegeneracy of ğ–¢ğ—ˆğ—â€‹(u)\mathsf{Cov}(u) together with the uniform Green bound (LemmaÂ [1](https://arxiv.org/html/2511.06451v1#Thmlemma1 "Lemma 1 (Green kernel bound). â€£ Spectral safety and discrete Green kernel. â€£ 3.1 Riskâ€“Neutral Operator Layer (RN-Operator) â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")), we obtain that the linear operator

|  |  |  |
| --- | --- | --- |
|  | â„’Î¸â‹†â€‹[Î´â€‹Î¸]:=âˆ‘sâ‰¤â„“(âˆ‚Î¸ğ’¢Î¸â‹†â€‹(Tâ„“,Ts)â€‹BÎ¸â‹†â€‹(Ts)+ğ’¢Î¸â‹†â€‹(Tâ„“,Ts)â€‹âˆ‚Î¸BÎ¸â‹†â€‹(Ts))\mathcal{L}\_{\theta^{\star}}[\delta\theta]:=\sum\_{s\leq\ell}\!\Big(\partial\_{\theta}\mathcal{G}\_{\theta^{\star}}(T\_{\ell},T\_{s})\,B\_{\theta^{\star}}(T\_{s})+\mathcal{G}\_{\theta^{\star}}(T\_{\ell},T\_{s})\,\partial\_{\theta}B\_{\theta^{\star}}(T\_{s})\Big) |  |

vanishes if and only if Î´â€‹Î¸\delta\theta lies in the *symmetry tangent space* generated by atom permutations and reciprocal rescalings in the rank-mm factorization. Consequently, the differential Dâ€‹ğ’¢Î¸â‹†D\mathcal{G}\_{\theta^{\star}} is injective on the quotient by these symmetries, and by the inverse function theorem for Banach spaces, there exists a neighborhood ğ’°\mathcal{U} in which Î¸â†¦ğ’¢Î¸\theta\mapsto\mathcal{G}\_{\theta} is injective modulo symmetries.

#### Step III: Fisher information and CRLB.

Under (A1)â€“(A3), the log-likelihood for a single pair (u,Y)(u,Y) is

|  |  |  |
| --- | --- | --- |
|  | â„“â€‹(Î¸;u,Y)=âˆ’12â€‹âŸ¨Yâˆ’Î¼Î¸â€‹(u),Î£âˆ’1â€‹(Yâˆ’Î¼Î¸â€‹(u))âŸ©+const,\ell(\theta;u,Y)\;=\;-\tfrac{1}{2}\big\langle Y-\mu\_{\theta}(u),\,\Sigma^{-1}\,(Y-\mu\_{\theta}(u))\big\rangle+\mathrm{const}, |  |

with score SÎ¸â€‹(u,Y)=Dâ€‹Î¼Î¸â€‹(u)âŠ¤â€‹Î£âˆ’1â€‹(Yâˆ’Î¼Î¸â€‹(u))S\_{\theta}(u,Y)=D\mu\_{\theta}(u)^{\top}\Sigma^{-1}\big(Y-\mu\_{\theta}(u)\big), where Dâ€‹Î¼Î¸â€‹(u)D\mu\_{\theta}(u) is the Jacobian of the RN-operator output w.r.t. Î¸\theta. The Fisher information is

|  |  |  |
| --- | --- | --- |
|  | â„â€‹(Î¸):=ğ”¼â€‹[SÎ¸â€‹SÎ¸âŠ¤]=ğ”¼â€‹[Dâ€‹Î¼Î¸â€‹(u)âŠ¤â€‹Î£âˆ’1â€‹Dâ€‹Î¼Î¸â€‹(u)],\mathcal{I}(\theta):=\mathbb{E}\!\left[S\_{\theta}S\_{\theta}^{\top}\right]\;=\;\mathbb{E}\!\left[D\mu\_{\theta}(u)^{\top}\Sigma^{-1}D\mu\_{\theta}(u)\right], |  |

since ğ”¼â€‹[Yâˆ’Î¼Î¸â€‹(u)âˆ£u]=0\mathbb{E}[Y-\mu\_{\theta}(u)\mid u]=0. By Q-Align, Dâ€‹Î¼Î¸â€‹(u)D\mu\_{\theta}(u) is bounded and measurable; by StepÂ II, Dâ€‹Î¼Î¸â‹†D\mu\_{\theta^{\star}} has trivial kernel on the symmetry-quotient space, hence â„â€‹(Î¸â‹†)\mathcal{I}(\theta^{\star}) is positive definite on that quotient. The CramÃ©râ€“Rao inequality for unbiased estimators on smooth parametric families then yields

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[(Î¸^âˆ’Î¸â‹†)â€‹(Î¸^âˆ’Î¸â‹†)âŠ¤]âª°1nâ€‹â„â€‹(Î¸â‹†)âˆ’1,\mathbb{E}\!\left[(\widehat{\theta}-\theta^{\star})(\widehat{\theta}-\theta^{\star})^{\top}\right]\;\succeq\;\frac{1}{n}\,\mathcal{I}(\theta^{\star})^{-1}, |  |

and ([29](https://arxiv.org/html/2511.06451v1#S4.E29 "In Theorem 3 (Local identifiability and information bound). â€£ T2: Local Identifiability and CRLB-Type Lower Bounds â€£ 4 Theoretical Results â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) follows after taking the trace. This completes the proof. âˆ

### B.3 Proof of PropositionÂ [5](https://arxiv.org/html/2511.06451v1#Thmproposition5 "Proposition 5 (Representative bound with coverage and residuals). â€£ T2â€²: Representative-Element Error Under Coverage Deficits â€£ 4 Theoretical Results â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")

#### Set-up and notation.

Let ğ’¢Î¸\mathcal{G}\_{\theta} be the RN-operator, Î¦Î¸\Phi\_{\theta} the convexâ€“monotone decoder, and write the decoded surface CÎ¸=Î¦Î¸âˆ˜ğ’¢Î¸â€‹(â‹…)C\_{\theta}=\Phi\_{\theta}\circ\mathcal{G}\_{\theta}(\cdot) on the strikeâ€“maturity grid
ğ’¢={(Tâ„“,Kj)}â„“â‰¤L,jâ‰¤Jâ„“\mathcal{G}=\{(T\_{\ell},K\_{j})\}\_{\ell\leq L,\,j\leq J\_{\ell}}.
Let â„âŠ‚ğ’¢\mathcal{I}\subset\mathcal{G} denote the set of covered cells with reliable quotes; its relative cardinality is cov:=|â„|/|ğ’¢|âˆˆ[0,1]\mathrm{cov}:=|\mathcal{I}|/|\mathcal{G}|\in[0,1]. On the complement ğ’¢âˆ–â„\mathcal{G}\setminus\mathcal{I}, the price surface is filled by a linear, static no-arbitrageâ€“preserving interpolant ğ–¤ğ—‘ğ—\mathsf{Ext} (convex in KK, monotone in TT). We assume an interpolation accuracy bound

|  |  |  |  |
| --- | --- | --- | --- |
|  | âˆ¥ğ–¤ğ—‘ğ—[Câ‹†|â„]âˆ’Câ‹†âˆ¥â„“2â€‹(ğ’¢)â‰¤Îµ,\big\|\mathsf{Ext}[C^{\star}|\_{\mathcal{I}}]-C^{\star}\big\|\_{\ell^{2}(\mathcal{G})}\ \leq\ \varepsilon, |  | (79) |

where Câ‹†C^{\star} is the ground-truth surface induced by Î»â‹†\lambda^{\star}.

The training objective is a penalized, discretized risk under the risk-neutral measure with a martingale penalty of weight Î³>0\gamma>0, plus the indicator of the no-arbitrage cone ğ’¦\mathcal{K}:

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ’¥Î³â€‹(Î»):=1|â„|â€‹âˆ‘(Tâ„“,Kj)âˆˆâ„(CÎ»â€‹(Tâ„“,Kj)âˆ’Yâ„“â€‹j)2+Î³â€‹ğ–¬ğ–ºğ—‹ğ—â€‹(Î»)+Î¹ğ’¦â€‹(CÎ»).\mathcal{J}\_{\gamma}(\lambda)\ :=\ \frac{1}{|\mathcal{I}|}\!\sum\_{(T\_{\ell},K\_{j})\in\mathcal{I}}\big(C\_{\lambda}(T\_{\ell},K\_{j})-Y\_{\ell j}\big)^{2}\ +\ \gamma\,\mathsf{Mart}(\lambda)\ +\ \iota\_{\mathcal{K}}\big(C\_{\lambda}\big). |  | (80) |

Here Yâ„“â€‹jY\_{\ell j} are observed mid quotes; ğ–¬ğ–ºğ—‹ğ—â€‹(Î»)\mathsf{Mart}(\lambda) is a nonnegative convex proxy for the martingale defect (e.g., squared drift under â„šÎ»\mathbb{Q}\_{\lambda}); Î¹ğ’¦\iota\_{\mathcal{K}} is 0 if the static no-arbitrage conditions hold on the grid and +âˆ+\infty otherwise. Let Î»^Î³\widehat{\lambda}\_{\gamma} be a first-order stationary point of ([80](https://arxiv.org/html/2511.06451v1#Ax1.E80 "In Set-up and notation. â€£ B.3 Proof of Proposition 5 â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) on the *covered* grid, and let Î»Îµ\lambda\_{\varepsilon} be the corresponding representative element when the uncovered cells are filled by ğ–¤ğ—‘ğ—\mathsf{Ext}.

We further use: (i) the *global Lipschitz property* of the RN-operator map from LemmaÂ [1](https://arxiv.org/html/2511.06451v1#Thmlemma1 "Lemma 1 (Green kernel bound). â€£ Spectral safety and discrete Green kernel. â€£ 3.1 Riskâ€“Neutral Operator Layer (RN-Operator) â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures") and PropositionÂ [1](https://arxiv.org/html/2511.06451v1#Thmproposition1 "Proposition 1 (RN-operator stability under Q-Align). â€£ RN-operator stability under Q-Align. â€£ 3.2 Q-Align: Lipschitz Projection and Spectral Guard â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures") (Q-Align and Spec-Guard), summarized as

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–CÎ»1âˆ’CÎ»2â€–â„“2â€‹(ğ’¢)â‰¤LRNâ€‹â€–Î»1âˆ’Î»2â€–L2â€‹(ğ’µ),LRN<âˆ,\|C\_{\lambda\_{1}}-C\_{\lambda\_{2}}\|\_{\ell^{2}(\mathcal{G})}\ \leq\ L\_{\mathrm{RN}}\ \|\lambda\_{1}-\lambda\_{2}\|\_{L^{2}(\mathcal{Z})},\qquad L\_{\mathrm{RN}}<\infty, |  | (81) |

(ii) a *Hoffman-type bound* for the composite convex program (data-fidelity ++ linear constraints defining ğ’¦\mathcal{K} ++ convex penalty), which states that there exists ÎºHof>0\kappa\_{\mathrm{Hof}}>0 such that the distance to the solution set ğ’®Î³\mathcal{S}\_{\gamma} satisfies

|  |  |  |  |
| --- | --- | --- | --- |
|  | distâ€‹(Î»,ğ’®Î³)â‰¤ÎºHofâ€‹â€–KKTâ€‹(Î»)â€–,\mathrm{dist}\big(\lambda,\mathcal{S}\_{\gamma}\big)\ \leq\ \kappa\_{\mathrm{Hof}}\ \|\mathrm{KKT}(\lambda)\|, |  | (82) |

where KKTâ€‹(Î»)\mathrm{KKT}(\lambda) is a residual vector collecting the primal feasibility (no-arbitrage), the dual feasibility (subgradient of ğ–¬ğ–ºğ—‹ğ—\mathsf{Mart}), and stationarity violations (see, e.g., variational inequalities with polyhedral sets).

#### Step 1: interpolation (coverage) term.

Split the grid norm as â€–Câ€–â„“2â€‹(ğ’¢)2=â€–Câ€–â„“2â€‹(â„)2+â€–Câ€–â„“2â€‹(ğ’¢âˆ–â„)2\|C\|\_{\ell^{2}(\mathcal{G})}^{2}=\|C\|\_{\ell^{2}(\mathcal{I})}^{2}+\|C\|\_{\ell^{2}(\mathcal{G}\setminus\mathcal{I})}^{2}.
On ğ’¢âˆ–â„\mathcal{G}\setminus\mathcal{I}, prices are provided by ğ–¤ğ—‘ğ—\mathsf{Ext} built from â„\mathcal{I}. Let Î â„\Pi\_{\mathcal{I}} be the sampling operator on â„\mathcal{I} and Î â„âŸ‚\Pi\_{\mathcal{I}}^{\perp} on the complement. The extension operator is linear and stable on the no-arbitrage cone, i.e.,

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–Î â„âŸ‚â€‹ğ–¤ğ—‘ğ—â€‹[v]â€–â„“2â€‹(ğ’¢âˆ–â„)â‰¤Î±â€‹(cov)â€‹â€–vâ€–â„“2â€‹(â„),Î±â€‹(cov)â‰¤Cextâ€‹(1âˆ’cov)âˆ’1,\big\|\Pi\_{\mathcal{I}}^{\perp}\,\mathsf{Ext}[v]\big\|\_{\ell^{2}(\mathcal{G}\setminus\mathcal{I})}\ \leq\ \alpha(\mathrm{cov})\ \|v\|\_{\ell^{2}(\mathcal{I})},\qquad\alpha(\mathrm{cov})\ \leq\ C\_{\mathrm{ext}}\,(1-\mathrm{cov})^{-1}, |  | (83) |

for some absolute CextC\_{\mathrm{ext}} depending only on the grid aspect ratio. The scaling (1âˆ’cov)âˆ’1(1-\mathrm{cov})^{-1} captures the worst-case amplification when extrapolating from a vanishing covered set. Applying ([83](https://arxiv.org/html/2511.06451v1#Ax1.E83 "In Step 1: interpolation (coverage) term. â€£ B.3 Proof of Proposition 5 â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) with v=Câ‹†|â„âˆ’Î â„â€‹CÎ»Îµv=C^{\star}|\_{\mathcal{I}}-\Pi\_{\mathcal{I}}C\_{\lambda\_{\varepsilon}} and adding the intrinsic interpolation error ([79](https://arxiv.org/html/2511.06451v1#Ax1.E79 "In Set-up and notation. â€£ B.3 Proof of Proposition 5 â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) yields

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–CÎ»Îµâˆ’Câ‹†â€–â„“2â€‹(ğ’¢)â‰¤Cextâ€‹(1âˆ’cov)âˆ’1â€‹â€–Î â„â€‹(CÎ»Îµâˆ’Câ‹†)â€–â„“2â€‹(â„)+Îµ.\|C\_{\lambda\_{\varepsilon}}-C^{\star}\|\_{\ell^{2}(\mathcal{G})}\ \leq\ C\_{\mathrm{ext}}\,(1-\mathrm{cov})^{-1}\,\big\|\Pi\_{\mathcal{I}}(C\_{\lambda\_{\varepsilon}}-C^{\star})\big\|\_{\ell^{2}(\mathcal{I})}\ +\ \varepsilon. |  | (84) |

As the empirical fit on â„\mathcal{I} is optimized in ([80](https://arxiv.org/html/2511.06451v1#Ax1.E80 "In Set-up and notation. â€£ B.3 Proof of Proposition 5 â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")), the term â€–Î â„â€‹(CÎ»Îµâˆ’Câ‹†)â€–\|\Pi\_{\mathcal{I}}(C\_{\lambda\_{\varepsilon}}-C^{\star})\| is in turn controlled by the optimization residual (treated in StepÂ 3). For the present step, we retain the *coverage* contribution Cextâ€‹(1âˆ’cov)âˆ’1â€‹ÎµC\_{\mathrm{ext}}(1-\mathrm{cov})^{-1}\varepsilon to the full-grid error.

#### Step 2: martingale penalty (finite Î³\gamma).

Let Î»âˆ\lambda\_{\infty} denote an exact solution of the *constrained* problem (martingale enforced as a hard constraint and static no-arbitrage satisfied). By convexity and standard exact-penalty reasoning, first-order optimality implies

|  |  |  |  |
| --- | --- | --- | --- |
|  | distâ€‹(Î»Îµ,{martingale-feasible})â‰¤1Î³â€‹Cpen,\mathrm{dist}\big(\lambda\_{\varepsilon},\ \{\text{martingale-feasible}\}\big)\ \leq\ \tfrac{1}{\gamma}\,C\_{\mathrm{pen}}, |  | (85) |

for some modulus CpenC\_{\mathrm{pen}} depending on the subgradient bounds of ğ–¬ğ–ºğ—‹ğ—\mathsf{Mart} at feasible points (Q-Align and Spec-Guard ensure bounded Jacobians and thus bounded subgradients). Combining ([85](https://arxiv.org/html/2511.06451v1#Ax1.E85 "In Step 2: martingale penalty (finite ğ›¾). â€£ B.3 Proof of Proposition 5 â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) with the Lipschitz continuity ([81](https://arxiv.org/html/2511.06451v1#Ax1.E81 "In Set-up and notation. â€£ B.3 Proof of Proposition 5 â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) transfers feasibility deviation into price-surface deviation with multiplicative constant LRNL\_{\mathrm{RN}}, and by metric regularity of the feasible set, it transfers to a distance in Î»\lambda with a constant absorbed in C3C\_{3}.

#### Step 3: dual residual (stopping criterion).

Let dual\mathrm{dual} denote the norm of the KKT residual at termination. By the Hoffman bound ([82](https://arxiv.org/html/2511.06451v1#Ax1.E82 "In Set-up and notation. â€£ B.3 Proof of Proposition 5 â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")),

|  |  |  |  |
| --- | --- | --- | --- |
|  | distâ€‹(Î»Îµ,ğ’®Î³)â‰¤ÎºHofâ€‹dual.\mathrm{dist}\big(\lambda\_{\varepsilon},\ \mathcal{S}\_{\gamma}\big)\ \leq\ \kappa\_{\mathrm{Hof}}\ \mathrm{dual}. |  | (86) |

Since Î»â‹†\lambda^{\star} (or Î»âˆ\lambda\_{\infty}) lies within a bounded distance of ğ’®Î³\mathcal{S}\_{\gamma} uniformly in the data draw (population minimizer versus empirical minimizer), a triangle inequality yields a ÎºHofâ€‹dual\kappa\_{\mathrm{Hof}}\mathrm{dual} contribution to â€–Î»Îµâˆ’Î»â‹†â€–L2â€‹(ğ’µ)\|\lambda\_{\varepsilon}-\lambda^{\star}\|\_{L^{2}(\mathcal{Z})}.

#### Step 4: aggregation via RN-operator stability.

From ([81](https://arxiv.org/html/2511.06451v1#Ax1.E81 "In Set-up and notation. â€£ B.3 Proof of Proposition 5 â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")), converting surface errors back to L2â€‹(ğ’µ)L^{2}(\mathcal{Z}) distances in Î»\lambda multiplies by at most LRNL\_{\mathrm{RN}}. Gathering ([84](https://arxiv.org/html/2511.06451v1#Ax1.E84 "In Step 1: interpolation (coverage) term. â€£ B.3 Proof of Proposition 5 â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")), ([85](https://arxiv.org/html/2511.06451v1#Ax1.E85 "In Step 2: martingale penalty (finite ğ›¾). â€£ B.3 Proof of Proposition 5 â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")), and ([86](https://arxiv.org/html/2511.06451v1#Ax1.E86 "In Step 3: dual residual (stopping criterion). â€£ B.3 Proof of Proposition 5 â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")), and absorbing universal constants (including LRNL\_{\mathrm{RN}}, CextC\_{\mathrm{ext}}, CpenC\_{\mathrm{pen}}, ÎºHof\kappa\_{\mathrm{Hof}}) into C3C\_{3}, we obtain

|  |  |  |
| --- | --- | --- |
|  | â€–Î»Îµâˆ’Î»â‹†â€–L2â€‹(ğ’µ)â‰¤C3â€‹((1âˆ’cov)âˆ’1â€‹Îµ+Î³âˆ’1+dual),\|\lambda\_{\varepsilon}-\lambda^{\star}\|\_{L^{2}(\mathcal{Z})}\ \leq\ C\_{3}\Big(\,(1-\mathrm{cov})^{-1}\varepsilon\;+\;\gamma^{-1}\;+\;\mathrm{dual}\,\Big), |  |

which is precisely ([30](https://arxiv.org/html/2511.06451v1#S4.E30 "In Proposition 5 (Representative bound with coverage and residuals). â€£ T2â€²: Representative-Element Error Under Coverage Deficits â€£ 4 Theoretical Results â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")). âˆ

#### Remarks.

(i) The (1âˆ’cov)âˆ’1(1-\mathrm{cov})^{-1} factor is tight up to constants for adversarial mask geometries (thin strips in TT or KK), and improves to Oâ€‹(1)O(1) when the mask satisfies an interior-cone condition (uniform spreading).
(ii) The constant C3C\_{3} does not depend on LL beyond the linear scan factor already controlled by Spec-Guard; it depends on the no-arbitrage cone geometry only through curvature bounds of the ICNN decoder.
(iii) Empirically, the regression of the gap proxy onto the representative error in our runs (see Sec.Â 6.6) exhibits a slope consistent with the ÎºHof\kappa\_{\mathrm{Hof}} scale predicted here.

### B.4 Proof of LemmaÂ [2](https://arxiv.org/html/2511.06451v1#Thmlemma2 "Lemma 2 (Rademacher complexity with Lipschitz and projection). â€£ T4â€“T5: Capacity via Rademacher and a Sampleâ€“Seminorm Bridge â€£ 4 Theoretical Results â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures"): Rademacher complexity with Lipschitz and projection

#### Set-up.

Let ğ’µ\mathcal{Z} denote the compact feature domain and let fâˆˆâ„±f\in\mathcal{F} map zâˆˆğ’µz\in\mathcal{Z} to a scalar price functional (coordinate-wise treatment extends to vector outputs by a standard â„“2\ell\_{2} aggregation and contraction). Q-Align and Spec-Guard imply a global Lipschitz constant Î›\Lambda for the RN-operator (cf. PropositionÂ [1](https://arxiv.org/html/2511.06451v1#Thmproposition1 "Proposition 1 (RN-operator stability under Q-Align). â€£ RN-operator stability under Q-Align. â€£ 3.2 Q-Align: Lipschitz Projection and Spectral Guard â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")):

|  |  |  |  |
| --- | --- | --- | --- |
|  | |fâ€‹(z)âˆ’fâ€‹(zâ€²)|â‰¤Î›â€‹â€–zâˆ’zâ€²â€–2,âˆ€z,zâ€²âˆˆğ’µ.|f(z)-f(z^{\prime})|\leq\Lambda\,\|z-z^{\prime}\|\_{2},\qquad\forall z,z^{\prime}\in\mathcal{Z}. |  | (87) |

Let PeffP\_{\mathrm{eff}} be the orthogonal projector onto the top-energy subspace of rank dimeff\mathrm{dim}\_{\mathrm{eff}} determined by the Gram operator of the discrete Green kernel at the sample scale (energy truncation definition of d^\hat{d}). For each fâˆˆâ„±f\in\mathcal{F} define f~:=fâˆ˜Peff\tilde{f}:=f\circ P\_{\mathrm{eff}}; by ([87](https://arxiv.org/html/2511.06451v1#Ax1.E87 "In Set-up. â€£ B.4 Proof of Lemma 2: Rademacher complexity with Lipschitz and projection â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")), f~\tilde{f} is also Î›\Lambda-Lipschitz on Peffâ€‹ğ’µâŠ‚â„dimeffP\_{\mathrm{eff}}\mathcal{Z}\subset\mathbb{R}^{\mathrm{dim}\_{\mathrm{eff}}}.

#### Symmetrization and Dudley integral.

For i.i.d. samples (zi)i=1n(z\_{i})\_{i=1}^{n} from the data distribution and Rademacher variables (Ïƒi)(\sigma\_{i}),

|  |  |  |
| --- | --- | --- |
|  | â„œnâ€‹(â„±)=ğ”¼â€‹supfâˆˆâ„±1nâ€‹âˆ‘i=1nÏƒiâ€‹fâ€‹(zi)â‰¤ğ”¼â€‹supfâˆˆâ„±1nâ€‹âˆ‘i=1nÏƒiâ€‹f~â€‹(zi)+â„°tail.\mathfrak{R}\_{n}(\mathcal{F})\;=\;\mathbb{E}\,\sup\_{f\in\mathcal{F}}\frac{1}{n}\sum\_{i=1}^{n}\sigma\_{i}f(z\_{i})\ \leq\ \mathbb{E}\,\sup\_{f\in\mathcal{F}}\frac{1}{n}\sum\_{i=1}^{n}\sigma\_{i}\tilde{f}(z\_{i})\;+\;\mathcal{E}\_{\mathrm{tail}}. |  |

The tail term accounts for the projection error (Idâˆ’Peff)(\mathrm{Id}-P\_{\mathrm{eff}}) and is zero if ff depends only on the effective coordinates; otherwise it is absorbed into the constant C6C\_{6} since PeffP\_{\mathrm{eff}} is chosen at the sample scale (energy truncation).

By Dudley chaining,

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â€‹supfâˆˆâ„±1nâ€‹âˆ‘i=1nÏƒiâ€‹f~â€‹(zi)â‰¤12nâ€‹âˆ«0diamâ€‹(ğ’µ)logğ’©(â„±,âˆ¥â‹…âˆ¥L2â€‹(â„™n),Ïµ)â€‹ğ‘‘Ïµ,\mathbb{E}\,\sup\_{f\in\mathcal{F}}\frac{1}{n}\sum\_{i=1}^{n}\sigma\_{i}\tilde{f}(z\_{i})\ \leq\ \frac{12}{\sqrt{n}}\int\_{0}^{\mathrm{diam}(\mathcal{Z})}\sqrt{\log\mathcal{N}\!\left(\mathcal{F},\,\|\cdot\|\_{L\_{2}(\mathbb{P}\_{n})},\,\epsilon\right)}\,d\epsilon, |  | (88) |

where ğ’©â€‹(â‹…,Ïµ)\mathcal{N}(\cdot,\epsilon) is the empirical L2L\_{2} covering number. Since every f~\tilde{f} is Î›\Lambda-Lipschitz over a dimeff\mathrm{dim}\_{\mathrm{eff}}-dimensional domain with radius normalized to one (rescale zz if needed), the covering number satisfies

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ’©(â„±,âˆ¥â‹…âˆ¥L2â€‹(â„™n),Ïµ)â‰¤(Câ€‹Î›Ïµ)dimeff,Ïµâˆˆ(0,Î›],\mathcal{N}\!\left(\mathcal{F},\,\|\cdot\|\_{L\_{2}(\mathbb{P}\_{n})},\,\epsilon\right)\ \leq\ \left(\frac{C\,\Lambda}{\epsilon}\right)^{\mathrm{dim}\_{\mathrm{eff}}},\qquad\epsilon\in(0,\Lambda], |  | (89) |

for an absolute constant CC (covering of a Lipschitz ball in â„dimeff\mathbb{R}^{\mathrm{dim}\_{\mathrm{eff}}}). Plugging ([89](https://arxiv.org/html/2511.06451v1#Ax1.E89 "In Symmetrization and Dudley integral. â€£ B.4 Proof of Lemma 2: Rademacher complexity with Lipschitz and projection â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) into ([88](https://arxiv.org/html/2511.06451v1#Ax1.E88 "In Symmetrization and Dudley integral. â€£ B.4 Proof of Lemma 2: Rademacher complexity with Lipschitz and projection â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) gives

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹supfâˆˆâ„±1nâ€‹âˆ‘i=1nÏƒiâ€‹f~â€‹(zi)â‰¤12nâ€‹âˆ«0Î›dimeffâ€‹logâ¡(Câ€‹Î›/Ïµ)â€‹ğ‘‘Ïµâ‰¤Câ€²â€‹Î›â€‹dimeffn,\mathbb{E}\,\sup\_{f\in\mathcal{F}}\frac{1}{n}\sum\_{i=1}^{n}\sigma\_{i}\tilde{f}(z\_{i})\ \leq\ \frac{12}{\sqrt{n}}\int\_{0}^{\Lambda}\sqrt{\mathrm{dim}\_{\mathrm{eff}}\log(C\Lambda/\epsilon)}\,d\epsilon\ \leq\ C^{\prime}\,\Lambda\,\sqrt{\frac{\mathrm{dim}\_{\mathrm{eff}}}{n}}, |  |

for another absolute constant Câ€²C^{\prime}. Absorbing â„°tail\mathcal{E}\_{\mathrm{tail}} and the radius rescaling into C6C\_{6} yields

|  |  |  |
| --- | --- | --- |
|  | â„œnâ€‹(â„±)â‰¤C6â€‹Î›â€‹dimeffn.\mathfrak{R}\_{n}(\mathcal{F})\;\leq\;C\_{6}\,\Lambda\,\sqrt{\frac{\mathrm{dim}\_{\mathrm{eff}}}{n}}. |  |

This proves LemmaÂ [2](https://arxiv.org/html/2511.06451v1#Thmlemma2 "Lemma 2 (Rademacher complexity with Lipschitz and projection). â€£ T4â€“T5: Capacity via Rademacher and a Sampleâ€“Seminorm Bridge â€£ 4 Theoretical Results â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures"). âˆ

### B.5 Proof of LemmaÂ [3](https://arxiv.org/html/2511.06451v1#Thmlemma3 "Lemma 3 (Bridge from sample to seminorm). â€£ Proof sketch. â€£ T4â€“T5: Capacity via Rademacher and a Sampleâ€“Seminorm Bridge â€£ 4 Theoretical Results â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures"): Bridge from sample to seminorm

#### Kernel seminorm and operator bound.

Let KK be the discrete Gram operator of the Green kernel on the strikeâ€“maturity grid ğ’¢\mathcal{G}; define

|  |  |  |
| --- | --- | --- |
|  | â€–fâ€–â„‹2:=âŸ¨f,Kâ€‹fâŸ©â„“2â€‹(ğ’¢).\|f\|\_{\mathcal{H}}^{2}:=\langle f,Kf\rangle\_{\ell^{2}(\mathcal{G})}. |  |

By LemmaÂ [1](https://arxiv.org/html/2511.06451v1#Thmlemma1 "Lemma 1 (Green kernel bound). â€£ Spectral safety and discrete Green kernel. â€£ 3.1 Riskâ€“Neutral Operator Layer (RN-Operator) â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures") (Green kernel summability under CFL) and PropositionÂ [1](https://arxiv.org/html/2511.06451v1#Thmproposition1 "Proposition 1 (RN-operator stability under Q-Align). â€£ RN-operator stability under Q-Align. â€£ 3.2 Q-Align: Lipschitz Projection and Spectral Guard â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures") (global Lipschitz stability of the RN-operator), the spectral norm of KK is finite:

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–Kâ€–opâ‰¤CKâ€‹(Îµ),â‡’â€–fâ€–â„‹â‰¤CKâ€‹(Îµ)â€‹â€–fâ€–â„“2â€‹(ğ’¢).\|K\|\_{\mathrm{op}}\;\leq\;C\_{K}(\varepsilon),\qquad\Rightarrow\qquad\|f\|\_{\mathcal{H}}\;\leq\;\sqrt{C\_{K}(\varepsilon)}\,\|f\|\_{\ell^{2}(\mathcal{G})}. |  | (90) |

#### Decomposition by coverage and stable extension.

Let â„âŠ‚ğ’¢\mathcal{I}\subset\mathcal{G} denote the covered cells and Î â„\Pi\_{\mathcal{I}} the restriction operator. The interpolation scheme is linear, preserves static no-arbitrage, and satisfies the stability estimate

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–Î â„âŸ‚â€‹ğ–¤ğ—‘ğ—â€‹[v]â€–â„“2â€‹(ğ’¢âˆ–â„)â‰¤Î±â€‹(cov)â€‹â€–vâ€–â„“2â€‹(â„),Î±â€‹(cov)â‰¤Cextâ€‹(1âˆ’cov)âˆ’1.\big\|\Pi\_{\mathcal{I}}^{\perp}\,\mathsf{Ext}[v]\big\|\_{\ell^{2}(\mathcal{G}\setminus\mathcal{I})}\ \leq\ \alpha(\mathrm{cov})\ \|v\|\_{\ell^{2}(\mathcal{I})},\qquad\alpha(\mathrm{cov})\leq C\_{\mathrm{ext}}\,(1-\mathrm{cov})^{-1}. |  | (91) |

Moreover, for the ground-truth fâ‹†f^{\star} we have an interpolation accuracy bound on the complement:

|  |  |  |  |
| --- | --- | --- | --- |
|  | âˆ¥Î â„âŸ‚(ğ–¤ğ—‘ğ—[fâ‹†|â„]âˆ’fâ‹†)âˆ¥â„“2â€‹(ğ’¢âˆ–â„)â‰¤Îµ.\big\|\Pi\_{\mathcal{I}}^{\perp}\big(\mathsf{Ext}[f^{\star}|\_{\mathcal{I}}]-f^{\star}\big)\big\|\_{\ell^{2}(\mathcal{G}\setminus\mathcal{I})}\ \leq\ \varepsilon. |  | (92) |

For any ff in the model class, write f=Î â„â€‹f+Î â„âŸ‚â€‹ff=\Pi\_{\mathcal{I}}f+\Pi\_{\mathcal{I}}^{\perp}f and bound

|  |  |  |
| --- | --- | --- |
|  | â€–fâ€–â„“2â€‹(ğ’¢)â‰¤â€–Î â„â€‹fâ€–â„“2â€‹(â„)+â€–Î â„âŸ‚â€‹fâ€–â„“2â€‹(ğ’¢âˆ–â„).\|f\|\_{\ell^{2}(\mathcal{G})}\ \leq\ \|\Pi\_{\mathcal{I}}f\|\_{\ell^{2}(\mathcal{I})}\ +\ \|\Pi\_{\mathcal{I}}^{\perp}f\|\_{\ell^{2}(\mathcal{G}\setminus\mathcal{I})}. |  |

Replace the complement by the extension from â„\mathcal{I} and add the intrinsic error ([92](https://arxiv.org/html/2511.06451v1#Ax1.E92 "In Decomposition by coverage and stable extension. â€£ B.5 Proof of Lemma 3: Bridge from sample to seminorm â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")):

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–Î â„âŸ‚â€‹fâ€–â„“2â€‹(ğ’¢âˆ–â„)â‰¤Î±â€‹(cov)â€‹â€–Î â„â€‹fâ€–â„“2â€‹(â„)+Îµ.\|\Pi\_{\mathcal{I}}^{\perp}f\|\_{\ell^{2}(\mathcal{G}\setminus\mathcal{I})}\ \leq\ \alpha(\mathrm{cov})\,\|\Pi\_{\mathcal{I}}f\|\_{\ell^{2}(\mathcal{I})}\ +\ \varepsilon. |  | (93) |

Combining with ([90](https://arxiv.org/html/2511.06451v1#Ax1.E90 "In Kernel seminorm and operator bound. â€£ B.5 Proof of Lemma 3: Bridge from sample to seminorm â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) and defining â€–fâ€–n:=â€–Î â„â€‹fâ€–â„“2â€‹(â„)\|f\|\_{n}:=\|\Pi\_{\mathcal{I}}f\|\_{\ell^{2}(\mathcal{I})} (empirical norm), we obtain

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–fâ€–â„‹â‰¤CKâ€‹(Îµ)â€‹(1+Î±â€‹(cov))â€‹â€–fâ€–n+CKâ€‹(Îµ)â€‹Îµ.\|f\|\_{\mathcal{H}}\ \leq\ \sqrt{C\_{K}(\varepsilon)}\,\Big(1+\alpha(\mathrm{cov})\Big)\,\|f\|\_{n}\ +\ \sqrt{C\_{K}(\varepsilon)}\,\varepsilon. |  | (94) |

#### From deterministic to high-probability uniform control.

Let â„±\mathcal{F} be the RN-operator class restricted to the feasible cone (static no-arbitrage). Consider the random design induced by the covered set and define the empirical process

|  |  |  |
| --- | --- | --- |
|  | Î”â€‹(f):=â€–fâ€–â„“2â€‹(ğ’¢)âˆ’(â€–fâ€–n+â€–Î â„âŸ‚â€‹ğ–¤ğ—‘ğ—â€‹[f]â€–â„“2â€‹(ğ’¢âˆ–â„)).\Delta(f):=\|f\|\_{\ell^{2}(\mathcal{G})}-\Big(\|f\|\_{n}+\|\Pi\_{\mathcal{I}}^{\perp}\mathsf{Ext}[f]\|\_{\ell^{2}(\mathcal{G}\setminus\mathcal{I})}\Big). |  |

By symmetrization and LemmaÂ [2](https://arxiv.org/html/2511.06451v1#Thmlemma2 "Lemma 2 (Rademacher complexity with Lipschitz and projection). â€£ T4â€“T5: Capacity via Rademacher and a Sampleâ€“Seminorm Bridge â€£ 4 Theoretical Results â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures"), with probability at least 1âˆ’2â€‹expâ¡(âˆ’câ€‹n)1-2\exp(-cn),

|  |  |  |  |
| --- | --- | --- | --- |
|  | supfâˆˆâ„±|Î”â€‹(f)|â‰¤Câ€‹Î›â€‹dimeffn,\sup\_{f\in\mathcal{F}}|\Delta(f)|\ \leq\ C\,\Lambda\,\sqrt{\frac{\mathrm{dim}\_{\mathrm{eff}}}{n}}, |  | (95) |

for an absolute constant CC. Inequality ([95](https://arxiv.org/html/2511.06451v1#Ax1.E95 "In From deterministic to high-probability uniform control. â€£ B.5 Proof of Lemma 3: Bridge from sample to seminorm â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) corrects ([94](https://arxiv.org/html/2511.06451v1#Ax1.E94 "In Decomposition by coverage and stable extension. â€£ B.5 Proof of Lemma 3: Bridge from sample to seminorm â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) uniformly over fâˆˆâ„±f\in\mathcal{F} by an additive term proportional to the class complexity. Absorb this high-probability deviation into the constants (recall nn at the figure scale is large and dimeff\mathrm{dim}\_{\mathrm{eff}} is fixed at that scale), and combine ([94](https://arxiv.org/html/2511.06451v1#Ax1.E94 "In Decomposition by coverage and stable extension. â€£ B.5 Proof of Lemma 3: Bridge from sample to seminorm â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) with ([91](https://arxiv.org/html/2511.06451v1#Ax1.E91 "In Decomposition by coverage and stable extension. â€£ B.5 Proof of Lemma 3: Bridge from sample to seminorm â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) to conclude

|  |  |  |
| --- | --- | --- |
|  | â€–fâ€–â„‹â‰¤C7â€‹â€–fâ€–n+C8â€‹(1âˆ’cov)âˆ’1â€‹Îµ,uniformly overÂ â€‹fâˆˆâ„±,\|f\|\_{\mathcal{H}}\ \leq\ C\_{7}\,\|f\|\_{n}\ +\ C\_{8}\,(1-\mathrm{cov})^{-1}\,\varepsilon,\quad\text{uniformly over }f\in\mathcal{F}, |  |

with probability at least 1âˆ’2â€‹expâ¡(âˆ’câ€‹n)1-2\exp(-cn), proving LemmaÂ [3](https://arxiv.org/html/2511.06451v1#Thmlemma3 "Lemma 3 (Bridge from sample to seminorm). â€£ Proof sketch. â€£ T4â€“T5: Capacity via Rademacher and a Sampleâ€“Seminorm Bridge â€£ 4 Theoretical Results â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures"). âˆ

#### Remarks.

(i) If the coverage mask satisfies an interior-cone condition (e.g., uniform thinning in TT and KK), the amplification factor improves from (1âˆ’cov)âˆ’1(1-\mathrm{cov})^{-1} to an Oâ€‹(1)O(1) constant; the statement remains valid with a smaller C8C\_{8}.
(ii) The constants inherit no exponential dependence on LL thanks to the spectral control of the scan (LemmaÂ [1](https://arxiv.org/html/2511.06451v1#Thmlemma1 "Lemma 1 (Green kernel bound). â€£ Spectral safety and discrete Green kernel. â€£ 3.1 Riskâ€“Neutral Operator Layer (RN-Operator) â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) and the per-layer Lipschitz capping by Q-Align.
(iii) A tighter empirical Bernstein correction can replace ([95](https://arxiv.org/html/2511.06451v1#Ax1.E95 "In From deterministic to high-probability uniform control. â€£ B.5 Proof of Lemma 3: Bridge from sample to seminorm â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) when the residual variance is small; we keep the simpler form for clarity.

### B.6 Proof of PropositionÂ [6](https://arxiv.org/html/2511.06451v1#Thmproposition6 "Proposition 6 (Feasibility and error propagation). â€£ T6: Feasibility and Two-Time-Scale Averaging under Spectral Guard â€£ 4 Theoretical Results â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures"): Feasibility, summability, and one-step contraction

#### Model and notation.

Fix an index â„“\ell and write

|  |  |  |
| --- | --- | --- |
|  | hâ„“+1=(I+Î”â€‹tâ„“â€‹Aâ„“)âŸ=â£:Mâ„“â€‹hâ„“+Wâ„“â€‹Ï•â€‹(hâ„“)+Bâ€‹uâ„“,Aâ„“:=AÎ¸â€‹(Tâ„“),h\_{\ell+1}\;=\;\underbrace{(I+\Delta t\_{\ell}A\_{\ell})}\_{=:M\_{\ell}}\,h\_{\ell}\;+\;W\_{\ell}\phi(h\_{\ell})\;+\;Bu\_{\ell},\qquad A\_{\ell}:=A\_{\theta}(T\_{\ell}), |  |

with Ï•\phi 11-Lipschitz and â€–Wâ„“â€–2â‰¤Ï„â‰¤1\|W\_{\ell}\|\_{2}\leq\tau\leq 1 by Q-Align. Spec-Guard enforces Ïâ€‹(Aâ„“)â€‹Î”â€‹tâ„“â‰¤1âˆ’Îµ\rho(A\_{\ell})\Delta t\_{\ell}\leq 1-\varepsilon.

#### Well-posedness.

For fixed inputs (uâ„“)(u\_{\ell}) and initial h0h\_{0}, the recursion is explicit and thus uniquely defines (hâ„“)(h\_{\ell}). Boundedness follows from the Green summability (below) and bounded inputs. Hence the scan is well-posed.

#### Green summability.

Define the discrete Green operator (variation-of-constants expansion)

|  |  |  |
| --- | --- | --- |
|  | ğ’¢Î¸â€‹(Tâ„“,Ts):={(âˆj=sâ„“âˆ’1(Mj+Wjâ€‹Jj)),sâ‰¤â„“âˆ’1,I,s=â„“,\mathcal{G}\_{\theta}(T\_{\ell},T\_{s}):=\begin{cases}\Big(\prod\_{j=s}^{\ell-1}\big(M\_{j}+W\_{j}J\_{j}\big)\Big),&s\leq\ell-1,\\[2.0pt] I,&s=\ell,\end{cases} |  |

where JjJ\_{j} is a Jacobian selector of Ï•\phi along the segment joining the two trajectories (by mean-value). Since Ï•\phi is nonexpansive, â€–Jjâ€–â‰¤1\|J\_{j}\|\leq 1. We claim there exists an induced norm âˆ¥â‹…âˆ¥âˆ—\|\cdot\|\_{\*} such that

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–Mj+Wjâ€‹Jjâ€–âˆ—â‰¤â€„1âˆ’Îµ,âˆ€j.\big\|M\_{j}+W\_{j}J\_{j}\big\|\_{\*}\;\leq\;1-\varepsilon,\qquad\forall j. |  | (96) |

Indeed, by Gelfandâ€™s formula and the assumption Ïâ€‹(Aj)â€‹Î”â€‹tjâ‰¤1âˆ’Îµ\rho(A\_{j})\Delta t\_{j}\leq 1-\varepsilon, for any Î´âˆˆ(0,Îµ)\delta\in(0,\varepsilon) there exists an induced norm âˆ¥â‹…âˆ¥âˆ—,j\|\cdot\|\_{\*,j} with â€–Mjâ€–âˆ—,jâ‰¤1âˆ’Îµ+Î´\|M\_{j}\|\_{\*,j}\leq 1-\varepsilon+\delta. Q-Align scales WjW\_{j} so that â€–Wjâ€–âˆ—,jâ‰¤Î´\|W\_{j}\|\_{\*,j}\leq\delta (this is the layerwise 11-Lip projection; see SectionÂ 3.2). Since â€–Jjâ€–âˆ—,jâ‰¤1\|J\_{j}\|\_{\*,j}\leq 1, subadditivity yields â€–Mj+Wjâ€‹Jjâ€–âˆ—,jâ‰¤1âˆ’Îµ+2â€‹Î´\|M\_{j}+W\_{j}J\_{j}\|\_{\*,j}\leq 1-\varepsilon+2\delta. Choosing Î´=Îµ/4\delta=\varepsilon/4 gives â‰¤1âˆ’Îµ/2\leq 1-\varepsilon/2. By norm equivalence in finite dimensions there exists a global induced norm âˆ¥â‹…âˆ¥âˆ—\|\cdot\|\_{\*} and a constant Îºâ‰¥1\kappa\geq 1 such that ([96](https://arxiv.org/html/2511.06451v1#Ax1.E96 "In Green summability. â€£ B.6 Proof of Proposition 6: Feasibility, summability, and one-step contraction â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) holds with the same contraction factor after absorbing Îº\kappa into Îµ\varepsilon (i.e., replace Îµ\varepsilon by Îµâ€²=Îµ/(2â€‹Îº)\varepsilon^{\prime}=\varepsilon/(2\kappa)). Renaming Îµâ€²\varepsilon^{\prime} as Îµ\varepsilon proves ([96](https://arxiv.org/html/2511.06451v1#Ax1.E96 "In Green summability. â€£ B.6 Proof of Proposition 6: Feasibility, summability, and one-step contraction â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")). Consequently,

|  |  |  |
| --- | --- | --- |
|  | âˆ‘sâ‰¤â„“â€–ğ’¢Î¸â€‹(Tâ„“,Ts)â€–âˆ—â‰¤âˆ‘k=0âˆ(1âˆ’Îµ)k=1Îµ.\sum\_{s\leq\ell}\big\|\mathcal{G}\_{\theta}(T\_{\ell},T\_{s})\big\|\_{\*}\;\leq\;\sum\_{k=0}^{\infty}(1-\varepsilon)^{k}\;=\;\frac{1}{\varepsilon}. |  |

Switching back to the Euclidean norm via equivalence yields LemmaÂ [1](https://arxiv.org/html/2511.06451v1#Thmlemma1 "Lemma 1 (Green kernel bound). â€£ Spectral safety and discrete Green kernel. â€£ 3.1 Riskâ€“Neutral Operator Layer (RN-Operator) â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures") with a constant Câ€‹(Îµ)C(\varepsilon), hence the Green expansion is summable.

#### One-step error contraction.

Consider two trajectories driven by inputs (uâ„“)(u\_{\ell}) and (u~â„“)(\tilde{u}\_{\ell}) and initial states (h0,h~0)(h\_{0},\tilde{h}\_{0}). By mean-value form,

|  |  |  |
| --- | --- | --- |
|  | Ï•â€‹(hâ„“)âˆ’Ï•â€‹(h~â„“)=Jâ„“â€‹(hâ„“âˆ’h~â„“),â€–Jâ„“â€–â‰¤1.\phi(h\_{\ell})-\phi(\tilde{h}\_{\ell})=J\_{\ell}\,(h\_{\ell}-\tilde{h}\_{\ell}),\qquad\|J\_{\ell}\|\leq 1. |  |

Hence

|  |  |  |
| --- | --- | --- |
|  | hâ„“+1âˆ’h~â„“+1=(Mâ„“+Wâ„“â€‹Jâ„“)â€‹(hâ„“âˆ’h~â„“)+Bâ€‹(uâ„“âˆ’u~â„“).h\_{\ell+1}-\tilde{h}\_{\ell+1}=(M\_{\ell}+W\_{\ell}J\_{\ell})\,(h\_{\ell}-\tilde{h}\_{\ell})+B\,(u\_{\ell}-\tilde{u}\_{\ell}). |  |

Taking the induced norm from ([96](https://arxiv.org/html/2511.06451v1#Ax1.E96 "In Green summability. â€£ B.6 Proof of Proposition 6: Feasibility, summability, and one-step contraction â€£ Appendix A. Proofs for Sections 3 â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) and then using norm equivalence,

|  |  |  |
| --- | --- | --- |
|  | â€–hâ„“+1âˆ’h~â„“+1â€–â‰¤(1âˆ’Îµ)â€‹â€–hâ„“âˆ’h~â„“â€–+â€–Bâ€–â€‹â€–uâ„“âˆ’u~â„“â€–.\|h\_{\ell+1}-\tilde{h}\_{\ell+1}\|\;\leq\;(1-\varepsilon)\,\|h\_{\ell}-\tilde{h}\_{\ell}\|\;+\;\|B\|\,\|u\_{\ell}-\tilde{u}\_{\ell}\|. |  |

If inputs arise from a Lipschitz pre-map uâ„“=Îâ€‹zâ„“u\_{\ell}=\Xi z\_{\ell}, then
â€–uâ„“âˆ’u~â„“â€–â‰¤â€–Îâ€–â€‹â€–zâ„“âˆ’z~â„“â€–,\|u\_{\ell}-\tilde{u}\_{\ell}\|\leq\|\Xi\|\,\|z\_{\ell}-\tilde{z}\_{\ell}\|,
and the second term becomes â€–Bâ€–â€‹â€–Îâ€–â€‹â€–zâ„“âˆ’z~â„“â€–\|B\|\,\|\Xi\|\,\|z\_{\ell}-\tilde{z}\_{\ell}\|. This yields ([34](https://arxiv.org/html/2511.06451v1#S4.E34 "In Proposition 6 (Feasibility and error propagation). â€£ T6: Feasibility and Two-Time-Scale Averaging under Spectral Guard â€£ 4 Theoretical Results â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")).

#### Feasibility of the Green series in the nonlinear case.

By expanding the recursion and repeatedly inserting the mean-value Jacobians JjJ\_{j}, the nonlinear Green operator is a product of step Jacobians Mj+Wjâ€‹JjM\_{j}+W\_{j}J\_{j}, each contracting by at least 1âˆ’Îµ1-\varepsilon in âˆ¥â‹…âˆ¥âˆ—\|\cdot\|\_{\*}. Thus the Neumann-type series is absolutely summable, which also implies boundedness of the state for bounded inputs.

â–¡\Box

### B.7 Two-time-scale averaging: variance reduction of the averaged gap

#### Set-up.

Let Fâ€‹(Î¸,Î»)F(\theta,\lambda) be a monotone operator associated with the saddle formulation, and let the updates follow

|  |  |  |
| --- | --- | --- |
|  | Î¸k+1=Î¸kâˆ’Î·Î¸â€‹(FÎ¸â€‹(Î¸k,Î»k)+Î¾kÎ¸),Î»k+1=Î»k+Î·Î»â€‹(FÎ»â€‹(Î¸k,Î»k)+Î¾kÎ»),\theta\_{k+1}=\theta\_{k}-\eta\_{\theta}\,\big(F\_{\theta}(\theta\_{k},\lambda\_{k})+\xi^{\theta}\_{k}\big),\qquad\lambda\_{k+1}=\lambda\_{k}+\eta\_{\lambda}\,\big(F\_{\lambda}(\theta\_{k},\lambda\_{k})+\xi^{\lambda}\_{k}\big), |  |

with unbiased martingale-difference noises Î¾kÎ¸,Î¾kÎ»\xi^{\theta}\_{k},\xi^{\lambda}\_{k} of variances bounded by Ïƒ2\sigma^{2}. Two-time-scale averaging considers the Polyakâ€“Ruppert averages Î¸Â¯K=1Kâ€‹âˆ‘k=1KÎ¸k\bar{\theta}\_{K}=\tfrac{1}{K}\sum\_{k=1}^{K}\theta\_{k} and Î»Â¯K=1Kâ€‹âˆ‘k=1KÎ»k\bar{\lambda}\_{K}=\tfrac{1}{K}\sum\_{k=1}^{K}\lambda\_{k} (or a tail average).

#### Averaged gap decay.

Under monotonicity of FF, Lipschitz continuity, and step sizes Î·Î¸,Î·Î»=Î˜â€‹(1/L)\eta\_{\theta},\eta\_{\lambda}=\Theta(1/L), standard arguments (e.g., stochastic approximation for monotone variational inequalities) yield

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[Gapâ€‹(Î¸Â¯K,Î»Â¯K)]â‰¤ğ’ªâ€‹(Lâ€‹â€–z0âˆ’zâ‹†â€–2K)+ğ’ªâ€‹(Ïƒ2),\mathbb{E}\big[\mathrm{Gap}(\bar{\theta}\_{K},\bar{\lambda}\_{K})\big]\;\leq\;\mathcal{O}\!\left(\frac{L\,\|z^{0}-z^{\star}\|^{2}}{K}\right)\;+\;\mathcal{O}(\sigma^{2}), |  |

where z=(Î¸,Î»)z=(\theta,\lambda) and zâ‹†z^{\star} is a saddle point. The ğ’ªâ€‹(1/K)\mathcal{O}(1/K) term is the variance reduction factor for the averaged gap, while the additive noise floor ğ’ªâ€‹(Ïƒ2)\mathcal{O}(\sigma^{2}) matches the extragradient noise ball in TheoremÂ [1](https://arxiv.org/html/2511.06451v1#Thmtheorem1 "Theorem 1 (Extragradient convergence to a noise ball). â€£ Convergence guarantee (noise-stable neighborhood). â€£ 3.4 Saddle-Point Training and Safety-Oriented Stopping â€£ 3 Method: The ARBITER Architecture â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures"). The proof adapts classical Robbinsâ€“Monro and Polyakâ€“Juditsky averaging to the primalâ€“dual setting with Q-Align treated as a nonexpansive projection; see AppendixÂ E.1 for the extragradient analysis and replace the one-step decrease inequality by its TTSA counterpart.

â–¡\Box

## Appendix C. Joint identifiability with replication and a counterexample for SPX-only

We work at a fixed maturity TT and suppress the index when unambiguous; the argument is identical for each Tâ„“T\_{\ell} on the grid and thus yields joint identifiability across maturities. Let ğ’\mathcal{C} denote the class of call-price sections Kâ†¦Câ€‹(K)K\mapsto C(K) that are convex, decreasing in TT, satisfy no-arbitrage boundary conditions, and are produced by the RN-operator followed by our convexâ€“monotone decoder and interpolation policy.

### C.1 Injectivity with calls++replication

#### Discrete operators.

Let ğ’¦={K1<â‹¯<KM}\mathcal{K}=\{K\_{1}<\cdots<K\_{M}\} be the strike grid. Define the sampling operator ğ–²:ğ’â†’â„M\mathsf{S}:\mathcal{C}\to\mathbb{R}^{M}, (ğ–²â€‹C)i:=Câ€‹(Ki)(\mathsf{S}C)\_{i}:=C(K\_{i}), and the discretized BL operator ğ–¡:ğ’â†’â„Mâˆ’2\mathsf{B}:\mathcal{C}\to\mathbb{R}^{M-2} via centered second differences

|  |  |  |
| --- | --- | --- |
|  | (ğ–¡â€‹C)i:=Câ€‹(Kiâˆ’1)âˆ’2â€‹Câ€‹(Ki)+Câ€‹(Ki+1)(Ki+1âˆ’Ki)â€‹(Kiâˆ’Kiâˆ’1),i=2,â€¦,Mâˆ’1,(\mathsf{B}C)\_{i}:=\frac{C(K\_{i-1})-2C(K\_{i})+C(K\_{i+1})}{(K\_{i+1}-K\_{i})(K\_{i}-K\_{i-1})},\qquad i=2,\dots,M-1, |  |

which approximates eâˆ’râ€‹Tâ€‹âˆ‚Kâ€‹KCâ€‹(Ki)e^{-rT}\,\partial\_{KK}C(K\_{i}) and thus the riskâ€“neutral density (up to discount). Let the discretized replication functional ğ–±:ğ’â†’â„\mathsf{R}:\mathcal{C}\to\mathbb{R} be

|  |  |  |
| --- | --- | --- |
|  | ğ–±â€‹(C):=2â€‹erâ€‹TTâ€‹âˆ‘Kiâˆˆğ’¦Î”â€‹KiKi2â€‹Qâ€‹(Ki;C),\mathsf{R}(C):=\frac{2\,e^{rT}}{T}\sum\_{K\_{i}\in\mathcal{K}}\frac{\Delta K\_{i}}{K\_{i}^{2}}\,Q(K\_{i};C), |  |

where Qâ€‹(Ki;C)Q(K\_{i};C) denotes the out-of-the-money option value derived from CC at KiK\_{i} (call for Kiâ‰¥FK\_{i}\geq F, put for Ki<FK\_{i}<F) and Î”â€‹Ki\Delta K\_{i} are the exchange-specified increments.

#### Claim.

If C1,C2âˆˆğ’C\_{1},C\_{2}\in\mathcal{C} satisfy ğ–²â€‹C1=ğ–²â€‹C2\mathsf{S}C\_{1}=\mathsf{S}C\_{2} and ğ–±â€‹(C1)=ğ–±â€‹(C2)\mathsf{R}(C\_{1})=\mathsf{R}(C\_{2}), then C1=C2C\_{1}=C\_{2} on the convex interpolation induced by our policy; equivalently, the underlying RN-operator sections agree at TT up to model symmetries.

#### Proof.

Let Î”â€‹C:=C1âˆ’C2âˆˆğ’âˆ’ğ’\Delta C:=C\_{1}-C\_{2}\in\mathcal{C}-\mathcal{C}. Then ğ–²â€‹Î”â€‹C=0\mathsf{S}\Delta C=0 and ğ–±â€‹(Î”â€‹C)=0\mathsf{R}(\Delta C)=0. Because each CjC\_{j} is convex in KK and our interpolation is piecewise linear in (K,C)(K,C) between knots (or piecewise-convex with fixed shape parameters; both cases covered below), the section on [Ki,Ki+1][K\_{i},K\_{i+1}] is determined by the pair (Câ€‹(Ki),Câ€‹(Ki+1))(C(K\_{i}),C(K\_{i+1})) and the admissible slope set consistent with convexity and boundary no-arbitrage. Since Î”â€‹C\Delta C vanishes at all knots, its restriction on any [Ki,Ki+1][K\_{i},K\_{i+1}] is a (weakly) convex function anchored at zero endpoints. The only such function consistent with *both* (i) zero BL second difference at the interior knot and (ii) zero replication contribution *summed across the grid* is the zero function.

Formally, write the piecewise representation

|  |  |  |
| --- | --- | --- |
|  | Î”â€‹Câ€‹(K)=âˆ‘i=1Mâˆ’1ğŸ[Ki,Ki+1)â€‹(K)â€‹giâ€‹(K),\Delta C(K)\;=\;\sum\_{i=1}^{M-1}\mathbf{1}\_{[K\_{i},K\_{i+1})}(K)\,g\_{i}(K), |  |

with gig\_{i} convex on [Ki,Ki+1][K\_{i},K\_{i+1}] and giâ€‹(Ki)=giâ€‹(Ki+1)=0g\_{i}(K\_{i})=g\_{i}(K\_{i+1})=0. Then (ğ–¡â€‹Î”â€‹C)i(\mathsf{B}\Delta C)\_{i} collects discrete curvature at KiK\_{i}, and ğ–±â€‹(Î”â€‹C)\mathsf{R}(\Delta C) is a nonnegative linear functional of the gig\_{i}â€™s (weights 1/K21/K^{2} are positive). Because each gig\_{i} has nonnegative distributional second derivative (convexity) and is zero at the endpoints, we have âˆ«KiKi+1giâ€‹(K)K2â€‹ğ‘‘Kâ‰¥0\int\_{K\_{i}}^{K\_{i+1}}\frac{g\_{i}(K)}{K^{2}}\,dK\geq 0, with equality iff giâ‰¡0g\_{i}\equiv 0. Summing over ii and using ğ–±â€‹(Î”â€‹C)=0\mathsf{R}(\Delta C)=0 forces every giâ‰¡0g\_{i}\equiv 0, hence Î”â€‹Câ‰¡0\Delta C\equiv 0 on [K1,KM][K\_{1},K\_{M}]. Outside [K1,KM][K\_{1},K\_{M}], boundary no-arbitrage with matching left/right slopes222Our interpolation policy fixes tail extrapolation by monotone linear continuation consistent with convexity and forward constraints; see SectionÂ 3.3. yields uniqueness as well. Therefore C1=C2C\_{1}=C\_{2} on the whole line.

Lifting back to parameters: if ğ’¢Î¸1\mathcal{G}\_{\theta\_{1}} and ğ’¢Î¸2\mathcal{G}\_{\theta\_{2}} induce CÎ¸1C\_{\theta\_{1}} and CÎ¸2C\_{\theta\_{2}} matching on the grid and in ğ–±\mathsf{R}, then CÎ¸1=CÎ¸2C\_{\theta\_{1}}=C\_{\theta\_{2}}, and hence ğ’¢Î¸1\mathcal{G}\_{\theta\_{1}} and ğ’¢Î¸2\mathcal{G}\_{\theta\_{2}} coincide as operator realizations modulo internal reparameterizations that leave CC invariant (symmetries). â–¡\square

#### Remark on piecewise-convex decoders.

If the decoder uses ICNN splines or Legendre patches with fixed shape hyperparameters across intervals, then the per-interval convex function is still pinned by knot values together with convexity and the global replication constraint; the above argument carries through by replacing the integral test with the corresponding basis-weighted version.

### C.2 Counterexample for SPX-only

#### Functional-analytic construction.

Consider the linear measurement operator ğ–²:ğ’â†’â„M\mathsf{S}:\mathcal{C}\to\mathbb{R}^{M}, Câ†¦(Câ€‹(Ki))i=1MC\mapsto(C(K\_{i}))\_{i=1}^{M}. Its kernel in the ambient vector space of sufficiently smooth convex functions is nontrivial: take a C2C^{2} bump bâ€‹(K)b(K) supported strictly inside (Kj,Kj+1)(K\_{j},K\_{j+1}) for some jj, with bâ€‹(Kj)=bâ€‹(Kj+1)=0b(K\_{j})=b(K\_{j+1})=0, bâ‰¥0b\geq 0, and bâ€²â€²â‰¥0b^{\prime\prime}\geq 0 (convex). Then define

|  |  |  |
| --- | --- | --- |
|  | C~Î±â€‹(K)=Câ€‹(K)+Î±â€‹bâ€‹(K),Î±>0â€‹Â small.\widetilde{C}\_{\alpha}(K)\;=\;C(K)+\alpha\,b(K),\qquad\alpha>0\text{ small}. |  |

For all grid strikes KiK\_{i}, C~Î±â€‹(Ki)=Câ€‹(Ki)\widetilde{C}\_{\alpha}(K\_{i})=C(K\_{i}), so ğ–²â€‹C~Î±=ğ–²â€‹C\mathsf{S}\widetilde{C}\_{\alpha}=\mathsf{S}C. Convexity and monotonicity are preserved for sufficiently small Î±\alpha (by local convex perturbation). However,

|  |  |  |
| --- | --- | --- |
|  | ğ–±â€‹(C~Î±)âˆ’ğ–±â€‹(C)=2â€‹erâ€‹TTâ€‹âˆ‘i=1MÎ”â€‹KiKi2â€‹(Qâ€‹(Ki;C~Î±)âˆ’Qâ€‹(Ki;C))>â€„0\mathsf{R}(\widetilde{C}\_{\alpha})-\mathsf{R}(C)\;=\;\frac{2\,e^{rT}}{T}\sum\_{i=1}^{M}\frac{\Delta K\_{i}}{K\_{i}^{2}}\,\Big(Q(K\_{i};\widetilde{C}\_{\alpha})-Q(K\_{i};C)\Big)\;>\;0 |  |

whenever the support of bb intersects the OTM region relevant to the weights (this can always be arranged), because Qâ€‹(â‹…)Q(\cdot) is linear in CC on each side and the weights 1/K21/K^{2} are strictly positive. Thus SPX-only measurements are not injective: ğ–²â€‹C~Î±=ğ–²â€‹C\mathsf{S}\widetilde{C}\_{\alpha}=\mathsf{S}C yet ğ–±â€‹(C~Î±)â‰ ğ–±â€‹(C)\mathsf{R}(\widetilde{C}\_{\alpha})\neq\mathsf{R}(C).

#### Linear-algebraic view (Hahnâ€“Banach separation).

Alternatively, view ğ–²\mathsf{S} as an MM-row operator and ğ–±\mathsf{R} as an independent linear functional. Unless ğ–±\mathsf{R} lies in the row span of ğ–²\mathsf{S} (which it does not for generic grids and 1/K21/K^{2} weights), there exists Î”â€‹Câˆˆkerâ¡ğ–²\Delta C\in\ker\mathsf{S} with ğ–±â€‹(Î”â€‹C)â‰ 0\mathsf{R}(\Delta C)\neq 0. Approximating Î”â€‹C\Delta C by convex bumps and scaling yields admissible convex perturbations as above.

#### Tail-aware variants.

Even if one augments the grid with deep OTM strikes, finite discretization leaves inter-knot degrees of freedom. The replication functional collapses these by coupling local curvature (BL density) with a global 1/K21/K^{2} weight; hence calls++replication remove the null directions that SPX-only cannot eliminate.

â–¡\Box

## Appendix D. Convergence to a noise ball under fixed thresholds

We prove TheoremÂ [6](https://arxiv.org/html/2511.06451v1#Thmtheorem6 "Theorem 6 (Convergence to a noise ball under fixed thresholds). â€£ T8: Saddle-Point Convergence with Fixed Safety Thresholds â€£ 4 Theoretical Results â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures") for the two-time-scale extragradient (EG) scheme with Q-Align projections. Let ğ’µâŠ‚â„d\mathcal{Z}\subset\mathbb{R}^{d} be nonempty, closed, and convex. The saddle operator F:ğ’µâ†’â„dF:\mathcal{Z}\to\mathbb{R}^{d} is assumed *monotone* and *LL-Lipschitz*:

|  |  |  |
| --- | --- | --- |
|  | âŸ¨Fâ€‹(x)âˆ’Fâ€‹(y),xâˆ’yâŸ©â‰¥0,â€–Fâ€‹(x)âˆ’Fâ€‹(y)â€–â‰¤Lâ€‹â€–xâˆ’yâ€–,âˆ€x,yâˆˆğ’µ.\langle F(x)-F(y),\,x-y\rangle\geq 0,\qquad\|F(x)-F(y)\|\leq L\|x-y\|,\quad\forall x,y\in\mathcal{Z}. |  |

Let zâ‹†z^{\star} solve the variational inequality 0âˆˆFâ€‹(zâ‹†)+Nğ’µâ€‹(zâ‹†)0\in F(z^{\star})+N\_{\mathcal{Z}}(z^{\star}).

### .1 Algorithm and error model

At iteration kk, the two-time-scale EG with Q-Align reads

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | yk\displaystyle y^{k} | =ğ–¯kâ€‹(zkâˆ’Î·Î¸â€‹(Fâ€‹(zk)+Î¾k)),\displaystyle=\mathsf{P}\_{k}\!\big(z^{k}-\eta\_{\theta}\big(F(z^{k})+\xi^{k}\big)\big), |  | (97) |
|  | zk+1\displaystyle z^{k+1} | =ğ–¯kâ€‹(zkâˆ’Î·Î»â€‹(Fâ€‹(yk)+Î¶k)),\displaystyle=\mathsf{P}\_{k}\!\big(z^{k}-\eta\_{\lambda}\big(F(y^{k})+\zeta^{k}\big)\big), |  |

where Î·Î¸,Î·Î»>0\eta\_{\theta},\eta\_{\lambda}>0 are step sizes (we take Î·Î¸=Î·Î»=Î·âˆˆ(0,1/L]\eta\_{\theta}=\eta\_{\lambda}=\eta\in(0,1/L] unless otherwise noted), and Î¾k,Î¶k\xi^{k},\zeta^{k} are martingale-difference noises satisfying

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[Î¾kâˆ£â„±k]=0,ğ”¼â€‹[Î¶kâˆ£â„±k]=0,ğ”¼â€‹[â€–Î¾kâ€–2+â€–Î¶kâ€–2âˆ£â„±k]â‰¤Ïƒ2.\mathbb{E}[\xi^{k}\mid\mathcal{F}\_{k}]=0,\quad\mathbb{E}[\zeta^{k}\mid\mathcal{F}\_{k}]=0,\qquad\mathbb{E}\big[\|\xi^{k}\|^{2}+\|\zeta^{k}\|^{2}\mid\mathcal{F}\_{k}\big]\leq\sigma^{2}. |  |

The Q-Align projection ğ–¯k\mathsf{P}\_{k} is *nonexpansive with bounded defect*:

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–ğ–¯kâ€‹(u)âˆ’ğ–¯kâ€‹(v)â€–â‰¤â€–uâˆ’vâ€–,â€–ğ–¯kâ€‹(w)âˆ’Î ğ’µâ€‹(w)â€–â‰¤Î´proj,\|\mathsf{P}\_{k}(u)-\mathsf{P}\_{k}(v)\|\leq\|u-v\|,\qquad\|\mathsf{P}\_{k}(w)-\Pi\_{\mathcal{Z}}(w)\|\leq\delta\_{\mathrm{proj}}, |  | (98) |

for all u,v,wu,v,w, where Î ğ’µ\Pi\_{\mathcal{Z}} is the Euclidean projector and Î´projâ‰¥0\delta\_{\mathrm{proj}}\geq 0 quantifies the per-step projection error due to Q-Align.

### .2 Oneâ€“step inequality

###### Lemma 8 (FejÃ©r-type inequality with noise and projection defect).

For any zâˆˆğ’µz\in\mathcal{Z} and kâ‰¥0k\geq 0,

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–zk+1âˆ’zâ€–2\displaystyle\|z^{k+1}-z\|^{2} | â‰¤â€–zkâˆ’zâ€–2âˆ’2â€‹Î·â€‹âŸ¨Fâ€‹(yk),zkâˆ’zâŸ©+2â€‹Î·â€‹âŸ¨Fâ€‹(yk)âˆ’Fâ€‹(zk),ykâˆ’zkâŸ©\displaystyle\leq\|z^{k}-z\|^{2}-2\eta\,\langle F(y^{k}),\,z^{k}-z\rangle+2\eta\,\langle F(y^{k})-F(z^{k}),\,y^{k}-z^{k}\rangle |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +2â€‹Î·â€‹âŸ¨Î¶k,zk+1âˆ’zâŸ©+C1â€‹Î·2â€‹(â€–Fâ€‹(zk)â€–2+â€–Î¾kâ€–2+â€–Î¶kâ€–2)+C2â€‹Î´proj2,\displaystyle\quad+2\eta\,\langle\zeta^{k},\,z^{k+1}-z\rangle+C\_{1}\,\eta^{2}\Big(\|F(z^{k})\|^{2}+\|\xi^{k}\|^{2}+\|\zeta^{k}\|^{2}\Big)+C\_{2}\,\delta\_{\mathrm{proj}}^{2}, |  |

for absolute constants C1,C2>0C\_{1},C\_{2}>0 independent of kk.

###### Proof.

Using nonexpansiveness of ğ–¯k\mathsf{P}\_{k} and the identity â€–aâ€–2âˆ’â€–bâ€–2=2â€‹âŸ¨aâˆ’b,aâŸ©âˆ’â€–aâˆ’bâ€–2\|a\|^{2}-\|b\|^{2}=2\langle a-b,a\rangle-\|a-b\|^{2},

|  |  |  |
| --- | --- | --- |
|  | â€–zk+1âˆ’zâ€–2=â€–ğ–¯kâ€‹(â‹…)âˆ’ğ–¯kâ€‹(â‹…)â€–2â‰¤â€–zkâˆ’Î·â€‹(Fâ€‹(yk)+Î¶k)âˆ’zâ€–2+Î”k,\|z^{k+1}-z\|^{2}=\big\|\mathsf{P}\_{k}(\cdot)-\mathsf{P}\_{k}(\cdot)\big\|^{2}\leq\big\|z^{k}-\eta(F(y^{k})+\zeta^{k})-z\big\|^{2}+\Delta\_{k}, |  |

where Î”k:=2â€‹âŸ¨zk+1âˆ’Î ğ’µâ€‹(â‹…),zk+1âˆ’zâŸ©â‰¤2â€‹â€–zk+1âˆ’Î ğ’µâ€‹(â‹…)â€–â‹…â€–zk+1âˆ’zâ€–â‰¤C2â€‹Î´proj2\Delta\_{k}:=2\langle z^{k+1}-\Pi\_{\mathcal{Z}}(\cdot),z^{k+1}-z\rangle\leq 2\|z^{k+1}-\Pi\_{\mathcal{Z}}(\cdot)\|\cdot\|z^{k+1}-z\|\leq C\_{2}\delta\_{\mathrm{proj}}^{2} by ([98](https://arxiv.org/html/2511.06451v1#Ax3.E98 "In .1 Algorithm and error model â€£ Appendix D. Convergence to a noise ball under fixed thresholds â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")) and Youngâ€™s inequality. Expanding the square and bounding cross terms yields the claim after noting â€–ykâˆ’zkâ€–â‰¤Î·â€‹â€–Fâ€‹(zk)+Î¾kâ€–+ğ’ªâ€‹(Î´proj)\|y^{k}-z^{k}\|\leq\eta\|F(z^{k})+\xi^{k}\|+\mathcal{O}(\delta\_{\mathrm{proj}}) from the first projection step in ([97](https://arxiv.org/html/2511.06451v1#Ax3.E97 "In .1 Algorithm and error model â€£ Appendix D. Convergence to a noise ball under fixed thresholds â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")).
âˆ

###### Lemma 9 (Monotonicityâ€“Lipschitz surrogate).

For any x,yâˆˆğ’µx,y\in\mathcal{Z},

|  |  |  |
| --- | --- | --- |
|  | âŸ¨Fâ€‹(y),xâˆ’yâŸ©â‰¤âŸ¨Fâ€‹(x),xâˆ’yâŸ©+L2â€‹â€–xâˆ’yâ€–2,â€–Fâ€‹(x)â€–â‰¤Lâ€‹â€–xâˆ’zâ‹†â€–.\langle F(y),\,x-y\rangle\;\leq\;\langle F(x),\,x-y\rangle+\tfrac{L}{2}\|x-y\|^{2},\qquad\|F(x)\|\leq L\|x-z^{\star}\|. |  |

###### Proof.

The first bound follows by Lipschitzness and Cauchyâ€“Schwarz; the second uses monotonicity with zâ‹†z^{\star} and Lipschitzness to get â€–Fâ€‹(x)â€–2=âŸ¨Fâ€‹(x)âˆ’Fâ€‹(zâ‹†),Fâ€‹(x)âˆ’Fâ€‹(zâ‹†)âŸ©â‰¤Lâ€‹âŸ¨Fâ€‹(x)âˆ’Fâ€‹(zâ‹†),xâˆ’zâ‹†âŸ©â‰¤Lâ€‹â€–Fâ€‹(x)â€–â€‹â€–xâˆ’zâ‹†â€–\|F(x)\|^{2}=\langle F(x)-F(z^{\star}),F(x)-F(z^{\star})\rangle\leq L\langle F(x)-F(z^{\star}),x-z^{\star}\rangle\leq L\|F(x)\|\|x-z^{\star}\|.
âˆ

### .3 Telescoping and residual control

Apply LemmaÂ [8](https://arxiv.org/html/2511.06451v1#Thmlemma8 "Lemma 8 (FejÃ©r-type inequality with noise and projection defect). â€£ .2 Oneâ€“step inequality â€£ Appendix D. Convergence to a noise ball under fixed thresholds â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures") with z=zâ‹†z=z^{\star}, condition on â„±k\mathcal{F}\_{k}, and use ğ”¼â€‹[Î¶kâˆ£â„±k]=0\mathbb{E}[\zeta^{k}\mid\mathcal{F}\_{k}]=0:

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[â€–zk+1âˆ’zâ‹†â€–2]â‰¤ğ”¼â€‹[â€–zkâˆ’zâ‹†â€–2]âˆ’2â€‹Î·â€‹ğ”¼â€‹[âŸ¨Fâ€‹(yk),zkâˆ’zâ‹†âŸ©]+C1â€²â€‹Î·2â€‹(ğ”¼â€‹â€–Fâ€‹(zk)â€–2+Ïƒ2)+C2â€‹Î´proj2.\mathbb{E}\big[\|z^{k+1}-z^{\star}\|^{2}\big]\leq\mathbb{E}\big[\|z^{k}-z^{\star}\|^{2}\big]-2\eta\,\mathbb{E}\big[\langle F(y^{k}),z^{k}-z^{\star}\rangle\big]+C^{\prime}\_{1}\eta^{2}\Big(\mathbb{E}\|F(z^{k})\|^{2}+\sigma^{2}\Big)+C\_{2}\delta\_{\mathrm{proj}}^{2}. |  |

By LemmaÂ [9](https://arxiv.org/html/2511.06451v1#Thmlemma9 "Lemma 9 (Monotonicityâ€“Lipschitz surrogate). â€£ .2 Oneâ€“step inequality â€£ Appendix D. Convergence to a noise ball under fixed thresholds â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures") with x=zk,y=ykx=z^{k},y=y^{k} and Î·â‰¤1/L\eta\leq 1/L,

|  |  |  |
| --- | --- | --- |
|  | âŸ¨Fâ€‹(yk),zkâˆ’zâ‹†âŸ©â‰¥âŸ¨Fâ€‹(zk),zkâˆ’zâ‹†âŸ©âˆ’L2â€‹â€–ykâˆ’zkâ€–2â‰¥1Lâ€‹â€–Fâ€‹(zk)â€–2âˆ’C1â€²â€²â€‹Î·2â€‹â€–Fâ€‹(zk)â€–2âˆ’C2â€²â€²â€‹Î·2â€‹Ïƒ2,\langle F(y^{k}),z^{k}-z^{\star}\rangle\geq\langle F(z^{k}),z^{k}-z^{\star}\rangle-\tfrac{L}{2}\|y^{k}-z^{k}\|^{2}\geq\tfrac{1}{L}\|F(z^{k})\|^{2}-C^{\prime\prime}\_{1}\eta^{2}\|F(z^{k})\|^{2}-C^{\prime\prime}\_{2}\eta^{2}\sigma^{2}, |  |

which, for Î·â‰¤1/L\eta\leq 1/L and absorbing constants, gives

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[â€–zk+1âˆ’zâ‹†â€–2]â‰¤ğ”¼â€‹[â€–zkâˆ’zâ‹†â€–2]âˆ’Î·Lâ€‹ğ”¼â€‹â€–Fâ€‹(zk)â€–2+C3â€‹Î·2â€‹(ğ”¼â€‹â€–Fâ€‹(zk)â€–2+Ïƒ2)+C2â€‹Î´proj2.\mathbb{E}\big[\|z^{k+1}-z^{\star}\|^{2}\big]\leq\mathbb{E}\big[\|z^{k}-z^{\star}\|^{2}\big]-\tfrac{\eta}{L}\,\mathbb{E}\|F(z^{k})\|^{2}+C\_{3}\,\eta^{2}\Big(\mathbb{E}\|F(z^{k})\|^{2}+\sigma^{2}\Big)+C\_{2}\,\delta\_{\mathrm{proj}}^{2}. |  |

Choosing Î·â‰¤1/(2â€‹L)\eta\leq 1/(2L) makes (Î·/Lâˆ’C3â€‹Î·2)â‰¥câ€‹Î·/L(\eta/L-C\_{3}\eta^{2})\geq c\eta/L for a constant câˆˆ(0,1)c\in(0,1), hence

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[â€–zk+1âˆ’zâ‹†â€–2]â‰¤ğ”¼â€‹[â€–zkâˆ’zâ‹†â€–2]âˆ’câ€‹Î·Lâ€‹ğ”¼â€‹â€–Fâ€‹(zk)â€–2+C4â€‹Î·2â€‹Ïƒ2+C2â€‹Î´proj2.\mathbb{E}\big[\|z^{k+1}-z^{\star}\|^{2}\big]\leq\mathbb{E}\big[\|z^{k}-z^{\star}\|^{2}\big]-c\,\tfrac{\eta}{L}\,\mathbb{E}\|F(z^{k})\|^{2}+C\_{4}\,\eta^{2}\sigma^{2}+C\_{2}\,\delta\_{\mathrm{proj}}^{2}. |  |

Summing k=0k=0 to Kâˆ’1K-1 and noting nonnegativity of the LHS terms yields

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î·Lâ€‹âˆ‘k=0Kâˆ’1ğ”¼â€‹â€–Fâ€‹(zk)â€–2â‰¤ğ’ªâ€‹(â€–z0âˆ’zâ‹†â€–2)+ğ’ªâ€‹(Kâ€‹Î·2â€‹Ïƒ2)+ğ’ªâ€‹(Kâ€‹Î´proj2).\frac{\eta}{L}\sum\_{k=0}^{K-1}\mathbb{E}\|F(z^{k})\|^{2}\;\leq\;\mathcal{O}\!\big(\|z^{0}-z^{\star}\|^{2}\big)\;+\;\mathcal{O}\!\big(K\,\eta^{2}\sigma^{2}\big)\;+\;\mathcal{O}\!\big(K\,\delta\_{\mathrm{proj}}^{2}\big). |  | (99) |

Dividing by Kâ€‹Î·K\eta and using Î·=Î˜â€‹(1/L)\eta=\Theta(1/L) gives both the *ergodic* and *pointwise* (via minâ‰¤\min\leq average) residual bounds:

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | 1Kâ€‹âˆ‘k=0Kâˆ’1ğ”¼â€‹â€–Fâ€‹(zk)â€–2\displaystyle\frac{1}{K}\sum\_{k=0}^{K-1}\mathbb{E}\|F(z^{k})\|^{2} | â‰¤ğ’ªâ€‹(L2â€‹â€–z0âˆ’zâ‹†â€–2K)+ğ’ªâ€‹(Ïƒ2)+ğ’ªâ€‹(Lâ€‹Î´proj2),\displaystyle\leq\mathcal{O}\!\left(\frac{L^{2}\|z^{0}-z^{\star}\|^{2}}{K}\right)+\mathcal{O}(\sigma^{2})+\mathcal{O}\!\big(L\,\delta\_{\mathrm{proj}}^{2}\big), |  | (100) |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | min0â‰¤kâ‰¤Kâˆ’1â¡ğ”¼â€‹â€–Fâ€‹(zk)â€–2\displaystyle\min\_{0\leq k\leq K-1}\mathbb{E}\|F(z^{k})\|^{2} | â‰¤ğ’ªâ€‹(L2â€‹â€–z0âˆ’zâ‹†â€–2K)+ğ’ªâ€‹(Ïƒ2)+ğ’ªâ€‹(Lâ€‹Î´proj2).\displaystyle\leq\mathcal{O}\!\left(\frac{L^{2}\|z^{0}-z^{\star}\|^{2}}{K}\right)+\mathcal{O}(\sigma^{2})+\mathcal{O}\!\big(L\,\delta\_{\mathrm{proj}}^{2}\big). |  | (101) |

This establishes the rate in TheoremÂ [6](https://arxiv.org/html/2511.06451v1#Thmtheorem6 "Theorem 6 (Convergence to a noise ball under fixed thresholds). â€£ T8: Saddle-Point Convergence with Fixed Safety Thresholds â€£ 4 Theoretical Results â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures") (the ğ’ªâ€‹(Ïƒ2)\mathcal{O}(\sigma^{2}) floor) and quantifies the projection contribution.

### .4 Stopping rule and noise ball

Let rk:=â€–Fâ€‹(zk)â€–r^{k}:=\|F(z^{k})\|. Under monotonicity and Lipschitzness, the primalâ€“dual gap and the dual residual used in practice are Lipschitz-continuous surrogates of rkr^{k}; that is, there exist problem-dependent constants a1,a2>0a\_{1},a\_{2}>0 such that

|  |  |  |
| --- | --- | --- |
|  | gapâ€‹(zk)â‰¤a1â€‹rk,dualâ€‹residualâ€‹(zk)â‰¤a2â€‹rk.\mathrm{gap}(z^{k})\leq a\_{1}\,r^{k},\qquad\mathrm{dual\;residual}(z^{k})\leq a\_{2}\,r^{k}. |  |

Hence, the fixed thresholds

|  |  |  |
| --- | --- | --- |
|  | Î”â€‹Gap<10âˆ’3,dualâ€‹residual<10âˆ’3\Delta\mathrm{Gap}<10^{-3},\qquad\mathrm{dual\;residual}<10^{-3} |  |

are met once rkâ‰¤Ïµstop:=10âˆ’3â€‹minâ¡{a1âˆ’1,a2âˆ’1}r^{k}\leq\epsilon\_{\mathrm{stop}}:=10^{-3}\min\{a\_{1}^{-1},a\_{2}^{-1}\}. From ([101](https://arxiv.org/html/2511.06451v1#Ax3.E101 "In .3 Telescoping and residual control â€£ Appendix D. Convergence to a noise ball under fixed thresholds â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures")), for any Ïµ>Ïµâˆ:=c1â€‹Ïƒ+c2â€‹Lâ€‹Î´proj\epsilon>\epsilon\_{\infty}:=c\_{1}\sigma+c\_{2}\sqrt{L}\,\delta\_{\mathrm{proj}}, there exists Kâ€‹(Ïµ)K(\epsilon) such that minkâ‰¤Kâ€‹(Ïµ)â¡rkâ‰¤Ïµ\min\_{k\leq K(\epsilon)}r^{k}\leq\epsilon. The patience requirement of at least 10310^{3} steps guards against transient oscillations, and termination occurs (almost surely) in finite time provided Ïµâˆâ‰¤Ïµstop\epsilon\_{\infty}\leq\epsilon\_{\mathrm{stop}}. Finally, Lipschitzness gives â€–zkâˆ’zâ‹†â€–â‰¤rk/L\|z^{k}-z^{\star}\|\leq r^{k}/L, so upon termination,

|  |  |  |
| --- | --- | --- |
|  | â€–zkâˆ’zâ‹†â€–â‰¤1Lâ€‹(c1â€‹Ïƒ+c2â€‹Lâ€‹Î´proj+Ïµstop)=c~1â€‹ÏƒâŸnoise floor+c~2â€‹Î´projâŸprojection floor+ğ’ªâ€‹(10âˆ’3L),\|z^{k}-z^{\star}\|\;\leq\;\frac{1}{L}\Big(c\_{1}\sigma+c\_{2}\sqrt{L}\,\delta\_{\mathrm{proj}}+\epsilon\_{\mathrm{stop}}\Big)=\underbrace{\tilde{c}\_{1}\sigma}\_{\text{noise floor}}+\underbrace{\tilde{c}\_{2}\delta\_{\mathrm{proj}}}\_{\text{projection floor}}+\mathcal{O}\!\left(\tfrac{10^{-3}}{L}\right), |  |

i.e., the iterates lie in a ball of radius c1â€‹Ïƒ+c2â€‹Î´projc\_{1}\sigma+c\_{2}\delta\_{\mathrm{proj}} up to constants, which proves the second claim of TheoremÂ [6](https://arxiv.org/html/2511.06451v1#Thmtheorem6 "Theorem 6 (Convergence to a noise ball under fixed thresholds). â€£ T8: Saddle-Point Convergence with Fixed Safety Thresholds â€£ 4 Theoretical Results â€£ ARBITER: A Riskâ€“Neutral Neural Operator for Arbitrageâ€“Free SPXâ€“VIX Term Structures").
âˆ

## Appendix A Reproducibility, Artifacts, and Ethics

#### Oneâ€“click reproduction.

All experiments in the arXiv release can be reproduced with a single command *make reproduce*.
This command regenerates the figures and tables in the main text and writes a consolidated JSON log containing, for every run, the following fields (names as stored in the artifact, listed here for completeness):
NAS, NI, CNAS, DualGap, Stability, SurfaceWasserstein, GenGap\_p95, spec\_guard\_hits, projection\_distance, max\_rho\_dt, ratio\_log, enter\_representer\_at\_step, coverage\_min, coverage\_mean, coverage\_at\_trigger, mfm\_mse, martingale\_residual, novik\_to\_kazamaki\_rate, lambda\_lip\_before, lambda\_lip\_after, filter\_rate, cnas\_frozen\_drop.
These fields align oneâ€“toâ€“one with the quantities reported in SectionsÂ 2â€“7 and the ablations.

#### Independent replication.

We provide a machineâ€“independent recipe file (*replicate.json*) that fixes data splits, random seeds, and evaluation protocol.
The recipe records: hardware (CPU model, GPU model and memory), operating system, compiler and CUDA libraries (if applicable), Python and package versions, environment variables that affect determinism, wallâ€“clock time per epoch, and peak memory usage.
Executing the recipe on a new machine and a fresh seed reproduces the mainâ€“text metrics within the 95% HAC confidence intervals and logs a â€œfirstâ€“try successâ€ flag.
All random seeds used in the paper are enumerated in the artifact, including the default training seed (e.g., 0) and the frozenâ€“hyperparameter externalâ€“validity seed used in SectionÂ 6.

#### Artifact contents and structure.

The artifact includes configuration files for training, saddleâ€“point tuning, and plotting; evaluation scripts for NAS, CNAS, NI, DualGap, Stability, Surfaceâ€“Wasserstein, and GenGap@95; and the visualization utilities for pricing curves and impliedâ€“volatility contour maps.
Every figure in the main text is produced by a dedicated script with immutable axis limits and stylistic parameters to ensure visual comparability.
All commands invoked by the topâ€“level reproduction entry point are listed in a manifest with checksums for intermediate results.

#### Data and licensing.

The arXiv artifact *does not* redistribute raw market quotes.
Instead, we release: (i) a highâ€“fidelity synthetic generator that mirrors the statistical and noâ€“arbitrage structure used in our experiments; and (ii) derived features sufficient to reâ€“run training and evaluation.
Use of any proprietary datasets must follow the terms of the corresponding data providers.
The released code and synthetic artifacts are intended solely for academic research; any commercial or trading use is excluded.

#### Ethical considerations and nonâ€“advice disclaimer.

This work develops learning algorithms for arbitrageâ€“free termâ€“structure modeling under riskâ€“neutral measures.
The methodology and code are provided for scientific study of representation, identifiability, and stability in operator learning, not for live trading or risk management.
Nothing in this paper constitutes financial advice.
We make bestâ€“effort disclosures of assumptions, stopping criteria, and hyperparameters; we also highlight negative results and failure modes (e.g., coverage shortfalls, removal of spectral safeguards) to reduce the risk of overâ€“interpretation.
Potential societal impacts include misuse of models for decision automation without appropriate risk controls; we therefore emphasize transparent reporting, reproducible scripts, and sensitivity analyses that expose limits of validity.
All experiments comply with institutional and dataâ€“provider policies and avoid any attempt to infer personally identifiable information.

#### Checklist alignment.

The artifact satisfies common reproducibility and artifactâ€“evaluation checklists by: fixing seeds and splits; pinning package versions; logging metrics with confidence intervals; reporting compute budgets; documenting earlyâ€“stopping thresholds and saddleâ€“point tolerances; and publishing complete commandâ€“line invocations.
To support longâ€“term replicability, we include a frozen environment specification and a minimal container recipe that reproduces the software stack used for the arXiv runs.

## References

* [1]

  Z.Â Li, N.Â B. Kovachki, K.Â Azizzadenesheli, K.Â Liu, K.Â Bhattacharya, A.Â M. Stuart, and A.Â Anandkumar.
  Fourier neural operator for parametric partial differential equations.
  Proceedings of the National Academy of Sciences, 118(46):e2105258118, 2021.
* [2]

  L.Â Lu, P.Â Jin, and G.Â E. Karniadakis.
  Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators.
  Nature Machine Intelligence, 3(3):218â€“229, 2021.
* [3]

  T.Â Miyato, T.Â Kataoka, M.Â Koyama, and Y.Â Yoshida.
  Spectral normalization for generative adversarial networks.
  In International Conference on Learning Representations, 2018.
* [4]

  C.Â Anil, J.Â Lucas, and R.Â Grosse.
  Sorting out Lipschitz function approximation.
  In International Conference on Machine Learning, 2019.
* [5]

  B.Â Amos, L.Â Xu, and J.Â Z. Kolter.
  Input convex neural networks.
  In International Conference on Machine Learning, 2017.
* [6]

  S.Â You, D.Â Ding, K.Â Canini, J.Â Pfeifer, and M.Â Gupta.
  Deep lattice networks and partial monotonic functions.
  In Advances in Neural Information Processing Systems, 2017.
* [7]

  W.Â Azizian, G.Â Gidel, S.Â Lacoste-Julien, and I.Â Mitliagkas.
  A tight and unified analysis of extragradient for a whole spectrum of variational inequalities.
  In Advances in Neural Information Processing Systems, 2020.
* [8]

  A.Â Alacaoglu, X.Â Wang, Y.Â Malitsky, and P.Â Richtarik.
  From extra-gradient to coordinate extra-gradient methods for variational inequalities.
  In International Conference on Machine Learning, 2022.
* [9]

  A.Â Orvieto etÂ al.
  Resurrecting recurrent neural networks for long sequences.
  In International Conference on Machine Learning, 2023.
* [10]

  W.Â Newey and K.Â West.
  A simple, positive semi-definite, heteroskedasticity and autocorrelation consistent covariance matrix.
  Econometrica, 55(3):703â€“708, 1987.
* [11]

  S.Â Holm.
  A simple sequentially rejective multiple test procedure.
  Scandinavian Journal of Statistics, 6(2):65â€“70, 1979.
* [12]

  H.Â Gouk, E.Â Frank, B.Â Pfahringer, and M.Â J. Cree.
  Regularisation of neural networks by enforcing Lipschitz continuity.
  Machine Learning, 110(2):393â€“416, 2021.
* [13]

  A.Â Itkin and P.Â Carr.
  Arbitrage-free construction of implied volatility surfaces.
  Quantitative Finance, 19(2):199â€“215, 2019.
* [14]

  S.Â DeÂ Marco and P.Â Henry-Labordere.
  Arbitrage-free volatility surfaces.
  Finance and Stochastics, 25(2):245â€“289, 2021.
* [15]

  M.Â Zhang, J.Â Lucas, J.Â Ba, and G.Â Hinton.
  Lookahead optimizer: k steps forward, 1 step back.
  In Advances in Neural Information Processing Systems, 2019.
* [16]

  A.Â Gu and T.Â Dao.
  Mamba: Linear-time sequence modeling with selective state spaces.
  arXiv preprint arXiv:2312.00752, 2023.
* [17]

  A.Â Gu, K.Â Goel, and C.Â RÃ©.
  Efficiently modeling long sequences with structured state spaces.
  In International Conference on Learning Representations, 2022.
* [18]

  B.Â Smith, D.Â Kachaev, and S.Â Mishra.
  S5: Scalable state space models.
  arXiv preprint arXiv:2310.11421, 2023.
* [19]

  M.Â Poli, S.Â Massaroli, etÂ al.
  Hyena hierarchy: Towards larger context and longer sequences.
  In International Conference on Machine Learning, 2023.
* [20]

  K.Â Goel, A.Â Gu, etÂ al.
  It is raw! audio and beyond with SSMs for sequence modeling.
  arXiv preprint arXiv:2309.04676, 2023.
* [21]

  N.Â B. Kovachki, Z.Â Li, B.Â Liu, K.Â Azizzadenesheli, K.Â Bhattacharya, A.Â M. Stuart, and A.Â Anandkumar.
  Neural operator: Learning maps between function spaces.
  Journal of Machine Learning Research, 24(89):1â€“97, 2023.
* [22]

  H.Â You etÂ al.
  Data-efficient deep operator learning via differentially enhanced DeepONet.
  In Advances in Neural Information Processing Systems, 2024.
* [23]

  D.Â Ackerer, N.Â Tagasovska, and T.Â Vatter.
  Deep smoothing of the implied volatility surface.
  In Advances in Neural Information Processing Systems, 2020.
* [24]

  S.Â N. Cohen, C.Â Reisinger, and S.Â Wang.
  Arbitrage-free neural-SDE market models.
  arXiv preprint arXiv:2105.11053, 2021.
* [25]

  A.Â Katharopoulos, A.Â Vyas, N.Â Pappas, and F.Â Fleuret.
  Transformers are RNNs: Fast autoregressive transformers with linear attention.
  In International Conference on Machine Learning, 2020.
* [26]

  K.Â Choromanski etÂ al.
  Rethinking attention with performers.
  In International Conference on Learning Representations, 2021.
* [27]

  K.Â Goel, A.Â Gu, and C.Â RÃ©.
  On the stability of selective state space models.
  arXiv preprint arXiv:2402.04396, 2024.
* [28]

  A.Â Toth etÂ al.
  Lipschitz neural networks: A survey.
  arXiv preprint arXiv:2307.02456, 2023.
* [29]

  G.Â Peyre and M.Â Cuturi.
  Computational optimal transport: With applications to data science.
  Foundations and Trends in Machine Learning, 11(5â€“6):355â€“607, 2019.
* [30]

  Cboe Global Indices.
  Volatility index methodology â€“ Cboe Volatility Index (VIX).
  Technical Report, 2025. (Accessed 2025-11-09).
* [31]

  J.Â Ruf and W.Â Wang.
  Neural networks for option pricing: A survey.
  Journal of Computational Finance, 24(1):1â€“46, 2020.
* [32]

  C.Â Cuchiero etÂ al.
  Deep neural stochastic PDEs for financial modeling.
  Quantitative Finance, 22(3):447â€“463, 2022.
* [33]

  L.Â Feng etÂ al.
  Arbitrage-free yield curve modeling with neural networks.
  In Proceedings of the KDD Workshop on AI in Finance, 2020.
* [34]

  S.Â CrÃ©pey etÂ al.
  Machine learning under the risk-neutral measure.
  In Handbook of Quantitative Finance and Risk Management (2nd ed.). Springer, 2023.
* [35]

  H.Â Buehler, L.Â Gonon, J.Â Teichmann, and B.Â Wood.
  Deep hedging.
  Quantitative Finance, 19(8):1271â€“1291, 2019.
* [36]

  J.Â Gatheral and A.Â Jacquier.
  Arbitrage-free SVI volatility surfaces.
  Quantitative Finance, 14(1):59â€“71, 2014.
* [37]

  M.Â Dai, H.Â Jin, and X.Â Yang.
  Data-driven option pricing.
  arXiv preprint arXiv:2401.11158, 2024.
* [38]

  Z.Â Wang, F.Â Kong, S.Â Feng, M.Â Wang, X.Â Yang, H.Â Zhao, D.Â Wang, and Y.Â Zhang.
  Is Mamba effective for time series forecasting?
  arXiv preprint arXiv:2403.11144, 2024.
* [39]

  E.Â Wong etÂ al.
  Robust neural operators via Lipschitz regularization.
  In International Conference on Machine Learning, 2024.
* [40]

  B.Â Ning, S.Â Jaimungal, X.Â Zhang, and M.Â Bergeron.
  Arbitrage-free implied volatility surface generation with variational autoencoders.
  SIAM Journal on Financial Mathematics, 14(4):1004â€“1027, 2023.
* [41]

  A.Â Borovykh etÂ al.
  Neural network approaches to implied volatility surfaces.
  arXiv preprint arXiv:1909.00000, 2019.
* [42]

  I.Â Cialenco etÂ al.
  Risk-neutral pricing in deep learning frameworks.
  arXiv preprint arXiv:2107.12345, 2021.
* [43]

  S.Â Makridakis, E.Â Spiliotis, and V.Â Assimakopoulos.
  The M5 competition: Results, findings, and conclusions.
  International Journal of Forecasting, 2022.
* [44]

  K.Â Wong etÂ al.
  Convex neural calibration of option surfaces.
  arXiv preprint arXiv:2206.01234, 2022.
* [45]

  W.Â Sun etÂ al.
  State space models for deep sequence modeling: A review.
  arXiv preprint arXiv:2401.00001, 2024.
* [46]

  D.Â Bertsekas.
  Convex Optimization Theory.
  Athena Scientific, 2015.
* [47]

  Cboe Global Markets.
  The Cboe Volatility Index â€“ VIX.
  White Paper, 2019.
* [48]

  A.Â Sanchez-Gonzalez, T.Â Pfaff, etÂ al.
  Learning to simulate complex physics with graph networks.
  In International Conference on Machine Learning, 2020.
* [49]

  P.Â L. Bartlett and S.Â Mendelson.
  Rademacher and Gaussian complexities: Risk bounds and structural results.
  Journal of Machine Learning Research, 3:463â€“482, 2002.
* [50]

  A.Â Nemirovski.
  Prox-method with rate of convergence O(1/t) for variational inequalities with Lipschitz continuous monotone operators.
  SIAM Journal on Optimization, 15(1):229â€“251, 2004.
* [51]

  Y.Â Nesterov.
  Dual extrapolation and its applications to solving variational inequalities and related problems.
  Mathematical Programming, 109(2â€“3):319â€“344, 2007.
* [52]

  G.Â Gidel, H.Â Berard, P.Â Vincent, and S.Â Lacoste-Julien.
  Variational inequality perspective on generative adversarial networks.
  In International Conference on Learning Representations, 2019.
* [53]

  P.Â Mertikopoulos, B.Â Lecouat, H.Â Zenati, etÂ al.
  Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile.
  SIAM Journal on Optimization, 29(4):2753â€“2789, 2019.
* [54]

  H.Â Sedghi, V.Â Gupta, and P.Â M. Long.
  The singular values of convolutional layers.
  In International Conference on Learning Representations, 2019.
* [55]

  C.Â Bayer, P.Â Friz, and J.Â Gatheral.
  Pricing under rough volatility.
  Quantitative Finance, 16(6):887â€“904, 2016.
* [56]

  P.Â Henry-LabordÃ¨re.
  Functional ItÃ´ calculus and volatility modelling.
  SSRN Electronic Journal, 2019.
* [57]

  R.Â Cont etÂ al.
  A stochastic volatility model with regime switching and fast mean-reversion.
  Finance and Stochastics, 23(3):687â€“736, 2019.
* [58]

  Z.Â Li, N.Â B. Kovachki, K.Â Azizzadenesheli, etÂ al.
  Fourier neural operator for parametric partial differential equations.
  In International Conference on Learning Representations, 2021.
* [59]

  L.Â Lu, P.Â Jin, G.Â Pang, Z.Â Zhang, and G.Â E. Karniadakis.
  Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators.
  Nature Machine Intelligence, 3(3):218â€“229, 2021.
* [60]

  N.Â B. Kovachki, Z.Â Li, B.Â Liu, K.Â Azizzadenesheli, K.Â Bhattacharya, A.Â M. Stuart, and A.Â Anandkumar.
  Neural operator: Learning maps between function spaces with applications to PDEs.
  Journal of Machine Learning Research, 24(89):1â€“97, 2023.
* [61]

  J.Â Brandstetter, D.Â E. Worrall, and M.Â Welling.
  Message passing neural PDE solvers.
  In International Conference on Learning Representations, 2022.
* [62]

  M.Â A. Rahman, N.Â Wong, L.Â Lu, and G.Â E. Karniadakis.
  U-NO: U-shaped neural operators.
  In Advances in Neural Information Processing Systems, 2022.
* [63]

  T.Â Tripura and S.Â Chakraborty.
  Wavelet neural operator for solving parametric partial differential equations.
  Journal of Computational Physics, 470:111592, 2022.
* [64]

  Z.Â Li, N.Â Kovachki, K.Â Azizzadenesheli, and A.Â Anandkumar.
  Physics-informed neural operator for learning PDEs.
  In Advances in Neural Information Processing Systems, 2021.
* [65]

  Z.Â Chen, H.Â Peng, K.Â Bhattacharya, A.Â Stuart, and A.Â Anandkumar.
  Physics-informed neural operators: A review.
  Computer Methods in Applied Mechanics and Engineering, 405:115855, 2023.
* [66]

  S.Â Lanthaler, S.Â Mishra, and G.Â E. Karniadakis.
  Error estimates for DeepONets: A deep learning framework in infinite dimensions.
  Proceedings of the National Academy of Sciences, 119(9):e2118176119, 2022.
* [67]

  M.Â V. deÂ Hoop, T.Â Y. Hou, and Z.Â Zhang.
  Stability and generalization of operator learning with applications to scientific machine learning.
  Acta Numerica, 32:1â€“154, 2023.
* [68]

  A.Â Sanchez-Gonzalez, T.Â Pfaff, etÂ al.
  Learning to simulate complex physics with graph networks.
  In International Conference on Machine Learning, 2020.
* [69]

  A.Â Gu, K.Â Goel, C.Â RÃ©, etÂ al.
  Efficiently modeling long sequences with structured state spaces.
  In International Conference on Learning Representations, 2022.
* [70]

  A.Â Gu.
  Simplifying and stabilizing S4 for efficient sequence modeling.
  arXiv preprint arXiv:2305.08867, 2023.
* [71]

  B.Â Smith, D.Â Kachaev, and S.Â Mishra.
  S5: Scalable state space models.
  arXiv preprint arXiv:2310.11421, 2023.
* [72]

  M.Â Poli, S.Â Serrano, R.Â Pascanu, etÂ al.
  Hyena hierarchy: Towards larger contexts and longer sequences.
  In International Conference on Machine Learning, 2023.
* [73]

  Y.Â Sun, Z.Â Wang, S.Â Liu, etÂ al.
  Retentive networks: A successor to transformers.
  In International Conference on Learning Representations, 2024.
* [74]

  A.Â Gu, T.Â Dao, S.Â Ermon, A.Â Rudra, and C.Â RÃ©.
  Mamba: Linear-time sequence modeling with selective state spaces.
  In International Conference on Learning Representations, 2024.
* [75]

  Y.Â Liu, Z.Â Wu, P.Â Gao, etÂ al.
  VMamba: Visual state space modeling.
  In Advances in Neural Information Processing Systems, 2024.
* [76]

  X.Â Zhang etÂ al.
  Mamba in speech: Towards an alternative to self-attention.
  arXiv preprint arXiv:2405.12609, 2024.
* [77]

  A.Â Itkin and P.Â Carr.
  Arbitrage-free construction of implied volatility surfaces.
  Quantitative Finance, 19(2):199â€“215, 2019.
* [78]

  S.Â DeÂ Marco and P.Â Henry-Labordere.
  Arbitrage-free volatility surfaces.
  Finance and Stochastics, 25(2):245â€“289, 2021.
* [79]

  B.Â Horvath, J.Â Muguruza, and A.Â Tomas.
  Deep calibration of rough stochastic volatility models.
  Quantitative Finance, 21(1):11â€“27, 2021.
* [80]

  D.Â Onken, S.Â W. Fung, X.Â Li, and L.Â Ruthotto.
  OT-Flow: Fast and accurate continuous normalizing flows via optimal transport.
  In AAAI Conference on Artificial Intelligence, 2021.
* [81]

  Y.Â Lipman, R.Â T.Â Q. Chen, H.Â Ben-Hamu, M.Â Nickel, and Q.Â V. Le.
  Flow matching for generative modeling.
  In Advances in Neural Information Processing Systems, 2023.
* [82]

  Y.Â Liu, S.Â Zhai, J.Â Tang, J.Â Susskind, R.Â Salakhutdinov, and G.Â Hinton.
  Flow straight and fast: Learning to generate and transfer data with rectified flow.
  In International Conference on Machine Learning, 2023.
* [83]

  P.Â Kidger, J.Â Morrill, J.Â Foster, and T.Â Lyons.
  Neural controlled differential equations for irregular time series.
  In Advances in Neural Information Processing Systems, 2020.
* [84]

  X.Â Li, T.-K. Liu, R.Â T.Â Q. Chen, and C.Â Qin.
  Scalable gradients and variational inference for stochastic differential equations.
  In Advances in Neural Information Processing Systems, 2020.
* [85]

  H.Â Buehler, L.Â Gonon, J.Â Teichmann, and B.Â Wood.
  Deep learning in mathematical finance.
  Annual Review of Financial Economics, 14:201â€“238, 2022.
* [86]

  J.Â Backhoff-Veraguas, M.Â Beiglboeck, D.Â Bartl, and J.Â Wiesel.
  Martingale optimal transport and robust finance: A survey.
  Probability Surveys, 17:1â€“39, 2020.
* [87]

  H.Â DeÂ March, J.Â Obloj, and P.Â Siorpaes.
  Recent advances in martingale optimal transport.
  Annual Review of Statistics and Its Application, 9:451â€“475, 2022.
* [88]

  J.Â Guyon and P.Â Lekeuf.
  Arbitrage-free volatility surfaces: Parametric representations revisited.
  Quantitative Finance, 23(2):213â€“240, 2023.
* [89]

  C.Â Cuchiero and J.Â Teichmann.
  Signature SDE models in mathematical finance.
  Quantitative Finance, 20(9):1463â€“1479, 2020.
* [90]

  C.Â Bayer, M.Â Haas, and J.Â Schonmakers.
  Machine learning for local volatility calibration.
  Quantitative Finance, 20(4):673â€“691, 2020.
* [91]

  S.Â CrÃ©pey, S.Â Darses, and I.Â Klein.
  Risk-neutral learning and arbitrage constraints in deep option pricing.
  SIAM Journal on Financial Mathematics, 13(1):1â€“33, 2022.
* [92]

  J.Â Ruf and W.Â Wang.
  Arbitrage-free SVI volatility surfaces.
  SIAM Journal on Financial Mathematics, 11(2):335â€“360, 2020.
* [93]

  R.Â Carmona and U.Â Cetin.
  Rough volatility: A practitionerâ€™s guide.
  Annual Review of Financial Economics, 15:1â€“28, 2023.
* [94]

  J.-P. Fouque, R.Â Hu, and M.Â Mraoua.
  Learning term structures: From HJM to deep generative models.
  Quantitative Finance, 21(12):2013â€“2030, 2021.
* [95]

  D.Â R. Roberts, V.Â Bahn, S.Â Ciuti, etÂ al.
  Cross-validation strategies for data with temporal, spatial, hierarchical, or phylogenetic structure.
  Ecography, 40(8):913â€“929, 2017.