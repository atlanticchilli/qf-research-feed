---
authors:
- Panagiotis G. Papaioannou
- Athanassios N. Yannacopoulos
doc_id: arxiv:2511.05030v2
family_id: arxiv:2511.05030
is_current: true
taxonomy:
  alpha_families: []
  asset_classes: []
  horizons: []
  themes: []
title: 'The Shape of Markets: Machine learning modeling and Prediction Using 2-Manifold
  Geometries'
url_abs: http://arxiv.org/abs/2511.05030v2
url_html: https://arxiv.org/html/2511.05030v2
venue: arXiv q-fin
version: 2
year: 2025
---


Panagiotis G. Papaioannou
â€ƒâ€ƒ
Athanassios N. Yannacopoulos

(Received: date / Accepted: date)

###### Abstract

We introduce a Geometry-Informed Model for financial forecasting by embedding high-dimensional market data onto constant-curvature 2-manifolds. Guided by the uniformization theorem,Thurston1997, we model market dynamics as Brownian motion on spherical (S2S^{2}), Euclidean (R2R^{2}), and hyperbolic (H2H^{2}) geometries. We further include the torus (Tâ€‹Â²T\texttwosuperior), a compact, flat manifold admissible as a quotient space of the Euclidean plane-â€”anticipating its relevance for capturing cyclical dynamics,doCarmoCurves. Manifold learning techniques infer the latent curvature from financial data, revealing the torus as the best-performing geometry. We interpret this result through a macroeconomic lens: the torusâ€™s circular dimensions align with endogenous cycles in output, interest rates, and inflation described by IS-LM theory Hicks1937. Our findings demonstrate the value of integrating differential geometry with data-driven inference for financial modeling,LopezDePrado2018AFML; CapponiLehalle2023.

## 1 Introduction

Financial markets are characterized by their complex, non-linear behaviors, posing significant challenges for modeling and prediction. While traditional approaches to market analysis rely heavily on statistical tools and Euclidean assumptions, these methods often fail to account for deeper geometric structures inherent in the data. A novel perspective can be gained by incorporating concepts from differential geometry, particularly the classification of 2-manifolds into three geometries: spherical, Euclidean, and hyperbolic.Thurston1997

The geometrization theorem, which underpins this classification, provides a foundational framework for understanding how spaces can be decomposed into simpler geometric components,Perelman2002. This insight, central to modern differential geometry, suggests that many complex systems, including financial markets, may exhibit characteristics aligning with one or more of these geometric types. As in physics and network science, the widespread application of geometric methods and their potential in financial market analysis is actively explored under active research,CapponiLehalle2023; SimonianFabozzi2019; LopezDePrado2018AFML; LopezDePrado2020MLAM; NoguerAlonso2021FinEAS. Recent studies explore the application of manifold learning techniques to financial market analysis and time series forecasting. These methods aim to extract low-dimensional representations of complex, high-dimensional data, revealing intrinsic structures and patterns (Y2017Nonlinear; G2018Nonlinear; Y2020Manifold). In Jansen2023 several trading practice applications and concepts are outlined, as well. Manifold learning approaches have been used for early warning systems in financial markets (Y2017Nonlinear; G2018Nonlinear), phase space reconstruction of financial time series (Y2014Manifold), and visualization of market states (Y2020Manifold). Researchers have also developed novel algorithms, such as information metric-based manifold learning (Y2017Nonlinear) and kernel entropy manifold learning (Y2014kernel), to improve the accuracy of financial analysis and prediction. Beyond finance, manifold learning techniques have been applied to various time series analysis tasks, including electroencephalography signal analysis (P2018Multivariate) and forecasting of high-dimensional time series (P2021Time), demonstrating their versatility in capturing complex dynamics across different domains.Perelman2002

Brownian motion, a stochastic process traditionally studied in Euclidean spaces, is a powerful tool for modeling random dynamics. Extending Brownian motion to spherical and hyperbolic geometries (Hsu2002; IkedaWatanabe1989) enables the representation of processes influenced by curvature and global topological features. Studies have explored diffusion on spheres (Gomez2021geometrical; M2000Brownian) and hyperbolic spaces (L2007Hyperbolic, Hsu2002StochasticAO), as well as fractional Brownian motion in both geometries (J2005Spherical). The effects of curvature on Brownian motion have been investigated using Riemann normal coordinates (Castro-Villarreal, 2010) and the Smoluchowski equation (Pavel2014Intrinsic). Research has shown that positive curvature slows diffusion, while negative curvature accelerates it (Pavel2010Brownian). The concept of Brownian motion has been generalized to metric spaces of constant curvature (S1981Brownian). Additionally, studies have examined the hydrodynamics of curved membranes and their impact on particulate mobility (Henle2010Hydrodynamics), providing insights into diffusion on biological structures like vesicles and membrane tethers.Hsu2002; IkedaWatanabe1989

Financial markets exhibit complex, nonlinear dynamics that may be better understood through non-Euclidean geometric frameworks (E2006Geometry; Ilinski1999Gauge;Ilinski2001; Emami2021; KellerRessel2021). These markets can be modeled using projective geometry, fiber bundles, and fractal geometry to capture their intricate structures and behaviors (E2006Geometry; Ilinski1999Gauge; B2004MIS; Lipton2001FX; Lipton2018SelectedWorks). Researchers have developed methods to incorporate non-Euclidean geometric information into filtering and machine learning algorithms, improving their accuracy in financial applications (Anastasis2017Geometric). Network geometry measures, such as discrete Ricci curvatures, can be used to analyze market instability and systemic risk (Samal2021Network). The concept of â€dark volatility,â€ a hidden factor influencing market behavior, has been explored using Einstein warped product manifolds (Pincak2023possible). These geometric approaches provide powerful tools for modeling asset behavior, forecasting volatility, and evaluating investment risks with greater precision (B2004MIS; Humphrey2011Financial).(Ilinski2001; KellerRessel2021; Emami2021)

This paper introduces a new methodology that leverages the interplay between geometry, stochastic processes, and machine learning to analyze and predict financial market behavior. The proposed approach consists of three steps:

1. 1.

   Brownian Motion Construction: Simulate Brownian motions within spherical, Euclidean, and hyperbolic geometries to represent market dynamics.
2. 2.

   Geometry Inference: Apply local gaussian curvature analysis, to infer the underlying geometry from observed trajectories.
3. 3.

   Market Prediction: Use the inferred geometry to inform predictions of future market behavior, leveraging the geometric context to enhance accuracy.

The key idea is to treat financial market dynamics as trajectories embedded in geometrically diverse spaces. This approach allows us to move beyond traditional Euclidean-based analyses, uncovering the â€shapeâ€ of market data and providing insights into its structural properties. For example, spherical geometry might correspond to cyclic or periodic market behaviors, Euclidean geometry to stable trends, and hyperbolic geometry to highly volatile and chaotic dynamics. By connecting geometric theory with practical market analysis, this work offers contributions in both theory and application. Theoretically, it bridges the abstract mathematical classification of 2-manifolds with stochastic processes and financial modeling. Practically, it provides a new tool for market prediction that is particularly well-suited for analyzing non-linear and non-stationary systems.

This study aims to demonstrate that the geometric classification of trajectories, combined with stochastic modeling and machine learning, can significantly improve the understanding and prediction of financial markets. The results highlight the power of geometry-informed models in capturing complex behaviors, offering new avenues for research and application in finance and beyond.

## 2 Methodology

##### Roadmap.

We first formalize Brownian motion constrained to embedded surfaces and derive explicit SDEs for the three geometries we use (Euclidean, spherical, hyperbolic), plus the torus via an intrinsic chart (Sec.Â [3.2](https://arxiv.org/html/2511.05030v2#S3.SS2 "3.2 Explicit SDEs by geometry â€£ 3 Geometry-Constrained Stochastic Dynamics: Explicit Forms â€£ The Shape of Markets: Machine learning modeling and Prediction Using 2-Manifold Geometries")). We then specify the logarithmic/exponential maps that move between the manifold and its tangent space (Sec.Â [3.4](https://arxiv.org/html/2511.05030v2#S3.SS4 "3.4 Logarithmic and exponential mappings â€£ 3 Geometry-Constrained Stochastic Dynamics: Explicit Forms â€£ The Shape of Markets: Machine learning modeling and Prediction Using 2-Manifold Geometries")), followed by data-driven estimation of manifold parameters (sphere radius; torus radii; hyperboloid axes) directly from the observed 3D path (Sec.Â [3.5](https://arxiv.org/html/2511.05030v2#S3.SS5 "3.5 Data-driven estimation of manifold parameters â€£ 3 Geometry-Constrained Stochastic Dynamics: Explicit Forms â€£ The Shape of Markets: Machine learning modeling and Prediction Using 2-Manifold Geometries")). Next, we describe our curvature-based geometry inference with a torus topological check (Sec.Â [3.6](https://arxiv.org/html/2511.05030v2#S3.SS6 "3.6 Local Gaussian Curvature Based Geometry Inference â€£ 3 Geometry-Constrained Stochastic Dynamics: Explicit Forms â€£ The Shape of Markets: Machine learning modeling and Prediction Using 2-Manifold Geometries")), and the forecasting pipeline: chart â†’\to log map â†’\to tangent-space PCA & time-series forecast â†’\to exponential map (Sec.Â [3.7](https://arxiv.org/html/2511.05030v2#S3.SS7 "3.7 Forecasting in manifold space and baseline comparison â€£ 3 Geometry-Constrained Stochastic Dynamics: Explicit Forms â€£ The Shape of Markets: Machine learning modeling and Prediction Using 2-Manifold Geometries")). We close with the explicit native-space baseline comparator and the translation from forecasts to volatility-scaled Profit and Loss (PnL) and portfolio construction (Sec.Â [3.9](https://arxiv.org/html/2511.05030v2#S3.SS9 "3.9 Real-Finance Data Pipeline: Expanding PCA, Eigenportfolios, and Forecasting â€£ 3 Geometry-Constrained Stochastic Dynamics: Explicit Forms â€£ The Shape of Markets: Machine learning modeling and Prediction Using 2-Manifold Geometries")). Simulation scenarios and additional implementation details are summarized in the Appendix to keep the section compact.

## 3 Geometry-Constrained Stochastic Dynamics: Explicit Forms

### 3.1 Ambient formulation and curvature drift

Let MâŠ‚â„3M\subset\mathbb{R}^{3} be a C2C^{2} embedded surface (2-manifold) with unit normal nâ€‹(x)n(x) at xâˆˆMx\in M and tangent projector

|  |  |  |
| --- | --- | --- |
|  | Pâ€‹(x)=Iâˆ’nâ€‹(x)â€‹nâ€‹(x)âŠ¤âˆˆâ„3Ã—3.P(x)\;=\;I-n(x)\,n(x)^{\top}\in\mathbb{R}^{3\times 3}. |  |

Brownian motion constrained to MM is the Stratonovich SDE (Hsu2002)

|  |  |  |  |
| --- | --- | --- | --- |
|  | dâ€‹Xt=Pâ€‹(Xt)âˆ˜dâ€‹Bt,dX\_{t}\;=\;P(X\_{t})\circ dB\_{t}, |  | (1) |

where BtB\_{t} is standard Brownian motion in â„3\mathbb{R}^{3}. In Ito form this becomes (Hsu2002)

|  |  |  |  |
| --- | --- | --- | --- |
|  | dâ€‹Xt=Pâ€‹(Xt)â€‹dâ€‹Btâˆ’12â€‹Hâ€‹(Xt)â€‹dâ€‹t,dX\_{t}\;=\;P(X\_{t})\,dB\_{t}\;-\;\frac{1}{2}\,H(X\_{t})\,dt, |  | (2) |

where Hâ€‹(x)H(x) is the *mean curvature vector*, normal to MM, that enforces the constraint (heuristically, âˆ’12â€‹H-\tfrac{1}{2}H pulls the path back onto MM). For an *implicit* surface M={x:Ï•â€‹(x)=0}M=\{x:\phi(x)=0\} with âˆ‡Ï•â€‹(x)â‰ 0\nabla\phi(x)\neq 0, we use

|  |  |  |
| --- | --- | --- |
|  | nâ€‹(x)=âˆ‡Ï•â€‹(x)â€–âˆ‡Ï•â€‹(x)â€–,Pâ€‹(x)=Iâˆ’nâ€‹(x)â€‹nâ€‹(x)âŠ¤,n(x)\;=\;\frac{\nabla\phi(x)}{\|\nabla\phi(x)\|},\qquad P(x)\;=\;I-n(x)n(x)^{\top}, |  |

and compute Hâ€‹(x)H(x) either analytically (when available) or via the implementation used in our simulator (AppendixÂ [B](https://arxiv.org/html/2511.05030v2#A2 "Appendix B Implementation details for SDEs and curvature terms â€£ The Shape of Markets: Machine learning modeling and Prediction Using 2-Manifold Geometries")).

### 3.2 Explicit SDEs by geometry

##### Euclidean (â„3\mathbb{R}^{3})(unconstained ambient).

Trivial case (no constraint). With coordinates Xt=(Xt(1),Xt(2),Xt(3))X\_{t}=(X\_{t}^{(1)},X\_{t}^{(2)},X\_{t}^{(3)}):

|  |  |  |  |
| --- | --- | --- | --- |
|  | dâ€‹Xt=dâ€‹Bt.dX\_{t}\;=\;dB\_{t}. |  | (3) |

##### Sphere S2â€‹(R)S^{2}(R) (radius RR).

With Ï•â€‹(x)=â€–xâ€–2âˆ’R2\phi(x)=\|x\|^{2}-R^{2}, we have nâ€‹(x)=x/â€–xâ€–n(x)=x/\|x\| and

|  |  |  |
| --- | --- | --- |
|  | Pâ€‹(x)=Iâˆ’xâ€‹xâŠ¤R2,Hâ€‹(x)=2R2â€‹x.P(x)=I-\frac{x\,x^{\top}}{R^{2}},\qquad H(x)=\frac{2}{R^{2}}\,x. |  |

Hence the Ito SDE on S2â€‹(R)S^{2}(R) is (Hsu2002)

|  |  |  |  |
| --- | --- | --- | --- |
|  | dâ€‹Xt=(Iâˆ’Xtâ€‹XtâŠ¤R2)â€‹dâ€‹Btâˆ’1R2â€‹Xtâ€‹dâ€‹t.dX\_{t}\;=\;\Big(I-\frac{X\_{t}X\_{t}^{\top}}{R^{2}}\Big)\,dB\_{t}\;-\;\frac{1}{R^{2}}X\_{t}\,dt. |  | (4) |

For R=1R=1 this reduces to dâ€‹Xt=(Iâˆ’Xtâ€‹XtâŠ¤)â€‹dâ€‹Btâˆ’Xtâ€‹dâ€‹tdX\_{t}=(I-X\_{t}X\_{t}^{\top})dB\_{t}-X\_{t}\,dt.

##### Torus T2â€‹(R,r)T^{2}(R,r) (major radius RR, minor radius rr).

We work both in implicit embedding and in intrinsic angles.

* â€¢

  *Implicit embedding:* Ï•â€‹(x)=(Râˆ’x12+x22)2+x32âˆ’r2=0\displaystyle\phi(x)=\big(R-\sqrt{x\_{1}^{2}+x\_{2}^{2}}\big)^{2}+x\_{3}^{2}-r^{2}=0. Then

  |  |  |  |
  | --- | --- | --- |
  |  | nâ€‹(x)=âˆ‡Ï•â€‹(x)â€–âˆ‡Ï•â€‹(x)â€–,Pâ€‹(x)=Iâˆ’nâ€‹(x)â€‹nâ€‹(x)âŠ¤,n(x)=\frac{\nabla\phi(x)}{\|\nabla\phi(x)\|},\qquad P(x)=I-n(x)n(x)^{\top}, |  |

  and we evolve ([2](https://arxiv.org/html/2511.05030v2#S3.E2 "In 3.1 Ambient formulation and curvature drift â€£ 3 Geometry-Constrained Stochastic Dynamics: Explicit Forms â€£ The Shape of Markets: Machine learning modeling and Prediction Using 2-Manifold Geometries")) with this projector and the corresponding Hâ€‹(x)H(x) (AppendixÂ [B](https://arxiv.org/html/2511.05030v2#A2 "Appendix B Implementation details for SDEs and curvature terms â€£ The Shape of Markets: Machine learning modeling and Prediction Using 2-Manifold Geometries")).
* â€¢

  *Intrinsic chart (Î¸,Ï†)âˆˆ[0,2â€‹Ï€)2(\theta,\varphi)\in[0,2\pi)^{2}:*

  |  |  |  |
  | --- | --- | --- |
  |  | Î¨â€‹(Î¸,Ï†)=((R+râ€‹cosâ¡Ï†)â€‹cosâ¡Î¸(R+râ€‹cosâ¡Ï†)â€‹sinâ¡Î¸râ€‹sinâ¡Ï†).\Psi(\theta,\varphi)=\begin{pmatrix}(R+r\cos\varphi)\cos\theta\\[1.99997pt] (R+r\cos\varphi)\sin\theta\\[1.99997pt] r\sin\varphi\end{pmatrix}. |  |

  The metric is diagonal: gÎ¸â€‹Î¸=(R+râ€‹cosâ¡Ï†)2g\_{\theta\theta}=(R+r\cos\varphi)^{2}, gÏ†â€‹Ï†=r2g\_{\varphi\varphi}=r^{2},doCarmoRiemannian, hence gÎ¸â€‹Î¸=(R+râ€‹cosâ¡Ï†)âˆ’2g^{\theta\theta}=(R+r\cos\varphi)^{-2}, gÏ†â€‹Ï†=râˆ’2g^{\varphi\varphi}=r^{-2} and |g|=râ€‹(R+râ€‹cosâ¡Ï†)\sqrt{|g|}=r(R+r\cos\varphi). The Ito SDE realizing generator 12â€‹Î”g\tfrac{1}{2}\Delta\_{g} is

  |  |  |  |  |  |
  | --- | --- | --- | --- | --- |
  |  | dâ€‹Î¸t\displaystyle d\theta\_{t} | =1R+râ€‹cosâ¡Ï†tâ€‹dâ€‹Wt(1),\displaystyle=\frac{1}{R+r\cos\varphi\_{t}}\,dW\_{t}^{(1)}, |  | (5) |
  |  |  |  |  |  |
  | --- | --- | --- | --- | --- |
  |  | dâ€‹Ï†t\displaystyle d\varphi\_{t} | =1râ€‹dâ€‹Wt(2)âˆ’sinâ¡Ï†t2â€‹râ€‹(R+râ€‹cosâ¡Ï†t)â€‹dâ€‹t.\displaystyle=\frac{1}{r}\,dW\_{t}^{(2)}\;-\;\frac{\sin\varphi\_{t}}{2r\,(R+r\cos\varphi\_{t})}\,dt. |  | (6) |

  Cartesian positions follow by Xt=Î¨â€‹(Î¸t,Ï†t)X\_{t}=\Psi(\theta\_{t},\varphi\_{t}).

##### Hyperbolic H2H^{2} (hyperboloid model).

Use parameters (u,v)âˆˆâ„Ã—[0,2â€‹Ï€)(u,v)\in\mathbb{R}\times[0,2\pi) and constants a>0a>0, c>0c>0:

|  |  |  |
| --- | --- | --- |
|  | Î¦â€‹(u,v)=(aâ€‹coshâ¡uâ€‹cosâ¡vaâ€‹coshâ¡uâ€‹sinâ¡vcâ€‹sinhâ¡u).\Phi(u,v)=\begin{pmatrix}a\cosh u\cos v\\ a\cosh u\sin v\\ c\sinh u\end{pmatrix}. |  |

Then guâ€‹u=Eâ€‹(u)=a2â€‹sinh2â¡u+c2â€‹cosh2â¡ug\_{uu}=E(u)=a^{2}\sinh^{2}u+c^{2}\cosh^{2}u, gvâ€‹v=Gâ€‹(u)=a2â€‹cosh2â¡ug\_{vv}=G(u)=a^{2}\cosh^{2}u,doCarmoRiemannian, so guâ€‹u=1/Eg^{uu}=1/E, gvâ€‹v=1/Gg^{vv}=1/G, |g|=Eâ€‹(u)â€‹Gâ€‹(u)=aâ€‹coshâ¡uâ€‹Eâ€‹(u)\sqrt{|g|}=\sqrt{E(u)G(u)}=a\cosh u\,\sqrt{E(u)}. The Ito SDE (generator 12â€‹Î”g\tfrac{1}{2}\Delta\_{g}) reads

|  |  |  |  |
| --- | --- | --- | --- |
|  | dâ€‹ut\displaystyle du\_{t} | =1Eâ€‹(ut)â€‹dâ€‹Wt(1)+12â€‹âˆ‚u[lnâ¡(|g|â€‹guâ€‹u)]u=utâ€‹dâ€‹t\displaystyle=\frac{1}{\sqrt{E(u\_{t})}}\,dW\_{t}^{(1)}\;+\;\frac{1}{2}\,\partial\_{u}\!\left[\ln\!\big(\sqrt{|g|}\,g^{uu}\big)\right]\_{u=u\_{t}}\,dt |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | =1Eâ€‹(ut)â€‹dâ€‹Wt(1)+12â€‹(tanhâ¡utâˆ’Eâ€²â€‹(ut)2â€‹Eâ€‹(ut))â€‹dâ€‹t,\displaystyle=\frac{1}{\sqrt{E(u\_{t})}}\,dW\_{t}^{(1)}\;+\;\frac{1}{2}\left(\tanh u\_{t}-\frac{E^{\prime}(u\_{t})}{2E(u\_{t})}\right)dt, |  | (7) |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | dâ€‹vt\displaystyle dv\_{t} | =1Gâ€‹(ut)â€‹dâ€‹Wt(2)=1aâ€‹coshâ¡utâ€‹dâ€‹Wt(2),\displaystyle=\frac{1}{\sqrt{G(u\_{t})}}\,dW\_{t}^{(2)}\;=\;\frac{1}{a\cosh u\_{t}}\,dW\_{t}^{(2)}, |  | (8) |

with Eâ€²â€‹(u)=2â€‹(a2+c2)â€‹sinhâ¡uâ€‹coshâ¡uE^{\prime}(u)=2(a^{2}+c^{2})\sinh u\cosh u. Cartesian positions are Xt=Î¦â€‹(ut,vt)X\_{t}=\Phi(u\_{t},v\_{t}).

Remark (ambient implementation). We also implement ([2](https://arxiv.org/html/2511.05030v2#S3.E2 "In 3.1 Ambient formulation and curvature drift â€£ 3 Geometry-Constrained Stochastic Dynamics: Explicit Forms â€£ The Shape of Markets: Machine learning modeling and Prediction Using 2-Manifold Geometries")) directly in â„3\mathbb{R}^{3} using Pâ€‹(x)P(x) and a closed-form mean-curvature drift Hâ€‹(x)H(x) for each implicit surface; formulas above and the implementation are equivalent modulo time discretization (AppendixÂ [B](https://arxiv.org/html/2511.05030v2#A2 "Appendix B Implementation details for SDEs and curvature terms â€£ The Shape of Markets: Machine learning modeling and Prediction Using 2-Manifold Geometries")).

### 3.3 Nonlinear forecasting via machine learning regressors

Beyond the linear Vector AutoRegression (VAR) model, we also tested two nonlinear regressorsâ€”*Random Forests (RF)* and *Gaussian Process Regression (GP)*â€”to assess whether local nonlinearities in the tangent-space coordinates could enhance predictive accuracy.
Both approaches share the same input representation, where the target variable yty\_{t} (one coordinate of the tangent vector series) is regressed on its LL most recent lags:

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ’Ÿ={(ğ±t,yt)}t=LTâˆ’1,ğ±t=[ytâˆ’1,ytâˆ’2,â€¦,ytâˆ’L]âŠ¤.\mathcal{D}=\{(\mathbf{x}\_{t},y\_{t})\}\_{t=L}^{T-1},\qquad\mathbf{x}\_{t}=[y\_{t-1},y\_{t-2},\dots,y\_{t-L}]^{\top}. |  | (9) |

At each step, the models are trained on ğ’Ÿ\mathcal{D} and used to predict y^T+1=fâ€‹(ğ±T)\widehat{y}\_{T+1}=f(\mathbf{x}\_{T}), where fâ€‹(â‹…)f(\cdot) denotes the learned regression mapping.

##### Random Forest regression.

A Random Forest (breiman2001random) constructs an ensemble of BB regression trees {fbâ€‹(â‹…)}b=1B\{f\_{b}(\cdot)\}\_{b=1}^{B}, each trained on a bootstrap resample of ğ’Ÿ\mathcal{D} and a random subset of predictors.
The forecast corresponds to the ensemble mean:

|  |  |  |  |
| --- | --- | --- | --- |
|  | y^T+1(RF)=1Bâ€‹âˆ‘b=1Bfbâ€‹(ğ±T),\widehat{y}\_{T+1}^{(\mathrm{RF})}=\frac{1}{B}\sum\_{b=1}^{B}f\_{b}(\mathbf{x}\_{T}), |  | (10) |

where each fbf\_{b} partitions the lagged feature space â„L\mathbb{R}^{L} into piecewise-constant regions and averages the training observations within the corresponding leaf.
RF models approximate nonlinear relationships and capture variable interactions without assuming parametric structure.
In our implementation, we used B=100B=100 trees and a memory length of L=5L=5 lags for all tangent coordinates (xt,yt,zt)(x\_{t},y\_{t},z\_{t}).

##### Gaussian Process regression.

The Gaussian Process (GP) model treats the regression function fâ€‹(â‹…)f(\cdot) as a random function drawn from a Gaussian process prior:

|  |  |  |  |
| --- | --- | --- | --- |
|  | fâ€‹(â‹…)âˆ¼ğ’¢â€‹ğ’«â€‹(mâ€‹(â‹…),kâ€‹(â‹…,â‹…)),f(\cdot)\sim\mathcal{GP}(m(\cdot),k(\cdot,\cdot)), |  | (11) |

with mean function mâ€‹(â‹…)m(\cdot) (set to zero) and covariance kernel kâ€‹(ğ±,ğ±â€²)k(\mathbf{x},\mathbf{x}^{\prime}) encoding smoothness and correlation structure.
Given training data ğ’Ÿ\mathcal{D}, the posterior predictive distribution at a new input ğ±T\mathbf{x}\_{T} is Gaussian:

|  |  |  |  |
| --- | --- | --- | --- |
|  | pâ€‹(yT+1|ğ±T,ğ’Ÿ)=ğ’©â€‹(ğ¤TâŠ¤â€‹(K+Ïƒn2â€‹I)âˆ’1â€‹ğ²,kâ€‹(ğ±T,ğ±T)âˆ’ğ¤TâŠ¤â€‹(K+Ïƒn2â€‹I)âˆ’1â€‹ğ¤T),p(y\_{T+1}\,|\,\mathbf{x}\_{T},\mathcal{D})=\mathcal{N}\big(\mathbf{k}\_{T}^{\top}(K+\sigma\_{n}^{2}I)^{-1}\mathbf{y},\;k(\mathbf{x}\_{T},\mathbf{x}\_{T})-\mathbf{k}\_{T}^{\top}(K+\sigma\_{n}^{2}I)^{-1}\mathbf{k}\_{T}\big), |  | (12) |

where KK is the kernel matrix with [K]iâ€‹j=kâ€‹(ğ±i,ğ±j)[K]\_{ij}=k(\mathbf{x}\_{i},\mathbf{x}\_{j}), and ğ¤T=[kâ€‹(ğ±1,ğ±T),â€¦,kâ€‹(ğ±N,ğ±T)]âŠ¤\mathbf{k}\_{T}=[k(\mathbf{x}\_{1},\mathbf{x}\_{T}),\dots,k(\mathbf{x}\_{N},\mathbf{x}\_{T})]^{\top}.
The mean of this posterior gives the forecast:

|  |  |  |  |
| --- | --- | --- | --- |
|  | y^T+1(GP)=ğ¤TâŠ¤â€‹(K+Ïƒn2â€‹I)âˆ’1â€‹ğ².\widehat{y}\_{T+1}^{(\mathrm{GP})}=\mathbf{k}\_{T}^{\top}(K+\sigma\_{n}^{2}I)^{-1}\mathbf{y}. |  | (13) |

We used a MatÃ©rn kernel with an additive constant term,

|  |  |  |  |
| --- | --- | --- | --- |
|  | kâ€‹(ğ±,ğ±â€²)=kMaternâ€‹(ğ±,ğ±â€²)+kconstâ€‹(ğ±,ğ±â€²),k(\mathbf{x},\mathbf{x}^{\prime})=k\_{\mathrm{Matern}}(\mathbf{x},\mathbf{x}^{\prime})+k\_{\mathrm{const}}(\mathbf{x},\mathbf{x}^{\prime}), |  | (14) |

which balances smooth local trends with global level shifts, providing flexibility for financial data where both slow drifts and abrupt changes can occur.

##### Forecast integration.

In the forecasting pipeline, both RF and GP models operate on the tangent-space coordinates and provide one-step-ahead predictions v^t+1\widehat{v}\_{t+1}, which are then lifted back to the manifold via the exponential map,

|  |  |  |  |
| --- | --- | --- | --- |
|  | x^t+1=expÎ¼^â¡(v^t+1),\widehat{x}\_{t+1}=\exp\_{\widehat{\mu}}(\widehat{v}\_{t+1}), |  | (15) |

thus preserving the geometric structure of the original process. For our analysis, we used a rolling window framework for both ML methods, using 25 observations (trading days) for training and predicting one-step ahead points.

### 3.4 Logarithmic and exponential mappings

Let Txâ€‹MT\_{x}M denote the tangent plane at xâˆˆMx\in M.Pennec2018; doCarmoRiemannian

##### Sphere S2â€‹(R)S^{2}(R).

For unit vectors Î¼,pâˆˆS2â€‹(R)\mu,p\in S^{2}(R) define Î¸=arccosâ¡(Î¼â‹…pR2)âˆˆ[0,Ï€]\theta=\arccos\!\big(\tfrac{\mu\cdot p}{R^{2}}\big)\in[0,\pi]. Then

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | logÎ¼â¡(p)\displaystyle\log\_{\mu}(p) | ={ğŸ,p=Î¼,Î¸sinâ¡Î¸â€‹Î Î¼â€‹p,pâ‰ Î¼,\displaystyle=\begin{cases}\mathbf{0},&p=\mu,\\[3.50006pt] \dfrac{\theta}{\sin\theta}\;\Pi\_{\mu}\,p,&p\neq\mu,\end{cases} |  | (16) |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | expÎ¼â¡(v)\displaystyle\exp\_{\mu}(v) | =cosâ¡(â€–vâ€–/R)â€‹Î¼+Râ€‹sinâ¡(â€–vâ€–/R)â€‹vâ€–vâ€–,vâˆˆTÎ¼â€‹S2â€‹(R).\displaystyle=\cos\!\big(\|v\|/R\big)\,\mu\;+\;R\,\sin\!\big(\|v\|/R\big)\,\dfrac{v}{\|v\|},\qquad v\in T\_{\mu}S^{2}(R). |  | (17) |

See Pennec2018; doCarmoRiemannian for closed forms.

##### Torus T2â€‹(R,r)T^{2}(R,r) (angle chart).doCarmoCurves

Use angles (Î¸,Ï†)âˆˆ[0,2â€‹Ï€)2(\theta,\varphi)\in[0,2\pi)^{2}. For base point (Î¸0,Ï†0)(\theta\_{0},\varphi\_{0}) and point (Î¸,Ï†)(\theta,\varphi),

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | log(Î¸0,Ï†0)â¡(Î¸,Ï†)\displaystyle\log\_{(\theta\_{0},\varphi\_{0})}(\theta,\varphi) | =(wrapâ€‹(Î¸âˆ’Î¸0),wrapâ€‹(Ï†âˆ’Ï†0)),\displaystyle=\big(\mathrm{wrap}(\theta-\theta\_{0}),\;\mathrm{wrap}(\varphi-\varphi\_{0})\big), |  | (18) |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | exp(Î¸0,Ï†0)â¡(Î”â€‹Î¸,Î”â€‹Ï†)\displaystyle\exp\_{(\theta\_{0},\varphi\_{0})}(\Delta\theta,\Delta\varphi) | =(Î¸0+Î”â€‹Î¸,Ï†0+Î”â€‹Ï†)mod2â€‹Ï€,\displaystyle=\big(\theta\_{0}+\Delta\theta,\;\varphi\_{0}+\Delta\varphi\big)\bmod 2\pi, |  | (19) |

where wrapâ€‹(Î±)=((Î±+Ï€)mod2â€‹Ï€)âˆ’Ï€\mathrm{wrap}(\alpha)=((\alpha+\pi)\bmod 2\pi)-\pi is the [âˆ’Ï€,Ï€)[-\pi,\pi) minimizer. Mapping to â„3\mathbb{R}^{3} uses Î¨\Psi above.

##### Hyperbolic (hyperboloid chart).doCarmoRiemannian

In coordinates (u,v)(u,v) with metric diagâ€‹(Eâ€‹(u),Gâ€‹(u))\mathrm{diag}(E(u),G(u)),

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | log(u0,v0)â¡(u,v)\displaystyle\log\_{(u\_{0},v\_{0})}(u,v) | =(uâˆ’u0,wrapâ€‹(vâˆ’v0)),\displaystyle=\big(u-u\_{0},\;\mathrm{wrap}(v-v\_{0})\big), |  | (20) |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | exp(u0,v0)â¡(Î”â€‹u,Î”â€‹v)\displaystyle\exp\_{(u\_{0},v\_{0})}(\Delta u,\Delta v) | =(u0+Î”â€‹u,v0+Î”â€‹vmod2â€‹Ï€),\displaystyle=\big(u\_{0}+\Delta u,\;v\_{0}+\Delta v\bmod 2\pi\big), |  | (21) |

then lift to â„3\mathbb{R}^{3} via Î¦\Phi. For small moves these coincide with the Riemannian log/exp in these orthogonal coordinates.

##### Karcher mean and tangent PCA.Karcher1977; Pennec2018

Given points {xi}âŠ‚M\{x\_{i}\}\subset M, we compute the intrinsic (Karcher) mean Î¼â‹†\mu^{\star} by iterating Î¼â†expÎ¼â¡(1Nâ€‹âˆ‘ilogÎ¼â¡(xi))\mu\leftarrow\exp\_{\mu}\!\big(\tfrac{1}{N}\sum\_{i}\log\_{\mu}(x\_{i})\big) until convergence. We then project data to TÎ¼â‹†â€‹MT\_{\mu^{\star}}M with logÎ¼â‹†â¡(xi)\log\_{\mu^{\star}}(x\_{i}) and perform PCA there (this is the â€œtangent-PCAâ€ used in our forecasting pipeline).

### 3.5 Data-driven estimation of manifold parameters

A key ingredient of our pipeline is that the geometry is not treated as fixed; instead, its *intrinsic parameters* are inferred on the fly from the observed 3D path {xt=(xt,yt,zt)}tâ‰¤tâ‹†\{x\_{t}=(x\_{t},y\_{t},z\_{t})\}\_{t\leq t^{\star}}. This subsection formalizes the estimation steps we implement for the sphere, torus and oneâ€“sheeted hyperboloid, matching the code used in our experiments.CazalsPouget2005; CohenSteiner2006

##### Notation.

Let Mâ€‹(Ï‘)M(\vartheta) denote a parametric surface embedded in â„3\mathbb{R}^{3} with parameter vector Ï‘\vartheta (e.g., radius RR on the sphere; major/minor radii (R,r)(R,r) on the torus; semiâ€“axes (a,b,c)(a,b,c) on the hyperboloid). Given a stream {xt}\{x\_{t}\} up to time tâ‹†t^{\star}, we estimate Ï‘^tâ‹†\widehat{\vartheta}\_{t^{\star}} and then work in the corresponding chart for mapping, forecasting and lifting. Throughout, angles are unwrapped *mod* 2â€‹Ï€2\pi using the shortestâ€“-arc convention to preserve continuity in the tangent space (seeÂ ([23](https://arxiv.org/html/2511.05030v2#S3.E23 "In Torus ğ‘‡Â²â¢(ğ‘…,ğ‘Ÿ): methodâ€“-of-â€“moments from toroidal coordinates â€£ 3.5 Data-driven estimation of manifold parameters â€£ 3 Geometry-Constrained Stochastic Dynamics: Explicit Forms â€£ The Shape of Markets: Machine learning modeling and Prediction Using 2-Manifold Geometries")) below).

#### Sphere S2â€‹(R)S^{2}(R): Karcher mean and radius

For spherical segments, we first compute a *Karcher (FrÃ©chet) mean* Î¼^âˆˆS2\widehat{\mu}\in S^{2} of the points {xt/â€–xtâ€–}\{x\_{t}/\|x\_{t}\|\} by iterating Î¼â†expÎ¼â¡(1nâ€‹âˆ‘tlogÎ¼â¡(xt))\mu\leftarrow\exp\_{\mu}\!\big(\tfrac{1}{n}\sum\_{t}\log\_{\mu}(x\_{t})\big) until convergence;Karcher1977

An estimate of the radius follows from

|  |  |  |
| --- | --- | --- |
|  | R^tâ‹†=argâ€‹minR>0â€‹âˆ‘tâ‰¤tâ‹†(â€–xtâ€–âˆ’R)2=1tâ‹†â€‹âˆ‘tâ‰¤tâ‹†â€–xtâ€–.\widehat{R}\_{t^{\star}}\;=\;\operatorname\*{arg\,min}\_{R>0}\,\sum\_{t\leq t^{\star}}\big(\|x\_{t}\|-R\big)^{2}\;=\;\frac{1}{t^{\star}}\sum\_{t\leq t^{\star}}\|x\_{t}\|. |  |

This R^\widehat{R} is needed since we wish to keep track of the physical scale;

#### Torus T2â€‹(R,r)T^{2}(R,r): methodâ€“-of-â€“moments from toroidal coordinates

We convert Cartesian observations to toroidal angles (Î¸t,Ï•t)(\theta\_{t},\phi\_{t}) and the auxiliary radial quantity Ït=xt2+yt2\rho\_{t}=\sqrt{x\_{t}^{2}+y\_{t}^{2}} : a standard torus (major radius RR, minor radius rr) in the *Reinhardt* parameterization is

|  |  |  |
| --- | --- | --- |
|  | x=(R+râ€‹cosâ¡Ï•)â€‹cosâ¡Î¸,y=(R+râ€‹cosâ¡Ï•)â€‹sinâ¡Î¸,z=râ€‹sinâ¡Ï•.x=(R+r\cos\phi)\cos\theta,\quad y=(R+r\cos\phi)\sin\theta,\quad z=r\sin\phi. |  |

From Ït=xt2+yt2=R+râ€‹cosâ¡Ï•t\rho\_{t}=\sqrt{x\_{t}^{2}+y\_{t}^{2}}=R+r\cos\phi\_{t} it follows that
ğ”¼â€‹[Ït]=R,Varâ¡(Ït)=r22â€‹ifÂ â€‹Ï•tâˆ¼Unif.\mathbb{E}[\rho\_{t}]=R,\quad\operatorname{Var}(\rho\_{t})=\tfrac{r^{2}}{2}\ \text{if }\phi\_{t}\sim\text{Unif}.
We adopt a simple method of moments on the *minor angle* and its cosine (normalized units):

|  |  |  |  |
| --- | --- | --- | --- |
|  | R^=1tâ‹†â€‹âˆ‘tâ‰¤tâ‹†cosâ¡Ï•t,r^=1tâ‹†â€‹âˆ‘tâ‰¤tâ‹†(cosâ¡Ï•tâˆ’R^)2,\widehat{R}\;=\;\frac{1}{t^{\star}}\sum\_{t\leq t^{\star}}\cos\phi\_{t},\qquad\widehat{r}\;=\;\sqrt{\frac{1}{t^{\star}}\sum\_{t\leq t^{\star}}\big(\cos\phi\_{t}-\widehat{R}\big)^{2}}, |  | (22) |

In practice, for stability we use R^\widehat{R} fromÂ ([22](https://arxiv.org/html/2511.05030v2#S3.E22 "In Torus ğ‘‡Â²â¢(ğ‘…,ğ‘Ÿ): methodâ€“-of-â€“moments from toroidal coordinates â€£ 3.5 Data-driven estimation of manifold parameters â€£ 3 Geometry-Constrained Stochastic Dynamics: Explicit Forms â€£ The Shape of Markets: Machine learning modeling and Prediction Using 2-Manifold Geometries")) while the *instantaneous* tube radius is proxied by the latest Ïtâ‹†\rho\_{t^{\star}}, which helps track slow deformations of the tube thickness in nonâ€“stationary segments. doCarmoCurves

Tangent space steps use the 2â€‹Ï€2\pi wrapped *shortest arc* differences

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î”â€‹Î¸t=((Î¸tâˆ’Î¸0+Ï€)mod2â€‹Ï€)âˆ’Ï€,Î”â€‹Ï•t=((Ï•tâˆ’Ï•0+Ï€)mod2â€‹Ï€)âˆ’Ï€,\Delta\theta\_{t}\;=\;((\theta\_{t}-\theta\_{0}+\pi)\bmod 2\pi)-\pi,\qquad\Delta\phi\_{t}\;=\;((\phi\_{t}-\phi\_{0}+\pi)\bmod 2\pi)-\pi, |  | (23) |

#### One sheeted hyperboloid x2/a2+y2/b2âˆ’z2/c2=1x^{2}/a^{2}+y^{2}/b^{2}-z^{2}/c^{2}=1: nonlinear Least Squares

For hyperbolic patches we assume the oneâ€“sheeted quadratic model

|  |  |  |
| --- | --- | --- |
|  | Fâ€‹(x,y,z;a,b,c)=x2a2+y2b2âˆ’z2c2âˆ’1=â€„0.F(x,y,z;a,b,c)\;=\;\frac{x^{2}}{a^{2}}+\frac{y^{2}}{b^{2}}-\frac{z^{2}}{c^{2}}-1\;=\;0. |  |

Given {xt}\{x\_{t}\}, we fit (a,b,c)(a,b,c) by nonlinear least squares

|  |  |  |  |
| --- | --- | --- | --- |
|  | (a^,b^,c^)âˆˆargâ¡mina,b,c>0â€‹âˆ‘tâ‰¤tâ‹†Fâ€‹(xt,yt,zt;a,b,c)2,(\widehat{a},\widehat{b},\widehat{c})\;\in\;\arg\min\_{a,b,c>0}\ \sum\_{t\leq t^{\star}}F(x\_{t},y\_{t},z\_{t};a,b,c)^{2}, |  | (24) |

just before transforming the stream to hyperbolic coordinates and entering the log, forecast, exp cycle

The coordinate chart uses (u,v)(u,v) with
x=aâ€‹coshâ¡uâ€‹cosâ¡v,y=bâ€‹coshâ¡uâ€‹sinâ¡v,z=câ€‹sinhâ¡ux=a\cosh u\cos v,\ y=b\cosh u\sin v,\ z=c\sinh u,
and the forward/backward conversions are handled by implementing the numerically stable choice u=arcsinhâ¡(z/c)u=\operatorname{arcsinh}(z/c) and v=atan2â¡(y/b,x/a)v=\operatorname{atan2}(y/b,x/a)).

On this chart, the log map again uses shortest arc wrapping on vv and simple differencing on uu; the exp map reâ€“adds those increments around the base point. doCarmoRiemannian

#### Chart transition inside the â€™motion mixingâ€™ framework

At each step tâ‹†t^{\star}, the pipeline (i) picks/updates the active geometry Mâ€‹(Ï‘^tâ‹†)M(\widehat{\vartheta}\_{t^{\star}}), (ii) *reâ€“charts* the recent window to the intrinsic coordinates of MM, (iii) applies *tangent space* PCA/forecasting, and (iv) lifts the predicted tangent vector back to â„3\mathbb{R}^{3} through the geometry exp map and its embedding.

Concretely:

Sphere (S).â€„
xtâ†¦x~t:=xtâ€–xtâ€–âˆˆS2â€‹(R)â†’logÎ¼^vtâˆˆTÎ¼^â€‹S2â€‹(R)â†’PGA + forecastv^t+1â†’expÎ¼^x^t+1.x\_{t}\;\mapsto\;\tilde{x}\_{t}:=\frac{x\_{t}}{\|x\_{t}\|}\in S^{2}(R)\;\xrightarrow{\ \log\_{\widehat{\mu}}\ }\;v\_{t}\in T\_{\widehat{\mu}}S^{2}(R)\;\xrightarrow{\ \text{PGA + forecast}\ }\;\widehat{v}\_{t+1}\;\xrightarrow{\ \exp\_{\widehat{\mu}}\ }\;\widehat{x}\_{t+1}.

Torus (T).â€„
xtâ†¦(Î¸t,Ï•t)â†’log(Î¸0,Ï•0)vtâ†’PGA + forecastv^t+1â†’exp(Î¸0,Ï•0)(Î¸^,Ï•^)â†’Î¨(R^,r^)x^t+1,x\_{t}\;\mapsto\;(\theta\_{t},\phi\_{t})\;\xrightarrow{\ \log\_{(\theta\_{0},\phi\_{0})}\ }\;v\_{t}\;\xrightarrow{\ \text{PGA + forecast}\ }\;\widehat{v}\_{t+1}\;\xrightarrow{\ \exp\_{(\theta\_{0},\phi\_{0})}\ }\;(\widehat{\theta},\widehat{\phi})\;\xrightarrow{\ \Psi\_{(\widehat{R},\widehat{r})}\ }\;\widehat{x}\_{t+1},
where Î¨(R,r):S1Ã—S1â†’â„3\Psi\_{(R,r)}:S^{1}\times S^{1}\to\mathbb{R}^{3} is the standard torus embedding.

Hyperbolic (H).â€„
xtâ†¦(ut,vt)â†’log(u0,v0)wtâ†’PGA + forecastw^t+1â†’exp(u0,v0)(u^,v^)â†’Î¦(a^,b^,c^)x^t+1,x\_{t}\;\mapsto\;(u\_{t},v\_{t})\;\xrightarrow{\ \log\_{(u\_{0},v\_{0})}\ }\;w\_{t}\;\xrightarrow{\ \text{PGA + forecast}\ }\;\widehat{w}\_{t+1}\;\xrightarrow{\ \exp\_{(u\_{0},v\_{0})}\ }\;(\widehat{u},\widehat{v})\;\xrightarrow{\ \Phi\_{(\widehat{a},\widehat{b},\widehat{c})}\ }\;\widehat{x}\_{t+1},
where Î¦(a,b,c)\Phi\_{(a,b,c)} embeds the hyperbolic chart (e.g., hyperboloid model) into â„3\mathbb{R}^{3}.

By explicitly reâ€“estimating (R^)(\widehat{R}) or (R^,r)(\widehat{R},r) or (a^,b^,c^)(\widehat{a},\widehat{b},\widehat{c}) at each step (or on an expanding/rolling schedule), we avoid confusing the *learned geometry* with the auxiliary linear reduction used inside the tangent space,Pennec2018. The code performs these updates right before building the logâ€“-tangent cloud and forecasting the principal coordinates.

#### Numerical considerations

* â€¢

  Angle wrapping. All angular differences use the shortestâ€“arc ruleÂ ([23](https://arxiv.org/html/2511.05030v2#S3.E23 "In Torus ğ‘‡Â²â¢(ğ‘…,ğ‘Ÿ): methodâ€“-of-â€“moments from toroidal coordinates â€£ 3.5 Data-driven estimation of manifold parameters â€£ 3 Geometry-Constrained Stochastic Dynamics: Explicit Forms â€£ The Shape of Markets: Machine learning modeling and Prediction Using 2-Manifold Geometries")) to keep tangent vectors small and numerically stable on compact directions (torus S1Ã—S1S^{1}{\times}S^{1} and the angular coordinate vv on the hyperboloid).
* â€¢

  Stability at small steps. Spherical log/exp guard against sinâ¡Î¸â‰ˆ0\sin\theta\!\approx\!0 and â€–vâ€–â‰ˆ0\|v\|\!\approx\!0 (returning zeros or the base point), preventing blowâ€“ups when points are nearly aligned.
* â€¢

  Robust lifting. The torus and hyperboloid use explicit closedâ€“form embeddings for *lifting* the predicted coordinates back to â„3\mathbb{R}^{3}, ensuring that x^t+1\widehat{x}\_{t+1} lies on Mâ€‹(Ï‘^tâ‹†)M(\widehat{\vartheta}\_{t^{\star}}) by construction.

### 3.6 Local Gaussian Curvature Based Geometry Inference

We infer the latent geometry directly from the data via a *local differentialâ€“geometric fit* combined with *topological validation*. The pipeline operates on a 3D trajectory
Xt=(xt,yt,zt)âˆˆâ„3X\_{t}=(x\_{t},y\_{t},z\_{t})\in\mathbb{R}^{3} obtained either from simulation or from real data after a 3D embedding (e.g., expanding-window PCA of returns). We show the dynamics of the simulations and from the real data in [1](https://arxiv.org/html/2511.05030v2#S3.F1 "Figure 1 â€£ 3.6 Local Gaussian Curvature Based Geometry Inference â€£ 3 Geometry-Constrained Stochastic Dynamics: Explicit Forms â€£ The Shape of Markets: Machine learning modeling and Prediction Using 2-Manifold Geometries").

![Refer to caption](images/CartesianPathDFMain_1.png)

(a) Scenario 1

![Refer to caption](images/CartesianPathDFMain_2.png)

(b) Scenario 2

![Refer to caption](images/CartesianPathDFMain_3.png)

(c) Scenario 3

![Refer to caption](images/CartesianPathDFMain_4.png)

(d) Scenario 4

![Refer to caption](images/CartesianPathDFMain_5.png)

(e) Scenario 5

![Refer to caption](images/CartesianPathDFMain_6.png)

(f) Scenario 6

![Refer to caption](images/CartesianPathDFMain_7.png)

(g) Scenario 7

![Refer to caption](images/CartesianPathDFMain_PCA3D.png)

(h) Financial data 3D PCA embedding

Figure 1: Simulated Brownian-motion scenarios: time-series panels (aâ€“g) and the corresponding 3D PCA embedding (h).

At each time tt we consider a window ğ’²t={Xs:sâˆˆ[t0,t]}\mathcal{W}\_{t}=\{X\_{s}:s\in[t\_{0},t]\} ( expanding) with |ğ’²t|â‰¥m0|\mathcal{W}\_{t}|\geq m\_{0} points and compute a local quadratic Monge patch fit,CazalsPouget2005; CohenSteiner2006,

|  |  |  |  |
| --- | --- | --- | --- |
|  | z=aâ€‹x2+bâ€‹xâ€‹y+câ€‹y2+dâ€‹x+eâ€‹y+foverÂ â€‹ğ’²t,z\;=\;ax^{2}+bxy+cy^{2}+dx+ey+f\quad\text{over }\mathcal{W}\_{t}, |  | (25) |

by weighted least squares. Writing A=[x2,xâ€‹y,y2,x,y,â€‰1]A=\big[x^{2},\,xy,\,y^{2},\,x,\,y,\,\mathbf{1}\big] and Î²=(a,b,c,d,e,f)âŠ¤\beta=(a,b,c,d,e,f)^{\top},
the estimator is

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î²^t=argâ¡minÎ²â¡â€–Wtâ€‹(Aâ€‹Î²âˆ’z)â€–22=(AâŠ¤â€‹Wt2â€‹A)âˆ’1â€‹AâŠ¤â€‹Wt2â€‹z,\hat{\beta}\_{t}\;=\;\arg\min\_{\beta}\;\|W\_{t}(A\beta-z)\|\_{2}^{2}\;=\;(A^{\top}W\_{t}^{2}A)^{-1}A^{\top}W\_{t}^{2}z, |  | (26) |

where Wt=diagâ€‹(w1,â€¦,w|ğ’²t|)W\_{t}=\mathrm{diag}(\,w\_{1},\dots,w\_{|\mathcal{W}\_{t}|}\,) is optional exponential weighting
(*recent* samples weighted more), with wiâˆeâˆ’Î±â€‹(tâˆ’si)w\_{i}\propto e^{-\alpha(t-s\_{i})}.
Given a^t,b^t,c^t\hat{a}\_{t},\hat{b}\_{t},\hat{c}\_{t}, the *local Gaussian curvature* at time tt is (Monge gauge approximation),CazalsPouget2005,

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ktâ‰ˆ4â€‹a^tâ€‹c^tâˆ’b^t2(1+4â€‹a^t2+b^t2+4â€‹c^t2)2.K\_{t}\;\approx\;\frac{4\hat{a}\_{t}\hat{c}\_{t}-\hat{b}\_{t}^{2}}{\big(1+4\hat{a}\_{t}^{2}+\hat{b}\_{t}^{2}+4\hat{c}\_{t}^{2}\big)^{2}}. |  | (27) |

Operationally, we pre-smooth (x,y,z)(x,y,z) (short moving average) and compute KtK\_{t} on an expanding or rolling window with a minimal sample size m0m\_{0} (see Appendix for defaults).

Towards validating that the method approximates satisfactorily the underlying Gaussian curvature, Fig.Â [2](https://arxiv.org/html/2511.05030v2#S3.F2 "Figure 2 â€£ 3.6 Local Gaussian Curvature Based Geometry Inference â€£ 3 Geometry-Constrained Stochastic Dynamics: Explicit Forms â€£ The Shape of Markets: Machine learning modeling and Prediction Using 2-Manifold Geometries") recovers the expected signatures on benchmarksâ€”-K>0K>0 on S2S^{2}, mixed KK on T2T^{2}, and K<0K<0 on H2H^{2}â€”and yields plausible, intermittent curvature on the finance path; â€œuniformly sampledâ€ means draws from each surfaceâ€™s Riemannian (area) measure (density âˆdâ€‹A\propto dA).doCarmoRiemannian

|  |  |  |  |
| --- | --- | --- | --- |
|  | SphereÂ â€‹S2â€‹(R):\displaystyle\textbf{Sphere }S^{2}(R): | xâ€‹(Î¸,Ï†)=(Râ€‹cosâ¡Î¸â€‹sinâ¡Ï†,Râ€‹sinâ¡Î¸â€‹sinâ¡Ï†,Râ€‹cosâ¡Ï†),\displaystyle x(\theta,\varphi)=\big(R\cos\theta\,\sin\varphi,\;R\sin\theta\,\sin\varphi,\;R\cos\varphi\big), |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | (Î¸,Ï†)âˆˆ[0,2â€‹Ï€)Ã—[0,Ï€],dâ€‹A=R2â€‹sinâ¡Ï†â€‹dâ€‹Î¸â€‹dâ€‹Ï†.\displaystyle(\theta,\varphi)\in[0,2\pi)\times[0,\pi],\hskip 18.49988ptdA=R^{2}\sin\varphi\,d\theta\,d\varphi. |  |

|  |  |  |  |
| --- | --- | --- | --- |
|  | TorusÂ â€‹T2â€‹(R,r):\displaystyle\textbf{Torus }T^{2}(R,r): | xâ€‹(Î¸,Ï•)=((R+râ€‹cosâ¡Ï•)â€‹cosâ¡Î¸,(R+râ€‹cosâ¡Ï•)â€‹sinâ¡Î¸,râ€‹sinâ¡Ï•),\displaystyle x(\theta,\phi)=\big((R+r\cos\phi)\cos\theta,\;(R+r\cos\phi)\sin\theta,\;r\sin\phi\big), |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | (Î¸,Ï•)âˆˆ[0,2â€‹Ï€)2,dâ€‹A=râ€‹(R+râ€‹cosâ¡Ï•)â€‹dâ€‹Î¸â€‹dâ€‹Ï•.\displaystyle(\theta,\phi)\in[0,2\pi)^{2},\hskip 18.49988ptdA=r\,(R+r\cos\phi)\,d\theta\,d\phi. |  |

|  |  |  |  |
| --- | --- | --- | --- |
|  | Hyperboloid:\displaystyle\textbf{Hyperboloid}: | xâ€‹(u,v)=(Aâ€‹coshâ¡uâ€‹cosâ¡v,Aâ€‹coshâ¡uâ€‹sinâ¡v,câ€‹sinhâ¡u),\displaystyle x(u,v)=\big(A\cosh u\cos v,\;A\cosh u\sin v,\;c\sinh u\big), |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | vâˆˆ[0,2â€‹Ï€),uâˆˆ[umin,umax],dâ€‹A=Aâ€‹coshâ¡uâ€‹A2â€‹sinh2â¡u+c2â€‹cosh2â¡uâ€‹dâ€‹uâ€‹dâ€‹v.\displaystyle v\in[0,2\pi),\;u\in[u\_{\min},u\_{\max}],\qquad dA=A\cosh u\,\sqrt{A^{2}\sinh^{2}u+c^{2}\cosh^{2}u}\,du\,dv. |  |

Practical uniform sampling:
(i) S2S^{2}: sample U,Vâˆ¼Unifâ€‹(0,1)U,V\!\sim\!\mathrm{Unif}(0,1), set Î¸=2â€‹Ï€â€‹U\theta=2\pi U, cosâ¡Ï†=1âˆ’2â€‹V\cos\varphi=1-2V.
(ii) T2T^{2}: sample Î¸âˆ¼Unifâ€‹[0,2â€‹Ï€)\theta\!\sim\!\mathrm{Unif}[0,2\pi) and Ï•\phi by rejection with target âˆR+râ€‹cosâ¡Ï•\propto R+r\cos\phi.
(iii) Hyperboloid: sample vâˆ¼Unifâ€‹[0,2â€‹Ï€)v\!\sim\!\mathrm{Unif}[0,2\pi) and uu on [umin,umax][u\_{\min},u\_{\max}] with target proportional to the uuâ€“marginal of dâ€‹AdA.

![Refer to caption](images/curvature_plot_sphere.png)

(a) Sphere S2S^{2} (uniform sampling)

![Refer to caption](images/curvature_plot_torus.png)

(b) Torus T2T^{2} (uniform sampling)

![Refer to caption](images/curvature_plot_hyperboloid.png)

(c) Hyperboloid H2H^{2} (uniform sampling)

![Refer to caption](images/curvature_plot_ExpWindow_PCA_3D.png)

(d) Finance path (expanding-window PCA 3D)

Figure 2: Local Gaussian curvature estimates KK across benchmark shapes and the real-data embedded path. The benchmarks provide sign/scale references; the finance panel shows intermittent, regime-like curvature bursts.

##### Geometry decision from KtK\_{t}.

Since K>0K>0 on spheres, K<0K<0 on hyperbolic patches, and Kâ‰ˆ0K\approx 0 in flat regions, we use thresholds 0<Îº+â‰ª10<\kappa\_{+}\ll 1, 0<Îºâˆ’â‰ª10<\kappa\_{-}\ll 1 to define

|  |  |  |
| --- | --- | --- |
|  | Sphere-like if,Â â€‹Ktâ‰¥Îº+,\text{\bf Sphere-like if, }K\_{t}\geq\kappa\_{+},\quad |  |

|  |  |  |
| --- | --- | --- |
|  | Hyperbolic-like if,Â â€‹Ktâ‰¤âˆ’Îºâˆ’,\text{\bf Hyperbolic-like if, }K\_{t}\leq-\kappa\_{-},\quad |  |

|  |  |  |
| --- | --- | --- |
|  | Flat (Euclidean-like) if,Â â€‹|Kt|<minâ¡(Îº+,Îºâˆ’).\text{\bf Flat (Euclidean-like) if, }|K\_{t}|<\min(\kappa\_{+},\kappa\_{-}). |  |

However, tori are *mixed-curvature* surfaces (both signs occur), so we complement KtK\_{t} with a topological test.

##### Topological validation via persistent homology (Torus test).

We form a *Takens embedding*, Takens1981, over the window (Takens1981) ğ’²t\mathcal{W}\_{t}:

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ’¯t=[Xs,Xsâˆ’Ï„,â€¦,Xsâˆ’(mâˆ’1)â€‹Ï„]âˆˆâ„3â€‹m,\mathcal{T}\_{t}\;=\;\big[\,X\_{s},\,X\_{s-\tau},\,\dots,\,X\_{s-(m-1)\tau}\,\big]\in\mathbb{R}^{3m}, |  | (28) |

with delay Ï„\tau and embedding dimension mm. We compute Vietoris Rips persistent homology of ğ’¯t\mathcal{T}\_{t} and count persistent 1-cycles (Betti 11 features),EdelsbrunnerHarer2010; Ghrist2008. A torus satisfies Î²1=2\beta\_{1}=2 (and Î²2=1\beta\_{2}=1),EdelsbrunnerHarer2010, so we flag

|  |  |  |
| --- | --- | --- |
|  | Torus-like ifÂ â€‹#â€‹{H1â€‹Â lifetimes>Ïµ}â‰¥2\text{\bf Torus-like if }\ \#\{\text{H}\_{1}\text{ lifetimes}>\epsilon\}\ \geq 2 |  |

|  |  |  |
| --- | --- | --- |
|  | (optionally: andÂ â€‹#â€‹{H2â€‹Â lifetimes>Ïµ}â‰¥1)\quad(\text{optionally: and }\#\{\text{H}\_{2}\text{ lifetimes}>\epsilon\}\geq 1) |  |

with persistence threshold Ïµ\epsilon calibrated to the scale of the point cloud,Gidea2018; Ismail2022; Arvanitis2024. *In the current context, we implement the basic Î²1\beta\_{1}â€“based detector over sliding windows*.

##### Final geometry decision rule.

For each window:

|  |  |  |
| --- | --- | --- |
|  | If Torus-like: geometryÂ =T2;\text{If Torus-like: geometry }=T^{2}; |  |

|  |  |  |
| --- | --- | --- |
|  | else ifÂ â€‹Ktâ‰¥Îº+:S2;\text{else if }K\_{t}\geq\kappa\_{+}:S^{2}; |  |

|  |  |  |
| --- | --- | --- |
|  | else ifÂ â€‹Ktâ‰¤âˆ’Îºâˆ’:H2;\text{else if }K\_{t}\leq-\kappa\_{-}:H^{2}; |  |

|  |  |  |
| --- | --- | --- |
|  | else: Euclidean. |  |

Notes.â€„
(i) We use expanding windows for stability on real data and rolling windows in stress tests;
(ii) exponential weighting (Î±>0\alpha>0) emphasizes recency in the quadratic fit;
(iii) the torus test can be run on the joint (x,y,z)(x,y,z) embedding or on the scalar curvature series KtK\_{t} (scalar Takens) with similar thresholds.

### 3.7 Forecasting in manifold space and baseline comparison

##### Manifold-aware forecasting

1. 1.

   *Regime inference:* Infer geometry using local gaussian curvature information - section [3.6](https://arxiv.org/html/2511.05030v2#S3.SS6 "3.6 Local Gaussian Curvature Based Geometry Inference â€£ 3 Geometry-Constrained Stochastic Dynamics: Explicit Forms â€£ The Shape of Markets: Machine learning modeling and Prediction Using 2-Manifold Geometries")
2. 2.

   *Tangent velocities:* Compute vt=Pâ€‹(Xtâˆ’1)â€‹(Xtâˆ’Xtâˆ’1)v\_{t}=P(X\_{t-1})\,(X\_{t}-X\_{t-1}) to approximate intrinsic velocity.
3. 3.

   *Tangent PCA:* Project vtv\_{t} on top-dd principal axes to obtain coefficients ctâˆˆâ„dc\_{t}\in\mathbb{R}^{d}.Jolliffe2002
4. 4.

   *Time-series models:* Fit VAR(pp), to {ct}\{c\_{t}\} and forecast c^t+1:t+h\hat{c}\_{t+1:t+h}. We use rolling window VAR(p=25p=25) in our application.
5. 5.

   *Lift back:* Reconstruct v^t+k\hat{v}\_{t+k} from c^t+k\hat{c}\_{t+k}, update X^t+k\hat{X}\_{t+k} on MM via expX^t+kâˆ’1â¡(v^t+k)\exp\_{\hat{X}\_{t+k-1}}(\hat{v}\_{t+k}) (or ambient step with projection), yielding path forecasts.

##### Baseline (Native-space) forecasting â€” explicit comparator.

We *explicitly compare* against a geometry-agnostic baseline that applies the same predictors *directly in the input space*: fit VAR to the raw â„3\mathbb{R}^{3} series {Xt}\{X\_{t}\} (or its first differences) without manifold embeddings or tangent/PCA steps, and produce X^t+1:t+hnative\hat{X}^{\text{native}}\_{t+1:t+h}. All training windows, horizons, and hyperparameters are matched to ensure a fair comparison. LopezDePrado2018AFML; Jansen2023; Jolliffe2002

Input time series
X1:Tâˆˆâ„3X\_{1:T}\in\mathbb{R}^{3}

Geometry inference
Curvature KtK\_{t} + PH torus flag

Tangent velocity
vt=Pâ€‹(Xtâˆ’1)â€‹(Xtâˆ’Xtâˆ’1)v\_{t}=P(X\_{t-1})(X\_{t}-X\_{t-1})

Tangent PCA
ctâˆˆâ„dc\_{t}\in\mathbb{R}^{d}

Forecasters
VAR / GP / RF on ctc\_{t}

Lift back
v^t+1â†’X^t+1\hat{v}\_{t+1}\!\rightarrow\!\hat{X}\_{t+1}

Allocation
Inverse-vol + tilt
Curvature gating
MM


Baseline (native space)
Inverse-vol and VAR/GP/RF directly on XtX\_{t}
No curvature/PH

Figure 3: Flow chart of the manifold-aware pipeline â€“ geometry (MM) via curvature KtK\_{t} and persistent homology â€“ with an explicit native-space baseline.

Sphere S2S^{2}Torus T2T^{2}Hyperbolic H2H^{2}Txâ€‹S2T\_{x}S^{2}Tyâ€‹T2T\_{y}T^{2}Tzâ€‹H2T\_{z}H^{2}logx\log\_{x}expx\exp\_{x}logy\log\_{y}expy\exp\_{y}logz\log\_{z}expz\exp\_{z}


Tangent PCA â‡’\;\Rightarrow\; VAR / GP / RF forecasts â‡’\;\Rightarrow\; Lift back to manifold


Figure 4: Manifold embedding (log), tangent-space forecasting, and lifting (exp). Labels are placed *inside* the geometry boxes.

![Refer to caption](images/figure_sphere_logexp_predict.png)


Figure 5: Sphere S2S^{2}: log map to tangent at Î¼\mu, prediction v^\hat{v}, and lift X^=expÎ¼â¡(v^)\hat{X}=\exp\_{\mu}(\hat{v}).

![Refer to caption](images/figure_torus_logexp_predict.png)


Figure 6: Torus T2T^{2}: log map, tangent-space prediction, and lifting back via expÎ¼\exp\_{\mu}.

![Refer to caption](images/figure_hyperbolic_logexp_predict.png)


Figure 7: Hyperbolic H2H^{2}: log map, tangent-space prediction, and lifting via expÎ¼\exp\_{\mu}.

### 3.8 Euclidean Null Control: Correlated Brownian Motions

To verify that our pipeline does *not* confuse linear factor structure (e.g., PCA geometry) with *intrinsic* manifold geometry, we run a Euclidean â€œnullâ€ experiment based on correlated Brownian motions. This control is designed to isolate what PCA can explain in a flat space and to demonstrate that our curvature/topology inference remains neutral (flat) when no curved geometry is present.

##### Construction (flat â„n\mathbb{R}^{n} model).

Fix nâ‰¥3n\geq 3, horizon TT, and an equicorrelation level Ïâˆˆ[0,1)\rho\in[0,1).
Let

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î£Ï=Ïâ€‹â€‰11âŠ¤+(1âˆ’Ï)â€‹In(PSD, full-rank forÂ â€‹Ï<1),\Sigma\_{\rho}\;=\;\rho\,\bm{1}\bm{1}^{\top}+(1-\rho)\,I\_{n}\qquad(\text{PSD, full-rank for }\rho<1), |  | (29) |

and let LL be a Cholesky factor of Î£Ï\Sigma\_{\rho}.
Generate i.i.d. standard normal innovations Ztâˆ¼ğ’©â€‹(0,In)Z\_{t}\sim\mathcal{N}(0,I\_{n}) and set

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î”â€‹Wt=Lâ€‹Ztâˆ¼ğ’©â€‹(0,Î£Ï),Wt=âˆ‘s=1tÎ”â€‹Ws,t=1,â€¦,T.\Delta W\_{t}\;=\;L\,Z\_{t}\;\sim\;\mathcal{N}(0,\Sigma\_{\rho}),\qquad W\_{t}\;=\;\sum\_{s=1}^{t}\Delta W\_{s},\quad t=1,\dots,T. |  | (30) |

Thus {Wt}\{W\_{t}\} is a *multivariate Brownian motion in flat â„n\mathbb{R}^{n}* with constant diffusion Î£Ï\Sigma\_{\rho} (no curvature, no manifold constraints).RevuzYor1999

##### What this controls for.

In the equicorrelation model, any structure is purely *linear correlation*. If one performs PCA on the cross-section,

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î»1=â€„1+(nâˆ’1)â€‹Ï,Î»2=â‹¯=Î»n=â€„1âˆ’Ï,\lambda\_{1}\;=\;1+(n-1)\rho,\qquad\lambda\_{2}=\cdots=\lambda\_{n}\;=\;1-\rho, |  | (31) |

revealing one dominant â€œmarketâ€ factor and nâˆ’1n{-}1 equal idiosyncratic directions. Projecting trajectories onto the top 2â€“3 PCs is a *linear* rotation/scale; it does *not* induce curvature.Jolliffe2002
Hence, if our method were merely â€œlearning PCAâ€™s geometry,â€ it would incorrectly report non-flat geometry here. The null control checks that it does not.

##### Embedding and diagnostics.

To match the rest of our pipeline, we form a 3D time series Xtâˆˆâ„3X\_{t}\in\mathbb{R}^{3} either by:
(i) selecting three coordinates of WtW\_{t}, or
(ii) projecting WtW\_{t} onto the first three PCs (purely linear compression).
On this 3D path we then compute:

1. 1.

   Local Gaussian curvature KtK\_{t} via a weighted quadratic Monge patch fit
   z=aâ€‹x2+bâ€‹xâ€‹y+câ€‹y2+dâ€‹x+eâ€‹y+fz=ax^{2}+bxy+cy^{2}+dx+ey+f on rolling/expanding windows (SectionÂ [4.2](https://arxiv.org/html/2511.05030v2#S4.SS2 "4.2 Curvature statistics and regime assignment (finance path) â€£ 4 Results â€£ The Shape of Markets: Machine learning modeling and Prediction Using 2-Manifold Geometries")).
2. 2.

   Topological torus test via Takens embedding and persistent homology
   (looking for two persistent H1H\_{1} cycles).

##### Expected outcome under the null.

Because the data live in an *affine* (flat) subspace:

* â€¢

  Ktâ‰ˆ0K\_{t}\approx 0 up to finite-sample noise. For any small thresholds Îº+,Îºâˆ’>0\kappa\_{+},\kappa\_{-}>0,

  |  |  |  |
  | --- | --- | --- |
  |  | â„™â€‹(|Kt|<minâ¡(Îº+,Îºâˆ’))â€‹is high,\mathbb{P}\big(|K\_{t}|<\min(\kappa\_{+},\kappa\_{-})\big)\;\text{is high,} |  |

  so the decision rule classifies as *Euclidean/flat*.
* â€¢

  The persistent homology *does not* show two long-lived 1-cycles, hence no torus flag.

##### Forecasting and allocation implications.

In a flat regime:

* â€¢

  Tangent-space PCA of increments and native-space modeling are *effectively equivalent* (both are linear).
* â€¢

  The curvature gating Î»t\lambda\_{t} satisfies Î»tâ‰ƒ1\lambda\_{t}\simeq 1 (no expansion/contraction).
* â€¢

  Portfolio weights collapse to the baseline inverse-vol rule (plus any small return tilt), with no systematic advantage to manifold-aware steps.

This Euclidean null control demonstrates that our procedure *does not* mistake PCAâ€™s linear factor structure for intrinsic manifold geometry. Curvature/topology estimators remain flat when the data-generating process is flat, and any gains observed in the main experiments arise from genuine nonlinear geometric structure rather than artifacts of linear dimension reduction.

### 3.9 Real-Finance Data Pipeline: Expanding PCA, Eigenportfolios, and Forecasting

We apply our methodology to a broad multi-asset universe (equities, sectors, rates, credit, commodities, volatility indices; full tickers in AppendixÂ [A](https://arxiv.org/html/2511.05030v2#A1 "Appendix A Data Universe and Pre-processing â€£ The Shape of Markets: Machine learning modeling and Prediction Using 2-Manifold Geometries")). Raw daily prices (Yahoo Finance) ; log-returns are formed and basic long-only (LO) and risk-parity (RP) benchmark series are computed for reference. The LO and RP benchmarks (the latter built via inverse-volatility scaling) are saved alongside the panel of returns for later comparison.

##### Expanding-window PCA and 3D embedding (eigenportfolios).

Let Rtâˆˆâ„NR\_{t}\in\mathbb{R}^{N} denote the cross-section of asset returns at time tt. For tâ‰¥t0t\geq t\_{0}, we standardize the history {R1,â€¦,Rt}\{R\_{1},\dots,R\_{t}\} and compute PCA loadings {u1,t,u2,t,u3,t}\{u\_{1,t},u\_{2,t},u\_{3,t}\} and eigenvalues {Î»1,t,Î»2,t,Î»3,t}\{\lambda\_{1,t},\lambda\_{2,t},\lambda\_{3,t}\} on an *expanding* window (Jolliffe2002; Avellaneda2022; LopezDePrado2020MLAM. To avoid look-ahead, eigenportfolio ii at time tt uses the *lagged* loadings:

|  |  |  |  |
| --- | --- | --- | --- |
|  | pi,t=ui,tâˆ’1âŠ¤â€‹Rt,i=1,2,3,p\_{i,t}\;=\;u\_{i,t-1}^{\top}R\_{t},\qquad i=1,2,3, |  | (32) |

so pi,tp\_{i,t} is the (out-of-sample) return of the iith eigenportfolio. Stacking the three principal sleeves yields a *3D embedded path* in cumulative form

|  |  |  |  |
| --- | --- | --- | --- |
|  | Xt=(âˆ‘sâ‰¤tp1,s,âˆ‘sâ‰¤tp2,s,âˆ‘sâ‰¤tp3,s)âŠ¤âˆˆâ„3,X\_{t}\;=\;\Big(\,\sum\_{s\leq t}p\_{1,s},\ \sum\_{s\leq t}p\_{2,s},\ \sum\_{s\leq t}p\_{3,s}\,\Big)^{\top}\in\mathbb{R}^{3}, |  | (33) |

which serves as the input trajectory for our geometry-aware predictor. We display the expanding window curvature estimation for the PCA embedded financial dataset in Figure [8](https://arxiv.org/html/2511.05030v2#S3.F8 "Figure 8 â€£ Expanding-window PCA and 3D embedding (eigenportfolios). â€£ 3.9 Real-Finance Data Pipeline: Expanding PCA, Eigenportfolios, and Forecasting â€£ 3 Geometry-Constrained Stochastic Dynamics: Explicit Forms â€£ The Shape of Markets: Machine learning modeling and Prediction Using 2-Manifold Geometries")

![Refer to caption](images/PCA_3D_Plot_and_Curvature.png)


Figure 8: PCA projections evolution (Upper panel) and curvature estimation (Lower panel) for the real financial dataset. xx is PC1 (1st eigenportfolio), yy is the PC2 (2nd eigenportfolio) and zz the PC3 projection (3rd eigenportfolio), respectively

##### Geometry signal on finance: local curvature on the PCA path.

On the 3D path XtX\_{t} we estimate the *local Gaussian curvature* KtK\_{t} by fitting quadratic Monge patches on rolling/expanding neighborhoods (SectionÂ [4.2](https://arxiv.org/html/2511.05030v2#S4.SS2 "4.2 Curvature statistics and regime assignment (finance path) â€£ 4 Results â€£ The Shape of Markets: Machine learning modeling and Prediction Using 2-Manifold Geometries")). This series feeds the allocation rules and the curvature-aware benchmarks reported later.

##### Forecasting on the embedded path.

Given X1:tX\_{1:t}, we forecast the next embedded point X^t+1\widehat{X}\_{t+1} either (i) *natively in the 3D Euclidean path* or (ii) *geometry-aware* by choosing a geometry Mâˆˆ{â„2,S2,T2,H2}M\in\{\mathbb{R}^{2},S^{2},T^{2},H^{2}\}, mapping to the tangent space via logÎ¼\log\_{\mu}, forecasting principal tangent coefficients (VAR / GP / RF), and lifting back via expÎ¼\exp\_{\mu} (Section [4.2](https://arxiv.org/html/2511.05030v2#S4.SS2 "4.2 Curvature statistics and regime assignment (finance path) â€£ 4 Results â€£ The Shape of Markets: Machine learning modeling and Prediction Using 2-Manifold Geometries")).

##### Translating forecasts to trading signals and PnL.Maillard2010; Roncalli2013

Let Î”â€‹Xt+1=Xt+1âˆ’Xt\Delta X\_{t+1}=X\_{t+1}-X\_{t} be realized 3D increments and Î”â€‹X^t+1=X^t+1âˆ’Xt\widehat{\Delta X}\_{t+1}=\widehat{X}\_{t+1}-X\_{t}. We form directional, volatility-scaled signals on each coordinate kâˆˆ{x,y,z}k\in\{x,y,z\} as

|  |  |  |  |
| --- | --- | --- | --- |
|  | sk,t+1=signâ¡(Î”â€‹X^k,t+1)Ïƒ^k,ts\_{k,t+1}\;=\;\frac{\operatorname{sign}(\widehat{\Delta X}\_{k,t+1})}{\widehat{\sigma}\_{k,t}} |  | (34) |

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î”â€‹XÂ¯k,t(500)=1500â€‹âˆ‘j=0499Î”â€‹Xk,tâˆ’j,Ïƒ^k,t(500)=1499â€‹âˆ‘j=0499(Î”â€‹Xk,tâˆ’jâˆ’Î”â€‹XÂ¯k,t(500))2.\bar{\Delta X}\_{k,t}^{(500)}=\frac{1}{500}\sum\_{j=0}^{499}\Delta X\_{k,t-j},\qquad\widehat{\sigma}\_{k,t}^{(500)}=\sqrt{\frac{1}{499}\sum\_{j=0}^{499}\!\left(\Delta X\_{k,t-j}-\bar{\Delta X}\_{k,t}^{(500)}\right)^{2}}\,. |  | (35) |

and computes coordinate PnL by pnlk,t+1=sk,t+1â€‹Î”â€‹Xk,t+1\,\mathrm{pnl}\_{k,t+1}=s\_{k,t+1}\,\Delta X\_{k,t+1}\,.
  
  
Annualized Sharpe ratios, Shâ€‹[]\mathrm{Sh}[], are reported as

|  |  |  |  |
| --- | --- | --- | --- |
|  | Shâ€‹[pnlk]=252â€‹pnlÂ¯kstdevâ€‹(pnlk),pnlTot,t=âˆ‘kâˆˆ{x,y,z}pnlk,t.\mathrm{Sh}[\mathrm{pnl}\_{k}]\;=\;\sqrt{252}\,\frac{\overline{\mathrm{pnl}}\_{k}}{\mathrm{stdev}(\mathrm{pnl}\_{k})}\,,\qquad\mathrm{pnl}\_{\text{Tot},t}\;=\;\sum\_{k\in\{x,y,z\}}\mathrm{pnl}\_{k,t}. |  | (36) |

### 3.10 Eigenvalue-Weighted Sleeves from *3D PCA Space* (Expanding SVD)

In the finance application we first build the *embedded 3D PCA path* of eigenportfolios
(SectionÂ [3.9](https://arxiv.org/html/2511.05030v2#S3.SS9 "3.9 Real-Finance Data Pipeline: Expanding PCA, Eigenportfolios, and Forecasting â€£ 3 Geometry-Constrained Stochastic Dynamics: Explicit Forms â€£ The Shape of Markets: Machine learning modeling and Prediction Using 2-Manifold Geometries")): for each date tt we have

|  |  |  |
| --- | --- | --- |
|  | Xt=(X1,t,X2,t,X3,t)âŠ¤âˆˆâ„3,X\_{t}\;=\;\big(X\_{1,t},\,X\_{2,t},\,X\_{3,t}\big)^{\top}\in\mathbb{R}^{3}, |  |

where the coordinates are the out-of-sample cumulative eigenportfolio sleeves (PC1, PC2, PC3). *Then*, within this 3D space, we compute an *expanding-window SVD/PCA* to obtain time-varying variance levels and use those *3D-space eigenvalues* to weight the sleeves,Jolliffe2002.

Let Î”â€‹Xs:=Xsâˆ’Xsâˆ’1\Delta X\_{s}:=X\_{s}-X\_{s-1} and fix an expanding window ğ’²t={1,â€¦,t}\mathcal{W}\_{t}=\{1,\dots,t\}.
Define the 3Ã—33\times 3 sample covariance on ğ’²t\mathcal{W}\_{t},

|  |  |  |
| --- | --- | --- |
|  | Î£t(3â€‹D)=1|ğ’²t|â€‹âˆ‘sâˆˆğ’²t(Î”â€‹Xsâˆ’Î”â€‹XÂ¯t)â€‹(Î”â€‹Xsâˆ’Î”â€‹XÂ¯t)âŠ¤,Î”â€‹XÂ¯t=1|ğ’²t|â€‹âˆ‘sâˆˆğ’²tÎ”â€‹Xs.\Sigma^{(3D)}\_{t}\;=\;\frac{1}{|\mathcal{W}\_{t}|}\sum\_{s\in\mathcal{W}\_{t}}\big(\Delta X\_{s}-\overline{\Delta X}\_{t}\big)\big(\Delta X\_{s}-\overline{\Delta X}\_{t}\big)^{\top},\qquad\overline{\Delta X}\_{t}=\frac{1}{|\mathcal{W}\_{t}|}\sum\_{s\in\mathcal{W}\_{t}}\Delta X\_{s}. |  |

Compute the spectral decomposition (equivalently, SVD of the 3Ã—|ğ’²t|3\times|\mathcal{W}\_{t}| matrix of Î”â€‹Xs\Delta X\_{s})

|  |  |  |
| --- | --- | --- |
|  | Î£t(3â€‹D)=Qtâ€‹Î›tâ€‹QtâŠ¤,Î›t=diagâ€‹(Î»1â€‹(t),Î»2â€‹(t),Î»3â€‹(t)),Î»1â€‹(t)â‰¥Î»2â€‹(t)â‰¥Î»3â€‹(t)â‰¥0.\Sigma^{(3D)}\_{t}\;=\;Q\_{t}\,\Lambda\_{t}\,Q\_{t}^{\top},\qquad\Lambda\_{t}=\mathrm{diag}\!\big(\lambda\_{1}(t),\lambda\_{2}(t),\lambda\_{3}(t)\big),\;\;\lambda\_{1}(t)\!\geq\!\lambda\_{2}(t)\!\geq\!\lambda\_{3}(t)\!\geq\!0. |  |

These Î»iâ€‹(t)\lambda\_{i}(t) are the *expanding-window eigenvalues in the 3D PCA space*

##### Eigenvalue-driven sleeve weights in 3D space.

We map the variance levels into normalized sleeve weights

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ci,t=Î»iâ€‹(t)Î»1â€‹(t)+Î»2â€‹(t)+Î»3â€‹(t),i=1,2,3,C\_{i,t}\;=\;\frac{\lambda\_{i}(t)}{\lambda\_{1}(t)+\lambda\_{2}(t)+\lambda\_{3}(t)}\,,\qquad i=1,2,3, |  | (37) |

so that directions with larger expanding-window energy in the 3D PCA space receive higher allocation.

##### Forecast integration and portfolio return.

Let X^t+1\widehat{X}\_{t+1} be the predicted point from either (i) the *native-space* forecaster, or
(ii) the *geometry-aware* (logâ€“forecastâ€“exp) forecaster; define
Î”â€‹X^i,t+1=X^i,t+1âˆ’Xi,t\widehat{\Delta X}\_{i,t+1}=\widehat{X}\_{i,t+1}-X\_{i,t} and the directional signal
si,t+1=signâ€‹(Î”â€‹X^i,t+1)s\_{i,t+1}=\mathrm{sign}(\widehat{\Delta X}\_{i,t+1}). The eigenvalue-weighted eigenportfolio return is

|  |  |  |  |
| --- | --- | --- | --- |
|  | rt+1(eig,â€‰3â€‹D)=âˆ‘i=13Ci,tâ€‹si,t+1â€‹pi,t+1,pi,t+1â€‹Â the out-of-sample return of sleeveÂ â€‹i.r^{(\mathrm{eig},\,3D)}\_{t+1}\;=\;\sum\_{i=1}^{3}C\_{i,t}\;s\_{i,t+1}\;p\_{i,t+1},\qquad p\_{i,t+1}\text{ the out-of-sample return of sleeve }i. |  | (38) |

All evaluation metrics (MAE/RMSE/Sign, Sharpe, cumulative PnL) are computed identically across (i) and (ii) to ensure a fair comparison; results are reported in SectionÂ [4](https://arxiv.org/html/2511.05030v2#S4 "4 Results â€£ The Shape of Markets: Machine learning modeling and Prediction Using 2-Manifold Geometries").SimonianLopezdePradoFabozzi2024

##### Design intent.

This makes the *allocation* responsive to the *expanding-window variance structure *of the 3D PCA space itself** (via Î»iâ€‹(t)\lambda\_{i}(t)), while the *forecasting layer* tests whether geometry-aware predictions improve the *directional timing* si,t+1s\_{i,t+1} relative to a Euclidean baseline.

##### Curvature-aware aggregation and geometry-weighted benchmarks.

Beyond the pure RP eigenportfolios framework, we also report:
(i) a *curvature-gated* aggregation that buckets timestamps by KtK\_{t} (negative/near-zero/positive) and averages the appropriate geometry-run PnLs (torus / Euclidean / all geometries) and
(ii) expanding-window geometry weighting by ex-post Sharpe/returns as a sanity check. These appear in the merged report alongside LO and RP asset benchmarks.

Design intent.
This setup ensures the following:
(i) the 3D embedding reflects *time-varying* linear structure (eigenportfolios) while all *nonlinear* effects are captured by curvature/topology on the embedded path;
(ii) forecasts are compared *like-for-like* against a native-space baseline that applies the same predictors without manifold steps; and
(iii) portfolio construction is neutral (inverse-vol) and modular curvature gating is reported separately so its incremental value can be isolated.

## 4 Results

### 4.1 â€™Forecast to Tradingâ€™ Evaluation Design

We assess the methodology on (i) *simulated* regimes and (ii) *real finance* data (SectionÂ [3.9](https://arxiv.org/html/2511.05030v2#S3.SS9 "3.9 Real-Finance Data Pipeline: Expanding PCA, Eigenportfolios, and Forecasting â€£ 3 Geometry-Constrained Stochastic Dynamics: Explicit Forms â€£ The Shape of Markets: Machine learning modeling and Prediction Using 2-Manifold Geometries")). Because trading payoff is highly sensitive to *direction*, we report both statistical errors (MAE/RMSE) and trading metrics (Sharpe/Sortino/Calmar, hit-rate). All comparisons are *like-for-like* between:

* â€¢

  Native-space forecasts in the 3D PCA embedding, and
* â€¢

  Geometry-aware forecasts (logâ€“-forecast-â€“exp) on Mâˆˆ{â„2,S2,T2,H2}M\in\{\mathbb{R}^{2},S^{2},T^{2},H^{2}\}.

Inputs, windows, and forecaster class are held fixed across arms.

### 4.2 Curvature statistics and regime assignment (finance path)

Using the expanding-window estimator on the 3D PCA path, we analyze the series {Kt}\{K\_{t}\} (4491 non-missing points, from 2007 to 2025). Basic distributional facts:

|  |  |  |
| --- | --- | --- |
|  | meanâ€‹(K)=âˆ’0.0207,sdâ€‹(K)=0.0186,\text{mean}(K)=-0.0207,\quad\text{sd}(K)=0.0186,\quad |  |

|  |  |  |
| --- | --- | --- |
|  | minâ¡K=âˆ’0.125,Q25=âˆ’0.0247,Q50=âˆ’0.0186,\min K=-0.125,\quad\mathrm{Q}\_{25}=-0.0247,\ \mathrm{Q}\_{50}=-0.0186,\ |  |

|  |  |  |
| --- | --- | --- |
|  | Q75=âˆ’0.0088,maxâ¡K=0.0142.\mathrm{Q}\_{75}=-0.0088,\ \max K=0.0142. |  |

With curvature thresholds (Îº+,Îºâˆ’)=(+0.01,âˆ’0.01)(\kappa\_{+},\kappa\_{-})=(+0.01,-0.01), the time share by regime is:

|  |  |  |
| --- | --- | --- |
|  | â„™â€‹(Ktâ‰¤Îºâˆ’)âŸhyperbolic-like=73.9%,â„™â€‹(|Kt|â‰¤0.01)âŸnear-flat=24.4%,â„™â€‹(Ktâ‰¥Îº+)âŸspherical-like=1.7%.\underbrace{\mathbb{P}(K\_{t}\leq\kappa\_{-})}\_{\text{hyperbolic-like}}=73.9\%,\qquad\underbrace{\mathbb{P}(|K\_{t}|\leq 0.01)}\_{\text{near-flat}}=24.4\%,\qquad\underbrace{\mathbb{P}(K\_{t}\geq\kappa\_{+})}\_{\text{spherical-like}}=1.7\%. |  |

At looser thresholds (e.g., Ï„=0.005\tau=0.005) the near-flat share rises to 18.4%18.4\% and the positive share remains small (3.3%3.3\%), while 78.3%78.3\% remains negative. Serial dependence is strong (ACF(1)=0.997=0.997), indicating *persistent* curvature regimes rather than high-frequency noise.

##### Time segmentation (annual shares, Ï„=0.01\tau=0.01).

A compact, label-based classification by year assigns â€œhyperbolic-likeâ€ when â„™â€‹(Ktâ‰¤âˆ’0.01)>50%\mathbb{P}(K\_{t}\leq-0.01)>\!50\% and â„™â€‹(Ktâ‰¥0.01)<10%\mathbb{P}(K\_{t}\geq 0.01)<10\%; â€œEuclidean/flat-likeâ€ when â„™â€‹(|Kt|â‰¤0.01)>60%\mathbb{P}(|K\_{t}|\leq 0.01)>\!60\% and both tails are small; and â€œtorus-like (mixed)â€ when both tails are material (>10%>\!10\% each). The resulting picture is:

* â€¢

  2007â€“-2010: Euclidean/flat-like (near-flat share dominates; mean KK mild negative).
* â€¢

  2011â€“â€“2020: Hyperbolic-like (negative curvature dominates; virtually no positive tail).
* â€¢

  2021: *Torus-like (mixed)* â€” â„™â€‹(Ktâ‰¤âˆ’0.01)â‰ˆ48%\mathbb{P}(K\_{t}\leq-0.01)\approx 48\%, â„™â€‹(|Kt|â‰¤0.01)â‰ˆ25%\mathbb{P}(|K\_{t}|\leq 0.01)\approx 25\%, â„™â€‹(Ktâ‰¥0.01)â‰ˆ26%\mathbb{P}(K\_{t}\geq 0.01)\approx 26\%.
* â€¢

  2022â€“-2025: Strongly hyperbolic-like (negative share â‰ˆ100%\approx 100\%, increasingly negative mean KK).

##### Interpretation.

At face value, the *sign* distribution is skewed negative (hyperbolic-like), but two features align with a *toroidal scaffold* dominating the overall dynamics:
(i) the presence of both negative and positive curvature episodes (albeit asymmetric), and
(ii) extended stretches of near-flat curvature between negative bursts.
On a standard torus T2â€‹(R,r)T^{2}(R,r) the Gaussian curvature varies with the minor angle Ï•\phi,

|  |  |  |
| --- | --- | --- |
|  | Kâ€‹(Ï•)=cosâ¡Ï•râ€‹[R+râ€‹cosâ¡Ï•],K(\phi)=\frac{\cos\phi}{r\,[R+r\cos\phi]}, |  |

so trajectories that spend more time near the inner saddle (Ï•â‰ˆÏ€\phi\!\approx\!\pi) naturally produce a distribution with large negative mass, occasional small positive excursions (outer bulge, Ï•â‰ˆ0\phi\!\approx\!0), and plateaus near Kâ‰ˆ0K\!\approx\!0 when the path lingers on transition bands. This is exactly what we observe: long negative runs (median run length â‰ˆ14\approx 14 time points; max âˆ¼3000\sim 3000), short positive runs (median â‰ˆ3\approx 3), and nontrivial flat intervals.

Assignment. Aggregating across 2007 to 2025, the curvature statistics point to *hyperbolic-biased toroidal dynamics*: topologically consistent with one (or multiple) torii, but with the path predominantly visiting the saddle side (negative KK), punctuated by mixed-curvature episodes (notably 2021) and near-flat interludes. This reconciles (a) the dominance of negative curvature in the estimator with (b) the empirical finding that toroidal embeddings often deliver the strongest predictive sleeves and portfolio lift when those mixed/periodic phases emerge.

### 4.3 Topological validation via persistent homology

We provide a topological check for *toroidal* behavior in the real-data embedding by estimating the homology of a delay-embedded attractor (Takensâ€™ embedding, Takens1981) built from the 3D PCA path (and, where informative, from individual sleeves). The exhibit comprises: (i) a Takens attractor plot and (ii) persistent homology summaries (diagrams/barcodes) up to H2H\_{2}. The torus test is *heuristic but informative*: a T2T^{2}-like attractor typically shows one connected component (Î²0=1\beta\_{0}\!=\!1), *two* long-lived 1-cycles (Î²1â‰ˆ2\beta\_{1}\!\approx\!2), and, in dense surface samples, a weak 2-class (Î²2â‰ˆ1\beta\_{2}\!\approx\!1). Trajectory samples often recover the two H1H\_{1} classes cleanly, with H2H\_{2} less stable.

##### Takens embedding and filtration.

Given a univariate coordinate xtx\_{t} (e.g., a PCA sleeve) or a multivariate series Xtâˆˆâ„3X\_{t}\in\mathbb{R}^{3}, we construct a delay map

|  |  |  |
| --- | --- | --- |
|  | ğ’¯m,Ï„â€‹(xt)=(xt,xtâˆ’Ï„,â€¦,xtâˆ’(mâˆ’1)â€‹Ï„)âˆˆâ„m,\mathcal{T}\_{m,\tau}(x\_{t})=\big(x\_{t},\ x\_{t-\tau},\ \dots,\ x\_{t-(m-1)\tau}\big)\in\mathbb{R}^{m}, |  |

(or stack sleeves to get â„3â€‹m\mathbb{R}^{3m}), choose (m,Ï„)(m,\tau) via standard criteria (mutual information / first ACF minimum; false nearest neighbors), standardize the cloud, and compute Vietorisâ€“Rips persistence up to dimension 2. We summarize HkH\_{k} by lifetimes â„“=bâˆ’d\ell=b-d (birth bb, death dd), the number of long bars, and null-comparisons.

![Refer to caption](images/H1_max_lifetime_timeseries.png)

(a) H1 max lifetime (time series)

![Refer to caption](images/n_persistent_H1_timeseries.png)

(b) # persistent H1 loops (time series)

![Refer to caption](images/torus_flag_annual_bar.png)

(c) Annual share of torus-flagged windows

![Refer to caption](images/scatter_H1max_vs_nH1.png)

(d) H1 max lifetime vs #H1 loops (flag split)

Figure 9: Persistent homology diagnostics on the real-data embedding. High *H1* lifetimes (a) co-occur with elevated counts of persistent 1-cycles (b); year-by-year, torus-like intervals dominate (c). The moderate positive association in (d) indicates that when the two principal loops of a torus persist strongly, overall 1-dimensional topological activity is also elevated.

##### Topological evidence for a torus-like manifold (real data).

Visual inspection of the emerged torus-like shape is displayed in Figure [10](https://arxiv.org/html/2511.05030v2#S4.F10 "Figure 10 â€£ Topological evidence for a torus-like manifold (real data). â€£ 4.3 Topological validation via persistent homology â€£ 4 Results â€£ The Shape of Markets: Machine learning modeling and Prediction Using 2-Manifold Geometries"). The behavior directs us on thinking more heavily on the existence of multiple torii, with different radii â€™stitchedâ€™ together in a â€™smoothâ€™ way â€“ a statement which we try to quantify using the TDA analysis below.

![Refer to caption](images/takens_attractor.png)


Figure 10: Takenâ€™s embedding and the emergence of a torus-like shape

Under the computational topological data analysis framework, across 4,251 windows, the torus test is satisfied in 88.4% of cases, with 20 contiguous â€œtorusâ€ runs; the three longest spans (2007â€“2013, 2015â€“2019, 2021â€“2023) show elevated topological persistence (e.g., mean H1H\_{1} max lifetime up to 1.34 in 2021â€“2023). A moderate association between H1H\_{1} max lifetime and the number of persistent 1-cycles (Ï=0.374\rho\!=\!0.374) suggests that when the two principal loops are strong, overall 1D topological activity is also high. Taken together, these diagnostics are *consistent with* a torus-like, two-cycle latent geometry dominating much of the sample, in line with the curvature analysis.

We call an interval *torus-like* if, on a sliding window, the following hold:

|  |  |  |  |
| --- | --- | --- | --- |
|  | #â€‹{longÂ H1Â bars}â‰¥2,â„“1(1)+â„“2(1)âˆ‘jâ„“j(1)â‰¥Ïâ‹†,â„“2(1)â‰¥q0.95null,\#\{\text{long $H\_{1}$ bars}\}\ \geq 2,\qquad\frac{\ell^{(1)}\_{1}+\ell^{(1)}\_{2}}{\sum\_{j}\ell^{(1)}\_{j}}\ \geq\ \rho\_{\star},\qquad\ell^{(1)}\_{2}\geq q\_{0.95}^{\text{null}}, |  | (39) |

where â„“1(1)â‰¥â„“2(1)\ell^{(1)}\_{1}\geq\ell^{(1)}\_{2} are the top two H1H\_{1} lifetimes, Ïâ‹†âˆˆ[0.4,0.7]\rho\_{\star}\in[0.4,0.7] is a concentration threshold, and q0.95nullq\_{0.95}^{\text{null}} is the 95th percentile of the top-H1H\_{1} lifetime computed from a *null ensemble* preserving low-order statistics (e.g., phase-randomized surrogates or equicorrelated Brownian surrogates). This controls for spurious loops created by noise or purely Euclidean drift.

### 4.4 Trading Translation and PnL Construction

Let Xt=(X1,t,X2,t,X3,t)âŠ¤X\_{t}=(X\_{1,t},X\_{2,t},X\_{3,t})^{\top} be the expanding-PCA 3D path of eigenportfolio cumulatives and X^t+1\widehat{X}\_{t+1} the one step ahead prediction.

##### Directional signals.

For sleeve iâˆˆ{1,2,3}i\in\{1,2,3\} define the predicted increment and sign:

|  |  |  |
| --- | --- | --- |
|  | Î”â€‹X^i,t+1=X^i,t+1âˆ’Xi,t,si,t+1=signâ¡(Î”â€‹X^i,t+1)âˆˆ{âˆ’1,0,1}.\widehat{\Delta X}\_{i,t+1}=\widehat{X}\_{i,t+1}-X\_{i,t},\qquad s\_{i,t+1}=\operatorname{sign}\big(\widehat{\Delta X}\_{i,t+1}\big)\in\{-1,0,1\}. |  |

##### 500-day volatility scale (per sleeve).

With trailing window 500,

|  |  |  |
| --- | --- | --- |
|  | Î”â€‹XÂ¯i,t(500)=1500â€‹âˆ‘j=0499Î”â€‹Xi,tâˆ’j,Ïƒ^i,t(500)=1499â€‹âˆ‘j=0499(Î”â€‹Xi,tâˆ’jâˆ’Î”â€‹XÂ¯i,t(500))2.\bar{\Delta X}\_{i,t}^{(500)}=\frac{1}{500}\sum\_{j=0}^{499}\Delta X\_{i,t-j},\quad\widehat{\sigma}\_{i,t}^{(500)}=\sqrt{\frac{1}{499}\sum\_{j=0}^{499}\big(\Delta X\_{i,t-j}-\bar{\Delta X}\_{i,t}^{(500)}\big)^{2}}. |  |

##### Coordinate PnL and total.

The volatility scaled PnL of sleeve ii is

|  |  |  |
| --- | --- | --- |
|  | pnli,t+1=si,t+1Ïƒ^i,t(500)â€‹Î”â€‹Xi,t+1,pnlTot,t+1=13â€‹âˆ‘i=13pnli,t+1,\mathrm{pnl}\_{i,t+1}=\frac{s\_{i,t+1}}{\widehat{\sigma}\_{i,t}^{(500)}}\,\Delta X\_{i,t+1},\qquad\mathrm{pnl}\_{\mathrm{Tot},t+1}=\tfrac{1}{3}\sum\_{i=1}^{3}\mathrm{pnl}\_{i,t+1}, |  |

with annualized Sharpe Sh=252â€‹pnlÂ¯/stdevâ€‹(pnl)\mathrm{Sh}=\sqrt{252}\,\overline{\mathrm{pnl}}/\mathrm{stdev}(\mathrm{pnl}).
For *eigenvalue weighted* sleeves (SectionÂ [3.10](https://arxiv.org/html/2511.05030v2#S3.SS10 "3.10 Eigenvalue-Weighted Sleeves from 3D PCA Space (Expanding SVD) â€£ 3 Geometry-Constrained Stochastic Dynamics: Explicit Forms â€£ The Shape of Markets: Machine learning modeling and Prediction Using 2-Manifold Geometries")), we also form

|  |  |  |
| --- | --- | --- |
|  | rt+1(eig,3â€‹D)=âˆ‘i=13Ci,tâ€‹si,t+1â€‹pi,t+1,Ci,t=Î»iâ€‹(t)âˆ‘j=13Î»jâ€‹(t),r^{(\mathrm{eig},3D)}\_{t+1}=\sum\_{i=1}^{3}C\_{i,t}\,s\_{i,t+1}\,p\_{i,t+1},\quad C\_{i,t}=\frac{\lambda\_{i}(t)}{\sum\_{j=1}^{3}\lambda\_{j}(t)}, |  |

where pi,t+1p\_{i,t+1} is the out-of-sample eigenportfolio return and Î»iâ€‹(t)\lambda\_{i}(t) are the expanding-window *3D-space* eigenvalues.Bailey2014DSR; Bailey2014PBO

### 4.5 Simulated Regimes: Crossâ€“Manifold Performance

Table 1: Crossâ€“manifold performance on simulated data,Hsu2002; doCarmoRiemannian. Best MAE/RMSE/MAPE and Sharpe per dataâ€“manifold block highlighted.

| Data Manifold | MAE | RMSE | MAPE (%) | Sharpe Ratios (x,y,z) | Model Geometry |
| --- | --- | --- | --- | --- | --- |
| ğ•Š2\mathbb{S}^{2} | 0.100.10 | 0.670.67 | 304.20304.20 | 0.021, âˆ’-0.020, âˆ’-0.301 | VAR (â„3\mathbb{R}^{3}) |
| 0.260.26 | 0.300.30 | 153.19153.19 | âˆ’-0.158, âˆ’-0.207, âˆ’-0.484 | ğ•Š2\mathbb{S}^{2} GIM |
| 0.390.39 | 0.510.51 | 205.47205.47 | âˆ’-0.199, âˆ’-0.282, 0.215 | ğ•‹2\mathbb{T}^{2} GIM |
| 0.02 | 0.04 | 49.59 | 0.407, 0.022, 0.020 | â„2\mathbb{H}^{2} GIM |
| ğ•‹2\mathbb{T}^{2} | 11.1911.19 | 321.87321.87 | 24â€‰693.7724\,693.77 | âˆ’-0.119, 0.590, 0.228 | VAR (â„3\mathbb{R}^{3}) |
| 0.71 | 0.99 | 245.98 | 1.871, 2.777, 2.483 | ğ•Š2\mathbb{S}^{2} GIM |
| 0.900.90 | 1.321.32 | 554.40554.40 | âˆ’-0.817, âˆ’-0.751, 0.502 | ğ•‹2\mathbb{T}^{2} GIM |
| 0.770.77 | 1.131.13 | 383.95383.95 | 1.425, 1.896, âˆ’-0.067 | â„2\mathbb{H}^{2} GIM |
| â„3\mathbb{R}^{3} | 0.130.13 | 0.860.86 | 636.79636.79 | âˆ’-0.018, 0.129, âˆ’-0.009 | VAR (â„3\mathbb{R}^{3}) |
| 0.200.20 | 0.280.28 | 376.74376.74 | âˆ’-0.152, 0.038, âˆ’-0.580 | ğ•Š2\mathbb{S}^{2} GIM |
| 0.380.38 | 0.510.51 | 871.02871.02 | âˆ’-0.336, âˆ’-0.086, 0.052 | ğ•‹2\mathbb{T}^{2} GIM |
| 0.030.03 | 0.050.05 | 118.70118.70 | âˆ’-0.195, 0.141, âˆ’-0.104 | â„2\mathbb{H}^{2} GIM |
| â„2\mathbb{H}^{2} | 0.840.84 | 18.5318.53 | 1066.881066.88 | 0.074, 0.186, âˆ’-0.056 | VAR (â„3\mathbb{R}^{3}) |
| 0.220.22 | 0.280.28 | 203.06203.06 | 0.116, 0.421, 0.252 | ğ•Š2\mathbb{S}^{2} GIM |
| 0.360.36 | 0.470.47 | 311.37311.37 | âˆ’-0.400, âˆ’-0.458, 0.276 | ğ•‹2\mathbb{T}^{2} GIM |
| 0.12 | 0.21 | 131.43 | 0.381, 0.037, 0.014 | â„2\mathbb{H}^{2} GIM |

### 4.6 Validation on Correlated Brownian Motions â€“ CBMs (Euclidean Null)

To check that geometry performance is not a PCA artifact, we evaluate on equicorrelated CBMs projected to 3D .

##### High correlation Ï=0.9\rho=0.9:

* â€¢

  Euclidean: Total Sharpe 0.211\mathbf{0.211}
* â€¢

  Spherical: âˆ’0.119-0.119 â€ƒToroidal: âˆ’0.172-0.172 â€ƒHyperbolic: âˆ’0.109-0.109

##### Moderate correlation Ï=0.6\rho=0.6:

* â€¢

  Toroidal: Total Sharpe 0.273\mathbf{0.273}
* â€¢

  Euclidean: 0.0970.097 â€ƒSpherical: âˆ’0.337-0.337 â€ƒHyperbolic: âˆ’0.310-0.310

##### Summary.

On *flat* CBMs (Ï=0.9\rho{=}0.9), Euclidean dominates; with weaker correlation (Ï=0.6\rho{=}0.6) a toroidal slice can outperform, consistent with cyclical structure emerging after projection. Together with SectionÂ [3.8](https://arxiv.org/html/2511.05030v2#S3.SS8 "3.8 Euclidean Null Control: Correlated Brownian Motions â€£ 3 Geometry-Constrained Stochastic Dynamics: Explicit Forms â€£ The Shape of Markets: Machine learning modeling and Prediction Using 2-Manifold Geometries"), these controls indicate that the gains reported earlier in the real financial data are associated with *intrinsic* curvature/cyclicity captured by the manifold step, not PCA alone.

### 4.7 Financial Data: Coordinate Sharpe by Geometry,Maillard2010; Roncalli2013

Table 2: Sharpe ratios by geometry and coordinate (from prior version; reported as-is). Best per column in bold.

| Geometry | xx | yy | zz | Total |
| --- | --- | --- | --- | --- |
| Euclidean (E) | âˆ’0.112-0.112 | âˆ’0.368-0.368 | âˆ’0.442-0.442 | âˆ’0.322-0.322 |
| Spherical (S) | 0.177 | 0.338 | âˆ’0.037-0.037 | 0.273 |
| Toroidal (T) | 0.161 | âˆ’0.252-0.252 | 0.721 | 0.274 |
| Hyperbolic (H) | âˆ’0.094-0.094 | âˆ’0.284-0.284 | 0.357 | âˆ’0.065-0.065 |

To put these results in context, we compare the geometry-informed strategies against conventional benchmark portfolios: the equally weighted long only (LO) portfolio of all assets, and a risk parity portfolio (RP) that balances contributions to volatility. Over the same 2005 to 2025 period, the equally weighted portfolio achieved a Sharpe ratio of roughly 0.39 and the RP portfolio about 0.44. The standout is the toroidal zz coordinate (Sharpe 0.7210.721), consistent with a latent *cyclic* component amplified by a T2T^{2} embedding. In other words, a trading strategy informed by the toroidal geometry signals would have outperformed a passive diversified portfolio, highlighting the practical value of the geometric approach. Notably, even the spherical modelâ€™s Sharpe (0.270.27) is on par with the benchmarks, while the Euclidean forecast modelâ€™s negative Sharpe is clearly inferior. These findings reinforce that embedding financial time series in an appropriate curved space can extract predictive signals that traditional methods overlook. (Summary table of best performing strategies vs benchmarks in Table [3](https://arxiv.org/html/2511.05030v2#S4.T3 "Table 3 â€£ 4.7 Financial Data: Coordinate Sharpe by Geometry,Maillard2010; Roncalli2013 â€£ 4 Results â€£ The Shape of Markets: Machine learning modeling and Prediction Using 2-Manifold Geometries"),Avellaneda2022)

Table 3: Summary Table : Strategy-level Sharpe ratios (2005â€“-2025). Geometry-Informed Modeling (GIM) variants vs. benchmarks.

|  |  |
| --- | --- |
| Strategy (configuration) | Sharpe |
| GIM (integrated: eigenvalue weighting + curvature gating) | 0.64 |
| Long-Only benchmark (equally weighted) | 0.3900.390 |
| Risk Parity benchmark (inverse-vol) | 0.4390.439 |
| Native PCA VAR (Euclidean baseline, weighted) | âˆ’0.386-0.386 |
| Best single sleeve (toroidal zz-coordinate) | 0.721 |

The toroidal zz-sleeve (0.721) is a *concentrated, ex post* winner; it fully loads on one predictive direction and enjoys the strongest regime episodes, but it also carries higher *selection risk* and sensitivity to geometry misâ€“specification. By contrast, the integrated GIM (0.64) is an *ex ante ensemble* that (i) diversifies across sleeves via expanding-â€“SVD eigenvalue weights, (ii) applies curvature gating to scale risk in adverse geometry, and (iii) hedges against transient regime flips. These safeguards intentionally trade a slice of peak Sharpe for *stability, lower selection error, and reduced drawdown/turnover*. In other words, the single best sleeve sets a performance *upper bound* for that specific regime, whereas the integrated portfolio is designed to be more robust across regimes. As an upgrade, we could tune this trade-off by shrinking the curvature gate or increasing the â€œtemperatureâ€ of eigenvalue weights to move the ensemble closer to the concentrated sleeve â€“ leaving this to be part of our future research on the topic.

##### Integrated GIM (eigenvalue + curvature) outperforms all baselines.

When we combine the geometry aware forecaster with *expanding-â€“SVD eigenvalue weighting* of sleeves (Ci,tC\_{i,t} from the 3D PCA space) and *curvature gating* of the risk budget (with torus split when flagged), the full GIM attains an annualized Sharpe of around 0.64 on the real data test setâ€”-*the highest across all configurations*. This exceeds the RP baseline (0.4390.439, +0.199+\!0.199), and LO (0.3900.390, +0.248+\!0.248), and dominates the native space baselines (âˆ’0.386/âˆ’0.289-0.386/-0.289). The lift is consistent with (i) concentrating exposure on currently energetic modes via Î»iâ€‹(t)\lambda\_{i}(t), (ii) scaling total risk with regime curvature (Î»t\lambda\_{t} expands in trending/negative-â€“curvature phases and contracts in mean reverting/positive curvature phases), and (iii) honoring toroidal two-â€“cycle structure when present. Because inputs, predictors, and PnL translation are held fixed relative to the baselines, these gains isolate the added value of *geometry informed variance structure and regime awareness*.

#### 4.7.1 Augmenting VAR with machine-learning predictors

To test whether generic nonlinear learning methods can extract residual structure beyond the manifold-informed VAR, we extended the tangent-space forecasting system by incorporating *Random Forest (RF)* and *Gaussian Process Regression (GP)* models as auxiliary predictors.
Specifically, each manifold coordinate (x,y,z)(x,y,z) was forecasted using VAR, RF, and GP individually, and their outputs were combined through a linear ensemble to assess whether local nonlinearities not captured by the linear VAR could improve out-of-sample predictive accuracy.

While both RF and GP models offer flexible functional forms, their results show only marginal differences relative to the baseline VAR.
TableÂ [4](https://arxiv.org/html/2511.05030v2#S4.T4 "Table 4 â€£ 4.7.1 Augmenting VAR with machine-learning predictors â€£ 4.7 Financial Data: Coordinate Sharpe by Geometry,Maillard2010; Roncalli2013 â€£ 4 Results â€£ The Shape of Markets: Machine learning modeling and Prediction Using 2-Manifold Geometries") summarizes their performance across geometries.
In both setups, the toroidal configuration continues to dominate with Sharpe ratios in the range 0.380.38â€“0.470.47, whereas Euclidean and spherical geometries remain negative, and hyperbolic slightly positive.
This finding suggests that the core predictive information arises primarily from the manifoldâ€™s geometric organization rather than from generic nonlinear learners.

Table 4: Comparative forecasting performance (Sharpe ratios) for manifold geometries using Gaussian Process (GP) and Random Forest (RF) predictors (2005â€“2025).

|  |  |  |
| --- | --- | --- |
| Geometry / Coordinate | GP | RF |
| xx (Euclidean) | âˆ’0.1039-0.1039 | 0.02920.0292 |
| yy (Euclidean) | 0.00090.0009 | âˆ’0.2689-0.2689 |
| zz (Euclidean) | âˆ’0.4099-0.4099 | âˆ’0.5331-0.5331 |
| xx (Sphere) | âˆ’0.0326-0.0326 | âˆ’0.0318-0.0318 |
| yy (Sphere) | 0.22500.2250 | 0.25470.2547 |
| zz (Sphere) | âˆ’0.3606-0.3606 | âˆ’0.3709-0.3709 |
| xx (Torus) | 0.30130.3013 | 0.22300.2230 |
| yy (Torus) | âˆ’0.2570-0.2570 | âˆ’0.1551-0.1551 |
| zz (Torus) | 0.4052 | 0.4867 |
| xx (Hyperbolic) | 0.09650.0965 | âˆ’0.1095-0.1095 |
| yy (Hyperbolic) | 0.03120.0312 | âˆ’0.1599-0.1599 |
| zz (Hyperbolic) | 0.09460.0946 | 0.17320.1732 |
| Aggregate by Geometry |  |  |
| Euclidean | âˆ’0.3965-0.3965 | âˆ’0.5403-0.5403 |
| Sphere | âˆ’0.3484-0.3484 | âˆ’0.3560-0.3560 |
| Torus | 0.3783 | 0.4677 |
| Hyperbolic | 0.09820.0982 | 0.15110.1511 |

* â€¢

  Note. Both Gaussian Process and Random Forest regressors cannot provide any significant marginal improvements relative to the geometry-informed VAR baseline. The toroidal configuration remains the most predictive, consistent with the cyclic market dynamics revealed by curvature analysis.

## 5 Discussion and Future Research

### 5.1 ISâ€“LM foundations for (multi-)torus dynamics: a mathematical sketch

##### Caveat.

The connection we outline is *hypothetical*. It shows how standard macro adjustment equations *could* generate (i) one or more cyclical degrees of freedom and (ii) a phase representation that naturally lives on a torus (or a product of tori). In that sense, we simply outline an assumption, a hypothesis on the connection and we do not claim any structural identification in this paper.

#### 1. ISâ€“LM(+Phillips) linear core and oscillatory modes,Hicks1937

Let y:=Yâˆ’Yâ‹†y:=Y-Y^{\star} be the output gap, r:=iâˆ’iâ‹†r:=i-i^{\star} the (policy/market) rate gap, and Ï€:=Î âˆ’Î â‹†\pi:=\Pi-\Pi^{\star} the inflation gap. A minimalist continuous-time linear adjustment writes

|  |  |  |  |
| --- | --- | --- | --- |
|  | yË™=âˆ’Ïâ€‹yâˆ’Ïƒâ€‹r+Î·t,rË™=Ï•â€‹yâˆ’Ï‡â€‹r+Ïˆâ€‹Ï€+Î½t,Ï€Ë™=Îºâ€‹yâˆ’Ï‘â€‹Ï€+Î¾t,(Ï,Ïƒ,Ï•,Ï‡,Ïˆ,Îº,Ï‘)>0,\begin{aligned} \dot{y}&=-\rho\,y\;-\;\sigma\,r\;+\;\eta\_{t},\\ \dot{r}&=\ \phi\,y\;-\;\chi\,r\;+\;\psi\,\pi\;+\;\nu\_{t},\\ \dot{\pi}&=\ \kappa\,y\;-\;\vartheta\,\pi\;+\;\xi\_{t},\end{aligned}\quad(\rho,\sigma,\phi,\chi,\psi,\kappa,\vartheta)>0, |  | (40) |

with small shocks (Î·t,Î½t,Î¾t)(\eta\_{t},\nu\_{t},\xi\_{t}). In matrix form ğ¬Ë™=Aâ€‹ğ¬+ğ®t\dot{\mathbf{s}}=A\,\mathbf{s}+\mathbf{u}\_{t} for ğ¬=(y,r,Ï€)âŠ¤\mathbf{s}=(y,r,\pi)^{\top} and

|  |  |  |
| --- | --- | --- |
|  | A=[âˆ’Ïâˆ’Ïƒ0Ï•âˆ’Ï‡ÏˆÎº0âˆ’Ï‘].A=\begin{bmatrix}-\rho&-\sigma&0\\ \ \phi&-\chi&\ \psi\\ \ \kappa&0&-\vartheta\end{bmatrix}. |  |

The (y,r)(y,r)-subsystem has complex eigenvalues (oscillatory adjustment,Benhabib1979; Barnett2008) iff

|  |  |  |  |
| --- | --- | --- | --- |
|  | (Ïâˆ’Ï‡)2<â€„4â€‹Ïƒâ€‹Ï•.(\rho-\chi)^{2}\;<\;4\,\sigma\,\phi. |  | (41) |

Coupling to Ï€\pi via (Ïˆ,Îº)(\psi,\kappa) adds a further feedback loop. For parameter regions near a Hopf boundary, the linear core exhibits *one* dominant oscillatory mode; additional macro blocks (below) provide further cycles.

##### Note on â€œHopfâ€.Kuznetsov2013

â€œHopfâ€ refers to the *Hopf bifurcation*: in a smooth system xË™=fâ€‹(x,Î±)\dot{x}=f(x,\alpha), an equilibrium undergoes a qualitative change when the Jacobian has a simple pair of purely imaginary eigenvalues Â±iâ€‹Ï‰0\pm i\omega\_{0} at Î±=Î±0\alpha=\alpha\_{0} (others stable) and the real part crosses zero transversally. Locally, dynamics on the 2D center manifold reduce to

|  |  |  |
| --- | --- | --- |
|  | zË™=(Î¼+iâ€‹Ï‰0)â€‹zâˆ’câ€‹|z|2â€‹z+â‹¯,zâˆˆâ„‚,Î¼=Î±âˆ’Î±0,\dot{z}=(\mu+i\omega\_{0})z-c|z|^{2}z+\cdots,\quad z\in\mathbb{C},\ \mu=\alpha-\alpha\_{0}, |  |

so for a *supercritical* Hopf (Rec>0\real c>0, Î¼>0\mu>0) a stable limit cycle appears with one phase Î¸âˆˆS1\theta\in S^{1}. If a second weakly coupled Hopf mode coexists (e.g., another macro block), two phases (Î¸,Ï•)âˆˆS1Ã—S1(\theta,\phi)\in S^{1}\times S^{1} arise, i.e. a torus T2T^{2} (and, with several blocks, a product of tori).

#### 2. Hopf reduction and phase dynamics (one torus)

Near a (supercritical) Hopf set of ([40](https://arxiv.org/html/2511.05030v2#S5.E40 "In 1. ISâ€“LM(+Phillips) linear core and oscillatory modes,Hicks1937 â€£ 5.1 ISâ€“LM foundations for (multi-)torus dynamics: a mathematical sketch â€£ 5 Discussion and Future Research â€£ The Shape of Markets: Machine learning modeling and Prediction Using 2-Manifold Geometries")), the macro state admits a two-dimensional center manifold with normal form

|  |  |  |  |
| --- | --- | --- | --- |
|  | zË™=(Î¼+iâ€‹Ï‰)â€‹zâˆ’câ€‹|z|2â€‹z+Îµt,zâˆˆâ„‚,Î¼,Ï‰âˆˆâ„,Re(c)>0,\dot{z}\;=\;(\mu+i\,\omega)\,z\;-\;c\,|z|^{2}z\;+\;\varepsilon\_{t},\qquad z\in\mathbb{C},\ \ \mu,\omega\in\mathbb{R},\ \ \real(c)>0, |  | (42) |

whose stable limit cycle (for Î¼>0\mu>0) has amplitude |z|=Î¼/Re(c)|z|=\sqrt{\mu/\real(c)}. Writing z=Ïâ€‹eiâ€‹Î¸z=\rho e^{i\theta},

|  |  |  |
| --- | --- | --- |
|  | ÏË™=Î¼â€‹Ïâˆ’Re(c)â¡Ï3+â€¦,Î¸Ë™=Ï‰+Î¶â€‹(Ï)+â€¦,\dot{\rho}\;=\;\mu\,\rho-\real(c)\,\rho^{3}+\ldots,\qquad\dot{\theta}\;=\;\omega+\zeta(\rho)+\ldots, |  |

so the long-run dynamics reduce to a *single* phase variable Î¸âˆˆS1\theta\in S^{1} (one circle). If a second, weakly coupled Hopf mode is present (e.g., from an additional macro block), we obtain two phases (Î¸,Ï•)âˆˆS1Ã—S1(\theta,\phi)\in S^{1}\times S^{1}, i.e., a *torus* T2T^{2}.

#### 3. From one torus to several: a multi-block macro

Empirically, multiple macro-financial subsystems can oscillate: a core ISâ€“LM block; a credit/liquidity (financial accelerator) block; an external (open-economy ISâ€“LMâ€“BP) block; a term-structure block, etc. A parsimonious representation treats each block s=1,â€¦,Ss=1,\dots,S as a weakly nonlinear oscillator with complex state z(s)z^{(s)}:

|  |  |  |  |
| --- | --- | --- | --- |
|  | zË™(s)=(Î¼s+iâ€‹Ï‰s)â€‹z(s)âˆ’csâ€‹|z(s)|2â€‹z(s)+âˆ‘â„“â‰ sÎ“sâ€‹â„“â€‹(z(â„“),z(s))+Îµt(s).\dot{z}^{(s)}\;=\;\big(\mu\_{s}+i\,\omega\_{s}\big)z^{(s)}\;-\;c\_{s}|z^{(s)}|^{2}z^{(s)}\;+\;\sum\_{\ell\neq s}\Gamma\_{s\ell}\big(z^{(\ell)},z^{(s)}\big)\;+\;\varepsilon\_{t}^{(s)}. |  | (43) |

Writing z(s)=Ïsâ€‹eiâ€‹Î¸(s)z^{(s)}=\rho\_{s}e^{i\theta^{(s)}} and projecting on the limit cycles (Ïsâ‰ˆÏsâ‹†\rho\_{s}\approx\rho\_{s}^{\star}), we obtain a *phase network,Ikeda2012*,

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î¸Ë™(s)=Ï‰s+âˆ‘â„“â‰ s[ÎºÎ¸â€‹Î¸(sâ€‹â„“)â€‹sinâ¡(Î¸(â„“)âˆ’Î¸(s)âˆ’Î±Î¸â€‹Î¸(sâ€‹â„“))+ÎºÎ¸â€‹Ï•(sâ€‹â„“)â€‹sinâ¡(Ï•(â„“)âˆ’Î¸(s)âˆ’Î±Î¸â€‹Ï•(sâ€‹â„“))]+ÏƒÎ¸(s)â€‹WË™t(s),\dot{\theta}^{(s)}\;=\;\omega\_{s}\;+\;\sum\_{\ell\neq s}\!\!\Big[\kappa\_{\theta\theta}^{(s\ell)}\sin\!\big(\theta^{(\ell)}-\theta^{(s)}-\alpha\_{\theta\theta}^{(s\ell)}\big)\;+\;\kappa\_{\theta\phi}^{(s\ell)}\sin\!\big(\phi^{(\ell)}-\theta^{(s)}-\alpha\_{\theta\phi}^{(s\ell)}\big)\Big]\;+\;\sigma^{(s)}\_{\theta}\,\dot{W}\_{t}^{(s)}, |  | (44) |

(and analogously for a second phase Ï•(s)\phi^{(s)} if block ss has two distinct cycles). Collecting the phases

|  |  |  |
| --- | --- | --- |
|  | Î¦t=(Î¸t(1),Ï•t(1),â€¦,Î¸t(S),Ï•t(S))âˆˆT2â€‹S=S1Ã—S1âŸblock 1Ã—â‹¯Ã—S1Ã—S1âŸblockÂ S,\Phi\_{t}\;=\;\big(\theta\_{t}^{(1)},\phi\_{t}^{(1)},\ \ldots,\ \theta\_{t}^{(S)},\phi\_{t}^{(S)}\big)\ \in\ T^{2S}\;=\;\underbrace{S^{1}\times S^{1}}\_{\text{block 1}}\times\cdots\times\underbrace{S^{1}\times S^{1}}\_{\text{block $S$}}, |  |

the latent macro state evolves on a *product of tori*. This is what we mean by â€œmultiple toriiâ€: several two-cycle subsystems, each contributing its own S1Ã—S1S^{1}\times S^{1} factor.

##### Slowly varying cycle strengths (time-varying radii).

Policy stance or global liquidity can modulate the effective cycle amplitudes. A simple parameterization is

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ïsâ‹†â€‹(t)=Rsâ€‹(Î¶t),0<Rsâ€‹(â‹…)<âˆ,\rho\_{s}^{\star}(t)\;=\;R\_{s}\big(\zeta\_{t}\big),\qquad 0<R\_{s}(\cdot)<\infty, |  | (45) |

for a slowly moving driver Î¶t\zeta\_{t}; geometrically, this induces a *torus bundle* with time-varying â€œradiiâ€ (cycle strengths).

#### 4. From macro phases to the asset embedding

Let Xtâˆˆâ„3X\_{t}\in\mathbb{R}^{3} denote the 3D asset embedding (our eigenportfolio coordinates). A reduced-form map from macro phases to the conditional drift is

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â€‹[Î”â€‹Xt+1âˆ£â„±t]=Gâ€‹(Î¦t),Gâ€‹(Î¦)=âˆ‘s=1Sâˆ‘m,nâˆˆâ„¤Cm,n(s)â€‹[cosâ¡(mâ€‹Î¸(s)+nâ€‹Ï•(s))sinâ¡(mâ€‹Î¸(s)+nâ€‹Ï•(s))cosâ¡(mâ€‹Î¸(s)âˆ’nâ€‹Ï•(s))],\mathbb{E}[\Delta X\_{t+1}\mid\mathcal{F}\_{t}]\;=\;G\big(\Phi\_{t}\big),\quad G(\Phi)\;=\;\sum\_{s=1}^{S}\ \sum\_{m,n\in\mathbb{Z}}C^{(s)}\_{m,n}\,\begin{bmatrix}\cos(m\theta^{(s)}+n\phi^{(s)})\\ \sin(m\theta^{(s)}+n\phi^{(s)})\\ \cos(m\theta^{(s)}-n\phi^{(s)})\end{bmatrix}, |  | (46) |

with slowly varying loadings Cm,n(s)C^{(s)}\_{m,n}. Under mild smoothness and time-scale separation, the embedded path inherits the *topology* (loops) of the underlying T2â€‹ST^{2S} and displays local curvature patterns (negative/near-zero/positive) as the phases traverse saddle/transition/compact bands of the induced geometry.

#### 5. What â€œmultiâ€“torusâ€ implies for our diagnostics

* â€¢

  Curvature. Mixtures of cycles produce *mixed* Gaussian curvature with a negative skew if trajectories frequent â€œsaddleâ€ corridors (amplification-dominant passages), interspersed with near-zero bands (slow transitions) and occasional positive patches (constraint-dominant).
* â€¢

  Persistent homology. Windows with two *dominant* cycles exhibit two long H1H\_{1} features; when additional blocks become energetic, extra (weaker) H1H\_{1} loops may appear. The strength of the top two loops should co-move with cycle amplitudes Ïsâ‹†â€‹(t)\rho\_{s}^{\star}(t) in ([45](https://arxiv.org/html/2511.05030v2#S5.E45 "In Slowly varying cycle strengths (time-varying radii). â€£ 3. From one torus to several: a multi-block macro â€£ 5.1 ISâ€“LM foundations for (multi-)torus dynamics: a mathematical sketch â€£ 5 Discussion and Future Research â€£ The Shape of Markets: Machine learning modeling and Prediction Using 2-Manifold Geometries")).
* â€¢

  Spectral content. The embedded coordinates should show peaks near the macro cycle frequencies {Ï‰s}\{\omega\_{s}\} (possibly time-varying), with cross-modulation when couplings in ([44](https://arxiv.org/html/2511.05030v2#S5.E44 "In 3. From one torus to several: a multi-block macro â€£ 5.1 ISâ€“LM foundations for (multi-)torus dynamics: a mathematical sketch â€£ 5 Discussion and Future Research â€£ The Shape of Markets: Machine learning modeling and Prediction Using 2-Manifold Geometries")) tighten.

##### Why this matters for our GIM.

Working in charts aligned with the local geometry (log map â†’\to tangent forecast â†’\to exp map) lets the forecaster track *where on the torus(es)* the system currently moves. The eigenvalue weighting concentrates on sleeves that presently carry the strongest cycle energy, while curvature gating tempers risk in amplification-heavy passages. Our empirical outperformance is *consistent with* this picture, without pinning down a single structural model.

A stylized ISâ€“LM(+extensions) can sustain one or more weakly coupled limit cycles. Near Hopf regimes and with additional macro blocks, the latent state admits a phase representation on T2â€‹ST^{2S} (a product of tori) with slowly varying radii. If such a structure is relevant, it offers a parsimonious rationale for the torus-like geometric and topological signatures we document, and for the gains from geometry-aware forecasting. Establishing causality requires a dedicated macroâ€“finance state-space estimation and is left for future work.

##### Portfolio Management Context

However, in the context of trading/portfolio management applications, we model *assets*, not macro variables - yet some macro cycles may leave a geometric imprint usable for forecasting. A minimal, testable set of assumptions under which sharing information between the macro (phase) space and the asset PCA space is valid is:

* â€¢

  Lowâ€“dimensional driver (A1). There exists a latent macro state Î¶t=(Î¸t,Ï•t)âˆˆS1Ã—S1\zeta\_{t}=(\theta\_{t},\phi\_{t})\in S^{1}\times S^{1} such that the *conditional* mean of the embedded increments is a smooth function of Î¶t\zeta\_{t}:

  |  |  |  |
  | --- | --- | --- |
  |  | ğ”¼â€‹[Î”â€‹Xt+1âˆ£â„±t]=gâ€‹(Î¶t),gâˆˆC1,â€–âˆ‡gâ€–<âˆ.\mathbb{E}[\Delta X\_{t+1}\mid\mathcal{F}\_{t}]\;=\;g(\zeta\_{t}),\qquad g\in C^{1},\ \ \|\nabla g\|<\infty. |  |
* â€¢

  Factor alignment (A2). Asset excess returns admit a conditional factor model

  |  |  |  |
  | --- | --- | --- |
  |  | rt+1=Bâ€‹(Î¶t)â€‹ft+1+ut+1,ğ”¼â€‹[ut+1âˆ£â„±t]=0,r\_{t+1}\;=\;B(\zeta\_{t})\,f\_{t+1}+u\_{t+1},\quad\mathbb{E}[u\_{t+1}\mid\mathcal{F}\_{t}]=0, |  |

  where the top PCA eigenportfolios span the space of priced factors ft+1f\_{t+1} up to rotation; thus the 3D PCA path XtX\_{t} captures the common macroâ€“driven component.
* â€¢

  Slow variation / timeâ€“scale separation (A3). Loadings Bâ€‹(Î¶t)B(\zeta\_{t}) and the PCA basis/eigenvalues vary *slowly* at the forecast horizon:
  â€–Bâ€‹(Î¶t+1)âˆ’Bâ€‹(Î¶t)â€–=oâ€‹(1),\|B(\zeta\_{t+1})-B(\zeta\_{t})\|=o(1),
  so expanding-window PCA and eigenvalue weights track the evolving subspace without introducing spurious curvature.
* â€¢

  Weak feedback (A4). Macro phases are not instantaneously determined by asset prices (no simultaneity at tt); any feedback is *lagged* or small, so using Î¶t\zeta\_{t} (or its geometric proxies) for prediction avoids circularity.
* â€¢

  No-arbitrage consistency (A5). Risk premia are functions of Î¶t\zeta\_{t} (via Bâ€‹(Î¶t)B(\zeta\_{t}) and prices of risk), so that conditioning on Î¶t\zeta\_{t} (or its geometric signature such as local curvature KtK\_{t}) is economically meaningful and not eliminated by static arbitrage.
* â€¢

  Measurement invariance (A6). The geometry inferred from XtX\_{t} is invariant to orthogonal rotations of the PCA coordinates; hence torus-/hyperbolic-like signatures reflect the *stateâ€™s* topology, not an arbitrary basis choice.

Under (A1)â€“(A6) the map Î¶tâ†¦(ğ”¼â€‹[Î”â€‹Xt+1âˆ£â„±t],Varâ¡[Î”â€‹Xt+1âˆ£â„±t])\zeta\_{t}\mapsto(\mathbb{E}[\Delta X\_{t+1}\mid\mathcal{F}\_{t}],\operatorname{Var}[\Delta X\_{t+1}\mid\mathcal{F}\_{t}]) is well-defined and sufficiently smooth, legitimizing the use of geometry-informed forecasts and geometry-modulated allocation on asset returns. Practically, these assumptions can be probed via (i) rotation tests on PCA sleeves, (ii) stability tests for Bâ€‹(â‹…)B(\cdot) across subsamples, and (iii) event studies verifying that identified macro shocks shift the geometric proxies (KtK\_{t}, phase speeds) before changes in expected returns.

##### Machine Learners Contribution

: Further experiments incorporating machine-learning forecasts (RF and GP) as auxiliary predictors to the VAR confirmed that generic nonlinear learners add little incremental value once curvature and manifold structure are accounted for.
Both GP and RF models produced similar ranking of geometriesâ€”with the torus remaining the dominant topologyâ€”but their absolute gains were modest.
This outcome reinforces the interpretation that market predictability arises less from complex functional approximations and more from the geometric and topological constraints shaping the evolution of financial states.

### 5.2 Future research agenda (testable directions)

##### (A) Macroâ€“GIM coupling in a state-space (hypothesis testing).

Embed a small macro block ğ¦t\mathbf{m}\_{t} (output gap, inflation, term structure, liquidity indicators) alongside geometric phases (Î¸t,Ï•t)(\theta\_{t},\phi\_{t}) in a joint state-space. Estimate via particle/MCMC filters on S1Ã—S1S^{1}\times S^{1} with observation in â„3\mathbb{R}^{3}. Test whether shocks to ğ¦t\mathbf{m}\_{t} *shift* curvature regimes and phase speeds, and whether geometry improves macro nowcasts.

##### (B) Geometry-aware nowcasting.

Combine GIM with mixed-frequency nowcasting: update (Î¸t,Ï•t)(\theta\_{t},\phi\_{t}) and the tangent-space forecasts as high-frequency data arrive, and reweight sleeves by expanding-SVD eigenvalues and curvature gates. This evaluates whether real-time macro signals *enhance* geometry-aware timing.NoguerAlonso2021FinEAS

##### (C) Identification and causality.

Use event studies (policy announcements, macro releases) and geometry-aware Granger tests to assess if curvature/phase changes *precede* or *follow* macro innovations. This distinguishes descriptive fit from predictive content.

##### (D) Multi-torus/multi-sector structure.

Allow several coupled tori {(Î¸t(s),Ï•t(s))}s\{(\theta\_{t}^{(s)},\phi\_{t}^{(s)})\}\_{s} to represent sectoral/sovereign cycles; study whether cross-couplings explain the observed curvature skew better than a single-torus hypothesis.

##### (E) Model class extensions.

* â€¢

  *Riemannian dynamics:* Specify VAR/GP directly on TÎ¼â€‹MT\_{\mu}M with curvature-corrected propagation and compare to Euclidean projections.
* â€¢

  *Time-varying shape parameters:* Let (Rt,rt)(R\_{t},r\_{t}) for T2T^{2} or (at,bt,ct)(a\_{t},b\_{t},c\_{t}) for the hyperboloid evolve stochastically; jointly estimate with forecasts.
* â€¢

  *SPD-covariance coupling:* Model sleeve covariances on the SPD manifold (Logâ€“Euclidean or affine-invariant) and fuse with torus phases to capture varianceâ€“phase interactions.

##### (F) Policy sensitivity and stress tests.

Simulate counterfactuals under alternative policy rules (LM slope, reaction coefficients) and quantify how curvature regimes and portfolio outcomes *might* change. This frames geometry as a compact policy-sensitivity diagnostic.

##### (G) Robustness and external validity.

Replicate across markets and horizons; vary expanding-window lengths; perform (i) geometry-aware forecast, (ii) eigenvalue weighting, (iii) curvature gating, to isolate contributions. Where possible, preregister hypotheses about curvature shifts around known macro events.

Summary. Our evidence is *consistent* withâ€”but does not establishâ€”a torus-like macroâ€“financial geometry in which two interacting cycles produce the observed curvature mix. Treating this as a working hypothesis suggests concrete tests (state-space coupling, event identification, multi-torus structures) and practical extensions (nowcasting + GIM) that can clarify whether the geometry carries independent predictive content or simply organizes existing macro signals more effectively.

## 6 Appendices

## Appendix A Data Universe and Pre-processing

The dataset used in this study encompasses a broad spectrum of financial assets, covering the period from January 1, 2005, to August 3, 2025. The assets include:

* â€¢

  Broad Market Indices: S&P 500 (SPY), NASDAQ-100 (QQQ), Dow Jones Industrial Average (DIA), Russell 2000 (IWM), MSCI Emerging Markets (EEM), and MSCI EAFE (EFA).
* â€¢

  Sector ETFs: Technology Select Sector SPDR (XLK), Financial Select Sector SPDR (XLF), Health Care Select Sector SPDR (XLV), Energy Select Sector SPDR (XLE), Consumer Discretionary Select Sector SPDR (XLY), Consumer Staples Select Sector SPDR (XLP), Utilities Select Sector SPDR (XLU), Industrial Select Sector SPDR (XLI), and Materials Select Sector SPDR (XLB).
* â€¢

  Bond ETFs: iShares 20+ Year Treasury Bond ETF (TLT), iShares 7-10 Year Treasury Bond ETF (IEF), iShares 1-3 Year Treasury Bond ETF (SHY), iShares iBoxx $ Investment Grade Corporate Bond ETF (LQD), iShares iBoxx $ High Yield Corporate Bond ETF (HYG), iShares TIPS Bond ETF (TIP), and iShares MBS ETF (MBB).
* â€¢

  Commodity ETFs: SPDR Gold Shares (GLD), iShares Silver Trust (SLV), Invesco DB Agriculture Fund (DBA), Teucrium Corn Fund (CORN), Invesco DB Base Metals Fund (DBB), Aberdeen Standard Physical Platinum Shares ETF (PPLT), and Aberdeen Standard Physical Palladium Shares ETF (PALL).
* â€¢

  Oil ETFs: United States Oil Fund (USO), ProShares Ultra Bloomberg Crude Oil (UCO), Invesco DB Oil Fund (DBO), and SPDR S&P Oil & Gas Exploration & Production ETF (XOP).
* â€¢

  Non-U.S. ETFs: iShares MSCI ACWI ex U.S. ETF (ACWX), Vanguard FTSE All-World ex-US ETF (VEU), iShares MSCI Europe ETF (IEUR), iShares MSCI Pacific ex Japan ETF (EPP), iShares Latin America 40 ETF (ILF), iShares MSCI Canada ETF (EWC), iShares MSCI United Kingdom ETF (EWU), iShares MSCI South Korea ETF (EWY), iShares MSCI Australia ETF (EWA), iShares MSCI Taiwan ETF (EWT), and Vanguard FTSE Emerging Markets ETF (VWO).
* â€¢

  Dividend and Volatility ETFs: Vanguard Dividend Appreciation ETF (VIG), iShares Select Dividend ETF (DVY), SPDR S&P Dividend ETF (SDY), and ProShares VIX Short-Term Futures ETF (VIXY).
* â€¢

  Alternative ETFs: iShares Clean Energy ETF (ICLN) and Global X Lithium & Battery Tech ETF (LIT).

This diverse dataset provides comprehensive coverage of global financial markets, enabling robust testing of the proposed methodologies across various asset classes and economic regimes.

## Appendix B Implementation details for SDEs and curvature terms

##### Ambient Euler Maruyama.

We discretize ([2](https://arxiv.org/html/2511.05030v2#S3.E2 "In 3.1 Ambient formulation and curvature drift â€£ 3 Geometry-Constrained Stochastic Dynamics: Explicit Forms â€£ The Shape of Markets: Machine learning modeling and Prediction Using 2-Manifold Geometries")) with step hh:

|  |  |  |
| --- | --- | --- |
|  | Xk+1=Xk+Pâ€‹(Xk)â€‹Î”â€‹Wkâˆ’12â€‹Hâ€‹(Xk)â€‹h,Î”â€‹Wkâˆ¼ğ’©â€‹(0,hâ€‹I3).X\_{k+1}\;=\;X\_{k}\;+\;P(X\_{k})\,\Delta W\_{k}\;-\;\frac{1}{2}H(X\_{k})\,h,\quad\Delta W\_{k}\sim\mathcal{N}(0,hI\_{3}). |  |

For M=S2M=S^{2} we use Pâ€‹(x)=Iâˆ’xâ€‹xâŠ¤â€–xâ€–2P(x)=I-\frac{xx^{\top}}{\|x\|^{2}} and Hâ€‹(x)=2â€–xâ€–2â€‹xH(x)=\frac{2}{\|x\|^{2}}x. For the torus we use the implicit form Ï•â€‹(x)=(Râˆ’x12+x22)2+x32âˆ’r2\phi(x)=\big(R-\sqrt{x\_{1}^{2}+x\_{2}^{2}}\big)^{2}+x\_{3}^{2}-r^{2} with Pâ€‹(x)=Iâˆ’âˆ‡Ï•â€‹âˆ‡Ï•âŠ¤â€–âˆ‡Ï•â€–2P(x)=I-\frac{\nabla\phi\,\nabla\phi^{\top}}{\|\nabla\phi\|^{2}} and a consistent mean-curvature drift (implemented in closed form). Hyperbolic segments use the implicit quadratic form Ï•â€‹(x)=a1â€‹x12+a2â€‹x22+a3â€‹x32âˆ’1\phi(x)=a\_{1}x\_{1}^{2}+a\_{2}x\_{2}^{2}+a\_{3}x\_{3}^{2}-1 with signature (+,âˆ’,+)(+,-,+), projector from the gradient, and corresponding curvature drift.

##### Intrinsic charts.

For torus and hyperbolic we also implement the intrinsic SDEs ([5](https://arxiv.org/html/2511.05030v2#S3.E5 "In 2nd item â€£ Torus ğ‘‡Â²â¢(ğ‘…,ğ‘Ÿ) (major radius ğ‘…, minor radius ğ‘Ÿ). â€£ 3.2 Explicit SDEs by geometry â€£ 3 Geometry-Constrained Stochastic Dynamics: Explicit Forms â€£ The Shape of Markets: Machine learning modeling and Prediction Using 2-Manifold Geometries"))â€“([6](https://arxiv.org/html/2511.05030v2#S3.E6 "In 2nd item â€£ Torus ğ‘‡Â²â¢(ğ‘…,ğ‘Ÿ) (major radius ğ‘…, minor radius ğ‘Ÿ). â€£ 3.2 Explicit SDEs by geometry â€£ 3 Geometry-Constrained Stochastic Dynamics: Explicit Forms â€£ The Shape of Markets: Machine learning modeling and Prediction Using 2-Manifold Geometries")) and ([7](https://arxiv.org/html/2511.05030v2#S3.E7 "In Hyperbolic ğ»Â² (hyperboloid model). â€£ 3.2 Explicit SDEs by geometry â€£ 3 Geometry-Constrained Stochastic Dynamics: Explicit Forms â€£ The Shape of Markets: Machine learning modeling and Prediction Using 2-Manifold Geometries"))â€“([8](https://arxiv.org/html/2511.05030v2#S3.E8 "In Hyperbolic ğ»Â² (hyperboloid model). â€£ 3.2 Explicit SDEs by geometry â€£ 3 Geometry-Constrained Stochastic Dynamics: Explicit Forms â€£ The Shape of Markets: Machine learning modeling and Prediction Using 2-Manifold Geometries")), then map to â„3\mathbb{R}^{3} via Î¨\Psi and Î¦\Phi respectively at output time.

## Appendix C Scenario catalog

We use seven scenarios of length TT:

* â€¢

  Scen. 1 (Pattern): [S2,H2,â„3][S^{2},H^{2},\mathbb{R}^{3}] blocks of length 500, repeated thrice, ending with S2S^{2}.
* â€¢

  Scen. 2 (Random order): a randomized permutation of S2,H2,â„3S^{2},H^{2},\mathbb{R}^{3} blocks of length 500, ending with S2S^{2}.
* â€¢

  Scen. 3 (Pure S2S^{2}): one S2S^{2} segment of length 5000.
* â€¢

  Scen. 4 (Pure T2T^{2}): one torus segment of length 5000 (angle chart simulation, output in â„3\mathbb{R}^{3}).
* â€¢

  Scen. 5 (Pure Euclid): one â„3\mathbb{R}^{3} Brownian segment of length 5000.
* â€¢

  Scen. 6 (Pure H2H^{2}): one hyperbolic segment of length 5000 (absorbing threshold to prevent numeric explosion).
* â€¢

  Scen. 7 (Comprehensive): repeated rotation of S2,H2,â„3,T2S^{2},H^{2},\mathbb{R}^{3},T^{2} blocks (e.g., 5 cycles Ã—\times 4 blocks Ã—\times 500 points).

## Appendix D Coordinate transforms,doCarmoCurves; doCarmoRiemannian

##### Spherical â†”\leftrightarrow Cartesian (radius RR).

|  |  |  |  |
| --- | --- | --- | --- |
|  |  | CartÂ â†Â Sph:x=Râ€‹sinâ¡Ï•â€‹cosâ¡Î¸,y=Râ€‹sinâ¡Ï•â€‹sinâ¡Î¸,z=Râ€‹cosâ¡Ï•.\displaystyle\text{Cart $\leftarrow$ Sph:}\quad x=R\sin\phi\,\cos\theta,\;\;y=R\sin\phi\,\sin\theta,\;\;z=R\cos\phi. |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | SphÂ â†Â Cart:R=â€–xâ€–,Î¸=arctanâ¡2â€‹(y,x),Ï•=arccosâ¡(z/â€–xâ€–).\displaystyle\text{Sph $\leftarrow$ Cart:}\quad R=\|x\|,\;\;\theta=\arctan 2(y,x),\;\;\phi=\arccos(z/\|x\|). |  |

##### Torus â†”\leftrightarrow Cartesian.

|  |  |  |  |
| --- | --- | --- | --- |
|  |  | CartÂ â†Â Torus:x=(R+râ€‹cosâ¡Ï†)â€‹cosâ¡Î¸,y=(R+râ€‹cosâ¡Ï†)â€‹sinâ¡Î¸,z=râ€‹sinâ¡Ï†.\displaystyle\text{Cart $\leftarrow$ Torus:}\quad x=(R+r\cos\varphi)\cos\theta,\;\;y=(R+r\cos\varphi)\sin\theta,\;\;z=r\sin\varphi. |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | TorusÂ â†Â Cart:Î¸=arctanâ¡2â€‹(y,x),Ï†=arctanâ¡2â€‹(z,x2+y2âˆ’R).\displaystyle\text{Torus $\leftarrow$ Cart:}\quad\theta=\arctan 2(y,x),\;\;\varphi=\arctan 2\!\big(z,\ \sqrt{x^{2}+y^{2}}-R\big). |  |

##### Hyperboloid (rotational) â†”\leftrightarrow Cartesian.

|  |  |  |  |
| --- | --- | --- | --- |
|  |  | CartÂ â†Â Hyp:x=aâ€‹coshâ¡uâ€‹cosâ¡v,y=aâ€‹coshâ¡uâ€‹sinâ¡v,z=câ€‹sinhâ¡u.\displaystyle\text{Cart $\leftarrow$ Hyp:}\quad x=a\cosh u\cos v,\;\;y=a\cosh u\sin v,\;\;z=c\sinh u. |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | HypÂ â†Â Cart:v=arctanâ¡2â€‹(y/a,x/a),u=arcsinhâ€‹(z/c).\displaystyle\text{Hyp $\leftarrow$ Cart:}\quad v=\arctan 2(y/a,x/a),\;\;u=\mathrm{arcsinh}(z/c). |  |

For regime transitions we convert once at the beginning of the new segment, simulate in the appropriate chart if needed, and always output in Cartesian space for continuity.