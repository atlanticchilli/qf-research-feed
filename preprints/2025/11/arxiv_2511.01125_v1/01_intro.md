---
authors:
- Takashi Furuya
- Anastasis Kratsios
- Dylan PossamaÃ¯
- Bogdan RaoniÄ‡
doc_id: arxiv:2511.01125v1
family_id: arxiv:2511.01125
is_current: true
taxonomy:
  alpha_families: []
  asset_classes: []
  horizons: []
  themes: []
title: 'One model to solve them all: 2BSDE families via neural operators'
url_abs: http://arxiv.org/abs/2511.01125v1
url_html: https://arxiv.org/html/2511.01125v1
venue: arXiv q-fin
version: 1
year: 2025
---


Takashi Furuya 111Department of Biomedical Engineering, Doshisha University, KyÅto, Japan, takashi.furuya0101@gmail.com.
â€ƒâ€ƒ
Anastasis Kratsios 222Department of Mathematics, McMaster University, McMaster University, Hamilton, Canada, kratsiosa@mcmaster.ca.
â€ƒâ€ƒ
Dylan PossamaÃ¯ 333ETH ZÃ¼rich, Department of Mathematics, ZÃ¼rich, Switzerland, dylan.possamai@math.ethz.ch.
â€ƒâ€ƒ
Bogdan RaoniÄ‡ 444ETH ZÃ¼rich, Seminar for Applied Mathematics and ETH AI Center, ZÃ¼rich, Switzerland, braonic@ethz.ch.

(November 2, 2025)

###### Abstract

We introduce a mild generative variant of the classical neural operator model, which leverages Kolmogorovâ€“Arnold networks to solve infinite families of second-order backward stochastic differential equations (22BSDEs) on regular bounded Euclidean domains with random terminal time. Our first main result shows that the solution operator associated with a broad range of 22BSDE families is approximable by appropriate neural operator models. We then identify a structured subclass of (infinite) families of 22BSDEs whose neural operator approximation requires only a polynomial number of parameters in the reciprocal approximation rate, as opposed to the exponential requirement in general worst-case neural operator guarantees.

Key words: Neural operators, solution operators, backward stochastic differential equations, exponential approximation rates.

## 1 Introduction

Fix a positive integer dâˆˆâ„•â‹†d\in\mathbb{N}^{\star}. We work on a filtered probability space (Î©,â„±,ğ”½â‰”(â„±t)tâˆˆ[0,âˆ),â„™)\big(\Omega,{\cal F},\mathbb{F}\coloneqq({\cal F}\_{t})\_{t\in[0,\infty)},\mathbb{P}\big) carrying a dd-dimensional (ğ”½,â„™)(\mathbb{F},\mathbb{P})â€“Brownian motion WW. Fix a sufficiently regular bounded open domain ğ’ŸâŠ‚â„d\mathcal{D}\subset\mathbb{R}^{d}, as well as maps Î¼:â„dâŸ¶â„d\mu:\mathbb{R}^{d}\longrightarrow\mathbb{R}^{d}, Î£:â„dâŸ¶â„dÃ—d\Sigma:\mathbb{R}^{d}\longrightarrow\mathbb{R}^{d\times d}, and f:â„dÃ—â„Ã—â„dÃ—â„dÃ—dâŸ¶â„f:\mathbb{R}^{d}\times\mathbb{R}\times\mathbb{R}^{d}\times\mathbb{R}^{d\times d}\longrightarrow\mathbb{R}, as well as an initial point xâˆˆğ’Ÿx\in\mathcal{D}.
We are interested in simultaneously approximately solving each 2BSDE in the (non-empty) compact infinite family â„¬âŠ†(Xâ‹…,Yâ‹…g,f0,Zâ‹…g,f0,Î¥g,f0,Ag,f0)}(g,f0)âˆˆğ”š\mathcal{B}\subseteq(X\_{\cdot},Y\_{\cdot}^{g,f\_{\text{$0$}}},Z\_{\cdot}^{g,f\_{\text{$0$}}},\Upsilon^{g,f\_{\text{$0$}}},A^{g,f\_{\text{$0$}}})\}\_{(g,f\_{\text{$0$}})\in\mathfrak{W}} where ğ”š\mathfrak{W} is a suitable subset of the Sobolev spaces W1,pâ€‹(âˆ‚ğ’Ÿ)Ã—W1,pâ€‹(ğ’Ÿ)W^{1,p}(\partial\mathcal{D})\times W^{1,p}(\mathcal{D}). These 2BSDEs are defined through the system

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | Xt\displaystyle X\_{t} | =x+âˆ«0tÎ²â€‹(Xs)â€‹ds+âˆ«0tÎ£â€‹(Xs)â€‹dWs,tâ‰¥0,â„™â€‹â€“a.s.,Ï„â‰”inf{tâ‰¥0:Xtâˆ‰ğ’Ÿ},\displaystyle=x+\int\_{0}^{t}\beta(X\_{s})\mathrm{d}s+\int\_{0}^{t}\,\Sigma(X\_{s})\mathrm{d}W\_{s},\;t\geq 0,\;\mathbb{P}\text{\rm--a.s.},\;\tau\coloneqq\inf\big\{t\geq 0:X\_{t}\notin{\cal D}\big\}, |  | (SDE) |
|  |  |  |  |
| --- | --- | --- | --- |
|  | Ytg,f0\displaystyle Y\_{t}^{g,f\_{\text{$0$}}} | =gâ€‹(XÏ„)âŸPerturbation+âˆ«tâˆ§Ï„Ï„(fâ€‹(Xs,Ysg,f0,Zsg,f0,Î¥sg,f0)âŸReference generator+f0â€‹(Xs)âŸPerturbationâˆ’12â€‹Trâ€‹[Î£â€‹(Xs)â€‹Î£âŠ¤â€‹(Xs)â€‹Î¥sg,f0])â€‹ds\displaystyle=\underbrace{g(X\_{\tau})}\_{\text{\tiny Perturbation}}+\int\_{t\wedge\tau}^{\tau}\bigg(\underbrace{f\big(X\_{s},Y\_{s}^{g,f\_{\text{$0$}}},Z\_{s}^{g,f\_{\text{$0$}}},\Upsilon\_{s}^{g,f\_{\text{$0$}}}\big)}\_{\text{\tiny Reference generator}}+\underbrace{f\_{0}(X\_{s})}\_{\text{\tiny Perturbation}}-\frac{1}{2}\mathrm{Tr}\big[\Sigma(X\_{s})\Sigma^{\top}(X\_{s})\Upsilon\_{s}^{g,f\_{\text{$0$}}}\big]\bigg)\mathrm{d}s |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | âˆ’âˆ«tâˆ§Ï„Ï„Zsg,f0â‹…dXs,tâˆˆ[0,Ï„),â„™â€‹â€“a.s.,\displaystyle\quad-\int\_{t\wedge\tau}^{\tau}Z\_{s}^{g,f\_{\text{$0$}}}\cdot\mathrm{d}X\_{s},\;t\in[0,\tau),\;\mathbb{P}\text{\rm--a.s.}, |  | (FBSDE) |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | Ztg,f0\displaystyle Z\_{t}^{g,f\_{\text{$0$}}} | =z0+âˆ«0tAsg,f0â€‹ds+âˆ«0tÎ¥sg,f0â€‹dXs,tâˆˆ[0,Ï„),â„™â€‹â€“a.s..\displaystyle=z\_{0}+\int\_{0}^{t}A\_{s}^{g,f\_{\text{$0$}}}\mathrm{d}s+\int\_{0}^{t}\Upsilon\_{s}^{g,f\_{\text{$0$}}}\mathrm{d}X\_{s},\;t\in[0,\tau),\;\mathbb{P}\text{\rm--a.s.}. |  | (2BSDE) |

Using a variant (see [SectionËœ3.1](https://arxiv.org/html/2511.01125v1#S3.SS1 "3.1 Elliptic PDE representation of the 2BSDE system â€£ 3 Main results â€£ One model to solve them all: 2BSDE families via neural operators") below for the proof) of the results ofÂ [Cheridito, Soner, Touzi, and Victoir](https://arxiv.org/html/2511.01125v1#bib.bib16) [[16](https://arxiv.org/html/2511.01125v1#bib.bib16)] for 2BSDEs with random terminal time Ï„\tau, as above, for each pair (g,f0)âˆˆğ”š(g,f\_{0})\in\mathfrak{W}, if the following elliptic PDE

|  |  |  |  |
| --- | --- | --- | --- |
|  | fâ€‹(x,uâ€‹(x),âˆ‡uâ€‹(x),âˆ‡2uâ€‹(x))=âˆ’f0â€‹(x),xâˆˆğ’Ÿâ€‹uâ€‹(x)=gâ€‹(x),xâˆˆâˆ‚ğ’Ÿ,\displaystyle f\big(x,u(x),\nabla u(x),\nabla^{2}u(x)\big)=-f\_{0}(x),\;x\in\mathcal{D}\;u(x)=g(x),\;x\in\partial\mathcal{D}, |  | (1.1) |

admits a smooth enough solution, then the 2BSDE systemÂ ([SDE](https://arxiv.org/html/2511.01125v1#S1.Ex1 "Equation SDE â€£ 1 Introduction â€£ One model to solve them all: 2BSDE families via neural operators")), ([FBSDE](https://arxiv.org/html/2511.01125v1#S1.Ex3 "Equation FBSDE â€£ 1 Introduction â€£ One model to solve them all: 2BSDE families via neural operators")), ([2BSDE](https://arxiv.org/html/2511.01125v1#S1.Ex4 "Equation 2BSDE â€£ 1 Introduction â€£ One model to solve them all: 2BSDE families via neural operators")) admits a solution of the form

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | Ytg,f0\displaystyle Y\_{t}^{g,f\_{\text{$0$}}} | =uâ€‹(Xt),Ztg,f0=âˆ‡uâ€‹(Xt),Î¥tg,f0=âˆ‡2uâ€‹(Xt),Atg,f0=â„’â€‹âˆ‡uâ€‹(Xt),tâˆˆ[0,Ï„),â„™â€‹â€“a.s.,\displaystyle=u(X\_{t}),\;Z\_{t}^{g,f\_{\text{$0$}}}=\nabla u(X\_{t}),\;\Upsilon\_{t}^{g,f\_{\text{$0$}}}=\nabla^{2}u(X\_{t}),\;A\_{t}^{g,f\_{\text{$0$}}}=\mathcal{L}\nabla u(X\_{t}),\;t\in[0,\tau),\;\mathbb{P}\text{\rm--a.s.}, |  | (1.2) |

where â„’\mathcal{L} denotes the generator associated to the forward process XX (without the drift term), defined for any continuous bounded test function ff on â„d\mathbb{R}^{d} by

|  |  |  |
| --- | --- | --- |
|  | â„’â€‹(f)â‰”12â€‹Trâ€‹[Î£â€‹(x)â€‹Î£â€‹(x)âŠ¤â€‹âˆ‡2fâ€‹(x)],xâˆˆâ„d,\mathcal{L}(f)\coloneqq\frac{1}{2}\mathrm{Tr}\big[\Sigma(x)\Sigma(x)^{\top}\nabla^{2}f(x)\big],\;x\in\mathbb{R}^{d}, |  |

seeÂ [[16](https://arxiv.org/html/2511.01125v1#bib.bib16), Equations (2.9) and (2.11)] for a similar result in the parabolic case.

Our first main result, [TheoremËœ3.7](https://arxiv.org/html/2511.01125v1#S3.Thmtheorem7 "Theorem 3.7 (Approximability of the perturbation-to-solution map). â€£ 3.2 General approximability guarantee â€£ 3 Main results â€£ One model to solve them all: 2BSDE families via neural operators"), guarantees that the following solution map is approximable by a neural operator

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | Î“+:W1,âˆâ€‹(ğ’Ÿ;â„)Ã—W1,âˆâ€‹(ğ’Ÿ;â„)\displaystyle{\Gamma^{\text{$+$}}}:W^{1,\infty}(\mathcal{D};\mathbb{R})\times W^{1,\infty}(\mathcal{D};\mathbb{R}) | âŸ¶W1,âˆâ€‹(ğ’Ÿ;â„)\displaystyle\longrightarrow W^{1,\infty}(\mathcal{D};\mathbb{R}) |  | (1.3) |
|  | (f0,g)\displaystyle(f\_{0},g) | âŸ¼u\displaystyle\longmapsto u |  |

where f0f\_{0} and gg are the source and boundary data of the PDE inÂ ([1.1](https://arxiv.org/html/2511.01125v1#S1.E1 "Equation 1.1 â€£ 1 Introduction â€£ One model to solve them all: 2BSDE families via neural operators")), respectively; which equivalently perturb the generator and the terminal condition of the associated 2BSDEs with random terminal time Ï„\tau inÂ ([FBSDE](https://arxiv.org/html/2511.01125v1#S1.Ex3 "Equation FBSDE â€£ 1 Introduction â€£ One model to solve them all: 2BSDE families via neural operators")).

Consequently, the solution map associated to the family of second-order BSDEs is approximable by our stochastic neural operator model (which extends the neural operator model of [Furuya and Kratsios](https://arxiv.org/html/2511.01125v1#bib.bib31) in [[31](https://arxiv.org/html/2511.01125v1#bib.bib31), Definition 4] from the classical BSDE setting to 22BSDEs). This result thus provides a 22BSDE analogue of neural operator approximability results, which typically follow a two-step strategy: first, establish a quantitative universal approximation theorem for general HÃ¶lder-continuous functions with the same source and target as the solution map (see *e.g.* [Lu, Jin, Pang, Zhang, and Karniadakis](https://arxiv.org/html/2511.01125v1#bib.bib64) [[64](https://arxiv.org/html/2511.01125v1#bib.bib64)], [Korolev](https://arxiv.org/html/2511.01125v1#bib.bib46) [[46](https://arxiv.org/html/2511.01125v1#bib.bib46)], [Galimberti, Kratsios, and Livieri](https://arxiv.org/html/2511.01125v1#bib.bib33) [[33](https://arxiv.org/html/2511.01125v1#bib.bib33)], [Yu, Becquey, Halikias, Mallory, and Townsend](https://arxiv.org/html/2511.01125v1#bib.bib96) [[96](https://arxiv.org/html/2511.01125v1#bib.bib96)], [Lanthaler, Mishra, and Karniadakis](https://arxiv.org/html/2511.01125v1#bib.bib56) [[56](https://arxiv.org/html/2511.01125v1#bib.bib56)], [Lu, Jin, and Karniadakis](https://arxiv.org/html/2511.01125v1#bib.bib63) [[63](https://arxiv.org/html/2511.01125v1#bib.bib63)], [Lanthaler, Li, and Stuart](https://arxiv.org/html/2511.01125v1#bib.bib57) [[57](https://arxiv.org/html/2511.01125v1#bib.bib57)], [Kratsios, Furuya, Benitez, Lassas, and deÂ Hoop](https://arxiv.org/html/2511.01125v1#bib.bib50) [[50](https://arxiv.org/html/2511.01125v1#bib.bib50)], [Schwab, Stein, and Zech](https://arxiv.org/html/2511.01125v1#bib.bib85) [[85](https://arxiv.org/html/2511.01125v1#bib.bib85)] [GÃ¶deke and Fernsel](https://arxiv.org/html/2511.01125v1#bib.bib38) [[38](https://arxiv.org/html/2511.01125v1#bib.bib38)], [Furuya, Taniguchi, and Okuda](https://arxiv.org/html/2511.01125v1#bib.bib32) [[32](https://arxiv.org/html/2511.01125v1#bib.bib32)], and [Adcock, Brugiapaglia, Dexter, and Moraga](https://arxiv.org/html/2511.01125v1#bib.bib4) [[4](https://arxiv.org/html/2511.01125v1#bib.bib4)]); second, show that the solution map is sufficiently continuous, for instance HÃ¶lder-continuous, often via a perturbation analysis, verifying in turn it is in the scope of the main theorem, see [Alvarez, Ekren, Kratsios, and Yang](https://arxiv.org/html/2511.01125v1#bib.bib5) [[5](https://arxiv.org/html/2511.01125v1#bib.bib5)], [Horvath, Kratsios, Limmer, and Yang](https://arxiv.org/html/2511.01125v1#bib.bib43) [[43](https://arxiv.org/html/2511.01125v1#bib.bib43)], [Lanthaler and Stuart](https://arxiv.org/html/2511.01125v1#bib.bib55) [[55](https://arxiv.org/html/2511.01125v1#bib.bib55)] or [Firouzi, Yang, and Kratsios](https://arxiv.org/html/2511.01125v1#bib.bib30) [[30](https://arxiv.org/html/2511.01125v1#bib.bib30)].

One may ask if favourable approximation rates are achievable if the reference generator ff is simple enough, while still of course having a meaningful structure for several applications in optimal control and mathematical finance. Indeed, in
[TheoremËœ3.11](https://arxiv.org/html/2511.01125v1#S3.Thmtheorem11 "Theorem 3.11 (Exponential approximation rates: solution operator to the elliptic problem). â€£ 3.3.1 Semi-linear elliptic PDE â€£ 3.3 Feasible rates â€£ 3 Main results â€£ One model to solve them all: 2BSDE families via neural operators")
we show that this is the case when the reference generator is of the simplified form

|  |  |  |  |
| --- | --- | --- | --- |
|  | fâ€‹(x,y,z,w):=âˆ’Trâ€‹[Î³â€‹(x)â€‹w]âˆ’divâ€‹(Î³)â€‹(x)â‹…z+Î¼â€‹(x)â‹…z+Î»â€‹(x)â€‹y+f~â€‹(x,y)f(x,y,z,w):=-\mathrm{Tr}\big[\gamma(x)w\big]-\mathrm{div}(\gamma)(x)\cdot z+\mu(x)\cdot z+\lambda(x)y+\tilde{f}(x,y) |  | (1.4) |

for some smooth enough maps Î»:â„dâŸ¶â„\lambda:\mathbb{R}^{d}\longrightarrow\mathbb{R}, Î³:â„dâŸ¶â„dÃ—d\gamma:\mathbb{R}^{d}\longrightarrow\mathbb{R}^{d\times d}, and Î¼:â„dâŸ¶â„d\mu:\mathbb{R}^{d}\longrightarrow\mathbb{R}^{d} and where f~:â„dÃ—â„âŸ¶â„\tilde{f}:\mathbb{R}^{d}\times\mathbb{R}\longrightarrow\mathbb{R} is still sufficiently smooth. In this setting, we reduce the general fully non-linear elliptic PDE inÂ ([1.1](https://arxiv.org/html/2511.01125v1#S1.E1 "Equation 1.1 â€£ 1 Introduction â€£ One model to solve them all: 2BSDE families via neural operators")) to the following semi-linear form

|  |  |  |  |
| --- | --- | --- | --- |
|  | âˆ’âˆ‡â‹…Î³â€‹âˆ‡uâ€‹(x)+Î¼â€‹(x)â‹…âˆ‡uâ€‹(x)+Î»â€‹(x)â€‹uâ€‹(x)+f~â€‹(x,u)=âˆ’f0â€‹(x)âŸPerturbation,xâˆˆğ’Ÿ,uâ€‹(x)=gâ€‹(x)âŸPerturbation,xâˆˆâˆ‚ğ’Ÿ.\displaystyle-\nabla\cdot\gamma\nabla u(x)+\mu(x)\cdot\nabla u(x)+\lambda(x)u(x)+\tilde{f}(x,u)=\underbrace{-f\_{0}(x)}\_{\text{\tiny Perturbation}},\;x\in\mathcal{D},\;u(x)=\underbrace{g(x)}\_{\text{\tiny Perturbation}},\;x\in\partial\mathcal{D}. |  | (1.5) |

[TheoremËœ3.11](https://arxiv.org/html/2511.01125v1#S3.Thmtheorem11 "Theorem 3.11 (Exponential approximation rates: solution operator to the elliptic problem). â€£ 3.3.1 Semi-linear elliptic PDE â€£ 3.3 Feasible rates â€£ 3 Main results â€£ One model to solve them all: 2BSDE families via neural operators") both extends [[31](https://arxiv.org/html/2511.01125v1#bib.bib31), Theorem 1] by allowing Î¼\mu and Î»\lambda to be non-zero and Î£\Sigma to be non-constant and positive-definite, while no longer requiring any *a priori* knowledge of the PDE itself to be hard-coded into our design of the NO.
This is because the latter authors use explicit knowledge of Greenâ€™s function associated with the PDE
âˆ’âˆ‡â‹…Î³â€‹âˆ‡uâ€‹(x)+Î¼â€‹(x)â‹…âˆ‡uâ€‹(x)+Î»â€‹(x)â€‹uâ€‹(x)-\nabla\cdot\gamma\nabla u(x)+\mu(x)\cdot\nabla u(x)+\lambda(x)u(x)
to show that it admits a decomposition
Î¦â€‹(xâˆ’y)+Î¨â€‹(x,y)\Phi(x-y)+\Psi(x,y),
where Î¦\Phi is a â€˜difficult to approximateâ€™ singular part and Î¨\Psi is an â€˜easily approximatedâ€™ smooth part.
The convolution with the singular component Î¦\Phi is then hard-coded into each of their NO architectures by leveraging the explicit closed form for Î¦\Phi obtained inÂ [[11](https://arxiv.org/html/2511.01125v1#bib.bib11)].
In contrast, in our approach no such closed-form nor *a priori* knowledge of the PDE is required in our NO build. As should be expected, these extensions also come at the cost of devising an entirely different proof strategy.

The PDE inÂ ([1.5](https://arxiv.org/html/2511.01125v1#S1.E5 "Equation 1.5 â€£ 1 Introduction â€£ One model to solve them all: 2BSDE families via neural operators")) can be connected back to the 22BSDEÂ ([FBSDE](https://arxiv.org/html/2511.01125v1#S1.Ex3 "Equation FBSDE â€£ 1 Introduction â€£ One model to solve them all: 2BSDE families via neural operators")) either when the divergence of Î³\gamma is absorbed into Î¼\mu, or
in the special case where Î³\gamma is divergence-free, *i.e.* divâ€‹(Î³)âŠ¤=0\mathrm{div}(\gamma)^{\top}=0, implying that âˆ‡â‹…Î³âˆ‡u=Tr[Î³âˆ‡u)\nabla\cdot\gamma\nabla u=\mathrm{Tr}[\gamma\nabla u). In addition, when Î³\gamma is valued in the set of semi-definite matrices, and we take for Î£\Sigma any matrix square root of eâ€‹2â€‹Î³e2\gamma (that is to say Î£â€‹Î£âŠ¤=2â€‹Î³\Sigma\Sigma^{\top}=2\gamma), then ([1.5](https://arxiv.org/html/2511.01125v1#S1.E5 "Equation 1.5 â€£ 1 Introduction â€£ One model to solve them all: 2BSDE families via neural operators")) reduces to the more standard Hamiltonâ€“Jacobiâ€“Bellmanâ€“type semilinear equation

|  |  |  |  |
| --- | --- | --- | --- |
|  | f~â€‹(x,u)+Î»â€‹(x)â€‹uâ€‹(x)+Î¼â€‹(x)â‹…âˆ‡uâ€‹(x)âˆ’12â€‹Trâ€‹[Î£â€‹(x)â€‹Î£â€‹(x)âŠ¤â€‹âˆ‡2uâ€‹(x)]=âˆ’f0â€‹(x),xâˆˆğ’Ÿ.\tilde{f}(x,u)+\lambda(x)u(x)+\mu(x)\cdot\nabla u(x)-\frac{1}{2}\,\mathrm{Tr}\big[\Sigma(x)\Sigma(x)^{\top}\nabla^{2}u(x)\big]=-f\_{0}(x),\;x\in\mathcal{D}. |  | (1.6) |

In dimension dâ‰¥2d\geq 2, there is a whole zoology of divergence-free Î³\gamma; thus this special case completely subsumes the case where Î³\gamma is constant, as considered in [[31](https://arxiv.org/html/2511.01125v1#bib.bib31)]. For example, when d=2d=2, if Î³\gamma is positive-definiteâ€“valued then there exist a twice continuously differentiable potential Ï†Î³:â„2âŸ¶â„\varphi\_{\gamma}:\mathbb{R}^{2}\longrightarrow\mathbb{R} (this is the so-called Airy potential) such that
Î³â€‹(x)=RâŠ¤â€‹(âˆ‡2Ï†Î³â€‹(x))â€‹R\gamma(x)=R^{\top}\big(\nabla^{2}\varphi\_{\gamma}(x)\big)R
for the symplectic matrix Râ‰”e1â€‹e2âŠ¤âˆ’e2â€‹e1âŠ¤R\coloneqq e\_{1}e\_{2}^{\top}-e\_{2}e\_{1}^{\top} (where (e1,e2(e\_{1},e\_{2} is the canonical basis of â„2\mathbb{R}^{2}). A simple non-constant example of such an Airy potential Ï†Î³\varphi\_{\gamma} which additionally yields a positive-definite Î³\gamma is Ï†Î³â€‹(x,y)â‰”(x2+y2)2\varphi\_{\gamma}(x,y)\coloneqq(x^{2}+y^{2})^{2}.

Our first objective is, therefore, to simultaneously approximate the solution operator to general families of fully non-linear elliptic problemsÂ ([1.1](https://arxiv.org/html/2511.01125v1#S1.E1 "Equation 1.1 â€£ 1 Introduction â€£ One model to solve them all: 2BSDE families via neural operators")) and to obtain favourable rates for semi-linear special cases of the formÂ ([1.5](https://arxiv.org/html/2511.01125v1#S1.E5 "Equation 1.5 â€£ 1 Introduction â€£ One model to solve them all: 2BSDE families via neural operators")). Our strategy will be to construct a neural operator (NO) model which directly approximates ([TheoremËœ3.7](https://arxiv.org/html/2511.01125v1#S3.Thmtheorem7 "Theorem 3.7 (Approximability of the perturbation-to-solution map). â€£ 3.2 General approximability guarantee â€£ 3 Main results â€£ One model to solve them all: 2BSDE families via neural operators") resp.Â [TheoremËœ3.11](https://arxiv.org/html/2511.01125v1#S3.Thmtheorem11 "Theorem 3.11 (Exponential approximation rates: solution operator to the elliptic problem). â€£ 3.3.1 Semi-linear elliptic PDE â€£ 3.3 Feasible rates â€£ 3 Main results â€£ One model to solve them all: 2BSDE families via neural operators")) the coefficient-to-solution operator mapping any (g,f0)âˆˆğ”š(g,f\_{0})\in\mathfrak{W} to the elliptic PDE it defines viaÂ ([1.1](https://arxiv.org/html/2511.01125v1#S1.E1 "Equation 1.1 â€£ 1 Introduction â€£ One model to solve them all: 2BSDE families via neural operators")) (resp.Â ([1.5](https://arxiv.org/html/2511.01125v1#S1.E5 "Equation 1.5 â€£ 1 Introduction â€£ One model to solve them all: 2BSDE families via neural operators"))).
Then, using the connections between elliptic PDEs and 2BSDEs with random terminal time inÂ ([1.2](https://arxiv.org/html/2511.01125v1#S1.E2 "Equation 1.2 â€£ 1 Introduction â€£ One model to solve them all: 2BSDE families via neural operators")) formalised by our non-linear Feynmanâ€“Kac formula in [SectionËœ3.1](https://arxiv.org/html/2511.01125v1#S3.SS1 "3.1 Elliptic PDE representation of the 2BSDE system â€£ 3 Main results â€£ One model to solve them all: 2BSDE families via neural operators"), we construct an adapter transforming the functions output for our NO to tuples of stochastic processes approximating the solution to the family of associated 2BSDEs, see [TheoremËœ3.13](https://arxiv.org/html/2511.01125v1#S3.Thmtheorem13 "Theorem 3.13. â€£ 3.3.2 Solutions to the family of second-order BSDEs â€£ 3.3 Feasible rates â€£ 3 Main results â€£ One model to solve them all: 2BSDE families via neural operators").

### 1.1 Related literature

There is a mature numerical literature on secondâ€“order BSDEs (2BSDEs), including weak approximation and timeâ€“discretisation schemes by [PossamaÃ¯ and Tan](https://arxiv.org/html/2511.01125v1#bib.bib80)Â [[80](https://arxiv.org/html/2511.01125v1#bib.bib80)], [Ren and Tan](https://arxiv.org/html/2511.01125v1#bib.bib82)Â [[82](https://arxiv.org/html/2511.01125v1#bib.bib82)], [Yang, Zhao, and Zhou](https://arxiv.org/html/2511.01125v1#bib.bib94)Â [[94](https://arxiv.org/html/2511.01125v1#bib.bib94)], and the recent non-equidistant scheme of [Pak, Hwang, and Kim](https://arxiv.org/html/2511.01125v1#bib.bib75)Â [[75](https://arxiv.org/html/2511.01125v1#bib.bib75)].
Learningâ€“based approaches have also appeared (*e.g.*, [Beck, E, and Jentzen](https://arxiv.org/html/2511.01125v1#bib.bib7)Â [[7](https://arxiv.org/html/2511.01125v1#bib.bib7)], [Pereira, Wang, Chen, Reed, and Theodorou](https://arxiv.org/html/2511.01125v1#bib.bib77)Â [[77](https://arxiv.org/html/2511.01125v1#bib.bib77)], [Duong](https://arxiv.org/html/2511.01125v1#bib.bib24)Â [[24](https://arxiv.org/html/2511.01125v1#bib.bib24)], [Xiao, Qiu, and Nikan](https://arxiv.org/html/2511.01125v1#bib.bib93)Â [[93](https://arxiv.org/html/2511.01125v1#bib.bib93)]),
but these methods are essentially *perâ€“instance*: they must be reâ€“run (or reâ€“trained) whenever coefficients or boundary data change.
By contrast, we learn a *solution operator* that acts on the entire compact family of problems indexed by (g,f0)(g,f\_{0}), so a single trained model simultaneously solves all members of the family, both at the PDE and at the 2BSDE level via the PDEâ€“(2)BSDE correspondence ([Cheridito, Soner, Touzi, and Victoir](https://arxiv.org/html/2511.01125v1#bib.bib16)Â [[16](https://arxiv.org/html/2511.01125v1#bib.bib16)]; see also [Pardoux](https://arxiv.org/html/2511.01125v1#bib.bib76)Â [[76](https://arxiv.org/html/2511.01125v1#bib.bib76)], [Soner, Touzi, and Zhang](https://arxiv.org/html/2511.01125v1#bib.bib87)Â [[87](https://arxiv.org/html/2511.01125v1#bib.bib87)]).

When it comes to finiteâ€“dimensional ML for non-linear PDEs, a large body of work trains a finiteâ€“dimensional network for each target PDE separately (*e.g.*,
[NÃ¼sken and Richter](https://arxiv.org/html/2511.01125v1#bib.bib74)Â [[74](https://arxiv.org/html/2511.01125v1#bib.bib74)],
[Pham, Warin, and Germain](https://arxiv.org/html/2511.01125v1#bib.bib78)Â [[78](https://arxiv.org/html/2511.01125v1#bib.bib78)],
[Germain, LauriÃ¨re, Pham, and Warin](https://arxiv.org/html/2511.01125v1#bib.bib34), [Germain, Pham, and Warin](https://arxiv.org/html/2511.01125v1#bib.bib35), [Germain, Pham, and Warin](https://arxiv.org/html/2511.01125v1#bib.bib36)Â [[34](https://arxiv.org/html/2511.01125v1#bib.bib34), [35](https://arxiv.org/html/2511.01125v1#bib.bib35), [36](https://arxiv.org/html/2511.01125v1#bib.bib36)],
[Lefebvre, Loeper, and Pham](https://arxiv.org/html/2511.01125v1#bib.bib58)Â [[58](https://arxiv.org/html/2511.01125v1#bib.bib58)],
[Zhou, Han, and Lu](https://arxiv.org/html/2511.01125v1#bib.bib97)Â [[97](https://arxiv.org/html/2511.01125v1#bib.bib97)],
[Hu and LauriÃ¨re](https://arxiv.org/html/2511.01125v1#bib.bib44)Â [[44](https://arxiv.org/html/2511.01125v1#bib.bib44)],
[Nguwi, Penent, and Privault](https://arxiv.org/html/2511.01125v1#bib.bib73)Â [[73](https://arxiv.org/html/2511.01125v1#bib.bib73)]).
Provable *exponential* behaviour in this setting typically requires strong structure:
either linear secondâ€“order elliptic operators ([Marcati and Schwab](https://arxiv.org/html/2511.01125v1#bib.bib66)Â [[66](https://arxiv.org/html/2511.01125v1#bib.bib66), [67](https://arxiv.org/html/2511.01125v1#bib.bib67)]) or analyticity of the *single* target solution, so that one may invoke classical exponential approximation of analytic functions by neural networks ([Mhaskar and Micchelli](https://arxiv.org/html/2511.01125v1#bib.bib69)Â [[69](https://arxiv.org/html/2511.01125v1#bib.bib69)], [Mhaskar](https://arxiv.org/html/2511.01125v1#bib.bib68)Â [[68](https://arxiv.org/html/2511.01125v1#bib.bib68)], [E and Wang](https://arxiv.org/html/2511.01125v1#bib.bib25)Â [[25](https://arxiv.org/html/2511.01125v1#bib.bib25)]).

On the other hand, neural operators (NOs) learn the infiniteâ€“dimensional coefficientâ€“toâ€“solution map and hence *simultaneously* solve all PDEs in a parametric class with a single model; see the early universality observation of [Chen and Chen](https://arxiv.org/html/2511.01125v1#bib.bib15)Â [[15](https://arxiv.org/html/2511.01125v1#bib.bib15)], the DeepONet/FNO line ([Lu, Jin, and Karniadakis](https://arxiv.org/html/2511.01125v1#bib.bib63)Â [[63](https://arxiv.org/html/2511.01125v1#bib.bib63)], [Kovachki, Li, Liu, Azizzadenesheli, Bhattacharya, Stuart, and Anandkumar](https://arxiv.org/html/2511.01125v1#bib.bib47)Â [[47](https://arxiv.org/html/2511.01125v1#bib.bib47)]), the CNO universality [RaoniÄ‡, Molinaro, deÂ Ryck, Rohner, Bartolucci, Alaifari, Mishra, and deÂ BÃ©zenac](https://arxiv.org/html/2511.01125v1#bib.bib81)Â [[81](https://arxiv.org/html/2511.01125v1#bib.bib81)], and a large set of abstract guarantees in Banach/Besov/Sobolev and nonâ€“linear metric settings
([Yu, Becquey, Halikias, Mallory, and Townsend](https://arxiv.org/html/2511.01125v1#bib.bib96)Â [[96](https://arxiv.org/html/2511.01125v1#bib.bib96)],
[Lu, Jin, Pang, Zhang, and Karniadakis](https://arxiv.org/html/2511.01125v1#bib.bib64)Â [[64](https://arxiv.org/html/2511.01125v1#bib.bib64)],
[Lanthaler, Mishra, and Karniadakis](https://arxiv.org/html/2511.01125v1#bib.bib56)Â [[56](https://arxiv.org/html/2511.01125v1#bib.bib56)],
[Adcock, Brugiapaglia, Dexter, and Moraga](https://arxiv.org/html/2511.01125v1#bib.bib2)Â [[2](https://arxiv.org/html/2511.01125v1#bib.bib2)],
[Korolev](https://arxiv.org/html/2511.01125v1#bib.bib46)Â [[46](https://arxiv.org/html/2511.01125v1#bib.bib46)],
[Cuchiero, Schmocker, and Teichmann](https://arxiv.org/html/2511.01125v1#bib.bib17)Â [[17](https://arxiv.org/html/2511.01125v1#bib.bib17)],
[Neufeld and Schmocker](https://arxiv.org/html/2511.01125v1#bib.bib72)Â [[72](https://arxiv.org/html/2511.01125v1#bib.bib72)],
[Kratsios, Furuya, Benitez, Lassas, and deÂ Hoop](https://arxiv.org/html/2511.01125v1#bib.bib50)Â [[50](https://arxiv.org/html/2511.01125v1#bib.bib50)],
[Adcock, Brugiapaglia, Dexter, and Moraga](https://arxiv.org/html/2511.01125v1#bib.bib4)Â [[4](https://arxiv.org/html/2511.01125v1#bib.bib4)],
[GÃ¶deke and Fernsel](https://arxiv.org/html/2511.01125v1#bib.bib38)Â [[38](https://arxiv.org/html/2511.01125v1#bib.bib38)],
[Lanthaler and Stuart](https://arxiv.org/html/2511.01125v1#bib.bib55)Â [[55](https://arxiv.org/html/2511.01125v1#bib.bib55)],
[Schwab, Stein, and Zech](https://arxiv.org/html/2511.01125v1#bib.bib85)Â [[85](https://arxiv.org/html/2511.01125v1#bib.bib85)],
[deÂ Hoop, Lassas, and Wong](https://arxiv.org/html/2511.01125v1#bib.bib20)Â [[20](https://arxiv.org/html/2511.01125v1#bib.bib20)],
[Furuya, Taniguchi, and Okuda](https://arxiv.org/html/2511.01125v1#bib.bib32)Â [[32](https://arxiv.org/html/2511.01125v1#bib.bib32)],
[Kratsios, Schmocker, and Zimmermann](https://arxiv.org/html/2511.01125v1#bib.bib52)Â [[52](https://arxiv.org/html/2511.01125v1#bib.bib52)],
[Acciaio, Kratsios, and Pammer](https://arxiv.org/html/2511.01125v1#bib.bib1)Â [[1](https://arxiv.org/html/2511.01125v1#bib.bib1)],
[Kratsios, Liu, Lassas, deÂ Hoop, and Dokmanic](https://arxiv.org/html/2511.01125v1#bib.bib49)Â [[49](https://arxiv.org/html/2511.01125v1#bib.bib49)]).
Within this line, *exponential* (sometimes â€˜exponentialâ€“inâ€“depthâ€™) expression rates are known for holomorphic operator classes ([Adcock, Dexter, and MoragaÂ Scheuermann](https://arxiv.org/html/2511.01125v1#bib.bib3)Â [[3](https://arxiv.org/html/2511.01125v1#bib.bib3)]), for certain linear elliptic PDEs (including polytopal domains) ([Marcati and Schwab](https://arxiv.org/html/2511.01125v1#bib.bib66)Â [[66](https://arxiv.org/html/2511.01125v1#bib.bib66), [67](https://arxiv.org/html/2511.01125v1#bib.bib67)]), and for specific semilinear elliptic equations on smooth domains ([Furuya and Kratsios](https://arxiv.org/html/2511.01125v1#bib.bib31)Â [[31](https://arxiv.org/html/2511.01125v1#bib.bib31)]).
Other exponential statements rely either on superâ€“expressive activations with effectively infinite pseudoâ€“dimension ([Shen, Yang, and Zhang](https://arxiv.org/html/2511.01125v1#bib.bib86)Â [[86](https://arxiv.org/html/2511.01125v1#bib.bib86)], [Pollard](https://arxiv.org/html/2511.01125v1#bib.bib79)Â [[79](https://arxiv.org/html/2511.01125v1#bib.bib79)], [Alvarez, Ekren, Kratsios, and Yang](https://arxiv.org/html/2511.01125v1#bib.bib5)Â [[5](https://arxiv.org/html/2511.01125v1#bib.bib5)]) or on implicit/equilibriumâ€“layer constructions exploiting convex variational structure ([Kratsios, Neufeld, and Schmocker](https://arxiv.org/html/2511.01125v1#bib.bib51)Â [[51](https://arxiv.org/html/2511.01125v1#bib.bib51)]).

Our contribution in this landscape is that we design a NO that *simultaneously* (i)(i) approximates the solution operator of a broad class of secondâ€“order elliptic PDEs/2BSDEs and (iâ€‹i)(ii) retains *exponentialâ€“inâ€“depth* rates in a substantially more general semi-linear regime than in the closest prior work.
Concretely

1. (i)(i)

   *familyâ€“level learning.*
   We approximate the coefficientâ€“toâ€“solution map Î“+\Gamma^{\text{$+$}} on a compact infinite family indexed by (f0,g)(f\_{0},g), hence a single training phase serves the whole family (PDEs and the associated 2BSDEs).
   For fully nonâ€“linear elliptic equations we obtain general operatorâ€“level approximability (algebraic rates) by combining quantitative NO universality on Besov/Sobolev scales ([Yu, Becquey, Halikias, Mallory, and Townsend](https://arxiv.org/html/2511.01125v1#bib.bib96)Â [[96](https://arxiv.org/html/2511.01125v1#bib.bib96)], [Lu, Jin, Pang, Zhang, and Karniadakis](https://arxiv.org/html/2511.01125v1#bib.bib64)Â [[64](https://arxiv.org/html/2511.01125v1#bib.bib64)], [Lanthaler, Mishra, and Karniadakis](https://arxiv.org/html/2511.01125v1#bib.bib56)Â [[56](https://arxiv.org/html/2511.01125v1#bib.bib56)], [Adcock, Brugiapaglia, Dexter, and Moraga](https://arxiv.org/html/2511.01125v1#bib.bib2)Â [[2](https://arxiv.org/html/2511.01125v1#bib.bib2)], [Korolev](https://arxiv.org/html/2511.01125v1#bib.bib46)Â [[46](https://arxiv.org/html/2511.01125v1#bib.bib46)], [Galimberti, Kratsios, and Livieri](https://arxiv.org/html/2511.01125v1#bib.bib33)Â [[33](https://arxiv.org/html/2511.01125v1#bib.bib33)]) with stability of the solution map (Krylovâ€“type assumptions; cf. [Krylov](https://arxiv.org/html/2511.01125v1#bib.bib54)Â [[54](https://arxiv.org/html/2511.01125v1#bib.bib54)]).
2. (iâ€‹i)(ii)

   *Exponential rates for semi-linear equations under *general* forward dynamics.*
   In the semi-linear case

   |  |  |  |
   | --- | --- | --- |
   |  | âˆ’âˆ‡â‹…Î³â€‹(x)â€‹âˆ‡u+Î¼â€‹(x)â‹…âˆ‡u+Î»â€‹(x)â€‹u+f~â€‹(x,u)=âˆ’f0â€‹(x),u|âˆ‚ğ’Ÿ=g,-\nabla\!\cdot\!\gamma(x)\nabla u+\mu(x)\!\cdot\!\nabla u+\lambda(x)u+\tilde{f}(x,u)=-f\_{0}(x),\;u|\_{\partial{\cal D}}=g, |  |

   with smooth, uniformly elliptic Î³\gamma and smooth Î¼\mu, Î»\lambda, we implement the classical fixedâ€“point map by a non-local NO layer built from (approximated) Green kernels; existence/regularity of Green functions for variableâ€“coefficient operators is standard ([Kim and Sakellaris](https://arxiv.org/html/2511.01125v1#bib.bib45)Â [[45](https://arxiv.org/html/2511.01125v1#bib.bib45)]).
   This yields accuracy Îµ\varepsilon with *logarithmic* depth L=Oâ€‹(logâ¡(1/Îµ))L=O(\log(1/\varepsilon)), *constant* width, and a finite non-local rank that scales polynomially in 1/Îµ1/\varepsilon.
   Compared to [Furuya and Kratsios](https://arxiv.org/html/2511.01125v1#bib.bib31)Â [[31](https://arxiv.org/html/2511.01125v1#bib.bib31)], which hardâ€“codes the singular part of the Greenâ€™s kernel and effectively assumes a driftless, constantâ€“diffusion forward (so that the singular Î¦\Phi is known in closed form), our construction does *not* require a closedâ€“form kernel split and therefore covers far more general, stateâ€“dependent ItÃ´ diffusions in the forward process and variableâ€“coefficient elliptic operators, while preserving exponential depthâ€“rates.
3. (iâ€‹iâ€‹i)(iii)

   *From PDE to (2)(2)BSDE at the operator level.*
   Because each 2BSDE in the family admits the PDE representation, our NO for the elliptic map transfers directly to a NO for the (Y,Z,Î¥,A)(Y,Z,\Upsilon,A)â€“processes associated with the *entire* 2BSDE family.

Building upon the finite-dimensional lower bounds ofÂ [Yarotsky](https://arxiv.org/html/2511.01125v1#bib.bib95) [[95](https://arxiv.org/html/2511.01125v1#bib.bib95)] , it was recently shown in [Lanthaler and Stuart](https://arxiv.org/html/2511.01125v1#bib.bib55) [[55](https://arxiv.org/html/2511.01125v1#bib.bib55)] that arbitrary continuousâ€”or even several times continuously FrÃ©chet differentiableâ€”non-linear operators between Sobolev spaces cannot be uniformly approximated on compact sets by NOs without requiring an exponential number of trainable parameters in the reciprocal approximation error.
Consequently, without additional structure beyond simple smoothness, there are insurmountable obstructions to operator learning. Thus, even if one could establish HÃ¶lder-continuity of the coefficient-to-solution operator in the fully non-linear setting (*e.g.* using results of [Taylor](https://arxiv.org/html/2511.01125v1#bib.bib88) [[88](https://arxiv.org/html/2511.01125v1#bib.bib88)], which we do show in [SectionËœA.4](https://arxiv.org/html/2511.01125v1#A1.SS4 "A.4 Stability estimate of general solution operator â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators")) the solution operator would still not be regular enough to permit meaningful approximation rates. In such cases, any quantitative result is practically no more meaningful than an existential statement on the approximability of the coefficient-to-solution operator (see [TheoremËœ3.7](https://arxiv.org/html/2511.01125v1#S3.Thmtheorem7 "Theorem 3.7 (Approximability of the perturbation-to-solution map). â€£ 3.2 General approximability guarantee â€£ 3 Main results â€£ One model to solve them all: 2BSDE families via neural operators")), akin to the qualitative (rate-free) universal abstract approximation results of [Chen and Chen](https://arxiv.org/html/2511.01125v1#bib.bib14) [[14](https://arxiv.org/html/2511.01125v1#bib.bib14)], [Benth, Detering, and Galimberti](https://arxiv.org/html/2511.01125v1#bib.bib8) [[8](https://arxiv.org/html/2511.01125v1#bib.bib8)], or [Bilokopytov and Xanthos](https://arxiv.org/html/2511.01125v1#bib.bib9) [[9](https://arxiv.org/html/2511.01125v1#bib.bib9)] for other NO architectures.

When it comes to the closest exponentialâ€“rate results available in the literature, relative to linear/holomorphic NO rates ([Marcati and Schwab](https://arxiv.org/html/2511.01125v1#bib.bib66)Â [[66](https://arxiv.org/html/2511.01125v1#bib.bib66), [67](https://arxiv.org/html/2511.01125v1#bib.bib67)], [Adcock, Dexter, and MoragaÂ Scheuermann](https://arxiv.org/html/2511.01125v1#bib.bib3)Â [[3](https://arxiv.org/html/2511.01125v1#bib.bib3)]), we require neither analyticity nor specialised domains; and unlike exponential claims relying on superâ€“expressive activations or implicit/equilibrium layers ([Shen, Yang, and Zhang](https://arxiv.org/html/2511.01125v1#bib.bib86)Â [[86](https://arxiv.org/html/2511.01125v1#bib.bib86)], [Pollard](https://arxiv.org/html/2511.01125v1#bib.bib79)Â [[79](https://arxiv.org/html/2511.01125v1#bib.bib79)], [Alvarez, Ekren, Kratsios, and Yang](https://arxiv.org/html/2511.01125v1#bib.bib5)Â [[5](https://arxiv.org/html/2511.01125v1#bib.bib5)], [Kratsios, Neufeld, and Schmocker](https://arxiv.org/html/2511.01125v1#bib.bib51)Â [[51](https://arxiv.org/html/2511.01125v1#bib.bib51)]), our architecture maintains finite capacity per layer with explicit depth/width/rank scaling.
Crucially, compared to [Furuya and Kratsios](https://arxiv.org/html/2511.01125v1#bib.bib31)Â [[31](https://arxiv.org/html/2511.01125v1#bib.bib31)], our exponential regime permits markedly *more general* forward dynamics and variableâ€“coefficient elliptic operators, because the Greenâ€“kernel is learned/approximated rather than injected in closed form.

## 2 Preliminaries

### 2.1 Notation

Let pâˆˆ(1,âˆ)p\in(1,\infty).
We denote by pâ€²âˆˆ(1,âˆ)p^{\prime}\in(1,\infty) the conjugate of pp such that 1/p+1/pâ€²=11/p+1/p^{\prime}=1. We let â„•\mathbb{N} be set of non-negative integers, â„•â‹†\mathbb{N}^{\star} the set of positive integers, and â„¤\mathbb{Z} the set of all negative and non-negative integers. We henceforth fix an ambient dimension555InÂ [[31](https://arxiv.org/html/2511.01125v1#bib.bib31)] an explicit expression for the singular part of the Greenâ€™s function associated to the stopped forward processâ€™s induced elliptic PDE was required, which additionally constrained dâ‰¥3d\geq 3 there, but not herein. dâˆˆâ„•â‹†d\in\mathbb{N}^{\star}; and let ğ•Šd+\mathbb{S}\_{d}^{\text{$+$}} denote the set of dÃ—dd\times d (real) positive-definite matrices. Recall that, every symmetric positive definite matrix Aâˆˆğ•Šd+A\in\mathbb{S}\_{d}^{+} has a unique well-defined square-root given by Aâ‰”logâ¡(expâ¡(A)/2)\sqrt{A}\coloneqq\log(\exp(A)/2) where exp\exp is the matrix exponential and log\log is its (unique) inverse on ğ•Šd+\mathbb{S}\_{d}^{\text{$+$}}, see *e.g*. [Arabpour, Armstrong, Galimberti, Kratsios, and Livieri](https://arxiv.org/html/2511.01125v1#bib.bib6) [[6](https://arxiv.org/html/2511.01125v1#bib.bib6), Lemma C.5]. For any dâˆˆâ„•â‹†d\in\mathbb{N}^{\star} denote the FrÃ¶benius norm of any dÃ—dd\times d matrix AA by â€–Aâ€–F\|A\|\_{\text{$F$}}. Given any metric space (ğ’³,Ï)(\mathcal{X},\rho), any xâˆˆğ’³x\in\mathcal{X}, and any radius râ‰¥0r\geq 0, we define the open ball B(ğ’³,Ï)â€‹(x,r)â‰”{uâˆˆğ’³:Ïâ€‹(x,u)<r}B\_{(\mathcal{X},\rho)}(x,r)\coloneqq\{u\in\mathcal{X}:\rho(x,u)<r\}. Given any two vector spaces VV and WW, and any xâˆˆVx\in V and yâˆˆWy\in W, we write xâŠ•yâ‰”(x,y)=VÃ—Wx\oplus y\coloneqq(x,y)=V\times W.

For any pâ‰¥1p\geq 1, we let â„“pâ€‹(â„¤)\ell^{p}(\mathbb{Z}) be the set of real-valued sequences (un)nâˆˆâ„¤(u\_{n})\_{n\in\mathbb{Z}} indexed by â„¤\mathbb{Z} such that

|  |  |  |
| --- | --- | --- |
|  | âˆ‘nâˆˆâ„¤|un|p<âˆ.\sum\_{n\in\mathbb{Z}}|u\_{n}|^{p}<\infty. |  |

We also let ğ•ƒpâ€‹(â„)\mathbb{L}^{p}(\mathbb{R}) be the set of pp-integrable Lebesgue-measurable functions on â„\mathbb{R}.

For any Iâˆˆâ„•I\in\mathbb{N}, we use CIâ€‹(â„)C^{I}(\mathbb{R}) to denote the vector space of real-valued at-least II-times continuously differentiable functions on â„\mathbb{R}, and CcIâ€‹(â„)C\_{c}^{I}(\mathbb{R}) for the subset thereof consisting of those compactly supported functions therein.
For any (s,d,D)âˆˆ(â„•â‹†)3(s,d,D)\in(\mathbb{N}^{\star})^{3}, we write Csâ€‹(ğ’Ÿ,â„D)C^{s}(\mathcal{D},\mathbb{R}^{D}) (resp. Câˆâ€‹(ğ’Ÿ,â„D)C^{\infty}(\mathcal{D},\mathbb{R}^{D})) for set of functions from â„d\mathbb{R}^{d} to â„D\mathbb{R}^{D} which are at-least ss-times (resp. smooth) continuously differentiable when restricted to ğ’Ÿ\mathcal{D}. We refer the reader to AppendixÂ [A.2.3](https://arxiv.org/html/2511.01125v1#A1.SS2.SSS3 "A.2.3 Besov spaces on domains â€£ A.2 Proof of Theorem 3.7 â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators") for wavelet-centric definitions of Besov, and thus Sobolev, spaces.

Throughout this paper, (Î©,â„±,ğ”½â‰”(â„±t)tâ‰¥0,â„™)(\Omega,\mathcal{F},\mathbb{F}\coloneqq(\mathcal{F}\_{t})\_{t\geq 0},\mathbb{P}) will denote a filtered probability space satisfying the usual conditions.
For any T>0T>0 we use â„‹T2\mathcal{H}\_{T}^{2} to denote the class of square-integrable predictable processes X:[0,T]Ã—Î©âŸ¶â„X:[0,T]\times\Omega\longrightarrow\mathbb{R}.

### 2.2 Deep learning

Neural operators (NOs) extend deep learning from finite-dimensional vector spaces to infinite-dimensional Banach spaces, with standard NOs specialising in function-to-function mappings. Broadly speaking, there are three types of NO builds between function spaces: the Fourier neural operatorâ€“type builds (FNO), which iteratively use finitely parametrised integral-kernel affine transformations between their non-linearities; DeepONet-type architectures (see [Lu, Jin, and Karniadakis](https://arxiv.org/html/2511.01125v1#bib.bib63) [[63](https://arxiv.org/html/2511.01125v1#bib.bib63)]) which learn to adaptively regress against learnable bases; and encoderâ€“-processorâ€“-decoder-type models, such as PCAâ€“Net (see [Chan, Jia, Gao, Lu, Zeng, and Ma](https://arxiv.org/html/2511.01125v1#bib.bib12) [[12](https://arxiv.org/html/2511.01125v1#bib.bib12)]) which project infinite-dimensional data using a Schauder basis before processing it via a standard finite-dimensional neural network, and then reassembles finite-dimensional basis functions using the networkâ€™s outputs as coefficients.

The first and last of these models tend to be more numerically stable, the middle construction can exhibit advantageous approximation rates, and the third model is more readily generalisable to nonâ€“-function space settings (see *e.g.* [Galimberti, Kratsios, and Livieri](https://arxiv.org/html/2511.01125v1#bib.bib33) [[33](https://arxiv.org/html/2511.01125v1#bib.bib33)])
by directly lifting the approximation guarantees for classical neural networks (see e.g.Â [Yarotsky](https://arxiv.org/html/2511.01125v1#bib.bib95), [Bolcskei, Grohs, Kutyniok, and Petersen](https://arxiv.org/html/2511.01125v1#bib.bib10), [DeVore, Hanin, and Petrova](https://arxiv.org/html/2511.01125v1#bib.bib23), [Gribonval, Kutyniok, Nielsen, and Voigtlaender](https://arxiv.org/html/2511.01125v1#bib.bib39), [Kratsios and Zamanlooy](https://arxiv.org/html/2511.01125v1#bib.bib53), [Shen, Yang, and Zhang](https://arxiv.org/html/2511.01125v1#bib.bib86), [Hong and Kratsios](https://arxiv.org/html/2511.01125v1#bib.bib41), [Schneider, Ullrich, and Vybiral](https://arxiv.org/html/2511.01125v1#bib.bib84)) to infinite dimensions. Our neural-operator build combines the best of the first two models using a two-branch structure: the top branch of an FNO-type, the bottom branch inspired by DeepONets, with coefficients shared between layers. Moreover, we map into nonâ€“-function space targets when applying our deep-learning model in the 2BSDE setting by transforming its function spaceâ€“valued outputs into processes via a â€˜Feynmanâ€“-Kac adapterâ€™, that is to say a custom non-trainable readout layer encoding our nonlinear Feynman-â€“Kac representation ([SectionËœ3.1](https://arxiv.org/html/2511.01125v1#S3.SS1 "3.1 Elliptic PDE representation of the 2BSDE system â€£ 3 Main results â€£ One model to solve them all: 2BSDE families via neural operators")). Finally, we allow the non-linearities injecting structure at each layer of our NO to be adaptive rather than fixed, as in classical NO builds, thereby maximizing their flexibility, for instance granting them the ability to exactly perform multiplication, a property not shared by classical piecewise-linear ReLU activation functions.

#### 2.2.1 Residual Kolmogorovâ€“Arnold networks (Resâ€“KANs)

The key idea behind Kolmogorovâ€“Arnold networks (KANs) is to make the activation function itself trainable. In KANs, one typically focuses on the spline part of the following definition [Liu, Wang, Vaidya, Ruehle, Halverson, Soljacic, Hou, and Tegmark](https://arxiv.org/html/2511.01125v1#bib.bib62) [[62](https://arxiv.org/html/2511.01125v1#bib.bib62)], with the role of the remaining part of the activation function being an afterthought, normally taken to some standard non-linearity such as the Swish or Sigmoid functions. In this paper, we explicitly exploit both parts of KANs activation functions, and as such, we add some basic structural requirements to the â€˜non-splineâ€™ part of the activation function (below inÂ ([2.1](https://arxiv.org/html/2511.01125v1#S2.E1 "Equation 2.1 â€£ 2.2.1 Residual Kolmogorovâ€“Arnold networks (Resâ€“KANs) â€£ 2.2 Deep learning â€£ 2 Preliminaries â€£ One model to solve them all: 2BSDE families via neural operators"))) which serves a pointed role in our approximation theory in connection with the multi-resolution analysis (MRA); see *e.g*. [Mallat](https://arxiv.org/html/2511.01125v1#bib.bib65) [[65](https://arxiv.org/html/2511.01125v1#bib.bib65)].

Specifically, the activation ÏƒÎ²:I:â„âŸ¶â„\sigma\_{\beta:\text{$I$}}:\mathbb{R}\longrightarrow\mathbb{R} maps any xâˆˆâ„x\in\mathbb{R} to a mixture of spline basis functions of varying degrees

|  |  |  |  |
| --- | --- | --- | --- |
|  | ÏƒÎ²:Iâ€‹(x)â‰”Î²âˆ’1â€‹ÏƒSâ€‹(x)+Î²0â€‹ÏƒWâ€‹(x)âŸSpectral structure+âˆ‘i=1IÎ²iâ€‹ğ’©iâ€‹(x)âŸLocal structure\sigma\_{\beta:\text{$I$}}(x)\coloneqq\underbrace{\beta\_{\text{$-$}1}\sigma\_{\text{$S$}}(x)+\beta\_{0}\sigma\_{\text{$W$}}(x)}\_{\text{\tiny Spectral structure}}+\underbrace{\sum\_{i=1}^{\text{$I$}}\beta\_{i}\mathcal{N}\_{i}(x)}\_{\text{\tiny Local structure}} |  | (2.1) |

where Iâˆˆâ„•I\in\mathbb{N}, Î²=(Î²âˆ’1,Î²0,â‹…,Î²I)âŠ¤âˆˆâ„I+2\beta=(\beta\_{\text{$-$}1},\beta\_{0},\cdot,\beta\_{\text{$I$}})^{\top}\in\mathbb{R}^{\text{$I$}+2} is a trainable vector of coefficients, and where for iâˆˆ{1,â€¦,I}i\in\{1,\dots,I\}, ğ’©i:â„âŸ¶â„\mathcal{N}\_{i}:\mathbb{R}\longrightarrow\mathbb{R} are the cardinal B-splines which, following [Mhaskar and Micchelli](https://arxiv.org/html/2511.01125v1#bib.bib70) [[70](https://arxiv.org/html/2511.01125v1#bib.bib70), Equation (4.28)], can be defined by ğ’©0â€‹(x)â‰”ğŸ[0,1)\mathcal{N}\_{0}(x)\coloneqq\mathbf{1}\_{[0,1)} and for any iâˆˆâ„•â‹†i\in\mathbb{N}^{\star}

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ’©Iâ€‹(x)â‰”âˆ‘j=0I+â€‹1(âˆ’1)jâ€‹(I+1j)I!â€‹ReLUâ€‹(xâˆ’j)I,xâˆˆâ„.\mathcal{N}\_{I}(x)\coloneqq\sum\_{j=0}^{\text{$I$}\text{$+$}1}\frac{(-1)^{j}\binom{I+1}{j}}{I!}\mathrm{ReLU}(x-j)^{I},\;x\in\mathbb{R}. |  | (2.2) |

Furthermore, ÏƒS:â„âŸ¶â„\sigma\_{\text{$S$}}:\mathbb{R}\longrightarrow\mathbb{R} as well as ÏƒW:â„âŸ¶â„\sigma\_{\text{$W$}}:\mathbb{R}\longrightarrow\mathbb{R} and satisfy the spectral properties in [SectionËœ2.2.1](https://arxiv.org/html/2511.01125v1#S2.SS2.SSS1 "2.2.1 Residual Kolmogorovâ€“Arnold networks (Resâ€“KANs) â€£ 2.2 Deep learning â€£ 2 Preliminaries â€£ One model to solve them all: 2BSDE families via neural operators") below. However, before turning to the properties, we elucidate the first few wavelets in [FigureËœ1](https://arxiv.org/html/2511.01125v1#S2.F1 "In 2.2.1 Residual Kolmogorovâ€“Arnold networks (Resâ€“KANs) â€£ 2.2 Deep learning â€£ 2 Preliminaries â€£ One model to solve them all: 2BSDE families via neural operators").

![Refer to caption](x1.png)


Figure 1: The cardinal BB-splines of orders I=0,1I=0,1, and 22.

###### Assumption 2.1 (Daubechies properties of order II).

Fix Iâˆˆâ„•I\in\mathbb{N}. The respective â€˜`scaleâ€™ and â€˜`waveletâ€™ activation function ÏƒS\sigma\_{\text{$S$}}
and ÏƒW{\sigma\_{\text{$W$}}} both belong to CcIâ€‹(â„)C^{I}\_{c}(\mathbb{R}) if I>0I>0 ((resp. L2â€‹(â„)L^{2}(\mathbb{R}) when I=0I=0 with compact essential support)) and satisfy the refinement equation of [Daubechies](https://arxiv.org/html/2511.01125v1#bib.bib18) [[18](https://arxiv.org/html/2511.01125v1#bib.bib18), Equation (3.47)], that is to say that there is a sequence of *low-pass filters* (hk)kâˆˆâ„¤âˆˆâ„“2â€‹(â„¤)(h\_{k})\_{k\in\mathbb{Z}}\in\ell^{2}(\mathbb{Z}) summing to 2\sqrt{2}, satisfying the *orthogonality condition*666SeeÂ [[18](https://arxiv.org/html/2511.01125v1#bib.bib18), Equation (3.18)]

|  |  |  |
| --- | --- | --- |
|  | âˆ‘kâˆˆâ„¤hkâˆ’2â€‹iâ€‹hkâˆ’2â€‹j=ğŸ{i=j},âˆ€(i,j)âˆˆâ„¤2,\sum\_{k\in\mathbb{Z}}h\_{k\text{$-$}2i}h\_{k\text{$-$}2j}=\mathbf{1}\_{\{i=j\}},\;\forall(i,j)\in\mathbb{Z}^{2}, |  |

and such that

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | ÏƒSâ€‹(x)\displaystyle\sigma\_{\text{$S$}}(x) | =2â€‹âˆ‘kâˆˆâ„¤hkâ€‹ÏƒSâ€‹(2â€‹xâˆ’k),xâˆˆâ„,\displaystyle=\sqrt{2}\sum\_{k\in\mathbb{Z}}h\_{k}\sigma\_{\text{$S$}}(2x-k),\;x\in\mathbb{R}, |  | (2.3) |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | ÏƒWâ€‹(x)\displaystyle\sigma\_{\text{$W$}}(x) | =2â€‹âˆ‘kâˆˆâ„¤(âˆ’1)kâ€‹h1âˆ’kâ€‹ÏƒSâ€‹(2â€‹xâˆ’k),xâˆˆâ„.\displaystyle=\sqrt{2}\sum\_{k\in\mathbb{Z}}(-1)^{k}h\_{1\text{$-$}k}\sigma\_{\text{$S$}}(2x-k),\;x\in\mathbb{R}. |  | (2.4) |

The existence of such activation functions (called Daubechies father and mother wavelets respectively), for arbitrary II, is guaranteed byÂ [Triebel](https://arxiv.org/html/2511.01125v1#bib.bib89) [[89](https://arxiv.org/html/2511.01125v1#bib.bib89), Theorem 1.61.(iâ€‹i)(ii)], while algorithmic constructions can be found inÂ [Daubechies](https://arxiv.org/html/2511.01125v1#bib.bib19) [[19](https://arxiv.org/html/2511.01125v1#bib.bib19), Chapter 6.4], and are standard in modern signal processing. Nevertheless, in the very low regularity regime where I=0I=0, the Haar system and the indicator function is a transparent example where [SectionËœ2.2.1](https://arxiv.org/html/2511.01125v1#S2.SS2.SSS1 "2.2.1 Residual Kolmogorovâ€“Arnold networks (Resâ€“KANs) â€£ 2.2 Deep learning â€£ 2 Preliminaries â€£ One model to solve them all: 2BSDE families via neural operators") holds.

###### Example 2.2 (Haar wavelets and indicator function for discontinuous regularity).

If I=0I=0 then, the indicator function of the unit interval ÏƒSâ‰”ğŸ[0,1)\sigma\_{\text{$S$}}\coloneqq\mathbf{1}\_{[0,1)} and the Haar wavelet ÏƒMâ‰”ğŸ[0,1/2)âˆ’ğŸ[1/2,1)\sigma\_{M}\coloneqq\mathbf{1}\_{[0,1/2)}-\mathbf{1}\_{[1/2,1)} satisfy [SectionËœ2.2.1](https://arxiv.org/html/2511.01125v1#S2.SS2.SSS1 "2.2.1 Residual Kolmogorovâ€“Arnold networks (Resâ€“KANs) â€£ 2.2 Deep learning â€£ 2 Preliminaries â€£ One model to solve them all: 2BSDE families via neural operators") with h0=h1=12h\_{0}=h\_{1}=\frac{1}{\sqrt{2}} and hk=0h\_{k}=0 whenever |k|â‰¥2|k|\geq 2. Thus, ÏƒM\sigma\_{M} and ÏƒS\sigma\_{S} belong to L2â€‹(â„)L^{2}(\mathbb{R}) as expected since I=0I=0.

In a KAN, this activation operates component-wise, with parameters tailored to each neuron. That is, for any integer kk, any xâˆˆâ„kx\in\mathbb{R}^{k}, and Î²â‰”(Î²1,â€¦,Î²k)âˆˆâ„(I+2)Ã—k\mathbf{\beta}\coloneqq(\beta\_{1},\dots,\beta\_{k})\in\mathbb{R}^{(I+2)\times k}, we define

|  |  |  |  |
| --- | --- | --- | --- |
|  | ÏƒÎ²:Iâˆ™:â„k\displaystyle\sigma\_{\mathbf{\beta}:\text{$I$}}\bullet:\mathbb{R}^{k} | âŸ¶â„k\displaystyle\longrightarrow\mathbb{R}^{k} |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | x=(x1,â€¦,xk)âŠ¤\displaystyle x=(x\_{1},\dots,x\_{k})^{\top} | âŸ¼(ÏƒÎ²1:Iâ€‹(x1),â€¦,ÏƒÎ²k:Iâ€‹(xk))âŠ¤.\displaystyle\longmapsto\big(\sigma\_{\beta\_{\text{$1$}}:\text{$I$}}(x\_{1}),\dots,\sigma\_{\beta\_{\text{$k$}}:\text{$I$}}(x\_{k})\big)^{\top}. |  | (2.5) |

We now introduce the core idea of *residual* KAN networks. These networks incorporate an additional *residual connection*, ensuring that signal is preserved during activation. Residual connections, standard in modern deep learning architectures, help stabilise training by preserving gradient flow and regularising the loss landscape, see [Riedi, Balestriero, and Baraniuk](https://arxiv.org/html/2511.01125v1#bib.bib83) [[83](https://arxiv.org/html/2511.01125v1#bib.bib83)]. They also mitigate vanishing gradients that can be caused by normalisation layers. Following [Acciaio, Kratsios, and Pammer](https://arxiv.org/html/2511.01125v1#bib.bib1) [[1](https://arxiv.org/html/2511.01125v1#bib.bib1)], we allow for flexible use of these residual paths, potentially modulated by a trainable gating mechanism.

More precisely, we fix positive integers doutd\_{\mathrm{out}} and dind\_{\mathrm{in}}, matrices (A,G)âˆˆâ„doutÃ—dinÃ—â„doutÃ—din(A,G)\in\mathbb{R}^{d\_{\text{$\rm out$}}\times d\_{\text{$\rm in$}}}\times\mathbb{R}^{d\_{\text{$\rm out$}}\times d\_{\text{$\rm in$}}}, with GG being diagonal (*i.e.* Gi,j=0G\_{i,j}=0 for (i,j)âˆˆ{1,â€¦,dout}Ã—{1,â€¦,din}(i,j)\in\{1,\dots,d\_{\rm out}\}\times\{1,\dots,d\_{\rm in}\} with iâ‰ ji\neq j), as well as bâˆˆâ„doutb\in\mathbb{R}^{d\_{\text{$\mathrm{out}$}}}, and Î²âˆˆâ„(I+2)Ã—dout\beta\in\mathbb{R}^{({I}\text{$+$}2)\times d\_{\text{$\mathrm{out}$}}}, a matrix of trainable coefficients. We then define for xâˆˆâ„dinx\in\mathbb{R}^{d\_{\text{$\rm in$}}}

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„’(x|A,b,Î²,G:I)â‰”ÏƒÎ²:Iâˆ™(Aâ€‹x+b)âŸKAN layer+Gâ€‹xâŸResidual connection\mathcal{L}(x|A,b,\beta,G:I)\coloneqq\underbrace{\sigma\_{\mathbf{\beta}:\text{$I$}}\bullet(Ax+b)}\_{\text{\tiny KAN layer}}+\underbrace{Gx}\_{\text{\tiny Residual connection}} |  | (2.6) |

Although compositions of such KAN layers define valid functions, these may lack higher-order smoothnessâ€”an issue for applications such as PDE solving that require high regularity. There are two ways to address this: (1) enforce that Î²i=0\beta\_{i}=0 for small ii, or (2) apply a smoothing layer at the output. We adopt the first strategy to ensure that the functions realised by our *smoothed residual KANs* are infinitely differentiable.

###### Definition 2.3 (Residual KANs (Resâ€“KANs)).

Let DD and II be positive integers, and let Î±>0\alpha>0. A residual Kolmogorovâ€“Arnold network ((Resâ€“KAN)) is a function f^:â„dâŸ¶â„D\widehat{f}:\mathbb{R}^{d}\longrightarrow\mathbb{R}^{D} with
representation, for some Lâˆˆâ„•â‹†L\in\mathbb{N}^{\star}

|  |  |  |  |
| --- | --- | --- | --- |
|  | f^=A(L)â€‹f(L)+b(L),\widehat{f}=A^{(L)}f^{(L)}+b^{(L)}, |  | (2.7) |

with

|  |  |  |
| --- | --- | --- |
|  | f(0)(x)=x,xâˆˆâ„d,f(â„“)=â„’(f(â„“âˆ’1)|A(â„“),b(â„“),Î²(â„“),G(â„“):I),â„“âˆˆ{1,â€¦,L},f^{(0)}(x)=x,\;x\in\mathbb{R}^{d},\;f^{(\ell)}=\mathcal{L}\big(f^{(\ell-1)}|A^{(\ell)},b^{(\ell)},\beta^{(\ell)},G^{(\ell)}:I\big),\;\ell\in\{1,\dots,L\}, |  |

where, for â„“âˆˆ{1,â€¦,L}\ell\in\{1,\dots,L\}, A(â„“)A^{(\ell)} and G(â„“)G^{(\ell)} are dâ„“+1Ã—dâ„“d\_{\ell+1}\times d\_{\ell} matrices with G(â„“)G^{(\ell)} diagonal, Î²(â„“)\beta^{(\ell)} is a (I+2)Ã—dâ„“+1(I+2)\times d\_{{\ell}{+}{1}} matrix, bâˆˆâ„dâ„“+1b\in\mathbb{R}^{d\_{\text{$\ell$}\text{$+$}\text{$1$}}}, for given positive integers (d0,â€¦,dL+1)(d\_{0},\dots,d\_{L+1}) satisfying d0=dd\_{0}=d and dL+1=Dd\_{L+1}=D. In addition, for any â„“âˆˆ{1,â€¦,L}\ell\in\{1,\dots,L\}, Î²(â„“)\beta^{(\ell)} satisfies the *sparsity* pattern ensuring smoothness777The âŒˆÎ±âŒ‰\lceil\alpha\rceil-time continuous differentiability of f^\hat{f} follows from that of B-splines (see [DeVore and Sharpley](https://arxiv.org/html/2511.01125v1#bib.bib22) [[22](https://arxiv.org/html/2511.01125v1#bib.bib22)]), and the chain rule.

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î²i,j(â„“)=0,i<âŒˆÎ±âŒ‰â€‹and,jâˆˆ{1,â€¦,dâ„“+1}.\beta^{(\ell)}\_{i,j}=0,\;i<\lceil\alpha\rceil{\;\mbox{\rm and},\;j\in\{1,\dots,d\_{\ell+1}\}}. |  | (2.8) |

We denote the class of all Resâ€“KANs with LL hidden layers, width Wâ‰”maxâ„“âˆˆ{1,â€¦,L+1}â¡dâ„“W\coloneqq\max\_{\ell\in\{1,\dots,L+1\}}d\_{\ell}, adaptivity parameter II, and smoothness parameter Î±\alpha, by Resâˆ’âˆ’KANL,WI,Î±â¡(â„d,â„D)\operatorname{Res--KAN}\_{\text{$L$},\text{$W$}}^{\text{$I$},\alpha}(\mathbb{R}^{d},\mathbb{R}^{D}).

#### 2.2.2 Neural operator architectures

We recall that we have fixed a constant 1<p<âˆ1<p<\infty and ğ’ŸâŠ‚â„d\mathcal{D}\subset\mathbb{R}^{d}, a bounded open domain. The classical neural operators are defined in, *e.g.*, [Kovachki, Li, Liu, Azizzadenesheli, Bhattacharya, Stuart, and Anandkumar](https://arxiv.org/html/2511.01125v1#bib.bib47) [[47](https://arxiv.org/html/2511.01125v1#bib.bib47)] or [Lanthaler, Li, and Stuart](https://arxiv.org/html/2511.01125v1#bib.bib57) [[57](https://arxiv.org/html/2511.01125v1#bib.bib57)].

Importantly, our NO architecture (see [FigureËœ2](https://arxiv.org/html/2511.01125v1#S2.F2 "In 2.2.2 Neural operator architectures â€£ 2.2 Deep learning â€£ 2 Preliminaries â€£ One model to solve them all: 2BSDE families via neural operators")) contains both encoderâ€“processorâ€“decoder (EPD) type and Fourier neural operator (FNO)-type â€˜branchesâ€™ at each layer, whereby spectral features and physical features are iteratively processed in parallel, and then combined together using the adaptively activated neurons spearheaded by the KAN paradigmÂ [[48](https://arxiv.org/html/2511.01125v1#bib.bib48)], rather than the static activation strategy of classical MLPs. The resulting architecture thus exhibits beneficial properties both of FNO-type models and EPD-type models.

![Refer to caption](x2.png)


Figure 2: The KANO ([SectionËœ2.2.2](https://arxiv.org/html/2511.01125v1#S2.SS2.SSS2 "2.2.2 Neural operator architectures â€£ 2.2 Deep learning â€£ 2 Preliminaries â€£ One model to solve them all: 2BSDE families via neural operators")) pipeline.

What is illustrated in [FigureËœ2](https://arxiv.org/html/2511.01125v1#S2.F2 "In 2.2.2 Neural operator architectures â€£ 2.2 Deep learning â€£ 2 Preliminaries â€£ One model to solve them all: 2BSDE families via neural operators") is as follows.

* 0)0)

  First boundary data (g)(g) and the PDE structure (G0)(G\_{0}) are concatenated into an input v0v\_{0}.
* 1)1)

  Learnable spectral features akin to FNOs are then extracted from v0v\_{0} and concatenated thereto.
* 2)2)

  At each processing iteration, the top NO branch applies a finite rank (R) integral operator, then all features are mixed and adaptively activated.
* 3)3)

  Finally the predictions are decoded via two branches: one applying another finite rank integral operator together as with to the FNO and the other leveraging a (trainable) spectral feature decoding akin to EPD, before both branches are mixed together to obtain the final prediction Î“^\hat{\Gamma}.

In the 2FBNO variant ([SectionËœ2.2.2](https://arxiv.org/html/2511.01125v1#S2.SS2.SSS2 "2.2.2 Neural operator architectures â€£ 2.2 Deep learning â€£ 2 Preliminaries â€£ One model to solve them all: 2BSDE families via neural operators")): Î“^â€‹(v0)\hat{\Gamma}(v\_{0}) is passed through the Feynmanâ€“Kac adapter (see [SectionËœ3.1](https://arxiv.org/html/2511.01125v1#S3.SS1 "3.1 Elliptic PDE representation of the 2BSDE system â€£ 3 Main results â€£ One model to solve them all: 2BSDE families via neural operators")).

This being said, we can now proceed with the definition. In the remainder of the paper din=2d\_{\mathrm{in}}=2, any tuple voutâˆˆW1,âˆâ€‹(ğ’Ÿ;â„)dinv\_{\mathrm{out}}\in W^{1,\infty}(\mathcal{D};\mathbb{R})^{d\_{\text{$\mathrm{in}$}}} will correspond to a pair of boundary and source data (g,f0)(g,f\_{0}), and dout=1d\_{\mathrm{out}}=1. However, since many of these result can be use in more general approximation theory of solutions operators to other PDEs, we keep the definition of our KANO model general enough to accomodate other applications.

###### Definition 2.4 (Kolmogorovâ€“Arnold neural operator (KANO)).

Fix positive integers dind\_{\mathrm{in}}, doutd\_{\mathrm{out}}, LL, WW, L^\widehat{L}, W^\widehat{W}, DadaD\_{\mathrm{ada}}, WadaW\_{\mathrm{ada}}, as well as smoothness parameters Î±>0\alpha>0 and Iâˆˆâ„•â‹†I\in\mathbb{N}^{\star} with 3â‰¤Î±â‰¤I3\leq\alpha\leq I.
We define a *Kolmogorovâ€“Arnold neural operator (KANO)* Î“^:W1,âˆâ€‹(ğ’Ÿ;â„)dinâŸ¶W1,âˆâ€‹(ğ’Ÿ;â„)dout\widehat{\Gamma}:W^{1,\infty}(\mathcal{D};\mathbb{R})^{d\_{\text{$\mathrm{in}$}}}\longrightarrow W^{1,\infty}(\mathcal{D};\mathbb{R})^{d\_{\text{$\mathrm{out}$}}} to be any map sending any vinâˆˆW1,âˆâ€‹(ğ’Ÿ,â„)dinv\_{\mathrm{in}}\in W^{1,\infty}(\mathcal{D},\mathbb{R})^{d\_{\text{$\mathrm{in}$}}} to some vL+1âˆˆW1,âˆâ€‹(ğ’Ÿ;â„)doutv\_{L+1}\in W^{1,\infty}(\mathcal{D};\mathbb{R})^{d\_{\text{$\mathrm{out}$}}} where vL+1v\_{L+1} is defined iteratively by

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | v0â€‹(x)\displaystyle v\_{0}(x) | â‰”(v0crsâ€‹(x)v0adaâ€‹(x))â‰”(vinâ€‹(x)âˆ«â„dvinâ€‹(y)âŠ¤â€‹Î¨^1:inâ€‹(y)â€‹dyâ‹®âˆ«â„dvinâ€‹(y)âŠ¤â€‹Î¨^K:inâ€‹(y)â€‹dy),xâˆˆğ’Ÿ,\displaystyle\coloneqq\begin{pmatrix}v\_{0}^{\mathrm{crs}}(x)\\[5.0pt] v\_{0}^{\mathrm{ada}}(x)\end{pmatrix}\coloneqq\begin{pmatrix}v\_{\mathrm{in}}(x)\\ \displaystyle\int\_{\mathbb{R}^{\text{$d$}}}v\_{\mathrm{in}}(y)^{\top}\widehat{\Psi}\_{1:\mathrm{in}}(y)\mathrm{d}y\\ \vdots\\ \displaystyle\int\_{\mathbb{R}^{\text{$d$}}}v\_{\mathrm{in}}(y)^{\top}\widehat{\Psi}\_{\text{$K$}:\mathrm{in}}(y)\mathrm{d}y\end{pmatrix},\;x\in{\cal D}, |  | (2.9) |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | vâ„“+1â€‹(x)\displaystyle v\_{\ell\text{$+$}1}(x) | â‰”(vâ„“+1crsâ€‹(x)vâ„“+1adaâ€‹(x))â‰”ÏƒÎ²â„“:Iâˆ™(Wâ„“â€‹(vâ„“crsâ€‹(x)+(K(â„“)â€‹vâ„“)â€‹(x)vâ„“adaâ€‹(x))+bâ„“â€‹(x)),â„“âˆˆ{0,â€¦,Lâˆ’1},xâˆˆğ’Ÿ,\displaystyle\coloneqq\begin{pmatrix}v\_{\ell+1}^{\mathrm{crs}}(x)\\[5.0pt] v\_{\ell+1}^{\mathrm{ada}}(x)\end{pmatrix}\coloneqq\sigma\_{\mathbf{\beta\_{\ell}}:I}\bullet\Bigg(W^{\ell}\begin{pmatrix}v\_{\ell}^{\mathrm{crs}}(x)+\big(K^{(\ell)}v\_{\ell}\big)(x)\\[5.0pt] v\_{\ell}^{\mathrm{ada}}(x)\end{pmatrix}+b^{\ell}(x)\Bigg),\;\ell\in\{0,\dots,L-1\},\;x\in{\cal D}, |  | (2.10) |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | vL+â€‹1â€‹(x)\displaystyle v\_{\text{$L$}\text{$+$}1}(x) | â‰”W(L)â€‹(vLcrsâ€‹(x)+(K(L)â€‹vL)â€‹(x)(vLada)âŠ¤â€‹(x)â€‹(Î¨^1:outâ€‹(x)â‹®Î¨^K:outâ€‹(x)))+b(L)â€‹(x),xâˆˆğ’Ÿ,\displaystyle\coloneqq W^{(\text{$L$})}\begin{pmatrix}v\_{\text{$L$}}^{\mathrm{crs}}(x)+\big(K^{({L})}v\_{\text{$L$}}\big)(x)\\[5.0pt] \big(v\_{\text{$L$}}^{\mathrm{ada}}\big)^{\top}(x)\begin{pmatrix}\widehat{\Psi}\_{1:\mathrm{out}}(x)\\ \vdots\\ \widehat{\Psi}\_{\text{$K$}:\mathrm{out}}(x)\end{pmatrix}\end{pmatrix}+b^{({L})}(x),\;x\in{\cal D}, |  | (2.11) |

where ÏƒÎ²:I\sigma\_{\mathbf{\beta}:I} is as inÂ [SectionËœ2.2.1](https://arxiv.org/html/2511.01125v1#S2.Ex3 "2.2.1 Residual Kolmogorovâ€“Arnold networks (Resâ€“KANs) â€£ 2.2 Deep learning â€£ 2 Preliminaries â€£ One model to solve them all: 2BSDE families via neural operators") and acts as inÂ ([2.1](https://arxiv.org/html/2511.01125v1#S2.E1 "Equation 2.1 â€£ 2.2.1 Residual Kolmogorovâ€“Arnold networks (Resâ€“KANs) â€£ 2.2 Deep learning â€£ 2 Preliminaries â€£ One model to solve them all: 2BSDE families via neural operators")). In particular, Î²â„“âˆˆâ„(I+2)Ã—dâ„“+1\beta\_{\ell}\in\mathbb{R}^{(I+2)\times d\_{\ell+1}}, each (Î¨^k:in)kâˆˆ{1,â€¦,K}\big(\widehat{\Psi}\_{k:\mathrm{in}}\big)\_{k\in\{1,\dots,K\}} and (Î¨^k:out)kâˆˆ{1,â€¦,K}\big(\widehat{\Psi}\_{k:\mathrm{out}}\big)\_{k\in\{1,\dots,K\}} are Resâ€“KANs of depth DadaD\_{\mathrm{ada}} and width WadaW\_{\mathrm{ada}}, and for any â„“âˆˆ{0,â€¦,L+1}\ell\in\{0,\dots,L+1\}, we have W(â„“)âˆˆâ„dâ„“+1Ã—dâ„“W^{(\ell)}\in\mathbb{R}^{d\_{\text{$\ell$}\text{$+$}\text{$1$}}\times d\_{\text{$\ell$}}}

|  |  |  |
| --- | --- | --- |
|  | (K(â„“)â€‹v)â€‹(x)â‰”âˆ«ğ’ŸkNN(â„“)â€‹(x,y)â€‹vâ€‹(y)â€‹dy,xâˆˆğ’Ÿ,vâˆˆLpâ€‹(ğ’Ÿ;â„)dâ„“,b(â„“)â€‹(x)â‰”bNN(â„“)â€‹(x),xâˆˆğ’Ÿ,\displaystyle\big(K^{(\ell)}v\big)(x)\coloneqq\int\_{\mathcal{D}}k\_{\text{$N$}\text{$N$}}^{(\ell)}(x,y)v(y)\mathrm{d}y,\;x\in\mathcal{D},\;v\in L^{p}(\mathcal{D};\mathbb{R})^{d\_{\text{$\ell$}}},\;b^{(\ell)}(x)\coloneqq b\_{\text{$N$}\text{$N$}}^{(\ell)}(x),\;x\in{\cal D}, |  |

where kNN(â„“)âˆˆResâ€“KANL^,W^I,Î±â€‹(â„dÃ—d,â„dâ„“+1Ã—dâ„“)k\_{\text{$N$}\text{$N$}}^{(\ell)}\in\text{\rm Res--KAN}\_{\hat{\text{$L$}},\hat{\text{$W$}}}^{\text{$I$},\alpha}(\mathbb{R}^{d\times d},\mathbb{R}^{d\_{\text{$\ell$}\text{$+$}\text{$1$}}\times d\_{\text{$\ell$}}}) and bNN(â„“)âˆˆResâ€“KANL^,W^I,Î±â€‹(â„d,â„dâ„“)b\_{\text{$N$}\text{$N$}}^{(\ell)}\in\text{\rm Res--KAN}\_{\hat{\text{$L$}},\hat{\text{$W$}}}^{\text{$I$},\alpha}(\mathbb{R}^{d},\mathbb{R}^{d\_{\text{$\ell$}}}) are Resâ€“KANs of depth L^\widehat{L} and width W^\widehat{W}.
We denote the above class of KANOs by

|  |  |  |
| --- | --- | --- |
|  | ğ’©â€‹ğ’ªL^,W^L,W,I,Î±â€‹(W1,âˆâ€‹(ğ’Ÿ;â„)din,W1,âˆâ€‹(ğ’Ÿ;â„)dout),\mathcal{NO}^{\text{$L$},\text{$W$},\text{$I$},\alpha}\_{\hat{\text{$L$}},\hat{\text{$W$}}}\big(W^{1,\infty}(\mathcal{D};\mathbb{R})^{d\_{\text{$\rm in$}}},W^{1,\infty}(\mathcal{D};\mathbb{R})^{d\_{\text{${\rm out}$}}}\big), |  |

which we abbreviate to ğ’©â€‹ğ’ªL^,W^L,W,I,Î±\mathcal{NO}^{\text{$L$},\text{$W$},\text{$I$},\alpha}\_{\hat{\text{$L$}},\hat{\text{$W$}}} when the dimensions and domains are contextually evident.

For Iâ‰”âŒˆsâŒ‰I\coloneqq\lceil s\rceil, we henceforth abbreviate

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ’©â€‹ğ’ªI,Î±â‰”â‹ƒ(L,L^,W,W^,Î±)âˆˆ(â„•â‹†)4Ã—(0,1)ğ’©â€‹ğ’ªL^,W^L,W,I,Î±,\mathcal{NO}\_{\text{$I$},\alpha}\coloneqq\bigcup\_{(\text{$L$},\hat{\text{$L$}},\text{$W$},\hat{\text{$W$}},\alpha)\in(\mathbb{N}^{\text{$\star$}})^{\text{$4$}}\times(0,1)}\mathcal{NO}^{\text{$L$},\text{$W$},\text{$I$},\alpha}\_{\hat{\text{$L$}},\hat{\text{$W$}}}, |  | (2.12) |

Motivated by the PDE representation of the solutions to each member of our family of second-order BSDEs, given inÂ ([1.2](https://arxiv.org/html/2511.01125v1#S1.E2 "Equation 1.2 â€£ 1 Introduction â€£ One model to solve them all: 2BSDE families via neural operators")), and due toÂ [[16](https://arxiv.org/html/2511.01125v1#bib.bib16)], we extend the (semi-)classical class of neural operators above to the following stochastic model as follows.

###### Definition 2.5 (22Generative neural operators (2FBNO)).

Fix dimensions dd, and dind\_{\rm in}, as well as smoothness parameters 3â‰¤Î±â‰¤I3\leq\alpha\leq I, with Iâˆˆâ„•â‹†I\in\mathbb{N}^{\star}, and fix depths Lâˆˆâ„•â‹†L\in\mathbb{N}^{\star}, L^âˆˆâ„•â‹†\widehat{L}\in\mathbb{N}^{\star}, and widths Wâˆˆâ„•â‹†W\in\mathbb{N}^{\star}, W^âˆˆâ„•â‹†\widehat{W}\in\mathbb{N}^{\star}. The class of forwardâ€“backward KANOs (2(2FBNOs)) â„±â€‹â„¬L^,W^,XL,W,I,Î±\mathcal{FB}^{\text{$L$},\text{$W$},\text{$I$},\alpha}\_{\hat{\text{$L$}},\hat{\text{$W$}},X} consists of all

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î“^:W1,âˆâ€‹(ğ’Ÿ,â„)din\displaystyle\widehat{\Gamma}:W^{1,\infty}(\mathcal{D},\mathbb{R})^{d\_{\text{$\rm in$}}} | âŸ¶(â„‹T2)4â‰”âˆi=14â„‹T2\displaystyle\longrightarrow(\mathcal{H}\_{T}^{2})^{4}\coloneqq\prod\_{i=1}^{4}\,\mathcal{H}\_{T}^{2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | fâ‰”(f1,â€¦,fdin)\displaystyle f\coloneqq(f\_{1},\dots,f\_{d\_{\rm in}}) | âŸ¼(Y^f,Z^f,Î¥^f,A^f),\displaystyle\longmapsto(\widehat{Y}^{f},\widehat{Z}^{f},\widehat{\Upsilon}^{f},\widehat{A}^{f}), |  |

for which there is a Î“âˆˆğ’©â€‹ğ’ªL^,W^L,W,I,Î±â€‹(W1,âˆâ€‹(ğ’Ÿ;â„)din,W1,âˆâ€‹(ğ’Ÿ;â„))\Gamma\in\mathcal{NO}^{\text{$L$},\text{$W$},\text{$I$},\alpha}\_{\hat{\text{$L$}},\hat{\text{$W$}}}(W^{1,\infty}(\mathcal{D};\mathbb{R})^{d\_{\text{$\rm in$}}},W^{1,\infty}(\mathcal{D};\mathbb{R})) satisfying the representation

|  |  |  |
| --- | --- | --- |
|  | Ytf=Î“â€‹(f)â€‹(Xt),Ztf=(âˆ‡Î“â€‹(f))â€‹(Xt),Î¥tf=(âˆ‡2Î“â€‹(f))â€‹(Xt),andâ€‹Atf=(â„’â€‹âˆ‡Î“â€‹(f))â€‹(Xt),\displaystyle Y\_{t}^{f}=\Gamma(f)(X\_{t}),\;Z\_{t}^{f}=\big(\nabla\Gamma(f)\big)(X\_{t}),\;\Upsilon\_{t}^{f}=\big(\nabla^{2}\Gamma(f)\big)(X\_{t}),\;\mbox{\rm and}\;A\_{t}^{f}=\big(\mathcal{L}\nabla\Gamma(f)\big)(X\_{t}), |  |

where, as before, â„’\mathcal{L} denotes the generator of XX, without the drift.

## 3 Main results

### 3.1 Elliptic PDE representation of the 2BSDE system

For the readerâ€™s convenience, we repeat the PDE inÂ ([1.1](https://arxiv.org/html/2511.01125v1#S1.E1 "Equation 1.1 â€£ 1 Introduction â€£ One model to solve them all: 2BSDE families via neural operators")).

|  |  |  |  |
| --- | --- | --- | --- |
|  | fâ€‹(x,uâ€‹(x),âˆ‡uâ€‹(x),âˆ‡2uâ€‹(x))=âˆ’f0â€‹(x),xâˆˆğ’Ÿ,uâ€‹(x)=gâ€‹(x),xâˆˆâˆ‚ğ’Ÿ,f\big(x,u(x),\nabla u(x),\nabla^{2}u(x)\big)=-f\_{0}(x),\;x\in\mathcal{D},\;u(x)=g(x),\;x\in\partial\mathcal{D}, |  | (3.1) |

###### Proposition 3.1 (Non-linear Feynmanâ€“Kacâ€™s formula).

Let uu be a classical solution to the PDE ([1.1](https://arxiv.org/html/2511.01125v1#S1.E1 "Equation 1.1 â€£ 1 Introduction â€£ One model to solve them all: 2BSDE families via neural operators")), such that all the quantities below are defined and continuous in time

|  |  |  |
| --- | --- | --- |
|  | Yt=uâ€‹(Xt),Zt=âˆ‡uâ€‹(Xt),Î¥t=âˆ‡2uâ€‹(Xt),At=â„’â€‹âˆ‡uâ€‹(Xt),tâˆˆ[0,Ï„),â„™â€‹â€“a.s.,Y\_{t}=u(X\_{t}),\;Z\_{t}=\nabla u(X\_{t}),\;\Upsilon\_{t}=\nabla^{2}u(X\_{t}),\;A\_{t}=\mathcal{L}\nabla u(X\_{t}),\;t\in[0,\tau),\;\mathbb{P}\text{\rm--a.s.}, |  |

where

|  |  |  |
| --- | --- | --- |
|  | Xt=x+âˆ«0tÎ²â€‹(Xs)â€‹ds+âˆ«0tÎ³â€‹(Xs)â€‹dWs,tâ‰¥0,â„™â€‹â€“a.s.,Ï„â‰”inf{tâ‰¥0:Xtâˆ‰ğ’Ÿ}.X\_{t}=x+\int\_{0}^{t}\beta(X\_{s})\mathrm{d}s+\int\_{0}^{t}\gamma(X\_{s})\mathrm{d}W\_{s},\;t\geq 0,\;\mathbb{P}\text{\rm--a.s.},\;\tau\coloneqq\inf\big\{t\geq 0:X\_{t}\notin{\cal D}\big\}. |  |

Then (Y,Z,Î¥,A)(Y,Z,\Upsilon,A) is a solution to ([FBSDE](https://arxiv.org/html/2511.01125v1#S1.Ex3 "Equation FBSDE â€£ 1 Introduction â€£ One model to solve them all: 2BSDE families via neural operators"))â€“([2BSDE](https://arxiv.org/html/2511.01125v1#S1.Ex4 "Equation 2BSDE â€£ 1 Introduction â€£ One model to solve them all: 2BSDE families via neural operators")).

###### Proof.

Since uu is smooth enough, we can apply ItÃ´â€™s formula to obtain for any tâˆˆ[0,Ï„)t\in[0,\tau)

|  |  |  |  |
| --- | --- | --- | --- |
|  | uâ€‹(Xt)\displaystyle u(X\_{t}) | =uâ€‹(XÏ„)âˆ’âˆ«tÏ„12â€‹Trâ€‹[Î³â€‹(Xs)â€‹Î³âŠ¤â€‹(Xs)â€‹âˆ‡2uâ€‹(Xs)]â€‹dsâˆ’âˆ«tÏ„âˆ‡uâ€‹(Xs)â‹…dXs,\displaystyle=u(X\_{\tau})-\int\_{t}^{\tau}\frac{1}{2}\mathrm{Tr}\big[\gamma(X\_{s})\gamma^{\top}(X\_{s})\nabla^{2}u(X\_{s})\big]\mathrm{d}s-\int\_{t}^{\tau}\nabla u(X\_{s})\cdot\mathrm{d}X\_{s}, |  |

as well as

|  |  |  |
| --- | --- | --- |
|  | âˆ‡uâ€‹(Xt)=âˆ‡uâ€‹(x)+âˆ«0tâˆ‡2uâ€‹(Xs)â€‹dXs+âˆ«0tâ„’â€‹âˆ‡uâ€‹(Xs)â€‹ds=âˆ‡uâ€‹(x)+âˆ«0tÎ¥sâ€‹dXs+âˆ«0tAsâ€‹ds.\displaystyle\nabla u(X\_{t})=\nabla u(x)+\int\_{0}^{t}\nabla^{2}u(X\_{s})\mathrm{d}X\_{s}+\int\_{0}^{t}{\cal L}\nabla u(X\_{s})\mathrm{d}s=\nabla u(x)+\int\_{0}^{t}\Upsilon\_{s}\mathrm{d}X\_{s}+\int\_{0}^{t}A\_{s}\mathrm{d}s. |  |

it follows by the PDE satisfied by uu that

|  |  |  |
| --- | --- | --- |
|  | uâ€‹(Xt)=gâ€‹(XÏ„)+âˆ«tÏ„(fâ€‹(Xs,Ys,Zs,Î¥s)+f0â€‹(Xs)âˆ’12â€‹Trâ€‹[Î³â€‹(Xs)â€‹Î³âŠ¤â€‹(Xs)â€‹Î¥s])â€‹dsâˆ’âˆ«tÏ„ZsâŠ¤â€‹dXs,u(X\_{t})=g(X\_{\tau})+\int\_{t}^{\tau}\bigg(f(X\_{s},Y\_{s},Z\_{s},\Upsilon\_{s})+f\_{0}(X\_{s})-\frac{1}{2}\mathrm{Tr}\big[\gamma(X\_{s})\gamma^{\top}(X\_{s})\Upsilon\_{s}\big]\bigg)\mathrm{d}s-\int\_{t}^{\tau}Z\_{s}^{\top}\mathrm{d}X\_{s}, |  |

as desired.
âˆ

### 3.2 General approximability guarantee

Let 0<Î´â‰¤10<\delta\leq 1 and let ğ•ŠdÎ´\mathbb{S}\_{d}^{\delta} denote the subset of ğ•Šd+\mathbb{S}^{\text{$+$}}\_{d} consisting of matrices satisfying the following nearâ€“norm preserving property: for every xâˆˆâ„dx\in\mathbb{R}^{d}

|  |  |  |
| --- | --- | --- |
|  | Î´â€‹â€–xâ€–2â‰¤xâ€‹AâŠ¤â€‹xâ‰¤1Î´â€‹â€–xâ€–2.\delta\|x\|^{2}\leq xA^{\top}x\leq\frac{1}{\delta}\|x\|^{2}. |  |

We write generically uâ€²{\textrm{u}^{\prime}} for (x0,â€¦,xd)âˆˆâ„1+d(x\_{0},\dots,x\_{d})\in\mathbb{R}^{1+d}, uâ€²â€²{\textrm{u}^{\prime\prime}} for any element of ğ•ŠdÎ´\mathbb{S}\_{d}^{\delta}, and uâ‰”(uâ€²,uâ€²â€²){\textrm{u}}\coloneqq({\textrm{u}^{\prime}},{\textrm{u}^{\prime\prime}}).

###### Setting 3.2.

and let GÂ¯:â„dâŸ¶[0,âˆ)\bar{G}:\mathbb{R}^{d}\longrightarrow[0,\infty) be Borel measurable.
Fix constants K0,KFâ‰¥0K\_{0},K\_{F}\geq 0,
LF,Cgâ‰¥0L\_{F},C\_{g}\geq 0,
and 0<Î´â‰¤10<\delta\leq 1. We require the following of the domain ğ’Ÿ\mathcal{D}.

###### Assumption 3.3 (Domain Regularity).

The domain ğ’ŸâŠ†â„d\mathcal{D}\subseteq\mathbb{R}^{d} is a non-empty bounded domain with C1,1C^{1,1}-boundary
satisfying the exterior ball condition.

Our general approximability result, for which favourable rates cannot generally be guaranteed, considers families of fully non-linear elliptic PDEs

|  |  |  |
| --- | --- | --- |
|  | fâ€‹(x,uâ€‹(x),âˆ‡uâ€‹(x),âˆ‡2uâ€‹(x))=0,xâˆˆğ’Ÿ,uâ€‹(x)=gâ€‹(x),xâˆˆâˆ‚ğ’Ÿ,f\big(x,u(x),\nabla u(x),\nabla^{2}u(x)\big)=0,\;x\in\mathcal{D},\;u(x)=g(x),\;x\in\partial\mathcal{D}, |  |

where the boundary data gâˆˆWk,pâ€‹(âˆ‚ğ’Ÿ)g\in W^{k,p}(\partial\mathcal{D}) is assumed to be sufficiently smooth, *i.e.* kâ‰¥2k\geq 2.

Following [Krylov](https://arxiv.org/html/2511.01125v1#bib.bib54) [[54](https://arxiv.org/html/2511.01125v1#bib.bib54), Chapter 14], our PDEs will have sufficiently regular solutions under the following conditions.

###### Assumption 3.4.

Assume that p>dp>d, and fix constants (c1,c2,R0)âˆˆ(0,1]3(c\_{1},c\_{2},R\_{0})\in(0,1]^{3}, LFâ‰¥0L\_{\text{$F$}}\geq 0, a function Ï‰F:[0,âˆ)âŸ¶[0,âˆ)\omega\_{\text{$F$}}:[0,\infty)\longrightarrow[0,\infty) with Ï‰Fâ€‹(0)=0\omega\_{\text{$F$}}(0)=0, a Borel measurable function GÂ¯:â„dâŸ¶[0,âˆ)\bar{G}:\mathbb{R}^{d}\longrightarrow[0,\infty), and Borel measurable functions FF and GG of the variables (u0,uâ€²,x)(u\_{0},{\textrm{u}^{\prime}},x) and (u,x)(u,x) respectively. We have

* (i)(i)

  f=F+Gf=F+G, and for all uâ€²â€²âˆˆğ•Šd+{\textrm{u}^{\prime\prime}}\in\mathbb{S}\_{d}^{\text{$+$}}, uâ€²âˆˆâ„1+d{\textrm{u}^{\prime}}\in\mathbb{R}^{1\text{$+$}d}, and xâˆˆâ„dx\in\mathbb{R}^{d}, we have

  |  |  |  |  |
  | --- | --- | --- | --- |
  |  | |Gâ€‹(u,x)|â‰¤c1â€‹â€–uâ€²â€²â€–F+c2â€‹â€–uâ€²â€–+GÂ¯â€‹(x),Fâ€‹(0,x)=0;\big|G({\textrm{u}},x)\big|\leq c\_{1}\|{\textrm{u}^{\prime\prime}}\|\_{\text{$F$}}+c\_{2}\|{\textrm{u}^{\prime}}\|+\bar{G}(x),\;F(0,x)=0; |  | (3.2) |
* (iâ€‹i)(ii)

  FF is LFL\_{F}â€“Lipschitz continuous with respect to uâ€²â€²;{\textrm{u}^{\prime\prime}};
* (iâ€‹iâ€‹i)(iii)

  for any vâˆˆâ„v\in\mathbb{R}, 0<râ‰¤R00<r\leq R\_{0}, and xâˆˆğ’Ÿx\in\mathcal{D}, there exists a convex function FÂ¯:ğ•ŠdâŸ¶[0,âˆ)\bar{F}:\mathbb{S}\_{d}\longrightarrow[0,\infty) such that

  1. (a)(a)

     FÂ¯â€‹(0,x)=0\bar{F}(0,x)=0, and âˆ‡uâ€²â€²FÂ¯\nabla\_{u^{\text{$\prime$}\text{$\prime$}}}\bar{F} has range in ğ•ŠdÎ´\mathbb{S}\_{d}^{\delta} at every point of twice differentiability of FÂ¯;\bar{F};
  2. (b)(b)

     for every uâ€²â€²âˆˆğ•Šd+{\textrm{u}^{\prime\prime}}\in\mathbb{S}\_{d}^{\text{$+$}} with â€–uâ€²â€²â€–F=1\|{\textrm{u}^{\prime\prime}}\|\_{F}=1, we have

     |  |  |  |  |
     | --- | --- | --- | --- |
     |  | infBâ€‹(r,x)âˆ©ğ’ŸsuprÂ¯>0|FÂ¯â€‹(uâ€²0,râ€‹uâ€²â€²,u)âˆ’FÂ¯â€‹(Ï„â€‹uâ€²â€²)|râ‰¤c2â€‹Volâ€‹(ğ’Ÿâˆ©Bâ€‹(r,x)),\inf\_{B(r,x)\cap\mathcal{D}}\sup\_{\bar{r}>0}\frac{\big|\bar{F}({\textrm{u}^{\prime}}\_{0},r{\textrm{u}^{\prime\prime}},u)-\bar{F}(\tau{\textrm{u}^{\prime\prime}})\big|}{r}\leq c\_{2}\mathrm{Vol}\big(\mathcal{D}\cap B(r,x)\big), |  | (3.3) |

     where Volâ€‹(A)\mathrm{Vol}(A) denotes the dd-dimensional Lebesgue measure of a Lebesgue-measurable set AâŠ†â„d;A\subseteq\mathbb{R}^{d};
  3. (c)(c)

     for any (u,v)âˆˆâ„2(u,v)\in\mathbb{R}^{2}, xâˆˆğ’Ÿx\in\mathcal{D}, and uâ€²âˆˆğ•Šd+{\textrm{u}^{\prime}}\in\mathbb{S}\_{d}^{\text{$+$}}, we have

     |  |  |  |  |
     | --- | --- | --- | --- |
     |  | |Fâ€‹(u,uâ€²â€²,x)âˆ’Fâ€‹(v,uâ€²â€²,x)|â‰¤Ï‰Fâ€‹(|uâˆ’v|)â€‹â€–uâ€²â€²â€–F.\big|F(u,{\textrm{u}^{\prime\prime}},x)-F(v,{\textrm{u}^{\prime\prime}},x)\big|\leq\omega\_{\text{$F$}}(|u-v|)\|{\textrm{u}^{\prime\prime}}\|\_{\text{$F$}}. |  | (3.4) |

The next definition introduces appropriate perturbations of the original PDE we consider, and uses notations from [SectionËœ3.2](https://arxiv.org/html/2511.01125v1#S3.SS2 "3.2 General approximability guarantee â€£ 3 Main results â€£ One model to solve them all: 2BSDE families via neural operators").

###### Definition 3.5 (PDE perturbation space ğ’³kâ€‹(r)\mathcal{X}\_{k}(r)).

Fix r>0r>0, kâˆˆâ„•â‹†k\in\mathbb{N}^{\star} and let ğ’³kâ€‹(r)\mathcal{X}\_{k}(r) consist of all pairs (GÂ¯0,g)âˆˆW2,pâ€‹(ğ’Ÿ)Ã—Wk,pâ€‹(ğ’Ÿ)(\bar{G}\_{0},g)\in W^{2,p}(\mathcal{D})\times W^{k,p}(\mathcal{D}) with â€–gâ€–Wk,pâ€‹(ğ’Ÿ)â‰¤r\|g\|\_{W^{\text{$k$}\text{$,$}\text{$p$}}(\mathcal{D})}\leq r.
Define G0â‰”G+GÂ¯0G\_{0}\coloneqq G+\bar{G}\_{0} and, for every pair (G0,g)âˆˆğ’³kâ€‹(r)(G\_{0},g)\in\mathcal{X}\_{k}(r) denote the solution to the following associated fully non-linear elliptic PDE by uGÂ¯0,gu\_{\bar{G}\_{\text{$0$}},g}

|  |  |  |  |
| --- | --- | --- | --- |
|  | (F+GâŸStructureÂ +GÂ¯0âŸPerturbation)â€‹(x,uâ€‹(x),âˆ‡uâ€‹(x),âˆ‡2uâ€‹(x))=0,âˆ€xâˆˆğ’Ÿ,uâ€‹(x)=gâ€‹(x)âŸPerturbation,âˆ€xâˆˆâˆ‚ğ’Ÿ.\displaystyle\bigg(\underbrace{F+G}\_{\mbox{\tiny\rm Structure \normalsize}}+\underbrace{\bar{G}\_{0}}\_{\mbox{\tiny\rm Perturbation}}\bigg)\big(x,u(x),\nabla u(x),\nabla^{2}u(x)\big)=0,\;\forall x\in\mathcal{D},\;u(x)=\underbrace{g(x)}\_{\mbox{\tiny\rm Perturbation}},\;\forall x\in\partial\mathcal{D}. |  | (3.5) |

###### Example 3.6 (Source perturbations only).

We can, of course, restrict ourselves to perturbations of the source condition itself only, in which case we may restrict our attention to GÂ¯0\bar{G}\_{0} which are constant in their first argument; *i.e.* GÂ¯0â€‹(u,x)=f0â€‹(x)\bar{G}\_{0}(u,x)=f\_{0}(x) for some f0âˆˆWk,pâ€‹(ğ’Ÿ)f\_{0}\in W^{k,p}(\mathcal{D}), similarly to the special case in ([1.5](https://arxiv.org/html/2511.01125v1#S1.E5 "Equation 1.5 â€£ 1 Introduction â€£ One model to solve them all: 2BSDE families via neural operators")).

###### Theorem 3.7 (Approximability of the perturbation-to-solution map).

Fix qâˆˆ[1,+âˆ)q\in[1,+\infty), let ğ’Ÿ\mathcal{D} be a bounded exterior-thick domain in â„d\mathbb{R}^{d} with C1,1C^{1,1}-boundary, let r>0r>0, k>1+maxâ¡{1,dp}k>1+\max\big\{1,\tfrac{d}{p}\big\}, and ğ’³âŠ†ğ’³kâ€‹(r)\mathcal{X}\subseteq\mathcal{X}\_{k}(r) be compact.

Suppose [SectionsËœ3.2](https://arxiv.org/html/2511.01125v1#S3.SS2 "3.2 General approximability guarantee â€£ 3 Main results â€£ One model to solve them all: 2BSDE families via neural operators") andÂ [3.2](https://arxiv.org/html/2511.01125v1#S3.SS2 "3.2 General approximability guarantee â€£ 3 Main results â€£ One model to solve them all: 2BSDE families via neural operators") hold
and that both ÏƒS\sigma\_{\text{$S$}} and ÏƒW\sigma\_{\text{$W$}} satisfy [SectionËœ2.2.1](https://arxiv.org/html/2511.01125v1#S2.SS2.SSS1 "2.2.1 Residual Kolmogorovâ€“Arnold networks (Resâ€“KANs) â€£ 2.2 Deep learning â€£ 2 Preliminaries â€£ One model to solve them all: 2BSDE families via neural operators").
Then, for every approximation error Îµ>0\varepsilon>0, there exists some neural operator Î“^âˆˆğ’©â€‹ğ’ªâŒˆkâŒ‰,1\hat{\Gamma}\in\mathcal{NO}\_{\lceil k\rceil,1}, cf.Â ([2.12](https://arxiv.org/html/2511.01125v1#S2.E12 "Equation 2.12 â€£ 2.2.2 Neural operator architectures â€£ 2.2 Deep learning â€£ 2 Preliminaries â€£ One model to solve them all: 2BSDE families via neural operators")), satisfying the uniform estimate

|  |  |  |  |
| --- | --- | --- | --- |
|  | sup(GÂ¯0,g)âˆˆğ’³â€–uGÂ¯0,gâˆ’Î“^â€‹(GÂ¯0,g)â€–W2,pâ€‹(ğ’Ÿ)<Îµ.\sup\_{(\bar{G}\_{\text{$0$}},g)\in\mathcal{X}}\,\big\|u\_{\bar{G}\_{\text{$0$}},g}-\hat{\Gamma}(\bar{G}\_{0},g)\big\|\_{W^{\text{$2$}\text{$,$}\text{$p$}}(\mathcal{D})}<\varepsilon. |  | (3.6) |

The proof of [TheoremËœ3.7](https://arxiv.org/html/2511.01125v1#S3.Thmtheorem7 "Theorem 3.7 (Approximability of the perturbation-to-solution map). â€£ 3.2 General approximability guarantee â€£ 3 Main results â€£ One model to solve them all: 2BSDE families via neural operators") is based on two ingredients. First, we establish the localâ€“Lipschitz regularity of the coefficient-to-solution map associated to our family of fully non-linear elliptic PDEs ([SectionËœA.4](https://arxiv.org/html/2511.01125v1#A1.SS4 "A.4 Stability estimate of general solution operator â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators")) verifying the only necessary condition for approximability by continuous models classes; such as our NO, namely continuityâ€”a property which need not be immediate for arbitrary coefficient-to-solution maps.
Next, we rely on [SectionËœA.3](https://arxiv.org/html/2511.01125v1#A1.SS3 "A.3 Proof of universal approximation â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators") which establishes a general universal approximation theorem for operators between Besov spaces.

In this sense, [SectionËœA.3](https://arxiv.org/html/2511.01125v1#A1.SS3 "A.3 Proof of universal approximation â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators") for our NO architecture which, among other things, can be regarded as a generalisation of [Kovachki, Li, Liu, Azizzadenesheli, Bhattacharya, Stuart, and Anandkumar](https://arxiv.org/html/2511.01125v1#bib.bib47) [[47](https://arxiv.org/html/2511.01125v1#bib.bib47), Theorem 11], which does not cover Besov spaces Bq,rsâ€‹(ğ’Ÿ)B\_{q,r}^{s}(\mathcal{D}) for finite values of qq and rr ( recall that Ws,pâ€‹(ğ’Ÿ)=Bq,rsâ€‹(ğ’Ÿ)W^{s,p}(\mathcal{D})=B\_{q,r}^{s}(\mathcal{D}) [[90](https://arxiv.org/html/2511.01125v1#bib.bib90), Remark 1.2]). We emphasise that here, the case of finite qq (and rr) is necessary since Ws,âˆâ€‹(ğ’Ÿ)W^{s,\infty}(\mathcal{D})-spaces are automatically excluded from both [SectionËœA.3](https://arxiv.org/html/2511.01125v1#A1.SS3 "A.3 Proof of universal approximation â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators") and [[47](https://arxiv.org/html/2511.01125v1#bib.bib47), Theorem 11], as well as any encoder-decoder-type model using basis expansions (*e.g.*Â [[33](https://arxiv.org/html/2511.01125v1#bib.bib33)]), since Ws,âˆW^{s,\infty} is not separable and thus cannot admit a Schauder basis. Additionally, since this space is non-separable and any realistic NO model must be parameterised by finitely many parameters and depend continuously on them, any realistic NO model defines a separable space, As such, it cannot be dense/universal in spaces of continuous functions between non-separable spacesâ€”again by elementary topological considerations.

We now consider the approximation of a specialized family of elliptic PDEs, whose solution operator exhibits enough structure so that it (not all continuous functions) can be approximated on non-separable space W1,âˆâ€‹(ğ’Ÿ)W^{1,\infty}(\mathcal{D}).

### 3.3 Feasible rates

#### 3.3.1 Semi-linear elliptic PDE

In what follows, we will make use of the map SÎ³,Î¼,Î»:W(d+3)/2,2â€‹(âˆ‚ğ’Ÿ;â„)âŸ¶W1,âˆâ€‹(ğ’Ÿ;â„)S\_{\gamma,\mu,\lambda}:W^{(d+3)/2,2}(\partial\mathcal{D};\mathbb{R})\longrightarrow W^{1,\infty}(\mathcal{D};\mathbb{R}) sending boundary data to domain data, and defined for each gâˆˆW(d+3)/2,2â€‹(âˆ‚ğ’Ÿ;â„)g\in W^{(d+3)/2,2}(\partial\mathcal{D};\mathbb{R}) by

|  |  |  |  |
| --- | --- | --- | --- |
|  | SÎ³,Î¼,Î»â€‹(g)â‰”wg,S\_{\gamma,\mu,\lambda}(g)\coloneqq w\_{g}, |  | (3.7) |

where wgâˆˆW(d+4)/2,2â€‹(ğ’Ÿ;â„)âŠ‚W1,âˆâ€‹(ğ’Ÿ;â„)w\_{g}\in W^{(d+4)/2,2}(\mathcal{D};\mathbb{R})\subset W^{1,\infty}(\mathcal{D};\mathbb{R}).
is the unique solution of

|  |  |  |
| --- | --- | --- |
|  | âˆ’âˆ‡â‹…Î³â€‹âˆ‡wg+Î¼â‹…âˆ‡wg+Î»â€‹wg=0â€‹inâ€‹ğ’Ÿ,wg=gâ€‹onâ€‹âˆ‚ğ’Ÿ.-\nabla\cdot\gamma\nabla w\_{g}+\mu\cdot\nabla w\_{g}+\lambda w\_{g}=0\ \mathrm{in}\ \mathcal{D},\;w\_{g}=g\ \mathrm{on}\ \partial\mathcal{D}. |  |

We assume the following on the maps Î³\gamma, Î¼\mu and Î»\lambda.

###### Assumption 3.8.

The maps Î³:ğ’ŸâŸ¶â„dÃ—d\gamma:\mathcal{D}\longrightarrow\mathbb{R}^{d\times d}, Î¼:ğ’ŸâŸ¶â„d\mu:\mathcal{D}\longrightarrow\mathbb{R}^{d}, and Î»:ğ’ŸâŸ¶â„\lambda:\mathcal{D}\longrightarrow\mathbb{R} satisfy the following conditions

* (i)(i)

  Î³âˆˆCâˆâ€‹(ğ’ŸÂ¯;â„dÃ—d)\gamma\in C^{\infty}(\bar{\mathcal{D}};\mathbb{R}^{d\times d}), Î¼âˆˆCâˆâ€‹(ğ’ŸÂ¯;â„d)\mu\in C^{\infty}(\bar{\mathcal{D}};\mathbb{R}^{d}), and Î»âˆˆCâˆâ€‹(ğ’ŸÂ¯;â„)\lambda\in C^{\infty}(\bar{\mathcal{D}};\mathbb{R}) where Câˆâ€‹(ğ’ŸÂ¯;â„d)C^{\infty}(\bar{\mathcal{D}};\mathbb{R}^{d}) and Câˆâ€‹(ğ’ŸÂ¯;â„dÃ—d)C^{\infty}(\bar{\mathcal{D}};\mathbb{R}^{d\times d}) denote the spaces of all dd-dimensional vector-valued and dÃ—dd\times d matrix-valued functions that are infinitely differentiable on ğ’Ÿ\mathcal{D} and whose derivatives admit continuous extensions to the closure ğ’ŸÂ¯\bar{\mathcal{D}};
* (iâ€‹i)(ii)

  Î³\gamma is uniformly elliptic and bounded in the sense that there are positive constants Î³0\gamma\_{0} and Î³1\gamma\_{1} such that

  |  |  |  |
  | --- | --- | --- |
  |  | Î³0â€‹â€–Î¾â€–2â‰¤Î¾âŠ¤â€‹Î³â€‹(x)â€‹Î¾â‰¤Î³1â€‹â€–Î¾â€–2,âˆ€(x,Î¾)âˆˆğ’ŸÃ—â„d;\gamma\_{0}\|\xi\|^{2}\leq\xi^{\top}\gamma(x)\xi\leq\gamma\_{1}\|\xi\|^{2},\;\forall(x,\xi)\in\mathcal{D}\times\mathbb{R}^{d}; |  |
* (iâ€‹iâ€‹i)(iii)

  Î¼\mu and Î»\lambda are such that

  |  |  |  |
  | --- | --- | --- |
  |  | Î»â‰¥0,andâ€‹Î»â‰¥âˆ‡â‹…Î¼â€‹âˆ‘i=1dâˆ‚xiÎ¼.\lambda\geq 0,\;\text{\rm and}\;\lambda\geq\nabla\cdot\mu\sum\_{i=1}^{d}\partial\_{x\_{i}}\mu. |  |

Next, we summaries our main assumptions on f~\tilde{f}.

###### Assumption 3.9.

The map f~:ğ’ŸÃ—â„âŸ¶â„\tilde{f}:\mathcal{D}\times\mathbb{R}\longrightarrow\mathbb{R} satisfies

* (i)(i)

  there exists Î´0>0\delta\_{0}>0 and Hâˆˆâ„•â‹†âˆ–{1,2}H\in\mathbb{N}^{\star}\setminus\{1,2\} such that

  |  |  |  |
  | --- | --- | --- |
  |  | f~â€‹(x,z)=âˆ‘h=0Hâˆ‚zhf~â€‹(x,0)h!â€‹zh,forâ€‹â€–zâ€–<Î´0,andâ€‹xâˆˆğ’Ÿ;\tilde{f}(x,z)=\sum\_{h=0}^{H}\frac{\partial\_{z}^{h}\tilde{f}(x,0)}{h!}z^{h},\;\text{\rm for}\;\|z\|<\delta\_{0},\;\text{\rm and}\;x\in\mathcal{D}; |  |
* (iâ€‹i)(ii)

  f~â€‹(â‹…,0)=âˆ‚z1f~â€‹(â‹…,0)=0;\tilde{f}(\cdot,0)=\partial\_{z}^{1}\tilde{f}(\cdot,0)=0;
* (iâ€‹iâ€‹i)(iii)

  âˆ‚zhf~â€‹(â‹…,0)âˆˆCâˆâ€‹(ğ’ŸÂ¯;â„)\partial\_{z}^{h}\tilde{f}(\cdot,0)\in C^{\infty}(\bar{\mathcal{D}};\mathbb{R}) for all hâˆˆ{2,â€¦,H}h\in\{2,\dots,H\}.

Assumption (i) posits that f~â€‹(x,z)\tilde{f}(x,z) is analytic at z=0z=0 and represented by a finite power series truncated at order HH. Assumption (ii) removes the zeroth- and first-order terms, which are already captured by f0â€‹(x)f\_{0}(x) and Î»â€‹(x)â€‹uâ€‹(x)\lambda(x)u(x) in ([1.5](https://arxiv.org/html/2511.01125v1#S1.E5 "Equation 1.5 â€£ 1 Introduction â€£ One model to solve them all: 2BSDE families via neural operators")). Assumption (iii) requires all coefficient functions to be smooth, ensuring a well-posed setting for the subsequent analysis.

Finally, we formulate a smallness assumption.

###### Assumption 3.10.

We take 0<Î´<Î´00<\delta<\delta\_{0} ((where Î´0\delta\_{0} comes from [SectionËœ3.3.1](https://arxiv.org/html/2511.01125v1#S3.SS3.SSS1 "3.3.1 Semi-linear elliptic PDE â€£ 3.3 Feasible rates â€£ 3 Main results â€£ One model to solve them all: 2BSDE families via neural operators").(i)(i))) so that

|  |  |  |
| --- | --- | --- |
|  | C1â€‹Î´<1,Ïâ‰”C2â€‹Î´<1,C3â€‹Î´<1,\displaystyle C\_{1}\delta<1,\;\rho\coloneqq C\_{2}\delta<1,\;C\_{3}\delta<1, |  |

where the positive constants C1C\_{1}, C2C\_{2}, C3C\_{3} will appear in ([A.6](https://arxiv.org/html/2511.01125v1#A1.E6 "Equation A.6 â€£ A.1.1 Well-posedness â€£ A.1 Proof of Theorem 3.11 â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators")), ([A.7](https://arxiv.org/html/2511.01125v1#A1.E7 "Equation A.7 â€£ A.1.1 Well-posedness â€£ A.1 Proof of Theorem 3.11 â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators")), and ([A.13](https://arxiv.org/html/2511.01125v1#A1.E13 "Equation A.13 â€£ A.1.2 Proof of Theorem 3.11 â€£ A.1 Proof of Theorem 3.11 â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators")), and depend only pp, dd, ğ’Ÿ\mathcal{D}, f~\tilde{f}, Î³\gamma, and Î¼\mu.

Under the above assumptions, we have the following approximation guarantee for the solution operator of the PDE associated with our randomly stopped second-order BSDE systemÂ ([SDE](https://arxiv.org/html/2511.01125v1#S1.Ex1 "Equation SDE â€£ 1 Introduction â€£ One model to solve them all: 2BSDE families via neural operators")), ([FBSDE](https://arxiv.org/html/2511.01125v1#S1.Ex3 "Equation FBSDE â€£ 1 Introduction â€£ One model to solve them all: 2BSDE families via neural operators")), ([2BSDE](https://arxiv.org/html/2511.01125v1#S1.Ex4 "Equation 2BSDE â€£ 1 Introduction â€£ One model to solve them all: 2BSDE families via neural operators")).

###### Theorem 3.11 (Exponential approximation rates: solution operator to the elliptic problem).

Let888This is need as our proof relies on the approximation results ofÂ [[45](https://arxiv.org/html/2511.01125v1#bib.bib45)] for the relevant Greenâ€™s function associated to our PDEs. dâ‰¥3d\geq 3.
Let [SectionsËœ3.3.1](https://arxiv.org/html/2511.01125v1#S3.SS3.SSS1 "3.3.1 Semi-linear elliptic PDE â€£ 3.3 Feasible rates â€£ 3 Main results â€£ One model to solve them all: 2BSDE families via neural operators"), [3.3.1](https://arxiv.org/html/2511.01125v1#S3.SS3.SSS1 "3.3.1 Semi-linear elliptic PDE â€£ 3.3 Feasible rates â€£ 3 Main results â€£ One model to solve them all: 2BSDE families via neural operators") andÂ [3.3.1](https://arxiv.org/html/2511.01125v1#S3.SS3.SSS1 "3.3.1 Semi-linear elliptic PDE â€£ 3.3 Feasible rates â€£ 3 Main results â€£ One model to solve them all: 2BSDE families via neural operators") hold.
Suppose that ğ’Ÿ\mathcal{D} is a bounded open set with Lipschitz boundary in â„d\mathbb{R}^{d}.
Let 1<s<21<s<2 and 1â‰¤p<ddâˆ’11\leq p<\frac{d}{d-1}.
Then, for any 0<Îµ<10<\varepsilon<1, there are positive integers LL, WW, L^\widehat{L}, W^\widehat{W}, and Î“âˆˆğ’©â€‹ğ’ªL^,W^L,W,I,Î±â€‹(W1,âˆâ€‹(ğ’Ÿ;â„)2,W1,âˆâ€‹(ğ’Ÿ;â„))\Gamma\in{\mathcal{NO}^{\text{$L$},\text{$W$},\text{$I$},\alpha}\_{\hat{\text{$L$}},\hat{\text{$W$}}}}(W^{1,\infty}(\mathcal{D};\mathbb{R})^{2},W^{1,\infty}(\mathcal{D};\mathbb{R})) such that

|  |  |  |
| --- | --- | --- |
|  | sup(f0,g)âˆˆâ„¬â€–Î“+â€‹(f0,g)âˆ’Î“â€‹(f0,SÎ³,Î¼,Î»â€‹(g))â€–W1,âˆâ€‹(ğ’Ÿ;â„)â‰¤Îµ.\sup\_{(f\_{0},g)\in{\cal B}}\big\|\Gamma^{\text{$+$}}(f\_{0},g)-\Gamma(f\_{0},S\_{\gamma,\mu,\lambda}(g))\big\|\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}(\mathcal{D};\mathbb{R})}\leq\varepsilon. |  |

where the supremum is taken over the set

|  |  |  |
| --- | --- | --- |
|  | â„¬â‰”BW1,âˆâ€‹(ğ’Ÿ;â„)â€‹(0,Î´2)Ã—BW(d+3)/2,2â€‹(âˆ‚ğ’Ÿ;â„)â€‹(0,Î´2).{\cal B}\coloneqq B\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}(\mathcal{D};\mathbb{R})}(0,\delta^{2})\times B\_{W^{\text{$($}\text{$d$}\text{$+$}\text{$3$}\text{$)$}\text{$/$}\text{$2$},2}(\partial\mathcal{D};\mathbb{R})}(0,\delta^{2}). |  |

Moreover, we have the following estimates for parameters L=Lâ€‹(Î“)L=L(\Gamma), W=Wâ€‹(Î“)W=W(\Gamma), L^=L^â€‹(Î“)\widehat{L}=\widehat{L}(\Gamma), and W^=W^â€‹(Î“)\widehat{W}=\widehat{W}(\Gamma),

|  |  |  |
| --- | --- | --- |
|  | Lâ‰¤Câ€‹logâ¡(Îµâˆ’1),Wâ‰¤C,L^â‰¤C,W^â‰¤Câ€‹Îµâˆ’1(sâˆ’1)â€‹p,L\leq C\log(\varepsilon^{-1}),\;W\leq C,\;\widehat{L}\leq C,\;\widehat{W}\leq C\varepsilon^{\text{$-$}\frac{1}{(s-1)p}}, |  |

where C>0C>0 depends only on ss, pp, dd, ğ’Ÿ\mathcal{D}, f~\tilde{f}, Î³\gamma, and Î¼\mu.

Our quantitative approximation rates are available because the family of elliptic PDEs considered here is well structured. In the fully general setting, however, since our NOs are continuous, one should not expect rates, as the solution operator should not even be expected to be continuous (let alone locallyâ€“Lipschitz continuous) which is necessary for approximability by the elementary uniform limit theorem from point-set topology, see [Munkres](https://arxiv.org/html/2511.01125v1#bib.bib71) [[71](https://arxiv.org/html/2511.01125v1#bib.bib71), Theorem 21.6].
In that caseâ€”even if the solution operator is only continuous for general fully non-linear familiesâ€”the best achievable rates are no better than worst-case bounds for approximating non-linear locallyâ€“Lipschitz continuous operators, see [[55](https://arxiv.org/html/2511.01125v1#bib.bib55)], which require an exponential increase in trainable neurons to achieve a linear decrease in error. Thus, even when approximability holds, any such â€˜rateâ€™ would be scarcely more informative than a simple existence statement.

Consequently, the principal obstacle is approximability, which is twofold:

(i)(i) the relevant solution operator in the fully non-linear elliptic case must be regular enough to be approximable by some universal deep-learning class;

(iâ€‹i)(ii) our models must be universal on the specific function spaces on which this solution map acts.

(i)(i) requires a stability analysis of our PDE family under coefficient perturbations, while (iâ€‹i)(ii) calls for a universal approximation theorem for our architecture, proved via basis-expansion techniques as in [SectionËœA.3](https://arxiv.org/html/2511.01125v1#A1.SS3 "A.3 Proof of universal approximation â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators"), akin in spirit to [[47](https://arxiv.org/html/2511.01125v1#bib.bib47), TheoremÂ 11], that holds on more general Besov spaces over regular Euclidean domains. This two-step scheme was introduced for deep learning in stochastic filtering [[42](https://arxiv.org/html/2511.01125v1#bib.bib42)] and refined for differential games in [[5](https://arxiv.org/html/2511.01125v1#bib.bib5), [29](https://arxiv.org/html/2511.01125v1#bib.bib29)].

#### 3.3.2 Solutions to the family of second-order BSDEs

We now derive the stochastic version of the above (deterministic) approximation theorem.
We additionally require the following regularity conditions.

###### Assumption 3.12 (Regularity of forward process).

There is some x0âˆˆğ’Ÿx\_{0}\in\mathcal{D} such that for each R>0R>0

1. (i)(i)

   ((local smoothness)): (Î²,Î³)âˆˆCbâˆâ€‹(Bâ„dâ€‹(x0,5â€‹R);â„dÃ—ğ•Šd+)2(\beta,\gamma)\in C\_{b}^{\infty}(B\_{\mathbb{R}^{d}}(x\_{0},5R);\mathbb{R}^{d}\times\mathbb{S}\_{d}^{+})^{2};
2. (iâ€‹i)(ii)

   ((local ellipticity)): Î³â€‹(x)â€‹Î³â€‹(x)âŠ¤â‰¥cx0,Râ€‹Id\gamma(x)\gamma(x)^{\top}\geq c\_{x\_{0},R}\mathrm{I}\_{d}, for every xâˆˆBâ„dâ€‹(x0,3â€‹R)x\in B\_{\mathbb{R}^{d}}(x\_{0},3R), for some 0<cx0,R<1;0<c\_{x\_{0},R}<1;
3. (iâ€‹iâ€‹i)(iii)

   there exists a unique strong solution toÂ ([SDE](https://arxiv.org/html/2511.01125v1#S1.Ex1 "Equation SDE â€£ 1 Introduction â€£ One model to solve them all: 2BSDE families via neural operators")).

###### Theorem 3.13.

Let [SectionsËœ3.3.1](https://arxiv.org/html/2511.01125v1#S3.SS3.SSS1 "3.3.1 Semi-linear elliptic PDE â€£ 3.3 Feasible rates â€£ 3 Main results â€£ One model to solve them all: 2BSDE families via neural operators"), [3.3.1](https://arxiv.org/html/2511.01125v1#S3.SS3.SSS1 "3.3.1 Semi-linear elliptic PDE â€£ 3.3 Feasible rates â€£ 3 Main results â€£ One model to solve them all: 2BSDE families via neural operators"), [3.3.1](https://arxiv.org/html/2511.01125v1#S3.SS3.SSS1 "3.3.1 Semi-linear elliptic PDE â€£ 3.3 Feasible rates â€£ 3 Main results â€£ One model to solve them all: 2BSDE families via neural operators") andÂ [3.3.2](https://arxiv.org/html/2511.01125v1#S3.SS3.SSS2 "3.3.2 Solutions to the family of second-order BSDEs â€£ 3.3 Feasible rates â€£ 3 Main results â€£ One model to solve them all: 2BSDE families via neural operators") hold, then, for any 0<Îµ<10<\varepsilon<1 and any time-window 0<Tâˆ’<T+0<T\_{\text{$-$}}<T\_{\text{$+$}}, there are integers LL, WW, Î”\Delta, HH, and Î“^âˆˆâ„±â€‹â„¬L^,W^,Ïƒ^L,W,ReQU\widehat{\Gamma}\in\mathcal{FB}\_{\hat{\text{$L$}},\hat{\text{$W$}},\hat{\text{$\sigma$}}}^{\text{$L$},\text{$W$},\text{$\rm ReQU$}} satisfying

|  |  |  |
| --- | --- | --- |
|  | sup(f,g)âˆˆâ„¬ğ”¼â„™â€‹[supÏ„âˆ§Tâˆ’â‰¤tâ‰¤T+âˆ§Ï„|Î“^â€‹(f,g)tâˆ’(Ytx,Ztx)|]â‰²Îµ,\sup\_{(f,g)\in{\cal B}}\,\mathbb{E}^{\mathbb{P}}\biggl[\sup\_{\tau\wedge T\_{\text{$-$}}\leq t\leq T\_{\text{$+$}}\wedge\tau}\,\Big|\widehat{\Gamma}(f,g)\_{t}-(Y^{x}\_{t},Z^{x}\_{t})\Big|\biggr]\lesssim\varepsilon, |  |

where the supremum is taken over the set

|  |  |  |
| --- | --- | --- |
|  | â„¬â‰”BW01,âˆâ€‹(ğ’Ÿ;â„)â€‹(0,Î´2)Ã—BH1+(d+1)/2â€‹(âˆ‚ğ’Ÿ;â„)â€‹(0,Î´2).{\cal B}\coloneqq B\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}\_{\text{$0$}}(\mathcal{D};\mathbb{R})}(0,\delta^{2})\times B\_{H^{\text{$1$}\text{$+$}\text{$($}\text{$d$}\text{$+$}\text{$1$}\text{$)$}\text{$/$}\text{$2$}}(\partial\mathcal{D};\mathbb{R})}(0,\delta^{2}). |  |

We have the same estimates for the parameters L=Lâ€‹(Î“)L=L(\Gamma), W=Wâ€‹(Î“)W=W(\Gamma), L^=L^â€‹(Î“)\widehat{L}=\widehat{L}(\Gamma), and W^=W^â€‹(Î“)\widehat{W}=\widehat{W}(\Gamma) as in [TheoremËœ3.11](https://arxiv.org/html/2511.01125v1#S3.Thmtheorem11 "Theorem 3.11 (Exponential approximation rates: solution operator to the elliptic problem). â€£ 3.3.1 Semi-linear elliptic PDE â€£ 3.3 Feasible rates â€£ 3 Main results â€£ One model to solve them all: 2BSDE families via neural operators").

## 4 Experimental results

In this section, we empirically validate our theoretical findings on two canonical benchmarks in the 2BSDE literature: the periodic semi-linear example of [Chassagneux, Chen, Frikha, and Zhou](https://arxiv.org/html/2511.01125v1#bib.bib13) [[13](https://arxiv.org/html/2511.01125v1#bib.bib13)] and the linearâ€“quadratic control example of [Pham, Warin, and Germain](https://arxiv.org/html/2511.01125v1#bib.bib78) [[78](https://arxiv.org/html/2511.01125v1#bib.bib78)]. We deploy the KANO architecture with a slight modification in the kernel layer (see [C.3](https://arxiv.org/html/2511.01125v1#A3.SS3 "C.3 Architectural details â€£ Appendix C Experimental details â€£ One model to solve them all: 2BSDE families via neural operators") for details). Specifically, rather than jointly learning both the kernel basis and its coefficients, we fix the basis to a Fourier system, obtained via uniform discretisation of the spatial domain, while retaining trainable, Resâ€“KANâ€“parametrised coefficients. Furthermore, skip connections parametrised by additional Resâ€“KAN layers are introduced on top of the learnable Fourier kernel coefficients. The resulting spectral layer follows the kernel introduced in [Li, Kovachki, Azizzadenesheli, Liu, Bhattacharya, Stuart, and Anandkumar](https://arxiv.org/html/2511.01125v1#bib.bib60) [[60](https://arxiv.org/html/2511.01125v1#bib.bib60)].

### 4.1 Periodic semi-linear case

In this experiment, we study the periodic semi-linear benchmark of [[13](https://arxiv.org/html/2511.01125v1#bib.bib13)] in dimension d=5d=5. This benchmark consists of trigonometric driftâ€“diffusion and has a closed-form solution uâ€‹(t,x)u(t,x) depending on âˆ‘i=15xi\sum\_{i=1}^{5}x\_{i}. This enables exact supervision of u,âˆ‡u,âˆ‡2uu,\nabla u,\nabla^{2}u and pathwise validation under periodic boundary conditions. The forwardâ€“-backward SDE system and its closed-form solution are detailed in [SectionËœC.1](https://arxiv.org/html/2511.01125v1#A3.SS1 "C.1 Periodic semi-linear case â€£ Appendix C Experimental details â€£ One model to solve them all: 2BSDE families via neural operators").

A KANO model is trained on 40964096 samples drawn according to the procedure in [SectionËœC.4](https://arxiv.org/html/2511.01125v1#A3.SS4 "C.4 Training pipeline â€£ Appendix C Experimental details â€£ One model to solve them all: 2BSDE families via neural operators"), and subsequently evaluated along independently generated trajectories using the Euler-â€“Maruyama sampler described in [SectionËœC.5](https://arxiv.org/html/2511.01125v1#A3.SS5 "C.5 Inference pipeline â€£ Appendix C Experimental details â€£ One model to solve them all: 2BSDE families via neural operators"). [FiguresËœ3](https://arxiv.org/html/2511.01125v1#S4.F3 "In 4.2 Linearâ€“quadratic case â€£ 4 Experimental results â€£ One model to solve them all: 2BSDE families via neural operators") andÂ [4](https://arxiv.org/html/2511.01125v1#S4.F4 "Figure 4 â€£ 4.2 Linearâ€“quadratic case â€£ 4 Experimental results â€£ One model to solve them all: 2BSDE families via neural operators") display the projections of two randomly selected trajectories onto the (x1,x2)(x\_{1},x\_{2})-plane, together with the corresponding ground-truth solutions uu, first and second partial derivatives âˆ‚u/âˆ‚x1\partial u/\partial x\_{1} and âˆ‚2u/âˆ‚x12\partial^{2}u/\partial x\_{1}^{2}, and the respective predictions produced by the trained model along these trajectories. We observe that the model is generally able to accurately capture the solution, as well as the first and second partial derivatives along the entire trajectories, with only minor discrepancies in the second derivatives.

### 4.2 Linearâ€“quadratic case

We next consider the LQ/Hamiltonâ€“Jacobi=-Bellman benchmark proposed in [[78](https://arxiv.org/html/2511.01125v1#bib.bib78)] in d=5d=5 (see [SectionËœC.2](https://arxiv.org/html/2511.01125v1#A3.SS2 "C.2 Linearâ€“quadratic (LQ) case â€£ Appendix C Experimental details â€£ One model to solve them all: 2BSDE families via neural operators") for details). It represents a HJB-type problem with quadratic cost, whose value function remains quadratic uâ€‹(t,x)=xâŠ¤â€‹Kâ€‹(t)â€‹xu(t,x)=x^{\top}K(t)x, and where Kâ€‹(t)K(t) satisfies a Riccati ODE. It offers analytic targets for u,âˆ‡u,âˆ‡2uu,\nabla u,\nabla^{2}u and a clean test of learning constant-in-space Hessians and optimal-feedback structure.

The same training and inference pipeline as described in the semi-linear case is used, with a KANO network trained on 40964096 samples. [FigureËœ5](https://arxiv.org/html/2511.01125v1#S4.F5 "In 4.2 Linearâ€“quadratic case â€£ 4 Experimental results â€£ One model to solve them all: 2BSDE families via neural operators") presents two random trajectories projected onto the (x1,x2)(x\_{1},x\_{2})-plane. The figure also compares the analytic solution uu, its gradient components âˆ‚u/âˆ‚x1\partial u/\partial x\_{1}, and the diagonal Hessian entries âˆ‚2u/âˆ‚x12\partial^{2}u/\partial x\_{1}^{2} with the corresponding model predictions along these paths. The predicted values of uu closely follow the analytical solution. The derivatives are recovered with satisfactory accuracy, and the Hessian, which is expected to remain constant in space, is also well captured. Although the estimated derivatives show some deviations from the smooth exact values, their overall accuracy remains high. In summary, the network effectively learns and reproduces the solution uu and its derivatives along the generated trajectories.

![Refer to caption](x3.png)


(a) Random paths

![Refer to caption](x4.png)


(b) Solutions

![Refer to caption](x5.png)


(c) First derivatives

![Refer to caption](x6.png)


(d) Second derivatives

Figure 3: Ground-truth and KANO-predicted solutions for the first randomly selected trajectory of the periodic semilinear example from [[13](https://arxiv.org/html/2511.01125v1#bib.bib13)]. Each panel shows the projection onto the (x1,x2)(x\_{1},x\_{2})-plane with uu, âˆ‚u/âˆ‚x1\partial u/\partial x\_{1}, and âˆ‚2u/âˆ‚x12\partial^{2}u/\partial x\_{1}^{2} along this path.



![Refer to caption](x7.png)


(a) Random paths

![Refer to caption](x8.png)


(b) Solutions

![Refer to caption](x9.png)


(c) First derivatives

![Refer to caption](x10.png)


(d) Second derivatives

Figure 4: Continuation of [FigureËœ3](https://arxiv.org/html/2511.01125v1#S4.F3 "In 4.2 Linearâ€“quadratic case â€£ 4 Experimental results â€£ One model to solve them all: 2BSDE families via neural operators"), showing the second randomly selected trajectory for the same semi-linear example.



![Refer to caption](x11.png)

![Refer to caption](x12.png)

![Refer to caption](x13.png)

![Refer to caption](x14.png)

![Refer to caption](x15.png)


(a) Random paths

![Refer to caption](x16.png)


(b) Solutions

![Refer to caption](x17.png)


(c) First derivatives

![Refer to caption](x18.png)


(d) Second derivatives

Figure 5: Comparison between the ground-truth and KANO-predicted solutions for the periodic linearâ€“quadratic example of [[78](https://arxiv.org/html/2511.01125v1#bib.bib78)]. The figure shows two randomly selected trajectories projected onto the (x1,x2)(x\_{1},x\_{2})-plane, together with the corresponding values of uu, âˆ‚u/âˆ‚x1\partial u/\partial x\_{1}, and âˆ‚2u/âˆ‚x12\partial^{2}u/\partial x\_{1}^{2} along these paths.

#### 4.2.1 Ablation on the sample size

We next train a model using eight times fewer training samples than before *i.e.*, 512 samples) and evaluate it following the same procedure as in previous experiments. The resulting quantities of interest are shown in [FigureËœ6](https://arxiv.org/html/2511.01125v1#S4.F6 "In 4.2.1 Ablation on the sample size â€£ 4.2 Linearâ€“quadratic case â€£ 4 Experimental results â€£ One model to solve them all: 2BSDE families via neural operators"). We observe that in the vicinity of t=0t=0, the solution uu is not well approximated, which in turn affects the accuracy of its first- and second-order partial derivatives. This behaviour is consistent with the theoretical discussion presented earlier: a sufficient number of training samples is required in the high-dimensional space â„d\mathbb{R}^{d} for the model to accurately capture the solution near t=0t=0.

![Refer to caption](x19.png)


(a) Random path

![Refer to caption](x20.png)


(b) Solution

![Refer to caption](x21.png)


(c) First derivative

![Refer to caption](x22.png)


(d) Second derivative

Figure 6: Comparison between the ground-truth and KANO-predicted solutions for the periodic linearâ€“quadratic example of [[78](https://arxiv.org/html/2511.01125v1#bib.bib78)] in *low training data regime*. The figure shows two randomly selected trajectories projected onto the (x1,x2)(x\_{1},x\_{2})-plane, together with the corresponding values of uu, âˆ‚u/âˆ‚x1\partial u/\partial x\_{1}, and âˆ‚2u/âˆ‚x12\partial^{2}u/\partial x\_{1}^{2} along these paths.

## Acknowledgements

Takashi Furuya was supported by JSPS KAKENHI Grant Number JP24K16949, 25H01453, JST CREST JPMJCR24Q5, JST ASPIRE JPMJAP2329.
Anastasis Kratsios acknowledges financial support from an NSERC Discovery Grant No. RGPIN-2023-04482 and No. DGECR-2023-00230, and they acknowledge that resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute999<https://vectorinstitute.ai/partnerships/current-partners/>; they would also like to thank Behnoosh Zamanlooy for her support.
Dylan PossamaÃ¯ gratefully acknowledges partial support by the SNF project MINT 205121-219818.

## Appendix A Proof of PDE results

### A.1 Proof of TheoremÂ [3.11](https://arxiv.org/html/2511.01125v1#S3.Thmtheorem11 "Theorem 3.11 (Exponential approximation rates: solution operator to the elliptic problem). â€£ 3.3.1 Semi-linear elliptic PDE â€£ 3.3 Feasible rates â€£ 3 Main results â€£ One model to solve them all: 2BSDE families via neural operators")

This appendix contains the proofs of our paperâ€™s main theoretical guarantees.

#### A.1.1 Well-posedness

Let GÎ³,Î¼,Î»â€‹(x,y)G\_{\gamma,\mu,\lambda}(x,y) be a (real-valued) Greenâ€™s function for âˆ’âˆ‡â‹…Î³â€‹âˆ‡+Î¼â‹…âˆ‡+Î»-\nabla\cdot\gamma\nabla+\mu\cdot\nabla+\lambda with a Dirichlet boundary condition, i.e., for yâˆˆğ’Ÿy\in\mathcal{D},

|  |  |  |
| --- | --- | --- |
|  | âˆ’âˆ‡â‹…Î³âˆ‡GÎ³,Î¼,Î»(â‹…,y)+Î¼â‹…âˆ‡GÎ³,Î¼,Î»(â‹…,y)+Î»GÎ³,Î¼,Î»(â‹…,y)=âˆ’Î´(â‹…âˆ’y)inğ’Ÿ,-\nabla\cdot\gamma\nabla G\_{\gamma,\mu,\lambda}(\cdot,y)+\mu\cdot\nabla G\_{\gamma,\mu,\lambda}(\cdot,y)+\lambda G\_{\gamma,\mu,\lambda}(\cdot,y)=-\delta(\cdot-y)\ \mathrm{in}\ \mathcal{D}, |  |

|  |  |  |
| --- | --- | --- |
|  | GÎ³,Î¼,Î»â€‹(â‹…,y)=0â€‹onâ€‹âˆ‚ğ’Ÿ.G\_{\gamma,\mu,\lambda}(\cdot,y)=0\ \mathrm{on}\ \partial\mathcal{D}. |  |

###### Lemma A.1.

Let [SectionËœ3.3.1](https://arxiv.org/html/2511.01125v1#S3.SS3.SSS1 "3.3.1 Semi-linear elliptic PDE â€£ 3.3 Feasible rates â€£ 3 Main results â€£ One model to solve them all: 2BSDE families via neural operators") hold.
Then, we have

|  |  |  |
| --- | --- | --- |
|  | GÎ³,Î¼,Î»âˆˆWs,pâ€‹(ğ’ŸÃ—ğ’Ÿ;â„).G\_{\gamma,\mu,\lambda}\in W^{s,p}(\mathcal{D}\times\mathcal{D};\mathbb{R}). |  |

where 1â‰¤p<ddâˆ’11\leq p<\frac{d}{d-1} and 1â‰¤s<21\leq s<2.

###### Proof.

From [[45](https://arxiv.org/html/2511.01125v1#bib.bib45), Theorem 8.1]101010Note that our setting is that Î³\gamma and Î¼\mu are smooth. Thus, they are uniformly Dini continuous, which implies that they are of Dini mean oscillation., the Green function GÎ³,Î¼,Î»â€‹(x,y)G\_{\gamma,\mu,\lambda}(x,y) for the operator Lâ€‹uâ‰”âˆ’âˆ‡â‹…Î³â€‹âˆ‡u+Î¼â‹…âˆ‡u+Î»â€‹uLu\coloneqq-\nabla\cdot\gamma\nabla u+\mu\cdot\nabla u+\lambda u can be estimated as for Î²âˆˆâ„•0d\beta\in\mathbb{N}^{d}\_{0} with |Î²|â‰¤1|\beta|\leq 1

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–âˆ‚xÎ²GÎ³,Î¼,Î»â€‹(x,y)â€–â‰¤C0â€‹â€–xâˆ’yâ€–1âˆ’d,\big\|\partial\_{x}^{\beta}G\_{\gamma,\mu,\lambda}(x,y)\big\|\leq C\_{0}\|x-y\|^{1-d}, |  | (A.1) |

where C0>0C\_{0}>0 is a constant depending on ğ’Ÿ\mathcal{D}, dd, Î²\beta, Î³\gamma, Î¼\mu, and Î»\lambda.
Also, applying [[45](https://arxiv.org/html/2511.01125v1#bib.bib45), Theorem 8.1] to the Green function gÎ³,Î¼,Î»â€‹(y,x)g\_{\gamma,\mu,\lambda}(y,x) for the adjoint operator LâŠ¤â€‹u=âˆ’âˆ‡â‹…(Î³âŠ¤â€‹âˆ‡u+Î¼â€‹u)+Î»L^{\top}u=-\nabla\cdot(\gamma^{\top}\nabla u+\mu u)+\lambda, the Green function gÎ³,Î¼,Î»â€‹(y,x)g\_{\gamma,\mu,\lambda}(y,x) can be estimated, for Î²âˆˆ(â„•â‹†)d\beta\in(\mathbb{N}^{\star})^{d} with â€–Î²â€–â‰¤1\|\beta\|\leq 1 by

|  |  |  |
| --- | --- | --- |
|  | â€–âˆ‚yÎ²gÎ³,Î¼,Î»â€‹(y,x)â€–â‰¤C0â€‹â€–yâˆ’xâ€–1âˆ’d.\big\|\partial\_{y}^{\beta}g\_{\gamma,\mu,\lambda}(y,x)\big\|\leq C\_{0}\|y-x\|^{1-d}. |  |

With [[45](https://arxiv.org/html/2511.01125v1#bib.bib45), Proposition 6.13] and [SectionËœ3.3.1](https://arxiv.org/html/2511.01125v1#S3.SS3.SSS1 "3.3.1 Semi-linear elliptic PDE â€£ 3.3 Feasible rates â€£ 3 Main results â€£ One model to solve them all: 2BSDE families via neural operators").(iâ€‹iâ€‹i)(iii), we see that Gâ€‹(x,y)=gâ€‹(y,x)G(x,y)=g(y,x) (xâ‰ yx\neq y), which implies that

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–âˆ‚yÎ²GÎ³,Î¼,Î»â€‹(x,y)â€–â‰¤C0â€‹â€–xâˆ’yâ€–1âˆ’d.\big\|\partial\_{y}^{\beta}G\_{\gamma,\mu,\lambda}(x,y)\big\|\leq C\_{0}\|x-y\|^{1-d}. |  | (A.2) |

We now choose R>0R>0 such that ğ’ŸâŠ‚Bâ„dâ€‹(0,R)\mathcal{D}\subset B\_{\mathbb{R}^{\text{$d$}}}(0,R).
Using ([A.2](https://arxiv.org/html/2511.01125v1#A1.E2 "Equation A.2 â€£ A.1.1 Well-posedness â€£ A.1 Proof of Theorem 3.11 â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators")), we estimate that for xâˆˆğ’Ÿx\in\mathcal{D} and Î²âˆˆ(â„•â‹†)d\beta\in(\mathbb{N}^{\star})^{d} with â€–Î²â€–â‰¤1\|\beta\|\leq 1

|  |  |  |  |
| --- | --- | --- | --- |
|  | âˆ«ğ’Ÿâ€–âˆ‚xÎ²GÎ³,Î¼,Î»â€‹(x,y)â€–pâ€‹dyâ‰²âˆ«ğ’Ÿâ€–xâˆ’yâ€–(1âˆ’d)â€‹pâ€‹dy=âˆ«xâˆ’ğ’Ÿâ€–zâ€–(1âˆ’d)â€‹pâ€‹dz\displaystyle\int\_{\mathcal{D}}\big\|\partial\_{x}^{\beta}G\_{\gamma,\mu,\lambda}(x,y)\big\|^{p}\mathrm{d}y\lesssim\int\_{\mathcal{D}}\|x-y\|^{(1-d)p}\mathrm{d}y=\int\_{x\text{$-$}\mathcal{D}}\|z\|^{(1-d)p}\mathrm{d}z | â‰¤âˆ«Bâ„dâ€‹(0,2â€‹R)â€–zâ€–(1âˆ’d)â€‹pâ€‹dz\displaystyle\leq\int\_{B\_{\text{$\mathbb{R}$}^{\text{$d$}}}(0,2R)}\|z\|^{(1-d)p}\mathrm{d}z |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | â‰²âˆ«02â€‹Rr(1âˆ’d)â€‹pâ€‹rdâˆ’1â€‹dr=âˆ«02â€‹Rr(dâˆ’1)â€‹(1âˆ’p)â€‹drâ‰²1,\displaystyle\lesssim\int\_{0}^{2R}r^{(1-d)p}r^{d-1}\mathrm{d}r=\int\_{0}^{2R}r^{(d-1)(1-p)}\mathrm{d}r\lesssim 1, |  | (A.3) |

where we have used that 1<p<ddâˆ’11<p<\frac{d}{d-1}.
We can obtain the estimate for the derivative with respect to yy similarly, using now ([A.2](https://arxiv.org/html/2511.01125v1#A1.E2 "Equation A.2 â€£ A.1.1 Well-posedness â€£ A.1 Proof of Theorem 3.11 â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators")).
Note that we use the symbol â‰²\lesssim to omit a multiplicative constant that is independent of xx on the left-hand side.
âˆ

Using the Green function GÎ³,Î¼,Î»â€‹(x,y)G\_{\gamma,\mu,\lambda}(x,y), we define an integral operator encoding ([1.5](https://arxiv.org/html/2511.01125v1#S1.E5 "Equation 1.5 â€£ 1 Introduction â€£ One model to solve them all: 2BSDE families via neural operators")) by:

|  |  |  |  |
| --- | --- | --- | --- |
|  | uâ€‹(x)â‰”âˆ«ğ’ŸGÎ³,Î¼,Î»â€‹(x,y)â€‹(f~â€‹(y,uâ€‹(y))âˆ’fâ€‹(y))â€‹dy+wgâ€‹(x),xâˆˆğ’Ÿ,u(x)\coloneqq\int\_{\mathcal{D}}G\_{\gamma,\mu,\lambda}(x,y)\big(\tilde{f}(y,u(y))-f(y)\big)\mathrm{d}y+w\_{g}(x),\;x\in\mathcal{D}, |  | (A.4) |

where f0âˆˆW1,âˆâ€‹(ğ’Ÿ;â„)f\_{0}\in W^{1,\infty}(\mathcal{D};\mathbb{R}) and wgâ€‹(x)âˆˆWd+42,2â€‹(ğ’Ÿ;â„)w\_{g}(x)\in W^{\frac{d+4}{2},2}(\mathcal{D};\mathbb{R}) is the unique solution of

|  |  |  |
| --- | --- | --- |
|  | âˆ’âˆ‡â‹…Î³â€‹âˆ‡wg+Î¼â‹…âˆ‡wg+Î»â€‹wg=0,onâ€‹D,wg=g,onâ€‹âˆ‚D.-\nabla\cdot\gamma\nabla w\_{g}+\mu\cdot\nabla w\_{g}+\lambda w\_{g}=0,\;\mathrm{on}\;D,\;w\_{g}=g,\;\mathrm{on}\;\partial D. |  |

where gâˆˆWd+32,2â€‹(âˆ‚ğ’Ÿ)g\in W^{\frac{d+3}{2},2}(\partial\mathcal{D}).
Note that, it is well known that a linear elliptic equation has the unique solution wgw\_{g} (see, e.g., [[37](https://arxiv.org/html/2511.01125v1#bib.bib37)]).
By the Sobolev embedding theorem (see, *e.g.*, [Evans](https://arxiv.org/html/2511.01125v1#bib.bib26) [[26](https://arxiv.org/html/2511.01125v1#bib.bib26), Section 5.6.3]) we have

|  |  |  |
| --- | --- | --- |
|  | W(d+4)/2,2â€‹(ğ’Ÿ)âŠ‚C(d+4)/2âˆ’d/2âˆ’1,Î¾0â€‹(ğ’ŸÂ¯)âŠ‚W1,âˆâ€‹(ğ’Ÿ),W^{(d+4)/2,2}(\mathcal{D})\subset C^{(d+4)/2-d/2-1,\xi\_{0}}(\overline{\mathcal{D}})\subset W^{1,\infty}(\mathcal{D}), |  |

where 0<Î¾0<10<\xi\_{0}<1 is a constant. Hence, wgâˆˆW1,âˆâ€‹(ğ’Ÿ)w\_{g}\in W^{1,\infty}(\mathcal{D}).
We define next the mapping TT by

|  |  |  |
| --- | --- | --- |
|  | Tâ€‹(u)â€‹(x)â‰”âˆ«ğ’ŸGÎ³,Î¼,Î»â€‹(x,y)â€‹(f~â€‹(y,uâ€‹(y))âˆ’f0â€‹(y))â€‹dy+wgâ€‹(x),xâˆˆğ’Ÿ,T(u)(x)\coloneqq\int\_{\mathcal{D}}G\_{\gamma,\mu,\lambda}(x,y)\big(\tilde{f}(y,u(y))-f\_{0}(y)\big)\mathrm{d}y+w\_{g}(x),\;x\in\mathcal{D}, |  |

We set

|  |  |  |
| --- | --- | --- |
|  | BW1,âˆâ€‹(0,Î´)â‰”{uâˆˆW1,âˆâ€‹(ğ’Ÿ;â„):â€–uâ€–W1,âˆâ€‹(ğ’Ÿ;â„)â‰¤Î´},B\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}}(0,\delta)\coloneqq\big\{u\in W^{1,\infty}(\mathcal{D};\mathbb{R}):\|u\|\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}(\mathcal{D};\mathbb{R})}\leq\delta\big\}, |  |

|  |  |  |
| --- | --- | --- |
|  | BW(d+3)/2,2â€‹(0,Î´)â‰”{gâˆˆW(d+3)/2,2â€‹(âˆ‚ğ’Ÿ;â„):â€–gâ€–Wd+3)/2,2â€‹(âˆ‚ğ’Ÿ;â„)â‰¤Î´}.B\_{W^{\text{$(d+3)/2$}\text{$,$}\text{$2$}}}(0,\delta)\coloneqq\big\{g\in W^{(d+3)/2,2}(\partial\mathcal{D};\mathbb{R}):\|g\|\_{W^{\text{$d+3)/2$}\text{$,$}\text{$2$}}(\partial\mathcal{D};\mathbb{R})}\leq\delta\big\}. |  |

Then, BW1,âˆâ€‹(0,Î´)B\_{W^{1,\infty}}(0,\delta) is a closed subset in W1,âˆâ€‹(ğ’Ÿ;â„)W^{1,\infty}(\mathcal{D};\mathbb{R}).

###### Lemma A.2.

Let [SectionsËœ3.3.1](https://arxiv.org/html/2511.01125v1#S3.SS3.SSS1 "3.3.1 Semi-linear elliptic PDE â€£ 3.3 Feasible rates â€£ 3 Main results â€£ One model to solve them all: 2BSDE families via neural operators"), [3.3.1](https://arxiv.org/html/2511.01125v1#S3.SS3.SSS1 "3.3.1 Semi-linear elliptic PDE â€£ 3.3 Feasible rates â€£ 3 Main results â€£ One model to solve them all: 2BSDE families via neural operators") andÂ [3.3.1](https://arxiv.org/html/2511.01125v1#S3.SS3.SSS1 "3.3.1 Semi-linear elliptic PDE â€£ 3.3 Feasible rates â€£ 3 Main results â€£ One model to solve them all: 2BSDE families via neural operators") hold.
Let fâˆˆBW1,âˆâ€‹(ğ’Ÿ;â„)â€‹(0,Î´2)f\in B\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}(\mathcal{D};\mathbb{R})}(0,\delta^{2}) and gâˆˆBW(d+3)/2,2â€‹(âˆ‚ğ’Ÿ;â„)â€‹(0,Î´2)g\in B\_{W^{\text{$($}\text{$d$}\text{$+$}\text{$3$}\text{$)$}\text{$/$}\text{$2$},\text{$2$}}(\partial\mathcal{D};\mathbb{R})}(0,\delta^{2}). Then, the map T:BW1,âˆâ€‹(0,Î´)âŸ¶BW1,âˆâ€‹(0,Î´)T:B\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}}(0,\delta)\longrightarrow B\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}}(0,\delta) is a Ï\rho-contraction where Ïâˆˆ(0,1)\rho\in(0,1) is defined in [SectionËœ3.3.1](https://arxiv.org/html/2511.01125v1#S3.SS3.SSS1 "3.3.1 Semi-linear elliptic PDE â€£ 3.3 Feasible rates â€£ 3 Main results â€£ One model to solve them all: 2BSDE families via neural operators").
In particular, there exists a unique solution of ([A.4](https://arxiv.org/html/2511.01125v1#A1.E4 "Equation A.4 â€£ A.1.1 Well-posedness â€£ A.1 Proof of Theorem 3.11 â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators")) in BW1,âˆâ€‹(0,Î´)B\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}}(0,\delta).

###### Proof.

We see that for xâˆˆğ’Ÿx\in\mathcal{D}

|  |  |  |  |
| --- | --- | --- | --- |
|  | Tâ€‹(w)â€‹(x)\displaystyle T(w)(x) | â‰”âˆ«ğ’ŸGÎ³,Î¼,Î»â€‹(x,y)â€‹[f~â€‹(y,wâ€‹(y))âˆ’f0â€‹(y)]â€‹dy+wgâ€‹(x)\displaystyle\coloneqq\int\_{\mathcal{D}}G\_{\gamma,\mu,\lambda}(x,y)\big[\tilde{f}(y,w(y))-f\_{0}(y)\big]\mathrm{d}y+w\_{g}(x) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =âˆ«ğ’ŸGÎ³,Î¼,Î»â€‹(x,y)â€‹(âˆ‘h=2Hâˆ‚zhf~â€‹(y,0)h!â€‹wâ€‹(y)hâˆ’f0â€‹(y))â€‹dy+wgâ€‹(x)\displaystyle=\int\_{\mathcal{D}}G\_{\gamma,\mu,\lambda}(x,y)\Bigg(\sum\_{h=2}^{H}\frac{\partial^{h}\_{z}\tilde{f}(y,0)}{h!}w(y)^{h}-f\_{0}(y)\Bigg)\mathrm{d}y+w\_{g}(x) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =âˆ‘h=2H1h!â€‹âˆ«ğ’ŸGÎ³,Î¼,Î»â€‹(x,y)â€‹âˆ‚zhf~â€‹(y,0)â€‹wâ€‹(y)hâ€‹dâ€‹yâˆ’âˆ«ğ’ŸGÎ³,Î¼,Î»â€‹(x,y)â€‹f0â€‹(y)â€‹dy+wgâ€‹(x).\displaystyle=\sum\_{h=2}^{H}\frac{1}{h!}\int\_{\mathcal{D}}G\_{\gamma,\mu,\lambda}(x,y)\partial^{h}\_{z}\tilde{f}(y,0)w(y)^{h}\mathrm{d}y-\int\_{\mathcal{D}}G\_{\gamma,\mu,\lambda}(x,y)f\_{0}(y)\mathrm{d}y+w\_{g}(x). |  |

First, we will show that T:BW1,âˆâ€‹(0,Î´)âŸ¶BW1,âˆâ€‹(0,Î´)T:B\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}}(0,\delta)\longrightarrow B\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}}(0,\delta).
Let wâˆˆBW1,âˆâ€‹(0,Î´)w\in B\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}}(0,\delta).
Using this, that f0f\_{0}, and wgw\_{g} are both in BW1,âˆâ€‹(0,Î´2)B\_{W^{1,\text{$\infty$}}}(0,\delta^{2}), and [SectionËœA.1.1](https://arxiv.org/html/2511.01125v1#A1.SS1.SSS1 "A.1.1 Well-posedness â€£ A.1 Proof of Theorem 3.11 â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators"), we see that for any Î²âˆˆ(â„•â‹†)d\beta\in(\mathbb{N}^{\star})^{d} with â€–Î²â€–â‰¤1\|\beta\|\leq 1, we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–âˆ‚xÎ²Tâ€‹(w)â€‹(x)â€–\displaystyle\big\|\partial\_{x}^{\beta}T(w)(x)\big\| | â‰²âˆ«ğ’Ÿâ€–âˆ‚xÎ²GÎ³,Î¼,Î»â€‹(x,y)â€–â€‹(âˆ‘h=2H1h!â€‹|wâ€‹(y)|h+|f0â€‹(y)|)â€‹dy+â€–âˆ‚xÎ²wgâ€‹(x)â€–\displaystyle\lesssim\int\_{\mathcal{D}}\big\|\partial\_{x}^{\beta}G\_{\gamma,\mu,\lambda}(x,y)\big\|\Bigg(\sum\_{h=2}^{H}\frac{1}{h!}|w(y)|^{h}+|f\_{0}(y)|\Bigg)\mathrm{d}y+\big\|\partial\_{x}^{\beta}w\_{g}(x)\big\| |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | â‰²Î´2â€‹âˆ«ğ’Ÿâ€–âˆ‚xÎ²GÎ³,Î¼,Î»â€‹(x,y)â€–â€‹dy+Î´2â‰²Î´2.\displaystyle\lesssim\delta^{2}\int\_{\mathcal{D}}\big\|\partial\_{x}^{\beta}G\_{\gamma,\mu,\lambda}(x,y)\big\|\mathrm{d}y+\delta^{2}\lesssim\delta^{2}. |  | (A.5) |

This means that Tâ€‹(w)âˆˆW1,âˆâ€‹(ğ’Ÿ;â„)T(w)\in W^{1,\infty}(\mathcal{D};\mathbb{R}).
We also see that

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–Tâ€‹(w)â€–W1,âˆâ€‹(ğ’Ÿ;â„)â‰¤C1â€‹Î´2,\|T(w)\|\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}(\mathcal{D};\mathbb{R})}\leq C\_{1}\delta^{2}, |  | (A.6) |

where C1>0C\_{1}>0 is a constant depending on pp, dd, ğ’Ÿ\mathcal{D}, f~\tilde{f}, Î³\gamma, and Î¼\mu.
By choosing Î´>0\delta>0 in [SectionËœ3.3.1](https://arxiv.org/html/2511.01125v1#S3.SS3.SSS1 "3.3.1 Semi-linear elliptic PDE â€£ 3.3 Feasible rates â€£ 3 Main results â€£ One model to solve them all: 2BSDE families via neural operators"), we have Tâ€‹wâˆˆBW1,âˆâ€‹(0,Î´)Tw\in B\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}}(0,\delta).

Next, we will show that T:BW1,âˆâ€‹(0,Î´)âŸ¶BW1,âˆâ€‹(0,Î´)T:B\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}}(0,\delta)\longrightarrow B\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}}(0,\delta) is a contraction mapping.
Let (w1,w2)âˆˆBW1,âˆâ€‹(0,Î´)Ã—BW1,âˆâ€‹(0,Î´)(w\_{1},w\_{2})\in B\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}}(0,\delta)\times B\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}}(0,\delta).
Since

|  |  |  |  |
| --- | --- | --- | --- |
|  | w1â€‹(y)hâˆ’w2â€‹(y)h\displaystyle w\_{1}(y)^{h}-w\_{2}(y)^{h} | =(âˆ‘i=0hâˆ’1w1â€‹(y)hâˆ’1âˆ’iâ€‹w2â€‹(y)i)â€‹(w1â€‹(y)âˆ’w2â€‹(y)),\displaystyle=\Bigg(\sum\_{i=0}^{h-1}w\_{1}(y)^{h-1-i}w\_{2}(y)^{i}\Bigg)\big(w\_{1}(y)-w\_{2}(y)\big), |  |

we deduce that for any Î²âˆˆ(â„•â‹†)d\beta\in(\mathbb{N}^{\star})^{d} with â€–Î²â€–â‰¤1\|\beta\|\leq 1,
by HÃ¶lderâ€™s inequality and [SectionËœA.1.1](https://arxiv.org/html/2511.01125v1#A1.SS1.SSS1 "A.1.1 Well-posedness â€£ A.1 Proof of Theorem 3.11 â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators")

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–âˆ‚xÎ²Tâ€‹(w1)â€‹(x)âˆ’âˆ‚xÎ²Tâ€‹(w2)â€‹(x)â€–\displaystyle\big\|\partial\_{x}^{\beta}T(w\_{1})(x)-\partial\_{x}^{\beta}T(w\_{2})(x)\big\| | â‰²âˆ‘h=2H1h!â€‹âˆ«ğ’Ÿâ€–âˆ‚xÎ²GÎ³,Î¼,Î»â€‹(x,y)â€–â€‹|w1â€‹(y)hâˆ’w2â€‹(y)h|â€‹dy\displaystyle\lesssim\sum\_{h=2}^{H}\frac{1}{h!}\int\_{\mathcal{D}}\big\|\partial\_{x}^{\beta}G\_{\gamma,\mu,\lambda}(x,y)\big\|\big|w\_{1}(y)^{h}-w\_{2}(y)^{h}\big|\mathrm{d}y |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤âˆ‘h=2H1h!â€‹âˆ‘i=0hâˆ’1âˆ«ğ’Ÿâ€–âˆ‚xÎ²GÎ³,Î¼,Î»â€‹(x,y)â€–â€‹|w1â€‹(y)hâˆ’1âˆ’iâ€‹w2â€‹(y)i|â€‹|w1â€‹(y)âˆ’w2â€‹(y)|â€‹dy\displaystyle\leq\sum\_{h=2}^{H}\frac{1}{h!}\sum\_{i=0}^{h-1}\int\_{\mathcal{D}}\big\|\partial\_{x}^{\beta}G\_{\gamma,\mu,\lambda}(x,y)\big\|\big|w\_{1}(y)^{h-1-i}w\_{2}(y)^{i}\big|\big|w\_{1}(y)-w\_{2}(y)\big|\mathrm{d}y |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤âˆ‘h=2Hhh!â€‹Î´hâˆ’1â€‹âˆ«ğ’Ÿâ€–âˆ‚xÎ²GÎ³,Î¼,Î»â€‹(x,y)â€–â‰²Î´â€‹â€–w1âˆ’w2â€–W1,âˆâ€‹(ğ’Ÿ;â„).\displaystyle\leq\sum\_{h=2}^{H}\frac{h}{h!}\delta^{h-1}\int\_{\mathcal{D}}\big\|\partial\_{x}^{\beta}G\_{\gamma,\mu,\lambda}(x,y)\big\|\lesssim\delta\big\|w\_{1}-w\_{2}\big\|\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}(\mathcal{D};\mathbb{R})}. |  |

Then, we have that

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–Tâ€‹(w1)âˆ’Tâ€‹(w2)â€–W1,âˆâ€‹(ğ’Ÿ;â„)â‰¤C2â€‹Î´â€‹â€–w1âˆ’w2â€–W1,âˆâ€‹(ğ’Ÿ;â„)=Ïâ€‹â€–w1âˆ’w2â€–W1,âˆâ€‹(ğ’Ÿ;â„),\big\|T(w\_{1})-T(w\_{2})\big\|\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}(\mathcal{D};\mathbb{R})}\leq C\_{2}\delta\|w\_{1}-w\_{2}\|\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}(\mathcal{D};\mathbb{R})}=\rho\|w\_{1}-w\_{2}\|\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}(\mathcal{D};\mathbb{R})}, |  | (A.7) |

where C2>0C\_{2}>0 is a constant depending on pp, dd, ğ’Ÿ\mathcal{D}, f~\tilde{f}, Î³\gamma, and Î¼\mu. By choosing Î´>0\delta>0 as in [SectionËœ3.3.1](https://arxiv.org/html/2511.01125v1#S3.SS3.SSS1 "3.3.1 Semi-linear elliptic PDE â€£ 3.3 Feasible rates â€£ 3 Main results â€£ One model to solve them all: 2BSDE families via neural operators"), we have that TT is Ï\rho-contraction mapping in BW1,âˆâ€‹(0,Î´)B\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}}(0,\delta).
âˆ

Given the previous result, and using Banachâ€™s fixed-point theorem, the following solution operator is well-defined

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î“+:BW1,âˆâ€‹(0,Î´2)Ã—BW(d+3)/2,2â€‹(0,Î´2)\displaystyle\Gamma^{\text{$+$}}:B\_{W^{\text{$1$},\text{$\infty$}}}(0,\delta^{2})\times B\_{W^{\text{$($}\text{$d$}\text{$+$}\text{$3$}\text{$)$}\text{$/$}\text{$2$},\text{$2$}}}(0,\delta^{2}) | âŸ¶BW1,âˆâ€‹(0,Î´)\displaystyle\longrightarrow B\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}}(0,\delta) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | (f0,g)\displaystyle(f\_{0},g) | âŸ¼u,\displaystyle\longmapsto u, |  |

where, uu is the unique solution of [EquationËœA.4](https://arxiv.org/html/2511.01125v1#A1.E4 "In A.1.1 Well-posedness â€£ A.1 Proof of Theorem 3.11 â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators") in BW1,âˆâ€‹(0,Î´)B\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}}(0,\delta).

#### A.1.2 Proof of TheoremÂ [3.11](https://arxiv.org/html/2511.01125v1#S3.Thmtheorem11 "Theorem 3.11 (Exponential approximation rates: solution operator to the elliptic problem). â€£ 3.3.1 Semi-linear elliptic PDE â€£ 3.3 Feasible rates â€£ 3 Main results â€£ One model to solve them all: 2BSDE families via neural operators")

We now prove TheoremÂ [3.11](https://arxiv.org/html/2511.01125v1#S3.Thmtheorem11 "Theorem 3.11 (Exponential approximation rates: solution operator to the elliptic problem). â€£ 3.3.1 Semi-linear elliptic PDE â€£ 3.3 Feasible rates â€£ 3 Main results â€£ One model to solve them all: 2BSDE families via neural operators") in a series of several steps. Throughout, the activation function applied component-wise to the neural operator layers in neural operatorâ€™s neurons, *i.e.* inÂ ([2.9](https://arxiv.org/html/2511.01125v1#S2.E9 "Equation 2.9 â€£ 2.2.2 Neural operator architectures â€£ 2.2 Deep learning â€£ 2 Preliminaries â€£ One model to solve them all: 2BSDE families via neural operators")), will always be taken to be the squared-ReLU function, that is to say Î²=(1,0,â€¦,0)\beta=(1,0,\dots,0) inÂ ([2.1](https://arxiv.org/html/2511.01125v1#S2.E1 "Equation 2.1 â€£ 2.2.1 Residual Kolmogorovâ€“Arnold networks (Resâ€“KANs) â€£ 2.2 Deep learning â€£ 2 Preliminaries â€£ One model to solve them all: 2BSDE families via neural operators")) for the neural operator.

Let (f0,g)âˆˆBW1,âˆâ€‹(0,Î´2)Ã—BW(d+3)/2,2â€‹(0,Î´2)(f\_{0},g)\in B\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}}(0,\delta^{2})\times B\_{W^{\text{$($}\text{$d$}\text{$+$}\text{$3$}\text{$)$}\text{$/$}\text{$2$},2}}(0,\delta^{2}) and let uâˆˆBW1,âˆâ€‹(0,Î´)u\in B\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}}(0,\delta) be a solution of ([A.4](https://arxiv.org/html/2511.01125v1#A1.E4 "Equation A.4 â€£ A.1.1 Well-posedness â€£ A.1 Proof of Theorem 3.11 â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators")), that is, Î“+â€‹(f,g)=u\Gamma^{\text{$+$}}(f,g)=u.
ByÂ [[48](https://arxiv.org/html/2511.01125v1#bib.bib48), Theorem 1], for any Îµ>0\varepsilon>0, there exist Resâ€“KANs, with representation as in [SectionËœ2.2.1](https://arxiv.org/html/2511.01125v1#S2.SS2.SSS1 "2.2.1 Residual Kolmogorovâ€“Arnold networks (Resâ€“KANs) â€£ 2.2 Deep learning â€£ 2 Preliminaries â€£ One model to solve them all: 2BSDE families via neural operators"), knâ€‹nh:â„dâŸ¶â„k\_{nn}^{h}:\mathbb{R}^{d}\longrightarrow\mathbb{R}, hâˆˆ{2,â€¦,H}h\in\{2,\dots,H\}, and knâ€‹nâ€²:â„dâŸ¶â„k^{\prime}\_{nn}:\mathbb{R}^{d}\longrightarrow\mathbb{R}
such that

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–knâ€‹nhâ€‹(x,y)âˆ’1h!â€‹GÎ³,Î¼,Î»â€‹(x,y)â€‹âˆ‚zhf~â€‹(y,0)â€–Wx,y1,pâ€‹(ğ’ŸÃ—ğ’Ÿ;â„)â‰¤Îµ,hâˆˆ{2,â€¦,H},\bigg\|k\_{nn}^{h}(x,y)-\frac{1}{h!}G\_{\gamma,\mu,\lambda}(x,y)\partial^{h}\_{z}\tilde{f}(y,0)\bigg\|\_{W^{\text{$1$}\text{$,$}\text{$p$}}\_{\text{$x$}\text{$,$}\text{$y$}}(\mathcal{D}\times\mathcal{D};\mathbb{R})}\leq\varepsilon,\;h\in\{2,\dots,H\}, |  | (A.8) |

and

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–knâ€‹nâ€²â€‹(x,y)âˆ’GÎ³,Î¼,Î»â€‹(x,y)â€–Wx,y1,pâ€‹(ğ’ŸÃ—ğ’Ÿ;â„)â‰¤Îµ,\big\|k^{\prime}\_{nn}(x,y)-G\_{\gamma,\mu,\lambda}(x,y)\big\|\_{W^{\text{$1$}\text{$,$}\text{$p$}}\_{\text{$x$}\text{$,$}\text{$y$}}(\mathcal{D}\times\mathcal{D};\mathbb{R})}\leq\varepsilon, |  | (A.9) |

where depths L^â€‹(knâ€‹nh)\widehat{L}(k\_{nn}^{h})
and L^â€‹(knâ€‹nâ€²)\widehat{L}(k^{\prime}\_{nn}) are of order ğ’ªâ€‹(1)\mathcal{O}(1), while the width of W^â€‹(knâ€‹nh)\widehat{W}(k\_{nn}^{h}) and W^â€‹(knâ€‹nâ€²)\widehat{W}(k^{\prime}\_{nn}) are of order ğ’ªâ€‹(Îµâˆ’1(sâˆ’1)â€‹p)\mathcal{O}(\varepsilon^{-\frac{1}{(s-1)p}}). Then, we define by

|  |  |  |
| --- | --- | --- |
|  | L^:=L^â€‹(Î“):=maxâ¡{L^â€‹(knâ€‹n1),â€¦,L^â€‹(knâ€‹nH),L^â€‹(knâ€‹nâ€²)},W^:=W^â€‹(Î“):=maxâ¡{W^â€‹(knâ€‹n1),â€¦,W^â€‹(knâ€‹nH),W^â€‹(knâ€‹nâ€²)},\widehat{L}:=\widehat{L}(\Gamma):=\max\{\widehat{L}(k\_{nn}^{1}),...,\widehat{L}(k\_{nn}^{H}),\widehat{L}(k^{\prime}\_{nn})\},\quad\widehat{W}:=\widehat{W}(\Gamma):=\max\{\widehat{W}(k\_{nn}^{1}),...,\widehat{W}(k\_{nn}^{H}),\widehat{W}(k^{\prime}\_{nn})\}, |  |

Then, they are estimated by

|  |  |  |  |
| --- | --- | --- | --- |
|  | {L^â‰¤C,W^â‰¤Câ€‹Îµâˆ’1(sâˆ’1)â€‹p,\begin{cases}\widehat{L}\leq C,\\[8.00003pt] \widehat{W}\leq C\varepsilon^{-\frac{1}{(s-1)p}},\end{cases} |  | (A.10) |

where C>0C>0 is a constant depending on dd, ss, HH, and pp.
We can then define the map TNNT\_{\text{$N$}\text{$N$}} by

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | TNNâ€‹(u)â€‹(x)\displaystyle T\_{\text{$N$}\text{$N$}}(u)(x) | â‰”âˆ‘h=2Hâˆ«ğ’Ÿknâ€‹nhâ€‹(x,y)â€‹(uâ€‹(y))hâ€‹dyâˆ’âˆ«ğ’Ÿknâ€‹nâ€²â€‹(x,y)â€‹fâ€‹(y)â€‹dy+wgâ€‹(x).\displaystyle\coloneqq\sum\_{h=2}^{H}\int\_{\mathcal{D}}k\_{nn}^{h}(x,y)(u(y))^{h}\mathrm{d}y-\int\_{\mathcal{D}}k^{\prime}\_{nn}(x,y)f(y)\mathrm{d}y+w\_{g}(x). |  | (A.11) |

###### Lemma A.3.

There exists a constant C4>0C\_{4}>0 depending on pp, dd, ğ’Ÿ\mathcal{D}, Î³\gamma, Î¼\mu, and Î»\lambda such that for any uâˆˆBW1,âˆâ€‹(ğ’Ÿ;â„)â€‹(0,Î´)u\in B\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}(\mathcal{D};\mathbb{R})}(0,\delta)

|  |  |  |
| --- | --- | --- |
|  | â€–Tâ€‹(u)âˆ’TNNâ€‹(u)â€–W1,âˆâ€‹(ğ’Ÿ;â„)â‰¤C4â€‹Îµ.\big\|T(u)-T\_{\text{$N$}\text{$N$}}(u)\big\|\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}(\mathcal{D};\mathbb{R})}\leq C\_{4}\varepsilon. |  |

###### Proof.

Let uâˆˆBW1,âˆâ€‹(ğ’Ÿ;â„)â€‹(0,Î´)u\in B\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}(\mathcal{D};\mathbb{R})}(0,\delta).
We see that for Î²âˆˆ(â„•â‹†)d\beta\in(\mathbb{N}^{\star})^{d} with â€–Î²â€–â‰¤1\|\beta\|\leq 1,

|  |  |  |  |
| --- | --- | --- | --- |
|  | |âˆ‚xÎ²Tâ€‹(u)â€‹(x)âˆ’âˆ‚xÎ²TNNâ€‹(u)â€‹(x)|\displaystyle\big|\partial\_{x}^{\beta}T(u)(x)-\partial\_{x}^{\beta}T\_{\text{$N$}\text{$N$}}(u)(x)\big| | â‰¤âˆ‘h=2Hâ€–knâ€‹nhâ€‹(x,y)âˆ’1h!â€‹GÎ³,Î¼,Î»â€‹(x,y)â€‹âˆ‚zhfâ€‹(y,0)â€–Wx,y1,pâ€‹(ğ’Ÿ;â„)â€‹(âˆ«ğ’Ÿ|uâ€‹(y)h|pâ€²â€‹dy)1/pâ€²\displaystyle\leq\sum\_{h=2}^{H}\bigg\|k\_{nn}^{h}(x,y)-\frac{1}{h!}G\_{\gamma,\mu,\lambda}(x,y)\partial^{h}\_{z}f(y,0)\bigg\|\_{W^{\text{$1$}\text{$,$}\text{$p$}}\_{\text{$x$}\text{$,$}\text{$y$}}(\mathcal{D};\mathbb{R})}\bigg(\int\_{\mathcal{D}}|u(y)^{h}|^{p^{\text{$\prime$}}}\mathrm{d}y\bigg)^{1/p^{\text{$\prime$}}} |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | +â€–knâ€‹nâ€²â€‹(x,y)âˆ’GÎ³,Î¼,Î»â€‹(x,y)â€–Wx,y1,pâ€‹(ğ’Ÿ;â„)â€‹(âˆ«ğ’Ÿ|fâ€‹(y)|pâ€²â€‹dy)1/pâ€²â‰¤C4â€‹Î´2â€‹Îµ<Îµ,\displaystyle\quad+\big\|k^{\prime}\_{nn}(x,y)-G\_{\gamma,\mu,\lambda}(x,y)\big\|\_{W^{\text{$1$}\text{$,$}\text{$p$}}\_{\text{$x$}\text{$,$}\text{$y$}}(\mathcal{D};\mathbb{R})}\bigg(\int\_{\mathcal{D}}|f(y)|^{p^{\text{$\prime$}}}\mathrm{d}y\bigg)^{1/p^{\text{$\prime$}}}\leq C\_{4}\delta^{2}\varepsilon<\varepsilon, |  | (A.12) |

which is exactly the desired result.
âˆ

###### Lemma A.4.

TNNT\_{\text{$N$}\text{$N$}} maps BW1,âˆâ€‹(0,Î´)B\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}}(0,\delta) to itself.

###### Proof.

Fix uâˆˆBW1,âˆâ€‹(ğ’Ÿ;â„)â€‹(0,Î´)u\in B\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}(\mathcal{D};\mathbb{R})}(0,\delta).
Using ([A.6](https://arxiv.org/html/2511.01125v1#A1.E6 "Equation A.6 â€£ A.1.1 Well-posedness â€£ A.1 Proof of Theorem 3.11 â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators")) and ([A.12](https://arxiv.org/html/2511.01125v1#A1.E12 "Equation A.12 â€£ A.1.2 Proof of Theorem 3.11 â€£ A.1 Proof of Theorem 3.11 â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators")), we see that

|  |  |  |
| --- | --- | --- |
|  | â€–TNNâ€‹(u)â€–W1,âˆâ€‹(ğ’Ÿ;â„)â‰¤â€–TNNâ€‹(u)â€–W1,âˆâ€‹(ğ’Ÿ;â„)+â€–Tâ€‹(u)âˆ’TNNâ€‹(u)â€–W1,âˆâ€‹(ğ’Ÿ;â„)â‰²Î´2.\big\|T\_{\text{$N$}\text{$N$}}(u)\big\|\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}(\mathcal{D};\mathbb{R})}\leq\big\|T\_{\text{$N$}\text{$N$}}(u)\big\|\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}(\mathcal{D};\mathbb{R})}+\big\|T(u)-T\_{\text{$N$}\text{$N$}}(u)\big\|\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}(\mathcal{D};\mathbb{R})}\lesssim\delta^{2}. |  |

Thus, we have that

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–TNNâ€‹(u)â€–W1,âˆâ€‹(ğ’Ÿ;â„)â‰¤C3â€‹Î´2,\|T\_{\text{$N$}\text{$N$}}(u)\|\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}(\mathcal{D};\mathbb{R})}\leq C\_{3}\delta^{2}, |  | (A.13) |

where C3>0C\_{3}>0 is a constant depending on ss, pp, dd, ğ’Ÿ\mathcal{D}, and Î³\gamma. By the choice of Î´\delta in [SectionËœ3.3.1](https://arxiv.org/html/2511.01125v1#S3.SS3.SSS1 "3.3.1 Semi-linear elliptic PDE â€£ 3.3 Feasible rates â€£ 3 Main results â€£ One model to solve them all: 2BSDE families via neural operators"), we see that â€–TNNâ€‹(u)â€–W1,âˆâ€‹(ğ’Ÿ;â„)â‰¤Î´\|T\_{\text{$N$}\text{$N$}}(u)\|\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}(\mathcal{D};\mathbb{R})}\leq\delta.

âˆ

We can now define for an arbitrary positive integer JJ, the map Î“J:BW1,âˆâ€‹(0,Î´2)Ã—BW1,âˆâ€‹(0,Î´2)âŸ¶W1,âˆâ€‹(ğ’Ÿ;â„)\Gamma\_{\text{$J$}}:B\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}}(0,\delta^{2})\times B\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}}(0,\delta^{2})\longrightarrow W^{1,\infty}(\mathcal{D};\mathbb{R}) by

|  |  |  |
| --- | --- | --- |
|  | Î“Jâ€‹(f0,wg)â‰”TNâ€‹Nâˆ˜â‹¯âˆ˜TNNâŸJâ€‹timesâ€‹(0)â‰•TNN[J]â€‹(0).\Gamma\_{\text{$J$}}(f\_{0},w\_{g})\coloneqq\underbrace{T\_{NN}\circ\cdots\circ T\_{\text{$N$}\text{$N$}}}\_{J\;\text{\rm times}}(0)\eqqcolon T\_{\text{$N$}\text{$N$}}^{[\text{$J$}]}(0). |  |

###### Lemma A.5.

Let Jâ‰”âŒˆlogâ¡(1/Îµ)/logâ¡(1/Ï)âŒ‰âˆˆâ„•J\coloneqq\lceil\log(1/\varepsilon)/\log(1/\rho)\rceil\in\mathbb{N}.
Then, there exists a constant C5>0C\_{5}>0 depending on pp, dd, ğ’Ÿ\mathcal{D}, Î³\gamma, Î¼\mu, and Î»\lambda such that for all (f0,g)âˆˆBW1,âˆâ€‹(0,Î´2)Ã—BW(d+3)/2,2â€‹(0,Î´2)(f\_{0},g)\in B\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}}(0,\delta^{2})\times B\_{W^{\text{$($}\text{$d$}\text{$+$}\text{$3$}\text{$)$}\text{$/$}\text{$2$},2}}(0,\delta^{2})

|  |  |  |
| --- | --- | --- |
|  | â€–Î“+â€‹(f0,g)âˆ’Î“Jâ€‹(f0,wg)â€–W1,âˆâ€‹(ğ’Ÿ)â‰¤C5â€‹Îµ.\big\|\Gamma^{\text{$+$}}(f\_{0},g)-\Gamma\_{\text{$J$}}(f\_{0},w\_{g})\big\|\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}(\mathcal{D})}\leq C\_{5}\varepsilon. |  |

###### Proof.

From [SectionËœA.1.1](https://arxiv.org/html/2511.01125v1#A1.SS1.SSS1 "A.1.1 Well-posedness â€£ A.1 Proof of Theorem 3.11 â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators"), T:BW1,âˆâ€‹(0,Î´)âŸ¶BW1,âˆâ€‹(0,Î´)T:B\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}}(0,\delta)\longrightarrow B\_{W^{1,\infty}}(0,\delta) is Ï\rho-contraction mapping, which implies that

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | â€–Î“+â€‹(f0,g)âˆ’T[J]â€‹(0)â€–W1,âˆâ€‹(ğ’Ÿ;â„)=â€–T[J]â€‹(u)âˆ’T[J]â€‹(0)â€–W1,âˆâ€‹(ğ’Ÿ;â„)\displaystyle\big\|\Gamma^{\text{$+$}}(f\_{0},g)-T^{\text{$[$}\text{$J$}\text{$]$}}(0)\big\|\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}(\mathcal{D};\mathbb{R})}=\big\|T^{\text{$[$}\text{$J$}\text{$]$}}(u)-T^{\text{$[$}\text{$J$}\text{$]$}}(0)\|\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}(\mathcal{D};\mathbb{R})} | â‰²ÏJâ€‹â€–uâ€–W1,âˆâ€‹(ğ’Ÿ;â„)â‰¤ÏJâ€‹Î´â‰²Îµ,\displaystyle\lesssim\rho^{J}\|u\|\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}(\mathcal{D};\mathbb{R})}\leq\rho^{J}\delta\lesssim\varepsilon, |  | (A.14) |

where uu is the unique solution of ([A.4](https://arxiv.org/html/2511.01125v1#A1.E4 "Equation A.4 â€£ A.1.1 Well-posedness â€£ A.1 Proof of Theorem 3.11 â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators")) in BW1,âˆâ€‹(0,Î´)B\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}}(0,\delta).
Next, we see that

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–T[J]â€‹(0)âˆ’Î“â€‹(f0,wg)â€–W1,âˆâ€‹(ğ’Ÿ;â„)\displaystyle\big\|T^{\text{$[$}\text{$J$}\text{$]$}}(0)-\Gamma(f\_{0},w\_{g})\big\|\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}(\mathcal{D};\mathbb{R})} | =â€–T[J]â€‹(0)âˆ’TNN[J]â€‹(0)â€–W1,âˆâ€‹(ğ’Ÿ;â„)\displaystyle=\big\|T^{\text{$[$}\text{$J$}\text{$]$}}(0)-T\_{\text{$N$}\text{$N$}}^{\text{$[$}\text{$J$}\text{$]$}}(0)\|\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}(\mathcal{D};\mathbb{R})} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤âˆ‘h=1Jâ€–(T[Jâˆ’h+1]âˆ˜TNN[hâˆ’1])â€‹(0)âˆ’(T[Jâˆ’h]âˆ˜TNN[h])â€‹(0)â€–W1,âˆâ€‹(ğ’Ÿ;â„)\displaystyle\leq\sum\_{h=1}^{\text{$J$}}\Big\|\big(T^{\text{$[$}\text{$J$}\text{$-$}h\text{$+$}1\text{$]$}}\circ T\_{\text{$N$}\text{$N$}}^{\text{$[$}h\text{$-$}1\text{$]$}}\big)(0)-\big(T^{\text{$[$}\text{$J$}\text{$-$}h\text{$]$}}\circ T\_{\text{$N$}\text{$N$}}^{\text{$[$}h\text{$]$}}\big)(0)\Big\|\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}(\mathcal{D};\mathbb{R})} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤âˆ‘h=1JÏJâˆ’â€‹hâ€‹â€–(Tâˆ˜TNN[hâˆ’1])â€‹(0)âˆ’(TNNâˆ˜TNN[hâˆ’1])â€‹(0)â€–W1,âˆâ€‹(ğ’Ÿ;â„)\displaystyle\leq\sum\_{h=1}^{\text{$J$}}\rho^{\text{$J$}\text{$-$}h}\Big\|\big(T\circ T\_{\text{$N$}\text{$N$}}^{[h\text{$-$}1]}\big)(0)-\big(T\_{\text{$N$}\text{$N$}}\circ T\_{\text{$N$}\text{$N$}}^{[h\text{$-$}1]}\big)(0)\Big\|\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}(\mathcal{D};\mathbb{R})} |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | =âˆ‘h=1JÏJâˆ’â€‹hâ€‹â€–Tâ€‹(uh)âˆ’TNNâ€‹(uh)â€–W1,âˆâ€‹(ğ’Ÿ;â„),\displaystyle=\sum\_{h=1}^{\text{$J$}}\rho^{\text{$J$}\text{$-$}h}\big\|T(u\_{h})-T\_{\text{$N$}\text{$N$}}(u\_{h})\big\|\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}(\mathcal{D};\mathbb{R})}, |  | (A.15) |

where, we see that, by [SectionËœA.1.2](https://arxiv.org/html/2511.01125v1#A1.SS1.SSS2 "A.1.2 Proof of Theorem 3.11 â€£ A.1 Proof of Theorem 3.11 â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators")

|  |  |  |
| --- | --- | --- |
|  | uhâ‰”TNN[hâˆ’1]â€‹(0)âˆˆBW1,âˆâ€‹(0,Î´).u\_{h}\coloneqq T\_{\text{$N$}\text{$N$}}^{[h\text{$-$}1]}(0)\in B\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}}(0,\delta). |  |

Note that we define TNN[0]â‰”IdT\_{\text{$N$}\text{$N$}}^{[0]}\coloneqq\mathrm{Id}. By [SectionËœA.1.2](https://arxiv.org/html/2511.01125v1#A1.SS1.SSS2 "A.1.2 Proof of Theorem 3.11 â€£ A.1 Proof of Theorem 3.11 â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators"), we see that

|  |  |  |
| --- | --- | --- |
|  | â€–Tâ€‹(u)âˆ’TNNâ€‹(u)â€–W1,âˆâ€‹(ğ’Ÿ;â„)â‰¤C4â€‹Îµ,\displaystyle\big\|T(u)-T\_{\text{$N$}\text{$N$}}(u)\big\|\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}(\mathcal{D};\mathbb{R})}\leq C\_{4}\varepsilon, |  |

which implies that with ([A.15](https://arxiv.org/html/2511.01125v1#A1.E15 "Equation A.15 â€£ A.1.2 Proof of Theorem 3.11 â€£ A.1 Proof of Theorem 3.11 â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators"))

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | â€–T[J]â€‹(0)âˆ’Î“â€‹(f0,wg)â€–W1,âˆâ€‹(ğ’Ÿ;â„)â‰¤âˆ‘h=1JÏJâˆ’â€‹hâ€‹C5â€‹Îµâ‰¤âˆ‘h=0âˆÏhâ€‹C5â€‹Îµ=C51âˆ’Ïâ€‹Îµâ‰²Îµ.\displaystyle\big\|T^{\text{$[$}\text{$J$}\text{$]$}}(0)-\Gamma(f\_{0},w\_{g})\big\|\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}(\mathcal{D};\mathbb{R})}\leq\sum\_{h=1}^{\text{$J$}}\rho^{\text{$J$}\text{$-$}h}C\_{5}\varepsilon\leq\sum\_{h=0}^{\infty}\rho^{h}C\_{5}\varepsilon=\frac{C\_{5}}{1-\rho}\varepsilon\lesssim\varepsilon. |  | (A.16) |

Thus, by [EquationsËœA.14](https://arxiv.org/html/2511.01125v1#A1.E14 "In A.1.2 Proof of Theorem 3.11 â€£ A.1 Proof of Theorem 3.11 â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators") andÂ [A.16](https://arxiv.org/html/2511.01125v1#A1.E16 "Equation A.16 â€£ A.1.2 Proof of Theorem 3.11 â€£ A.1 Proof of Theorem 3.11 â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators"), we conclude that

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–Î“+â€‹(f0,g)âˆ’Î“â€‹(f0,wg)â€–W1,âˆâ€‹(ğ’Ÿ;â„)\displaystyle\big\|\Gamma^{\text{$+$}}(f\_{0},g)-\Gamma(f\_{0},w\_{g})\big\|\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}(\mathcal{D};\mathbb{R})} | â‰¤â€–Î“+â€‹(f0,g)âˆ’T[J]â€‹(0)â€–W1,âˆâ€‹(ğ’Ÿ;â„)+â€–T[J]â€‹(0)âˆ’Î“â€‹(f0,wg)â€–W1,âˆâ€‹(ğ’Ÿ;â„)â‰²Îµ.\displaystyle\leq\big\|\Gamma^{\text{$+$}}(f\_{0},g)-T^{\text{$[$}\text{$J$}\text{$]$}}(0)\big\|\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}(\mathcal{D};\mathbb{R})}+\big\|T^{\text{$[$}\text{$J$}\text{$]$}}(0)-\Gamma(f\_{0},w\_{g})\big\|\_{W^{\text{$1$}\text{$,$}\text{$\infty$}}(\mathcal{D};\mathbb{R})}\lesssim\varepsilon. |  |

âˆ

Let us remind the reader that Î“J\Gamma\_{\text{$J$}} is defined by

|  |  |  |
| --- | --- | --- |
|  | Î“Jâ€‹(f0,wg)=TNNâˆ˜â‹¯âˆ˜TNNâŸJâ€‹timesâ€‹(0)=TNN[J]â€‹(0).\Gamma\_{\text{$J$}}(f\_{0},w\_{g})=\underbrace{T\_{\text{$N$}\text{$N$}}\circ\cdots\circ T\_{\text{$N$}\text{$N$}}}\_{\text{$J$}\;\text{times}}(0)=T\_{\text{$N$}\text{$N$}}^{\text{$[$}\text{$J$}\text{$]$}}(0). |  |

where the operator TNNT\_{\text{$N$}\text{$N$}} is defined by

|  |  |  |  |
| --- | --- | --- | --- |
|  | TNNâ€‹(u)â€‹(x)\displaystyle T\_{\text{$N$}\text{$N$}}(u)(x) | =âˆ‘h=2Hâˆ«ğ’Ÿknâ€‹nhâ€‹(x,y)â€‹(uâ€‹(y))hâ€‹dyâˆ’âˆ«ğ’Ÿknâ€‹nâ€²â€‹(x,y)â€‹f0â€‹(y)â€‹dy+wgâ€‹(x)=âˆ‘h=2Hâˆ«ğ’Ÿknâ€‹nhâ€‹(x,y)â€‹(uâ€‹(y))hâ€‹dy+vf0,gâ€‹(x)\displaystyle=\sum\_{h=2}^{\text{$H$}}\int\_{\mathcal{D}}k\_{nn}^{h}(x,y)(u(y))^{h}\mathrm{d}y-\int\_{\mathcal{D}}k^{\prime}\_{nn}(x,y)f\_{0}(y)\mathrm{d}y+w\_{g}(x)=\sum\_{h=2}^{\text{$H$}}\int\_{\mathcal{D}}k\_{nn}^{h}(x,y)(u(y))^{h}\mathrm{d}y+v\_{f\_{0},g}(x) |  |

where

|  |  |  |
| --- | --- | --- |
|  | vf0,gâ€‹(x)â‰”âˆ’âˆ«ğ’Ÿknâ€‹nâ€²â€‹(x,y)â€‹f0â€‹(y)â€‹dy+wgâ€‹(x)v\_{f\_{0},g}(x)\coloneqq-\int\_{\mathcal{D}}k^{\prime}\_{nn}(x,y)f\_{0}(y)\mathrm{d}y+w\_{g}(x) |  |

We see that Î“Jâ€‹(f0,wg)â€‹(x)=vJâ€‹(x)\Gamma\_{\text{$J$}}(f\_{0},w\_{g})(x)=v\_{\text{$J$}}(x)
where v0â‰”0v\_{0}\coloneqq 0 and

|  |  |  |
| --- | --- | --- |
|  | vj+1â€‹(x)â‰”âˆ‘h=2Hâˆ«ğ’Ÿknâ€‹nhâ€‹(x,y)â€‹(vjâ€‹(y))hâ€‹dy+vf0,gâ€‹(x),jâˆˆ{0,â€¦,Jâˆ’1}.\displaystyle v\_{j+1}(x)\coloneqq\sum\_{h=2}^{\text{$H$}}\int\_{\mathcal{D}}k\_{nn}^{h}(x,y)(v\_{j}(y))^{h}\mathrm{d}y+v\_{f\_{0},g}(x),\;j\in\{0,\dots,J-1\}. |  |

We define

|  |  |  |
| --- | --- | --- |
|  | W(0)â‰”(0101)âˆˆâ„2Ã—2,W^{(0)}\coloneqq\begin{pmatrix}0&1\\ 0&1\\ \end{pmatrix}\in\mathbb{R}^{2\times 2}, |  |

and let KN(0):W1,âˆâ€‹(ğ’Ÿ;â„)2âŸ¶W1,âˆâ€‹(ğ’Ÿ;â„)2K^{(0)}\_{N}:W^{1,\infty}(\mathcal{D};\mathbb{R})^{2}\longrightarrow W^{1,\infty}(\mathcal{D};\mathbb{R})^{2} be defined by

|  |  |  |
| --- | --- | --- |
|  | (K(0)â€‹(f0wg))â€‹(x)â‰”âˆ«ğ’ŸkNN(0)â€‹(x,y)â€‹(fâ€‹(y)wgâ€‹(y))â€‹dy,\bigg(K^{(0)}\begin{pmatrix}f\_{0}\\ w\_{g}\end{pmatrix}\bigg)(x)\coloneqq\int\_{\mathcal{D}}k^{(0)}\_{\text{$N$}\text{$N$}}(x,y)\begin{pmatrix}f(y)\\ w\_{g}(y)\end{pmatrix}\mathrm{d}y, |  |

where

|  |  |  |
| --- | --- | --- |
|  | kNN(0)â€‹(x,y)â‰”(kNNâ€²â€‹(x,y)0kNNâ€²â€‹(x,y)0)âˆˆâ„2Ã—2.k^{(0)}\_{\text{$N$}\text{$N$}}(x,y)\coloneqq\begin{pmatrix}k^{\prime}\_{\text{$N$}\text{$N$}}(x,y)&0\\ k^{\prime}\_{\text{$N$}\text{$N$}}(x,y)&0\\ \end{pmatrix}\in\mathbb{R}^{2\times 2}. |  |

We therefore compute

|  |  |  |
| --- | --- | --- |
|  | W(0)â€‹(f0â€‹(x)wgâ€‹(x))+(K(0)â€‹(f0wg))â€‹(x)=(vf0,gâ€‹(x)vf0,gâ€‹(x))=(vf0,gâ€‹(x)v1â€‹(x)).W^{(0)}\begin{pmatrix}f\_{0}(x)\\ w\_{g}(x)\end{pmatrix}+\bigg(K^{(0)}\begin{pmatrix}f\_{0}\\ w\_{g}\end{pmatrix}\bigg)(x)=\begin{pmatrix}v\_{f\_{0},g}(x)\\ v\_{f\_{0},g}(x)\end{pmatrix}=\begin{pmatrix}v\_{f\_{0},g}(x)\\ v\_{1}(x)\end{pmatrix}. |  |

Next, we define FReQU:â„2âŸ¶â„HF\_{\text{${\rm ReQU}$}}:\mathbb{R}^{2}\longrightarrow\mathbb{R}^{H} by

|  |  |  |
| --- | --- | --- |
|  | FRâ€‹eâ€‹Qâ€‹Uâ€‹(u)â‰”(u1(u2)2â‹®(u2)H),u=(u1,u2)âˆˆâ„2,F\_{ReQU}(u)\coloneqq\begin{pmatrix}u\_{1}\\ (u\_{2})^{2}\\ \vdots\\ (u\_{2})^{H}\end{pmatrix},\quad u=(u\_{1},u\_{2})\in\mathbb{R}^{2}, |  |

which can have an exact implementation by a ReQU neural networks (see [Li, Tang, and Yu](https://arxiv.org/html/2511.01125v1#bib.bib59) [[59](https://arxiv.org/html/2511.01125v1#bib.bib59), Theorem 3.1]).

We define

|  |  |  |
| --- | --- | --- |
|  | W=(10â‹¯010â‹¯0)âˆˆâ„2Ã—H,W=\begin{pmatrix}1&0&\cdots&0\\ 1&0&\cdots&0\end{pmatrix}\in\mathbb{R}^{2\times\text{$H$}}, |  |

and K:W1,âˆâ€‹(ğ’Ÿ;â„)HâŸ¶W1,âˆâ€‹(ğ’Ÿ;â„)2K:W^{1,\infty}(\mathcal{D};\mathbb{R})^{\text{$H$}}\longrightarrow W^{1,\infty}(\mathcal{D};\mathbb{R})^{2}, for u=(u1,â€¦,uH)âˆˆW1,âˆâ€‹(ğ’Ÿ;â„)H+â€‹1u=(u\_{1},...,u\_{H})\in W^{1,\infty}(\mathcal{D};\mathbb{R})^{\text{$H$}\text{$+$}1}

|  |  |  |
| --- | --- | --- |
|  | (Kâ€‹u)â€‹(x)â‰”âˆ«ğ’ŸkNNâ€‹(x,y)â€‹uâ€‹(y)â€‹dy=(0âˆ‘h=2Hâˆ«ğ’Ÿknâ€‹nhâ€‹(x,y)â€‹uhâ€‹(y)â€‹dy),\displaystyle(Ku)(x)\coloneqq\int\_{\mathcal{D}}k\_{\text{$N$}\text{$N$}}(x,y)u(y)\mathrm{d}y=\begin{pmatrix}0\\[5.0pt] \displaystyle\sum\_{h=2}^{\text{$H$}}\int\_{\mathcal{D}}k\_{nn}^{h}(x,y)u\_{h}(y)\mathrm{d}y\end{pmatrix}, |  |

where

|  |  |  |
| --- | --- | --- |
|  | kNâ€‹Nâ€‹(x,y)â‰”(00â‹¯00knâ€‹n2â€‹(x,y)â‹¯knâ€‹nHâ€‹(x,y))âˆˆâ„2Ã—H,k\_{NN}(x,y)\coloneqq\begin{pmatrix}0&0&\cdots&0\\ 0&k\_{nn}^{2}(x,y)&\cdots&k\_{nn}^{\text{$H$}}(x,y)\end{pmatrix}\in\mathbb{R}^{2\times\text{$H$}}, |  |

Then, we have that for jâˆˆ{1,â€¦,Jâˆ’1}j\in\{1,...,J-1\}

|  |  |  |  |
| --- | --- | --- | --- |
|  | ((W+K)âˆ˜FReQUâ€‹(vf0,gvj))â€‹(x)=Wâ€‹(vf0,gâ€‹(x)(vjâ€‹(x))2â‹®(vjâ€‹(x))H)+Kâ€‹(vf0,g(vj)2â‹®(vj)H)â€‹(x)\displaystyle\bigg((W+K)\circ F\_{\text{$\rm ReQU$}}\begin{pmatrix}v\_{f\_{0},g}\\ v\_{j}\end{pmatrix}\bigg)(x)=W\begin{pmatrix}v\_{f\_{0},g}(x)\\ (v\_{j}(x))^{2}\\ \vdots\\ (v\_{j}(x))^{\text{$H$}}\end{pmatrix}+K\begin{pmatrix}v\_{f\_{0},g}\\ (v\_{j})^{2}\\ \vdots\\ (v\_{j})^{\text{$H$}}\end{pmatrix}(x) | =(vf0,gâ€‹(x)âˆ‘h=2Hâˆ«ğ’Ÿknâ€‹nhâ€‹(x,y)â€‹(vjâ€‹(y))hâ€‹dy+vf0,gâ€‹(x))\displaystyle=\begin{pmatrix}v\_{f\_{0},g}(x)\\ \displaystyle\sum\_{h=2}^{\text{$H$}}\int\_{\mathcal{D}}k\_{nn}^{h}(x,y)(v\_{j}(y))^{h}\mathrm{d}y+v\_{f\_{0},g}(x)\end{pmatrix} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =(vf0,gâ€‹(x)vj+1â€‹(x)).\displaystyle=\begin{pmatrix}v\_{f\_{0},g}(x)\\ v\_{j+1}(x)\end{pmatrix}. |  |

Denoting Wâ€²â‰”(0,1)âˆˆâ„1Ã—2W^{\prime}\coloneqq(0,1)\in\mathbb{R}^{1\times 2}, we finally obtain that

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î“Jâ€‹(f,wg)\displaystyle\Gamma\_{\text{$J$}}(f,w\_{g}) | =Wâ€²âˆ˜((W+K)âˆ˜FReQUâˆ˜â‹¯âˆ˜(W+K)âˆ˜FReQUâŸJâˆ’â€‹1â€‹times)âˆ˜(W(0)+K(0))â€‹(fwg).\displaystyle=W^{\prime}\circ\Big(\underbrace{(W+K)\circ F\_{\text{$\rm ReQU$}}\circ\cdots\circ(W+K)\circ F\_{\text{$\rm ReQU$}}}\_{\text{$J$}\text{$-$}1\;\text{times}}\Big)\circ\big(W^{(0)}+K^{(0)}\big)\begin{pmatrix}f\\ w\_{g}\end{pmatrix}. |  |

Since the ReQU network can be represented by the KANs network [[92](https://arxiv.org/html/2511.01125v1#bib.bib92), Theorem 3.2], we have, by the above construction,

|  |  |  |
| --- | --- | --- |
|  | Î“âˆˆğ’©â€‹ğ’ªL^,W^L,W,I,Î±â€‹(W1,âˆâ€‹(ğ’Ÿ;â„)2,W1,âˆâ€‹(ğ’Ÿ;â„)).\Gamma\in\mathcal{NO}^{\text{$L$},\text{$W$},\text{$I$},\alpha}\_{\hat{\text{$L$}},\hat{\text{$W$}}}(W^{1,\infty}(\mathcal{D};\mathbb{R})^{2},W^{1,\infty}(\mathcal{D};\mathbb{R})). |  |

Moreover, the depth L=Lâ€‹(Î“)L=L(\Gamma) and width W=Wâ€‹(Î“)W=W(\Gamma) of the neural operator Î“\Gamma can be estimated via

|  |  |  |
| --- | --- | --- |
|  | Lâ€‹(Î“)â‰²Jâ‰¤Câ€‹logâ¡(Îµâˆ’1),Wâ€‹(Î“)â‰²Hâ‰¤C.\displaystyle L(\Gamma)\lesssim J\leq C\log(\varepsilon^{-1}),\;W(\Gamma)\lesssim H\leq C. |  |

This concludes our proof of TheoremÂ [3.11](https://arxiv.org/html/2511.01125v1#S3.Thmtheorem11 "Theorem 3.11 (Exponential approximation rates: solution operator to the elliptic problem). â€£ 3.3.1 Semi-linear elliptic PDE â€£ 3.3 Feasible rates â€£ 3 Main results â€£ One model to solve them all: 2BSDE families via neural operators"); where, again, Î±=s\alpha=s and Iâ‰”âŒˆÎ±âŒ‰I\coloneqq\lceil\alpha\rceil.

### A.2 Proof of TheoremÂ [3.7](https://arxiv.org/html/2511.01125v1#S3.Thmtheorem7 "Theorem 3.7 (Approximability of the perturbation-to-solution map). â€£ 3.2 General approximability guarantee â€£ 3 Main results â€£ One model to solve them all: 2BSDE families via neural operators")

The proof of our second main result relies on some tools from multi-resolution analysis and the wavelet theory of Besov spaces. We, therefore, now overview the necessary material.

#### A.2.1 Additional background

In what follows, we use Sâ€‹(â„d)S(\mathbb{R}^{d}) to denote the Schwartz space on â„d\mathbb{R}^{d} and consider the space of distributions defined as the topological dual Dâ€‹(ğ’Ÿ)â€²D(\mathcal{D})^{\prime}. We define the restriction operator sending any distribution gâˆˆSâ€‹(â„d)g\in S(\mathbb{R}^{d}) to g|ğ’ŸâˆˆDâ€‹(ğ’Ÿ)â€²g|\_{\mathcal{D}}\in D(\mathcal{D})^{\prime} defined by restriction of its action to test functions Ï†âˆˆDâ€‹(ğ’Ÿ)\varphi\in D(\mathcal{D}) *i.e.*

|  |  |  |
| --- | --- | --- |
|  | g|ğ’Ÿâ€‹(Ï†)â‰”gâ€‹(Ï†).g|\_{\mathcal{D}}(\varphi)\coloneqq g(\varphi). |  |

#### A.2.2 From wavelet para-bases to Besov spaces on Euclidean spaces

Fix uâˆˆâ„•u\in\mathbb{N} and (ÏƒS,ÏƒW)âˆˆCuâ€‹(â„)Ã—Cuâ€‹(â„)(\sigma\_{\text{$S$}},\sigma\_{\text{$W$}})\in C^{u}(\mathbb{R})\times C^{u}(\mathbb{\mathbb{R}})
satisfy [SectionËœ2.2.1](https://arxiv.org/html/2511.01125v1#S2.SS2.SSS1 "2.2.1 Residual Kolmogorovâ€“Arnold networks (Resâ€“KANs) â€£ 2.2 Deep learning â€£ 2 Preliminaries â€£ One model to solve them all: 2BSDE families via neural operators"); that is to say that ÏƒS\sigma\_{\text{$S$}} and ÏƒW\sigma\_{\text{$W$}} are Daubechies father (also known as scaling function) and mother wavelets (also known as wavelet function) respectively, in the sense of [[18](https://arxiv.org/html/2511.01125v1#bib.bib18)].
For each jâˆˆâ„•j\in\mathbb{N} define the sets

|  |  |  |
| --- | --- | --- |
|  | Gjâ‰”{{S,W}d,Â ifÂ â€‹j=0,{S,W}dâ£â‹†â‰”{S,W}dâˆ–{(S,â€¦,S)},Â ifÂ â€‹j>0.G^{j}\coloneqq\begin{cases}\{S,W\}^{d},\mbox{ if }j=0,\\ \{S,W\}^{d\star}\coloneqq\{S,W\}^{d}\setminus\{(S,\dots,S)\},\mbox{ if }j>0.\end{cases} |  |

Now, for each â€˜scaleâ€™ jâˆˆâ„•j\in\mathbb{N}, location mâˆˆâ„¤dm\in\mathbb{Z}^{d}, and each GâˆˆGjG\in G^{j}, define the tensorised Daubechies wavelet by

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î¨~G,mjâ€‹(x)â‰”2jâ€‹d/2â€‹âˆi=1dÏƒGiâ€‹(2jâ€‹xiâˆ’mi),xâˆˆâ„d,\widetilde{\Psi}\_{G,m}^{j}(x)\coloneqq 2^{jd/2}\prod\_{i=1}^{d}\sigma\_{\text{$G$}\_{\text{$i$}}}\big(2^{j}x\_{i}-m\_{i}\big),\;x\in\mathbb{R}^{d}, |  | (A.17) |

where Gâ‰”(G1,â€¦,Gd)G\coloneqq(G\_{1},\dots,G\_{d}).
Let ğ’ªâ‰”{(j,G,m):jâˆˆâ„•,GâˆˆGj,mâˆˆâ„¤d}\mathcal{O}\coloneqq\{(j,G,m):j\in\mathbb{N},\;G\in G^{j},\;m\in\mathbb{Z}^{d}\} and for each (j,G,m)âˆˆğ’ª(j,G,m)\in\mathcal{O} let

|  |  |  |
| --- | --- | --- |
|  | 1(Î²G,mj)2â‰”âˆ«â„d(Î¨~G,mjâ€‹(x))2â€‹dx,andâ€‹Î¨G,mjâ‰”1Î²G,mjâ€‹Î¨~G,mjâ€‹(x),xâˆˆâ„d.\frac{1}{(\beta\_{\text{$G$},m}^{j})^{2}}\coloneqq\int\_{\mathbb{R}^{\text{$d$}}}\,\big(\tilde{\Psi}\_{\text{$G$},m}^{j}(x)\big)^{2}\mathrm{d}x,\mbox{\rm and}\;\Psi\_{\text{$G$},m}^{j}\coloneqq\frac{1}{\beta\_{\text{$G$},m}^{j}}\tilde{\Psi}\_{\text{$G$},m}^{j}(x),\;x\in\mathbb{R}^{d}. |  |

Then, as discussed on [Triebel](https://arxiv.org/html/2511.01125v1#bib.bib90) [[90](https://arxiv.org/html/2511.01125v1#bib.bib90), page 13], for any uâˆˆâ„•u\in\mathbb{N} we have that (Î¨G,mj)(j,G,m)âˆˆğ’ª(\Psi\_{\text{$G$},m}^{j})\_{(j,\text{$G$},m)\in\mathcal{O}} is an orthonormal basis of L2â€‹(â„d)L^{2}(\mathbb{R}^{d}), and for every fâˆˆL2â€‹(â„d)f\in L^{2}(\mathbb{R}^{d})

|  |  |  |  |
| --- | --- | --- | --- |
|  | f=âˆ‘jâˆˆâ„•âˆ‘GâˆˆGjâˆ‘mâˆˆâ„¤dÎ»G,mjâ€‹2âˆ’jâ€‹d/2â€‹Î¨G,mj,whereâ€‹Î»G,mjâ‰”2jâ€‹d/2â€‹âˆ«â„dfâ€‹(x)â€‹Î¨G,mjâ€‹(x)â€‹dx,f=\sum\_{j\in\mathbb{N}}\sum\_{G\in G^{\text{$j$}}}\sum\_{m\in\mathbb{Z}^{\text{$d$}}}\,\lambda\_{\text{$G$},m}^{j}2^{\text{$-$}jd/2}\Psi\_{\text{$G$},m}^{j},\;\mbox{\rm where}\;\lambda\_{\text{$G$},m}^{j}\coloneqq 2^{jd/2}\int\_{\mathbb{R}^{\text{$d$}}}\,f(x)\Psi\_{\text{$G$},m}^{j}(x)\mathrm{d}x, |  | (A.18) |

where the series converge in L2â€‹(â„d)L^{2}(\mathbb{R}^{d}).

A key properties of Besov spaces, from the approximation theoretic lense, is that they are entirely determined by the decay/convergence rates of the sequences (Î»G,mj)(j,G,m)âˆˆğ’ª(\lambda\_{\text{$G$},m}^{j})\_{(j,\text{$G$},m)\in\mathcal{O}}, defined inÂ ([A.18](https://arxiv.org/html/2511.01125v1#A1.E18 "Equation A.18 â€£ A.2.2 From wavelet para-bases to Besov spaces on Euclidean spaces â€£ A.2 Proof of Theorem 3.7 â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators")). Indeed, for (q,r)âˆˆ(0,+âˆ]2(q,r)\in(0,+\infty]^{2} and sâˆˆâ„s\in\mathbb{R}, if

|  |  |  |  |
| --- | --- | --- | --- |
|  | u>maxâ¡{s,Ïƒqâˆ’s},whereâ€‹Ïƒqâ‰”dâ€‹maxâ¡{0,1qâˆ’1},u>\max\{s,\sigma\_{q}-s\},\;\mbox{where}\;\sigma\_{q}\coloneqq d\max\bigg\{0,\frac{1}{q}-1\bigg\}, |  | (A.19) |

as shown inÂ [[90](https://arxiv.org/html/2511.01125v1#bib.bib90), Theorem 1.20], fâˆˆSâ€‹(â„d)â€²f\in S(\mathbb{R}^{d})^{\prime} belongs to the Besov space BÂ¯q,rsâ€‹(â„d)\overline{B}\_{q,r}^{s}(\mathbb{R}^{d}) if and only if the sequence Î»â‹…â‰”(Î»G,mj)(j,G,m)âˆˆğ’ª\lambda\_{\cdot}\coloneqq(\lambda\_{\text{$G$},m}^{j})\_{(j,\text{$G$},m)\in\mathcal{O}}, defined by([A.18](https://arxiv.org/html/2511.01125v1#A1.E18 "Equation A.18 â€£ A.2.2 From wavelet para-bases to Besov spaces on Euclidean spaces â€£ A.2 Proof of Theorem 3.7 â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators")), satisfies

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–Î»â‹…â€–bq,rsrâ‰”âˆ‘j=0âˆ2jâ€‹râ€‹(sâˆ’d/q)â€‹âˆ‘GâˆˆGj(âˆ‘mâˆˆâ„¤d|Î»G,mj|q)r/q<âˆ,\|\lambda\_{\cdot}\|\_{b\_{\text{$q$}\text{$,$}\text{$r$}}^{\text{$s$}}}^{r}\coloneqq\sum\_{j=0}^{\infty}2^{jr(s-d/q)}\sum\_{G\in G^{\text{$j$}}}\,\Bigg(\sum\_{m\in\mathbb{Z}^{\text{$d$}}}|\lambda\_{\text{$G$},m}^{j}|^{q}\Bigg)^{r/q}<\infty, |  | (A.20) |

with the usual modifications to the left-hand side ofÂ ([A.20](https://arxiv.org/html/2511.01125v1#A1.E20 "Equation A.20 â€£ A.2.2 From wavelet para-bases to Besov spaces on Euclidean spaces â€£ A.2 Proof of Theorem 3.7 â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators")) if qq or rr are infinite. Additionally, the map fâŸ¼(2jâ€‹d/2â€‹âŸ¨f,Î¨G,mjâŸ©L2â€‹(â„d))(j,G,m)âˆˆğ’ªf\longmapsto(2^{jd/2}\langle f,\Psi\_{\text{$G$},m}^{j}\rangle\_{L^{\text{$2$}}(\mathbb{R}^{\text{$d$}})})\_{(j,\text{$G$},m)\in\mathcal{O}} is a bi-Lipschitz linear isomorphism between Bq,rsâ€‹(â„d)B\_{q,r}^{s}(\mathbb{R}^{d}) and the (quasiâ€“)Banach space bq,rsb\_{q,r}^{s} of all sequences for which the (quasi-)norm âˆ¥â‹…âˆ¥bq,rs\|\cdot\|\_{b\_{\text{$q$}\text{$,$}\text{$r$}}^{\text{$s$}}} is finite.

#### A.2.3 Besov spaces on domains

We begin with the definition of Besov spaces on any domain (proper open set with non-empty interior) OâŠ‚â„dO\subset\mathbb{R}^{d}, with closure OÂ¯\overline{O}. We write ğ’Ÿâ€‹(O)\mathcal{D}(O) for the space of complex-valued compactly supported smooth (test) functions on OO, topologized with the canonical (Limit of FrÃ©chet) LFâ€“topology. Its dual space Dâ€²â€‹(O)D^{\prime}(O) is the space of distributions on OO, and a distribution fâˆˆDâ€²â€‹(O)f\in D^{\prime}(O) is said to be supported on a set AâŠ†OA\subseteq O if fâ€‹(Ï†)=0f(\varphi)=0 for every Ï†âˆˆğ’Ÿâ€‹(O)\varphi\in\mathcal{D}(O) such that Ï†â€‹(x)=0\varphi(x)=0 for all xâˆ‰Ax\not\in A; the support suppâ€‹(f)\mathrm{supp}(f) is the smallest closed set KK with this property. For instance, if xâˆˆOx\in O then the Dirac distribution Î´xâ€‹(Ï†)â‰”Ï†â€‹(x)\delta\_{x}(\varphi)\coloneqq\varphi(x) has support suppâ€‹(Î´x)={x}\mathrm{supp}(\delta\_{x})=\{x\}, seeÂ [[90](https://arxiv.org/html/2511.01125v1#bib.bib90), Chapter 2, page 28] for further details and notation.
We now define the Besov (quasi-Banach) spaces on ğ’Ÿ\mathcal{D}.

###### Definition A.6 (Besov spaces on domains).

Let ğ’Ÿ\mathcal{D} be a domain, (q,r)âˆˆ(0,+âˆ]2(q,r)\in(0,+\infty]^{2}, and sâˆˆâ„s\in\mathbb{R}. The Besov space B~q,rsâ€‹(ğ’ŸÂ¯)\widetilde{B}\_{q,r}^{s}(\overline{\mathcal{D}}) consists of all fâˆˆBq,rsâ€‹(â„d)f\in B\_{q,r}^{s}(\mathbb{R}^{d}) supported in the closure ğ’ŸÂ¯\overline{\mathcal{D}} and B~q,rsâ€‹(ğ’Ÿ)\widetilde{B}\_{q,r}^{s}(\mathcal{D}) consists of all distributions fâˆˆDâ€‹(ğ’Ÿ)â€²f\in D(\mathcal{D})^{\prime} for which there exists some gâˆˆBq,rsâ€‹(ğ’ŸÂ¯)g\in B\_{q,r}^{s}(\overline{\mathcal{D}}) such that f=g|ğ’Ÿf=g|\_{\mathcal{D}}.
In either case, ğ”‡âˆˆ{ğ’Ÿ,ğ’ŸÂ¯}\mathfrak{D}\in\{\mathcal{D},\bar{\mathcal{D}}\}, we equip B~q,rsâ€‹(ğ”‡)\widetilde{B}\_{q,r}^{s}(\mathfrak{D}) with the interpolation norm

|  |  |  |
| --- | --- | --- |
|  | âˆ¥fâˆ¥B~q,rsâ€‹(ğ”‡)â‰”inf{âˆ¥gâˆ¥Bq,rsâ€‹(â„d):gâˆˆB~q,rs(ğ’ŸÂ¯),f=g|ğ”‡}.\|f\|\_{\tilde{B}\_{\text{$q$}\text{$,$}\text{$r$}}^{\text{$s$}}(\mathfrak{D})}\coloneqq\inf\Big\{\|g\|\_{B\_{\text{$q$}\text{$,$}\text{$r$}}^{\text{$s$}}(\mathbb{R}^{\text{$d$}})}:g\in\widetilde{B}\_{q,r}^{s}(\overline{\mathcal{D}}),\;f=g|\_{{\mathfrak{D}}}\Big\}. |  |

We define the Besov spaces BÂ¯q,rsâ€‹(ğ’Ÿ)\overline{B}\_{q,r}^{s}(\mathcal{D}) as follows

|  |  |  |  |
| --- | --- | --- | --- |
|  | BÂ¯q,rsâ€‹(ğ’Ÿ)â‰”{B~q,rsâ€‹(ğ’Ÿ),ifâ€‹â€„0<qâ‰¤âˆ,â€„0<râ‰¤âˆ,s>Ïƒq,Bq,r0â€‹(ğ’Ÿ),ifâ€‹â€„1<q<âˆ,â€„0<râ‰¤âˆ,s=0,Bq,rsâ€‹(ğ’Ÿ),ifâ€‹â€„0<qâ‰¤âˆ,â€„0<râ‰¤âˆ,s<0.\overline{B}^{s}\_{q,r}(\mathcal{D})\coloneqq\begin{cases}\widetilde{B}^{s}\_{q,r}(\mathcal{D}),\;\text{\rm if}\;0<q\leq\infty,\;0<r\leq\infty,\;s>\sigma\_{q},\\[5.0pt] B^{0}\_{q,r}(\mathcal{D}),\;\text{\rm if}\;1<q<\infty,\;0<r\leq\infty,\;s=0,\\[5.0pt] B^{s}\_{q,r}(\mathcal{D}),\;\text{\rm if}\;0<q\leq\infty,\;0<r\leq\infty,\;s<0.\end{cases} |  | (A.21) |

FollowingÂ [[90](https://arxiv.org/html/2511.01125v1#bib.bib90), Section 3], we now construct wavelet systems on arbitrary domains (open subsets Î©âŠ‚â„n\Omega\subset\mathbb{R}^{n}) using Whitney decompositions; an object which acts almost as a leitmotif in analysis from our PDE problems to fundamental result in the geometry of functions spacesÂ [[27](https://arxiv.org/html/2511.01125v1#bib.bib27), [28](https://arxiv.org/html/2511.01125v1#bib.bib28)].
The idea is to partition Î©\Omega into dyadic cubes whose sizes adapt to the distance from the boundary, and then build localized wavelet bases on these cubesâ€”maintaining the regularity and cancellation properties of classical â„n\mathbb{R}^{n} wavelets while conforming to the geometry of Î©\Omega.

These spaces can themselves be characterized in a similar way using compactly supported Daubechies wavelets. We fix a so-called approximate lattice â„¤ğ’ŸâŠ‚ğ’Ÿ\mathbb{Z}\_{\mathcal{D}}\subset\mathcal{D} consisting of points â„¤ğ’Ÿ=(xrj)(j,k)âˆˆâ„•Ã—{1,â€¦,Nj}\mathbb{Z}\_{\mathcal{D}}=(x\_{r}^{j})\_{(j,k)\in\mathbb{N}\times\{1,\dots,N\_{\text{$j$}}\}} where, for each jâˆˆâ„•j\in\mathbb{N}, Njâˆˆâ„•Â¯â‰”â„•âˆª{âˆ}N\_{j}\in\overline{\mathbb{N}}\coloneqq\mathbb{N}\cup\{\infty\} for which there exist positive constants c1c\_{1}, c2c\_{2}, c3c\_{3} satisfying the approximate â€˜lattice separation conditionâ€™ at any scale jâˆˆâ„•j\in\mathbb{N}

|  |  |  |  |
| --- | --- | --- | --- |
|  | |xrjâˆ’xrâ€²j|â‰¥c12j\big|x\_{r}^{j}-x\_{r^{\text{$\prime$}}}^{j}\big|\geq\frac{c\_{1}}{2^{j}} |  | (A.22) |

and the separation from the â€˜boundary conditionâ€™ at scale jâˆˆâ„•j\in\mathbb{N}

|  |  |  |  |
| --- | --- | --- | --- |
|  | inf{zâˆˆâ„d:â€–zâˆ’xrjâ€–â‰¤c2/2j}infuâˆˆâˆ‚ğ’Ÿâ€–zâˆ’uâ€–â‰¥c32j.\inf\_{\{z\in\mathbb{R}^{\text{$d$}}:\|z-x\_{\text{$r$}}^{\text{$j$}}\|\leq c\_{\text{$2$}}/2^{\text{$j$}}\}}\;\inf\_{u\in\partial\mathcal{D}}\|z-u\|\geq\frac{c\_{3}}{2^{j}}. |  | (A.23) |

Clearly the constants c1c\_{1}, c2c\_{2}, and c3c\_{3} may be chosen to guarantee the existence of such a â„¤ğ’Ÿ\mathbb{Z}\_{\mathcal{D}} for any domain ğ’Ÿ\mathcal{D}. Intuitively, â„¤ğ’Ÿ\mathbb{Z}\_{\mathcal{D}} acts precisely as the dyadic lattices â‹ƒjâˆˆâ„•2âˆ’jâ€‹â„¤d\bigcup\_{j\in\mathbb{N}}2^{\text{$-$}j}\mathbb{Z}^{d} does in â„d\mathbb{R}^{d} but is contained entirely within ğ’Ÿ\mathcal{D} and conditionÂ ([A.22](https://arxiv.org/html/2511.01125v1#A1.E22 "Equation A.22 â€£ A.2.3 Besov spaces on domains â€£ A.2 Proof of Theorem 3.7 â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators")) vacuously holds when ğ’Ÿ\mathcal{D} is replaced by the Euclidean space.

For any Lâˆˆâ„•L\in\mathbb{N}, to be specified retroactively, we denote ÏƒSL(â‹…)â‰”ÏƒS(2Lâ‹…)\sigma\_{\text{$S$}}^{\text{$L$}}(\cdot)\coloneqq\sigma\_{\text{$S$}}(2^{\text{$L$}}\cdot), ÏƒWL(â‹…)â‰”ÏƒW(2Lâ‹…)\sigma\_{\text{$W$}}^{\text{$L$}}(\cdot)\coloneqq\sigma\_{\text{$W$}}(2^{\text{$L$}}\cdot), and Î¨G,mj,Lâ‰”Î¨G,mj(2Lâ‹…)\Psi\_{\text{$G$},m}^{j,\text{$L$}}\coloneqq\Psi\_{\text{$G$},m}^{j}(2^{\text{$L$}}\cdot) for each (j,G,m)âˆˆğ’ª(j,G,m)\in\mathcal{O}. In other words, the factor LL rescales our setup and we will choose it so that our problem is properly â€˜shrunkâ€™ within our domain and aligned to the approximate lattice â„¤ğ’Ÿ\mathbb{Z}\_{\mathcal{D}}.

We are now ready to define wavelet classes tailored to general domains; we follow the terminology inÂ [[90](https://arxiv.org/html/2511.01125v1#bib.bib90), Definition 2.4], the existence of which is known (see *e.g.*Â [[90](https://arxiv.org/html/2511.01125v1#bib.bib90), Theorem 2.33]).

###### Definition A.7 (uu-wavelets).

Let ğ’Ÿ\mathcal{D} be an arbitrary domain in â„n\mathbb{R}^{n} with ğ’Ÿâ‰ â„n\mathcal{D}\neq\mathbb{R}^{n} and let â„¤ğ’Ÿ\mathbb{Z}\_{\mathcal{D}} ads well as Lâˆˆâ„•L\in\mathbb{N} and uâˆˆâ„•u\in\mathbb{N} be as above. Let Kâˆˆâ„•K\in\mathbb{N}, D>0D>0 and c4>0c\_{4}>0. Then, consider a sub-family of {Î¨G,mj:jâˆˆâ„•+,GâˆˆGj,mâˆˆâ„¤ğ’Ÿ}\{\Psi\_{G,m}^{j}:\,j\in\mathbb{N}\_{+},\,G\in G^{j},\,m\in\mathbb{Z}\_{\mathcal{D}}\}

|  |  |  |  |
| --- | --- | --- | --- |
|  | {Î¦rj:jâˆˆâ„•;râˆˆ{1,â€¦,Nj}},whereâ€‹Njâˆˆâ„•Â¯.\big\{\Phi\_{r}^{j}:j\in\mathbb{N};r\in\{1,\ldots,N\_{j}\}\big\},\;\text{\rm where}\;N\_{j}\in\overline{\mathbb{N}}. |  | (A.24) |

satisfying: suppâ€‹(Î¦rj)âŠ‚Bâ„dâ€‹(xrj,c2â€‹2âˆ’j)\mathrm{supp}(\Phi\_{r}^{j})\subset B\_{\mathbb{R}^{d}}\big(x\_{r}^{j},c\_{2}2^{-j}\big), jâˆˆâ„•j\in\mathbb{N},
is called a uu-wavelet system ((with respect to ğ’Ÿ)\mathcal{D}) if it consists of the following three possible types of functions

1. (i)(i)

   basic wavelets:
   Î¦r0=Î¨G,m0,L\Phi\_{r}^{0}=\Psi\_{\text{$G$},m}^{0,\text{$L$}} for some Gâˆˆ{S,W}dG\in\{S,W\}^{d}, and mâˆˆâ„¤d;m\in\mathbb{Z}^{d};
2. (iâ€‹i)(ii)

   interior wavelets:
   Î¦rj=Î¨G,mj,L\Phi\_{r}^{j}=\Psi\_{\text{$G$},m}^{j,\text{$L$}} for each jâˆˆâ„•j\in\mathbb{N}, and mâˆˆâ„¤ğ’Ÿm\in\mathbb{Z}\_{\mathcal{D}}
   such that distâ€‹(xrj,ğ’ŸÂ¯)â‰¥c4â€‹2âˆ’j,\mathrm{dist}(x\_{r}^{j},\bar{\mathcal{D}})\geq c\_{4}2^{\text{$-$}j},
   for some Gâˆˆ{S,W}dâ£â‹†;G\in\{S,W\}^{d\star};
3. (iâ€‹iâ€‹i)(iii)

   boundary wavelets:
   Î¦rj=âˆ‘{mâ€²âˆˆâ„¤d:â€–mâˆ’mâ€²â€–â‰¤K}dm,mâ€²jâ€‹Î¨F~,mâ€²j,L\Phi\_{r}^{j}=\sum\_{\{m^{\text{$\prime$}}\in\mathbb{Z}^{\text{$d$}}:\|m-m^{\text{$\prime$}}\|\leq K\}}d\_{m,m^{\text{$\prime$}}}^{j}\Psi\_{\tilde{\text{$F$}},m^{\text{$\prime$}}}^{j,\text{$L$}}, for each jâˆˆâ„•j\in\mathbb{N} for which distâ€‹(xrj,Î“)<c4â€‹2âˆ’j,\mathrm{dist}(x\_{r}^{j},\Gamma)<c\_{4}2^{\text{$-$}j},
   for some mâ‰”mâ€‹(j,r)âˆˆâ„¤dm\coloneqq m(j,r)\in\mathbb{Z}^{d} and dm,mâ€²jâˆˆâ„d\_{m,m^{\text{$\prime$}}}^{j}\in\mathbb{R} with

   |  |  |  |  |
   | --- | --- | --- | --- |
   |  | âˆ‘{mâ€²âˆˆâ„¤d:â€–mâˆ’mâ€²â€–â‰¤K}|dm,mâ€²j|â‰¤D,andâ€‹suppâ€‹(Î¨F~,mâ€²j,L)âŠ‚Bâ€‹(xrj,c2â€‹2âˆ’j).\sum\_{\{m^{\text{$\prime$}}\in\mathbb{Z}^{\text{$d$}}:\|m-m^{\text{$\prime$}}\|\leq K\}}|d\_{m,m^{\text{$\prime$}}}^{j}|\leq D,\;\text{\rm and}\;\mathrm{supp}\big(\Psi\_{\tilde{\text{$F$}},m^{\text{$\prime$}}}^{j,\text{$L$}}\big)\subset B(x\_{r}^{j},c\_{2}2^{\text{$-$}j}). |  | (A.25) |

We may now adapt the definition of the sequence spaces bq,rsb\_{q,r}^{s}, given inÂ ([A.20](https://arxiv.org/html/2511.01125v1#A1.E20 "Equation A.20 â€£ A.2.2 From wavelet para-bases to Besov spaces on Euclidean spaces â€£ A.2 Proof of Theorem 3.7 â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators")), to suit the approximate lattice â„¤ğ’Ÿ\mathbb{Z}\_{\mathcal{D}}, and thus the domain ğ’Ÿ\mathcal{D}.

###### Definition A.8 (Sequence space bq,rsb\_{q,r}^{s}).

Let ğ’Ÿ\mathcal{D} be an arbitrary domain in â„n\mathbb{R}^{n} with ğ’Ÿâ‰ â„n\mathcal{D}\neq\mathbb{R}^{n}, let â„¤ğ’Ÿ\mathbb{Z}\_{\mathcal{D}} be as above, sâˆˆâ„s\in\mathbb{R}, and (q,r)âˆˆ(o,âˆ]2(q,r)\in(o,\infty]^{2}. Then bq,rsâ€‹(â„¤ğ’Ÿ)b^{s}\_{q,r}(\mathbb{Z}\_{\mathcal{D}}) is the collection of all sequences

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î»â‰”{Î»rjâˆˆâ„‚:jâˆˆâ„•,râˆˆ{1,â€¦,Nj}},for someâ€‹Njâˆˆâ„•Â¯,\lambda\coloneqq\big\{\lambda\_{r}^{j}\in\mathbb{C}:j\in\mathbb{N},\;r\in\{1,\ldots,N\_{j}\}\big\},\;\text{\rm for some}\;N\_{j}\in\overline{\mathbb{N}}, |  | (A.26) |

such that

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–Î»â€–bq,rsâ€‹(â„¤ğ’Ÿ)qâ‰”âˆ‘j=0âˆ2jâ€‹(sâˆ’n/q)â€‹râ€‹(âˆ‘k=1Nj|Î»kj|q)r/q<âˆ.\|\lambda\|\_{b^{\text{$s$}}\_{\text{$q$}\text{$,$}\text{$r$}}(\mathbb{Z}\_{\text{$\mathcal{D}$}})}^{q}\coloneqq\sum\_{j=0}^{\infty}2^{j(s\text{$-$}n/q)r}\Bigg(\sum\_{k=1}^{N\_{\text{$j$}}}|\lambda\_{k}^{j}|^{q}\Bigg)^{r/q}<\infty. |  | (A.27) |

As we will see shortly, the wavelet system inÂ ([A.24](https://arxiv.org/html/2511.01125v1#A1.E24 "Equation A.24 â€£ A.2.3 Besov spaces on domains â€£ A.2 Proof of Theorem 3.7 â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators")) is a Schauder basis for several Besov spaces on domains, provided these domains possess a basic level of generic â€˜thicknessâ€™ and regularity of their boundaries. We begin by first noting the relationship between the Besov sequence and function spaces, with the same indices, if the domain has a regular enough boundary.

A domain ğ’ŸâŠ†â„d\mathcal{D}\subseteq\mathbb{R}^{d} is said to be special Lipschitz if there exists a Lipschitz-continuous map Î²:â„dâˆ’1âŸ¶â„\beta:\mathbb{R}^{d\text{$-$}1}\longrightarrow\mathbb{R} such that

|  |  |  |
| --- | --- | --- |
|  | ğ’Ÿ={(x~,xd)âˆˆâ„dâˆ’1Ã—â„:Î²â€‹(x)<xd}.\mathcal{D}=\big\{(\tilde{x},x\_{d})\in\mathbb{R}^{d\text{$-$}1}\times\mathbb{R}:\,\beta(x)<x\_{d}\big\}. |  |

A bounded Lipschitz domain ğ’ŸâŠ‚â„d\mathcal{D}\subset\mathbb{R}^{d} is a bounded domain ğ’Ÿ\mathcal{D} for which there exists a finite number of open balls (B1,â€¦,BN)(B\_{1},\dots,B\_{N}), for some Nâˆˆâ„•â‹†N\in\mathbb{N}^{\star}, where for nâˆˆ{1,â€¦,N}n\in\{1,\dots,N\} we have

|  |  |  |
| --- | --- | --- |
|  | Bnâ‰”{xâˆˆâ„d:â€–xâˆ’x(n)â€–<r(n)},for someâ€‹x(n)âˆˆâˆ‚ğ’Ÿ,and someâ€‹r(n)>0,B\_{n}\coloneqq\big\{x\in\mathbb{R}^{d}:\|x-x^{(n)}\|<r^{(n)}\big\},\;\text{\rm for some}\;x^{(n)}\in\partial\mathcal{D},\;\text{\rm and some}\;r^{(n)}>0, |  |

such that (Bn)nâˆˆ{1,â€¦,N}(B\_{n})\_{n\in\{1,\dots,N\}} is a cover of âˆ‚ğ’Ÿ\partial\mathcal{D}, and there exist rotations of special Lipschitz domains (ğ’Ÿ1,â€¦,ğ’ŸN)âŠ†(â„d)N(\mathcal{D}\_{1},\dots,\mathcal{D}\_{N})\subseteq(\mathbb{R}^{d})^{N} for which

|  |  |  |
| --- | --- | --- |
|  | Bnâˆ©ğ’Ÿ=Bnâˆ©ğ’Ÿn,nâˆˆ{1,â€¦,N}.B\_{n}\cap\mathcal{D}=B\_{n}\cap\mathcal{D}\_{n},\;n\in\{1,\dots,N\}. |  |

Now, given any domain with Lipschitz boundary, we may characterise the inclusion of any square-integrable function into a wide array of Besov spaces depending on its associated sequence Î»\lambda belonging to the â€˜little Besovâ€™ sequence space with the same indices. The following result is [[89](https://arxiv.org/html/2511.01125v1#bib.bib89), Corollary 4.28].

###### Lemma A.9 (Wavelet paraâ€“bases in Besov and Triebelâ€“Lizorkin spaces on bounded Lipschitz domains).

Fix (q,r)âˆˆ(1,âˆ)2(q,r)\in(1,\infty)^{2}. For K>0K>0 small enough, if 5â€‹d/2<K5d/2<K and sâˆˆ(âˆ’K,K)s\in(-K,K) then fâˆˆğ’Ÿâ€‹(ğ’Ÿ)â€²f\in\mathcal{D}(\mathcal{D})^{\prime} belongs to BÂ¯q,rsâ€‹(ğ’Ÿ)\overline{B}\_{q,r}^{s}(\mathcal{D}) ((resp. FÂ¯q,rs(ğ’Ÿ))\overline{F}\_{q,r}^{s}(\mathcal{D})) if and only if admits the representation

|  |  |  |  |
| --- | --- | --- | --- |
|  | f=âˆ‘(j,G,m)âˆˆSğ’ŸÎ»G,mjâ€‹2âˆ’jâ€‹d/2â€‹Î¨G,mj,f=\sum\_{(j,\text{$G$},m)\in S^{\text{$\mathcal{D}$}}}\lambda\_{\text{$G$},m}^{j}2^{\text{$-$}jd/2}\Psi\_{\text{$G$},m}^{j}, |  | (A.28) |

and the following
holds

|  |  |  |
| --- | --- | --- |
|  | â€–(2jâ€‹(sâˆ’d/q)â€‹â€–(Î»G,mj)(G,m)âˆˆSjğ’Ÿâ€–â„“q)jâˆˆâ„•â€–â„“p<âˆ.\Big\|\big(2^{j(s\text{$-$}d/q)}\big\|(\lambda\_{\text{$G$},m}^{j})\_{(\text{$G$},m)\in S^{\text{$\mathcal{D}$}}\_{\text{$j$}}}\big\|\_{\ell^{q}}\big)\_{j\in\mathbb{N}}\Big\|\_{\ell^{p}}<\infty. |  |

In what follows, given any fâˆˆBÂ¯q,rsf\in\bar{B}\_{q,r}^{s} we write Î»â€‹(f)â‰”(Î»G,mj)j,G,mâˆˆSğ’Ÿ\lambda(f)\coloneqq(\lambda\_{\text{$G$},m}^{j})\_{j,G,m\in S^{\mathcal{D}}} for the sequence defined inÂ ([A.28](https://arxiv.org/html/2511.01125v1#A1.E28 "Equation A.28 â€£ A.2.3 Besov spaces on domains â€£ A.2 Proof of Theorem 3.7 â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators")); provided that it is unique. We denote the linear map fâ†¦Î»â€‹(f)f\mapsto\lambda(f) by II.

[SectionËœA.2.3](https://arxiv.org/html/2511.01125v1#A1.SS2.SSS3 "A.2.3 Besov spaces on domains â€£ A.2 Proof of Theorem 3.7 â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators") does not guarantee that the wavelet expansions themselves are uniquely determined. In general, these wavelet â€˜basesâ€™ are only frames. However, the next result shows that this is not necessarily the case for EE-thick domains.

We say that a domain is exterior thick, or EE-thick for short, if there are constants 0<cLâ‰¤cU0<c\_{\text{$L$}}\leq c\_{\text{$U$}} and j0â‰¥0j\_{0}\geq 0 such that for every jâˆˆâ„•j\in\mathbb{N} with jâ‰¥j0,j\geq j\_{0}, there is a dd-dimensional â€˜interiorâ€™ cube QâŠ‚ğ’ŸQ\subset\mathcal{D} with side-length

|  |  |  |
| --- | --- | --- |
|  | cLâ€‹2âˆ’jâ‰¤maxâ¡{â„“â€‹(Q),supzâˆˆQiinfuâˆˆâˆ‚ğ’Ÿâ€–zâˆ’uâ€–}â‰¤cUâ€‹2âˆ’jc\_{\text{$L$}}2^{\text{$-$}j}\leq\max\bigg\{\ell(Q),\sup\_{z\in Q^{\text{$i$}}}\inf\_{u\in\partial\mathcal{D}}\|z-u\|\bigg\}\leq c\_{\text{$U$}}2^{\text{$-$}j} |  |

where QiQ^{i} denotes the interior of any cube QQ in the norm relative topology on ğ’Ÿ\mathcal{D} and â„“â€‹(Q)\ell(Q) denotes its side-length; i.e. â„“â€‹(Q)â‰”supx,yâˆˆQâ€–xâˆ’yâ€–âˆ\ell(Q)\coloneqq\sup\_{x,y\in Q}\,\|x-y\|\_{\infty}; where âˆ¥â‹…âˆ¥âˆ\|\cdot\|\_{\infty} denotes the âˆ\infty-norm on â„d\mathbb{R}^{d}.
In the case of a thick exterior domain, we obtain a Schauder basis using our uu-wavelet expansion, see [[90](https://arxiv.org/html/2511.01125v1#bib.bib90), Theorem 3.13 (ii)].

###### Theorem A.10 (Wavelet-based Schauder bases).

Let ğ’Ÿ\mathcal{D} be an EE-thick domain in â„d\mathbb{R}^{d}. Define for uâˆˆâ„•â‹†u\in\mathbb{N}^{\star}

|  |  |  |
| --- | --- | --- |
|  | {Î¦rj:jâˆˆâ„•,râˆˆ{1,â€¦,Nj}},for someâ€‹Njâˆˆâ„•,\big\{\Phi\_{r}^{j}:j\in\mathbb{N},\;r\in\{1,\ldots,N\_{j}\}\big\},\;\text{\rm for some}\;N\_{j}\in\mathbb{N}, |  |

an orthonormal uu-wavelet basis in L2â€‹(ğ’Ÿ)L\_{2}(\mathcal{D}). Then let BÂ¯q,rsâ€‹(ğ’Ÿ)\overline{B}^{s}\_{q,r}(\mathcal{D}) be the space in [[89](https://arxiv.org/html/2511.01125v1#bib.bib89), Equation (3.46)] and let

|  |  |  |
| --- | --- | --- |
|  | u>maxâ¡{s,Ïƒq,râˆ’s},sâ‰ 0.u>\max\big\{s,\sigma\_{q,r}-s\big\},\;s\neq 0. |  |

Then fâˆˆğ’Ÿâ€²â€‹(ğ’Ÿ)f\in\mathcal{D}^{\prime}(\mathcal{D}) is an element of BÂ¯q,rsâ€‹(ğ’Ÿ)\overline{B}^{s}\_{q,r}(\mathcal{D}) if and only if it can be represented as

|  |  |  |
| --- | --- | --- |
|  | f=âˆ‘j=0âˆâˆ‘k=1NjÎ»kjâ€‹2âˆ’jâ€‹d/2â€‹Î¦kj,Î»âˆˆbq,rsâ€‹(â„¤ğ’Ÿ),f=\sum\_{j=0}^{\infty}\sum\_{k=1}^{N\_{\text{$j$}}}\lambda\_{k}^{j}2^{\text{$-$}jd/2}\Phi\_{k}^{j},\;\lambda\in b^{s}\_{q,r}(\mathbb{Z}\_{\mathcal{D}}), |  |

with convergence holding in ğ’Ÿâ€²â€‹(ğ’Ÿ)\mathcal{D}^{\prime}(\mathcal{D}) and locally in any spaces BÂ¯q,rÏƒâ€‹(ğ’Ÿ)\overline{B}^{\sigma}\_{q,r}(\mathcal{D}) with Ïƒq,r<s\sigma\_{q,r}<s.

Furthermore, if fâˆˆBÂ¯q,rsâ€‹(ğ’Ÿ)f\in\overline{B}^{s}\_{q,r}(\mathcal{D}) then the representation is unique with Î»=Î»â€‹(f)\lambda=\lambda(f) as inÂ ([A.28](https://arxiv.org/html/2511.01125v1#A1.E28 "Equation A.28 â€£ A.2.3 Besov spaces on domains â€£ A.2 Proof of Theorem 3.7 â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators")) and II the linear map in LemmaÂ [A.2.3](https://arxiv.org/html/2511.01125v1#A1.SS2.SSS3 "A.2.3 Besov spaces on domains â€£ A.2 Proof of Theorem 3.7 â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators") is an bi-Lipschitz isomorphism of Banach spaces mapping BÂ¯q.rsâ€‹(ğ’Ÿ)\overline{B}^{s}\_{q.r}(\mathcal{D}) onto bq,rsâ€‹(â„¤ğ’Ÿ)b^{s}\_{q,r}(\mathbb{Z}\_{\mathcal{D}}). If, in addition, q<âˆq<\infty, r<âˆr<\infty, then (Î¦kj){(j,k)âˆˆâ„•2:kâˆˆ{1,â€¦,Nj}}(\Phi\_{k}^{j})\_{\{(j,k)\in\mathbb{N}^{\text{$2$}}:k\in\{1,\dots,N\_{\text{$j$}}\}\}} is an unconditional basis in BÂ¯q,rsâ€‹(ğ’Ÿ)\overline{B}^{s}\_{q,r}(\mathcal{D}).

Having covered the necessary background, we now prove our universal approximation result, see [SectionËœA.3](https://arxiv.org/html/2511.01125v1#A1.SS3 "A.3 Proof of universal approximation â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators") below.

### A.3 Proof of universal approximation

We now express the previous result in terms of neural networks.

###### Lemma A.11 (Wavelet implementation on domains).

Let ğ’Ÿ\mathcal{D} be a bounded domain with Lipschitz boundary111111The following result holds, more general on (Ïµ,Î´)(\epsilon,\delta)-domains and thus on any Lipschitz domain; however, we will not need that level of generality in the remainder of our paper., let ÏƒW\sigma\_{\text{$W$}} and ÏƒS\sigma\_{\text{$S$}} satisfy [SectionËœ2.2.1](https://arxiv.org/html/2511.01125v1#S2.SS2.SSS1 "2.2.1 Residual Kolmogorovâ€“Arnold networks (Resâ€“KANs) â€£ 2.2 Deep learning â€£ 2 Preliminaries â€£ One model to solve them all: 2BSDE families via neural operators") and sâ‰¥2s\geq 2.
Let Gâˆˆ{S,W}dâ£â‹†G\in\{S,W\}^{d\star}, jâˆˆâ„•j\in\mathbb{N}, and mâˆˆâ„¤ğ’Ÿm\in\mathbb{Z}\_{\mathcal{D}}. Then there exists a Resâ€“KAN
Î¨^G,mj:â„dâŸ¶â„\widehat{\Psi}\_{\text{$G$},m}^{j}:\mathbb{R}^{d}\longrightarrow\mathbb{R} of depth dd, width at-most 2â€‹d+12d+1, and using at-most (5â€‹d2+25â€‹d+2)/2(5d^{2}+25d+2)/2 non-zero parameters satisfying

|  |  |  |
| --- | --- | --- |
|  | Î¨G,mjâ€‹(x)=Î¨^G,mj,xâˆˆâ„d.\Psi\_{\text{$G$},m}^{j}(x)=\widehat{\Psi}\_{\text{$G$},m}^{j},\;x\in\mathbb{R}^{d}. |  |

Our proof will use a recent result,Â [[48](https://arxiv.org/html/2511.01125v1#bib.bib48), Lemma 1], which shows that the dd-ary multiplication operator can be exactly implemented using Resâ€“KANs, but only locally. This is in contrast to ReLU MLPs, which can only approximate it locally.

###### Lemma A.12 (Exact multiplication on arbitrarily large hypercubes).

For every dâˆˆâ„•â‹†d\in\mathbb{N}^{\star} and each M>0M>0, there exists a Resâ€“KAN Ã—d2:â„dâŸ¶â„\times^{2}\_{d}:\mathbb{R}^{d}\longrightarrow\mathbb{R} satisfying for each xâˆˆ[âˆ’M,M]dx\in[-M,M]^{d}

|  |  |  |
| --- | --- | --- |
|  | Ã—d2(x)=âˆi=1dxi.\times^{2}\_{d}(x)=\prod\_{i=1}^{d}x\_{i}. |  |

Moreover Ã—d2\times^{2}\_{d} has depth dd, width at-most 2â€‹d+12d+1, and at-most (5â€‹d2+21â€‹d)/2(5d^{2}+21d)/2 non-zero parameters.

We can now proceed with the

###### Proof of LemmaÂ [A.3](https://arxiv.org/html/2511.01125v1#A1.SS3 "A.3 Proof of universal approximation â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators").

Recall that [SectionËœ2.2.1](https://arxiv.org/html/2511.01125v1#S2.SS2.SSS1 "2.2.1 Residual Kolmogorovâ€“Arnold networks (Resâ€“KANs) â€£ 2.2 Deep learning â€£ 2 Preliminaries â€£ One model to solve them all: 2BSDE families via neural operators"), implies that ÏƒS\sigma\_{\text{$S$}} inÂ ([2.1](https://arxiv.org/html/2511.01125v1#S2.E1 "Equation 2.1 â€£ 2.2.1 Residual Kolmogorovâ€“Arnold networks (Resâ€“KANs) â€£ 2.2 Deep learning â€£ 2 Preliminaries â€£ One model to solve them all: 2BSDE families via neural operators")) is a scaling function (father wavelet) and ÏƒW\sigma\_{\text{$W$}} inÂ ([2.1](https://arxiv.org/html/2511.01125v1#S2.E1 "Equation 2.1 â€£ 2.2.1 Residual Kolmogorovâ€“Arnold networks (Resâ€“KANs) â€£ 2.2 Deep learning â€£ 2 Preliminaries â€£ One model to solve them all: 2BSDE families via neural operators")) is the corresponding mother wavelet. In fact, by [SectionËœ2.2.1](https://arxiv.org/html/2511.01125v1#S2.SS2.SSS1 "2.2.1 Residual Kolmogorovâ€“Arnold networks (Resâ€“KANs) â€£ 2.2 Deep learning â€£ 2 Preliminaries â€£ One model to solve them all: 2BSDE families via neural operators"), both are Daubechies wavelets and are thus are in Cuâ€‹(â„)C^{u}(\mathbb{R}) and compactly supported. By their continuity, they are thus bounded. Whence, there is some M>0M>0 such that ÏƒGâ€‹(â„)âŠ†[âˆ’M,M]\sigma\_{\text{$G$}}(\mathbb{R})\subseteq[-M,M] for each Gâˆˆ{S,W}G\in\{S,W\}.

Consequently, for every specification G=(G1,â€¦,Gd)âˆˆ{S,W}dâ£â‹†G=(G\_{1},\dots,G\_{d})\in\{S,W\}^{d\star}, for every jâˆˆâ„¤j\in\mathbb{Z}, we may represent the ((multivariate)) Daubechies wavelet Î¨G,mj\Psi\_{\text{$G$},m}^{j}, defined by rescaling the associated un-normalised wavelet Î¨~G,mj\widetilde{\Psi}\_{\text{$G$},m}^{j} inÂ ([A.17](https://arxiv.org/html/2511.01125v1#A1.E17 "Equation A.17 â€£ A.2.2 From wavelet para-bases to Besov spaces on Euclidean spaces â€£ A.2 Proof of Theorem 3.7 â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators")), by

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î¨G,mj(â‹…)=âˆi=1d2jâ€‹d/2Î²G,WjÏƒGi(2jâ€‹d/2â‹…âˆ’m)=(âˆi=1d2jâ€‹d/2Î²G,Wj)âˆi=1dÏƒGi(W0jâ‹…âˆ’m)\displaystyle\Psi\_{G,m}^{j}(\cdot)=\prod\_{i=1}^{d}\frac{2^{jd/2}}{\beta^{j}\_{\text{$G$},\text{$W$}}}\sigma\_{\text{$G$}\_{\text{$i$}}}\big(2^{jd/2}\cdot-m\big)=\Bigg(\prod\_{i=1}^{d}\frac{2^{jd/2}}{\beta^{j}\_{\text{$G$},\text{$W$}}}\Bigg)\prod\_{i=1}^{d}\sigma\_{\text{$G$}\_{\text{$i$}}}\big(W\_{0}^{j}\cdot-m\big) | â‰•ÎºG,Wjâˆi=1dÏƒGi(W0jâ‹…âˆ’m)\displaystyle\eqqcolon\kappa\_{\text{$G$},\text{$W$}}^{j}\prod\_{i=1}^{d}\sigma\_{\text{$G$}\_{\text{$i$}}}\big(W\_{0}^{j}\cdot-m\big) |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | =ÎºG,MjÃ—d2âˆ˜ÏƒGi(W0jâ‹…âˆ’m),\displaystyle=\kappa\_{\text{$G$},\text{$M$}}^{j}\times\_{d}^{2}\circ\sigma\_{\text{$G$}\_{\text{$i$}}}\big(W\_{0}^{j}\cdot-m\big), |  | (A.29) |

where Î²G,Wjâ‰”â€–Î¨G,mjâ€–L2â€‹(â„)\beta\_{G,W}^{j}\coloneqq\|\Psi\_{G,m}^{j}\|\_{L^{2}(\mathbb{R})}
where W0jâ‰”2jâ€‹d/2â€‹IdW\_{0}^{j}\coloneqq 2^{jd/2}\mathrm{I}\_{d}, mâˆˆâ„¤dm\in\mathbb{Z}^{d} and whereÂ ([A.29](https://arxiv.org/html/2511.01125v1#A1.Ex67 "Equation A.29 â€£ A.3 Proof of universal approximation â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators")) holds by [[48](https://arxiv.org/html/2511.01125v1#bib.bib48), Lemma 1] (having chosen MM large enough); where Ã—d2:â„dâŸ¶â„\times^{2}\_{d}:\mathbb{R}^{d}\longrightarrow\mathbb{R} is a Resâ€“KAN with depth dd, width at-most 2â€‹d+12d+1, and at-most 5â€‹d2+21â€‹d2\frac{5d^{2}+21d}{2} non-zero parameters.

Now, making use of the chosen structure of the â€˜non-splineâ€™ factor in our trainable activation function ÏƒÎ²:I\sigma\_{\beta:I} inÂ [EquationËœ2.1](https://arxiv.org/html/2511.01125v1#S2.E1 "In 2.2.1 Residual Kolmogorovâ€“Arnold networks (Resâ€“KANs) â€£ 2.2 Deep learning â€£ 2 Preliminaries â€£ One model to solve them all: 2BSDE families via neural operators"), for each iâˆˆ{1,â€¦,d}i\in\{1,\dots,d\}, if Gi=SG\_{i}=S we set Î²i=(1)âŠ•0I+1\beta\_{i}=(1)\oplus 0\_{I+1} and if Gi=WG\_{i}=W we set Î²i=(0)âŠ•(1)âŠ•0I\beta\_{i}=(0)\oplus(1)\oplus 0\_{I}
Then,Â ([A.29](https://arxiv.org/html/2511.01125v1#A1.Ex67 "Equation A.29 â€£ A.3 Proof of universal approximation â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators")) can be re-expressed as

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | Î¨G,mj\displaystyle\Psi\_{\text{$G$},m}^{j} | â‰”ÎºG,WjÃ—d2âˆ˜ÏƒGi(W0jâ‹…âˆ’m)\displaystyle\coloneqq\kappa\_{\text{$G$},\text{$W$}}^{j}\times\_{d}^{2}\circ\sigma\_{\text{$G$}\_{\text{$i$}}}\big(W\_{0}^{j}\cdot-m\big) |  | (A.30) |

Now by [[48](https://arxiv.org/html/2511.01125v1#bib.bib48), Lemma 1], Ã—d2\times\_{d}^{2} can be implemented by a ReLU MLP of depth dd, width 2â€‹d+12d+1, and using at-most (5â€‹d2+21â€‹d)/2(5d^{2}+21d)/2 non-zero parameters. Consequently, Ã—d2\times^{2}\_{d} is representable/implementable by a ReLU MLP with depth dd, width at-most 2â€‹d+12d+1, and using at-most (5â€‹d2+25â€‹d+2)/2(5d^{2}+25d+2)/2 non-zero parameters.
âˆ

A direct consequence of the previous result is the following.

###### Proposition A.13 (Resâ€“KAN basis of Besov spaces).

Let ğ’Ÿ\mathcal{D} be a bounded exterior-thick domain, (q,r)âˆˆ(1,âˆ)2(q,r)\in(1,\infty)^{2}, and sâ‰¥2s\geq 2. Then, there is a Schauder basis

|  |  |  |  |
| --- | --- | --- | --- |
|  | {Î¦^kj:jâˆˆâ„•,kâˆˆ{1,â€¦,Nj}},for someâ€‹Njâˆˆâ„•Â¯,\big\{\widehat{\Phi}\_{k}^{j}:j\in\mathbb{N},\;k\in\{1,\ldots,N\_{j}\}\big\},\;\text{\rm for some}\;N\_{j}\in\overline{\mathbb{N}}, |  | (A.31) |

of BÂ¯q,rsâ€‹(ğ’Ÿ)\overline{B}\_{q,r}^{s}(\mathcal{D}) consisting of uu-wavelets. Moreover, for each such k,jk,j, Î¦^kj\widehat{\Phi}\_{k}^{j} is implementable by a Resâ€“KAN of depth dd, width at-most 2â€‹d+12d+1, and using at-most (5â€‹d2+25â€‹d+2)/2(5d^{2}+25d+2)/2 non-zero parameters.

###### Proof.

This is a direct consequence of [SectionËœA.3](https://arxiv.org/html/2511.01125v1#A1.SS3 "A.3 Proof of universal approximation â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators"), [SectionËœA.2.3](https://arxiv.org/html/2511.01125v1#A1.SS2.SSS3 "A.2.3 Besov spaces on domains â€£ A.2 Proof of Theorem 3.7 â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators"), and of [[90](https://arxiv.org/html/2511.01125v1#bib.bib90), Theorem 3.13 (iâ€‹i)(ii)].
âˆ

We now prove the universality of our models in the class of HÃ¶lder continuous maps between Besov spaces; recall the notationÂ ([2.12](https://arxiv.org/html/2511.01125v1#S2.E12 "Equation 2.12 â€£ 2.2.2 Neural operator architectures â€£ 2.2 Deep learning â€£ 2 Preliminaries â€£ One model to solve them all: 2BSDE families via neural operators")).
We write Hldâ¡(BÂ¯q,rsâ€‹(ğ’Ÿ),BÂ¯q,rsâ€‹(ğ’Ÿ))\operatorname{Hld}(\bar{B}^{s}\_{q,r}(\mathcal{D}),\bar{B}^{s}\_{q,r}(\mathcal{D})) for the set of all
Î±\alphaâ€“HÃ¶lder continuous maps from BÂ¯q,rsâ€‹(ğ’Ÿ)\bar{B}^{s}\_{q,r}(\mathcal{D}) to itself, for some 0<Î±â‰¤10<\alpha\leq 1.

###### Proposition A.14 (Universal approximation).

Let dâˆˆâ„•+d\in\mathbb{N}\_{+}, s>0s>0, and ğ’Ÿ\mathcal{D} be a bounded exterior-thick domain in â„d\mathbb{R}^{d}, (q,r)âˆˆ(1,âˆ)2(q,r)\in(1,\infty)^{2} and 2â‰¤s2\leq s, and let Iâ‰”âŒˆsâŒ‰I\coloneqq\lceil s\rceil.
If ÏƒS\sigma\_{\text{$S$}} and ÏƒW\sigma\_{\text{$W$}} satisfy [SectionËœ2.2.1](https://arxiv.org/html/2511.01125v1#S2.SS2.SSS1 "2.2.1 Residual Kolmogorovâ€“Arnold networks (Resâ€“KANs) â€£ 2.2 Deep learning â€£ 2 Preliminaries â€£ One model to solve them all: 2BSDE families via neural operators"), then ğ’©â€‹ğ’ªI,Î±\mathcal{NO}\_{I,\alpha} is dense in Hldâ¡(BÂ¯q,rsâ€‹(ğ’Ÿ),BÂ¯q,r2â€‹(ğ’Ÿ))\operatorname{Hld}(\bar{B}^{s}\_{q,r}(\mathcal{D}),\bar{B}^{2}\_{q,r}(\mathcal{D})) for the ((relative)) topology induced by the topology of uniform convergence on compact sets.

###### Proof.

Since ğ’Ÿ\mathcal{D} is exterior-thick, sâ‰¥2s\geq 2, (q,r)âˆˆ(1,âˆ)2(q,r)\in(1,\infty)^{2}, ÏƒS\sigma\_{\text{$S$}} and ÏƒW\sigma\_{\text{$W$}} satisfy [SectionËœ2.2.1](https://arxiv.org/html/2511.01125v1#S2.SS2.SSS1 "2.2.1 Residual Kolmogorovâ€“Arnold networks (Resâ€“KANs) â€£ 2.2 Deep learning â€£ 2 Preliminaries â€£ One model to solve them all: 2BSDE families via neural operators"), and we set Iâ‰”âŒˆsâŒ‰I\coloneqq\lceil s\rceil then, [SectionËœA.3](https://arxiv.org/html/2511.01125v1#A1.SS3 "A.3 Proof of universal approximation â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators") guarantees that we may exhibit a Schauder basis of BÂ¯q,rsâ€‹(ğ’Ÿ)\overline{B}^{s}\_{q,r}(\mathcal{D}) consisting only of Resâ€“KANs, as inÂ ([A.31](https://arxiv.org/html/2511.01125v1#A1.E31 "Equation A.31 â€£ A.3 Proof of universal approximation â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators")).

Pick an enumeration (Î¨^kâ„“jâ„“)â„“âˆˆâ„•\big(\widehat{\Psi}\_{k\_{\text{$\ell$}}}^{j\_{\text{$\ell$}}}\big)\_{\ell\in\mathbb{N}} thereof.
Now, let ğ”‰\mathfrak{F} consist of all functions F^:BÂ¯q,rsâ€‹(ğ’Ÿ)âŸ¶BÂ¯q,r2â€‹(ğ’Ÿ)\widehat{F}:\bar{B}^{s}\_{q,r}(\mathcal{D})\longrightarrow\bar{B}^{2}\_{q,r}(\mathcal{D}) of the form inÂ [[33](https://arxiv.org/html/2511.01125v1#bib.bib33), Equation 16] and [[33](https://arxiv.org/html/2511.01125v1#bib.bib33), Definition 6 (Neural filters)]

|  |  |  |  |
| --- | --- | --- | --- |
|  | F^â‰”(Î¨^k1j1,â€¦,Î¨^kKjK)âŠ¤â€‹f^ReLUâˆ˜(âˆ«â„dfâ€‹(x)â€‹Î¨^k1j1â€‹dxâ‹®âˆ«â„dfâ€‹(x)â€‹Î¨^kKjKâ€‹dx)\widehat{F}\coloneqq\Big(\widehat{\Psi}^{j\_{\text{$1$}}}\_{k\_{\text{$1$}}},\dots,\widehat{\Psi}^{j\_{\text{$K$}}}\_{k\_{\text{$K$}}}\Big)^{\top}\,\widehat{f}\_{\text{$\rm ReLU$}}\circ\begin{pmatrix}\displaystyle\int\_{\mathbb{R}^{\text{$d$}}}\,f(x)\widehat{\Psi}^{j\_{\text{$1$}}}\_{k\_{\text{$1$}}}\mathrm{d}x\\ \vdots\\ \displaystyle\int\_{\mathbb{R}^{\text{$d$}}}\,f(x)\widehat{\Psi}^{j\_{\text{$K$}}}\_{k\_{\text{$K$}}}\mathrm{d}x\end{pmatrix} |  | (A.32) |

for some Kâˆˆâ„•â‹†K\in\mathbb{N}^{\star}, and where f^ReLU:â„KâŸ¶â„K\widehat{f}\_{\text{$\rm ReLU$}}:\mathbb{R}^{K}\longrightarrow\mathbb{R}^{K} is a ReLU feed-forward neural network defined as iteratively mapping any xâˆˆâ„Kx\in\mathbb{R}^{K} to the vector f^ReLUâ€‹(x)â‰”xL+1\hat{f}\_{\operatorname{ReLU}}(x)\coloneqq x\_{L+1} defined recursively by

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | xL+1\displaystyle x\_{L+1} | â‰”WL+1â€‹xLâˆˆâ„dL+1â‰”â„dK\displaystyle\coloneqq W\_{L+1}x\_{L}\in\mathbb{R}^{d\_{L+1}}\coloneqq\mathbb{R}^{d\_{K}} |  | (A.33) |
|  | xâ„“+1\displaystyle x\_{\ell+1} | â‰”ReLUâ€‹(Wâ„“â€‹xâ„“+bâ„“)âˆˆâ„dâ„“+1,xâˆˆâ„K,Lâˆˆâ„•+,Â forÂ â€‹â„“âˆˆ{0,â€¦,L}\displaystyle\coloneqq\mathrm{ReLU}\big(W\_{\ell}x\_{\ell}+b\_{\ell}\big)\in\mathbb{R}^{d\_{\ell+1}},\;x\in\mathbb{R}^{K},\;L\in\mathbb{N}\_{+},\,\,\mbox{ for }\ell\in\{0,\dots,L\} |  |
|  | x0\displaystyle x\_{0} | â‰”xâˆˆâ„d0â‰”â„dK.\displaystyle\coloneqq x\in\mathbb{R}^{d\_{0}}\coloneqq\mathbb{R}^{d\_{K}}. |  |

where the layer widths are (d0,â€¦,dL+â€‹1)âˆˆ(â„•+)L+2(d\_{0},\dots,d\_{\text{$L$}\text{$+$}1})\in(\mathbb{N}\_{+})^{L\text{$+$}2},
K=d0=dL+â€‹1K=d\_{0}=d\_{\text{$L$}\text{$+$}1}, and for each such â„“\ell, we have Wâ„“âˆˆâ„dâ„“+1Ã—dâ„“W\_{\ell}\in\mathbb{R}^{d\_{\text{$\ell$}\text{$+$}\text{$1$}}\times d\_{\text{$\ell$}}}, as well as bâ„“âˆˆâ„dâ„“+1b\_{\ell}\in\mathbb{R}^{d\_{\text{$\ell$}\text{$+$}\text{$1$}}}.

Since (Î¨^kâ„“jâ„“)â„“âˆˆâ„•\big(\widehat{\Psi}\_{k\_{\text{$\ell$}}}^{j\_{\text{$\ell$}}}\big)\_{\ell\in\mathbb{N}} is a Schauder basis of the Banach space BÂ¯q,rsâ€‹(ğ’Ÿ)\bar{B}^{s}\_{q,r}(\mathcal{D}) and of BÂ¯q,r2â€‹(ğ’Ÿ)\bar{B}^{2}\_{q,r}(\mathcal{D}) thenÂ [[33](https://arxiv.org/html/2511.01125v1#bib.bib33), Theorem 1] implies that ğ”‰\mathfrak{F} is dense in Hldâ¡(BÂ¯q,rsâ€‹(ğ’Ÿ),BÂ¯q,r2â€‹(ğ’Ÿ))\operatorname{Hld}(\bar{B}^{s}\_{q,r}(\mathcal{D}),\bar{B}^{2}\_{q,r}(\mathcal{D})) for the (relative) topology induced by the topology of uniform convergence on compact sets. In other words, for every compact ğ’¦âŠ†BÂ¯q,rsâ€‹(ğ’Ÿ)\mathcal{K}\subseteq\bar{B}^{s}\_{q,r}(\mathcal{D}), every Îµ>0\varepsilon>0, and 0<Î±â‰¤10<\alpha\leq 1, and every Î±\alphaâ€“HÃ¶lder continuous map f:BÂ¯q,rsâ€‹(ğ’Ÿ)âŸ¶BÂ¯q,r2â€‹(ğ’Ÿ)f:\bar{B}^{s}\_{q,r}(\mathcal{D})\longrightarrow\bar{B}^{2}\_{q,r}(\mathcal{D}), there is some F^âˆˆğ”‰\widehat{F}\in\mathfrak{F} satisfying

|  |  |  |  |
| --- | --- | --- | --- |
|  | supuâˆˆğ’¦â€–Fâ€‹(u)âˆ’F^â€‹(u)â€–W2,pâ€‹(ğ’Ÿ)<Îµ.\sup\_{u\in\mathcal{K}}\|F(u)-\widehat{F}(u)\big\|\_{W^{\text{$2$}\text{$,$}\text{$p$}}(\mathcal{D})}<\varepsilon. |  | (A.34) |

To deduce our claim, we will show that ğ”‰âŠ†ğ’©â€‹ğ’ªI,Î±\mathfrak{F}\subseteq\mathcal{NO}\_{\text{$I$},\alpha}. Let F^\widehat{F} be an arbitrary element of ğ”‰\mathfrak{F}, which thus admits a representation as inÂ ([A.32](https://arxiv.org/html/2511.01125v1#A1.E32 "Equation A.32 â€£ A.3 Proof of universal approximation â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators")).

Now, for every â„“âˆˆ{0,â€¦,Lâˆ’1}\ell\in\{0,\dots,L-1\}, let bâ„“â€‹(x)â‰”ğŸ(d+dâ„“+1)Ã—(d+dâ„“+1)â€‹x+ğŸdâŠ•bâ„“b^{\ell}(x)\coloneqq\mathbf{0}\_{(d+d\_{\text{$\ell$}\text{$+$}\text{$1$}})\times(d+d\_{\text{$\ell$}\text{$+$}\text{$1$}})}x+\mathbf{0}\_{d}\oplus b\_{\ell} be a constant Resâ€“KAN, see [EquationËœ2.7](https://arxiv.org/html/2511.01125v1#S2.E7 "In 2.2.1 Residual Kolmogorovâ€“Arnold networks (Resâ€“KANs) â€£ 2.2 Deep learning â€£ 2 Preliminaries â€£ One model to solve them all: 2BSDE families via neural operators"), where ğŸ(d+dâ„“+1)Ã—(d+dâ„“+1)â€‹x\mathbf{0}\_{(d+d\_{\text{$\ell$}\text{$+$}\text{$1$}})\times(d+d\_{\text{$\ell$}\text{$+$}\text{$1$}})}x is the (d+dâ„“+1)Ã—(d+dâ„“+1)(d+d\_{\text{$\ell$}\text{$+$}\text{$1$}})\times(d+d\_{\text{$\ell$}\text{$+$}\text{$1$}}) zero matrix and ğŸdâˆˆâ„d\mathbf{0}\_{d}\in\mathbb{R}^{d} is the zero vector therein. Now, for every â„“âˆˆ{1,â€¦,Lâˆ’1}\ell\in\{1,\dots,L-1\} define the matrix Wâ„“â‰”ğŸdÃ—dâŠ—Wâ„“W^{\ell}\coloneqq\mathbf{0}\_{d\times d}\otimes W\_{\ell}, where âŠ—\otimes denotes the Kronecker product and let WLâ‰”(0KÃ—d|WL)W^{L}\coloneqq(0\_{K\times d}|W\_{L}) denotes the column-wise concatenation of the matrix 0KÃ—d0\_{K\times d} with the matrix WLW\_{L}
.
Now, for each â„“âˆˆ{1,â€¦,L}\ell\in\{1,\dots,L\} let Î²â„“â‰”(0,0,1,0,â€¦,0)âˆˆâ„dâ„“+1+2\beta\_{\ell}\coloneqq(0,0,1,0,\dots,0)\in\mathbb{R}^{d\_{\text{$\ell$}\text{$+$}\text{$1$}}+2}. With these specifications, we see that the KANO Î“\Gamma with representationÂ ([2.2.2](https://arxiv.org/html/2511.01125v1#S2.SS2.SSS2 "2.2.2 Neural operator architectures â€£ 2.2 Deep learning â€£ 2 Preliminaries â€£ One model to solve them all: 2BSDE families via neural operators")) (where din=1d\_{\rm in}=1 and dout=1d\_{\rm out}=1) is exactly equal to F^\widehat{F}. We have thus shown that ğ”‰âŠ†ğ’©â€‹ğ’ªI,Î±\mathfrak{F}\subseteq\mathcal{NO}\_{\text{$I$},\alpha}, which concludes our proof.
âˆ

### A.4 Stability estimate of general solution operator

###### Lemma A.15 (Linear stability of perturbations to PDE).

Under [SectionsËœ3.2](https://arxiv.org/html/2511.01125v1#S3.SS2 "3.2 General approximability guarantee â€£ 3 Main results â€£ One model to solve them all: 2BSDE families via neural operators") andÂ [3.2](https://arxiv.org/html/2511.01125v1#S3.SS2 "3.2 General approximability guarantee â€£ 3 Main results â€£ One model to solve them all: 2BSDE families via neural operators"),
if r>0r>0 and k>1+maxâ¡{1,d/p}k>1+\max\{1,d/p\} then there exists a constant L2,k,ğ’Ÿ>0L\_{2,k,\text{$\mathcal{D}$}}>0 such that the non-linear operator

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | Î“Gen:ğ’³kâ€‹(r)\displaystyle\Gamma\_{\text{$\rm Gen$}}:\mathcal{X}\_{k}(r) | âŸ¶Wp2â€‹(ğ’Ÿ)\displaystyle\longrightarrow W^{2}\_{p}(\mathcal{D}) |  | (A.35) |
|  | (GÂ¯0,g)\displaystyle(\bar{G}\_{0},g) | âŸ¼uGÂ¯0,g,\displaystyle\longmapsto u\_{\bar{\text{$G$}}\_{\text{$0$}},g}, |  |

is L2,k,ğ’ŸL\_{2,k,\text{$\mathcal{D}$}}â€“Lipschitz continuous.

###### Proof.

UnderÂ [SectionsËœ3.2](https://arxiv.org/html/2511.01125v1#S3.SS2 "3.2 General approximability guarantee â€£ 3 Main results â€£ One model to solve them all: 2BSDE families via neural operators") andÂ [3.2](https://arxiv.org/html/2511.01125v1#S3.SS2 "3.2 General approximability guarantee â€£ 3 Main results â€£ One model to solve them all: 2BSDE families via neural operators") we may applyÂ [[54](https://arxiv.org/html/2511.01125v1#bib.bib54), Theorem 14.1.3] to deduce that for every ((GÂ¯0,g),(GÂ¯0â€²,gâ€²))âˆˆğ’³Ã—ğ’³((\bar{G}\_{0},g),(\bar{G}\_{0}^{\prime},g^{\prime}))\in\mathcal{X}\times{\cal X} and the respective solutions uGÂ¯0,g,uGÂ¯0â€²,gâ€²u\_{\bar{\text{$G$}}\_{\text{$0$}},g},u\_{\bar{\text{$G$}}\_{\text{$0$}}^{\text{$\prime$}},g^{\text{$\prime$}}} (which exist byÂ [[54](https://arxiv.org/html/2511.01125v1#bib.bib54), Theorem 14.1.5]) to their elliptic PDE inÂ ([3.5](https://arxiv.org/html/2511.01125v1#S3.E5 "Equation 3.5 â€£ 3.2 General approximability guarantee â€£ 3 Main results â€£ One model to solve them all: 2BSDE families via neural operators")) with G+GÂ¯0G+\bar{G}\_{0} and G+GÂ¯0â€²G+\bar{G}\_{0}^{\prime} respectively instead of GG, we have the estimate

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–uGÂ¯0,gâˆ’uGÂ¯0â€²,gâ€²â€–Wp2â€‹(ğ’Ÿ)â‰²â€–GÂ¯0âˆ’GÂ¯0â€²â€–Lpâ€‹(ğ’Ÿ)+â€–gâˆ’gâ€²â€–W2,pâ€‹(ğ’Ÿ)+â€–uGÂ¯0,gâˆ’uGÂ¯0â€²,gâ€²â€–Câ€‹(ğ’Ÿ),\|u\_{\text{$\bar{G}$}\_{\text{$0$}},g}-u\_{\text{$\bar{G}$}\_{\text{$0$}}^{\text{$\prime$}},g^{\text{$\prime$}}}\|\_{W\_{\text{$p$}}^{\text{$2$}}(\mathcal{D})}\lesssim\|\bar{G}\_{0}-\bar{G}\_{0}^{\prime}\|\_{L^{\text{$p$}}(\mathcal{D})}+\|g-g^{\prime}\|\_{W^{\text{$2$}\text{$,$}\text{$p$}}(\mathcal{D})}+\|u\_{\text{$\bar{G}$}\_{\text{$0$}},g}-u\_{\text{$\bar{G}$}\_{\text{$0$}}^{\text{$\prime$}},g^{\text{$\prime$}}}\|\_{C(\mathcal{D})}, |  | (A.36) |

where â‰²\lesssim suppress a multiplicative constant depending only on c1c\_{1}, c2c\_{2}, R0R\_{0}, Î´\delta, LFL\_{F}, Ï‰F\omega\_{F}, and on the domain ğ’Ÿ\mathcal{D}. Next, applyingÂ [[54](https://arxiv.org/html/2511.01125v1#bib.bib54), Lemma 6.6.10] we deduce that there is an absolute constant C>0C>0 such that â€–uGÂ¯0,gâˆ’uGÂ¯0â€²,gâ€²â€–Câ€‹(ğ’Ÿ)â‰¤Câ€‹supxâˆˆâˆ‚ğ’Ÿ|gâ€‹(x)âˆ’gâ€²â€‹(x)|=â€–gâˆ’gâ€²â€–Câ€‹(âˆ‚ğ’Ÿ)\|u\_{\text{$\bar{G}$}\_{\text{$0$}},g}-u\_{\text{$\bar{G}$}\_{\text{$0$}}^{\text{$\prime$}},g^{\text{$\prime$}}}\|\_{C(\mathcal{D})}\leq C\sup\_{x\in\partial\mathcal{D}}|g(x)-{g}^{\prime}(x)|=\|g-g^{\prime}\|\_{C(\partial\mathcal{D})}. Consequently,Â ([A.36](https://arxiv.org/html/2511.01125v1#A1.E36 "Equation A.36 â€£ A.4 Stability estimate of general solution operator â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators")) may be bounded above by

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–uGÂ¯0,gâˆ’uGÂ¯0â€²,gâ€²â€–Wp2â€‹(ğ’Ÿ)\displaystyle\|u\_{\text{$\bar{G}$}\_{\text{$0$}},g}-u\_{\text{$\bar{G}$}\_{\text{$0$}}^{\text{$\prime$}},g^{\text{$\prime$}}}\|\_{W\_{\text{$p$}}^{\text{$2$}}(\mathcal{D})} | â‰²â€–GÂ¯0âˆ’GÂ¯0â€²â€–Lpâ€‹(ğ’Ÿ)+â€–gâˆ’gâ€²â€–W2,pâ€‹(ğ’Ÿ)+â€–gâˆ’gâ€²â€–Câ€‹(âˆ‚ğ’Ÿ)\displaystyle\lesssim\|\bar{G}\_{0}-\bar{G}\_{0}^{\prime}\|\_{L^{\text{$p$}}(\mathcal{D})}+\|g-g^{\prime}\|\_{W^{\text{$2$}\text{$,$}\text{$p$}}(\mathcal{D})}+\|g-g^{\prime}\|\_{C(\partial\mathcal{D})} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤â€–GÂ¯0âˆ’GÂ¯0â€²â€–W2,pâ€‹(ğ’Ÿ)+â€–gâˆ’gâ€²â€–W2,pâ€‹(ğ’Ÿ)+â€–gâˆ’gâ€²â€–Câ€‹(âˆ‚ğ’Ÿ)\displaystyle\leq\|\bar{G}\_{0}-\bar{G}\_{0}^{\prime}\|\_{W^{\text{$2$}\text{$,$}\text{$p$}}(\mathcal{D})}+\|g-g^{\prime}\|\_{W^{\text{$2$}\text{$,$}\text{$p$}}(\mathcal{D})}+\|g-g^{\prime}\|\_{C(\partial\mathcal{D})} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤â€–GÂ¯0âˆ’GÂ¯0â€²â€–W2,pâ€‹(ğ’Ÿ)+â€–gâˆ’gâ€²â€–W2,pâ€‹(ğ’Ÿ)+â€–gâˆ’gâ€²â€–Câ€‹(ğ’Ÿ)\displaystyle\leq\|\bar{G}\_{0}-\bar{G}\_{0}^{\prime}\|\_{W^{\text{$2$}\text{$,$}\text{$p$}}(\mathcal{D})}+\|g-g^{\prime}\|\_{W^{\text{$2$}\text{$,$}\text{$p$}}(\mathcal{D})}+\|g-g^{\prime}\|\_{C(\mathcal{D})} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤â€–GÂ¯0âˆ’GÂ¯0â€²â€–W2,pâ€‹(ğ’Ÿ)+â€–gâˆ’gâ€²â€–W2,pâ€‹(ğ’Ÿ)+â€–gâˆ’gâ€²â€–Wk,pâ€‹(ğ’Ÿ)\displaystyle\leq\|\bar{G}\_{0}-\bar{G}\_{0}^{\prime}\|\_{W^{\text{$2$}\text{$,$}\text{$p$}}(\mathcal{D})}+\|g-g^{\prime}\|\_{W^{\text{$2$}\text{$,$}\text{$p$}}(\mathcal{D})}+\|g-g^{\prime}\|\_{W^{\text{$k$}\text{$,$}\text{$p$}}(\mathcal{D})} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤C~2,k,ğ’Ÿâ€‹â€–GÂ¯0âˆ’GÂ¯0â€²â€–Wk,pâ€‹(ğ’Ÿ)+C~2,k,ğ’Ÿâ€‹â€–gâˆ’gâ€²â€–Wk,pâ€‹(ğ’Ÿ)+â€–gâˆ’gâ€²â€–Wk,pâ€‹(ğ’Ÿ)\displaystyle\leq\widetilde{C}\_{2,k,\text{$\mathcal{D}$}}\,\|\bar{G}\_{0}-\bar{G}\_{0}^{\prime}\|\_{W^{\text{$k$}\text{$,$}\text{$p$}}(\mathcal{D})}+\widetilde{C}\_{2,k,\text{$\mathcal{D}$}}\,\|g-g^{\prime}\|\_{W^{\text{$k$}\text{$,$}\text{$p$}}(\mathcal{D})}+\|g-g^{\prime}\|\_{W^{\text{$k$}\text{$,$}\text{$p$}}(\mathcal{D})} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤L2,k,ğ’Ÿâ€‹(â€–GÂ¯0âˆ’GÂ¯0â€²â€–Wk,pâ€‹(ğ’Ÿ)+â€–gâˆ’gâ€²â€–Wk,pâ€‹(ğ’Ÿ)),\displaystyle\leq L\_{2,k,\text{$\mathcal{D}$}}\,\Big(\|\bar{G}\_{0}-\bar{G}\_{0}^{\prime}\|\_{W^{\text{$k$}\text{$,$}\text{$p$}}(\mathcal{D})}+\|g-g^{\prime}\|\_{W^{\text{$k$}\text{$,$}\text{$p$}}(\mathcal{D})}\Big), |  |

where we used in the fourth line the Sobolev embedding TheoremÂ [[26](https://arxiv.org/html/2511.01125v1#bib.bib26), Section 5.6.3], which holds provided that kâ‰¤1+âŒˆdpâŒ‰k\leq 1+\lceil\tfrac{d}{p}\rceil, where the existence of the constant C~2,k,ğ’Ÿ>0\widetilde{C}\_{2,k,\text{$\mathcal{D}$}}>0 (which only depends on 22, kk, and on ğ’Ÿ\mathcal{D}) as well as the validity of the fifth line are ensured since we have assumed that 2<k2<k so that the Rellichâ€“-Kondrachov TheoremÂ [[88](https://arxiv.org/html/2511.01125v1#bib.bib88), Proposition 4.4] implies that W2,pâ€‹(ğ’Ÿ)W^{2,p}(\mathcal{D}) is compactly embedded in Wk,pâ€‹(ğ’Ÿ)W^{k,p}(\mathcal{D}), and C:=2â€‹C~2,k,ğ’Ÿ+1>1C:=2\widetilde{C}\_{2,k,\text{$\mathcal{D}$}}+1>1.
âˆ

We are now ready to establish our approximability result for the solution operator corresponding to the more general class of fully non-linear elliptic PDEs.

###### Proof of TheoremÂ [3.7](https://arxiv.org/html/2511.01125v1#S3.Thmtheorem7 "Theorem 3.7 (Approximability of the perturbation-to-solution map). â€£ 3.2 General approximability guarantee â€£ 3 Main results â€£ One model to solve them all: 2BSDE families via neural operators").

UnderÂ [SectionsËœ3.2](https://arxiv.org/html/2511.01125v1#S3.SS2 "3.2 General approximability guarantee â€£ 3 Main results â€£ One model to solve them all: 2BSDE families via neural operators") andÂ [3.2](https://arxiv.org/html/2511.01125v1#S3.SS2 "3.2 General approximability guarantee â€£ 3 Main results â€£ One model to solve them all: 2BSDE families via neural operators"), [SectionËœA.4](https://arxiv.org/html/2511.01125v1#A1.SS4 "A.4 Stability estimate of general solution operator â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators") applies and guarantees that the non-linear operator Î“Gen\Gamma\_{\text{$\rm Gen$}}, defined inÂ ([A.35](https://arxiv.org/html/2511.01125v1#A1.E35 "Equation A.35 â€£ A.4 Stability estimate of general solution operator â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators")), is L2,k,ğ’ŸL\_{2,k,\text{$\mathcal{D}$}}â€“Lipschitz continuous on ğ’³kâ€‹(r)\mathcal{X}\_{k}(r). Now, since 2<k<âˆ2<k<\infty and ÏƒS\sigma\_{\text{$S$}} and ÏƒW\sigma\_{\text{$W$}} satisfy [SectionËœ2.2.1](https://arxiv.org/html/2511.01125v1#S2.SS2.SSS1 "2.2.1 Residual Kolmogorovâ€“Arnold networks (Resâ€“KANs) â€£ 2.2 Deep learning â€£ 2 Preliminaries â€£ One model to solve them all: 2BSDE families via neural operators"), we may apply [SectionËœA.3](https://arxiv.org/html/2511.01125v1#A1.SS3 "A.3 Proof of universal approximation â€£ Appendix A Proof of PDE results â€£ One model to solve them all: 2BSDE families via neural operators") to deduce that for every Îµ>0\varepsilon>0 and every non-empty compact subset ğ’³âŠ†ğ’³kâ€‹(r)\mathcal{X}\subseteq\mathcal{X}\_{k}(r)(in the relative topology induced by inclusion in W2,p(ğ’Ÿ)Ã—Wk,p(ğ’Ÿ))W^{2,p}(\mathcal{D})\times W^{k,p}(\mathcal{D})) equipped with the norm topology)Â there exists Î“^âˆˆğ’©â€‹ğ’ªâŒˆkâŒ‰,1\hat{\Gamma}\in\mathcal{NO}\_{\lceil k\rceil,1} satisfying the uniform estimate

|  |  |  |  |
| --- | --- | --- | --- |
|  | sup(GÂ¯0,g)âˆˆğ’³â€–Î“Genâ€‹(GÂ¯0,g)âˆ’Î“^â€‹(GÂ¯0,g)â€–W2,pâ€‹(ğ’Ÿ)<Îµ.\sup\_{(\bar{\text{$G$}}\_{\text{$0$}},g)\in\mathcal{X}}\,\big\|\Gamma\_{\text{$\rm Gen$}}(\bar{G}\_{0},g)-\hat{\Gamma}(\bar{G}\_{0},g)\big\|\_{W^{\text{$2$}\text{$,$}\text{$p$}}(\mathcal{D})}<\varepsilon. |  | (A.37) |

Noting that, by definition, uGÂ¯0,g=Î“Genâ€‹(GÂ¯0,g)u\_{\text{$\bar{G}$}\_{\text{$0$}},g}=\Gamma\_{\text{$\rm Gen$}}(\bar{G}\_{0},g) for each (GÂ¯0,g)âˆˆğ’³(\bar{G}\_{0},g)\in\mathcal{X} concludes the proof.
âˆ

## Appendix B Proof of stochastic results

To derive the stochastic counterparts of our results, we emphasise that our approach does not rely on any unconventional lifting channelsâ€”such as those introduced inÂ [[31](https://arxiv.org/html/2511.01125v1#bib.bib31)]â€”which are non-standard within the operator learning literature and were originally proposed to enforce additional smoothness.
Instead, we are able to combine the Bernstein and Sobolev inequalities with ItÃ´-type formulas in a compatible manner, without imposing excessive smoothness assumptions on the PDE solutions. This is achieved through the following transfer principle, which requires conditions we borrow from [deÂ Marco](https://arxiv.org/html/2511.01125v1#bib.bib21) [[21](https://arxiv.org/html/2511.01125v1#bib.bib21)].

###### Assumption B.1 (Regularity of the forward process).

1. (i)(i)

   there is Î·â‰¥0\eta\geq 0 such that Î¼\mu and Î³\gamma inÂ ([SDE](https://arxiv.org/html/2511.01125v1#S1.Ex1 "Equation SDE â€£ 1 Introduction â€£ One model to solve them all: 2BSDE families via neural operators")) are of class CâˆC^{\infty} on â„dâˆ–Bâ„dâ€‹(0,Î·)Â¯\mathbb{R}^{d}\setminus\overline{B\_{\mathbb{R}^{d}}(0,\eta)}.
   Moreover, for every R>0R>0 and x0âˆˆâ„dx\_{0}\in\mathbb{R}^{d}, Î¼\mu and Î³\gamma are smooth on Bâ„dâ€‹(x0,3â€‹R)âŠ‚â„dâˆ–Bâ„dâ€‹(0,Î·)Â¯;B\_{\mathbb{R}^{d}}(x\_{0},3R)\subset\mathbb{R}^{d}\setminus\overline{B\_{\mathbb{R}^{d}}(0,\eta)};
2. (iâ€‹i)(ii)

   there exist positive exponents qq and qÂ¯>0\bar{q}>0, as well as constants
   0<C0<10<C\_{0}<1, Ck>0C\_{k}>0 (for every multiâ€“index Î±\alpha with |Î±|=kâ‰¥1|\alpha|=k\geq 1) such that

   |  |  |  |  |  |
   | --- | --- | --- | --- | --- |
   |  | |âˆ‚Î±Î¼iâ€‹(x)|+|âˆ‚Î±Î³i,jâ€‹(x)|\displaystyle|\partial\_{\alpha}\mu^{i}(x)|+|\partial\_{\alpha}\gamma^{i,j}(x)| | â‰¤Ckâ€‹(1+â€–xâ€–q),xâˆˆâ„d,(i,j)âˆˆ{1,â€¦,d}2,\displaystyle\leq C\_{k}(1+\|x\|^{q}),\;x\in\mathbb{R}^{d},\;(i,j)\in\{1,\dots,d\}^{2}, |  | (B.1) |
   |  |  |  |  |  |
   | --- | --- | --- | --- | --- |
   |  | C0â€‹â€–xâ€–âˆ’qÂ¯â€‹Id\displaystyle C\_{0}\|x\|^{-\bar{q}}\mathrm{I}\_{d} | â‰¤Î³â€‹(x)â€‹Î³â€‹(x)âŠ¤,â€–xâ€–>Î·;\displaystyle\leq\gamma(x)\gamma(x)^{\top},\;\|x\|>\eta; |  | (B.2) |
3. (iâ€‹iâ€‹i)(iii)

   for every p>0p>0, sup0â‰¤sâ‰¤tğ”¼â„™â€‹[â€–Xsâ€–p]<âˆ;\sup\_{0\leq s\leq t}\mathbb{E}^{\mathbb{P}}[\|X\_{s}\|^{p}]<\infty;
4. (iâ€‹v)(iv)

   ([SDE](https://arxiv.org/html/2511.01125v1#S1.Ex1 "Equation SDE â€£ 1 Introduction â€£ One model to solve them all: 2BSDE families via neural operators")) admits a strong solution.

Under these conditions, the process XX admits for every tâˆˆ(0,T]t\in(0,T]
a smooth density satisfying some
Gaussian-type decay and derivative bounds, as shown inÂ [[21](https://arxiv.org/html/2511.01125v1#bib.bib21), Theorem 2.2].
In what follows, if it exists, for any time tâ‰¥0t\geq 0, we denote the density of the law XtX\_{t} with respect to the Lebesgue measure on BRâ€‹(y0)B\_{\text{$R$}}(y\_{0}), for any y0âˆˆğ’Ÿy\_{0}\in\mathcal{D} and R>0R>0, by Ït,y0âˆˆL1â€‹(BRâ€‹(y0);[0,âˆ))\rho\_{t,y\_{\text{$0$}}}\in L^{1}(B\_{\text{$R$}}(y\_{0});[0,\infty)), where

|  |  |  |
| --- | --- | --- |
|  | L1â€‹(BRâ€‹(y0);[0,âˆ))â‰”{uâˆˆL1â€‹(BRâ€‹(y0)):uâ€‹(x)â‰¥0,Lebesgueâ€“a.e.}.L^{1}(B\_{\text{$R$}}(y\_{0});[0,\infty))\coloneqq\big\{u\in L^{1}(B\_{\text{$R$}}(y\_{0})):u(x)\geq 0,\;\text{\rm Lebesgue--a.e.}\}. |  |

###### Lemma B.2 (Transfer trick).

Let 1â‰¤s<âˆ1\leq s<\infty, 1â‰¤râ‰¤âˆ1\leq r\leq\infty, x0âˆˆğ’Ÿx\_{0}\in\mathcal{D} be such that ğ’ŸâŠ†BRâ€‹(x0)\mathcal{D}\subseteq B\_{R}(x\_{0}) be a compact domain, and (u,u^)âˆˆWs,râ€‹(ğ’Ÿ)Ã—Ws,râ€‹(ğ’Ÿ)(u,\hat{u})\in W^{s,r}(\mathcal{D})\times W^{s,r}(\mathcal{D}) be such that

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–uâˆ’u^â€–Ws,râ€‹(ğ’Ÿ)â‰¤Îµ.\|u-\hat{u}\|\_{W^{\text{$s$}\text{$,$}\text{$r$}}(\mathcal{D})}\leq\varepsilon. |  | (B.3) |

Suppose that XX satisfiesÂ ([SDE](https://arxiv.org/html/2511.01125v1#S1.Ex1 "Equation SDE â€£ 1 Introduction â€£ One model to solve them all: 2BSDE families via neural operators")) and [AppendixËœB](https://arxiv.org/html/2511.01125v1#A2 "Appendix B Proof of stochastic results â€£ One model to solve them all: 2BSDE families via neural operators") and Ï„\tau is the first exit time of XX from ğ’Ÿ{\cal D}.
If rr is finite, then additionally assume that there is some 0<Î´ğ’Ÿ0<\delta\_{\text{$\mathcal{D}$}} such that dâ€‹(0,ğ’Ÿ)â‰”infxâˆˆğ’Ÿâ€–xâ€–2â‰¥Î´ğ’Ÿd(0,\mathcal{D})\coloneqq\inf\_{x\in\mathcal{D}}\|x\|\_{2}\geq\delta\_{\text{$\mathcal{D}$}} and fix a time-window 0<Tâˆ’<T+0<T\_{\text{$-$}}<T\_{\text{$+$}}.
Then

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | ğ”¼â„™â€‹[âˆ«Tâˆ’T+âˆ‘|Î²|â‰¤sâ€–DÎ²â€‹uâ€‹(Xt)âˆ’DÎ²â€‹u^â€‹(Xt)â€–â€‹dâ€‹t]\displaystyle\mathbb{E}^{\mathbb{P}}\Bigg[\int\_{T\_{\text{$-$}}}^{T\_{\text{$+$}}}\sum\_{|\beta|\leq s}\big\|D^{\beta}u(X\_{t})-D^{\beta}\hat{u}(X\_{t})\big\|\mathrm{d}t\Bigg] | â‰²r,T+,ğ’ŸÎµâ€‹(CT++1Tâˆ’3â€‹d/2âˆ’1),ifâ€‹â€„1â‰¤r<âˆ,\displaystyle\lesssim\_{r,\text{$T$}\_{\text{$+$}},\text{$\mathcal{D}$}}\varepsilon\bigg(C\_{\text{$T$}\_{\text{$+$}}}+\frac{1}{T\_{\text{$-$}}^{3d/2-1}}\bigg),\;\mbox{\rm if}1\leq r<\infty, |  | (B.4) |
|  | essupâ„™â€‹{sup0â‰¤tâ‰¤Ï„â€–DÎ²â€‹uâ€‹(Xtâ€‹(Ï‰))âˆ’DÎ²â€‹u^â€‹(Xtâ€‹(Ï‰))â€–}\displaystyle\mathrm{essup}^{\mathbb{P}}\bigg\{\sup\_{0\leq t\leq\tau}\big\|D^{\beta}u(X\_{t}(\omega))-D^{\beta}\,\hat{u}(X\_{t}(\omega))\big\|\bigg\} | â‰¤Îµ,ifâ€‹r=âˆ,\displaystyle\leq\varepsilon,\;\mbox{\rm if}\;r=\infty, |  |

where CT+>0C\_{\text{$T$}\_{\text{$+$}}}>0 is a constant depending only on T+T\_{\text{$+$}}.

###### Proof.

For the case where r=âˆr=\infty, simply note that Xtâˆ¨Ï„âˆˆğ’ŸX\_{t\vee\tau}\in\mathcal{D}. â„™\mathbb{P}â€“a.s. Thus, for â„™\mathbb{P}â€“almost every Ï‰âˆˆÎ©\omega\in\Omega we have that

|  |  |  |
| --- | --- | --- |
|  | âˆ‘|Î²|â‰¤sâ€–DÎ²â€‹uâ€‹(Xtâ€‹(Ï‰))âˆ’DÎ²â€‹u^â€‹(Xtâ€‹(Ï‰))â€–â‰¤supxâˆˆğ’Ÿâ€–DÎ²â€‹(uâˆ’u^)â€‹(x)â€–=â€–uâˆ’u^â€–Ws,râ€‹(ğ’Ÿ)â‰¤Îµ,\sum\_{|\beta|\leq s}\big\|D^{\beta}u(X\_{t}(\omega))-D^{\beta}\hat{u}(X\_{t}(\omega))\big\|\leq\sup\_{x\in\mathcal{D}}\big\|D^{\beta}(u-\hat{u})(x)\big\|=\|u-\hat{u}\|\_{W^{\text{$s$}\text{$,$}\text{$r$}}(\mathcal{D})}\leq\varepsilon, |  |

where the last inequality holds since sâ‰¥1s\geq 1. Consequently,Â ([B.4](https://arxiv.org/html/2511.01125v1#A2.E4 "Equation B.4 â€£ Appendix B Proof of stochastic results â€£ One model to solve them all: 2BSDE families via neural operators")) holds.

We now turn our attention to the case where 1â‰¤r<âˆ1\leq r<\infty. Define Ï„â‹†â‰”T+âˆ§(Ï„âˆ¨Tâˆ’)\tau^{\star}\coloneqq T\_{\text{$+$}}\wedge(\tau\vee T\_{\text{$-$}}). Note that, if tâˆˆ[Tâˆ’,T+]t\in[T\_{\text{$-$}},T\_{\text{$+$}}] then Xtâˆ§Ï„â‹†âˆˆğ’ŸÂ¯X\_{t\wedge\tau^{\text{$\star$}}}\in\bar{\mathcal{D}}, â„™\mathbb{P}â€“a.s.
In particular, since ğ’Ÿ\mathcal{D} is bounded, then for any tâ‰¥0t\geq 0, Xtâˆ§Ï„â‹†âˆˆLâˆâ€‹([0,T+]Ã—Î©,â„d)X\_{t\wedge\tau^{\text{$\star$}}}\in L^{\infty}([0,T\_{\text{$+$}}]\times\Omega,\mathbb{R}^{d}); whence, we may apply the Fubiniâ€“Tonelli theorem to deduce that

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | ğ”¼â„™â€‹[âˆ«Tâˆ’T+âˆ‘|Î²|â‰¤sâ€–DÎ²â€‹uâ€‹(Xt)âˆ’DÎ²â€‹u^â€‹(Xt)â€–â€‹dâ€‹t]\displaystyle\mathbb{E}^{\mathbb{P}}\Bigg[\int\_{T\_{\text{$-$}}}^{T\_{\text{$+$}}}\sum\_{|\beta|\leq s}\big\|D^{\beta}u(X\_{t})-D^{\beta}\hat{u}(X\_{t})\big\|\mathrm{d}t\Bigg] | =âˆ«Tâˆ’T+ğ”¼â„™â€‹[âˆ‘|Î²|â‰¤sâ€–DÎ²â€‹uâ€‹(Xt)âˆ’DÎ²â€‹u^â€‹(Xt)â€–]â€‹dt.\displaystyle=\int\_{T\_{\text{$-$}}}^{T\_{\text{$+$}}}\mathbb{E}^{\mathbb{P}}\Bigg[\sum\_{|\beta|\leq s}\big\|D^{\beta}u(X\_{t})-D^{\beta}\hat{u}(X\_{t})\big\|\Bigg]\mathrm{d}t. |  | (B.5) |

Now, since we are operating under [AppendixËœB](https://arxiv.org/html/2511.01125v1#A2 "Appendix B Proof of stochastic results â€£ One model to solve them all: 2BSDE families via neural operators"), we may applyÂ [[21](https://arxiv.org/html/2511.01125v1#bib.bib21), Theorem 2.2] to show that Ït,x0âˆˆL+1â€‹(BRâ€‹(x0))\rho\_{t,x\_{\text{$0$}}}\in L^{1}\_{\text{$+$}}(B\_{R}(x\_{0})) exists and there is a constant Cr,T+>0C\_{r,T\_{\text{$+$}}}>0, depending only on rr and T+T\_{\text{$+$}}, such that for every xâˆˆBRâ€‹(x0)x\in B\_{R}(x\_{0}) we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | |Ït,x0â€‹(x)|â‰¤Cr,T+â€‹(1+1t3â€‹d/2)â€‹â€–xâ€–âˆ’r.|\rho\_{t,x\_{\text{$0$}}}(x)|\leq C\_{r,T\_{\text{$+$}}}\bigg(1+\frac{1}{t^{3d/2}}\bigg)\|x\|^{\text{$-$}r}. |  | (B.6) |

In particular, since ğ’ŸâŠ†BRâ€‹(x0)\mathcal{D}\subseteq B\_{R}(x\_{0}) thenÂ ([B.6](https://arxiv.org/html/2511.01125v1#A2.E6 "Equation B.6 â€£ Appendix B Proof of stochastic results â€£ One model to solve them all: 2BSDE families via neural operators")) holds for every xâˆˆğ’Ÿx\in\mathcal{D}. Consequently,Â ([B.5](https://arxiv.org/html/2511.01125v1#A2.E5 "Equation B.5 â€£ Appendix B Proof of stochastic results â€£ One model to solve them all: 2BSDE families via neural operators")) andÂ ([B.6](https://arxiv.org/html/2511.01125v1#A2.E6 "Equation B.6 â€£ Appendix B Proof of stochastic results â€£ One model to solve them all: 2BSDE families via neural operators")) imply that

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â„™â€‹[âˆ«Tâˆ’T+âˆ‘|Î²|â‰¤sâ€–DÎ²â€‹uâ€‹(Xt)âˆ’DÎ²â€‹u^â€‹(Xt)â€–â€‹dâ€‹t]\displaystyle\mathbb{E}^{\mathbb{P}}\Bigg[\int\_{T\_{\text{$-$}}}^{T\_{\text{$+$}}}\sum\_{|\beta|\leq s}\big\|D^{\beta}u(X\_{t})-D^{\beta}\hat{u}(X\_{t})\big\|\mathrm{d}t\Bigg] | =âˆ«Tâˆ’T+âˆ«ğ’Ÿpt,x0â€‹(x)â€‹âˆ‘|Î²|â‰¤sâ€–DÎ²â€‹uâ€‹(x)âˆ’DÎ²â€‹u^â€‹(x)â€–â€‹dâ€‹xâ€‹dâ€‹t\displaystyle=\int\_{T\_{\text{$-$}}}^{T\_{\text{$+$}}}\int\_{\mathcal{D}}p\_{t,x\_{\text{$0$}}}(x)\sum\_{|\beta|\leq s}\big\|D^{\beta}u(x)-D^{\beta}\hat{u}(x)\big\|\mathrm{d}x\mathrm{d}t |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤âˆ«Tâˆ’T+(âˆ«ğ’Ÿpt,x0â€‹(x)râ€²â€‹dx)1/râ€²\displaystyle\leq\int\_{T\_{\text{$-$}}}^{T\_{\text{$+$}}}\bigg(\int\_{\mathcal{D}}p\_{t,x\_{\text{$0$}}}(x)^{r^{\text{$\prime$}}}\mathrm{d}x\bigg)^{1/r^{\text{$\prime$}}} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | Ã—(âˆ«ğ’Ÿâˆ‘|Î²|â‰¤sâ€–DÎ²â€‹uâ€‹(x)âˆ’DÎ²â€‹u^â€‹(x)â€–râ€‹dâ€‹x)1/râ€‹dâ€‹t\displaystyle\quad\times\Bigg(\int\_{\mathcal{D}}\sum\_{|\beta|\leq s}\big\|D^{\beta}u(x)-D^{\beta}\hat{u}(x)\big\|^{r}\mathrm{d}x\Bigg)^{1/r}\mathrm{d}t |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤âˆ«Tâˆ’T+(âˆ«ğ’ŸCr,T+râ€²â€‹(1+1t3â€‹d/2)râ€²â€‹â€–xâ€–âˆ’(râ€‹râ€²)â€‹dx)1/râ€²\displaystyle\leq\int\_{T\_{\text{$-$}}}^{T\_{\text{$+$}}}\bigg(\int\_{\mathcal{D}}C\_{r,\text{$T$}\_{\text{$+$}}}^{r^{\text{$\prime$}}}\bigg(1+\frac{1}{t^{3d/2}}\bigg)^{r^{\text{$\prime$}}}\|x\|^{\text{$-$}(rr^{\text{$\prime$}})}\mathrm{d}x\bigg)^{1/r^{\text{$\prime$}}} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | Ã—(âˆ«ğ’Ÿâˆ‘|Î²|â‰¤sâ€–DÎ²â€‹uâ€‹(x)âˆ’DÎ²â€‹u^â€‹(x)â€–râ€‹dâ€‹x)1/râ€‹dâ€‹t,\displaystyle\quad\times\Bigg(\int\_{\mathcal{D}}\sum\_{|\beta|\leq s}\big\|D^{\beta}u(x)-D^{\beta}\hat{u}(x)\big\|^{r}\mathrm{d}x\Bigg)^{1/r}\mathrm{d}t, |  |

where the second line follows by HÃ¶lderâ€™s inequality with 1r+1râ€²=1\tfrac{1}{r}+\tfrac{1}{r^{\prime}}=1 (since 1<r<âˆ1<r<\infty).
Now, the term

|  |  |  |
| --- | --- | --- |
|  | (âˆ«ğ’Ÿâˆ‘|Î²|â‰¤sâ€–DÎ²â€‹uâ€‹(x)âˆ’DÎ²â€‹u^â€‹(x)â€–râ€‹dâ€‹x)1/r,\Bigg(\int\_{\mathcal{D}}\sum\_{|\beta|\leq s}\big\|D^{\beta}u(x)-D^{\beta}\hat{u}(x)\big\|^{r}\mathrm{d}x\Bigg)^{1/r}, |  |

is precisely the WâŒŠsâŒ‹,râ€‹(ğ’Ÿ)W^{\lfloor s\rfloor,r}(\mathcal{D}) norm of (uâˆ’u^)(u-\hat{u}), which is bounded above by the Ws,râ€‹(ğ’Ÿ)W^{s,r}(\mathcal{D})-norm, which in turn is bounded above by Îµ\varepsilon, recallÂ ([B.3](https://arxiv.org/html/2511.01125v1#A2.E3 "Equation B.3 â€£ Appendix B Proof of stochastic results â€£ One model to solve them all: 2BSDE families via neural operators")). Hence

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â„™â€‹[âˆ«Tâˆ’T+âˆ‘|Î²|â‰¤sâ€–DÎ²â€‹uâ€‹(Xt)âˆ’DÎ²â€‹u^â€‹(Xt)â€–â€‹dâ€‹t]\displaystyle\mathbb{E}^{\mathbb{P}}\Bigg[\int\_{T\_{\text{$-$}}}^{T\_{\text{$+$}}}\sum\_{|\beta|\leq s}\big\|D^{\beta}u(X\_{t})-D^{\beta}\hat{u}(X\_{t})\big\|\mathrm{d}t\Bigg] | â‰¤Îµâ€‹âˆ«Tâˆ’T+(âˆ«ğ’ŸCr,T+râ€²â€‹(1+1t3â€‹d/2)râ€²â€‹â€–xâ€–âˆ’(râ€‹râ€²)â€‹dx)1/râ€²â€‹dt\displaystyle\leq\varepsilon\int\_{T\_{\text{$-$}}}^{T\_{\text{$+$}}}\bigg(\int\_{\mathcal{D}}C\_{r,T\_{\text{$+$}}}^{r^{\text{$\prime$}}}\bigg(1+\frac{1}{t^{3d/2}}\bigg)^{r^{\text{$\prime$}}}\,\|x\|^{\text{$-$}(rr^{\text{$\prime$}})}\mathrm{d}x\bigg)^{1/r^{\text{$\prime$}}}\mathrm{d}t |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤Cr,T+â€‹Îµâ€‹Volâ€‹(ğ’Ÿ)1/râ€²Î´ğ’Ÿrâ€‹âˆ«Tâˆ’T+(1+1t3â€‹d/2)â€‹dt\displaystyle\leq C\_{r,T\_{\text{$+$}}}\varepsilon\frac{\mathrm{Vol}(\mathcal{D})^{1/r^{\text{$\prime$}}}}{\delta\_{\text{$\mathcal{D}$}}^{r}}\int\_{T\_{\text{$-$}}}^{T\_{\text{$+$}}}\bigg(1+\frac{1}{t^{3d/2}}\bigg)\mathrm{d}t |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤Cr,T+â€‹Îµâ€‹Volâ€‹(ğ’Ÿ)1/râ€²Î´ğ’Ÿrâ€‹(T+âˆ’Tâˆ’+Tâˆ’1âˆ’3â€‹d/2âˆ’T+1âˆ’3â€‹d/23â€‹d/2âˆ’1)\displaystyle\leq C\_{r,T\_{\text{$+$}}}\varepsilon\frac{\mathrm{Vol}(\mathcal{D})^{1/r^{\text{$\prime$}}}}{\delta\_{\text{$\mathcal{D}$}}^{r}}\bigg(T\_{\text{$+$}}-T\_{\text{$-$}}+\frac{T\_{\text{$-$}}^{1-3d/2}-T\_{\text{$+$}}^{1-3d/2}}{3d/2-1}\bigg) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤Îµâ€‹Cp,T+,ğ’Ÿâ€‹(CT++1Tâˆ’3â€‹d/2âˆ’1)\displaystyle\leq\varepsilon{C}\_{p,T\_{\text{$+$}},\mathcal{D}}\bigg(C\_{T\_{\text{$+$}}}+\frac{1}{T\_{\text{$-$}}^{3d/2-1}}\bigg) |  |

where we used the assumption that dâ€‹(ğ’Ÿ,0)â‰¥Î´ğ’Ÿ>0d(\mathcal{D},0)\geq\delta\_{\text{$\mathcal{D}$}}>0 and a simple supremum-bound, and where we defined

|  |  |  |
| --- | --- | --- |
|  | Cp,T+,ğ’Ÿâ‰”Cp,T+â€‹2â€‹Vâ€‹oâ€‹lâ€‹(ğ’Ÿ)1/râ€²(3â€‹dâˆ’2)â€‹Î´ğ’Ÿr,andâ€‹CT+â‰”(3â€‹d2âˆ’1)â€‹T+.{C}\_{p,T\_{\text{$+$}},\mathcal{D}}\coloneqq C\_{p,T\_{\text{$+$}}}\frac{2\mathrm{Vol}(\mathcal{D})^{1/r^{\text{$\prime$}}}}{(3d-2)\delta\_{\text{$\mathcal{D}$}}^{r}},\;\text{\rm and}\;C\_{T\_{\text{$+$}}}\coloneqq\bigg(\frac{3d}{2}-1\bigg)T\_{\text{$+$}}. |  |

âˆ

## Appendix C Experimental details

### C.1 Periodic semi-linear case

We consider a periodic example from [[13](https://arxiv.org/html/2511.01125v1#bib.bib13)] in d=5d=5 dimension, with T=1T=1, in which the forward SDE is given by

|  |  |  |  |
| --- | --- | --- | --- |
|  | dâ€‹Xt(i)\displaystyle\mathrm{d}X\_{t}^{(i)} | =biâ€‹(Xt(i))â€‹dâ€‹t+Ïƒi,iâ€‹(Xt(i))â€‹dâ€‹Wt(i),iâˆˆ{1,â€¦,d},\displaystyle=b\_{i}\!\big(X\_{t}^{(i)}\big)\,\mathrm{d}t+\sigma\_{i,i}\!\big(X\_{t}^{(i)}\big)\,\mathrm{d}W\_{t}^{(i)},\;i\in\{1,\dots,d\}, |  |

and the coefficients of the SDE are given by

|  |  |  |  |
| --- | --- | --- | --- |
|  | biâ€‹(x)\displaystyle b\_{i}(x) | â‰”0.2â€‹sinâ¡(2â€‹Ï€â€‹xi),Ïƒi,jâ€‹(x)â‰”1dâ€‹Ï€â€‹(0.25+0.1â€‹cosâ¡(2â€‹Ï€â€‹xi))â€‹â€‰1{i=j},(i,j)âˆˆ{1,â€¦,d}2.\displaystyle\coloneqq 0.2\,\sin(2\pi x\_{i}),\;\sigma\_{i,j}(x)\coloneqq\frac{1}{\sqrt{d}\,\pi}\Big(0.25+0.1\cos(2\pi x\_{i})\Big)\,\mathbf{1}\_{\{i=j\}},\;(i,j)\in\{1,\dots,d\}^{2}. |  |

The coefficients of the backward SDE

|  |  |  |  |
| --- | --- | --- | --- |
|  | dâ€‹Yt\displaystyle\mathrm{d}Y\_{t} | =âˆ’fâ€‹(t,Xt,Yt,Zt)â€‹dâ€‹t+Ztâ‹…dâ€‹Wt,YT=gâ€‹(XT),\displaystyle=-\,f\!\big(t,X\_{t},Y\_{t},Z\_{t}\big)\,\mathrm{d}t\;+\;Z\_{t}\cdot\mathrm{d}W\_{t},\;Y\_{T}=g\!\big(X\_{T}\big), |  |

are given by

|  |  |  |
| --- | --- | --- |
|  | gâ€‹(x)â‰”1Ï€â€‹(sinâ¡(2â€‹Ï€â€‹âˆ‘i=1dxi)+cosâ¡(2â€‹Ï€â€‹âˆ‘i=1dxi)),\displaystyle g(x)\coloneqq\frac{1}{\pi}\Bigg(\sin\bigg(2\pi\sum\_{i=1}^{d}x\_{i}\bigg)+\cos\bigg(2\pi\sum\_{i=1}^{d}x\_{i}\bigg)\Bigg), |  |
|  |  |  |
| --- | --- | --- |
|  | fâ€‹(t,x,y,z)â‰”2â€‹Ï€2â€‹yâ€‹âˆ‘i=1dÏƒi,iâ€‹(x)2âˆ’âˆ‘i=1dbiâ€‹(x)Ïƒi,iâ€‹(x)â€‹zi+hâ€‹(t,x),\displaystyle f(t,x,y,z)\coloneqq 2\pi^{2}y\sum\_{i=1}^{d}\sigma\_{i,i}(x)^{2}-\sum\_{i=1}^{d}\frac{b\_{i}(x)}{\sigma\_{i,i}(x)}z\_{i}+h(t,x), |  |

where

|  |  |  |
| --- | --- | --- |
|  | hâ€‹(t,x)â‰”2â€‹(cosâ¡(2â€‹Ï€â€‹âˆ‘i=1dxi+2â€‹Ï€â€‹(Tâˆ’t))âˆ’sinâ¡(2â€‹Ï€â€‹âˆ‘i=1dxi+2â€‹Ï€â€‹(Tâˆ’t))).h(t,x)\coloneqq 2\Bigg(\cos\bigg(2\pi\sum\_{i=1}^{d}x\_{i}+2\pi(T-t)\bigg)-\sin\bigg(2\pi\sum\_{i=1}^{d}x\_{i}+2\pi(T-t)\bigg)\Bigg). |  |

The explicit solution uu is given by

|  |  |  |
| --- | --- | --- |
|  | uâ€‹(t,x)=1Ï€â€‹(sinâ¡(Î¸â€‹(t,x))+cosâ¡(Î¸â€‹(t,x))),u(t,x)=\frac{1}{\pi}\big(\sin(\theta(t,x))+\cos(\theta(t,x))\big), |  |

where

|  |  |  |
| --- | --- | --- |
|  | Î¸â€‹(t,x)â‰”2â€‹Ï€â€‹(âˆ‘i=1dxi+(Tâˆ’t)).\theta(t,x)\coloneqq 2\pi\Bigg(\sum\_{i=1}^{d}x\_{i}+(T-t)\Bigg). |  |

The spatial derivatives of uu are given by

|  |  |  |
| --- | --- | --- |
|  | âˆ‚uâˆ‚xiâ€‹(t,x)=2â€‹(cosâ¡(Î¸â€‹(t,x))âˆ’sinâ¡(Î¸â€‹(t,x))),iâˆˆ{1,â€¦,d},\frac{\partial u}{\partial x\_{i}}(t,x)=2\big(\cos(\theta(t,x))-\sin(\theta(t,x))\big),\;i\in\{1,\dots,d\}, |  |

and

|  |  |  |
| --- | --- | --- |
|  | âˆ‚2uâˆ‚xiâ€‹âˆ‚xjâ€‹(t,x)=âˆ’4â€‹Ï€â€‹(sinâ¡(Î¸â€‹(t,x))+cosâ¡(Î¸â€‹(t,x))),(i,j)âˆˆ{1,â€¦,d}2.\frac{\partial^{2}u}{\partial x\_{i}\partial x\_{j}}(t,x)=-4\pi\big(\sin(\theta(t,x))+\cos(\theta(t,x))\big),\;(i,j)\in\{1,\dots,d\}^{2}. |  |

### C.2 Linearâ€“quadratic (LQ) case

We consider a linearâ€“quadratic case from [[78](https://arxiv.org/html/2511.01125v1#bib.bib78)] in d=5d=5 dimension, with T=1T=1. The forward SDE is a controlled process XtX\_{t} in â„d\mathbb{R}^{d}, defined by

|  |  |  |
| --- | --- | --- |
|  | dâ€‹Xt=(Aâ€‹Xt+Bâ€‹Î±t)â€‹dâ€‹t+Dâ€‹Î±tâ€‹dâ€‹Wt,\mathrm{d}X\_{t}=(AX\_{t}+B\alpha\_{t})\mathrm{d}t+D\alpha\_{t}\mathrm{d}W\_{t}, |  |

where Î±t\alpha\_{t} is a control process in â„\mathbb{R}, (B,D)âˆˆâ„dÃ—â„d(B,D)\in\mathbb{R}^{d}\times\mathbb{R}^{d} and Aâˆˆâ„dÃ—dA\in\mathbb{R}^{d\times d}. The quadratic cost that is minimised is

|  |  |  |
| --- | --- | --- |
|  | Jâ€‹(Î±)â‰”ğ”¼â€‹bâ€‹iâ€‹gâ€‹gâ€‹[âˆ«0T(XtâŠ¤â€‹Qâ€‹Xt+Î±t2â€‹N)â€‹dt+XTâŠ¤â€‹Pâ€‹XT],J(\alpha)\coloneqq\mathbb{E}\\ bigg[\int\_{0}^{T}\big(X\_{t}^{\top}QX\_{t}+\alpha\_{t}^{2}N\big)\mathrm{d}t+X\_{T}^{\top}PX\_{T}\bigg], |  |

where PP and QQ are non-negative, symmetric dÃ—dd\times d matrices and N>0N>0.

The Bellman PDE associated with this process admits an explicit solution given by a quadratic form

|  |  |  |
| --- | --- | --- |
|  | uâ€‹(t,x)=xTâ€‹Kâ€‹(t)â€‹x,u(t,x)=x^{T}K(t)x, |  |

where Kâ€‹(t)K(t) solves the Ricatti equation

|  |  |  |
| --- | --- | --- |
|  | KË™+AâŠ¤â€‹K+Kâ€‹A+Qâˆ’Kâ€‹Bâ€‹BâŠ¤â€‹KN+DâŠ¤â€‹Kâ€‹D=0,Kâ€‹(T)=P.\dot{K}+A^{\top}K+KA+Q-\frac{KBB^{\top}K}{N+D^{\top}KD}=0,\;K(T)=P. |  |

In all the simulations, we set

|  |  |  |
| --- | --- | --- |
|  | A=Id,B=D=Id,Q=P=1dâ€‹Id,N=d.A=\mathrm{I}\_{d},\;B=D=\mathrm{I}\_{d},\;Q=P=\frac{1}{d}\mathrm{I}\_{d},\;N=d. |  |

The stochastic coefficients associated to the controlled process are set to

|  |  |  |
| --- | --- | --- |
|  | Ïƒ=1dâ€‹Id,andâ€‹Î¼â€‹(t,x)=x.\sigma=\frac{1}{\sqrt{d}}\mathrm{I}\_{d},\;\textnormal{and}\;\mu(t,x)=x. |  |

In our isotropic setup, the Riccati matrix remains proportional to the identity, *i.e.*

|  |  |  |
| --- | --- | --- |
|  | Kâ€‹(t)=kâ€‹(t)â€‹Id.K(t)=k(t)\mathrm{I}\_{d}. |  |

Then, the explicit forms of the spatial derivatives of uu are given by

|  |  |  |
| --- | --- | --- |
|  | âˆ‡xuâ€‹(t,x)=2â€‹Kâ€‹(t)â€‹x=2â€‹kâ€‹(t)â€‹x,Dx2â€‹uâ€‹(t,x)=2â€‹Kâ€‹(t)=2â€‹kâ€‹(t)â€‹Id.\nabla\_{x}u(t,x)=2K(t)x=2k(t)x,\;D\_{x}^{2}u(t,x)=2K(t)=2k(t)\mathrm{I}\_{d}. |  |

To compute the solution uu and its derivatives, we employ a fourth-order Runge-â€“Kutta (RK4) scheme to numerically approximate Kâ€‹(t)K(t) (the solution of the Riccati equation).

### C.3 Architectural details

The KANO architecture follows a lift-â€“processâ€“=project design. The input features are first lifted to a higher-dimensional latent space using a feed-forward network, producing an initial latent representation v(0)v^{(0)}.

After lifting, a composition of several KANO blocks is applied to iteratively refine this latent field:

|  |  |  |
| --- | --- | --- |
|  | v(â„“+1)=Î¦(â„“)â€‹(v(â„“),x),â„“âˆˆ{0,â€¦,Lâˆ’1},v^{(\ell+1)}=\Phi^{(\ell)}(v^{(\ell)},x),\;\ell\in\{0,\dots,L-1\}, |  |

where each block Î¦(â„“)\Phi^{(\ell)} performs a structured operator update combining coordinate encoding, spectral convolution, and residual connection. Each KANO block consists of three main components

1. 1.

   a positional encoder maps the spatial coordinates through a Resâ€“KAN network, producing coordinate-dependent features

   |  |  |  |
   | --- | --- | --- |
   |  | vpos=bâ€‹(x);v\_{\mathrm{pos}}=b(x); |  |
2. 2.

   a spectral kernel path performs a spectral convolution in the frequency domain, analogous to the Fourier neural operator (FNO) [[60](https://arxiv.org/html/2511.01125v1#bib.bib60)]. Specifically, the feature field is transformed via a two-dimensional fast Fourier transform (FFT), filtered by learnable complex-valued multipliers, and then mapped back to the spatial domain

   |  |  |  |
   | --- | --- | --- |
   |  | vkfâ€‹(x)=â„±âˆ’1â€‹(W^â€‹(k)â€‹â„±â€‹[vin]â€‹(k)),v\_{\mathrm{kf}}(x)=\mathcal{F}^{-1}\big(\hat{W}(k)\mathcal{F}[v\_{\mathrm{in}}](k)\big), |  |

   where â„±\mathcal{F} and â„±âˆ’1\mathcal{F}^{-1} denote the forward and inverse Fourier transforms, and W^â€‹(k)\hat{W}(k) are learnable complex weights restricted to a finite number of Fourier modes and parametrised as Resâ€“KANs;
3. 3.

   a residual path applies a Resâ€“KAN transformation on the tensor obtained by concatenating (vpos,vkf,vin)\big(v\_{\mathrm{pos}},v\_{\mathrm{kf}},v\_{\mathrm{in}}\big).

After stacking LL such KANO blocks, the resulting field v(L)v^{(L)} is projected back to the target dimension through a final projection layer. This composition enables multiscale feature extraction, efficient global coupling through spectral convolution, and local adaptivity through Resâ€“KAN-based non-linear transformations.

We restrict our training to a 22D uniform grid that spans the first two coordinates of the dd-dimensional space, while conditioning the model pointwise on the remaining dâˆ’2d-2 coordinates. The procedure for generating random training samples is described in detail in [SectionËœC.4](https://arxiv.org/html/2511.01125v1#A3.SS4 "C.4 Training pipeline â€£ Appendix C Experimental details â€£ One model to solve them all: 2BSDE families via neural operators"). Our model is trained to approximate 2D slices of the solution along the (x1,x2)(x\_{1},x\_{2})-coordinates in â„+Ã—â„d\mathbb{R\_{+}}\times\mathbb{R}^{d}. Once trained, the model can be evaluated at any point in time and space by approximating the solution over these 2D slices and querying the corresponding (x1,x2)(x\_{1},x\_{2}) values (see [SectionËœC.5](https://arxiv.org/html/2511.01125v1#A3.SS5 "C.5 Inference pipeline â€£ Appendix C Experimental details â€£ One model to solve them all: 2BSDE families via neural operators") for details). This type of restricted operator learning is efficient due to the following reasons.

* â€¢

  Uniform grids enable efficient kernels. During training, the coordinates (x1,x2)(x\_{1},x\_{2}) are placed on a uniform grid, enabling convolution-like kernel layers to be computed efficiently via FFTs. This reduces the per-layer complexity from dense Oâ€‹(s4)O(s^{4}) to Oâ€‹(s2â€‹logâ¡s)O(s^{2}\log s), making spectral kernels both computationally efficient and numerically stable.
* â€¢

  Learning high-dimensional maps through 2D evaluations.
  The operator is evaluated over the full s2s^{2} grid simultaneously, while the remaining coordinates (x3,â€¦,xd)(x\_{3},\dots,x\_{d}) and time tt are provided as additional input channels. This setup allows the network to capture intrinsic symmetries in the problem and to perform restricted operator learning, approximating uâ€‹(t,x)u(t,x) across â„d\mathbb{R}^{d} by predicting values at multiple 22D locations in parallel.
* â€¢

  2D offers the optimal balance; 3D becomes costly.
  Extending the FFT-based grid to three dimensions increases computational and memory demands to Oâ€‹(s3â€‹logâ¡s)O(s^{3}\log s) per pass and substantially raises activation and storage costs. In practice, 22D grids strike the best balance between expressivity (capturing many spatial query points per sample) and efficiency, while still encoding dd-dimensional dependencies through the auxiliary input channels.

Note that spectral convolution on uniform grids is employed to improve the training efficiency of the model. In operator learning settings, various efficient kernel architectures exist, see [Kovachki, Li, Liu, Azizzadenesheli, Bhattacharya, Stuart, and Anandkumar](https://arxiv.org/html/2511.01125v1#bib.bib47) [[47](https://arxiv.org/html/2511.01125v1#bib.bib47)], including convolution-based kernels, see [RaoniÄ‡, Molinaro, deÂ Ryck, Rohner, Bartolucci, Alaifari, Mishra, and deÂ BÃ©zenac](https://arxiv.org/html/2511.01125v1#bib.bib81) [[81](https://arxiv.org/html/2511.01125v1#bib.bib81)], wavelet-based kernels, see [Tripura and Chakraborty](https://arxiv.org/html/2511.01125v1#bib.bib91) [[91](https://arxiv.org/html/2511.01125v1#bib.bib91)], and transformer-based kernels, see [Herde, RaoniÄ‡, Rohner, KÃ¤ppeli, Molinaro, deÂ BÃ©zenac, and Mishra](https://arxiv.org/html/2511.01125v1#bib.bib40) [[40](https://arxiv.org/html/2511.01125v1#bib.bib40)] or [Li, Meidani, and Farimani](https://arxiv.org/html/2511.01125v1#bib.bib61) [[61](https://arxiv.org/html/2511.01125v1#bib.bib61)], among others. The choice of the spectral kernel here is made solely to demonstrate that training a neural operator in the 2BSDE setting is feasible.

### C.4 Training pipeline

In all our experiments, we draw samples from the domain uniformly. To draw a random training sample, we first draw a random time, as well as random locations for the dâˆ’2d-2 dimensions (the first 2 dimensions (xâ€‹1,xâ€‹2)(x1,x2) are already sampled on uniform grids),

|  |  |  |
| --- | --- | --- |
|  | tâˆˆ[0,T],c=(x3,â€¦,xd)âˆˆ[0,1)dâˆ’2.t\in[0,T],\;c=(x\_{3},\dots,x\_{d})\in[0,1)^{d-2}. |  |

To get the training samples, we evaluate the model on a *uniform* sÃ—ss\times s grid for the first two coordinates

|  |  |  |
| --- | --- | --- |
|  | ğ’¢â‰”{(x1p,x2q):x1p=psâˆ’1,x2q=qsâˆ’1,(p,q)âˆˆ{0,â€¦,sâˆ’1}2},\mathcal{G}\coloneqq\bigg\{(x\_{1}^{p},x\_{2}^{q}):x\_{1}^{p}=\frac{p}{s-1},\;x\_{2}^{q}=\frac{q}{s-1},\;(p,q)\in\{0,\dots,s-1\}^{2}\bigg\}, |  |

and denote Nâ‰”s2N\coloneqq s^{2} and Xâ‰”((x1â€‹n,x2â€‹n))nâˆˆ{1,â€¦,N}X\coloneqq\big((x\_{1n},x\_{2n})\big)\_{n\in\{1,\dots,N\}} the grid.

At each grid node nn, the model receives the feature vector

|  |  |  |
| --- | --- | --- |
|  | Ï•nâ‰”(t,X,x3,â€¦,xd)âˆˆâ„1+2+(dâˆ’2)=â„d+1,\phi\_{n}\coloneqq\big(t,\,X,\,x\_{3},\dots,x\_{d}\big)\in\mathbb{R}^{1+2+(d-2)}=\mathbb{R}^{d+1}, |  |

*i.e.* time and the (dâˆ’2)(d-2) extra coordinates are *channels* constant across the 2d grid. A neural operator FÎ¸F\_{\theta} maps these inputs to the â„sÃ—s\mathbb{R}^{s\times s} field,

|  |  |  |
| --- | --- | --- |
|  | u^Î¸â€‹(t,X,x3,â€¦,xd)=FÎ¸â€‹(Ï•n)âˆˆâ„sÃ—s.\hat{u}\_{\theta}\big(t,X,x\_{3},\dots,x\_{d}\big)=F\_{\theta}\big(\phi\_{n}\big)\in\mathbb{R}^{s\times s}. |  |

### C.5 Inference pipeline

At test time, the learned approximation u^Î¸\hat{u}\_{\theta} can be evaluated at any query (t,x)(t,x) in the domain by either of the following.

* â€¢

  *Spectral/Fourier synthesis.*
  If the decoder is spectral, we evaluate the Fourierâ€“like synthesis operator at the desired coordinates to obtain u^Î¸â€‹(t,x)\hat{u}\_{\theta}(t,x) directly.
  This is naturally suited to periodic problems and preserves differentiability with respect to (t,x)(t,x), enabling gradients to be obtained by automatic differentiation.
* â€¢

  *Grid interpolation.*
  When the model outputs values on a uniform sÃ—ss\times s grid in (x1,x2)(x\_{1},x\_{2}) at a given time tt, we interpolate that grid to any (x1,x2)(x\_{1},x\_{2}) in the domain (*e.g.* bilinear/bicubic interpolation).
  This route is simple, fast, and it requires no change to the trained model.

To evaluate the models along random paths, we generate dd-dimensional SDE trajectories using the Eulerâ€“Maruyama scheme,

|  |  |  |
| --- | --- | --- |
|  | Xn+1(i)=Xn(i)+biâ€‹(Xn(i))â€‹Î”â€‹t+Ïƒi,iâ€‹(Xn(i))â€‹Î”â€‹tâ€‹Î¾n(i),Î¾n(i)âˆ¼ğ’©â€‹(0,1).X\_{n+1}^{(i)}=X\_{n}^{(i)}+b\_{i}(X\_{n}^{(i)})\Delta t+\sigma\_{i,i}(X\_{n}^{(i)})\sqrt{\Delta t}\xi\_{n}^{(i)},\;\xi\_{n}^{(i)}\sim\mathcal{N}(0,1). |  |

The trained model is then evaluated along these trajectories, and its predictions are compared against the exact solution uu and its first- and second-order partial derivatives. Derivatives of the neural operator are approximated using first-order finite difference scheme. To obtain model outputs at arbitrary spatial locations, we employ bilinear interpolation over the (x1,x2)(x\_{1},x\_{2}) grid.

## References

* Acciaio etÂ al. [2024]

  B.Â Acciaio, A.Â Kratsios, and G.Â Pammer.
  Designing universal causal deep learning models: the geometric (hyper) transformer.
  *Mathematical Finance*, 34(2):671â€“735, 2024.
* Adcock etÂ al. [2022]

  B.Â Adcock, S.Â Brugiapaglia, N.Â Dexter, and S.Â Moraga.
  On efficient algorithms for computing near-best polynomial approximations to high-dimensional, Hilbert-valued functions from limited samples.
  *ArXiv preprint arXiv:2203.13908*, 2022.
* Adcock etÂ al. [2024]

  B.Â Adcock, N.Â Dexter, and S.Â MoragaÂ Scheuermann.
  Optimal deep learning of holomorphic operators between banach spaces.
  In A.Â Globerson, L.Â Mackey, D.Â Belgrave, A.Â Fan, U.Â Paquet, J.Â Tomczak, and C.Â Zhang, editors, *Proceedings of the 38th conference on advances in neural information processing systems ((NeurIPS 2024)), December 10â€“15, 2024, Vancouver, British Columbia, Canada*, volumeÂ 37, pages 27725â€“27789, 2024.
* Adcock etÂ al. [2025]

  B.Â Adcock, S.Â Brugiapaglia, N.Â Dexter, and S.Â Moraga.
  Near-optimal learning of Banach-valued, high-dimensional functions via deep neural networks.
  *Neural Networks*, 181(106761):1â€“25, 2025.
* Alvarez etÂ al. [2024]

  G.Â Alvarez, I.Â Ekren, A.Â Kratsios, and X.Â Yang.
  Neural operators can play dynamic Stackelberg games.
  *ArXiv preprint arXiv:2411.09644*, 2024.
* Arabpour etÂ al. [2024]

  R.Â Arabpour, J.Â Armstrong, L.Â Galimberti, A.Â Kratsios, and G.Â Livieri.
  Low-dimensional approximations of the conditional law of Volterra processes: a non-positive curvature approach.
  *ArXiv preprint arXiv:2405.20094*, 2024.
* Beck etÂ al. [2019]

  C.Â Beck, W.Â E, and A.Â Jentzen.
  Machine learning approximation algorithms for high-dimensional fully nonlinear partial differential equations and second-order backward stochastic differential equations.
  *Journal of Nonlinear Science*, 29(4):1563â€“1619, 2019.
* Benth etÂ al. [2023]

  F.Â E. Benth, N.Â Detering, and L.Â Galimberti.
  Neural networks in FrÃ©chet spaces.
  *Annals of Mathematics and Artificial Intelligence*, 91(1):75â€“103, 2023.
* Bilokopytov and Xanthos [2025]

  E.Â Bilokopytov and F.Â Xanthos.
  A universal approximation theorem and its applications to vector lattice theory.
  *ArXiv preprint arXiv:2507.20219*, 2025.
* Bolcskei etÂ al. [2019]

  Helmut Bolcskei, Philipp Grohs, Gitta Kutyniok, and Philipp Petersen.
  Optimal approximation with sparsely connected deep neural networks.
  *SIAM Journal on Mathematics of Data Science*, 1(1):8â€“45, 2019.
* Cao and Wan [2022]

  D.Â Cao and J.Â Wan.
  Expansion of Greenâ€™s function and regularity of Robinâ€™s function for elliptic operators in divergence form.
  *Annali della Scuola Normale Superiore di Pisa - Classe di Scienze*, to appear, 2022.
* Chan etÂ al. [2015]

  T.-H. Chan, K.Â Jia, S.Â Gao, J.Â Lu, Z.Â Zeng, and Y.Â Ma.
  PCANet: simple deep learning baseline for image classification?
  *IEEE Transactions on Image Processing*, 24(12):5017â€“5032, 2015.
* Chassagneux etÂ al. [2023]

  J.-F. Chassagneux, J.Â Chen, N.Â Frikha, and C.Â Zhou.
  A learning scheme by sparse grids and Picard approximations for semilinear parabolic pdes.
  *IMA Journal of Numerical Analysis*, 43(5):3109â€“3168, 2023.
* Chen and Chen [1993]

  T.Â Chen and H.Â Chen.
  Approximations of continuous functionals by neural networks with application to dynamic systems.
  *IEEE Transactions on Neural Networks*, 4(6):910â€“918, 1993.
* Chen and Chen [1995]

  T.Â Chen and H.Â Chen.
  Universal approximation to nonlinear operators by neural networks with arbitrary activation functions and its application to dynamical systems.
  *IEEE Transactions on Neural Networks*, 6(4):911â€“917, 1995.
* Cheridito etÂ al. [2007]

  P.Â Cheridito, H.Â M. Soner, N.Â Touzi, and N.Â Victoir.
  Second-order backward stochastic differential equations and fully nonlinear parabolic PDEs.
  *Communications on Pure and Applied Mathematics*, 60(7):1081â€“1110, 2007.
* Cuchiero etÂ al. [2023]

  C.Â Cuchiero, P.Â Schmocker, and J.Â Teichmann.
  Global universal approximation of functional input maps on weighted spaces.
  *ArXiv preprint arXiv:2306.03303*, 2023.
* Daubechies [1988]

  I.Â Daubechies.
  Orthonormal bases of compactly supported wavelets.
  *Communications on Pure and Applied Mathematics*, 41(7):909â€“996, 1988.
* Daubechies [1992]

  I.Â Daubechies.
  *Ten lectures on wavelets*, volumeÂ 61 of *CBMSâ€“NSF regional conference series in applied mathematics*.
  Society for Industrial and Applied Mathematics, Philadelphia, Pennsylvania, 1992.
* deÂ Hoop etÂ al. [2022]

  M.Â V. deÂ Hoop, M.Â Lassas, and C.Â A. Wong.
  Deep learning architectures for nonlinear operator functions and nonlinear inverse problems.
  *Mathematical Statistics and Learning*, 4(1):1â€“86, 2022.
* deÂ Marco [2011]

  S.Â deÂ Marco.
  Smoothness and asymptotic estimates of densities for SDEs with locally smooth coefficients and applications to square rootâ€“type diffusions.
  *The Annals of Applied Probability*, 21(4):1282â€“1321, 2011.
* DeVore and Sharpley [1993]

  R.Â A. DeVore and R.Â C. Sharpley.
  Besov spaces on domains in â„ğ••\mathbb{{R}^{d}}.
  *Transactions of the American Mathematical Society*, 335(2):843â€“864, 1993.
* DeVore etÂ al. [2021]

  Ronald DeVore, Boris Hanin, and Guergana Petrova.
  Neural network approximation.
  *Acta Numerica*, 30:327â€“444, 2021.
* Duong [2023]

  H.Â Duong.
  *Solving high-dimensional fully nonlinear convex partial differential equations using deep learning*.
  PhD thesis, Florida State University, 2023.
* E and Wang [2018]

  W.Â E and Q.Â Wang.
  Exponential convergence of the deep neural network approximation for analytic functions.
  *Science China Mathematics*, 61(10):1733â€“1740, 2018.
* Evans [2010]

  L.Â C. Evans.
  *Partial differential equations*, volumeÂ 19 of *Graduate studies in mathematics*.
  American Mathematical Society, 2nd edition, 2010.
* Fefferman [2006]

  C.Â Fefferman.
  Whitneyâ€™s extension problem for Cm{C}^{m}.
  *Annals of Mathematics*, 164:313â€“359, 2006.
* Fefferman etÂ al. [2014]

  C.Â Fefferman, A.Â Israel, and G.Â Luli.
  Sobolev extension by linear operators.
  *Journal of the American Mathematical Society*, 27(1):69â€“145, 2014.
* Firoozi etÂ al. [2025]

  D.Â Firoozi, A.Â Kratsios, and X.Â Yang.
  Simultaneously solving infinitely many LQ mean field games in Hilbert spaces: the power of neural operators.
  *ArXiv preprint arXiv:2510.20017*, 2025.
* Firouzi etÂ al. [2025]

  D.Â Firouzi, X.Â Yang, and A.Â Kratsios.
  Simultaneously solving infinitely many LQ mean field games in Hilbert spaces: the power of neural operators.
  *In preparation*, 2025.
* Furuya and Kratsios [2024]

  T.Â Furuya and A.Â Kratsios.
  Simultaneously solving FBSDEs with neural operators of logarithmic depth, constant width, and sub-linear rank.
  *ArXiv preprint arXiv:2410.14788*, 2024.
* Furuya etÂ al. [2025]

  T.Â Furuya, K.Â Taniguchi, and S.Â Okuda.
  Quantitative approximation for neural operators in nonlinear parabolic equations.
  In *The thirteenth international conference on learning representations ((ICLR 2025)), April 24â€“28, 2025, Singapore*, pages 1â€“29, 2025.
* Galimberti etÂ al. [2025]

  L.Â Galimberti, A.Â Kratsios, and G.Â Livieri.
  Designing universal causal deep learning models: the case of infinite-dimensional dynamical systems from stochastic analysis.
  *Constructive Approximation*, to appear, 2025.
* Germain etÂ al. [2022a]

  M.Â Germain, M.Â LauriÃ¨re, H.Â Pham, and X.Â Warin.
  DeepSets and their derivative networks for solving symmetric PDEs.
  *Journal of Scientific Computing*, 91(63):1â€“33, 2022a.
* Germain etÂ al. [2022b]

  M.Â Germain, H.Â Pham, and X.Â Warin.
  Approximation error analysis of some deep backward schemes for nonlinear PDEs.
  *SIAM Journal on Scientific Computing*, 44(1):A28â€“A56, 2022b.
* Germain etÂ al. [2023]

  M.Â Germain, H.Â Pham, and X.Â Warin.
  Neural networksâ€“based algorithms for stochastic control and PDEs in finance.
  In A.Â Capponi and C.-A. Lehalle, editors, *Machine learning and data sciences for financial markets*, pages 426â€“452. Cambridge University Press, 2023.
* Gilbarg and Trudinger [2001]

  D.Â Gilbarg and N.Â S. Trudinger.
  *Elliptic partial differential equations of second order*, volume 224 of *Classics in mathematics*.
  Springer Berlin, Heidelberg, second edition, 2001.
* GÃ¶deke and Fernsel [2025]

  J.Â GÃ¶deke and P.Â Fernsel.
  New universal operator approximation theorem for encoderâ€“decoder architectures.
  *ArXiv preprint arXiv:2503.24092*, 2025.
* Gribonval etÂ al. [2022]

  R.Â Gribonval, G.Â Kutyniok, M.Â Nielsen, and F.Â Voigtlaender.
  Approximation spaces of deep neural networks.
  *Constructive Approximation*, 55(1):259â€“367, 2022.
* Herde etÂ al. [2024]

  Maximilian Herde, B.Â RaoniÄ‡, T.Â Rohner, R.Â KÃ¤ppeli, R.Â Molinaro, E.Â deÂ BÃ©zenac, and S.Â Mishra.
  Poseidon: efficient foundation models for PDEs.
  In *Proceedings of the 38th conference on advances in neural information processing systems ((NeurIPS 2024)), December 10â€“15, 2024, Vancouver, British Columbia, Canada*, volumeÂ 37, pages 72525â€“72624, 2024.
* Hong and Kratsios [2024]

  Ruiyang Hong and Anastasis Kratsios.
  Bridging the gap between approximation and learning via optimal approximation by relu mlps of maximal regularity.
  *arXiv preprint arXiv:2409.12335*, 2024.
* Horvath etÂ al. [2023]

  B.Â Horvath, A.Â Kratsios, Y.Â Limmer, and X.Â Yang.
  Deep Kalman filters can filter.
  *SSRN preprint 4615215*, 2023.
* Horvath etÂ al. [2025]

  B.Â Horvath, A.Â Kratsios, Y.Â Limmer, and X.Â Yang.
  Transformers can solve non-linear and non-Markovian filtering problems in continuous time for conditionally Gaussian signals.
  *ArXiv preprint arXiv:2310.19603*, 2025.
* Hu and LauriÃ¨re [2024]

  R.Â Hu and N.Â LauriÃ¨re.
  Recent developments in machine learning methods for stochastic control and games.
  *Numerical Algebra, Control and Optimization*, 14(3):435â€“525, 2024.
* Kim and Sakellaris [2019]

  S.Â Kim and G.Â Sakellaris.
  Greenâ€™s function for second order elliptic equations with singular lower order coefficients.
  *Communications in Partial Differential Equations*, 44(3):228â€“270, 2019.
* Korolev [2022]

  Y.Â Korolev.
  Two-layer neural networks with values in a Banach space.
  *SIAM Journal on Mathematical Analysis*, 54(6):6358â€“6389, 2022.
* Kovachki etÂ al. [2023]

  N.Â B. Kovachki, Z.Â Li, B.Â Liu, K.Â Azizzadenesheli, K.Â Bhattacharya, A.Â M. Stuart, and A.Â Anandkumar.
  Neural operator: learning maps between function spaces with applications to PDEs.
  *Journal of Machine Learning Research*, 24(89):1â€“97, 2023.
* Kratsios and Furuya [2025]

  A.Â Kratsios and T.Â Furuya.
  Kolmogorovâ€“Arnold networks: approximation and learning guarantees for functions and their derivatives.
  *ArXiv preprint arXiv:2504.15110*, 2025.
* Kratsios etÂ al. [2023]

  A.Â Kratsios, C.Â Liu, M.Â Lassas, M.Â V. deÂ Hoop, and I.Â Dokmanic.
  Universal geometric deep learning via geometric attention.
  *ArXiv preprint arXiv:2304.12231*, 2023.
* Kratsios etÂ al. [2024]

  A.Â Kratsios, T.Â Furuya, J.Â A.Â L. Benitez, M.Â Lassas, and M.Â deÂ Hoop.
  Mixture of experts soften the curse of dimensionality in operator learning.
  *ArXiv preprint arXiv:2404.09101*, 2024.
* Kratsios etÂ al. [2025a]

  A.Â Kratsios, A.Â Neufeld, and P.Â Schmocker.
  Generative neural operators of log-complexity can simultaneously solve infinitely many convex programs.
  *ArXiv preprint arXiv:2508.14995*, 2025a.
* Kratsios etÂ al. [2025b]

  A.Â Kratsios, P.Â Schmocker, and P.Â Zimmermann.
  Deep inverse problem for double phase equation.
  *In preparation*, 2025b.
* Kratsios and Zamanlooy [2022]

  K.Â Kratsios and B.Â Zamanlooy.
  Do ReLU networks have an edge when approximating compactly-supported functions?
  *Transactions on Machine Learning Research*, August:1â€“22, 2022.
* Krylov [2018]

  N.Â V. Krylov.
  *Sobolev and viscosity solutions for fully nonlinear elliptic and parabolic equations*, volume 233 of *Mathematical surveys and monographs*.
  American Mathematical Society, Providence, Rhode Island, 2018.
* Lanthaler and Stuart [2025]

  S.Â Lanthaler and A.Â M. Stuart.
  The parametric complexity of operator learning.
  *IMA Journal of Numerical Analysis*, to appear, 2025.
* Lanthaler etÂ al. [2022]

  S.Â Lanthaler, S.Â Mishra, and G.Â E. Karniadakis.
  Error estimates for DeepONets: a deep learning framework in infinite dimensions.
  *Transactions of Mathematics and Its Applications*, 6(1):1â€“141, 2022.
* Lanthaler etÂ al. [2025]

  S.Â Lanthaler, Z.Â Li, and A.Â M. Stuart.
  Nonlocality and nonlinearity implies universality in operator learning.
  *Constructive Approximation*, 62:261â€“303, 2025.
* Lefebvre etÂ al. [2023]

  W.Â Lefebvre, G.Â Loeper, and H.Â Pham.
  Differential learning methods for solving fully nonlinear PDEs.
  *Digital Finance*, 5(1):183â€“229, 2023.
* Li etÂ al. [2020]

  B.Â Li, S.Â Tang, and H.Â Yu.
  Better approximations of high dimensional smooth functions by deep neural networks with rectified power units.
  *Communications in Computational Physics*, 27:379â€“411, 2020.
* Li etÂ al. [2021]

  Z.Â Li, N.Â Kovachki, K.Â Azizzadenesheli, B.Â Liu, K.Â Bhattacharya, A.Â Stuart, and A.Â Anandkumar.
  Fourier neural operator for parametric partial differential equations.
  In *International conference on learning representations ((ICLR 2021))*, pages 1â€“16, 2021.
* Li etÂ al. [2023]

  Z.Â Li, K.Â Meidani, and A.Â B. Farimani.
  Transformer for partial differential equationsâ€™ operator learning.
  *Transactions on Machine Learning Research*, April:1â€“34, 2023.
* Liu etÂ al. [2025]

  Z.Â Liu, Y.Â Wang, S.Â Vaidya, F.Â Ruehle, J.Â Halverson, M.Â Soljacic, T.Â Y. Hou, and M.Â Tegmark.
  KAN: Kolmogorovâ€“Arnold networks.
  In *The thirteenth international conference on learning representations ((ICLR 2025))*, pages 1â€“47, 2025.
* Lu etÂ al. [2019]

  L.Â Lu, P.Â Jin, and G.Â E. Karniadakis.
  DeepONet: learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators.
  *ArXiv preprint arXiv:1910.03193*, 2019.
* Lu etÂ al. [2021]

  L.Â Lu, P.Â Jin, G.Â Pang, Z.Â Zhang, and G.Â E. Karniadakis.
  Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators.
  *Nature Machine Intelligence*, 3:218â€“229, 2021.
* Mallat [1989]

  S.Â G. Mallat.
  Multiresolution approximations and wavelet orthonormal bases of L2â€‹(ğ•£){L}^{2}({\mathbb{r}}).
  *Transactions of the American Mathematical Society*, 315(1):69â€“87, 1989.
* Marcati and Schwab [2023]

  C.Â Marcati and C.Â Schwab.
  Exponential convergence of deep operator networks for elliptic partial differential equations.
  *SIAM Journal on Numerical Analysis*, 61(3):1513â€“1545, 2023.
* Marcati and Schwab [2025]

  C.Â Marcati and C.Â Schwab.
  Expression rates of neural operators for linear elliptic PDEs in polytopes.
  *Foundations of Computational Mathematics*, to appear, 2025.
* Mhaskar [1996]

  H.Â N. Mhaskar.
  Neural networks for optimal approximation of smooth and analytic functions.
  *Neural Computation*, 8(1):164â€“177, 1996.
* Mhaskar and Micchelli [1995]

  H.Â N. Mhaskar and C.Â A. Micchelli.
  Degree of approximation by neural and translation networks with a single hidden layer.
  *Advances in Applied Mathematics*, 16(2):151â€“183, 1995.
* Mhaskar and Micchelli [1992]

  H.Â N. Mhaskar and CharlesÂ A. Micchelli.
  Approximation by superposition of sigmoidal and radial basis functions.
  *Advances in Applied Mathematics*, 13(3):350â€“373, 1992.
* Munkres [2000]

  J.Â R. Munkres.
  *Topology*.
  Prentice Hall, Inc., Upper Saddle River, NJ, second edition, 2000.
* Neufeld and Schmocker [2023]

  A.Â Neufeld and P.Â Schmocker.
  Universal approximation property of Banach spaceâ€“valued random feature models including random neural networks.
  *ArXiv preprint arXiv:2312.08410*, 2023.
* Nguwi etÂ al. [2024]

  J.Â Y. Nguwi, G.Â Penent, and N.Â Privault.
  A deep branching solver for fully nonlinear partial differential equations.
  *Journal of Computational Physics*, 499:112712, 2024.
* NÃ¼sken and Richter [2021]

  N.Â NÃ¼sken and L.Â Richter.
  Solving high-dimensional Hamiltonâ€“Jacobiâ€“Bellman PDEs using neural networks: perspectives from the theory of controlled diffusions and measures on path space.
  *Partial Differential Equations and Applications*, 2(48):1â€“48, 2021.
* Pak etÂ al. [2025]

  C.-G. Pak, H.-J. Hwang, and M.-C. Kim.
  A nonequidistant multistep scheme for second order backward stochastic differential equations with applications to stochastic optimal control.
  *International Journal of Applied and Computational Mathematics*, 11(58):1â€“19, 2025.
* Pardoux [1998]

  Ã‰. Pardoux.
  Backward stochastic differential equations and viscosity solutions of systems of semilinear parabolic and elliptic PDEs of second order.
  In L.Â Decreusefond, B.Â Ã˜ksendal, J.Â Gjerde, and ÃœstÃ¼nel A.S., editors, *Stochastic analysis and related topics VI. Proceedings of the sixth Osloâ€“Silivri workshop, Geilo, 1996*, volumeÂ 42 of *Progress in probability*, pages 79â€“127, 1998.
* Pereira etÂ al. [2020]

  M.Â Pereira, Z.Â Wang, T.Â Chen, E.Â Reed, and E.Â Theodorou.
  Feynmanâ€“Kac neural network architectures for stochastic control using second-order FBSDE theory.
  In *Proceedings of the 2nd conference on learning for dynamics and control*, volume 120 of *Proceedings of machine learning research*, pages 728â€“738, 2020.
* Pham etÂ al. [2021]

  H.Â Pham, X.Â Warin, and M.Â Germain.
  Neural networks-based backward scheme for fully nonlinear PDEs.
  *SN Partial Differential Equations and Applications*, 2(16):1â€“24, 2021.
* Pollard [1984]

  D.Â Pollard.
  *Convergence of stochastic processes*.
  Springer series in statistics. Springer New York, NY, 1984.
* PossamaÃ¯ and Tan [2015]

  D.Â PossamaÃ¯ and X.Â Tan.
  Weak approximation of second-order BSDEs.
  *The Annals of Applied Probability*, 25(5):2535â€“2562, 2015.
* RaoniÄ‡ etÂ al. [2023]

  B.Â RaoniÄ‡, R.Â Molinaro, T.Â deÂ Ryck, T.Â Rohner, F.Â Bartolucci, R.Â Alaifari, S.Â Mishra, and E.Â deÂ BÃ©zenac.
  Convolutional neural operators for robust and accurate learning of PDEs.
  In *Proceedings of the 37th conference on advances in neural information processing systems ((NeurIPS 2023)), December 10â€“16, 2023, New Orleans, Louisiana, United States of America*, volumeÂ 36, pages 77187â€“77200, 2023.
* Ren and Tan [2017]

  Z.Â Ren and X.Â Tan.
  On the convergence of monotone schemes for path-dependent PDE.
  *Stochastic Processes and their Applications*, 127(6):1738â€“1762, 2017.
* Riedi etÂ al. [2023]

  R.Â H. Riedi, R.Â Balestriero, and R.Â G. Baraniuk.
  Singular value perturbation and deep network optimization.
  *Constructive Approximation*, 57(2):807â€“852, 2023.
* Schneider etÂ al. [2025]

  Cornelia Schneider, Mario Ullrich, and Jan Vybiral.
  Nonlocal techniques for the analysis of deep relu neural network approximations.
  *arXiv preprint arXiv:2504.04847*, 2025.
* Schwab etÂ al. [2025]

  C.Â Schwab, A.Â Stein, and J.Â Zech.
  Deep operator network approximation rates for Lipschitz operators.
  *Analysis and Applications*, to appear, 2025.
* Shen etÂ al. [2022]

  Z.Â Shen, H.Â Yang, and S.Â Zhang.
  Deep network approximation: achieving arbitrary accuracy with fixed number of neurons.
  *Journal of Machine Learning Research*, 23(276):1â€“60, 2022.
* Soner etÂ al. [2012]

  H.Â M. Soner, N.Â Touzi, and J.Â Zhang.
  Wellposedness of second order backward SDEs.
  *Probability Theory and Related Fields*, 153(1â€“2):149â€“190, 2012.
* Taylor [2023]

  M.Â E. Taylor.
  *Partial differential equations I. Basic theory*, volume 115 of *Applied mathematical sciences*.
  Springer Cham, third edition, 2023.
* Triebel [2006]

  H.Â Triebel.
  *Theory of function spaces. III*, volume 100 of *Monographs in mathematics*.
  BirkhÃ¤user Basel, 2006.
* Triebel [2008]

  H.Â Triebel.
  *Function spaces and wavelets on domains*, volumeÂ 7 of *Tracts in mathematics*.
  European Mathematical Society, ZÃ¼rich, 2008.
* Tripura and Chakraborty [2022]

  T.Â Tripura and S.Â Chakraborty.
  Wavelet neural operator: a neural operator for parametric partial differential equations.
  *ArXiv preprint arXiv:2205.02191*, 2022.
* Wang etÂ al. [2024]

  Y.Â Wang, J.Â W. Siegel, Z.Â Liu, and T.Â Y. Hou.
  On the expressiveness and spectral bias of KANs.
  *ArXiv preprint arXiv:2410.01803*, 2024.
* Xiao etÂ al. [2024]

  X.Â Xiao, W.Â Qiu, and O.Â Nikan.
  Numerical approximation based on deep convolutional neural network for high-dimensional fully nonlinear merged PDEs and 2BSDEs.
  *Mathematical Methods in the Applied Sciences*, 47(7):6184â€“6204, 2024.
* Yang etÂ al. [2019]

  J.Â Yang, W.Â Zhao, and T.Â Zhou.
  Explicit deferred correction methods for second-order forward backward stochastic differential equations.
  *Journal of Scientific Computing*, 79(3):1409â€“1432, 2019.
* Yarotsky [2017]

  D.Â Yarotsky.
  Error bounds for approximations with deep ReLU networks.
  *Neural Networks*, 94:103â€“114, 2017.
* Yu etÂ al. [2021]

  A.Â Yu, C.Â Becquey, D.Â Halikias, M.Â E. Mallory, and A.Â Townsend.
  Arbitrary-depth universal approximation theorems for operator neural networks.
  *ArXiv preprint arXiv:2109.11354*, 2021.
* Zhou etÂ al. [2021]

  M.Â Zhou, J.Â Han, and J.Â Lu.
  Actorâ€“critic method for high dimensional static Hamiltonâ€“Jacobiâ€“Bellman partial differential equations based on neural networks.
  *SIAM Journal on Scientific Computing*, 43(6):A4043â€“A4066, 2021.