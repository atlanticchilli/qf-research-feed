---
authors:
- Hasib Uddin Molla
- Ankit Banarjee
- Matthew Backhouse
- Jinniao Qiu
doc_id: arxiv:2511.08735v1
family_id: arxiv:2511.08735
is_current: true
taxonomy:
  alpha_families: []
  asset_classes: []
  horizons: []
  themes: []
title: A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications
url_abs: http://arxiv.org/abs/2511.08735v1
url_html: https://arxiv.org/html/2511.08735v1
venue: arXiv q-fin
version: 1
year: 2025
---


Hasib Uddin Molla
Corresponding author. Email: mdhasibuddin.molla@ucalgary.ca
Department of Mathematics and Statistics,
University of Calgary

Ankit Banarjee
Department of Mathematics and Statistics,
University of Calgary

Matthew Backhouse
Department of Mathematics and Statistics,
University of Calgary

Jinniao Qiu
Department of Mathematics and Statistics,
University of Calgary

###### Abstract

In this work, we extend deep learning-based numerical methods to fully coupled forward-backward stochastic differential equations (FBSDEs) within a non-Markovian framework. Error estimates and convergence are provided. In contrast to the existing literature, our approach not only analyzes the non-Markovian framework but also addresses fully coupled settings, in which both the drift and diffusion coefficients of the forward process may be random and depend on the backward components YYand ZZ. Furthermore, we illustrate the practical applicability of our framework by addressing utility maximization problems under rough volatility, which are solved numerically with the proposed deep learning-based methods.

## 1 Introduction

Let (Œ©,‚Ñ±,(‚Ñ±t)t‚àà[0,T],‚Ñô)(\Omega,\mathscr{F},(\mathscr{F}\_{t})\_{t\in[0,T]},\mathbb{P}) be a complete filtered probability space, where the filtration (‚Ñ±t)t‚àà[0,T](\mathscr{F}\_{t})\_{t\in[0,T]} is the augmented filtration generated by an mm-dimensional Wiener process (Wt)t‚àà[0,T](W\_{t})\_{t\in[0,T]}. The predictable œÉ\sigma-algebra on Œ©√ó[0,T]\Omega\times[0,T] associated to (‚Ñ±t)t‚àà[0,T](\mathscr{F}\_{t})\_{t\in[0,T]} is denoted by ùí´\mathscr{P}.
  
We consider the following coupled forward-backward stochastic differential equation (FBSDE):

|  |  |  |  |
| --- | --- | --- | --- |
|  | {d‚ÄãXt=b‚Äã(t,Vt,Xt,Yt,Zt)‚Äãd‚Äãt+œÉ‚Äã(t,Vt,Xt,Yt,Zt)‚Äãd‚ÄãWt,t‚àà[0,T],X0=x0,‚àíd‚ÄãYt=f‚Äã(t,Vt,Xt,Yt,Zt)‚Äãd‚Äãt‚àíZt‚Äãd‚ÄãWt,t‚àà[0,T],YT=g‚Äã(VT,XT).\begin{cases}dX\_{t}&=b(t,V\_{t},X\_{t},Y\_{t},Z\_{t})dt+\sigma(t,V\_{t},X\_{t},Y\_{t},Z\_{t})dW\_{t},\ t\in[0,T],\\ X\_{0}&=x\_{0},\\ -dY\_{t}&=f(t,V\_{t},X\_{t},Y\_{t},Z\_{t})dt-Z\_{t}dW\_{t},\ t\in[0,T],\\ Y\_{T}&=g(V\_{T},X\_{T}).\end{cases} |  | (1.1) |

where T>0T>0 is a fixed finite terminal time, and the solution triple (Xt,Yt,Zt)(X\_{t},Y\_{t},Z\_{t}) takes values in ‚Ñùd√ó‚Ñùd0√ó‚Ñùd0√óm\mathbb{R}^{d}\times\mathbb{R}^{d\_{0}}\times\mathbb{R}^{d\_{0}\times m}. The functions b,œÉ,fb,\sigma,f, and gg take values in appropriate spaces consistent with the dimensions of the problem. Wiener process WtW\_{t} is decomposed as Wt=(W~t,Bt)W\_{t}=(\widetilde{W}\_{t},B\_{t}), where W~t\widetilde{W}\_{t} is m1m\_{1}-dimensional and BtB\_{t} is m2m\_{2}-dimensional, with m1+m2=mm\_{1}+m\_{2}=m. When m1=0m\_{1}=0 or m2=0m\_{2}=0, we mean W=BW=B or W=W~W=\widetilde{W} respectively.
  
The following assumption is imposed on the exogenous stochastic process VV.

###### Assumption 1.1.

The process VV has continuous trajectories, takes values in ‚Ñùm0\mathbb{R}^{m\_{0}}, and is adapted to the filtration generated by the Wiener process W~\widetilde{W}. In addition, it is integrable in the sense that

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Äã[‚à´0T|Vs|‚Äãùëës]<‚àû,T>0.\mathbb{E}\left[\int\_{0}^{T}|V\_{s}|ds\right]<\infty,\quad T>0. |  |

It is important to emphasize that VV, and even the joint process (X,V)(X,V), is not assumed to be Markovian or to possess the semi-martingale property. In fact, neither of our main examples satisfies these properties.
  
FBSDEs have found numerous applications in modelling optimization problems in mathematical finance and have been the subject of growing interest over the past three decades. Considerable effort has been devoted to understanding the solvability of fully coupled FBSDEs, particularly over arbitrary time horizons and under minimal regularity assumptions on the coefficients.
One of the earliest approaches is the contraction mapping technique introduced by Antonelli [FAntonelli1993BFSDEs], which establishes the existence and uniqueness of solutions but only for sufficiently small time intervals. The four-step scheme developed by Ma et al. [JMaPProtterJYong1994SolvingFBSDEs4StepScheme] provides a more direct method of solution by linking the problem to a system of parabolic partial differential equations (PDEs). Nonetheless, this method therein is restricted to the Markovian setting. Another approach is the so-called continuation method, proposed by Hu and Peng [YHuSPeng1995SolFBSDEs] and further developed by Yong [JYong1997FindAdaptedSolFBSDEMethodofCont], which accommodates non-Markovian frameworks and is applicable over general time horizons. This method has proven effective in broadening the class of FBSDEs for which well-posedness can be established.
  
Under the non-Markovian framework, the resolution of an FBSDE is closely linked to the existence of a random field u‚Äã(x,t)u(x,t) such that Y‚Äã(t)=u‚Äã(X‚Äã(t),t),t‚àà[0,T]Y(t)=u(X(t),t),t\in[0,T]. This random field, known as the decoupling field of the FBSDE, satisfies an associated quasilinear backward stochastic partial differential equation (BSPDE). The connection between FBSDEs and BSPDEs, as well as the conditions under which they are well-posed, has been investigated in [JMaHYinJZhang2012OnNon-MarkovFBSDEsandBSPDEs, JMaZWuDZhangJZhang2015OnWellPossedofFBSDEsAUnifiedAppr];
in the Markovian case, the decoupling field becomes deterministic and satisfies a deterministic quasilinear parabolic PDE. This correspondence between FBSDEs and BSPDEs forms the foundation of the stochastic Feynman‚ÄìKac formula, which provides a probabilistic representation for the solution of certain classes of partial differential equations.
  
Over the past decades, researchers have developed several approaches for numerical approximations of FBSDEs. Some of these approaches exploit the connections between FBSDEs and quasilinear parabolic partial differential equations (PDEs), particularly through the four-step scheme. These methods rely on the numerical solutions of parabolic PDEs, for example through finite difference techniques, which typically become computationally prohibitive in high-dimensional contexts due to the curse of dimensionality.
To overcome the limitations of PDE-based approaches, alternative methodologies based on time discretization schemes have been explored. For decoupled FBSDEs, the discretization of the forward component is relatively straightforward, whereas the backward component involves the computation of conditional expectations at each time step‚Äìa task that poses significant challenges, particularly in high-dimensional settings. Such time discretization-based schemes for decoupled FBSDEs are studied by Zhang [JZhang2004NumericalSchemeBSDEs], while regression-based Monte-Carlo methods were proposed by Bouchard and Touzi [BBouchardNTouzi2004DiscreteTimeApprxMCsimulBSDE], and further developed by Gobet et al. [EGobetJPLemorXWarin2005RegressionBasedMCtoSolveBSDEs] among many others.
Despite ongoing efforts to reduce the computational complexity associated with evaluating conditional expectations, this remains a significant obstacle for high-dimensional problems. In the case of coupled FBSDEs, these difficulties are exacerbated, as time discretization schemes often require iterative procedures such as Picard iterations, which further increase computational cost. To address this, Bender and Zhang [CBenderJZhang2008TimeDiscrMarkovIteraCoupledDFBSDEs] proposed a time discretization scheme incorporating Markovian iteration to mitigate the complexity inherent in solving coupled FBSDEs numerically.
  
Since most numerical algorithms for FBSDEs and parabolic PDEs suffer from the so-called curse of dimensionality, which renders them inefficient or infeasible in high-dimensional settings, a class of deep learning-based algorithms has recently emerged to overcome this challenge. Notably, several methods have been developed for decoupled FBSDEs and their associated parabolic PDEs [EWeinanJHanAJent2017DeepLearnNumrMethodHighDimParbPDEsBSDEs, JHanAJentWE2018SolvingHighDimEqusDeepLearnin, CHureHPhamXWarin2020DeepBShcemeHighDimNonlPDEs], demonstrating the capacity to mitigate the challenges associated with high-dimensional problems.
These deep learning-based approaches, commonly referred to as Deep BSDE methods, have since been extended to some coupled FBSDEs [JHanJLong2020ConvgDeepBSDEforCoupledFBSDEs, SJiSPengYPengXZhang2020ThreeAlgorithmsforSolvHighDimCoupledFBSDEsDeepLearn, HUMollaJQiu20221NumercFBSDE]. In particular, Han and Long [JHanJLong2020ConvgDeepBSDEforCoupledFBSDEs] established a convergence analysis and obtained a posteriori error estimates for the Deep BSDE scheme in the coupled setting. However, these studies have largely been confined to the Markovian framework and to cases where the coefficients of the forward SDE do not depend on the unknown process ZZ. More recently, Bayer et al. [CBayerJQiuYYao2020PricingOptnRoughVolatBSPDEs] extended the application of Deep BSDE methods to a class of FBSDEs under a non-Markovian framework but restricted to a depcoupled setting.
  
We emphasize that the FBSDEs considered in this work are non-Markovian, in the sense that their coefficients may exhibit randomness through dependence on external non-Markovian processes, such as VV in our case. However, this form of randomness, arising from an exogenous process, is distinct from that in path-dependent FBSDEs, where the coefficients depend explicitly on the entire trajectory of the forward process XX. Such path-dependent FBSDEs typically arise in stochastic control problems with memory or path-dependent features, including applications to exotic derivatives such as Asian, barrier, or lookback options. These classes of problems are also naturally connected to the theory of path-dependent partial differential equations (PPDEs) (see [TPhamJZhang\_2014\_ZeroSumGamePathDependent],[Saporito2019StochControlPathDependent] for example). Neural network-based methods [SaporitoZhang2021NeuralNetPathDependentPDE],[QiFeng\_2023\_DeepSignatureFBSDEAlgo] can be used for solving path-dependent partial differential equations (PPDEs) and FBSDEs.

In this work, we investigate Deep BSDE methods for fully coupled non-Markovian FBSDEs. Error estimates are obtained for a class of coupled FBSDEs in which both the drift and diffusion components of the forward process may be random and depend on the backward process ZZ and YY. The remainder of this paper is organized as follows. In Section 2, we introduce the necessary notations and assumptions, review key properties of solutions to FBSDEs, and provide a brief overview of neural network approximation techniques. Section 3 presents the framework of our proposed numerical scheme and derives an estimate for the associated simulation error. In Section 4, we carry out the convergence analysis of the scheme. Section 5 details the algorithm for the numerical approximation of fully coupled non-Markovian FBSDEs. Finally, in Section 6, we illustrate the effectiveness of the proposed method through two numerical examples.

## 2 Preliminaries

### 2.1 Notations and Assumptions

In this section, we introduce the notations and assumptions used throughout the paper. Let œÄ:0=t0<t1<t2‚Äã‚ãØ<tN‚àí1<tN=T\pi:0=t\_{0}<t\_{1}<t\_{2}\cdots<t\_{N-1}<t\_{N}=T denote a partition of the time interval [0,T][0,T] into NN subintervals (ti,ti+1)(t\_{i},t\_{i+1}). We define the mesh size of the partition as

|  |  |  |
| --- | --- | --- |
|  | h:=max‚Å°{ti+1‚àíti:i=0,1,‚ãØ,N‚àí1}.h:=\max\{t\_{i+1}-t\_{i}:i=0,1,\cdots,N-1\}. |  |

Without loss of generality, we assume that the partition is uniform, i.e., h=ti+1‚àítih=t\_{i+1}-t\_{i}, for all i=0,1,‚ãØ,N‚àí1i=0,1,\cdots,N-1 and h<1h<1 (small). We also assume that our processes are all one-dimensional, that is, d=d0=m=1d=d\_{0}=m=1.
  
For Œ∏j:=(xj,yj,zj),j=1,2,\theta\_{j}:=(x\_{j},y\_{j},z\_{j}),j=1,2, and for œÜ=b,œÉ,f\varphi=b,\sigma,f, denote

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | g1‚Äã(x1,x2)\displaystyle g\_{1}(x\_{1},x\_{2}) | =1x1‚â†x2‚Äã[g‚Äã(VT,x1)‚àíg‚Äã(VT,x2)]/[x1‚àíx2];\displaystyle=1\_{x\_{1}\neq x\_{2}}[g(V\_{T},x\_{1})-g(V\_{T},x\_{2})]/[x\_{1}-x\_{2}]; |  | (2.1) |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | œÜ1‚Äã(t,Œ∏1,Œ∏2)\displaystyle\varphi\_{1}(t,\theta\_{1},\theta\_{2}) | =1x1‚â†x2‚Äã[œÜ‚Äã(t,Vt,x1,y1,z1)‚àíœÜ‚Äã(t,Vt,x2,y1,z1)]/[x1‚àíx2];\displaystyle=1\_{x\_{1}\neq x\_{2}}[\varphi(t,V\_{t},x\_{1},y\_{1},z\_{1})-\varphi(t,V\_{t},x\_{2},y\_{1},z\_{1})]/[x\_{1}-x\_{2}]; |  | (2.2) |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | œÜ2‚Äã(t,Œ∏1,Œ∏2)\displaystyle\varphi\_{2}(t,\theta\_{1},\theta\_{2}) | =1y1‚â†y2‚Äã[œÜ‚Äã(t,Vt,x2,y1,z1)‚àíœÜ‚Äã(t,Vt,x2,y2,z1)]/[y1‚àíy2];\displaystyle=1\_{y\_{1}\neq y\_{2}}[\varphi(t,V\_{t},x\_{2},y\_{1},z\_{1})-\varphi(t,V\_{t},x\_{2},y\_{2},z\_{1})]/[y\_{1}-y\_{2}]; |  | (2.3) |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | œÜ3‚Äã(t,Œ∏1,Œ∏2)\displaystyle\varphi\_{3}(t,\theta\_{1},\theta\_{2}) | =1z1‚â†z2‚Äã[œÜ‚Äã(t,Vt,x2,y2,z1)‚àíœÜ‚Äã(t,Vt,x2,y2,z2)]/[z1‚àíz2].\displaystyle=1\_{z\_{1}\neq z\_{2}}[\varphi(t,V\_{t},x\_{2},y\_{2},z\_{1})-\varphi(t,V\_{t},x\_{2},y\_{2},z\_{2})]/[z\_{1}-z\_{2}]. |  | (2.4) |

Then we define

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | F¬Ø‚Äã(t,y)\displaystyle\overline{F}(t,y) | =esssup(supx1‚â†x2,y1‚â†y2,z1‚â†z2F‚Äã(Œ∏1,Œ∏2;t,y)),\displaystyle=\operatorname\*{esssup}\left(\sup\_{x\_{1}\neq x\_{2},y\_{1}\neq y\_{2},z\_{1}\neq z\_{2}}F(\theta\_{1},\theta\_{2};t,y)\right), |  | (2.5) |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | F¬Ø‚Äã(t,y)\displaystyle\underline{F}(t,y) | =essinf(infx1‚â†x2,y1‚â†y2,z1‚â†z2F‚Äã(Œ∏1,Œ∏2;t,y)),\displaystyle=\operatorname\*{essinf}\left(\inf\_{x\_{1}\neq x\_{2},y\_{1}\neq y\_{2},z\_{1}\neq z\_{2}}F(\theta\_{1},\theta\_{2};t,y)\right), |  | (2.6) |

where

|  |  |  |  |
| --- | --- | --- | --- |
|  | F‚Äã(Œ∏1,Œ∏2;t,y)=f1+f2‚Äãy+y‚Äã(b1+b2‚Äãy)+(f3+b3‚Äãy)‚Äãy‚Äã(œÉ1+œÉ2‚Äãy)1‚àíœÉ3‚Äãy.F(\theta\_{1},\theta\_{2};t,y)=f\_{1}+f\_{2}y+y(b\_{1}+b\_{2}y)+\frac{(f\_{3}+b\_{3}y)y(\sigma\_{1}+\sigma\_{2}y)}{1-\sigma\_{3}y}. |  | (2.7) |

###### Assumption 2.1.

There are three constants c1,c2,c3c\_{1},c\_{2},c\_{3} satisfying

|  |  |  |  |
| --- | --- | --- | --- |
|  | c1>0,0<c2<c3,c1‚Äãc3<1,c\_{1}>0,\hskip 11.38109pt0<c\_{2}<c\_{3},\hskip 11.38109ptc\_{1}c\_{3}<1, |  | (2.8) |

and also there exists another constant œµ=œµ‚Äã(T)>0\epsilon=\epsilon(T)>0 such that either one of the following three conditions holds:

1. (Case I)

   |œÉ3|‚â§c1,|g1|‚â§c2\left|{\sigma\_{3}}\right|\leq c\_{1},\left|{g\_{1}}\right|\leq c\_{2}; and F¬Ø‚Äã(t,c3)‚â§œµ,F¬Ø‚Äã(t,‚àíc3)‚â•‚àíœµ\overline{F}(t,c\_{3})\leq\epsilon,\underline{F}(t,-c\_{3})\geq-\epsilon.
2. (Case II)

   |œÉ3|‚â•c1‚àí1,|g1|‚â•c2‚àí1\left|{\sigma\_{3}}\right|\geq c\_{1}^{-1},\left|{g\_{1}}\right|\geq c\_{2}^{-1}, and both œÉ3\sigma\_{3} and g1g\_{1} keep the same sign, meaning, the dependence of the forward diffusion on the backward control variable ZZ (through œÉ3\sigma\_{3}) and the slope of the terminal condition g1g\_{1} are both uniformly bounded away from zero, and they act in the same direction ‚Äî either both positive or both negative.
3. (Case III)

   œÉ3‚Äãg1‚â§c1‚Äãc2\sigma\_{3}g\_{1}\leq c\_{1}c\_{2}, and either œÉ3\sigma\_{3} or g1g\_{1} keeps the same sign, meaning, the coefficients œÉ3\sigma\_{3} and g1g\_{1} are not required to share the same sign, but at least one remains sign-consistent, and their combined effect is bounded so the system remains stable.

###### Assumption 2.2.

The coefficients (b,œÉ,f)‚Äã(‚ãÖ,X,Y,Z)(b,\sigma,f)(\cdot,X,Y,Z) are (‚Ñ±t)t‚àà[0,T](\mathcal{F}\_{t})\_{t\in[0,T]}-progressively measurable, for each (X,Y,Z)‚àà‚Ñùd√ó‚Ñùd0√ó‚Ñùd0√óm(X,Y,Z)\in\mathbb{R}^{d}\times\mathbb{R}^{d\_{0}}\times\mathbb{R}^{d\_{0}\times m}, and the terminal condition g‚Äã(‚ãÖ,X)g(\cdot,X) is ‚Ñ±T\mathcal{F}\_{T}-measurable for any fixed X‚àà‚ÑùdX\in\mathbb{R}^{d}. All coefficient functions may be random, but their randomness is through the dependence on the process VV. Moreover, the following integrability condition holds:

|  |  |  |  |
| --- | --- | --- | --- |
|  | I02:=ùîº‚Äã[‚à´0T(|b|2+|f|2+|œÉ|2)‚Äã(t,Vt,0,0,0)‚Äãùëët+|g‚Äã(VT,0)|2].I^{2}\_{0}:=\mathbb{E}\bigg[\int\_{0}^{T}\big(|b|^{2}+|f|^{2}+|\sigma|^{2}\big)(t,V\_{t},0,0,0)dt+|g(V\_{T},0)|^{2}\bigg]. |  | (2.9) |

###### Assumption 2.3.

There are positive constants kb,kf,K,by,bz,fx,fz,œÉx,œÉy,œÉz,k\_{b},\,k\_{f},\,K,\,b\_{y},\,b\_{z},\,f\_{x},\,f\_{z},\,\sigma\_{x},\,\sigma\_{y},\,\sigma\_{z}, and gxg\_{x} such that for all (xi,yi,zi)‚àà‚Ñùd√ó‚Ñùd0√ó‚Ñùd0√óm(x\_{i},y\_{i},z\_{i})\in\mathbb{R}^{d}\times\mathbb{R}^{d\_{0}}\times\mathbb{R}^{d\_{0}\times m}, i=1,2i=1,2, it holds that a.s.,

1. (1)

   |  |  |  |  |
   | --- | --- | --- | --- |
   |  | {[b‚Äã(t,Vt,x1,y1,z1)‚àíb‚Äã(t,Vt,x2,y1,z1)]‚ãÖ‚ñ≥‚Äãx‚â§kb‚Äã|‚ñ≥‚Äãx|2,[f‚Äã(t,Vt,x1,y1,z1)‚àíf‚Äã(t,Vt,x1,y2,z1)]‚ãÖ‚ñ≥‚Äãy‚â§kf‚Äã|‚ñ≥‚Äãy|2;\left\{\begin{array}[]{l}{[b(t,V\_{t},x\_{1},y\_{1},z\_{1})-b(t,V\_{t},x\_{2},y\_{1},z\_{1})]\cdot\triangle x\leq k\_{b}|\triangle x|^{2}},\\ {[f(t,V\_{t},x\_{1},y\_{1},z\_{1})-f(t,V\_{t},x\_{1},y\_{2},z\_{1})]\cdot\triangle y\leq k\_{f}|\triangle y|^{2}};\end{array}\right. |  | (2.10) |
2. (2)

   |  |  |  |  |
   | --- | --- | --- | --- |
   |  | {|b(t,Vt,x1,y1,z1)‚àíb(t,Vt,x2,y2,z2)]|2‚â§K|‚ñ≥x|2+by|‚ñ≥y|2+bz|‚ñ≥z|2,|f‚Äã(t,Vt,x1,y1,z1)‚àíf‚Äã(t,Vt,x2,y2,z2)|2‚â§fx‚Äã|‚ñ≥‚Äãx|2+K‚Äã|‚ñ≥‚Äãy|2+fz‚Äã|‚ñ≥‚Äãz|2,|œÉ‚Äã(t,Vt,x1,y1,z1)‚àíœÉ‚Äã(t,Vt,x2,y2,z2)|2‚â§œÉx‚Äã|‚ñ≥‚Äãx|2+œÉy‚Äã|‚ñ≥‚Äãy|2+œÉz‚Äã|‚ñ≥‚Äãz|2,|g‚Äã(VT,x1)‚àíg‚Äã(VT,x2)|2‚â§gx‚Äã|‚ñ≥‚Äãx|2,\left\{\begin{array}[]{l}{|b(t,V\_{t},x\_{1},y\_{1},z\_{1})-b(t,V\_{t},x\_{2},y\_{2},z\_{2})]|^{2}\leq K|\triangle x|^{2}+b\_{y}|\triangle y|^{2}+b\_{z}|\triangle z|^{2}},\\ {|f(t,V\_{t},x\_{1},y\_{1},z\_{1})-f(t,V\_{t},x\_{2},y\_{2},z\_{2})|^{2}\leq f\_{x}|\triangle x|^{2}+K|\triangle y|^{2}+f\_{z}|\triangle z|^{2}},\\ {|\sigma(t,V\_{t},x\_{1},y\_{1},z\_{1})-\sigma(t,V\_{t},x\_{2},y\_{2},z\_{2})|^{2}\leq\sigma\_{x}|\triangle x|^{2}+\sigma\_{y}|\triangle y|^{2}+\sigma\_{z}|\triangle z|^{2}},\\ {|g(V\_{T},x\_{1})-g(V\_{T},x\_{2})|^{2}\leq g\_{x}|\triangle x|^{2}},\\ \end{array}\right. |  | (2.11) |

   where (‚ñ≥‚Äãx,‚ñ≥‚Äãy,‚ñ≥‚Äãz)=(x1‚àíx2,y1‚àíy2,z1‚àíz2)(\triangle x,\triangle y,\triangle z)=(x\_{1}-x\_{2},y\_{1}-y\_{2},z\_{1}-z\_{2}).

###### Assumption 2.4.

There exists a continuous and increasing function œÅ:[0,‚àû)‚Üí[0,‚àû)\rho:[0,\infty)\rightarrow[0,\infty) with œÅ‚Äã(0)=0\rho(0)=0 such that for any 0‚â§ti‚â§tj‚â§T0\leq t\_{i}\leq t\_{j}\leq T, it holds that

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùîº‚Äã[|œï‚Äã(ti,Vti,x,y,z)‚àíœï‚Äã(tj,Vtj,x,y,z)|2]‚â§œÅ‚Äã(|ti‚àítj|),\mathbb{E}\bigg[|\phi(t\_{i},V\_{t\_{i}},x,y,z)-\phi(t\_{j},V\_{t\_{j}},x,y,z)|^{2}\bigg]\leq\rho(|t\_{i}-t\_{j}|), |  | (2.12) |

where œï=b,œÉ,f\phi=b,\sigma,f.

Here, we present an example of fully coupled forward-backward stochastic differential equations of the form ([1.1](https://arxiv.org/html/2511.08735v1#S1.E1 "In 1 Introduction ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")), that satisfy the Assumptions [2.1](https://arxiv.org/html/2511.08735v1#S2.Thmass1 "Assumption 2.1. ‚Ä£ 2.1 Notations and Assumptions ‚Ä£ 2 Preliminaries ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")-[2.4](https://arxiv.org/html/2511.08735v1#S2.Thmass4 "Assumption 2.4. ‚Ä£ 2.1 Notations and Assumptions ‚Ä£ 2 Preliminaries ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications").

###### Example 2.1.

Fix T>0T>0 and let WW be a 1-dimensional Brownian motion and VtV\_{t} be the following Ornstein‚ÄìUhlenbeck process:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Vt=v0+W^t,V\_{t}=v\_{0}+\widehat{W}\_{t}, |  | (2.13) |

where, W^t\widehat{W}\_{t} is the fractional Brownian motion process with Hurst index H‚àà(0,12)H\in(0,\frac{1}{2}) and is defined as

|  |  |  |  |
| --- | --- | --- | --- |
|  | W^t‚âî‚à´0tùí¶‚Äã(t‚àís)‚ÄãùëëWs,ùí¶‚Äã(r)‚âî2‚ÄãH‚ÄãrH‚àí1/2,r>0.\widehat{W}\_{t}\coloneqq\int\_{0}^{t}\mathcal{K}(t-s)\,dW\_{s},\quad\mathcal{K}(r)\coloneqq\sqrt{2H}r^{H-1/2},\quad r>0. |  | (2.14) |

Then, with some bounded constants s3,k1‚â†0s\_{3},k\_{1}\neq 0, we define the coupled forward-backward stochastic differential equations as:

|  |  |  |  |
| --- | --- | --- | --- |
|  | {d‚ÄãXt=(‚àísin‚Å°Xt+tanh‚Å°Yt+Zt+cos‚Å°Vt)‚Äãd‚Äãt+(Xt+Yt+s3‚ÄãZt+Vt)‚Äãd‚ÄãWt,t‚àà[0,T],X0=0,‚àíd‚ÄãYt=(Xt+cos‚Å°Yt+sin‚Å°Zt+|Vt|)‚Äãd‚Äãt‚àíZt‚Äãd‚ÄãWt,t‚àà[0,T],YT=k1‚ÄãXT+‚à´0Tsin‚Å°Vt‚Äãd‚Äãt.\begin{cases}dX\_{t}&=\big(-\sin{X\_{t}}+\tanh{Y\_{t}}+Z\_{t}+\cos{V\_{t}}\big)dt+\big(X\_{t}+Y\_{t}+s\_{3}Z\_{t}+V\_{t}\big)dW\_{t},\ t\in[0,T],\\ X\_{0}&=0,\\ -dY\_{t}&=\big(X\_{t}+\cos{Y\_{t}}+\sin{Z\_{t}}+\sqrt{|V\_{t}|}\big)dt-Z\_{t}dW\_{t},\ t\in[0,T],\\ Y\_{T}&=k\_{1}X\_{T}+\int\_{0}^{T}\sin{V\_{t}}dt.\end{cases} |  | (2.15) |

Notice that, here œÉ3=s3‚â†0\sigma\_{3}=s\_{3}\neq 0 and g1=k1‚â†0g\_{1}=k\_{1}\neq 0 ensures that coefficient function œÉ\sigma is monotone in ZZ and gg is monotone in XX. And we can find positive constants c,‚Äãc2,c3c\_{,}c\_{2},c\_{3} in Assumption [2.1](https://arxiv.org/html/2511.08735v1#S2.Thmass1 "Assumption 2.1. ‚Ä£ 2.1 Notations and Assumptions ‚Ä£ 2 Preliminaries ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications") based on s3s\_{3} and k1k\_{1}. Also, coefficient functions b,f,œÉb,f,\sigma and gg satisfy corresponding conditions from Assumptions [2.2](https://arxiv.org/html/2511.08735v1#S2.Thmass2 "Assumption 2.2. ‚Ä£ 2.1 Notations and Assumptions ‚Ä£ 2 Preliminaries ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")-[2.4](https://arxiv.org/html/2511.08735v1#S2.Thmass4 "Assumption 2.4. ‚Ä£ 2.1 Notations and Assumptions ‚Ä£ 2 Preliminaries ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications").

### 2.2 Solutions to FBSDEs

The following result about the well-posedness of non-Markovian coupled FBSDEs owes to Ma et al. [JMaZWuDZhangJZhang2015OnWellPossedofFBSDEsAUnifiedAppr].

###### Theorem 2.2 (Wellpossedness of FBSDE).

Suppose all the processes involved are one-dimensional and that Assumptions [2.1](https://arxiv.org/html/2511.08735v1#S2.Thmass1 "Assumption 2.1. ‚Ä£ 2.1 Notations and Assumptions ‚Ä£ 2 Preliminaries ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications") -[2.3](https://arxiv.org/html/2511.08735v1#S2.Thmass3 "Assumption 2.3. ‚Ä£ 2.1 Notations and Assumptions ‚Ä£ 2 Preliminaries ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications") hold. Then it holds that

1. (1)

   FBSDE ([1.1](https://arxiv.org/html/2511.08735v1#S1.E1 "In 1 Introduction ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) admits a unique solution (X,Y,Z)(X,Y,Z), and there exists a constant C>0C>0,depending only on TT, the Lipschitz constants in Assumption [2.3](https://arxiv.org/html/2511.08735v1#S2.Thmass3 "Assumption 2.3. ‚Ä£ 2.1 Notations and Assumptions ‚Ä£ 2 Preliminaries ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications"), and c1,c2,c3c\_{1},c\_{2},c\_{3}, such that

   |  |  |  |  |
   | --- | --- | --- | --- |
   |  | ùîº‚Äã[sup0‚â§t‚â§T(|Xt|2+|Yt|2)+‚à´0T|Zt|2‚Äãùëët]‚â§C‚Äã(ùîº‚Äã|x0|2+I02);\mathbb{E}\bigg[\sup\_{0\leq t\leq T}\bigg(|X\_{t}|^{2}+|Y\_{t}|^{2}\bigg)+\int\_{0}^{T}|Z\_{t}|^{2}dt\bigg]\leq C\big(\mathbb{E}|x\_{0}|^{2}+I\_{0}^{2}\big); |  | (2.16) |
2. (2)

   FBSDE ([1.1](https://arxiv.org/html/2511.08735v1#S1.E1 "In 1 Introduction ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) possesses a decoupling field uu such that Yt=u‚Äã(t,Xt)Y\_{t}=u(t,X\_{t}) and u1:=u‚Äã(t,x1)‚àíu‚Äã(t,x2)x1‚àíx2u\_{1}:=\frac{u(t,x\_{1})-u(t,x\_{2})}{x\_{1}-x\_{2}} satisfies the corresponding property of g1g\_{1} with c2c\_{2} being replaced by c3c\_{3} in Assumption [2.1](https://arxiv.org/html/2511.08735v1#S2.Thmass1 "Assumption 2.1. ‚Ä£ 2.1 Notations and Assumptions ‚Ä£ 2 Preliminaries ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications").

###### Remark 2.1.

Here, we would like to clarify the property of the decoupling field uu in terms of the difference defined as u1u\_{1}. Recall that, for the existence of the unique solution of FBSDE ([1.1](https://arxiv.org/html/2511.08735v1#S1.E1 "In 1 Introduction ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")), we need to satisfy only one of the three cases in Assumption [2.1](https://arxiv.org/html/2511.08735v1#S2.Thmass1 "Assumption 2.1. ‚Ä£ 2.1 Notations and Assumptions ‚Ä£ 2 Preliminaries ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications"). And corresponding to each of these cases, the resulting decoupling field will satisfy the following:

1. (Case I)

   |u1|‚â§c3\left|{u\_{1}}\right|\leq c\_{3}.
2. (Case II)

   |u1|‚â•c3‚àí1\left|{u\_{1}}\right|\geq c\_{3}^{-1}, and u1u\_{1} keeps the same sign, meaning uu is monotone with respect to XX.
3. (Case III)

   œÉ3‚Äãu1‚â§c1‚Äãc3\sigma\_{3}u\_{1}\leq c\_{1}c\_{3}, and if gg is monotone with respect to XX then uu is also monotone with respect to XX.

Next, we state and prove the following continuity result.

###### Theorem 2.3 (Continuity of Solution).

Under the assumptions of Theorem [2.2](https://arxiv.org/html/2511.08735v1#S2.Thmthm2 "Theorem 2.2 (Wellpossedness of FBSDE). ‚Ä£ 2.2 Solutions to FBSDEs ‚Ä£ 2 Preliminaries ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications") we have

|  |  |  |
| --- | --- | --- |
|  | supiùîº‚Äã[supt‚àà[ti,ti+1](|Xt‚àíXti|2+|Yt‚àíYti|2)]\displaystyle\sup\_{i}\mathbb{E}\bigg[\sup\_{t\in[t\_{i},t\_{i+1}]}\bigg(|X\_{t}-X\_{t\_{i}}|^{2}+|Y\_{t}-Y\_{t\_{i}}|^{2}\bigg)\bigg] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚â§C‚Äã(I02+ùîº‚Äã|x0|2)‚Äãh+C‚Äãsupiùîº‚Äã[‚à´titi+1|Zs|2‚Äãùëës+‚à´titi+1|œÉ|2‚Äã(s,Vs,0,0,0)‚Äãùëës].\displaystyle\leq C\big(I\_{0}^{2}+\mathbb{E}|x\_{0}|^{2}\big)h+C\sup\_{i}\mathbb{E}\left[\int\_{t\_{i}}^{t\_{i+1}}|Z\_{s}|^{2}ds+\int\_{t\_{i}}^{t\_{i+1}}|\sigma|^{2}(s,V\_{s},0,0,0)ds\right]. |  | (2.17) |

###### Proof.

Assume 0‚â§v‚â§t‚â§T0\leq v\leq t\leq T. From FBSDE ([1.1](https://arxiv.org/html/2511.08735v1#S1.E1 "In 1 Introduction ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")), we have the following truncated form:

|  |  |  |  |
| --- | --- | --- | --- |
|  | {Xt=Xv+‚à´vtb‚Äã(s,Vs,Xs,Ys,Zs)‚Äãùëës+‚à´vtœÉ‚Äã(s,Vs,Xs,Ys,Zs)‚ÄãùëëWs,Yv=Yt+‚à´vtf‚Äã(s,Vs,Xs,Ys,Zs)‚Äãùëës‚àí‚à´vtZs‚ÄãùëëWs.\begin{cases}X\_{t}&=X\_{v}+\int\_{v}^{t}b(s,V\_{s},X\_{s},Y\_{s},Z\_{s})ds+\int\_{v}^{t}\sigma(s,V\_{s},X\_{s},Y\_{s},Z\_{s})dW\_{s},\\ Y\_{v}&=Y\_{t}+\int\_{v}^{t}f(s,V\_{s},X\_{s},Y\_{s},Z\_{s})ds-\int\_{v}^{t}Z\_{s}dW\_{s}.\end{cases} |  | (2.18) |

For the forward component, we estimate

|  |  |  |
| --- | --- | --- |
|  | |Xt‚àíXv|2=|‚à´vtb‚Äã(s,Vs,Xs,Ys,Zs)‚Äãùëës|2+|‚à´vtœÉ‚Äã(s,Vs,Xs,Ys,Zs)‚ÄãùëëWs|2+2‚Äã[‚à´vtb‚Äã(s,Vs,Xs,Ys,Zs)‚Äãùëës‚ãÖ‚à´vtœÉ‚Äã(s,Vs,Xs,Ys,Zs)‚ÄãùëëWs]\begin{split}|X\_{t}-X\_{v}|^{2}&=\bigg|\int\_{v}^{t}b(s,V\_{s},X\_{s},Y\_{s},Z\_{s})ds\bigg|^{2}+\bigg|\int\_{v}^{t}\sigma(s,V\_{s},X\_{s},Y\_{s},Z\_{s})dW\_{s}\bigg|^{2}\\ &+2\bigg[\int\_{v}^{t}b(s,V\_{s},X\_{s},Y\_{s},Z\_{s})ds\cdot\int\_{v}^{t}\sigma(s,V\_{s},X\_{s},Y\_{s},Z\_{s})dW\_{s}\bigg]\end{split} |  |

Applying Young‚Äôs inequality gives

|  |  |  |  |
| --- | --- | --- | --- |
|  | |Xt‚àíXv|2‚â§2‚Äã|‚à´vtb‚Äã(s,Vs,Xs,Ys,Zs)‚Äãùëës|2+2‚Äã|‚à´vtœÉ‚Äã(s,Vs,Xs,Ys,Zs)‚ÄãùëëWs|2|X\_{t}-X\_{v}|^{2}\leq 2\bigg|\int\_{v}^{t}b(s,V\_{s},X\_{s},Y\_{s},Z\_{s})ds\bigg|^{2}+2\bigg|\int\_{v}^{t}\sigma(s,V\_{s},X\_{s},Y\_{s},Z\_{s})dW\_{s}\bigg|^{2} |  | (2.19) |

By the Cauchy-Schwarz inequality and the estimates from Theorem [2.2](https://arxiv.org/html/2511.08735v1#S2.Thmthm2 "Theorem 2.2 (Wellpossedness of FBSDE). ‚Ä£ 2.2 Solutions to FBSDEs ‚Ä£ 2 Preliminaries ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications"), we obtain

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Äã[supœÑ‚àà[v,t]|‚à´vœÑb‚Äã(s,Vs,Xs,Ys,Zs)‚Äãùëës|2]‚â§ùîº‚Äã[‚à´vt12‚Äãùëës‚ãÖ‚à´vt|b‚Äã(s,Vs,Xs,Ys,Zs)|2‚Äãùëës]‚â§(t‚àív)‚Äãùîº‚Äã‚à´vt2‚Äã(|b‚Äã(s,Vs,0,0,0)|2+K‚Äã|Xs|2+by‚Äã|Ys|2+bz‚Äã|Zs|2)‚Äãùëës‚â§C‚Äã(I02+ùîº‚Äã|x0|2)‚Äã(t‚àív)+C‚Äã(t‚àív)‚Äãùîº‚Äã‚à´vt|Zs|2‚Äãùëës.\begin{split}\mathbb{E}\Bigg[\sup\_{\tau\in[v,t]}\bigg|\int\_{v}^{\tau}b(s,V\_{s},X\_{s},Y\_{s},Z\_{s})ds\bigg|^{2}\Bigg]&\leq\mathbb{E}\bigg[\int\_{v}^{t}1^{2}ds\cdot\int\_{v}^{t}|b(s,V\_{s},X\_{s},Y\_{s},Z\_{s})|^{2}ds\bigg]\\ &\leq(t-v)\mathbb{E}\int\_{v}^{t}2\bigg(|b(s,V\_{s},0,0,0)|^{2}+K|X\_{s}|^{2}+b\_{y}|Y\_{s}|^{2}+b\_{z}|Z\_{s}|^{2}\bigg)ds\\ &\leq C\big(I\_{0}^{2}+\mathbb{E}|x\_{0}|^{2}\big)(t-v)+C(t-v)\mathbb{E}\int\_{v}^{t}|Z\_{s}|^{2}ds.\end{split} |  |

For the stochastic integral, Doob‚Äôs inequality yields

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Äã[supœÑ‚àà[v,t]|‚à´vœÑœÉ‚Äã(s,Vs,Xs,Ys,Zs)‚ÄãùëëWs|2]\displaystyle\mathbb{E}\Bigg[\sup\_{\tau\in[v,t]}\bigg|\int\_{v}^{\tau}\sigma(s,V\_{s},X\_{s},Y\_{s},Z\_{s})dW\_{s}\bigg|^{2}\Bigg] |  |
|  |  |  |
| --- | --- | --- |
|  | ‚â§C‚Äãùîº‚Äã‚à´vt|œÉ‚Äã(s,Vs,Xs,Ys,Zs)|2‚Äãùëës\displaystyle\leq C\mathbb{E}\int\_{v}^{t}|\sigma(s,V\_{s},X\_{s},Y\_{s},Z\_{s})|^{2}ds |  |
|  |  |  |
| --- | --- | --- |
|  | ‚â§C‚Äãùîº‚Äã‚à´vt(2‚Äã|œÉ‚Äã(s,Vs,0,0,0)|2+œÉx‚Äã|Xs|2+œÉy‚Äã|Ys|2+œÉz‚Äã|Zs|2)‚Äãùëës\displaystyle\leq C\mathbb{E}\int\_{v}^{t}\bigg(2|\sigma(s,V\_{s},0,0,0)|^{2}+\sigma\_{x}|X\_{s}|^{2}+\sigma\_{y}|Y\_{s}|^{2}+\sigma\_{z}|Z\_{s}|^{2}\bigg)ds |  |
|  |  |  |
| --- | --- | --- |
|  | ‚â§C‚Äã(I02+ùîº‚Äã|x0|2)‚Äã(t‚àív)+C‚Äãùîº‚Äã[‚à´vt|Zs|2‚Äãùëës+‚à´vt|œÉ|2‚Äã(s,Vs,0,0,0)‚Äãùëës].\displaystyle\leq C\big(I\_{0}^{2}+\mathbb{E}|x\_{0}|^{2}\big)(t-v)+C\mathbb{E}\left[\int\_{v}^{t}|Z\_{s}|^{2}ds+\int\_{v}^{t}|\sigma|^{2}(s,V\_{s},0,0,0)ds\right]. |  |

Taking the supremum over œÑ‚àà[v,t]\tau\in[v,t] and then expectation on both sides of ([2.19](https://arxiv.org/html/2511.08735v1#S2.E19 "In 2.2 Solutions to FBSDEs ‚Ä£ 2 Preliminaries ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")), and finally using the above estimates, we conclude that

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Äã[supœÑ‚àà[v,t]|XœÑ‚àíXv|2]‚â§C‚Äã(I02+ùîº‚Äã|x0|2)‚Äã(t‚àív)+C‚Äãùîº‚Äã[‚à´vt|Zs|2‚Äãùëës+‚à´vt|œÉ|2‚Äã(s,Vs,0,0,0)‚Äãùëës].\mathbb{E}\Bigg[\sup\_{\tau\in[v,t]}|X\_{\tau}-X\_{v}|^{2}\Bigg]\leq C\big(I\_{0}^{2}+\mathbb{E}|x\_{0}|^{2}\big)(t-v)+C\mathbb{E}\left[\int\_{v}^{t}|Z\_{s}|^{2}ds+\int\_{v}^{t}|\sigma|^{2}(s,V\_{s},0,0,0)ds\right]. |  |

In particular, for any i‚àà{0,‚ãØ,N‚àí1}i\in\{0,\cdots,N-1\},

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Äã[supt‚àà[ti,ti+1]|Xt‚àíXti|2]‚â§C‚Äã(I02+ùîº‚Äã|x0|2)‚Äãh+C‚Äãùîº‚Äã[‚à´titi+1|Zs|2‚Äãùëës+‚à´titi+1|œÉ|2‚Äã(s,Vs,0,0,0)‚Äãùëës],\mathbb{E}\bigg[\sup\_{t\in[t\_{i},t\_{i+1}]}|X\_{t}-X\_{t\_{i}}|^{2}\bigg]\leq C\big(I\_{0}^{2}+\mathbb{E}|x\_{0}|^{2}\big)h+C\mathbb{E}\left[\int\_{t\_{i}}^{t\_{i+1}}|Z\_{s}|^{2}ds+\int\_{t\_{i}}^{t\_{i+1}}|\sigma|^{2}(s,V\_{s},0,0,0)ds\right], |  |

and hence,

|  |  |  |  |
| --- | --- | --- | --- |
|  | supiùîº‚Äã[supt‚àà[ti,ti+1]|Xt‚àíXti|2]‚â§C‚Äã(I02+ùîº‚Äã|x0|2)‚Äãh+C‚Äãùîº‚Äãsupi[‚à´titi+1|Zs|2‚Äãùëës+‚à´titi+1|œÉ|2‚Äã(s,Vs,0,0,0)‚Äãùëës].\sup\_{i}\mathbb{E}\bigg[\sup\_{t\in[t\_{i},t\_{i+1}]}|X\_{t}-X\_{t\_{i}}|^{2}\bigg]\leq C\big(I\_{0}^{2}+\mathbb{E}|x\_{0}|^{2}\big)h+C\mathbb{E}\sup\_{i}\left[\int\_{t\_{i}}^{t\_{i+1}}|Z\_{s}|^{2}ds+\int\_{t\_{i}}^{t\_{i+1}}|\sigma|^{2}(s,V\_{s},0,0,0)ds\right]. |  | (2.20) |

  

For the backward equation in ([2.18](https://arxiv.org/html/2511.08735v1#S2.E18 "In 2.2 Solutions to FBSDEs ‚Ä£ 2 Preliminaries ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")), we similarly estimate

|  |  |  |
| --- | --- | --- |
|  | |Yt‚àíYv|2‚â§|‚à´vtf‚Äã(s,Vs,Xs,Ys,Zs)‚Äãùëës|2+|‚à´vtZs‚ÄãùëëWs|2+2[|‚à´vtf(s,Vs,Xs,Ys,Zs)ds|.|‚à´vtZsdWs|]‚â§2‚Äã|‚à´vtf‚Äã(s,Vs,Xs,Ys,Zs)‚Äãùëës|2+2‚Äã|‚à´vtZs‚ÄãùëëWs|2.\begin{split}|Y\_{t}-Y\_{v}|^{2}&\leq\bigg|\int\_{v}^{t}f(s,V\_{s},X\_{s},Y\_{s},Z\_{s})ds\bigg|^{2}+\bigg|\int\_{v}^{t}Z\_{s}dW\_{s}\bigg|^{2}\\ &+2\bigg[\bigg|\int\_{v}^{t}f(s,V\_{s},X\_{s},Y\_{s},Z\_{s})ds\bigg|.\bigg|\int\_{v}^{t}Z\_{s}dW\_{s}\bigg|\bigg]\\ &\leq 2\bigg|\int\_{v}^{t}f(s,V\_{s},X\_{s},Y\_{s},Z\_{s})ds\bigg|^{2}+2\bigg|\int\_{v}^{t}Z\_{s}dW\_{s}\bigg|^{2}.\end{split} |  |

Using arguments similar to those for the forward component, we obtain

|  |  |  |  |
| --- | --- | --- | --- |
|  | supiùîº‚Äã[supt‚àà[ti,ti+1]|Yt‚àíYti|2]‚â§C‚Äã(I02+ùîº‚Äã|x0|2)‚Äãh+C‚Äãsupiùîº‚Äã‚à´titi+1|Zs|2‚Äãùëës.\sup\_{i}\mathbb{E}\bigg[\sup\_{t\in[t\_{i},t\_{i+1}]}|Y\_{t}-Y\_{t\_{i}}|^{2}\bigg]\leq C\big(I\_{0}^{2}+\mathbb{E}|x\_{0}|^{2}\big)h+C\sup\_{i}\mathbb{E}\int\_{t\_{i}}^{t\_{i+1}}|Z\_{s}|^{2}ds. |  | (2.21) |

This completes the proof.
‚àé

We now consider the Euler discretization scheme for the coupled FBSDE ([1.1](https://arxiv.org/html/2511.08735v1#S1.E1 "In 1 Introduction ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")):

|  |  |  |  |
| --- | --- | --- | --- |
|  | {Xti+1œÄ=XtiœÄ+b‚Äã(ti,Vti,XtiœÄ,YtiœÄ,ZtiœÄ)‚ÄãŒî‚Äãti+œÉ‚Äã(ti,Vti,XtiœÄ,YtiœÄ,ZtiœÄ)‚ÄãŒî‚ÄãWti,Yti+1œÄ=YtiœÄ‚àíf‚Äã(ti,Vti,XtiœÄ,YtiœÄ,ZtiœÄ)‚ÄãŒî‚Äãti+ZtiœÄ‚ÄãŒî‚ÄãWti,\displaystyle\begin{cases}X^{\pi}\_{t\_{i+1}}&=X^{\pi}\_{t\_{i}}+b(t\_{i},V\_{t\_{i}},X^{\pi}\_{t\_{i}},Y^{\pi}\_{t\_{i}},Z^{\pi}\_{t\_{i}})\Delta t\_{i}+\sigma(t\_{i},V\_{t\_{i}},X^{\pi}\_{t\_{i}},Y^{\pi}\_{t\_{i}},Z^{\pi}\_{t\_{i}})\Delta W\_{t\_{i}},\\ Y^{\pi}\_{t\_{i+1}}&=Y^{\pi}\_{t\_{i}}-f(t\_{i},V\_{t\_{i}},X^{\pi}\_{t\_{i}},Y^{\pi}\_{t\_{i}},Z^{\pi}\_{t\_{i}})\Delta t\_{i}+Z^{\pi}\_{t\_{i}}\Delta W\_{t\_{i}},\end{cases} |  | (2.22) |

where we use the forward representation for the backward SDE and Œî‚ÄãWti:=Wti+1‚àíWti\Delta W\_{t\_{i}}:=W\_{t\_{i+1}}-W\_{t\_{i}}. Taking conditional expectations with respect to ‚Ñ±ti\mathcal{F}\_{t\_{i}} on both sides of second equation in ([2.22](https://arxiv.org/html/2511.08735v1#S2.E22 "In 2.2 Solutions to FBSDEs ‚Ä£ 2 Preliminaries ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")), we obtain

|  |  |  |
| --- | --- | --- |
|  | YtiœÄ=ùîº‚Äã[Yti+1œÄ+f‚Äã(ti,vti,XtiœÄ,YtiœÄ,ZtiœÄ)‚ÄãŒî‚Äãti|‚Ñ±ti].Y^{\pi}\_{t\_{i}}=\mathbb{E}\left[Y^{\pi}\_{t\_{i+1}}+f(t\_{i},v\_{t\_{i}},X^{\pi}\_{t\_{i}},Y^{\pi}\_{t\_{i}},Z^{\pi}\_{t\_{i}})\Delta t\_{i}|\mathcal{F}\_{t\_{i}}\right]. |  |

Multiplying Œî‚ÄãWti\Delta W\_{t\_{i}} on both sides of the second equation in ([2.22](https://arxiv.org/html/2511.08735v1#S2.E22 "In 2.2 Solutions to FBSDEs ‚Ä£ 2 Preliminaries ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) and again taking conditional expectations with respect to ‚Ñ±ti\mathcal{F}\_{t\_{i}} again, we have:

|  |  |  |
| --- | --- | --- |
|  | ZtiœÄ=1Œî‚Äãti‚Äãùîº‚Äã[Yti+1œÄ‚ÄãŒî‚ÄãWti|‚Ñ±ti].Z\_{t\_{i}}^{\pi}=\frac{1}{\Delta t\_{i}}\mathbb{E}\left[Y^{\pi}\_{t\_{i+1}}\Delta W\_{t\_{i}}|\mathcal{F}\_{t\_{i}}\right]. |  |

Inspired by the above observations and in the spirit of [JZhang2004NumericalSchemeBSDEs], we propose the following discrete approximation scheme:

|  |  |  |  |
| --- | --- | --- | --- |
|  | {X¬Ø0œÄ=x0,X¬Øti+1œÄ=X¬ØtiœÄ+b‚Äã(ti,Vti,X¬ØtiœÄ,Y¬ØtiœÄ,Z¬ØtiœÄ)‚Äãh+œÉ‚Äã(ti,Vti,X¬ØtiœÄ,Y¬ØtiœÄ,Z¬ØtiœÄ)‚ÄãŒî‚ÄãWti,Y¬ØtNœÄ=g‚Äã(VT,X¬ØtNœÄ),Z¬ØtiœÄ=1h‚Äãùîº‚Äã[Y¬Øti+1œÄ‚ÄãŒî‚ÄãWti|‚Ñ±ti],Y¬ØtiœÄ=ùîº‚Äã[Y¬Øti+1œÄ+f‚Äã(ti,Vti,X¬ØtiœÄ,Y¬ØtiœÄ,Z¬ØtiœÄ)‚Äãh|‚Ñ±ti].\left\{\begin{array}[]{l}\overline{X}\_{0}^{\pi}=x\_{0},\\ \overline{X}\_{t\_{i+1}}^{\pi}=\overline{X}\_{t\_{i}}^{\pi}+b\big(t\_{i},V\_{t\_{i}},\overline{X}\_{t\_{i}}^{\pi},\overline{Y}\_{t\_{i}}^{\pi},\overline{Z}\_{t\_{i}}^{\pi}\big)h+\sigma\big(t\_{i},V\_{t\_{i}},\overline{X}\_{t\_{i}}^{\pi},\overline{Y}\_{t\_{i}}^{\pi},\overline{Z}\_{t\_{i}}^{\pi}\big)\Delta W\_{t\_{i}},\\ \overline{Y}\_{t\_{N}}^{\pi}=g(V\_{T},\overline{X}\_{t\_{N}}^{\pi}),\\ \overline{Z}\_{t\_{i}}^{\pi}=\frac{1}{h}\mathbb{E}\big[\overline{Y}\_{t\_{i+1}}^{\pi}\Delta W\_{t\_{i}}\big|\mathcal{F}\_{t\_{i}}\big],\\ \overline{Y}\_{t\_{i}}^{\pi}=\mathbb{E}\bigg[\overline{Y}\_{t\_{i+1}}^{\pi}+f\big(t\_{i},V\_{t\_{i}},\overline{X}\_{t\_{i}}^{\pi},\overline{Y}\_{t\_{i}}^{\pi},\overline{Z}\_{t\_{i}}^{\pi}\big)h\big|\mathcal{F}\_{t\_{i}}\bigg].\end{array}\right. |  | (2.23) |

  

Our next theorem in this section provides an estimate for the difference between the solution of the continuous FBSDE system ([1.1](https://arxiv.org/html/2511.08735v1#S1.E1 "In 1 Introduction ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) and its discrete approximation ([2.23](https://arxiv.org/html/2511.08735v1#S2.E23 "In 2.2 Solutions to FBSDEs ‚Ä£ 2 Preliminaries ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")). Before stating the theorem, we present two auxiliary lemmas whose proofs, along with the proof of the theorem, are given in the appendix.

###### Lemma 2.4.

Fix ii. For l=1,2l=1,2, let

|  |  |  |
| --- | --- | --- |
|  | Xti+1l=Xtil+b‚Äã(ti,Vti,Xtil,Ytil,Ztil)‚Äãh+‚à´titi+1Œ±tl‚Äãùëët+œÉ‚Äã(ti,Vti,Xtil,Ytil,Ztil)‚ÄãŒî‚ÄãWti+‚à´titi+1Œ≤tl‚ÄãùëëWt,\displaystyle X\_{t\_{i+1}}^{l}=X\_{t\_{i}}^{l}+b(t\_{i},V\_{t\_{i}},X\_{t\_{i}}^{l},Y\_{t\_{i}}^{l},Z\_{t\_{i}}^{l})h+\int\_{t\_{i}}^{t\_{i+1}}\alpha\_{t}^{l}dt+\sigma(t\_{i},V\_{t\_{i}},X\_{t\_{i}}^{l},Y\_{t\_{i}}^{l},Z\_{t\_{i}}^{l})\Delta W\_{t\_{i}}+\int\_{t\_{i}}^{t\_{i+1}}\beta\_{t}^{l}dW\_{t}, |  |

where Xtil,Ytil,ZtilX\_{t\_{i}}^{l},Y\_{t\_{i}}^{l},Z\_{t\_{i}}^{l} are ‚Ñ±ti\mathcal{F}\_{t\_{i}}-measurable and Œ±l,Œ≤l\alpha^{l},\beta^{l} are (‚Ñ±t)t‚àà[0,T](\mathcal{F}\_{t})\_{t\in[0,T]}-adapted. Then for any Œªj>0,j=1,2\lambda\_{j}>0,\;j=1,2, it holds that

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùîºti‚Äã[|Œî‚ÄãXti+1|2]\displaystyle\mathbb{E}\_{t\_{i}}\big[|\Delta X\_{t\_{i+1}}|^{2}\big] | ‚â§(1+A1‚Äãh)‚Äã|Œî‚ÄãXti|2+A2‚Äãh‚Äã|Œî‚ÄãYti|2+A3‚Äãh‚Äã|Œî‚ÄãZti|2\displaystyle\leq(1+A\_{1}h)|\Delta X\_{t\_{i}}|^{2}+A\_{2}h|\Delta Y\_{t\_{i}}|^{2}+A\_{3}h|\Delta Z\_{t\_{i}}|^{2} |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | +(1+4Œª1+1Œª2+Œª2)‚Äãùîºti‚Äã[‚à´titi+1(|Œî‚ÄãŒ±t|2+|Œî‚ÄãŒ≤t|2)‚Äãùëët],\displaystyle+\bigg(1+\frac{4}{\lambda\_{1}}+\frac{1}{\lambda\_{2}}+\lambda\_{2}\bigg)\mathbb{E}\_{t\_{i}}\bigg[\int\_{t\_{i}}^{t\_{i+1}}\big(|\Delta\alpha\_{t}|^{2}+|\Delta\beta\_{t}|^{2}\big)dt\bigg], |  | (2.24) |

where the differences are defined as

|  |  |  |
| --- | --- | --- |
|  | Œî‚ÄãX:=X1‚àíX2,Œî‚ÄãY:=Y1‚àíY2,Œî‚ÄãZ:=Z1‚àíZ2,Œî‚ÄãŒ±:=Œ±1‚àíŒ±2,Œî‚ÄãŒ≤:=Œ≤1‚àíŒ≤2,\Delta X:=X^{1}-X^{2},\hskip 5.69054pt\Delta Y:=Y^{1}-Y^{2},\hskip 5.69054pt\Delta Z:=Z^{1}-Z^{2},\hskip 5.69054pt\Delta\alpha:=\alpha^{1}-\alpha^{2},\hskip 5.69054pt\Delta\beta:=\beta^{1}-\beta^{2}, |  |

and the constants A1,A2,A3A\_{1},A\_{2},A\_{3} are given by

|  |  |  |  |
| --- | --- | --- | --- |
|  | A1\displaystyle A\_{1} | =Œª1+(2Œª1+h+Œª2‚Äãh)‚ÄãK+(1+Œª1)‚ÄãœÉx,\displaystyle=\lambda\_{1}+\bigg(\frac{2}{\lambda\_{1}}+h+\lambda\_{2}h\bigg)K+(1+\lambda\_{1})\sigma\_{x}, |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | A2\displaystyle A\_{2} | =(2Œª1+h+Œª2‚Äãh)‚Äãby+(1+Œª1)‚ÄãœÉy,\displaystyle=\bigg(\frac{2}{\lambda\_{1}}+h+\lambda\_{2}h\bigg)b\_{y}+(1+\lambda\_{1})\sigma\_{y}, |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | A3\displaystyle A\_{3} | =(2Œª1+h+Œª2‚Äãh)‚Äãbz+(1+Œª1)‚ÄãœÉz.\displaystyle=\bigg(\frac{2}{\lambda\_{1}}+h+\lambda\_{2}h\bigg)b\_{z}+(1+\lambda\_{1})\sigma\_{z}. |  |

###### Lemma 2.5.

Fix ii. For l=1,2l=1,2, let

|  |  |  |
| --- | --- | --- |
|  | Ytil=Yti+1l+f‚Äã(ti,Vti,Xtil,Ytil,Z^til)‚Äãh+‚à´titi+1Œ≥tl‚Äãùëët‚àí‚à´titi+1Ztl‚ÄãùëëWt,\displaystyle Y\_{{t\_{i}}}^{l}=Y\_{t\_{i+1}}^{l}+f(t\_{i},V\_{t\_{i}},X\_{t\_{i}}^{l},Y\_{t\_{i}}^{l},\hat{Z}\_{t\_{i}}^{l})h+\int\_{t\_{i}}^{t\_{i+1}}\gamma\_{t}^{l}dt-\int\_{t\_{i}}^{t\_{i+1}}Z\_{t}^{l}dW\_{t}, |  |

where Xtil,Ytil,ZtilX\_{t\_{i}}^{l},Y\_{t\_{i}}^{l},Z\_{t\_{i}}^{l} are ‚Ñ±ti\mathcal{F}\_{t\_{i}}-measurable, Œ≥l\gamma^{l} is (‚Ñ±t)t‚àà[0,T](\mathcal{F}\_{t})\_{t\in[0,T]}-adapted and

|  |  |  |
| --- | --- | --- |
|  | Z^til:=1h‚Äãùîºti‚Äã[Yti+1l‚ÄãŒî‚ÄãWti].\hat{Z}\_{t\_{i}}^{l}:=\frac{1}{h}\mathbb{E}\_{t\_{i}}\big[Y\_{t\_{i+1}}^{l}\Delta W\_{t\_{i}}\big]. |  |

Then for any Œªj>0,j=3,4,5\lambda\_{j}>0,\;j=3,4,5, it holds that

|  |  |  |  |
| --- | --- | --- | --- |
|  | (1‚àíA6‚Äãh)‚Äã|Œî‚ÄãYti|2+A7‚Äãh‚Äã|Œî‚ÄãZ^ti|2\displaystyle(1-A\_{6}h)|\Delta Y\_{{t\_{i}}}|^{2}+A\_{7}h|\Delta\hat{Z}\_{{t\_{i}}}|^{2} | ‚â§eA4‚Äãh‚Äãùîºti‚Äã|Œî‚ÄãYti+1|2+A5‚Äãh‚Äã|Œî‚ÄãXti|2\displaystyle\leq e^{A\_{4}h}\mathbb{E}\_{t\_{i}}|\Delta Y\_{{t\_{i+1}}}|^{2}+A\_{5}h|\Delta X\_{t\_{i}}|^{2} |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | +(1+1Œª4‚Äãh+1Œª3)‚Äãùîºti‚Äã|‚à´titi+1Œî‚ÄãŒ≥t‚Äãùëët|2,\displaystyle+\left(1+\frac{1}{\lambda\_{4}h}+\frac{1}{\lambda\_{3}}\right)\mathbb{E}\_{t\_{i}}\bigg|\int\_{t\_{i}}^{t\_{i+1}}\Delta\gamma\_{t}dt\bigg|^{2}, |  | (2.25) |

where the differences are defined as

|  |  |  |
| --- | --- | --- |
|  | Œî‚ÄãX:=X1‚àíX2,Œî‚ÄãY:=Y1‚àíY2,Œî‚ÄãZ^:=Z^1‚àíZ^2,Œî‚ÄãŒ≥:=Œ≥1‚àíŒ≥2,\Delta X:=X^{1}-X^{2},\hskip 5.69054pt\Delta Y:=Y^{1}-Y^{2},\hskip 5.69054pt\Delta\hat{Z}:=\hat{Z}^{1}-\hat{Z}^{2},\hskip 5.69054pt\Delta\gamma:=\gamma^{1}-\gamma^{2}, |  |

and the constants A4,A5,A6,A7A\_{4},A\_{5},A\_{6},A\_{7} are given by

|  |  |  |  |
| --- | --- | --- | --- |
|  | A4\displaystyle A\_{4} | =1h‚Äãln‚Å°(1+A~4‚Äãh),\displaystyle=\frac{1}{h}\ln(1+\tilde{A}\_{4}h), |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | A~4\displaystyle\tilde{A}\_{4} | =Œª4+1Œª5‚Äã(1+Œª4‚Äãh),\displaystyle=\lambda\_{4}+\frac{1}{\lambda\_{5}}(1+\lambda\_{4}h), |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | A5\displaystyle A\_{5} | =(1+Œª4‚Äãh)‚Äãfx‚Äã(h+Œª5),\displaystyle=(1+\lambda\_{4}h)f\_{x}(h+\lambda\_{5}), |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | A6\displaystyle A\_{6} | =(1+Œª4‚Äãh)‚ÄãK‚Äã(h+Œª5),\displaystyle=(1+\lambda\_{4}h)K(h+\lambda\_{5}), |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | A7\displaystyle A\_{7} | =1‚àíA~7,\displaystyle=1-\tilde{A}\_{7}, |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | A~7\displaystyle\tilde{A}\_{7} | =Œª3+(1+Œª4‚Äãh)‚Äã(h+Œª5)‚Äãfz.\displaystyle=\lambda\_{3}+(1+\lambda\_{4}h)(h+\lambda\_{5})f\_{z}. |  |

###### Remark 2.2.

It is worth noting that, for i=1,2,‚ãØ,7i=1,2,\cdots,7, the limit A¬Øi=limh‚Üí0Ai\overline{A}\_{i}=\lim\_{h\rightarrow 0}A\_{i} exists and is finite, which will be useful in analyzing the behaviour of the scheme as the time step hh tends to zero.

###### Theorem 2.6.

Suppose that ({X¬ØtiœÄ}0‚â§i‚â§N,{Y¬ØtiœÄ}0‚â§i‚â§N,{Z¬ØtiœÄ}0‚â§i‚â§N‚àí1)\big(\{\overline{X}^{\pi}\_{t\_{i}}\}\_{0\leq i\leq N},\{\overline{Y}^{\pi}\_{t\_{i}}\}\_{0\leq i\leq N},\{\overline{Z}^{\pi}\_{t\_{i}}\}\_{0\leq i\leq N-1}\big) is a solution of ([2.23](https://arxiv.org/html/2511.08735v1#S2.E23 "In 2.2 Solutions to FBSDEs ‚Ä£ 2 Preliminaries ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) with X¬ØtiœÄ,Y¬ØtiœÄ,Z¬ØtiœÄ‚ààùïÉ2‚Äã(Œ©,‚Ñ±ti,‚Ñô)\overline{X}\_{t\_{i}}^{\pi},\overline{Y}\_{t\_{i}}^{\pi},\overline{Z}\_{t\_{i}}^{\pi}\in\mathbb{L}^{2}(\Omega,\mathcal{F}\_{t\_{i}},\mathbb{P}). Also assume that the Assumption [2.4](https://arxiv.org/html/2511.08735v1#S2.Thmass4 "Assumption 2.4. ‚Ä£ 2.1 Notations and Assumptions ‚Ä£ 2 Preliminaries ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications") and all the assumptions of Theorem [2.2](https://arxiv.org/html/2511.08735v1#S2.Thmthm2 "Theorem 2.2 (Wellpossedness of FBSDE). ‚Ä£ 2.2 Solutions to FBSDEs ‚Ä£ 2 Preliminaries ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications") hold true and there exist constants Œªi>0,i=1,2,3,4,5\lambda\_{i}>0,i=1,2,3,4,5 such that A6‚Äãh<1,A~7<1A\_{6}h<1,\tilde{A}\_{7}<1 and

|  |  |  |  |
| --- | --- | --- | --- |
|  | (A¬Ø2‚Äã(1‚àíe‚àí(A¬Ø1+A¬Ø4)‚ÄãT)A¬Ø1+A¬Ø4+A¬Ø3A¬Ø7)‚Äã(e(A¬Ø1+A¬Ø4+A¬Ø6)‚ÄãT‚Äãgx+A¬Ø5‚Äã(e(A¬Ø1+A¬Ø4+A¬Ø6)‚ÄãT‚àí1)A¬Ø1+A¬Ø4)<1.\bigg(\frac{\overline{A}\_{2}(1-e^{-(\overline{A}\_{1}+\overline{A}\_{4})T})}{\overline{A}\_{1}+\overline{A}\_{4}}+\frac{\overline{A}\_{3}}{\overline{A}\_{7}}\bigg)\bigg(e^{(\overline{A}\_{1}+\overline{A}\_{4}+\overline{A}\_{6})T}g\_{x}+\frac{\overline{A}\_{5}(e^{(\overline{A}\_{1}+\overline{A}\_{4}+\overline{A}\_{6})T}-1)}{\overline{A}\_{1}+\overline{A}\_{4}}\bigg)<1. |  | (2.26) |

Then we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | supt‚àà[0,T]ùîº‚Äã[|Xt‚àíX¬ØtœÄ|2+|Yt‚àíY¬ØtœÄ|2]+ùîº‚Äã‚à´0T|Z^t‚àíZ¬ØtœÄ|2‚Äãùëët‚â§C‚ÄãM,\begin{split}\sup\_{t\in[0,T]}\mathbb{E}\bigg[|X\_{t}-\overline{X}\_{t}^{\pi}|^{2}+|Y\_{t}-\overline{Y}\_{t}^{\pi}|^{2}\bigg]+\mathbb{E}\int\_{0}^{T}|\hat{Z}\_{t}-\overline{Z}\_{t}^{\pi}|^{2}dt&\leq CM,\end{split} |  | (2.27) |

where

|  |  |  |  |
| --- | --- | --- | --- |
|  | M=\displaystyle M= | (I02+ùîº‚Äã|x0|2)‚Äãh+œÅ‚Äã(h)+1h‚Äãsup0‚â§k‚â§N‚àí1ùîº‚Äã‚à´tktk+1|Zt‚àíZ~tk|2‚Äãùëët\displaystyle\big(I\_{0}^{2}+\mathbb{E}|x\_{0}|^{2}\big)h+\rho(h)+\frac{1}{h}\sup\_{0\leq k\leq N-1}\mathbb{E}\int\_{t\_{k}}^{t\_{k+1}}|Z\_{t}-\tilde{Z}\_{t\_{k}}|^{2}dt |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +sup0‚â§k‚â§N‚àí1ùîº‚Äã[‚à´tktk+1|Zt|2‚Äãùëët+‚à´tktk+1(|f|2+|œÉ|2)‚Äã(t,Vt,0,0,0)‚Äãùëët],\displaystyle+\sup\_{0\leq k\leq N-1}\mathbb{E}\left[\int\_{t\_{k}}^{t\_{k+1}}|Z\_{t}|^{2}dt+\int\_{t\_{k}}^{t\_{k+1}}\big(|f|^{2}+|\sigma|^{2}\big)(t,V\_{t},0,0,0)dt\right], |  |

and X¬ØtœÄ=X¬ØtiœÄ,Y¬ØtœÄ=Y¬ØtiœÄ,Z¬ØtœÄ=Z¬ØtiœÄ,Z^t=Z^ti\overline{X}\_{t}^{\pi}=\overline{X}\_{t\_{i}}^{\pi},\overline{Y}\_{t}^{\pi}=\overline{Y}\_{t\_{i}}^{\pi},\overline{Z}\_{t}^{\pi}=\overline{Z}\_{t\_{i}}^{\pi},\hat{Z}\_{t}=\hat{Z}\_{t\_{i}}, for t‚àà[ti,ti+1)t\in[t\_{i},t\_{i+1}) and
Z~ti:=1h‚Äãùîº‚Äã[‚à´titi+1Zt‚Äãùëët|‚Ñ±ti]\tilde{Z}\_{t\_{i}}:=\frac{1}{h}\mathbb{E}\bigg[\int\_{t\_{i}}^{t\_{i+1}}Z\_{t}dt\big|\mathcal{F}\_{t\_{i}}\bigg], Z^ti=1h‚Äãùîº‚Äã[Yti+1‚ÄãŒî‚ÄãWti|‚Ñ±ti]\hat{Z}\_{t\_{i}}=\frac{1}{h}\mathbb{E}\bigg[Y\_{t\_{i+1}}\Delta W\_{t\_{i}}\big|\mathcal{F}\_{t\_{i}}\bigg].

### 2.3 Neural Network Approximation

Deep learning offers a powerful framework for the approximation of high-dimensional functions. In this section, we describe the architecture of fully connected feedforward neural networks and recall the universal approximation theorem that underpins their theoretical capacity.

Consider a fully-connected feedforward neural network with input dimension of did^{i} and output dimension dod^{o}, consisting of J+1J+1 layers, where J+1‚àà‚Ñï‚àñ{1,2}J+1\in\mathbb{N}\setminus\{1,2\}. Let the number of neurons in each layer be denoted by knk\_{n} for n=0,‚Ä¶,Jn=0,\ldots,J, where k0=di‚Äãand‚ÄãkN=dok\_{0}=d^{i}\ \text{and}\ k\_{N}=d^{o}. For simplicity, assume that all hidden layers contain the same number of neurons, i.e., kn=kk\_{n}=k for all n=1,‚ãØ,J‚àí1n=1,\cdots,J-1.

The neural network defines a function from ‚Ñùdi\mathbb{R}^{d^{i}} to ‚Ñùdo\mathbb{R}^{d^{o}}, represented as a composition of affine transformations and activation functions:

|  |  |  |  |
| --- | --- | --- | --- |
|  | x‚àà‚Ñùdi‚Ü¶BJ‚àòœ±‚àòBJ‚àí1‚àò‚ãØ‚àòœ±‚àòB1‚Äã(x)‚àà‚Ñùdo.x\in\mathbb{R}^{d^{i}}\mapsto B\_{J}\circ\varrho\circ B\_{J-1}\circ\cdots\circ\varrho\circ B\_{1}(x)\in\mathbb{R}^{d^{o}}. |  | (2.28) |

Here, the composition is defined as f1‚àòf2‚Äã(x)=f1‚Äã(f2‚Äã(x))f\_{1}\circ f\_{2}(x)=f\_{1}(f\_{2}(x)). Each BnB\_{n} denotes an affine transformation corresponding to layer nn, defined as

|  |  |  |
| --- | --- | --- |
|  | Bn‚Äã(x)=ùí≤n‚Äãx+Œ≤n,B\_{n}(x)=\mathcal{W}\_{n}x+\beta\_{n}, |  |

where ùí≤n\mathcal{W}\_{n} is the weight matrix and Œ≤n\beta\_{n} is the bias vector of the nnth layer of the network. Specifically, B1:‚Ñùdi‚Ü¶‚Ñùk,BJ:‚Ñùk‚Ü¶‚Ñùdo,and‚ÄãBn:‚Ñùk‚Ü¶‚Ñùk,n=2,‚Ä¶,J‚àí1B\_{1}:\mathbb{R}^{d^{i}}\mapsto\mathbb{R}^{k},B\_{J}:\mathbb{R}^{k}\mapsto\mathbb{R}^{d^{o}},\ \text{and}\ B\_{n}:\mathbb{R}^{k}\mapsto\mathbb{R}^{k},n=2,\ldots,J-1. The activation function œ±\varrho is applied component-wise to the output of each hidden layer. The final layer does not include a nonlinearity and is taken to be the identity function.

Let Œ∏=(ùí≤n,Œ≤n)n=1J\theta=(\mathcal{W}\_{n},\beta\_{n})\_{n=1}^{J} denote the collection of all trainable parameters in the network. For a given architecture specified by (di,do,J,k(d^{i},d^{o},J,k, the total number of parameters in a network is given by

|  |  |  |
| --- | --- | --- |
|  | Jk=‚àën=0J‚àí1(kn+1)‚Äãkn+1=(di+1)‚Äãk+(k+1)‚Äãk‚Äã(J‚àí2)+(k+1)‚Äãdo,J\_{k}=\sum\_{n=0}^{J-1}(k\_{n}+1)k\_{n+1}=(d^{i}+1)k+(k+1)k(J-2)+(k+1)d^{o}, |  |

so that Œ∏‚àà‚ÑùJk\theta\in\mathbb{R}^{J\_{k}}. Let Œò\Theta be the set of all possible values of Œ∏\theta, and if there are no constraints on parameters, then Œò=‚ÑùJk\Theta=\mathbb{R}^{J\_{k}}. We denote the neural network function defined in ([2.28](https://arxiv.org/html/2511.08735v1#S2.E28 "In 2.3 Neural Network Approximation ‚Ä£ 2 Preliminaries ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) by ùí≥ùí©‚Äã(‚ãÖ;Œ∏)\mathcal{X}^{\mathcal{N}}(\cdot;\theta). The class of all such neural networks, with a fixed structure (di,do,J,k)(d^{i},d^{o},J,k) and activation function œ±\varrho is denoted by ùí©‚Äãùí©di,do,J,kœ±‚Äã(Œò)\mathcal{NN}\_{d^{i},d^{o},J,k}^{\varrho}(\Theta).

Here, we state the generalized universal approximation theorem by Bayer et al. [CBayerJQiuYYao2020PricingOptnRoughVolatBSPDEs], who extend the universal approximation theorem for the approximation of functions defined on a finite-dimensional space to random functions in an infinite-dimensional space.

###### Lemma 2.7.

For each T0‚àà(0,T],J‚àà‚Ñï+‚àñ{1},T\_{0}\in(0,T],J\in\mathbb{N}^{+}\setminus\{1\}, and di,do‚àà‚Ñï+d^{i},d^{o}\in\mathbb{N}^{+}, the function set

|  |  |  |
| --- | --- | --- |
|  | {Œ¶k‚Äã(Wt1,‚ãØ,Wti,x;Œ∏):Œ¶k‚Äã(‚ãÖ;Œ∏)‚ààùí©‚Äãùí©di+i,do,J,kœ±‚Äã(‚ÑùJk),k,i‚àà‚Ñï+,0<t1<t2‚Äã‚ãØ<ti‚â§T0}\displaystyle\big\{\Phi\_{k}(W\_{t\_{1}},\cdots,W\_{t\_{i}},x;\theta):\Phi\_{k}(\cdot;\theta)\in\mathcal{N}\mathcal{N}^{\varrho}\_{d^{i}+i,d^{o},J,k}(\mathbb{R}^{J\_{k}}),k,i\in\mathbb{N}^{+},0<t\_{1}<t\_{2}\cdots<t\_{i}\leq T\_{0}\big\} |  |

is dense in L2‚Äã(Œ©√ó‚Ñùdi,‚Ñ±T0‚äó‚Ñ¨‚Äã(‚Ñùdi),‚Ñô‚Äã(œâ)‚äód‚Äãx;‚Ñùdo)L^{2}(\Omega\times\mathbb{R}^{d^{i}},\mathcal{F}\_{T\_{0}}\otimes\mathcal{B}(\mathbb{R}^{d^{i}}),\mathbb{P}(\omega)\otimes dx;\mathbb{R}^{d^{o}}), whenever œ±\varrho is continuous and non-constant.

The above universal approximation theorem gives the justification to use neural network approximation to approximate the decoupling random field that appears in non-Markovian coupled FBSDEs.
As remarked in [CBayerJQiuYYao2020PricingOptnRoughVolatBSPDEs], the œÉ\sigma-algebra ‚Ñ±T0\mathcal{F}\_{T\_{0}} is generated by {Ws:s‚àà[0,T0]}\{W\_{s}:s\in[0,T\_{0}]\} where the process WW and accordingly the dependence of it in the assertion of Lemma [2.7](https://arxiv.org/html/2511.08735v1#S2.Thmthm7 "Lemma 2.7. ‚Ä£ 2.3 Neural Network Approximation ‚Ä£ 2 Preliminaries ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications") may be replaced by any continuous process.

## 3 Numerical Scheme and Simulation Error for Non-Markovian Coupled-FBSDE

Without loss of generality, we take d0=1d\_{0}=1 in what follows. Inspired by the approximation results in Lemma [2.7](https://arxiv.org/html/2511.08735v1#S2.Thmthm7 "Lemma 2.7. ‚Ä£ 2.3 Neural Network Approximation ‚Ä£ 2 Preliminaries ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications") under the non-Markovian framework, the backward processes (Yt,Zt)(Y\_{t},Z\_{t}) is to be expressed a function of the forward process XtX\_{t} and the historical paths {Ws,Vs:0‚â§s‚â§t}\{W\_{s},V\_{s}:0\leq s\leq t\} (or just {Ws:0‚â§s‚â§t}\{W\_{s}:0\leq s\leq t\} or {Vs:0‚â§s‚â§t}\{V\_{s}:0\leq s\leq t\}) because the randomness of the coefficients is from the continuous process {Vs:0‚â§s‚â§t}\{V\_{s}:0\leq s\leq t\} or essentially from the Wiener process {Ws:0‚â§s‚â§t}\{W\_{s}:0\leq s\leq t\}.

Accordingly, we reformulate the problem of solving the FBSDE ([1.1](https://arxiv.org/html/2511.08735v1#S1.E1 "In 1 Introduction ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) as the following stochastic optimization problem:

|  |  |  |  |
| --- | --- | --- | --- |
|  | infùí¥0,ùíµiùîè^‚Äã(ùí¥0,ùíµ0,‚ãØ,ùíµN‚àí1):=ùîº‚Äã|g‚Äã(VT,XTœÄ)‚àíYTœÄ|2,\displaystyle\inf\_{\mathcal{Y}\_{0},\mathcal{Z}\_{i}}\quad\widehat{\mathfrak{L}}(\mathcal{Y}\_{0},\mathcal{Z}\_{0},\cdots,\mathcal{Z}\_{N-1}):=\mathbb{E}|g(V\_{T},X^{\pi}\_{T})-Y^{\pi}\_{T}|^{2}, |  | (3.1) |

subject to,

|  |  |  |  |
| --- | --- | --- | --- |
|  | {X0œÄ=x0,Y0œÄ=ùí¥0‚Äã(x0),ZtiœÄ=ùíµi‚Äã(XtiœÄ,{Ws,Vs:0‚â§s‚â§ti}),Xti+1œÄ=XtiœÄ+b‚Äã(ti,Vti,XtiœÄ,YtiœÄ,ZtiœÄ)‚Äã‚ñ≥‚Äãti+œÉ‚Äã(ti,Vti,XtiœÄ,YtiœÄ,ZtiœÄ)‚Äã‚ñ≥‚ÄãWti,Yti+1œÄ=YtiœÄ‚àíf‚Äã(ti,Vti,XtiœÄ,YtiœÄ,ZtiœÄ)‚ÄãŒî‚Äãti+ZtiœÄ‚ÄãŒî‚ÄãWti.\begin{cases}X^{\pi}\_{0}=x\_{0},\quad Y^{\pi}\_{0}=\mathcal{Y}\_{0}(x\_{0}),\\ Z^{\pi}\_{t\_{i}}=\mathcal{Z}\_{i}\big(X^{\pi}\_{t\_{i}},\{W\_{s},V\_{s}:0\leq s\leq t\_{i}\}\big),\\ X^{\pi}\_{t\_{i+1}}=X^{\pi}\_{t\_{i}}+b(t\_{i},V\_{t\_{i}},X^{\pi}\_{t\_{i}},Y^{\pi}\_{t\_{i}},Z^{\pi}\_{t\_{i}})\triangle t\_{i}+\sigma(t\_{i},V\_{t\_{i}},X^{\pi}\_{t\_{i}},Y^{\pi}\_{t\_{i}},Z^{\pi}\_{t\_{i}})\triangle W\_{t\_{i}},\\ Y^{\pi}\_{t\_{i+1}}=Y^{\pi}\_{t\_{i}}-f(t\_{i},V\_{t\_{i}},X^{\pi}\_{t\_{i}},Y^{\pi}\_{t\_{i}},Z^{\pi}\_{t\_{i}})\Delta t\_{i}+Z^{\pi}\_{t\_{i}}\Delta W\_{t\_{i}}.\end{cases} |  | (3.2) |

Our next result, Theorem [3.2](https://arxiv.org/html/2511.08735v1#S3.Thmthm2 "Theorem 3.2. ‚Ä£ 3 Numerical Scheme and Simulation Error for Non-Markovian Coupled-FBSDE ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications") will show that the error between the solution (Xt,Yt,Zt)(X\_{t},Y\_{t},Z\_{t}), of the continuous FBSDE ([1.1](https://arxiv.org/html/2511.08735v1#S1.E1 "In 1 Introduction ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) and its discrete counterpart (XtœÄ,YtœÄ,ZtœÄ)(X^{\pi}\_{t},Y^{\pi}\_{t},Z^{\pi}\_{t}), defined in ([3.2](https://arxiv.org/html/2511.08735v1#S3.E2 "In 3 Numerical Scheme and Simulation Error for Non-Markovian Coupled-FBSDE ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")), is bounded by the objective function ùîº‚Äã|g‚Äã(VT,XTœÄ)‚àíYTœÄ|2\mathbb{E}|g(V\_{T},X^{\pi}\_{T})-Y^{\pi}\_{T}|^{2} of the above optimization problem.

Therefore, if the functions ùí¥0,ùíµ0,‚ãØ,ùíµN‚àí1\mathcal{Y}\_{0},\mathcal{Z}\_{0},\cdots,\mathcal{Z}\_{N-1} can be approximated using neural networks such that the objective function is minimized, ideally to zero, of the above optimization problem can be minimized to zero, then the resulting solution of the optimization problem will serve as an approximation to the solution of the original FBSDE ([1.1](https://arxiv.org/html/2511.08735v1#S1.E1 "In 1 Introduction ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")). Consequently, the formulation of the stochastic optimization problem in ([3.1](https://arxiv.org/html/2511.08735v1#S3.E1 "In 3 Numerical Scheme and Simulation Error for Non-Markovian Coupled-FBSDE ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications"))‚Äì([3.2](https://arxiv.org/html/2511.08735v1#S3.E2 "In 3 Numerical Scheme and Simulation Error for Non-Markovian Coupled-FBSDE ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")), along with the error estimates provided in the forthcoming theorem, forms the backbone of our proposed numerical scheme for solving non-Markovian, fully coupled-FBSDEs. A detailed implementation of this scheme will be presented in Section [5](https://arxiv.org/html/2511.08735v1#S5 "5 Algorithm for Coupled non-Markovian FBSDEs ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications").

Before we proceed to Theorem¬†[3.2](https://arxiv.org/html/2511.08735v1#S3.Thmthm2 "Theorem 3.2. ‚Ä£ 3 Numerical Scheme and Simulation Error for Non-Markovian Coupled-FBSDE ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications"), we must introduce and prove a key Lemma.

Inspired by the structure of the discretized system ([2.23](https://arxiv.org/html/2511.08735v1#S2.E23 "In 2.2 Solutions to FBSDEs ‚Ä£ 2 Preliminaries ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")), we consider the following system of equations:

|  |  |  |  |
| --- | --- | --- | --- |
|  | {X0œÄ=x0,Xti+1œÄ=XtiœÄ+b‚Äã(ti,Vti,XtiœÄ,YtiœÄ,ZtiœÄ)‚Äã‚ñ≥‚Äãti+œÉ‚Äã(ti,Vti,XtiœÄ,YtiœÄ,ZtiœÄ)‚Äã‚ñ≥‚ÄãWti,ZtiœÄ=1h‚Äãùîº‚Äã[Yti+1œÄ‚Äã‚ñ≥‚ÄãWti|‚Ñ±ti],YtiœÄ=ùîº‚Äã[Yti+1œÄ+f‚Äã(ti,Vti,XtiœÄ,YtiœÄ,ZtiœÄ)‚Äãh|‚Ñ±ti].\left\{\begin{array}[]{l}{X^{\pi}\_{0}=x\_{0}},\\ {X^{\pi}\_{t\_{i+1}}=X^{\pi}\_{t\_{i}}+b(t\_{i},V\_{t\_{i}},X^{\pi}\_{t\_{i}},Y^{\pi}\_{t\_{i}},Z^{\pi}\_{t\_{i}})\triangle t\_{i}+\sigma(t\_{i},V\_{t\_{i}},X^{\pi}\_{t\_{i}},Y^{\pi}\_{t\_{i}},Z^{\pi}\_{t\_{i}})\triangle W\_{t\_{i}}},\\ {Z^{\pi}\_{t\_{i}}=\frac{1}{h}\mathbb{E}[Y^{\pi}\_{t\_{i+1}}\triangle W\_{t\_{i}}|\mathcal{F}\_{t\_{i}}]},\\ {Y^{\pi}\_{t\_{i}}=\mathbb{E}[Y^{\pi}\_{t\_{i+1}}+f(t\_{i},V\_{t\_{i}},X^{\pi}\_{t\_{i}},Y^{\pi}\_{t\_{i}},Z^{\pi}\_{t\_{i}})h|\mathcal{F}\_{t\_{i}}]}.\end{array}\right. |  | (3.3) |

Note that the key distinction between ([2.23](https://arxiv.org/html/2511.08735v1#S2.E23 "In 2.2 Solutions to FBSDEs ‚Ä£ 2 Preliminaries ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) and ([3.3](https://arxiv.org/html/2511.08735v1#S3.E3 "In 3 Numerical Scheme and Simulation Error for Non-Markovian Coupled-FBSDE ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) lies in the absence of a specified terminal condition in ([3.3](https://arxiv.org/html/2511.08735v1#S3.E3 "In 3 Numerical Scheme and Simulation Error for Non-Markovian Coupled-FBSDE ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")), resulting in the system admitting infinitely many solutions. In particular, both (XœÄ,YœÄ,ZœÄ)(X^{\pi},Y^{\pi},Z^{\pi}), as defined in ([3.2](https://arxiv.org/html/2511.08735v1#S3.E2 "In 3 Numerical Scheme and Simulation Error for Non-Markovian Coupled-FBSDE ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) and (X¬ØœÄ,Y¬ØœÄ,Z¬ØœÄ)(\overline{X}^{\pi},\overline{Y}^{\pi},\overline{Z}^{\pi}) from ([2.23](https://arxiv.org/html/2511.08735v1#S2.E23 "In 2.2 Solutions to FBSDEs ‚Ä£ 2 Preliminaries ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")), satisfy the system ([3.3](https://arxiv.org/html/2511.08735v1#S3.E3 "In 3 Numerical Scheme and Simulation Error for Non-Markovian Coupled-FBSDE ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")).

###### Lemma 3.1.

Let j=1,2j=1,2, and suppose that ({XtiœÄ,j}0‚â§i‚â§N,{YtiœÄ,j}0‚â§i‚â§N,{ZtiœÄ,j}0‚â§i‚â§N‚àí1)\big(\{X^{\pi,j}\_{t\_{i}}\}\_{0\leq i\leq N},\{Y^{\pi,j}\_{t\_{i}}\}\_{0\leq i\leq N},\{Z^{\pi,j}\_{t\_{i}}\}\_{0\leq i\leq N-1}\big) are two solutions of the system ([3.3](https://arxiv.org/html/2511.08735v1#S3.E3 "In 3 Numerical Scheme and Simulation Error for Non-Markovian Coupled-FBSDE ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) with Xti+1œÄ,j,Yti+1œÄ,j‚ààùïÉ2‚Äã(Œ©,‚Ñ±ti+1,‚Ñô)X^{\pi,j}\_{t\_{i+1}},Y^{\pi,j}\_{t\_{i+1}}\in\mathbb{L}^{2}(\Omega,\mathcal{F}\_{t\_{i+1}},\mathbb{P}) for all 0‚â§i‚â§N‚àí10\leq i\leq N-1, j=1,2j=1,2. Let Œª0,Œª1>0,Œª2>fz\lambda\_{0},\lambda\_{1}>0,\lambda\_{2}>f\_{z}, and suppose hh is sufficiently small. Define the constants:

|  |  |  |  |
| --- | --- | --- | --- |
|  | A1\displaystyle A\_{1} | :=K‚Äãh+œÉx+2‚Äãkb+Œª0+Œª1,\displaystyle:=Kh+\sigma\_{x}+2k\_{b}+\lambda\_{0}+\lambda\_{1}, |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | A2\displaystyle A\_{2} | :=(h+Œª1‚àí1)‚Äãby+œÉy,\displaystyle:=(h+\lambda\_{1}^{-1})b\_{y}+\sigma\_{y}, |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | A3\displaystyle A\_{3} | :=(h+Œª0‚àí1)‚Äãbz+œÉz,\displaystyle:=(h+\lambda\_{0}^{-1})b\_{z}+\sigma\_{z}, |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | A4\displaystyle A\_{4} | :=‚àíh‚àí1‚Äãln‚Å°[1‚àí(2‚Äãkf+Œª2)‚Äãh],\displaystyle:=-h^{-1}\ln[1-(2k\_{f}+\lambda\_{2})h], |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | A5\displaystyle A\_{5} | :=fx‚ÄãŒª2‚àí1‚Äã(1‚àí(2‚Äãkf+Œª2)‚Äãh)‚àí1,\displaystyle:=f\_{x}\lambda\_{2}^{-1}\big(1-(2k\_{f}+\lambda\_{2})h\big)^{-1}, |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | A6\displaystyle A\_{6} | :=(1‚àífz‚ÄãŒª2‚àí1)‚Äã(1‚àí(2‚Äãkf+Œª2)‚Äãh)‚àí1.\displaystyle:=(1-f\_{z}\lambda\_{2}^{-1})\big(1-(2k\_{f}+\lambda\_{2})h\big)^{-1}. |  |

Let Œ¥‚ÄãXi=XtiœÄ,1‚àíXtiœÄ,2,Œ¥‚ÄãYi=YtiœÄ,1‚àíYtiœÄ,2\delta X\_{i}=X\_{t\_{i}}^{\pi,1}-X\_{t\_{i}}^{\pi,2},\delta Y\_{i}=Y\_{t\_{i}}^{\pi,1}-Y\_{t\_{i}}^{\pi,2}. Then for any 1‚â§n‚â§N1\leq n\leq N, the following estimates hold:

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Äã|Œ¥‚ÄãXn|2‚â§A2‚Äã‚àëi=0n‚àí1eA1‚Äã(n‚àíi‚àí1)‚Äãh‚Äãùîº‚Äã|Œ¥‚ÄãYi|2‚Äãh+A3‚Äã‚àëi=0n‚àí1eA1‚Äã(n‚àíi‚àí1)‚Äãh‚Äãùîº‚Äã|Œ¥‚ÄãZi|2‚Äãh,\displaystyle\mathbb{E}|\delta X\_{n}|^{2}\leq A\_{2}\sum\_{i=0}^{n-1}e^{A\_{1}(n-i-1)h}\mathbb{E}|\delta Y\_{i}|^{2}h+A\_{3}\sum\_{i=0}^{n-1}e^{A\_{1}(n-i-1)h}\mathbb{E}|\delta Z\_{i}|^{2}h, |  |

and

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Äã|Œ¥‚ÄãYn|2+‚àëi=nN‚àí1A6‚ÄãeA4‚Äã(i‚àín)‚Äãh‚Äãùîº‚Äã|Œ¥‚ÄãZi|2‚Äãh‚â§eA4‚Äã(N‚àín)‚Äãh‚Äãùîº‚Äã|Œ¥‚ÄãYN|2+A5‚Äã‚àëi=nN‚àí1eA4‚Äã(i‚àín)‚Äãh‚Äãùîº‚Äã|Œ¥‚ÄãXi|2‚Äãh.\displaystyle\mathbb{E}|\delta Y\_{n}|^{2}+\sum\_{i=n}^{N-1}A\_{6}e^{A\_{4}(i-n)h}\mathbb{E}|\delta Z\_{i}|^{2}h\leq e^{A\_{4}(N-n)h}\mathbb{E}|\delta Y\_{N}|^{2}+A\_{5}\sum\_{i=n}^{N-1}e^{A\_{4}(i-n)h}\mathbb{E}|\delta X\_{i}|^{2}h. |  |

###### Proof.

Define the following differences:

|  |  |  |  |
| --- | --- | --- | --- |
|  | {Œ¥‚ÄãZi=ZtiœÄ,1‚àíZtiœÄ,2,Œ¥‚Äãbi=b‚Äã(ti,Vti,XtiœÄ,1,YtiœÄ,1,ZtiœÄ,1)‚àíb‚Äã(ti,Vti,XtiœÄ,2,YtiœÄ,2,ZtiœÄ,2),Œ¥‚ÄãœÉi=œÉ‚Äã(ti,Vti,XtiœÄ,1,YtiœÄ,1,ZtiœÄ,1)‚àíœÉ‚Äã(ti,Vti,XtiœÄ,2,YtiœÄ,2,ZtiœÄ,2),Œ¥‚Äãfi=f‚Äã(ti,Vti,XtiœÄ,1,YtiœÄ,1,ZtiœÄ,1)‚àíf‚Äã(ti,Vti,XtiœÄ,2,YtiœÄ,2,ZtiœÄ,2).\left\{\begin{array}[]{l}{\delta Z\_{i}=Z^{\pi,1}\_{t\_{i}}-Z^{\pi,2}\_{t\_{i}}},\\ {\delta b\_{i}=b(t\_{i},V\_{t\_{i}},X^{\pi,1}\_{t\_{i}},Y^{\pi,1}\_{t\_{i}},Z^{\pi,1}\_{t\_{i}})-b(t\_{i},V\_{t\_{i}},X^{\pi,2}\_{t\_{i}},Y^{\pi,2}\_{t\_{i}},Z^{\pi,2}\_{t\_{i}})},\\ {\delta\sigma\_{i}=\sigma(t\_{i},V\_{t\_{i}},X^{\pi,1}\_{t\_{i}},Y^{\pi,1}\_{t\_{i}},Z^{\pi,1}\_{t\_{i}})-\sigma(t\_{i},V\_{t\_{i}},X^{\pi,2}\_{t\_{i}},Y^{\pi,2}\_{t\_{i}},Z^{\pi,2}\_{t\_{i}})},\\ {\delta f\_{i}=f(t\_{i},V\_{t\_{i}},X^{\pi,1}\_{t\_{i}},Y^{\pi,1}\_{t\_{i}},Z^{\pi,1}\_{t\_{i}})-f(t\_{i},V\_{t\_{i}},X^{\pi,2}\_{t\_{i}},Y^{\pi,2}\_{t\_{i}},Z^{\pi,2}\_{t\_{i}})}.\end{array}\right. |  | (3.4) |

From system ([3.3](https://arxiv.org/html/2511.08735v1#S3.E3 "In 3 Numerical Scheme and Simulation Error for Non-Markovian Coupled-FBSDE ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")), we then obtain the update rules:

|  |  |  |  |
| --- | --- | --- | --- |
|  | {Œ¥‚ÄãXi+1=Œ¥‚ÄãXti+Œ¥‚Äãbi‚Äãh+Œ¥‚ÄãœÉi‚Äã‚ñ≥‚ÄãWti,Œ¥‚ÄãZi=1h‚Äãùîº‚Äã[Œ¥‚ÄãYi+1‚Äã‚ñ≥‚ÄãWti|‚Ñ±ti],Œ¥‚ÄãYi=ùîº‚Äã[Œ¥‚ÄãYi+1+Œ¥‚Äãfi‚Äãh|‚Ñ±ti].\left\{\begin{array}[]{l}{\delta X\_{i+1}=\delta X\_{t\_{i}}+\delta b\_{i}h+\delta\sigma\_{i}\triangle W\_{t\_{i}}},\\ {\delta Z\_{i}=\frac{1}{h}\mathbb{E}[\delta Y\_{i+1}\triangle W\_{t\_{i}}|\mathcal{F}\_{t\_{i}}]},\\ {\delta Y\_{i}=\mathbb{E}[\delta Y\_{i+1}+\delta f\_{i}h|\mathcal{F}\_{t\_{i}}]}.\end{array}\right. |  | (3.5) |

Step 1: Estimate for Œ¥‚ÄãX\delta X
  
Squaring and then taking expectations on both sides of the first equation of ([3.5](https://arxiv.org/html/2511.08735v1#S3.E5 "In 3 Numerical Scheme and Simulation Error for Non-Markovian Coupled-FBSDE ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")), we get

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùîº‚Äã|Œ¥‚ÄãXi+1|2=ùîº‚Äã|Œ¥‚ÄãXi+Œ¥‚Äãbi‚Äãh|2+h‚Äãùîº‚Äã|Œ¥‚ÄãœÉi|2.\displaystyle\mathbb{E}|\delta X\_{i+1}|^{2}=\mathbb{E}|\delta X\_{i}+\delta b\_{i}h|^{2}+h\mathbb{E}|\delta\sigma\_{i}|^{2}. |  | (3.6) |

Using Assumptions [2.3](https://arxiv.org/html/2511.08735v1#S2.Thmass3 "Assumption 2.3. ‚Ä£ 2.1 Notations and Assumptions ‚Ä£ 2 Preliminaries ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications") and [2.4](https://arxiv.org/html/2511.08735v1#S2.Thmass4 "Assumption 2.4. ‚Ä£ 2.1 Notations and Assumptions ‚Ä£ 2 Preliminaries ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications"), and applying standard inequalities, we find that for any Œª0,Œª1>0\lambda\_{0},\lambda\_{1}>0,

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Äã|Œ¥‚ÄãXi+1|2‚àíùîº‚Äã|Œ¥‚ÄãXi|2\displaystyle\mathbb{E}|\delta X\_{i+1}|^{2}-\mathbb{E}|\delta X\_{i}|^{2} |  |
|  |  |  |
| --- | --- | --- |
|  | =ùîº‚Äã|Œ¥‚Äãbi|2‚Äãh2+h‚Äãùîº‚Äã|Œ¥‚ÄãœÉi|2+2‚Äãh‚Äãùîº‚Äã[Œ¥‚ÄãXi‚ãÖ(b‚Äã(ti,Vti,XtiœÄ,1,YtiœÄ,1,ZtiœÄ,1)‚àíb‚Äã(ti,Vti,XtiœÄ,2,YtiœÄ,2,ZtiœÄ,2))]\displaystyle=\mathbb{E}|\delta b\_{i}|^{2}h^{2}+h\mathbb{E}|\delta\sigma\_{i}|^{2}+2h\mathbb{E}\big[\delta X\_{i}\cdot(b(t\_{i},V\_{t\_{i}},X^{\pi,1}\_{t\_{i}},Y^{\pi,1}\_{t\_{i}},Z^{\pi,1}\_{t\_{i}})-b(t\_{i},V\_{t\_{i}},X^{\pi,2}\_{t\_{i}},Y^{\pi,2}\_{t\_{i}},Z^{\pi,2}\_{t\_{i}}))\big] |  |
|  |  |  |
| --- | --- | --- |
|  | ‚â§(K‚Äãùîº‚Äã|Œ¥‚ÄãXi|2+by‚Äãùîº‚Äã|Œ¥‚ÄãYi|2+bz‚Äãùîº‚Äã|Œ¥‚ÄãZi|2)‚Äãh2+(œÉx‚Äãùîº‚Äã|Œ¥‚ÄãXi|2+œÉy‚Äãùîº‚Äã|Œ¥‚ÄãYi|2+œÉz‚Äãùîº‚Äã|Œ¥‚ÄãZi|2)‚Äãh\displaystyle\leq\big(K\mathbb{E}|\delta X\_{i}|^{2}+b\_{y}\mathbb{E}|\delta Y\_{i}|^{2}+b\_{z}\mathbb{E}|\delta Z\_{i}|^{2}\big)h^{2}+\big(\sigma\_{x}\mathbb{E}|\delta X\_{i}|^{2}+\sigma\_{y}\mathbb{E}|\delta Y\_{i}|^{2}+\sigma\_{z}\mathbb{E}|\delta Z\_{i}|^{2}\big)h |  |
|  |  |  |
| --- | --- | --- |
|  | +2‚Äãh‚Äãkb‚Äãùîº‚Äã|Œ¥‚ÄãXi|2+2‚Äãh‚Äãùîº‚Äã[by‚Äã|Œ¥‚ÄãYi|2+bz‚Äã|Œ¥‚ÄãZi|2‚Äã|Œ¥‚ÄãXi|]\displaystyle\;+2hk\_{b}\mathbb{E}|\delta X\_{i}|^{2}+2h\mathbb{E}\bigg[\sqrt{b\_{y}|\delta Y\_{i}|^{2}+b\_{z}|\delta Z\_{i}|^{2}}\,\,|\delta X\_{i}|\bigg] |  |
|  |  |  |
| --- | --- | --- |
|  | ‚â§(K‚Äãùîº‚Äã|Œ¥‚ÄãXi|2+by‚Äãùîº‚Äã|Œ¥‚ÄãYi|2+bz‚Äãùîº‚Äã|Œ¥‚ÄãZi|2)‚Äãh2+(œÉx‚Äãùîº‚Äã|Œ¥‚ÄãXi|2+œÉy‚Äãùîº‚Äã|Œ¥‚ÄãYi|2+œÉz‚ÄãE‚Äã|Œ¥‚ÄãZi|2)‚Äãh\displaystyle\leq\big(K\mathbb{E}|\delta X\_{i}|^{2}+b\_{y}\mathbb{E}|\delta Y\_{i}|^{2}+b\_{z}\mathbb{E}|\delta Z\_{i}|^{2}\big)h^{2}+\big(\sigma\_{x}\mathbb{E}|\delta X\_{i}|^{2}+\sigma\_{y}\mathbb{E}|\delta Y\_{i}|^{2}+\sigma\_{z}E|\delta Z\_{i}|^{2}\big)h |  |
|  |  |  |
| --- | --- | --- |
|  | +2‚Äãh‚Äãkb‚Äãùîº‚Äã|Œ¥‚ÄãXi|2+2‚Äãh‚Äãùîº‚Äã[by‚Äã|Œ¥‚ÄãYi|2‚Äã|Œ¥‚ÄãXi|2+bz‚Äã|Œ¥‚ÄãZi|2‚Äã|Œ¥‚ÄãXi|2]\displaystyle\;+2hk\_{b}\mathbb{E}|\delta X\_{i}|^{2}+2h\mathbb{E}\bigg[\sqrt{b\_{y}|\delta Y\_{i}|^{2}|\delta X\_{i}|^{2}}+\sqrt{b\_{z}|\delta Z\_{i}|^{2}|\delta X\_{i}|^{2}}\bigg] |  |
|  |  |  |
| --- | --- | --- |
|  | ‚â§(K‚Äãùîº‚Äã|Œ¥‚ÄãXi|2+by‚Äãùîº‚Äã|Œ¥‚ÄãYi|2+bz‚Äãùîº‚Äã|Œ¥‚ÄãZi|2)‚Äãh2+(œÉx‚Äãùîº‚Äã|Œ¥‚ÄãXi|2+œÉy‚Äãùîº‚Äã|Œ¥‚ÄãYi|2+œÉz‚Äãùîº‚Äã|Œ¥‚ÄãZi|2)‚Äãh\displaystyle\leq\big(K\mathbb{E}|\delta X\_{i}|^{2}+b\_{y}\mathbb{E}|\delta Y\_{i}|^{2}+b\_{z}\mathbb{E}|\delta Z\_{i}|^{2}\big)h^{2}+\big(\sigma\_{x}\mathbb{E}|\delta X\_{i}|^{2}+\sigma\_{y}\mathbb{E}|\delta Y\_{i}|^{2}+\sigma\_{z}\mathbb{E}|\delta Z\_{i}|^{2}\big)h |  |
|  |  |  |
| --- | --- | --- |
|  | +2‚Äãh‚Äãkb‚Äãùîº‚Äã|Œ¥‚ÄãXi|2+h‚ÄãŒª1‚Äãùîº‚Äã|Œ¥‚ÄãXi|2+h‚ÄãŒª1‚àí1‚Äãby‚ÄãE‚Äã|Œ¥‚ÄãYi|2+h‚ÄãŒª0‚Äãùîº‚Äã|Œ¥‚ÄãXi|2+h‚ÄãŒª0‚àí1‚Äãbz‚ÄãE‚Äã|Œ¥‚ÄãZi|2\displaystyle\;+2hk\_{b}\mathbb{E}|\delta X\_{i}|^{2}+h\lambda\_{1}\mathbb{E}|\delta X\_{i}|^{2}+h\lambda\_{1}^{-1}b\_{y}E|\delta Y\_{i}|^{2}+h\lambda\_{0}\mathbb{E}|\delta X\_{i}|^{2}+h\lambda\_{0}^{-1}b\_{z}E|\delta Z\_{i}|^{2} |  |
|  |  |  |
| --- | --- | --- |
|  | =(K‚Äãh+œÉx+2‚Äãkb+Œª1+Œª0)‚Äãh‚Äãùîº‚Äã|Œ¥‚ÄãXi|2+((h+Œª1‚àí1)‚Äãby+œÉy)‚Äãh‚Äãùîº‚Äã|Œ¥‚ÄãYi|2\displaystyle=(Kh+\sigma\_{x}+2k\_{b}+\lambda\_{1}+\lambda\_{0})h\,\mathbb{E}|\delta X\_{i}|^{2}+\big((h+\lambda\_{1}^{-1})b\_{y}+\sigma\_{y}\big)h\mathbb{E}|\delta Y\_{i}|^{2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | +((h+Œª0‚àí1)‚Äãbz+œÉz)‚Äãh‚Äãùîº‚Äã|Œ¥‚ÄãZi|2.\displaystyle\;+\big((h+\lambda\_{0}^{-1})b\_{z}+\sigma\_{z}\big)h\mathbb{E}|\delta Z\_{i}|^{2}. |  | (3.7) |

Denoting

|  |  |  |  |
| --- | --- | --- | --- |
|  | A1\displaystyle A\_{1} | =K‚Äãh+œÉx+2‚Äãkb+Œª1+Œª0,\displaystyle=Kh+\sigma\_{x}+2k\_{b}+\lambda\_{1}+\lambda\_{0}, |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | A2\displaystyle A\_{2} | =(h+Œª1‚àí1)‚Äãby+œÉy,\displaystyle=(h+\lambda\_{1}^{-1})b\_{y}+\sigma\_{y}, |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | A3\displaystyle A\_{3} | =(h+Œª0‚àí1)‚Äãbz+œÉz,\displaystyle=(h+\lambda\_{0}^{-1})b\_{z}+\sigma\_{z}, |  |

we obtain the recurrence:

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Äã|Œ¥‚ÄãXi+1|2‚â§(1+A1‚Äãh)‚Äãùîº‚Äã|Œ¥‚ÄãXi|2+A2‚Äãh‚Äãùîº‚Äã|Œ¥‚ÄãYi|2+A3‚Äãh‚Äãùîº‚Äã|Œ¥‚ÄãZi|2\mathbb{E}|\delta X\_{i+1}|^{2}\leq(1+A\_{1}h)\mathbb{E}|\delta X\_{i}|^{2}+A\_{2}h\mathbb{E}|\delta Y\_{i}|^{2}+A\_{3}h\mathbb{E}|\delta Z\_{i}|^{2} |  |

Then using the fact ùîº‚Äã|Œ¥‚ÄãX0|2=0\mathbb{E}|\delta X\_{0}|^{2}=0, we obtain by induction that, for 1‚â§n‚â§N1\leq n\leq N,

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùîº‚Äã|Œ¥‚ÄãXn|2\displaystyle\mathbb{E}|\delta X\_{n}|^{2} | ‚â§(1+A1‚Äãh)n‚Äãùîº‚Äã|Œ¥‚ÄãX0|2+‚àëi=0n‚àí1(1+A1‚Äãh)n‚àíi‚àí1‚ÄãA2‚Äãh‚Äãùîº‚Äã|Œ¥‚ÄãYi|2+‚àëi=0n‚àí1(1+A1‚Äãh)n‚àíi‚àí1‚ÄãA3‚Äãh‚Äãùîº‚Äã|Œ¥‚ÄãZi|2\displaystyle\leq(1+A\_{1}h)^{n}\mathbb{E}|\delta X\_{0}|^{2}+\sum\_{i=0}^{n-1}(1+A\_{1}h)^{n-i-1}A\_{2}h\mathbb{E}|\delta Y\_{i}|^{2}+\sum\_{i=0}^{n-1}(1+A\_{1}h)^{n-i-1}A\_{3}h\mathbb{E}|\delta Z\_{i}|^{2} |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | ‚â§‚àëi=0n‚àí1eA1‚Äãh‚Äã(n‚àíi‚àí1)‚ÄãA2‚Äãh‚Äãùîº‚Äã|Œ¥‚ÄãYi|2+‚àëi=0n‚àí1eA1‚Äãh‚Äã(n‚àíi‚àí1)‚ÄãA3‚Äãh‚Äãùîº‚Äã|Œ¥‚ÄãZi|2.\displaystyle\leq\sum\_{i=0}^{n-1}e^{A\_{1}h(n-i-1)}A\_{2}h\mathbb{E}|\delta Y\_{i}|^{2}+\sum\_{i=0}^{n-1}e^{A\_{1}h(n-i-1)}A\_{3}h\mathbb{E}|\delta Z\_{i}|^{2}. |  | (3.8) |

Step 2: Estimate for Œ¥‚ÄãY\delta Y and Œ¥‚ÄãZ\delta Z
  
By the martingale representation theorem there exists an (‚Ñ±t)t‚â•0‚àí\left(\mathcal{F}\_{t}\right)\_{t\geq 0}-adapted square-integrable process {Œ¥‚ÄãZ¬Øt}ti‚â§t‚â§ti+1\{\delta\bar{Z}\_{t}\}\_{t\_{i}\leq t\leq t\_{i+1}} such that

|  |  |  |  |
| --- | --- | --- | --- |
|  | Œ¥‚ÄãYi+1=E‚Äã[Œ¥‚ÄãYi+1|‚Ñ±ti]+‚à´titi+1Œ¥‚ÄãZ¬Øt‚ÄãùëëWt=Œ¥‚ÄãYi‚àíŒ¥‚Äãfi‚Äãh+‚à´titi+1Œ¥‚ÄãZ¬Øt‚ÄãùëëWt.\delta Y\_{i+1}=E[\delta Y\_{i+1}|\mathcal{F}\_{t\_{i}}]+\int\_{t\_{i}}^{t\_{i+1}}\delta\bar{Z}\_{t}dW\_{t}=\delta Y\_{i}-\delta f\_{i}h+\int\_{t\_{i}}^{t\_{i+1}}\delta\bar{Z}\_{t}dW\_{t}. |  | (3.9) |

Squaring on both sides and taking the expectation yields that

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | ùîº‚Äã|Œ¥‚ÄãYi+1|2\displaystyle\mathbb{E}|\delta Y\_{i+1}|^{2} | =ùîº‚Äã|Œ¥‚ÄãYi‚àíŒ¥‚Äãfi‚Äãh|2+‚à´titi+1ùîº‚Äã|Œ¥‚ÄãZ¬Øt|2‚Äãùëët.\displaystyle=\mathbb{E}|\delta Y\_{i}-\delta f\_{i}h|^{2}+\int\_{t\_{i}}^{t\_{i+1}}\mathbb{E}|\delta\bar{Z}\_{t}|^{2}dt. |  | (3.10) |

Rewriting this, and estimating the term Œ¥‚Äãfi\delta f\_{i}, we obtain for any Œª2>0\lambda\_{2}>0:

|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ùîº‚Äã|Œ¥‚ÄãYi+1|2‚àíùîº‚Äã|Œ¥‚ÄãYi|2\displaystyle\mathbb{E}|\delta Y\_{i+1}|^{2}-\mathbb{E}|\delta Y\_{i}|^{2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =ùîº‚Äã|Œ¥‚ÄãYi‚àíŒ¥‚Äãfi‚Äãh|2‚àíùîº‚Äã|Œ¥‚ÄãYi|2+‚à´titi+1ùîº‚Äã|Œ¥‚ÄãZ¬Øt|2‚Äãùëët\displaystyle=\mathbb{E}|\delta Y\_{i}-\delta f\_{i}h|^{2}-\mathbb{E}|\delta Y\_{i}|^{2}+\int\_{t\_{i}}^{t\_{i+1}}\mathbb{E}|\delta\bar{Z}\_{t}|^{2}dt |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â•‚à´titi+1ùîº‚Äã|Œ¥‚ÄãZ¬Øt|2‚Äãùëët‚àí2‚Äãh‚Äãùîº‚Äã[(f‚Äã(ti,Vti,Xti1,œÄ,Yti1,œÄ,Zti1,œÄ)‚àíf‚Äã(ti,Vti,Xti1,œÄ,Yti2,œÄ,Zti1,œÄ))‚ãÖŒ¥‚ÄãYi]\displaystyle\geq\int\_{t\_{i}}^{t\_{i+1}}\mathbb{E}|\delta\bar{Z}\_{t}|^{2}dt-2h\mathbb{E}\Big[(f(t\_{i},V\_{t\_{i}},X^{1,\pi}\_{t\_{i}},Y^{1,\pi}\_{t\_{i}},Z^{1,\pi}\_{t\_{i}})-f(t\_{i},V\_{t\_{i}},X^{1,\pi}\_{t\_{i}},Y^{2,\pi}\_{t\_{i}},Z^{1,\pi}\_{t\_{i}}))\cdot\delta Y\_{i}\Big] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚àí2‚Äãh‚Äãùîº‚Äã[(f‚Äã(ti,Vti,Xti1,œÄ,Yti2,œÄ,Zti1,œÄ)‚àíf‚Äã(ti,Vti,Xti2,œÄ,Yti2,œÄ,Zti2,œÄ))‚ãÖŒ¥‚ÄãYi]\displaystyle\quad-2h\mathbb{E}\Big[(f(t\_{i},V\_{t\_{i}},X^{1,\pi}\_{t\_{i}},Y^{2,\pi}\_{t\_{i}},Z^{1,\pi}\_{t\_{i}})-f(t\_{i},V\_{t\_{i}},X^{2,\pi}\_{t\_{i}},Y^{2,\pi}\_{t\_{i}},Z^{2,\pi}\_{t\_{i}}))\cdot\delta Y\_{i}\Big] |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | ‚â•‚à´titi+1ùîº‚Äã|Œ¥‚ÄãZ¬Øt|2‚Äãùëët‚àí2‚Äãkf‚Äãh‚Äãùîº‚Äã|Œ¥‚ÄãYi|2‚àí(Œª2‚Äãùîº‚Äã|Œ¥‚ÄãYi|2+Œª2‚àí1‚Äã(fx‚Äãùîº‚Äã|Œ¥‚ÄãXi|2+fz‚Äãùîº‚Äã|Œ¥‚ÄãZi|2))‚Äãh.\displaystyle\geq\int\_{t\_{i}}^{t\_{i+1}}\mathbb{E}|\delta\bar{Z}\_{t}|^{2}dt-2k\_{f}h\mathbb{E}|\delta Y\_{i}|^{2}-\Big(\lambda\_{2}\mathbb{E}|\delta Y\_{i}|^{2}+\lambda\_{2}^{-1}(f\_{x}\mathbb{E}|\delta X\_{i}|^{2}+f\_{z}\mathbb{E}|\delta Z\_{i}|^{2})\Big)\,h. |  | (3.11) |

Using ([3.9](https://arxiv.org/html/2511.08735v1#S3.E9 "In 3 Numerical Scheme and Simulation Error for Non-Markovian Coupled-FBSDE ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) into ([3.5](https://arxiv.org/html/2511.08735v1#S3.E5 "In 3 Numerical Scheme and Simulation Error for Non-Markovian Coupled-FBSDE ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) we can have

|  |  |  |
| --- | --- | --- |
|  | Œ¥‚ÄãZi=1h‚Äãùîº‚Äã[‚à´titi+1Œ¥‚ÄãZ¬Øt‚Äãùëët|‚Ñ±ti],\delta Z\_{i}=\frac{1}{h}\mathbb{E}\left[\int\_{t\_{i}}^{t\_{i+1}}\delta\bar{Z}\_{t}dt\Big|\mathcal{F}\_{t\_{i}}\right], |  |

which further implies that

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùîº|Œ¥Zi|2h=1hùîº|ùîº[‚à´titi+1Œ¥Z¬Øtdt|‚Ñ±ti]|2‚â§‚à´titi+1ùîº|Œ¥Z¬Øt|2dt.\displaystyle\mathbb{E}|\delta Z\_{i}|^{2}h=\frac{1}{h}\mathbb{E}\Bigg|\mathbb{E}\bigg[\int\_{t\_{i}}^{t\_{i+1}}\delta\bar{Z}\_{t}dt|\mathcal{F}\_{t\_{i}}\bigg]\Bigg|^{2}\leq\int\_{t\_{i}}^{t\_{i+1}}\mathbb{E}|\delta\bar{Z}\_{t}|^{2}dt. |  | (3.12) |

Plugging this into ([3](https://arxiv.org/html/2511.08735v1#S3.Ex23 "3 Numerical Scheme and Simulation Error for Non-Markovian Coupled-FBSDE ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) gives

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Äã|Œ¥‚ÄãYi+1|2‚â•(1‚àí(2‚Äãkf+Œª2)‚Äãh)‚Äãùîº‚Äã|Œ¥‚ÄãYi|2+(1‚àífz‚ÄãŒª2‚àí1)‚Äãh‚Äãùîº‚Äã|Œ¥‚ÄãZi|2‚àífx‚ÄãŒª2‚àí1‚Äãh‚Äãùîº‚Äã|Œ¥‚ÄãXi|2.\mathbb{E}|\delta Y\_{i+1}|^{2}\geq\big(1-(2k\_{f}+\lambda\_{2})h\big)\mathbb{E}|\delta Y\_{i}|^{2}+(1-f\_{z}\lambda\_{2}^{-1})h\mathbb{E}|\delta Z\_{i}|^{2}-f\_{x}\lambda\_{2}^{-1}h\mathbb{E}|\delta X\_{i}|^{2}. |  |

Then for any Œª2>fz\lambda\_{2}>f\_{z} and sufficiently small hh satisfying (2‚Äãkf+Œª2)‚Äãh<1(2k\_{f}+\lambda\_{2})h<1, we obtain

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Äã|Œ¥‚ÄãYi|2+(1‚àífz‚ÄãŒª2‚àí1)‚Äã(1‚àí(2‚Äãkf+Œª2)‚Äãh)‚àí1‚Äãùîº‚Äã|Œ¥‚ÄãZi|2‚Äãh\displaystyle\mathbb{E}|\delta Y\_{i}|^{2}+(1-f\_{z}\lambda\_{2}^{-1})\big(1-(2k\_{f}+\lambda\_{2})h\big)^{-1}\mathbb{E}|\delta Z\_{i}|^{2}h |  |
|  |  |  |
| --- | --- | --- |
|  | ‚â§(1‚àí(2‚Äãkf+Œª2)‚Äãh)‚àí1‚Äã(ùîº‚Äã|Œ¥‚ÄãYi+1|2+fx‚ÄãŒª2‚àí1‚Äãùîº‚Äã|Œ¥‚ÄãXi|2‚Äãh).\displaystyle\leq\big(1-(2k\_{f}+\lambda\_{2})h\big)^{-1}\left(\mathbb{E}|\delta Y\_{i+1}|^{2}+f\_{x}\lambda\_{2}^{-1}\mathbb{E}|\delta X\_{i}|^{2}h\right). |  |

Denoting

|  |  |  |  |
| --- | --- | --- | --- |
|  | A4\displaystyle A\_{4} | =‚àíh‚àí1‚Äãl‚Äãn‚Äã[1‚àí(2‚Äãkf+Œª2)‚Äãh],\displaystyle=-h^{-1}ln[1-(2k\_{f}+\lambda\_{2})h], |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | A5\displaystyle A\_{5} | =fx‚ÄãŒª2‚àí1‚Äã(1‚àí(2‚Äãkf+Œª2)‚Äãh)‚àí1,\displaystyle=f\_{x}\lambda\_{2}^{-1}\big(1-(2k\_{f}+\lambda\_{2})h\big)^{-1}, |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | A6\displaystyle A\_{6} | =(1‚àífz‚ÄãŒª2‚àí1)‚Äã(1‚àí(2‚Äãkf+Œª2)‚Äãh)‚àí1,\displaystyle=(1-f\_{z}\lambda\_{2}^{-1})\big(1-(2k\_{f}+\lambda\_{2})h\big)^{-1}, |  |

we obtain by induction that, for 1‚â§n‚â§N1\leq n\leq N,

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Äã|Œ¥‚ÄãYn|2+‚àëi=nN‚àí1A6‚ÄãeA4‚Äã(i‚àín)‚Äãh‚Äãùîº‚Äã|Œ¥‚ÄãZi|2‚Äãh‚â§eA4‚Äãh‚Äã(N‚àín)‚Äãùîº‚Äã|Œ¥‚ÄãYN|2+A5‚Äã‚àëi=nNeA4‚Äã(i‚àín)‚Äãh‚Äãùîº‚Äã|Œ¥‚ÄãXi|2‚Äãh.\mathbb{E}|\delta Y\_{n}|^{2}+\sum\_{i=n}^{N-1}A\_{6}e^{A\_{4}(i-n)h}\mathbb{E}|\delta Z\_{i}|^{2}h\leq e^{A\_{4}h(N-n)}\mathbb{E}|\delta Y\_{N}|^{2}+A\_{5}\sum\_{i=n}^{N}e^{A\_{4}(i-n)h}\mathbb{E}|\delta X\_{i}|^{2}h. |  |

‚àé

###### Remark 3.1.

Note that, using the notations from Lemma [3.1](https://arxiv.org/html/2511.08735v1#S3.Thmthm1 "Lemma 3.1. ‚Ä£ 3 Numerical Scheme and Simulation Error for Non-Markovian Coupled-FBSDE ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications"), for i=1,2,‚ãØ,6i=1,2,\cdots,6, the limits A¬Øi=limh‚Üí0Ai\overline{A}\_{i}=\lim\_{h\rightarrow 0}A\_{i} exists and are finite.

###### Theorem 3.2.

Assume the assumptions of Theorem [2.6](https://arxiv.org/html/2511.08735v1#S2.Thmthm6 "Theorem 2.6. ‚Ä£ 2.2 Solutions to FBSDEs ‚Ä£ 2 Preliminaries ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications") hold, and there exists constants Œªi>0,i=0,1,2\lambda\_{i}>0,i=0,1,2 such that Œª2>fz\lambda\_{2}>f\_{z}, (2‚Äãkf+Œª2)‚Äãh<1(2k\_{f}+\lambda\_{2})h<1 and A¬Ø0<1\overline{A}\_{0}<1 where

|  |  |  |
| --- | --- | --- |
|  | A¬Ø0=(A¬Ø2‚Äã1‚àíe‚àí(A¬Ø1+A¬Ø4)‚ÄãTA¬Ø1+A¬Ø4+A¬Ø3A¬Ø6)‚ãÖ(gx‚Äã(1+Œª3)‚Äãe(A¬Ø1+A¬Ø4)‚ÄãT+A¬Ø5‚Äãe(A¬Ø1+A¬Ø4)‚ÄãT‚àí1A¬Ø1+A¬Ø4),\overline{A}\_{0}=\left(\overline{A}\_{2}\frac{1-e^{-(\overline{A}\_{1}+\overline{A}\_{4})T}}{\overline{A}\_{1}+\overline{A}\_{4}}+\frac{\overline{A}\_{3}}{\overline{A}\_{6}}\right)\cdot\left(g\_{x}(1+\lambda\_{3})e^{(\overline{A}\_{1}+\overline{A}\_{4})T}+\overline{A}\_{5}\frac{e^{(\overline{A}\_{1}+\overline{A}\_{4})T}-1}{\overline{A}\_{1}+\overline{A}\_{4}}\right), |  |

and the constants A¬Ø1,‚ãØ,A¬Ø6\overline{A}\_{1},\cdots,\overline{A}\_{6} are as defined in Lemma [3.1](https://arxiv.org/html/2511.08735v1#S3.Thmthm1 "Lemma 3.1. ‚Ä£ 3 Numerical Scheme and Simulation Error for Non-Markovian Coupled-FBSDE ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications") and Remark [3.1](https://arxiv.org/html/2511.08735v1#S3.Thmrmk1 "Remark 3.1. ‚Ä£ 3 Numerical Scheme and Simulation Error for Non-Markovian Coupled-FBSDE ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications").

Then there exists a constant C>0C>0, independent of h, d, and m, such that for sufficiently small h,

|  |  |  |
| --- | --- | --- |
|  | supt‚àà[0,T](ùîº‚Äã|Xt‚àíX^tœÄ|2+ùîº‚Äã|Yt‚àíY^tœÄ|2)+‚à´0Tùîº‚Äã|Zt‚àíZ^tœÄ|2‚Äãùëët‚â§C‚Äã(M+ùîº‚Äã|g‚Äã(VT,XTœÄ)‚àíYTœÄ|2),\sup\_{t\in[0,T]}\big(\mathbb{E}|X\_{t}-\hat{X}\_{t}^{\pi}|^{2}+\mathbb{E}|Y\_{t}-\hat{Y}\_{t}^{\pi}|^{2}\big)+\int\_{0}^{T}\mathbb{E}|Z\_{t}-\hat{Z}\_{t}^{\pi}|^{2}dt\leq C\big(M+\mathbb{E}|g(V\_{T},X\_{T}^{\pi})-Y\_{T}^{\pi}|^{2}\big), |  |

where,

|  |  |  |  |
| --- | --- | --- | --- |
|  | M=\displaystyle M= | (I02+ùîº‚Äã|x0|2)‚Äãh+œÅ‚Äã(h)+1h‚Äãsup0‚â§k‚â§N‚àí1ùîº‚Äã‚à´tktk+1|Zt‚àíZ~tk|2‚Äãùëët\displaystyle\big(I\_{0}^{2}+\mathbb{E}|x\_{0}|^{2}\big)h+\rho(h)+\frac{1}{h}\sup\_{0\leq k\leq N-1}\mathbb{E}\int\_{t\_{k}}^{t\_{k+1}}|Z\_{t}-\tilde{Z}\_{t\_{k}}|^{2}dt |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +sup0‚â§k‚â§N‚àí1ùîº‚Äã[‚à´tktk+1|Zt|2‚Äãùëët+‚à´tktk+1(|f|2+|œÉ|2)‚Äã(t,Vt,0,0,0)‚Äãùëët],\displaystyle+\sup\_{0\leq k\leq N-1}\mathbb{E}\bigg[\int\_{t\_{k}}^{t\_{k+1}}|Z\_{t}|^{2}dt+\int\_{t\_{k}}^{t\_{k+1}}\big(|f|^{2}+|\sigma|^{2}\big)(t,V\_{t},0,0,0)dt\bigg], |  |

and piecewise constant approximations X^tœÄ,Y^tœÄ,Z^tœÄ\hat{X}\_{t}^{\pi},\hat{Y}\_{t}^{\pi},\hat{Z}\_{t}^{\pi} are defined as

|  |  |  |
| --- | --- | --- |
|  | X^tœÄ=XtiœÄ,Y^tœÄ=YtiœÄ,Z^tœÄ=ZtiœÄfort‚àà[ti,ti+1),\hat{X}\_{t}^{\pi}=X\_{t\_{i}}^{\pi},\;\hat{Y}\_{t}^{\pi}=Y\_{t\_{i}}^{\pi},\;\hat{Z}\_{t}^{\pi}=Z\_{t\_{i}}^{\pi}\quad\text{for}\quad t\in[t\_{i},t\_{i+1}), |  |

with (XœÄ,YœÄ,ZœÄ)(X^{\pi},Y^{\pi},Z^{\pi}) satisfying the discrete-time system ([3.2](https://arxiv.org/html/2511.08735v1#S3.E2 "In 3 Numerical Scheme and Simulation Error for Non-Markovian Coupled-FBSDE ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) and Z~tk:=1h‚Äãùîº‚Äã[‚à´tktk+1Zt‚Äãùëët|‚Ñ±tk]\tilde{Z}\_{t\_{k}}:=\frac{1}{h}\mathbb{E}\left[\int\_{t\_{k}}^{t\_{k+1}}Z\_{t}dt|\mathcal{F}\_{t\_{k}}\right].

###### Proof.

We use the same notation as Lemma [3.1](https://arxiv.org/html/2511.08735v1#S3.Thmthm1 "Lemma 3.1. ‚Ä£ 3 Numerical Scheme and Simulation Error for Non-Markovian Coupled-FBSDE ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications"). Let

|  |  |  |
| --- | --- | --- |
|  | XtiœÄ,1=XtiœÄ,YtiœÄ,1=YtiœÄ,ZtiœÄ,1=ZtiœÄ,X^{\pi,1}\_{t\_{i}}=X^{\pi}\_{t\_{i}},\;Y^{\pi,1}\_{t\_{i}}=Y^{\pi}\_{t\_{i}},\;Z^{\pi,1}\_{t\_{i}}=Z^{\pi}\_{t\_{i}}, |  |

and

|  |  |  |
| --- | --- | --- |
|  | XtiœÄ,2=X¬ØtiœÄ,YtiœÄ,2=Y¬ØtiœÄ,ZtiœÄ,2=Z¬ØtiœÄ,X^{\pi,2}\_{t\_{i}}=\overline{X}^{\pi}\_{t\_{i}},\;Y^{\pi,2}\_{t\_{i}}=\overline{Y}^{\pi}\_{t\_{i}},\;Z^{\pi,2}\_{t\_{i}}=\overline{Z}^{\pi}\_{t\_{i}}, |  |

where (XœÄ,YœÄ,ZœÄ)(X^{\pi},Y^{\pi},Z^{\pi}) is defined in ([3.2](https://arxiv.org/html/2511.08735v1#S3.E2 "In 3 Numerical Scheme and Simulation Error for Non-Markovian Coupled-FBSDE ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) and (X¬ØœÄ,Y¬ØœÄ,Z¬ØœÄ)(\overline{X}^{\pi},\overline{Y}^{\pi},\overline{Z}^{\pi}) is defined in ([2.23](https://arxiv.org/html/2511.08735v1#S2.E23 "In 2.2 Solutions to FBSDEs ‚Ä£ 2 Preliminaries ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")). Both (XœÄ,YœÄ,ZœÄ)(X^{\pi},Y^{\pi},Z^{\pi}) and (X¬ØœÄ,Y¬ØœÄ,Z¬ØœÄ)(\overline{X}^{\pi},\overline{Y}^{\pi},\overline{Z}^{\pi}) satisfy the system ([3.3](https://arxiv.org/html/2511.08735v1#S3.E3 "In 3 Numerical Scheme and Simulation Error for Non-Markovian Coupled-FBSDE ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")).

By the terminal condition, we have for any
Œª3>0\lambda\_{3}>0,

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùîº‚Äã|Œ¥‚ÄãYN|2=ùîº‚Äã|g‚Äã(VT,X¬ØTœÄ)‚àíYTœÄ|2‚â§(1+Œª3‚àí1)‚Äãùîº‚Äã|g‚Äã(VT,XTœÄ)‚àíYTœÄ|2+gx‚Äã(1+Œª3)‚Äãùîº‚Äã|Œ¥‚ÄãXN|2.\mathbb{E}|\delta Y\_{N}|^{2}=\mathbb{E}|g(V\_{T},\overline{X}^{\pi}\_{T})-Y^{\pi}\_{T}|^{2}\leq(1+\lambda\_{3}^{-1})\mathbb{E}|g(V\_{T},X^{\pi}\_{T})-Y^{\pi}\_{T}|^{2}+g\_{x}(1+\lambda\_{3})\mathbb{E}|\delta X\_{N}|^{2}. |  | (3.13) |

From Lemma [3.1](https://arxiv.org/html/2511.08735v1#S3.Thmthm1 "Lemma 3.1. ‚Ä£ 3 Numerical Scheme and Simulation Error for Non-Markovian Coupled-FBSDE ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications"), we also have the estimates:

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | e‚àíA1‚Äãn‚Äãh‚Äãùîº‚Äã|Œ¥‚ÄãXn|2‚â§\displaystyle e^{-A\_{1}nh}\mathbb{E}|\delta X\_{n}|^{2}\leq | ‚àëi=0n‚àí1e‚àíA1‚Äãh‚Äã(i+1)‚ÄãA2‚Äãh‚Äãùîº‚Äã|Œ¥‚ÄãYi|2+‚àëi=0n‚àí1e‚àíA1‚Äãh‚Äã(i+1)‚ÄãA3‚Äãh‚Äãùîº‚Äã|Œ¥‚ÄãZi|2,\displaystyle\sum\_{i=0}^{n-1}e^{-A\_{1}h(i+1)}A\_{2}h\mathbb{E}|\delta Y\_{i}|^{2}+\sum\_{i=0}^{n-1}e^{-A\_{1}h(i+1)}A\_{3}h\mathbb{E}|\delta Z\_{i}|^{2}, |  | (3.14) |

and

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | eA4‚Äãn‚Äãh‚Äãùîº‚Äã|Œ¥‚ÄãYn|2+‚àëi=nN‚àí1eA4‚Äãi‚Äãh‚ÄãA6‚Äãùîº‚Äã|Œ¥‚ÄãZi|2‚Äãh‚â§\displaystyle e^{A\_{4}nh}\mathbb{E}|\delta Y\_{n}|^{2}+\sum\_{i=n}^{N-1}e^{A\_{4}ih}A\_{6}\mathbb{E}|\delta Z\_{i}|^{2}h\leq | eA4‚ÄãT‚Äãùîº‚Äã|Œ¥‚ÄãYN|2+A5‚Äã‚àëi=nN‚àí1eA4‚Äãi‚Äãh‚Äãùîº‚Äã|Œ¥‚ÄãXi|2‚Äãh,\displaystyle e^{A\_{4}T}\mathbb{E}|\delta Y\_{N}|^{2}+A\_{5}\sum\_{i=n}^{N-1}e^{A\_{4}ih}\mathbb{E}|\delta X\_{i}|^{2}h, |  | (3.15) |

Now, we define:

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | P\displaystyle P | :=max0‚â§n‚â§N‚Å°e‚àíA1‚Äãn‚Äãh‚Äãùîº‚Äã|Œ¥‚ÄãXn|2,\displaystyle:=\max\_{0\leq n\leq N}e^{-A\_{1}nh}\mathbb{E}|\delta X\_{n}|^{2}, |  | (3.16) |

and

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | S\displaystyle S | :=max‚Å°{max0‚â§n‚â§N‚Å°(eA4‚Äãn‚Äãh‚Äãùîº‚Äã|Œ¥‚ÄãYn|2),‚àëi=0N‚àí1eA4‚Äãi‚Äãh‚ÄãA6‚Äãh‚Äãùîº‚Äã|Œ¥‚ÄãZi|2}.\displaystyle:=\max\left\{\max\_{0\leq n\leq N}(e^{A\_{4}nh}\mathbb{E}|\delta Y\_{n}|^{2}),\,\,\sum\_{i=0}^{N-1}e^{A\_{4}ih}A\_{6}h\mathbb{E}|\delta Z\_{i}|^{2}\right\}. |  | (3.17) |

Then by combining ([3.14](https://arxiv.org/html/2511.08735v1#S3.E14 "In 3 Numerical Scheme and Simulation Error for Non-Markovian Coupled-FBSDE ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) and ([3.15](https://arxiv.org/html/2511.08735v1#S3.E15 "In 3 Numerical Scheme and Simulation Error for Non-Markovian Coupled-FBSDE ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")), and using estimate from ([3.13](https://arxiv.org/html/2511.08735v1#S3.E13 "In 3 Numerical Scheme and Simulation Error for Non-Markovian Coupled-FBSDE ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) we get

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | P\displaystyle P | ‚â§(A2‚Äãh‚Äãe‚àíA1‚Äãh‚Äãe‚àí(A1+A4)‚ÄãT‚àí1e‚àí(A1+A4)‚Äãh‚àí1+A3A6‚Äãe‚àíA1‚Äãh)‚ÄãS,\displaystyle\leq\bigg(A\_{2}he^{-A\_{1}h}\frac{e^{-(A\_{1}+A\_{4})T}-1}{e^{-(A\_{1}+A\_{4})h}-1}+\frac{A\_{3}}{A\_{6}}e^{-A\_{1}h}\bigg)S, |  | (3.18) |

and

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | S\displaystyle S | ‚â§eA4‚ÄãT‚Äã(1+Œª3‚àí1)‚Äãùîº‚Äã|g‚Äã(VT,XTœÄ)‚àíYTœÄ|2+(gx‚Äã(1+Œª3)‚Äãe(A1+A4)‚ÄãT+A5‚Äãh‚Äãe(A1+A4)‚ÄãT‚àí1e(A1+A4)‚Äãh‚àí1)‚ÄãP.\displaystyle\leq e^{A\_{4}T}(1+\lambda\_{3}^{-1})\mathbb{E}|g(V\_{T},X^{\pi}\_{T})-Y^{\pi}\_{T}|^{2}+\bigg(g\_{x}(1+\lambda\_{3})e^{(A\_{1}+A\_{4})T}+A\_{5}h\frac{e^{(A\_{1}+A\_{4})T}-1}{e^{(A\_{1}+A\_{4})h}-1}\bigg)P. |  | (3.19) |

Set

|  |  |  |
| --- | --- | --- |
|  | A‚Äã(h)=(A2‚Äãh‚Äãe‚àíA1‚Äãh‚Äãe‚àí(A1+A4)‚ÄãT‚àí1e‚àí(A1+A4)‚Äãh‚àí1+A3A6‚Äãe‚àíA1‚Äãh)‚Äã(gx‚Äã(1+Œª3)‚Äãe(A1+A4)‚ÄãT+A5‚Äãh‚Äãe(A1+A4)‚ÄãT‚àí1e(A1+A4)‚Äãh‚àí1).A(h)=\left(A\_{2}he^{-A\_{1}h}\frac{e^{-(A\_{1}+A\_{4})T}-1}{e^{-(A\_{1}+A\_{4})h}-1}+\frac{A\_{3}}{A\_{6}}e^{-A\_{1}h}\right)\left(g\_{x}(1+\lambda\_{3})e^{(A\_{1}+A\_{4})T}+A\_{5}h\frac{e^{(A\_{1}+A\_{4})T}-1}{e^{(A\_{1}+A\_{4})h}-1}\right). |  |

If A‚Äã(h)<1A(h)<1, then we can have by combining estimates from ([3.18](https://arxiv.org/html/2511.08735v1#S3.E18 "In 3 Numerical Scheme and Simulation Error for Non-Markovian Coupled-FBSDE ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) and ([3.19](https://arxiv.org/html/2511.08735v1#S3.E19 "In 3 Numerical Scheme and Simulation Error for Non-Markovian Coupled-FBSDE ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) that

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | P\displaystyle P | ‚â§(1‚àíA‚Äã(h))‚àí1‚ÄãeA4‚ÄãT‚Äã(1+Œª3‚àí1)‚Äãùîº‚Äã|g‚Äã(VT,XTœÄ)‚àíYTœÄ|2‚Äã(A2‚Äãh‚Äãe‚àíA1‚Äãh‚Äãe‚àí(A1+A4)‚ÄãT‚àí1e‚àí(A1+A4)‚Äãh‚àí1+A3A6‚Äãe‚àíA1‚Äãh),\displaystyle\leq\big(1-A(h)\big)^{-1}e^{A\_{4}T}(1+\lambda\_{3}^{-1})\mathbb{E}|g(V\_{T},X^{\pi}\_{T})-Y^{\pi}\_{T}|^{2}\left(A\_{2}he^{-A\_{1}h}\frac{e^{-(A\_{1}+A\_{4})T}-1}{e^{-(A\_{1}+A\_{4})h}-1}+\frac{A\_{3}}{A\_{6}}e^{-A\_{1}h}\right), |  | (3.20) |

and

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | S\displaystyle S | ‚â§(1‚àíA‚Äã(h))‚àí1‚ÄãeA4‚ÄãT‚Äã(1+Œª3‚àí1)‚Äãùîº‚Äã|g‚Äã(VT,XTœÄ)‚àíYTœÄ|2.\displaystyle\leq\big(1-A(h)\big)^{-1}e^{A\_{4}T}(1+\lambda\_{3}^{-1})\mathbb{E}|g(V\_{T},X^{\pi}\_{T})-Y^{\pi}\_{T}|^{2}. |  | (3.21) |

Now, define

|  |  |  |  |
| --- | --- | --- | --- |
|  | P¬Ø\displaystyle\overline{P} | :=max0‚â§n‚â§N‚Å°e‚àíA¬Ø1‚Äãn‚Äãh‚Äãùîº‚Äã|Œ¥‚ÄãXn|2,\displaystyle:=\max\_{0\leq n\leq N}e^{-\overline{A}\_{1}nh}\mathbb{E}|\delta X\_{n}|^{2}, |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | S¬Ø\displaystyle\overline{S} | :=max‚Å°{max0‚â§n‚â§N‚Å°(eA¬Ø4‚Äãn‚Äãh‚Äãùîº‚Äã|Œ¥‚ÄãYn|2),‚àëi=0N‚àí1eA¬Ø4‚Äãi‚Äãh‚ÄãA¬Ø6‚Äãh‚Äãùîº‚Äã|Œ¥‚ÄãZi|2}.\displaystyle:=\max\left\{\max\_{0\leq n\leq N}\big(e^{\overline{A}\_{4}nh}\mathbb{E}|\delta Y\_{n}|^{2}\big),\,\,\sum\_{i=0}^{N-1}e^{\overline{A}\_{4}ih}\overline{A}\_{6}h\mathbb{E}|\delta Z\_{i}|^{2}\right\}. |  |

Recall that Ai‚ÜíA¬ØiA\_{i}\rightarrow\overline{A}\_{i} as h‚Üí0h\to 0 for i=1,‚ãØ,6,i=1,\cdots,6, and that

|  |  |  |
| --- | --- | --- |
|  | limh‚Üí0A‚Äã(h)=(A¬Ø2‚Äã1‚àíe‚àí(A¬Ø1+A¬Ø4)‚ÄãTA¬Ø1+A¬Ø4+A¬Ø3A¬Ø6)‚Äã(gx‚Äã(1+Œª3)‚Äãe(A¬Ø1+A¬Ø4)‚ÄãT+A¬Ø5‚Äãe(A¬Ø1+A¬Ø4)‚ÄãT‚àí1A¬Ø1+A¬Ø4).\lim\_{h\to 0}A(h)=\left(\overline{A}\_{2}\frac{1-e^{-(\overline{A}\_{1}+\overline{A}\_{4})T}}{\overline{A}\_{1}+\overline{A}\_{4}}+\frac{\overline{A}\_{3}}{\overline{A}\_{6}}\right)\left(g\_{x}(1+\lambda\_{3})e^{(\overline{A}\_{1}+\overline{A}\_{4})T}+\overline{A}\_{5}\frac{e^{(\overline{A}\_{1}+\overline{A}\_{4})T}-1}{\overline{A}\_{1}+\overline{A}\_{4}}\right). |  |

When A¬Ø0<1\overline{A}\_{0}<1, comparing limh‚Üí0A‚Äã(h)\lim\_{h\to 0}A(h) and A0¬Ø\overline{A\_{0}}, we know that, for any œµ>0\epsilon>0, there exists Œª3>0\lambda\_{3}>0 and sufficiently small hh such that

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | P¬Ø‚â§\displaystyle\overline{P}\leq | (1+œµ)‚Äã(1‚àíA¬Ø0)‚àí1‚ÄãeA¬Ø4‚ÄãT‚Äã(1+Œª3‚àí1)‚Äã(A¬Ø2‚Äã1‚àíe‚àí(A¬Ø1+A¬Ø4)‚ÄãTA¬Ø1+A¬Ø4+A¬Ø3A¬Ø6)‚Äãùîº‚Äã|g‚Äã(VT,XTœÄ)‚àíYTœÄ|2,\displaystyle(1+\epsilon)\big(1-\overline{A}\_{0}\big)^{-1}e^{\overline{A}\_{4}T}(1+\lambda\_{3}^{-1})\bigg(\overline{A}\_{2}\frac{1-e^{-(\overline{A}\_{1}+\overline{A}\_{4})T}}{\overline{A}\_{1}+\overline{A}\_{4}}+\frac{\overline{A}\_{3}}{\overline{A}\_{6}}\bigg)\mathbb{E}|g(V\_{T},X^{\pi}\_{T})-Y^{\pi}\_{T}|^{2}, |  | (3.22) |

and

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | S¬Ø‚â§\displaystyle\overline{S}\leq | (1+œµ)‚Äã(1‚àíA¬Ø0)‚àí1‚ÄãeA¬Ø4‚ÄãT‚Äã(1+Œª3‚àí1)‚Äãùîº‚Äã|g‚Äã(VT,XTœÄ)‚àíYTœÄ|2.\displaystyle(1+\epsilon)\big(1-\overline{A}\_{0}\big)^{-1}e^{\overline{A}\_{4}T}(1+\lambda\_{3}^{-1})\mathbb{E}|g(V\_{T},X^{\pi}\_{T})-Y^{\pi}\_{T}|^{2}. |  | (3.23) |

By setting œµ=1\epsilon=1 and choosing suitable Œª3\lambda\_{3}, we obtain estimates of ùîº‚Äã|Œ¥‚ÄãXn|2,ùîº‚Äã|Œ¥‚ÄãYn|2\mathbb{E}|\delta X\_{n}|^{2},\mathbb{E}|\delta Y\_{n}|^{2} and ùîº‚Äã|Œ¥‚ÄãZn|2\mathbb{E}|\delta Z\_{n}|^{2} as

|  |  |  |
| --- | --- | --- |
|  | max0‚â§n‚â§N‚Å°ùîº‚Äã|Œ¥‚ÄãXn|2‚â§eA¬Ø1‚ÄãT‚à®0‚ÄãP¬Ø‚â§C‚Äã(Œª1,Œª2)‚Äãùîº‚Äã|g‚Äã(VT,XTœÄ)‚àíYTœÄ|2,\max\_{0\leq n\leq N}\mathbb{E}|\delta X\_{n}|^{2}\leq e^{\overline{A}\_{1}T\vee 0}\overline{P}\leq C(\lambda\_{1},\lambda\_{2})\mathbb{E}|g(V\_{T},X^{\pi}\_{T})-Y^{\pi}\_{T}|^{2}, |  |

|  |  |  |
| --- | --- | --- |
|  | max‚Å°{max0‚â§n‚â§N‚Å°ùîº‚Äã|Œ¥‚ÄãYn|2,‚àëi=0N‚àí1ùîº‚Äã|Œ¥‚ÄãZi|2‚Äãh}‚â§e‚àíA¬Ø4‚ÄãT‚à®0‚Äã(1+A¬Ø6‚àí1)‚ÄãS¬Ø‚â§C‚Äã(Œª1,Œª2)‚Äãùîº‚Äã|g‚Äã(VT,XTœÄ)‚àíYTœÄ|2.\max\left\{\max\_{0\leq n\leq N}\mathbb{E}|\delta Y\_{n}|^{2},\,\,\sum\_{i=0}^{N-1}\mathbb{E}|\delta Z\_{i}|^{2}h\right\}\leq e^{-\overline{A}\_{4}T\vee 0}(1+\overline{A}\_{6}^{-1})\overline{S}\leq C(\lambda\_{1},\lambda\_{2})\mathbb{E}|g(V\_{T},X^{\pi}\_{T})-Y^{\pi}\_{T}|^{2}. |  |

Finally, combining these bounds with the result of Theorem [2.6](https://arxiv.org/html/2511.08735v1#S2.Thmthm6 "Theorem 2.6. ‚Ä£ 2.2 Solutions to FBSDEs ‚Ä£ 2 Preliminaries ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications"), we complete the proof.
‚àé

## 4 A Convergence Analysis

In this section, we establish an estimate for the minimized objective function associated with the stochastic optimization problem ([3.1](https://arxiv.org/html/2511.08735v1#S3.E1 "In 3 Numerical Scheme and Simulation Error for Non-Markovian Coupled-FBSDE ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")-[3.2](https://arxiv.org/html/2511.08735v1#S3.E2 "In 3 Numerical Scheme and Simulation Error for Non-Markovian Coupled-FBSDE ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")). This result serves as a theoretical justification for employing our numerical scheme in the context of non-Markovian forward-backward stochastic differential equations (FBSDEs).

###### Theorem 4.1.

Suppose all assumptions of Theorem [2.3](https://arxiv.org/html/2511.08735v1#S2.Thmthm3 "Theorem 2.3 (Continuity of Solution). ‚Ä£ 2.2 Solutions to FBSDEs ‚Ä£ 2 Preliminaries ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications") hold. Let XTœÄ,Y0œÄ,YTœÄ,{ZtiœÄ}0‚â§i‚â§N‚àí1X^{\pi}\_{T},Y^{\pi}\_{0},Y^{\pi}\_{T},\{Z^{\pi}\_{t\_{i}}\}\_{0\leq i\leq N-1} be defined as in ([3.2](https://arxiv.org/html/2511.08735v1#S3.E2 "In 3 Numerical Scheme and Simulation Error for Non-Markovian Coupled-FBSDE ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")). Then, there exists a constant C>0C>0, independent of h,dh,d and mm, such that for sufficiently small hh,

|  |  |  |  |
| --- | --- | --- | --- |
|  | infùí¥0,ùíµi‚ààùí©‚Äãùí©‚Äã(‚ãÖ)ùîº\displaystyle\inf\_{\mathcal{Y}\_{0},\mathcal{Z}\_{i}\in\mathcal{NN}(\cdot)}\mathbb{E} | |g(VT,XTœÄ)‚àíYTœÄ|2‚â§C[(I02+ùîº|x0|2)h+œÅ(h)+hùîº‚à´0T|Zs|2ds+‚àëi=0N‚àí1Ezi\displaystyle|g(V\_{T},X^{\pi}\_{T})-Y^{\pi}\_{T}|^{2}\leq C\Bigg[\big(I\_{0}^{2}+\mathbb{E}|x\_{0}|^{2}\big)h+\rho(h)+h\mathbb{E}\int\_{0}^{T}|Z\_{s}|^{2}ds+\sum\_{i=0}^{N-1}E^{i}\_{z} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +infùí¥0,ùíµi‚ààùí©‚Äãùí©‚Äã(‚ãÖ)(‚àëi=0N‚àí1ùîº|Z~ti‚àíùíµi(XtiœÄ,{Ws,Vs:0‚â§s‚â§ti})|2h+ùîº|Y0‚àíùí¥0(x0)|2)],\displaystyle+\inf\_{\mathcal{Y}\_{0},\mathcal{Z}\_{i}\in\mathcal{NN}(\cdot)}\left(\sum\_{i=0}^{N-1}\mathbb{E}|\tilde{Z}\_{t\_{i}}-\mathcal{Z}\_{i}\big(X^{\pi}\_{t\_{i}},\{W\_{s},V\_{s}:0\leq s\leq t\_{i}\}\big)|^{2}h+\mathbb{E}|Y\_{0}-\mathcal{Y}\_{0}(x\_{0})|^{2}\right)\Bigg], |  |

where

|  |  |  |  |
| --- | --- | --- | --- |
|  | Z~ti\displaystyle\tilde{Z}\_{t\_{i}} | =h‚àí1‚Äãùîº‚Äã[‚à´titi+1Zt‚Äãùëët|‚Ñ±ti],\displaystyle=h^{-1}\mathbb{E}\Big[\int\_{t\_{i}}^{t\_{i+1}}Z\_{t}dt|\mathcal{F}\_{t\_{i}}\Big], |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | Ezi\displaystyle E^{i}\_{z} | =‚à´titi+1ùîº‚Äã|Zt‚àíZ~ti|2‚Äãùëët.\displaystyle=\int\_{t\_{i}}^{t\_{i+1}}\mathbb{E}|Z\_{t}-\tilde{Z}\_{t\_{i}}|^{2}dt. |  |

###### Proof.

Consider the piecewise constant Euler-Maruyama approximation defined on each interval [ti,ti+1)[t\_{i},t\_{i+1}). Define the numerical processes XtœÄX^{\pi}\_{t} and YtœÄY^{\pi}\_{t} by:

|  |  |  |  |
| --- | --- | --- | --- |
|  | {XtœÄ=XtiœÄ+b‚Äã(ti,Vti,XtiœÄ,YtiœÄ,ZtiœÄ)‚Äã(t‚àíti)+œÉ‚Äã(ti,Vti,XtiœÄ,YtiœÄ,ZtiœÄ)‚Äã(Wt‚àíWti),YtœÄ=YtiœÄ‚àíf‚Äã(ti,Vti,XtiœÄ,YtiœÄ,ZtiœÄ)‚Äã(t‚àíti)+ZtiœÄ‚Äã(Wt‚àíWti).\left\{\begin{array}[]{l}{X^{\pi}\_{t}=X^{\pi}\_{t\_{i}}+b(t\_{i},V\_{t\_{i}},X^{\pi}\_{t\_{i}},Y^{\pi}\_{t\_{i}},Z^{\pi}\_{t\_{i}})(t-t\_{i})+\sigma(t\_{i},V\_{t\_{i}},X^{\pi}\_{t\_{i}},Y^{\pi}\_{t\_{i}},Z^{\pi}\_{t\_{i}})(W\_{t}-W\_{t\_{i}})},\\ {Y^{\pi}\_{t}=Y^{\pi}\_{t\_{i}}-f(t\_{i},V\_{t\_{i}},X^{\pi}\_{t\_{i}},Y^{\pi}\_{t\_{i}},Z^{\pi}\_{t\_{i}})(t-t\_{i})+Z^{\pi}\_{t\_{i}}(W\_{t}-W\_{t\_{i}})}.\end{array}\right. |  | (4.1) |

Now, define the error processes by
Œ¥‚ÄãXt=Xt‚àíXtœÄ,Œ¥‚ÄãYt=Yt‚àíYtœÄ,Œ¥‚ÄãZt=Zt‚àíZtiœÄ‚Äã¬†for¬†‚Äãt‚àà[ti,ti+1)\delta X\_{t}=X\_{t}-X^{\pi}\_{t},\delta Y\_{t}=Y\_{t}-Y^{\pi}\_{t},\delta Z\_{t}=Z\_{t}-Z^{\pi}\_{t\_{i}}\text{ for }t\in[t\_{i},t\_{i+1}) and the coefficient differences by

|  |  |  |  |
| --- | --- | --- | --- |
|  | {Œ¥‚Äãbt=b‚Äã(t,Vt,Xt,Yt,Zt)‚àíb‚Äã(ti,Vti,XtiœÄ,YtiœÄ,ZtiœÄ),Œ¥‚ÄãœÉt=œÉ‚Äã(t,Vt,Xt,Yt,Zt)‚àíœÉ‚Äã(ti,Vti,XtiœÄ,YtiœÄ,ZtiœÄ),Œ¥‚Äãft=f‚Äã(t,Vt,Xt,Yt,Zt)‚àíf‚Äã(ti,Vti,XtiœÄ,YtiœÄ,ZtiœÄ).\left\{\begin{array}[]{l}{\delta b\_{t}=b(t,V\_{t},X\_{t},Y\_{t},Z\_{t})-b(t\_{i},V\_{t\_{i}},X^{\pi}\_{t\_{i}},Y^{\pi}\_{t\_{i}},Z^{\pi}\_{t\_{i}})},\\ {\delta\sigma\_{t}=\sigma(t,V\_{t},X\_{t},Y\_{t},Z\_{t})-\sigma(t\_{i},V\_{t\_{i}},X^{\pi}\_{t\_{i}},Y^{\pi}\_{t\_{i}},Z^{\pi}\_{t\_{i}})},\\ {\delta f\_{t}=f(t,V\_{t},X\_{t},Y\_{t},Z\_{t})-f(t\_{i},V\_{t\_{i}},X^{\pi}\_{t\_{i}},Y^{\pi}\_{t\_{i}},Z^{\pi}\_{t\_{i}})}.\end{array}\right. |  | (4.2) |

Then we have from ([4.1](https://arxiv.org/html/2511.08735v1#S4.E1 "In 4 A Convergence Analysis ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")),

|  |  |  |  |
| --- | --- | --- | --- |
|  | {d‚Äã(Œ¥‚ÄãXt)=Œ¥‚Äãbt‚Äãd‚Äãt+Œ¥‚ÄãœÉt‚Äãd‚ÄãWt,d‚Äã(Œ¥‚ÄãYt)=‚àíŒ¥‚Äãft‚Äãd‚Äãt+Œ¥‚ÄãZt‚Äãd‚ÄãWt.\left\{\begin{array}[]{l}{d(\delta X\_{t})=\delta b\_{t}dt+\delta\sigma\_{t}dW\_{t}},\\ {d(\delta Y\_{t})=-\delta f\_{t}dt+\delta Z\_{t}dW\_{t}}.\end{array}\right. |  | (4.3) |

Next, use of Ito^\hat{o}‚Äôs formula yields that

|  |  |  |  |
| --- | --- | --- | --- |
|  | {d‚Äã|Œ¥‚ÄãXt|2=(2‚Äã|Œ¥‚Äãbt|‚ãÖ|Œ¥‚ÄãXt|+|Œ¥‚ÄãœÉt|2)‚Äãd‚Äãt+2‚Äã|Œ¥‚ÄãXt|‚ãÖ|Œ¥‚ÄãœÉt|‚Äãd‚ÄãWt,d‚Äã|Œ¥‚ÄãYt|2=(‚àí2‚Äã|Œ¥‚Äãft|‚ãÖ|Œ¥‚ÄãYt|+|Œ¥‚ÄãZt|2)‚Äãd‚Äãt+2‚Äã|Œ¥‚ÄãYt|‚ãÖ|Œ¥‚ÄãZt|‚Äãd‚ÄãWt.\left\{\begin{array}[]{l}{d|\delta X\_{t}|^{2}=\big(2|\delta b\_{t}|\cdot|\delta X\_{t}|+|\delta\sigma\_{t}|^{2}\big)dt+2|\delta X\_{t}|\cdot|\delta\sigma\_{t}|dW\_{t}},\\ {d|\delta Y\_{t}|^{2}=\big(-2|\delta f\_{t}|\cdot|\delta Y\_{t}|+|\delta Z\_{t}|^{2}\big)dt+2|\delta Y\_{t}|\cdot|\delta Z\_{t}|dW\_{t}}.\end{array}\right. |  | (4.4) |

Now by taking expectations on both sides we get

|  |  |  |  |
| --- | --- | --- | --- |
|  | {ùîº‚Äã|Œ¥‚ÄãXt|2=ùîº‚Äã|Œ¥‚ÄãXti|2+ùîº‚Äã‚à´tit(2‚Äã|Œ¥‚Äãbs|‚ãÖ|Œ¥‚ÄãXs|+|Œ¥‚ÄãœÉs|2)‚Äãùëës,ùîº‚Äã|Œ¥‚ÄãYt|2=ùîº‚Äã|Œ¥‚ÄãYti|2+ùîº‚Äã‚à´tit(‚àí2‚Äã|Œ¥‚Äãfs|‚ãÖ|Œ¥‚ÄãYs|+|Œ¥‚ÄãZs|2)‚Äãùëës.\left\{\begin{array}[]{l}{\mathbb{E}|\delta X\_{t}|^{2}=\mathbb{E}|\delta X\_{t\_{i}}|^{2}+\mathbb{E}\int\_{t\_{i}}^{t}\big(2|\delta b\_{s}|\cdot|\delta X\_{s}|+|\delta\sigma\_{s}|^{2}\big)ds},\\ {\mathbb{E}|\delta Y\_{t}|^{2}=\mathbb{E}|\delta Y\_{t\_{i}}|^{2}+\mathbb{E}\int\_{t\_{i}}^{t}\big(-2|\delta f\_{s}|\cdot|\delta Y\_{s}|+|\delta Z\_{s}|^{2}\big)ds}.\end{array}\right. |  | (4.5) |

Then using Assumption [2.3](https://arxiv.org/html/2511.08735v1#S2.Thmass3 "Assumption 2.3. ‚Ä£ 2.1 Notations and Assumptions ‚Ä£ 2 Preliminaries ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications"), for any Œª5,Œª6>0\lambda\_{5},\lambda\_{6}>0, we have from the 1st equation of ([4.5](https://arxiv.org/html/2511.08735v1#S4.E5 "In 4 A Convergence Analysis ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) that

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùîº‚Äã|Œ¥‚ÄãXt|2‚â§\displaystyle\mathbb{E}|\delta X\_{t}|^{2}\leq\; | ùîº‚Äã|Œ¥‚ÄãXti|2+‚à´tit(Œª5‚Äãùîº‚Äã|Œ¥‚ÄãXs|2+Œª5‚àí1‚Äãùîº‚Äã|Œ¥‚Äãbs|2+ùîº‚Äã|Œ¥‚ÄãœÉs|2)‚Äãùëës\displaystyle\mathbb{E}|\delta X\_{t\_{i}}|^{2}+\int\_{t\_{i}}^{t}\big(\lambda\_{5}\mathbb{E}|\delta X\_{s}|^{2}+\lambda\_{5}^{-1}\mathbb{E}|\delta b\_{s}|^{2}+\mathbb{E}|\delta\sigma\_{s}|^{2}\big)ds |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚â§\displaystyle\leq\; | ùîº‚Äã|Œ¥‚ÄãXti|2+Œª5‚Äã‚à´titùîº‚Äã|Œ¥‚ÄãXs|2‚Äãùëës+‚à´tit(Œª5‚àí1+1)‚ÄãœÅ‚Äã(|s‚àíti|)‚Äãùëës\displaystyle\mathbb{E}|\delta X\_{t\_{i}}|^{2}+\lambda\_{5}\int\_{t\_{i}}^{t}\mathbb{E}|\delta X\_{s}|^{2}ds+\int\_{t\_{i}}^{t}(\lambda\_{5}^{-1}+1)\rho(|s-t\_{i}|)ds |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +‚à´tit((KŒª5‚àí1+œÉx)ùîº|Xs‚àíXtiœÄ|2+(byŒª5‚àí1+œÉy)ùîº|Ys‚àíYtiœÄ|2\displaystyle+\int\_{t\_{i}}^{t}\Big((K\lambda\_{5}^{-1}+\sigma\_{x})\mathbb{E}|X\_{s}-X^{\pi}\_{t\_{i}}|^{2}+(b\_{y}\lambda\_{5}^{-1}+\sigma\_{y})\mathbb{E}|Y\_{s}-Y^{\pi}\_{t\_{i}}|^{2} |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | +(bzŒª5‚àí1+œÉz)ùîº|Zs‚àíZtiœÄ|2)ds.\displaystyle+(b\_{z}\lambda\_{5}^{-1}+\sigma\_{z})\mathbb{E}|Z\_{s}-Z^{\pi}\_{t\_{i}}|^{2}\Big)ds. |  | (4.6) |

Note that, for any œµ1,œµ2>0\epsilon\_{1},\epsilon\_{2}>0,

|  |  |  |  |
| --- | --- | --- | --- |
|  | {ùîº‚Äã|Xs‚àíXtiœÄ|2‚â§(1+œµ1)‚Äãùîº‚Äã|Œ¥‚ÄãXti|2+(1+œµ1‚àí1)‚Äãùîº‚Äã|Xs‚àíXti|2,ùîº‚Äã|Ys‚àíYtiœÄ|2‚â§(1+œµ2)‚Äãùîº‚Äã|Œ¥‚ÄãYti|2+(1+œµ2‚àí1)‚Äãùîº‚Äã|Ys‚àíYti|2,\left\{\begin{array}[]{l}{\mathbb{E}|X\_{s}-X^{\pi}\_{t\_{i}}|^{2}\leq(1+\epsilon\_{1})\mathbb{E}|\delta X\_{t\_{i}}|^{2}+(1+\epsilon\_{1}^{-1})\mathbb{E}|X\_{s}-X\_{t\_{i}}|^{2}},\\ {\mathbb{E}|Y\_{s}-Y^{\pi}\_{t\_{i}}|^{2}\leq(1+\epsilon\_{2})\mathbb{E}|\delta Y\_{t\_{i}}|^{2}+(1+\epsilon\_{2}^{-1})\mathbb{E}|Y\_{s}-Y\_{t\_{i}}|^{2}},\end{array}\right. |  | (4.7) |

and we choose œµ1=Œª6‚Äã(K‚ÄãŒª5‚àí1+œÉx)‚àí1,œµ2=Œª6‚Äã(by‚ÄãŒª5‚àí1+œÉy)‚àí1\epsilon\_{1}=\lambda\_{6}(K\lambda\_{5}^{-1}+\sigma\_{x})^{-1},\epsilon\_{2}=\lambda\_{6}(b\_{y}\lambda\_{5}^{-1}+\sigma\_{y})^{-1}.
  
Also note, Theorem [2.3](https://arxiv.org/html/2511.08735v1#S2.Thmthm3 "Theorem 2.3 (Continuity of Solution). ‚Ä£ 2.2 Solutions to FBSDEs ‚Ä£ 2 Preliminaries ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications") yields

|  |  |  |  |
| --- | --- | --- | --- |
|  | sups‚àà[ti,ti+1)\displaystyle\sup\_{s\in[t\_{i},t\_{i+1})} | (ùîº‚Äã|Xs‚àíXti|2+ùîº‚Äã|Ys‚àíYti|2)\displaystyle\Big(\mathbb{E}|X\_{s}-X\_{t\_{i}}|^{2}+\mathbb{E}|Y\_{s}-Y\_{t\_{i}}|^{2}\Big) |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | ‚â§C‚Äã(I02+ùîº‚Äã|x0|2)‚Äãh+C‚Äãùîº‚Äã[‚à´titi+1|Zs|2‚Äãùëës+‚à´titi+1|œÉ|2‚Äã(s,Vs,0,0,0)‚Äãùëës].\displaystyle\leq C\big(I\_{0}^{2}+\mathbb{E}|x\_{0}|^{2}\big)h+C\mathbb{E}\left[\int\_{t\_{i}}^{t\_{i+1}}|Z\_{s}|^{2}ds+\int\_{t\_{i}}^{t\_{i+1}}|\sigma|^{2}(s,V\_{s},0,0,0)ds\right]. |  | (4.8) |

Now, first using the estimates from ([4.7](https://arxiv.org/html/2511.08735v1#S4.E7 "In 4 A Convergence Analysis ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) into ([4](https://arxiv.org/html/2511.08735v1#S4.Ex5 "4 A Convergence Analysis ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")), and then using the estimate from ([4](https://arxiv.org/html/2511.08735v1#S4.Ex8 "4 A Convergence Analysis ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")), we get

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùîº‚Äã|Œ¥‚ÄãXt|2‚â§\displaystyle\mathbb{E}|\delta X\_{t}|^{2}\leq\; | ùîº‚Äã|Œ¥‚ÄãXti|2+Œª5‚Äã‚à´titùîº‚Äã|Œ¥‚ÄãXs|2‚Äãùëës+‚à´tit(Œª5‚àí1+1)‚ÄãœÅ‚Äã(|s‚àíti|)‚Äãùëës\displaystyle\mathbb{E}|\delta X\_{t\_{i}}|^{2}+\lambda\_{5}\int\_{t\_{i}}^{t}\mathbb{E}|\delta X\_{s}|^{2}ds+\int\_{t\_{i}}^{t}(\lambda\_{5}^{-1}+1)\rho(|s-t\_{i}|)ds |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +‚à´tit((KŒª5‚àí1+œÉx)(1+œµ1)ùîº|Œ¥Xti|2+(KŒª5‚àí1+œÉx)(1+œµ1‚àí1)ùîº|Xs‚àíXti|2\displaystyle+\int\_{t\_{i}}^{t}\Big((K\lambda\_{5}^{-1}+\sigma\_{x})(1+\epsilon\_{1})\mathbb{E}|\delta X\_{t\_{i}}|^{2}+(K\lambda\_{5}^{-1}+\sigma\_{x})(1+\epsilon\_{1}^{-1})\mathbb{E}|X\_{s}-X\_{t\_{i}}|^{2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +(by‚ÄãŒª5‚àí1+œÉy)‚Äã(1+œµ2)‚Äãùîº‚Äã|Œ¥‚ÄãYti|2+(by‚ÄãŒª5‚àí1+œÉy)‚Äã(1+œµ2‚àí1)‚Äãùîº‚Äã|Ys‚àíYti|2\displaystyle+(b\_{y}\lambda\_{5}^{-1}+\sigma\_{y})(1+\epsilon\_{2})\mathbb{E}|\delta Y\_{t\_{i}}|^{2}+(b\_{y}\lambda\_{5}^{-1}+\sigma\_{y})(1+\epsilon\_{2}^{-1})\mathbb{E}|Y\_{s}-Y\_{t\_{i}}|^{2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +(bzŒª5‚àí1+œÉz)ùîº|Œ¥Zs|2)ds\displaystyle+(b\_{z}\lambda\_{5}^{-1}+\sigma\_{z})\mathbb{E}|\delta Z\_{s}|^{2}\Big)ds |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚â§\displaystyle\leq\; | (1+(Œª6+K‚ÄãŒª5‚àí1+œÉx)‚Äãh)‚Äãùîº‚Äã|Œ¥‚ÄãXti|2+(Œª6+by‚ÄãŒª5‚àí1+œÉy)‚Äãh‚Äãùîº‚Äã|Œ¥‚ÄãYti|2\displaystyle\big(1+(\lambda\_{6}+K\lambda\_{5}^{-1}+\sigma\_{x})h\big)\mathbb{E}|\delta X\_{t\_{i}}|^{2}+(\lambda\_{6}+b\_{y}\lambda\_{5}^{-1}+\sigma\_{y})h\mathbb{E}|\delta Y\_{t\_{i}}|^{2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +‚à´tit(bz‚ÄãŒª5‚àí1+œÉz)‚Äãùîº‚Äã|Œ¥‚ÄãZs|2‚Äãùëës+Œª5‚Äã‚à´titùîº‚Äã|Œ¥‚ÄãXs|2‚Äãùëës+C‚Äã((I02+ùîº‚Äã|x0|2)‚Äãh+œÅ‚Äã(h))‚Äãh\displaystyle+\int\_{t\_{i}}^{t}(b\_{z}\lambda\_{5}^{-1}+\sigma\_{z})\mathbb{E}|\delta Z\_{s}|^{2}ds+\lambda\_{5}\int\_{t\_{i}}^{t}\mathbb{E}|\delta X\_{s}|^{2}ds+C\Big(\big(I\_{0}^{2}+\mathbb{E}|x\_{0}|^{2}\big)h+\rho(h)\Big)h |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | +C‚Äãh‚Äãùîº‚Äã[‚à´titi+1|Zs|2‚Äãùëës+‚à´titi+1|œÉ|2‚Äã(s,Vs,0,0,0)‚Äãùëës].\displaystyle+Ch\mathbb{E}\left[\int\_{t\_{i}}^{t\_{i+1}}|Z\_{s}|^{2}ds+\int\_{t\_{i}}^{t\_{i+1}}|\sigma|^{2}(s,V\_{s},0,0,0)ds\right]. |  | (4.9) |

Then applying Gro¬®\ddot{o}nwall inequality into ([4](https://arxiv.org/html/2511.08735v1#S4.Ex9 "4 A Convergence Analysis ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) and using the estimate

|  |  |  |
| --- | --- | --- |
|  | ‚à´titi+1ùîº‚Äã|Œ¥‚ÄãZt|2‚Äãùëët‚â§(1+œµ3)‚Äãùîº‚Äã|Œ¥‚ÄãZ~ti|2‚Äãh+(1+œµ3‚àí1)‚ÄãEzi,\int\_{t\_{i}}^{t\_{i+1}}\mathbb{E}|\delta Z\_{t}|^{2}dt\leq(1+\epsilon\_{3})\mathbb{E}|\delta\tilde{Z}\_{t\_{i}}|^{2}h+(1+\epsilon\_{3}^{-1})E^{i}\_{z}, |  |

with œµ3=(bz‚ÄãŒª5‚àí1+œÉz)‚àí1‚ÄãŒª6\epsilon\_{3}=(b\_{z}\lambda\_{5}^{-1}+\sigma\_{z})^{-1}\lambda\_{6}, we obtain

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùîº\displaystyle\mathbb{E} | |Œ¥‚ÄãXti+1|2\displaystyle|\delta X\_{t\_{i+1}}|^{2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§eŒª5‚Äãh((1+(Œª6+KŒª5‚àí1+œÉx)h)ùîº|Œ¥Xti|2+(Œª6+byŒª5‚àí1+œÉy)hùîº|Œ¥Yti|2\displaystyle\leq e^{\lambda\_{5}h}\bigg(\big(1+(\lambda\_{6}+K\lambda\_{5}^{-1}+\sigma\_{x})h\big)\mathbb{E}|\delta X\_{t\_{i}}|^{2}+(\lambda\_{6}+b\_{y}\lambda\_{5}^{-1}+\sigma\_{y})h\mathbb{E}|\delta Y\_{t\_{i}}|^{2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +‚à´titi+1(bz‚ÄãŒª5‚àí1+œÉz)‚Äãùîº‚Äã|Œ¥‚ÄãZs|2‚Äãùëës+C‚Äã((I02+ùîº‚Äã|x0|2)‚Äãh+œÅ‚Äã(h))‚Äãh\displaystyle+\int\_{t\_{i}}^{t\_{i+1}}(b\_{z}\lambda\_{5}^{-1}+\sigma\_{z})\mathbb{E}|\delta Z\_{s}|^{2}ds+C\Big(\big(I\_{0}^{2}+\mathbb{E}|x\_{0}|^{2}\big)h+\rho(h)\Big)h |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +Chùîº[‚à´titi+1|Zs|2ds+‚à´titi+1|œÉ|2(s,Vs,0,0,0)ds])\displaystyle+Ch\mathbb{E}\left[\int\_{t\_{i}}^{t\_{i+1}}|Z\_{s}|^{2}ds+\int\_{t\_{i}}^{t\_{i+1}}|\sigma|^{2}(s,V\_{s},0,0,0)ds\right]\bigg) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§e(Œª5+Œª6+K‚ÄãŒª5‚àí1+œÉx)‚Äãh‚Äãùîº‚Äã|Œ¥‚ÄãXti|2+eŒª5‚Äãh‚Äã(by‚ÄãŒª5‚àí1+Œª6+œÉy)‚Äãh‚Äãùîº‚Äã|Œ¥‚ÄãYti|2+eŒª5‚Äãh‚Äã(bz‚ÄãŒª5‚àí1+Œª6+œÉz)‚Äãùîº‚Äã|Œ¥‚ÄãZ~ti|2‚Äãh\displaystyle\leq e^{(\lambda\_{5}+\lambda\_{6}+K\lambda\_{5}^{-1}+\sigma\_{x})h}\mathbb{E}|\delta X\_{t\_{i}}|^{2}+e^{\lambda\_{5}h}(b\_{y}\lambda\_{5}^{-1}+\lambda\_{6}+\sigma\_{y})h\mathbb{E}|\delta Y\_{t\_{i}}|^{2}+e^{\lambda\_{5}h}(b\_{z}\lambda\_{5}^{-1}+\lambda\_{6}+\sigma\_{z})\mathbb{E}|\delta\tilde{Z}\_{t\_{i}}|^{2}h |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +C‚ÄãEzi+C‚Äã((I02+ùîº‚Äã|x0|2)‚Äãh+œÅ‚Äã(h))‚Äãh+C‚Äãh‚Äãùîº‚Äã[‚à´titi+1|Zs|2‚Äãùëës+‚à´titi+1|œÉ|2‚Äã(s,Vs,0,0,0)‚Äãùëës]\displaystyle+CE^{i}\_{z}+C\Big(\big(I\_{0}^{2}+\mathbb{E}|x\_{0}|^{2}\big)h+\rho(h)\Big)h+Ch\mathbb{E}\left[\int\_{t\_{i}}^{t\_{i+1}}|Z\_{s}|^{2}ds+\int\_{t\_{i}}^{t\_{i+1}}|\sigma|^{2}(s,V\_{s},0,0,0)ds\right] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§eA7‚Äãh‚Äãùîº‚Äã|Œ¥‚ÄãXti|2+A8‚Äãùîº‚Äã|Œ¥‚ÄãYti|2‚Äãh+A9‚Äãùîº‚Äã|Œ¥‚ÄãZ~ti|2‚Äãh+C‚ÄãEzi+C‚Äã((I02+ùîº‚Äã|x0|2)‚Äãh+œÅ‚Äã(h))‚Äãh\displaystyle\leq e^{A\_{7}h}\mathbb{E}|\delta X\_{t\_{i}}|^{2}+A\_{8}\mathbb{E}|\delta Y\_{t\_{i}}|^{2}h+A\_{9}\mathbb{E}|\delta\tilde{Z}\_{t\_{i}}|^{2}h+CE^{i}\_{z}+C\Big(\big(I\_{0}^{2}+\mathbb{E}|x\_{0}|^{2}\big)h+\rho(h)\Big)h |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | +C‚Äãh‚Äãùîº‚Äã[‚à´titi+1|Zs|2‚Äãùëës+‚à´titi+1|œÉ|2‚Äã(s,Vs,0,0,0)‚Äãùëës],\displaystyle+Ch\mathbb{E}\left[\int\_{t\_{i}}^{t\_{i+1}}|Z\_{s}|^{2}ds+\int\_{t\_{i}}^{t\_{i+1}}|\sigma|^{2}(s,V\_{s},0,0,0)ds\right], |  | (4.10) |

where A7=K‚ÄãŒª5‚àí1+œÉx+Œª5+Œª6,A8=by‚ÄãŒª5‚àí1+2‚ÄãŒª6+œÉy,A9=bz‚ÄãŒª5‚àí1+2‚ÄãŒª6+œÉz,A\_{7}=K\lambda\_{5}^{-1}+\sigma\_{x}+\lambda\_{5}+\lambda\_{6},A\_{8}=b\_{y}\lambda\_{5}^{-1}+2\lambda\_{6}+\sigma\_{y},A\_{9}=b\_{z}\lambda\_{5}^{-1}+2\lambda\_{6}+\sigma\_{z}, and hh is sufficiently small.

Similarly, for any Œª5,Œª6>0\lambda\_{5},\lambda\_{6}>0, we have from the 2nd equation of ([4.5](https://arxiv.org/html/2511.08735v1#S4.E5 "In 4 A Convergence Analysis ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) that

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùîº‚Äã|Œ¥‚ÄãYt|2‚â§\displaystyle\mathbb{E}|\delta Y\_{t}|^{2}\leq\; | ùîº‚Äã|Œ¥‚ÄãYti|2+‚à´tit(Œª5‚Äãùîº‚Äã|Œ¥‚ÄãYs|2+Œª5‚àí1‚Äãùîº‚Äã|Œ¥‚Äãfs|2+ùîº‚Äã|Œ¥‚ÄãZs|2)‚Äãùëës\displaystyle\mathbb{E}|\delta Y\_{t\_{i}}|^{2}+\int\_{t\_{i}}^{t}\Big(\lambda\_{5}\mathbb{E}|\delta Y\_{s}|^{2}+\lambda\_{5}^{-1}\mathbb{E}|\delta f\_{s}|^{2}+\mathbb{E}|\delta Z\_{s}|^{2}\Big)ds |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚â§\displaystyle\leq\; | ùîº‚Äã|Œ¥‚ÄãYti|2+Œª5‚Äã‚à´titùîº‚Äã|Œ¥‚ÄãYs|2‚Äãùëës+‚à´titŒª5‚àí1‚Äã(fx‚Äãùîº‚Äã|Xs‚àíXtiœÄ|2+K‚Äãùîº‚Äã|Ys‚àíYtiœÄ|2)‚Äãùëës\displaystyle\mathbb{E}|\delta Y\_{t\_{i}}|^{2}+\lambda\_{5}\int\_{t\_{i}}^{t}\mathbb{E}|\delta Y\_{s}|^{2}ds+\int\_{t\_{i}}^{t}\lambda\_{5}^{-1}\Big(f\_{x}\mathbb{E}|X\_{s}-X^{\pi}\_{t\_{i}}|^{2}+K\mathbb{E}|Y\_{s}-Y^{\pi}\_{t\_{i}}|^{2}\Big)ds |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +‚à´titŒª5‚àí1‚ÄãœÅ‚Äã(|s‚àíti|)‚Äãùëës+(1+fz‚ÄãŒª5‚àí1)‚Äã‚à´titùîº‚Äã|Œ¥‚ÄãZs|2‚Äãùëës\displaystyle+\int\_{t\_{i}}^{t}\lambda\_{5}^{-1}\rho(|s-t\_{i}|)ds+(1+f\_{z}\lambda\_{5}^{-1})\int\_{t\_{i}}^{t}\mathbb{E}|\delta Z\_{s}|^{2}ds |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚â§\displaystyle\leq | (1+(K‚ÄãŒª5‚àí1+Œª6)‚Äãh)‚Äãùîº‚Äã|Œ¥‚ÄãYti|2+Œª5‚Äã‚à´titùîº‚Äã|Œ¥‚ÄãYs|2‚Äãùëës+(fx‚ÄãŒª5‚àí1+Œª6)‚Äãùîº‚Äã|Œ¥‚ÄãXti|2‚Äãh\displaystyle\big(1+(K\lambda\_{5}^{-1}+\lambda\_{6})h\big)\mathbb{E}|\delta Y\_{t\_{i}}|^{2}+\lambda\_{5}\int\_{t\_{i}}^{t}\mathbb{E}|\delta Y\_{s}|^{2}ds+(f\_{x}\lambda\_{5}^{-1}+\lambda\_{6})\mathbb{E}|\delta X\_{t\_{i}}|^{2}h |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +(1+fz‚ÄãŒª5‚àí1)‚Äã‚à´titùîº‚Äã|Œ¥‚ÄãZs|2‚Äãùëës+Œª5‚àí1‚ÄãœÅ‚Äã(h)‚Äãh+C‚Äã‚à´titùîº‚Äã[|Ys‚àíYti|2+|Xs‚àíXti|2]‚Äãùëës\displaystyle+(1+f\_{z}\lambda\_{5}^{-1})\int\_{t\_{i}}^{t}\mathbb{E}|\delta Z\_{s}|^{2}ds+\lambda\_{5}^{-1}\rho(h)h+C\int\_{t\_{i}}^{t}\mathbb{E}\bigg[|Y\_{s}-Y\_{t\_{i}}|^{2}+|X\_{s}-X\_{t\_{i}}|^{2}\bigg]ds |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚â§\displaystyle\leq\; | (1+(K‚ÄãŒª5‚àí1+Œª6)‚Äãh)‚Äãùîº‚Äã|Œ¥‚ÄãYti|2+Œª5‚Äã‚à´titùîº‚Äã|Œ¥‚ÄãYs|2‚Äãùëës+(fx‚ÄãŒª5‚àí1+Œª6)‚Äãùîº‚Äã|Œ¥‚ÄãXti|2‚Äãh\displaystyle\big(1+(K\lambda\_{5}^{-1}+\lambda\_{6})h\big)\mathbb{E}|\delta Y\_{t\_{i}}|^{2}+\lambda\_{5}\int\_{t\_{i}}^{t}\mathbb{E}|\delta Y\_{s}|^{2}ds+(f\_{x}\lambda\_{5}^{-1}+\lambda\_{6})\mathbb{E}|\delta X\_{t\_{i}}|^{2}h |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +(1+fz‚ÄãŒª5‚àí1)‚Äã‚à´titùîº‚Äã|Œ¥‚ÄãZs|2‚Äãùëës+C‚Äã((I02+ùîº‚Äã|x0|2)‚Äãh+œÅ‚Äã(h))‚Äãh\displaystyle+(1+f\_{z}\lambda\_{5}^{-1})\int\_{t\_{i}}^{t}\mathbb{E}|\delta Z\_{s}|^{2}ds+C\Big(\big(I\_{0}^{2}+\mathbb{E}|x\_{0}|^{2}\big)h+\rho(h)\Big)h |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | +C‚Äãh‚Äãùîº‚Äã[‚à´titi+1|Zs|2‚Äãùëës+‚à´titi+1|œÉ|2‚Äã(s,Vs,0,0,0)‚Äãùëës].\displaystyle+Ch\mathbb{E}\left[\int\_{t\_{i}}^{t\_{i+1}}|Z\_{s}|^{2}ds+\int\_{t\_{i}}^{t\_{i+1}}|\sigma|^{2}(s,V\_{s},0,0,0)ds\right]. |  | (4.11) |

Then once again using Gro¬®\ddot{o}nwall inequality, for sufficiently small hh, we have from ([4](https://arxiv.org/html/2511.08735v1#S4.Ex23 "4 A Convergence Analysis ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) that

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùîº‚Äã|Œ¥‚ÄãYti+1|2‚â§\displaystyle\mathbb{E}|\delta Y\_{t\_{i+1}}|^{2}\leq | eA10‚Äãh‚Äãùîº‚Äã|Œ¥‚ÄãYti|2+A11‚Äãùîº‚Äã|Œ¥‚ÄãXti|2‚Äãh+A12‚Äãùîº‚Äã|Œ¥‚ÄãZ~ti|2‚Äãh+C‚Äã((I02+ùîº‚Äã|x0|2)‚Äãh+œÅ‚Äã(h))‚Äãh\displaystyle e^{A\_{10}h}\mathbb{E}|\delta Y\_{t\_{i}}|^{2}+A\_{11}\mathbb{E}|\delta X\_{t\_{i}}|^{2}h+A\_{12}\mathbb{E}|\delta\tilde{Z}\_{t\_{i}}|^{2}h+C\Big(\big(I\_{0}^{2}+\mathbb{E}|x\_{0}|^{2}\big)h+\rho(h)\Big)h |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | +C‚Äãh‚Äãùîº‚Äã[‚à´titi+1|Zs|2‚Äãùëës+‚à´titi+1|œÉ|2‚Äã(s,Vs,0,0,0)‚Äãùëës],\displaystyle+Ch\mathbb{E}\left[\int\_{t\_{i}}^{t\_{i+1}}|Z\_{s}|^{2}ds+\int\_{t\_{i}}^{t\_{i+1}}|\sigma|^{2}(s,V\_{s},0,0,0)ds\right], |  | (4.12) |

where A10=Œª5+K‚ÄãŒª5‚àí1+Œª6,A11=fx‚ÄãŒª5‚àí1+2‚ÄãŒª6,A12=1+2‚ÄãŒª6+fz‚ÄãŒª5‚àí1A\_{10}=\lambda\_{5}+K\lambda\_{5}^{-1}+\lambda\_{6},A\_{11}=f\_{x}\lambda\_{5}^{-1}+2\lambda\_{6},A\_{12}=1+2\lambda\_{6}+f\_{z}\lambda\_{5}^{-1}.
  
Now define Mi=m‚Äãa‚Äãx‚Äã{ùîº‚Äã|Œ¥‚ÄãXi|2,ùîº‚Äã|Œ¥‚ÄãYi|2},0‚â§i‚â§N,M\_{i}=max\{\mathbb{E}|\delta X\_{i}|^{2},\mathbb{E}|\delta Y\_{i}|^{2}\},0\leq i\leq N, and then from estimates ([4](https://arxiv.org/html/2511.08735v1#S4.Ex16 "4 A Convergence Analysis ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) and ([4](https://arxiv.org/html/2511.08735v1#S4.Ex30 "4 A Convergence Analysis ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | Mi+1\displaystyle M\_{i+1} | ‚â§(em‚Äãa‚Äãx‚Äã{A7,A10}‚Äãh+m‚Äãa‚Äãx‚Äã{A8,A11}‚Äãh)‚ÄãMi+m‚Äãa‚Äãx‚Äã{A9,A12}‚Äãùîº‚Äã|Œ¥‚ÄãZ~ti|2‚Äãh+C‚ÄãEzi\displaystyle\leq\bigg(e^{max\{A\_{7},A\_{10}\}h}+max\{A\_{8},A\_{11}\}h\bigg)M\_{i}+max\{A\_{9},A\_{12}\}\mathbb{E}|\delta\tilde{Z}\_{t\_{i}}|^{2}h+CE^{i}\_{z} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +C‚Äã((I02+ùîº‚Äã|x0|2)‚Äãh+œÅ‚Äã(h))‚Äãh+C‚Äãh‚Äãùîº‚Äã[‚à´titi+1|Zs|2‚Äãùëës+‚à´titi+1|œÉ|2‚Äã(s,Vs,0,0,0)‚Äãùëës]\displaystyle\quad+C\Big(\big(I\_{0}^{2}+\mathbb{E}|x\_{0}|^{2}\big)h+\rho(h)\Big)h+Ch\mathbb{E}\left[\int\_{t\_{i}}^{t\_{i+1}}|Z\_{s}|^{2}ds+\int\_{t\_{i}}^{t\_{i+1}}|\sigma|^{2}(s,V\_{s},0,0,0)ds\right] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§e(m‚Äãa‚Äãx‚Äã{A7,A10}+m‚Äãa‚Äãx‚Äã{A8,A11})‚Äãh‚ÄãMi+m‚Äãa‚Äãx‚Äã{A9,A12}‚Äãùîº‚Äã|Œ¥‚ÄãZ~ti|2‚Äãh+C‚ÄãEzi\displaystyle\leq e^{(max\{A\_{7},A\_{10}\}+max\{A\_{8},A\_{11}\})h}M\_{i}+max\{A\_{9},A\_{12}\}\mathbb{E}|\delta\tilde{Z}\_{t\_{i}}|^{2}h+CE^{i}\_{z} |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | +C‚Äã((I02+ùîº‚Äã|x0|2)‚Äãh+œÅ‚Äã(h))‚Äãh+C‚Äãh‚Äãùîº‚Äã[‚à´titi+1|Zs|2‚Äãùëës+‚à´titi+1|œÉ|2‚Äã(s,Vs,0,0,0)‚Äãùëës].\displaystyle\quad+C\Big(\big(I\_{0}^{2}+\mathbb{E}|x\_{0}|^{2}\big)h+\rho(h)\Big)h+Ch\mathbb{E}\left[\int\_{t\_{i}}^{t\_{i+1}}|Z\_{s}|^{2}ds+\int\_{t\_{i}}^{t\_{i+1}}|\sigma|^{2}(s,V\_{s},0,0,0)ds\right]. |  | (4.13) |

By letting A13=m‚Äãa‚Äãx‚Äã{A7,A10}+m‚Äãa‚Äãx‚Äã{A8,A11},A14=m‚Äãa‚Äãx‚Äã{A9,A12}A\_{13}=max\{A\_{7},A\_{10}\}+max\{A\_{8},A\_{11}\},A\_{14}=max\{A\_{9},A\_{12}\} we further have

|  |  |  |  |
| --- | --- | --- | --- |
|  | Mi+1‚â§\displaystyle M\_{i+1}\leq | eA13‚Äãh‚ÄãMi+A14‚Äãùîº‚Äã|Œ¥‚ÄãZ~ti|2‚Äãh+C‚ÄãEzi+C‚Äã((I02+ùîº‚Äã|x0|2)‚Äãh+œÅ‚Äã(h))‚Äãh\displaystyle e^{A\_{13}h}M\_{i}+A\_{14}\mathbb{E}|\delta\tilde{Z}\_{t\_{i}}|^{2}h+CE^{i}\_{z}+C\Big(\big(I\_{0}^{2}+\mathbb{E}|x\_{0}|^{2}\big)h+\rho(h)\Big)h |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | +C‚Äãh‚Äãùîº‚Äã[‚à´titi+1|Zs|2‚Äãùëës+‚à´titi+1|œÉ|2‚Äã(s,Vs,0,0,0)‚Äãùëës].\displaystyle+Ch\mathbb{E}\left[\int\_{t\_{i}}^{t\_{i+1}}|Z\_{s}|^{2}ds+\int\_{t\_{i}}^{t\_{i+1}}|\sigma|^{2}(s,V\_{s},0,0,0)ds\right]. |  | (4.14) |

Since M0=ùîº‚Äã|Y0‚àíY0œÄ|2M\_{0}=\mathbb{E}|Y\_{0}-Y^{\pi}\_{0}|^{2}, we can also have

|  |  |  |  |
| --- | --- | --- | --- |
|  | MN‚â§\displaystyle M\_{N}\leq | A14‚ÄãeA13‚ÄãT‚Äã‚àëi=0N‚àí1ùîº‚Äã|Œ¥‚ÄãZ~ti|2‚Äãh\displaystyle A\_{14}e^{A\_{13}T}\sum\_{i=0}^{N-1}\mathbb{E}|\delta\tilde{Z}\_{t\_{i}}|^{2}h |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | +C‚Äã(T)‚Äã((I02+ùîº‚Äã|x0|2)‚Äãh+œÅ‚Äã(h)+ùîº‚Äã|Y0‚àíY0œÄ|2+h‚Äãùîº‚Äã‚à´0T|Zs|2‚Äãùëës+‚àëi=0N‚àí1Ezi).\displaystyle+C(T)\bigg(\big(I\_{0}^{2}+\mathbb{E}|x\_{0}|^{2}\big)h+\rho(h)+\mathbb{E}|Y\_{0}-Y^{\pi}\_{0}|^{2}+h\mathbb{E}\int\_{0}^{T}|Z\_{s}|^{2}ds+\sum\_{i=0}^{N-1}E^{i}\_{z}\bigg). |  | (4.15) |

Note that, for K¬Ø=max‚Å°{K,fx,fz,by,bz,œÉx,œÉy,œÉz}\bar{K}=\max\{K,f\_{x},f\_{z},b\_{y},b\_{z},\sigma\_{x},\sigma\_{y},\sigma\_{z}\}:

|  |  |  |
| --- | --- | --- |
|  | A14‚â§1+K¬Ø‚ÄãŒª5‚àí1+K¬Ø+2‚ÄãŒª6,A\_{14}\leq 1+\bar{K}\lambda\_{5}^{-1}+\bar{K}+2\lambda\_{6}, |  |

|  |  |  |
| --- | --- | --- |
|  | A13‚â§2‚ÄãK¬Ø+2‚ÄãK¬Ø‚ÄãŒª5‚àí1+Œª5+3‚ÄãŒª6.A\_{13}\leq 2\bar{K}+2\bar{K}\lambda\_{5}^{-1}+\lambda\_{5}+3\lambda\_{6}. |  |

Given any Œª4>0\lambda\_{4}>0, we can choose Œª6\lambda\_{6} small enough such that

|  |  |  |
| --- | --- | --- |
|  | A14‚ÄãeA13‚ÄãT‚â§(1+Œª4)‚Äã(K¬Ø‚ÄãŒª5‚àí1+K¬Ø+1)‚Äãe(2‚ÄãK¬Ø‚ÄãŒª5‚àí1+2‚ÄãK¬Ø+Œª5)‚ÄãT.A\_{14}e^{A\_{13}T}\leq(1+\lambda\_{4})(\bar{K}\lambda\_{5}^{-1}+\bar{K}+1)e^{(2\bar{K}\lambda\_{5}^{-1}+2\bar{K}+\lambda\_{5})T}. |  |

Thus

|  |  |  |  |
| --- | --- | --- | --- |
|  | MN‚â§\displaystyle M\_{N}\leq | (1+Œª4)‚Äã(K¬Ø‚ÄãŒª5‚àí1+K¬Ø+1)‚Äãe(2‚ÄãK¬Ø‚ÄãŒª5‚àí1+2‚ÄãK¬Ø+Œª5)‚ÄãT‚Äã‚àëi=0N‚àí1ùîº‚Äã|Œ¥‚ÄãZ~ti|2‚Äãh\displaystyle(1+\lambda\_{4})(\bar{K}\lambda\_{5}^{-1}+\bar{K}+1)e^{(2\bar{K}\lambda\_{5}^{-1}+2\bar{K}+\lambda\_{5})T}\sum\_{i=0}^{N-1}\mathbb{E}|\delta\tilde{Z}\_{t\_{i}}|^{2}h |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | +C‚Äã(T)‚Äã((I02+ùîº‚Äã|x0|2)‚Äãh+œÅ‚Äã(h)+ùîº‚Äã|Y0‚àíY0œÄ|2+h‚Äãùîº‚Äã‚à´0T|Zs|2‚Äãùëës+‚àëi=0N‚àí1Ezi),\displaystyle+C(T)\bigg(\big(I\_{0}^{2}+\mathbb{E}|x\_{0}|^{2}\big)h+\rho(h)+\mathbb{E}|Y\_{0}-Y^{\pi}\_{0}|^{2}+h\mathbb{E}\int\_{0}^{T}|Z\_{s}|^{2}ds+\sum\_{i=0}^{N-1}E^{i}\_{z}\bigg), |  | (4.16) |

Now, from the decomposition of the objective function, we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùîº‚Äã|g‚Äã(VT,XTœÄ)‚àíYTœÄ|2=\displaystyle\mathbb{E}|g(V\_{T},X^{\pi}\_{T})-Y^{\pi}\_{T}|^{2}= | ùîº‚Äã|g‚Äã(VT,XTœÄ)‚àíg‚Äã(VT,XT)+YT‚àíYTœÄ|2\displaystyle\mathbb{E}|g(V\_{T},X^{\pi}\_{T})-g(V\_{T},X\_{T})+Y\_{T}-Y^{\pi}\_{T}|^{2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚â§\displaystyle\leq | (1+(gx)‚àí1)‚Äãùîº‚Äã|g‚Äã(VT,XTœÄ)‚àíg‚Äã(VT,XT)|2+(1+gx)‚Äãùîº‚Äã|Œ¥‚ÄãYN|2\displaystyle\big(1+(\sqrt{g\_{x}})^{-1}\big)\mathbb{E}|g(V\_{T},X^{\pi}\_{T})-g(V\_{T},X\_{T})|^{2}+(1+\sqrt{g\_{x}})\mathbb{E}|\delta Y\_{N}|^{2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | ‚â§\displaystyle\leq | (gx+gx)‚Äãùîº‚Äã|Œ¥‚ÄãXN|2+(1+gx)‚Äãùîº‚Äã|Œ¥‚ÄãYN|2\displaystyle(g\_{x}+\sqrt{g\_{x}})\mathbb{E}|\delta X\_{N}|^{2}+(1+\sqrt{g\_{x}})\mathbb{E}|\delta Y\_{N}|^{2} |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | ‚â§\displaystyle\leq | (1+gx)2‚ÄãMN.\displaystyle(1+\sqrt{g\_{x}})^{2}M\_{N}. |  | (4.17) |

Then, by substituting the estimate for MNM\_{N} from ([4](https://arxiv.org/html/2511.08735v1#S4.Ex39 "4 A Convergence Analysis ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) into ([4](https://arxiv.org/html/2511.08735v1#S4.Ex40 "4 A Convergence Analysis ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) and choosing Œª5=a‚Äãr‚Äãg‚Äãm‚Äãi‚ÄãnŒª‚àà‚Ñù+‚ÄãH‚Äã(Œª)\lambda\_{5}=argmin\_{\lambda\in\mathbb{R}^{+}}H(\lambda), where

|  |  |  |
| --- | --- | --- |
|  | H‚Äã(Œª)=(1+gx)2‚Äãe(2‚ÄãK¬Ø‚ÄãŒª‚àí1+2‚ÄãK¬Ø+Œª)‚ÄãT‚Äã(K¬Ø‚ÄãŒª‚àí1+K¬Ø+1),H(\lambda)=(1+\sqrt{g\_{x}})^{2}e^{(2\bar{K}\lambda^{-1}+2\bar{K}+\lambda)T}(\bar{K}\lambda^{-1}+\bar{K}+1), |  |

we can have the following estimate

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùîº\displaystyle\mathbb{E} | |g‚Äã(VT,XTœÄ)‚àíYTœÄ|2‚â§(1+Œª4)‚ÄãHm‚Äãi‚Äãn‚Äã‚àëi=0N‚àí1ùîº‚Äã|Œ¥‚ÄãZ~ti|2‚Äãh\displaystyle|g(V\_{T},X^{\pi}\_{T})-Y^{\pi}\_{T}|^{2}\leq(1+\lambda\_{4})H\_{min}\sum\_{i=0}^{N-1}\mathbb{E}|\delta\tilde{Z}\_{t\_{i}}|^{2}h |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | +C‚Äã(T)‚Äã((I02+ùîº‚Äã|x0|2)‚Äãh+œÅ‚Äã(h)+ùîº‚Äã|Y0‚àíY0œÄ|2+h‚Äãùîº‚Äã‚à´0T|Zs|2‚Äãùëës+‚àëi=0N‚àí1Ezi),\displaystyle+C(T)\bigg(\big(I\_{0}^{2}+\mathbb{E}|x\_{0}|^{2}\big)h+\rho(h)+\mathbb{E}|Y\_{0}-Y^{\pi}\_{0}|^{2}+h\mathbb{E}\int\_{0}^{T}|Z\_{s}|^{2}ds+\sum\_{i=0}^{N-1}E^{i}\_{z}\bigg), |  | (4.18) |

with

|  |  |  |
| --- | --- | --- |
|  | Hm‚Äãi‚Äãn=minŒª‚àà‚Ñù+‚Å°H‚Äã(Œª).H\_{min}=\min\_{\lambda\in\mathbb{R}^{+}}H(\lambda). |  |

Finally the proof is complete by using the fact Y0œÄ=ùí¥0‚Äã(x0)Y^{\pi}\_{0}=\mathcal{Y}\_{0}(x\_{0}) and the representation ZtiœÄ=ùíµi‚Äã(XtiœÄ,{Ws,Vs;0‚â§s‚â§ti})Z\_{t\_{i}}^{\pi}=\mathcal{Z}\_{i}\big(X^{\pi}\_{t\_{i}},\{W\_{s},V\_{s};0\leq s\leq t\_{i}\}\big), and then taking the infimum on both sides of the equation ([4](https://arxiv.org/html/2511.08735v1#S4.Ex44 "4 A Convergence Analysis ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) with respect to ùí¥0\mathcal{Y}\_{0} and ùíµi,i=0,‚ãØ,N‚àí1\mathcal{Z}\_{i},i=0,\cdots,N-1, within neural network function classes of appropriate structure.

‚àé

## 5 Algorithm for Coupled non-Markovian FBSDEs

We consider the coupled forward-backward stochastic differential equation (FBSDE) given in ([1.1](https://arxiv.org/html/2511.08735v1#S1.E1 "In 1 Introduction ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")), explicitly written as

|  |  |  |  |
| --- | --- | --- | --- |
|  | {d‚ÄãXt=b‚Äã(t,Vt,Xt,Yt,Zt)‚Äãd‚Äãt+œÉ‚Äã(t,Vt,Xt,Yt,Zt)‚Äãd‚ÄãWt,t‚àà[0,T],X0=x0,‚àíd‚ÄãYt=f‚Äã(t,Vt,Xt,Yt,Zt)‚Äãd‚Äãt‚àíZt‚Äãd‚ÄãWt,t‚àà[0,T],YT=g‚Äã(VT,XT).\begin{cases}dX\_{t}&=b(t,V\_{t},X\_{t},Y\_{t},Z\_{t})dt+\sigma(t,V\_{t},X\_{t},Y\_{t},Z\_{t})dW\_{t},\ t\in[0,T],\\ X\_{0}&=x\_{0},\\ -dY\_{t}&=f(t,V\_{t},X\_{t},Y\_{t},Z\_{t})dt-Z\_{t}dW\_{t},\ t\in[0,T],\\ Y\_{T}&=g(V\_{T},X\_{T}).\end{cases} |  | (5.1) |

Under the Markovian framework, deep learning-based algorithms for high-dimensional coupled FBSDEs (or FBSPDEs) have already demonstrated remarkable efficiency, as we discussed in Section 1. Here, we extend the setting to the non-Markovian framework.

Inspired by the fact that, in a non-Markovian setting, the backward processes (Yt,Zt)(Y\_{t},Z\_{t}) can be represented as random functionals of the forward process XtX\_{t}, we aim to approximate (Yt,Zt)(Y\_{t},Z\_{t}) as functions of both XtX\_{t} and and the path information {Ws,Vs:0‚â§s‚â§t}\{W\_{s},V\_{s}:0\leq s\leq t\}. More precisely, we assume

|  |  |  |
| --- | --- | --- |
|  | Yt=ùí¥‚Äã(t,Xt,{Ws,Vs:0‚â§s‚â§t}),Zt=ùíµ‚Äã(t,Xt,{Ws,Vs:0‚â§s‚â§t}),Y\_{t}=\mathcal{Y}(t,X\_{t},\{W\_{s},V\_{s}:0\leq s\leq t\}),\;Z\_{t}=\mathcal{Z}(t,X\_{t},\{W\_{s},V\_{s}:0\leq s\leq t\}), |  |

for some deterministic functionals ùí¥\mathcal{Y} and ùíµ\mathcal{Z}. This representation forms a key component of our deep learning-based approximation scheme for the backward processes.

In practice, deep neural networks are employed to approximate the unknown functionals ùí¥,ùíµ\mathcal{Y},\mathcal{Z} by reformulating the original FBSDE problem as a stochastic optimization problem. Here, we present two deep learning-based algorithms for solving the coupled non-Markovian FBSDE, both inspired by the framework of [SJiSPengYPengXZhang2020ThreeAlgorithmsforSolvHighDimCoupledFBSDEsDeepLearn].

To describe these algorithms, we consider a uniform partition œÄ:t0=0<t1<‚ãØ<tN=T\pi:{t\_{0}=0<t\_{1}<\dots<t\_{N}=T} of the time interval [0,T][0,T]. We denote by XœÄX^{\pi} and YœÄY^{\pi} the discrete-time approximations of the processes XX and YY, respectively, obtained via the Euler scheme on the time grid œÄ\pi. We also define the Brownian motion increments as Œî‚ÄãWtj:=Wtj+1‚àíWtj\Delta W\_{t\_{j}}:=W\_{t\_{j+1}}-W\_{t\_{j}} for j=0,1,‚ãØ,N‚àí1j=0,1,\cdots,N-1.

The forward representation of the backward equation in ([1.1](https://arxiv.org/html/2511.08735v1#S1.E1 "In 1 Introduction ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) is given by:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Yt=Y0‚àí‚à´0tf‚Äã(s,Vs,Xs,Ys,Zs)‚Äãds+‚à´0tZs‚ÄãdWs,0‚â§t‚â§T.Y\_{t}=Y\_{0}-\int\_{0}^{t}f(s,V\_{s},X\_{s},Y\_{s},Z\_{s})\mathrm{d}s+\int\_{0}^{t}Z\_{s}\mathrm{d}W\_{s},\quad 0\leq t\leq T. |  | (5.2) |

This representation forms the foundation for the following two numerical algorithms designed to approximate the solution of coupled non-Markovian FBSDEs. Now, we describe the two algorithms.

### 5.1 Algorithm 1

The steps are as follows:

* ‚Ä¢

  Initialization starts with estimations ùí¥0\mathcal{Y}\_{0} and ùíµ0\mathcal{Z}\_{0} of Y0Y\_{0} and
  Z0Z\_{0} respectively and then calculate Xt1œÄX\_{t\_{1}}^{\pi} and Yt1œÄY\_{t\_{1}}^{\pi} by using Euler scheme as

  |  |  |  |
  | --- | --- | --- |
  |  | Xt1œÄ=X0+b‚Äã(t0,Vt0,X0,ùí¥0,ùíµ0)‚ÄãŒî‚Äãt0+œÉ‚Äã(t0,Vt0,X0,ùí¥0,ùíµ0)‚ÄãŒî‚ÄãWt0,Yt1œÄ=ùí¥0‚àíf‚Äã(t0,Vt0,X0,ùí¥0,ùíµ0)‚ÄãŒî‚Äãt0+ùíµ0‚ÄãŒî‚ÄãWt0.\begin{split}X\_{t\_{1}}^{\pi}&=X\_{0}+b(t\_{0},V\_{t\_{0}},X\_{0},\mathcal{Y}\_{0},\mathcal{Z}\_{0})\Delta t\_{0}+\sigma(t\_{0},V\_{t\_{0}},X\_{0},\mathcal{Y}\_{0},\mathcal{Z}\_{0})\Delta W\_{t\_{0}},\\ Y\_{t\_{1}}^{\pi}&=\mathcal{Y}\_{0}-f(t\_{0},V\_{t\_{0}},X\_{0},\mathcal{Y}\_{0},\mathcal{Z}\_{0})\Delta t\_{0}+\mathcal{Z}\_{0}\Delta W\_{t\_{0}}.\end{split} |  |
* ‚Ä¢

  For each j=1,2,‚ãØ,N‚àí1j=1,2,\cdots,N-1, a neural network ùíµjùí©‚Äã(‚ãÖ;Œ∏j)\mathcal{Z}\_{j}^{\mathcal{N}}(\cdot;\theta\_{j}) approximate Ztj=ùíµ‚Äã(tj,XtjœÄ,{Wti,Vti}i=0j)Z\_{t\_{j}}=\mathcal{Z}\big(t\_{j},X\_{t\_{j}}^{\pi},\{W\_{t\_{i}},V\_{t\_{i}}\}\_{i=0}^{j}\big) and Euler scheme calculates Xtj+1œÄX\_{t\_{j+1}}^{\pi} and Ytj+1œÄY\_{t\_{j+1}}^{\pi} as

  |  |  |  |
  | --- | --- | --- |
  |  | Xtj+1œÄ=XtjœÄ+b‚Äã(tj,Vtj,XtjœÄ,YtjœÄ,ùíµjùí©‚Äã(XtjœÄ,{Wti,Vti}i=0j;Œ∏j))‚ÄãŒî‚Äãtj+œÉ‚Äã(tj,Vtj,XtjœÄ,YtjœÄ,ùíµjùí©‚Äã(XtjœÄ,{Wti,Vti}i=0j;Œ∏j))‚ÄãŒî‚ÄãWtj,Ytj+1œÄ=YtjœÄ‚àíf‚Äã(tj,Vtj,XtjœÄ,YtjœÄ,ùíµjùí©‚Äã(XtjœÄ,{Wti,Vti}i=0j;Œ∏j))‚ÄãŒî‚Äãtj+ùíµjùí©‚Äã(XtjœÄ,{Wti,Vti}i=0j;Œ∏j)‚ÄãŒî‚ÄãWtj.\begin{split}X\_{t\_{j+1}}^{\pi}&=X\_{t\_{j}}^{\pi}+b\bigg(t\_{j},V\_{t\_{j}},X\_{t\_{j}}^{\pi},Y\_{t\_{j}}^{\pi},\mathcal{Z}\_{j}^{\mathcal{N}}\big(X\_{t\_{j}}^{\pi},\{W\_{t\_{i}},V\_{t\_{i}}\}\_{i=0}^{j};\theta\_{j}\big)\bigg)\Delta t\_{j}\\ &+\sigma\bigg(t\_{j},V\_{t\_{j}},X\_{t\_{j}}^{\pi},Y\_{t\_{j}}^{\pi},\mathcal{Z}\_{j}^{\mathcal{N}}\big(X\_{t\_{j}}^{\pi},\{W\_{t\_{i}},V\_{t\_{i}}\}\_{i=0}^{j};\theta\_{j}\big)\bigg)\Delta W\_{t\_{j}},\\ Y\_{t\_{j+1}}^{\pi}&=Y\_{t\_{j}}^{\pi}-f\bigg(t\_{j},V\_{t\_{j}},X\_{t\_{j}}^{\pi},Y\_{t\_{j}}^{\pi},\mathcal{Z}\_{j}^{\mathcal{N}}\big(X\_{t\_{j}}^{\pi},\{W\_{t\_{i}},V\_{t\_{i}}\}\_{i=0}^{j};\theta\_{j}\big)\bigg)\Delta t\_{j}+\mathcal{Z}\_{j}^{\mathcal{N}}\big(X\_{t\_{j}}^{\pi},\{W\_{t\_{i}},V\_{t\_{i}}\}\_{i=0}^{j};\theta\_{j}\big)\Delta W\_{t\_{j}}.\end{split} |  |
* ‚Ä¢

  Finally, the scheme is to optimize the loss function

  |  |  |  |
  | --- | --- | --- |
  |  | ùîè^‚Äã(ùí¥0,ùíµ0,Œ∏1,Œ∏2,‚ãØ,Œ∏N‚àí1):=ùîº‚Äã|YtNœÄ‚àíg‚Äã(VtN,XtNœÄ)|2,\widehat{\mathfrak{L}}\big(\mathcal{Y}\_{0},\mathcal{Z}\_{0},\theta\_{1},\theta\_{2},\cdots,\theta\_{N-1}\big):=\mathbb{E}\bigg|Y\_{t\_{N}}^{\pi}-g(V\_{t\_{N}},X\_{t\_{N}}^{\pi})\bigg|^{2}, |  |

  over all Œ∏=(ùí¥0,ùíµ0,Œ∏1,Œ∏2,‚ãØ,Œ∏N‚àí1)\theta=(\mathcal{Y}\_{0},\mathcal{Z}\_{0},\theta\_{1},\theta\_{2},\cdots,\theta\_{N-1}), and if

  |  |  |  |
  | --- | --- | --- |
  |  | Œ∏‚àó=(ùí¥0‚àó,ùíµ0‚àó,Œ∏1‚àó,Œ∏2‚àó,‚ãØ,Œ∏N‚àí1‚àó)‚ààarg‚Å°minŒ∏‚ààŒò‚Å°ùîè^‚Äã(Œ∏),\theta^{\*}=(\mathcal{Y}^{\*}\_{0},\mathcal{Z}^{\*}\_{0},\theta^{\*}\_{1},\theta^{\*}\_{2},\cdots,\theta^{\*}\_{N-1})\in\arg\min\_{\theta\in\Theta}\widehat{\mathfrak{L}}(\theta), |  |

  then ùí¥0‚àó\mathcal{Y}\_{0}^{\*} is the desired approximation of Y0Y\_{0} by this method.

### 5.2 Algorithm 2

The steps are as follows:

* ‚Ä¢

  Initialize with estimations ùí¥0\mathcal{Y}\_{0} and ùíµ0\mathcal{Z}\_{0} of Y0Y\_{0} and
  Z0Z\_{0} respectively and calculate Xt1œÄX\_{t\_{1}}^{\pi} and Yt1œÄY\_{t\_{1}}^{\pi} by using Euler scheme as

  |  |  |  |
  | --- | --- | --- |
  |  | Xt1œÄ=X0+b‚Äã(t0,Vt0,X0,ùí¥0,ùíµ0)‚ÄãŒî‚Äãt0+œÉ‚Äã(t0,Vt0,X0,ùí¥0,ùíµ0)‚ÄãŒî‚ÄãWt0,Yt1œÄ=ùí¥0‚àíf‚Äã(t0,Vt0,X0,ùí¥0,ùíµ0)‚ÄãŒî‚Äãt0+ùíµ0‚ÄãŒî‚ÄãWt0.\begin{split}X\_{t\_{1}}^{\pi}&=X\_{0}+b(t\_{0},V\_{t\_{0}},X\_{0},\mathcal{Y}\_{0},\mathcal{Z}\_{0})\Delta t\_{0}+\sigma(t\_{0},V\_{t\_{0}},X\_{0},\mathcal{Y}\_{0},\mathcal{Z}\_{0})\Delta W\_{t\_{0}},\\ Y\_{t\_{1}}^{\pi}&=\mathcal{Y}\_{0}-f(t\_{0},V\_{t\_{0}},X\_{0},\mathcal{Y}\_{0},\mathcal{Z}\_{0})\Delta t\_{0}+\mathcal{Z}\_{0}\Delta W\_{t\_{0}}.\end{split} |  |
* ‚Ä¢

  For each j=1,2,‚ãØ,N‚àí1j=1,2,\cdots,N-1, a neural network ùí≥jùí©‚Äã(‚ãÖ;Œ∏j)=(ùí¥jùí©‚Äã(‚ãÖ;Œ∏j),ùíµjùí©‚Äã(‚ãÖ;Œ∏j))\mathcal{X}\_{j}^{\mathcal{N}}(\cdot;\theta\_{j})=\big(\mathcal{Y}\_{j}^{\mathcal{N}}(\cdot;\theta\_{j}),\mathcal{Z}\_{j}^{\mathcal{N}}(\cdot;\theta\_{j})\big) approximates Ytj=ùí¥‚Äã(tj,XtjœÄ,{Wti,Vti}i=0j)Y\_{t\_{j}}=\mathcal{Y}\big(t\_{j},X\_{t\_{j}}^{\pi},\{W\_{t\_{i}},V\_{t\_{i}}\}\_{i=0}^{j}\big) and Ztj=ùíµ‚Äã(tj,XtjœÄ,{Wti,Vti}i=0j)Z\_{t\_{j}}=\mathcal{Z}\big(t\_{j},X\_{t\_{j}}^{\pi},\{W\_{t\_{i}},V\_{t\_{i}}\}\_{i=0}^{j}\big)
  with the associated local loss function, which is defined as

  |  |  |  |
  | --- | --- | --- |
  |  | ùîèj:=Œî‚Äãtj‚àí1‚ãÖùîº‚Äã|YtjœÄ‚àíùí¥jùí©‚Äã(XtjœÄ,{Wti,Vti}i=0j;Œ∏j)|2.\mathfrak{L}\_{j}:=\Delta t\_{j-1}\cdot\mathbb{E}\bigg|Y\_{t\_{j}}^{\pi}-\mathcal{Y}\_{j}^{\mathcal{N}}\big(X\_{t\_{j}}^{\pi},\{W\_{t\_{i}},V\_{t\_{i}}\}\_{i=0}^{j};\theta\_{j}\big)\bigg|^{2}. |  |

  Then Euler scheme calculates Xtj+1œÄX\_{t\_{j+1}}^{\pi} and Ytj+1œÄY\_{t\_{j+1}}^{\pi} as

  |  |  |  |
  | --- | --- | --- |
  |  | Xtj+1œÄ=XtjœÄ+b‚Äã(tj,Vtj,XtjœÄ,ùí¥jùí©‚Äã(XtjœÄ,{Wti,Vti}i=0j;Œ∏j),ùíµjùí©‚Äã(XtjœÄ,{Wti,Vti}i=0j;Œ∏j))‚ÄãŒî‚Äãtj+œÉ‚Äã(tj,Vtj,XtjœÄ,ùí¥jùí©‚Äã(XtjœÄ,{Wti,Vti}i=0j;Œ∏j),ùíµjùí©‚Äã(XtjœÄ,{Wti,Vti}i=0j;Œ∏j))‚ÄãŒî‚ÄãWtj,Ytj+1œÄ=YtjœÄ‚àíf‚Äã(tj,Vtj,XtjœÄ,ùí¥jùí©‚Äã(XtjœÄ,{Wti,Vti}i=0j;Œ∏j),ùíµjùí©‚Äã(XtjœÄ,{Wti,Vti}i=0j;Œ∏j))‚ÄãŒî‚Äãtj+ùíµjùí©‚Äã(XtjœÄ,{Wti,Vti}i=0j;Œ∏j)‚ÄãŒî‚ÄãWtj.\begin{split}X\_{t\_{j+1}}^{\pi}&=X\_{t\_{j}}^{\pi}+b\bigg(t\_{j},V\_{t\_{j}},X\_{t\_{j}}^{\pi},\mathcal{Y}\_{j}^{\mathcal{N}}\big(X\_{t\_{j}}^{\pi},\{W\_{t\_{i}},V\_{t\_{i}}\}\_{i=0}^{j};\theta\_{j}\big),\mathcal{Z}\_{j}^{\mathcal{N}}\big(X\_{t\_{j}}^{\pi},\{W\_{t\_{i}},V\_{t\_{i}}\}\_{i=0}^{j};\theta\_{j}\big)\bigg)\Delta t\_{j}\\ &+\sigma\bigg(t\_{j},V\_{t\_{j}},X\_{t\_{j}}^{\pi},\mathcal{Y}\_{j}^{\mathcal{N}}\big(X\_{t\_{j}}^{\pi},\{W\_{t\_{i}},V\_{t\_{i}}\}\_{i=0}^{j};\theta\_{j}\big),\mathcal{Z}\_{j}^{\mathcal{N}}\big(X\_{t\_{j}}^{\pi},\{W\_{t\_{i}},V\_{t\_{i}}\}\_{i=0}^{j};\theta\_{j}\big)\bigg)\Delta W\_{t\_{j}},\\ Y\_{t\_{j+1}}^{\pi}&=Y\_{t\_{j}}^{\pi}-f\bigg(t\_{j},V\_{t\_{j}},X\_{t\_{j}}^{\pi},\mathcal{Y}\_{j}^{\mathcal{N}}\big(X\_{t\_{j}}^{\pi},\{W\_{t\_{i}},V\_{t\_{i}}\}\_{i=0}^{j};\theta\_{j}\big),\mathcal{Z}\_{j}^{\mathcal{N}}\big(X\_{t\_{j}}^{\pi},\{W\_{t\_{i}},V\_{t\_{i}}\}\_{i=0}^{j};\theta\_{j}\big)\bigg)\Delta t\_{j}\\ &+\mathcal{Z}\_{j}^{\mathcal{N}}\big(X\_{t\_{j}}^{\pi},\{W\_{t\_{i}},V\_{t\_{i}}\}\_{i=0}^{j};\theta\_{j}\big)\Delta W\_{t\_{j}}.\end{split} |  |
* ‚Ä¢

  Finally, the scheme is to optimize the global loss function

  |  |  |  |
  | --- | --- | --- |
  |  | ùîè^‚Äã(ùí¥0,ùíµ0,Œ∏1,Œ∏2,‚ãØ,Œ∏N‚àí1):=‚àëj=1N‚àí1ùîèj+ùîº‚Äã|YtNœÄ‚àíg‚Äã(VtN‚ÄãXtNœÄ)|2,\widehat{\mathfrak{L}}(\mathcal{Y}\_{0},\mathcal{Z}\_{0},\theta\_{1},\theta\_{2},\cdots,\theta\_{N-1}):=\sum\_{j=1}^{N-1}\mathfrak{L}\_{j}+\mathbb{E}\bigg|Y\_{t\_{N}}^{\pi}-g(V\_{t\_{N}}X\_{t\_{N}}^{\pi})\bigg|^{2}, |  |

  over all Œ∏=(ùí¥0,ùíµ0,Œ∏1,Œ∏2,‚ãØ,Œ∏N‚àí1)\theta=(\mathcal{Y}\_{0},\mathcal{Z}\_{0},\theta\_{1},\theta\_{2},\cdots,\theta\_{N-1}), and if

  |  |  |  |
  | --- | --- | --- |
  |  | Œ∏‚àó=(ùí¥0‚àó,ùíµ0‚àó,Œ∏1‚àó,Œ∏2‚àó,‚ãØ,Œ∏N‚àí1‚àó)‚ààarg‚Å°minŒ∏‚ààŒò‚Å°ùîè^‚Äã(Œ∏),\theta^{\*}=(\mathcal{Y}^{\*}\_{0},\mathcal{Z}^{\*}\_{0},\theta^{\*}\_{1},\theta^{\*}\_{2},\cdots,\theta^{\*}\_{N-1})\in\arg\min\_{\theta\in\Theta}\widehat{\mathfrak{L}}(\theta), |  |

  then ùí¥0‚àó\mathcal{Y}\_{0}^{\*} is the desired approximation of Yt0œÄY^{\pi}\_{t\_{0}} by this method.

In both algorithms, the input dimension of the neural network varies with time, while the output dimension remains constant across all time steps. Specifically, at time step jj, the neural network receives an input of dimension d+(j+1)‚Äã(m0+m)d+(j+1)(m\_{0}+m), reflecting the inclusion of the state variable XtX\_{t}, and the historical path information {Ws,Vs:0‚â§s‚â§tj}\{W\_{s},V\_{s}:0\leq s\leq t\_{j}\} encoded over j+1j+1 time steps.

For Algorithm 1, the output dimension of the neural network is m‚Äãd0md\_{0}, and for Algorithm 2, it is (1+m)‚Äãd0(1+m)d\_{0}. In both cases, the output dimension remains unchanged across time steps and corresponds to the approximated values of the backward processes at that step.

## 6 Numerical Examples with Applications

For the stochastic variance VtV\_{t}, we consider rough Bergomi model (see¬†[CBayerPFrizJHGath2016PricingUnderRoughVolat]), given by

|  |  |  |
| --- | --- | --- |
|  | Vt=Œæt‚Äã‚Ñ∞‚Äã(Œ∑‚ÄãW^t),V\_{t}=\xi\_{t}\mathcal{E}(\eta\widehat{W}\_{t}), |  |

where Œæt\xi\_{t} is the forward variance curve (typically computed from the implied volatility surface), ‚Ñ∞\mathcal{E} denotes the Wick exponential, defined by

|  |  |  |
| --- | --- | --- |
|  | ‚Ñ∞‚Äã(Z)=exp‚Å°(Z‚àí12‚Äãvar‚Å°(Z)),\mathcal{E}(Z)=\exp(Z-\frac{1}{2}\operatorname{var}(Z)), |  |

for a zero-mean normal random variable ZZ, and Œ∑‚â•0\eta\geq 0 is a model parameter. The process W^\widehat{W} is a fractional Brownian motion of Riemann-Liouville type with Hurst index H‚àà(0,1/2)H\in(0,1/2), defined by

|  |  |  |  |
| --- | --- | --- | --- |
|  | W^t‚âî‚à´0tùí¶‚Äã(t‚àís)‚ÄãùëëW~s,ùí¶‚Äã(r)‚âî2‚ÄãH‚ÄãrH‚àí1/2,r>0,\widehat{W}\_{t}\coloneqq\int\_{0}^{t}\mathcal{K}(t-s)\,d\widetilde{W}\_{s},\quad\mathcal{K}(r)\coloneqq\sqrt{2H}r^{H-1/2},\quad r>0, |  | (6.1) |

where W~\widetilde{W} is a standard Brownian motion.

### 6.1 Example 1

Our first example is based on Example 2 from Han and Long [JHanJLong2020ConvgDeepBSDEforCoupledFBSDEs], where we consider the following coupled FBSDE:

|  |  |  |  |
| --- | --- | --- | --- |
|  | {Xt=X0+‚à´0t[Œº‚Äã(œÉ‚ÄãœÅ‚ÄãYs‚Äãe‚àír‚Äã(T‚àís)‚ÄãŒª‚Äãcos‚Å°(Xs+‚à´0ssin‚Å°(Vr)‚Äãùëër)‚àíZs)]‚Äãùëës+‚à´0tœÉ‚ÄãYs‚Äã(œÅ‚Äãd‚ÄãWs+1‚àíœÅ2‚Äãd‚ÄãBs),X0=œÄ2,‚àíd‚ÄãYt=[‚àírYt+12e‚àí3‚Äãr‚Äã(T‚àít)œÉ2Œª3sin3(Xt+‚à´0tsin(Vs)ds)‚àísin(Vt)e‚àír‚Äã(T‚àít)Œªcos(Xt+‚à´0tsin(Vs)ds)]dt‚àíZtdWt‚àíZt~dBt,YT=Œª‚Äãsin‚Å°(XT+‚à´0Tsin‚Å°(Vs)‚Äãùëës).\displaystyle\begin{cases}X\_{t}&=X\_{0}+\int\_{0}^{t}\bigg[\mu\bigg(\sigma\rho Y\_{s}e^{-r(T-s)}\lambda\cos\Big(X\_{s}+\int\_{0}^{s}\sin{(V\_{r})}dr\Big)-Z\_{s}\bigg)\bigg]ds\\ &\quad+\int\_{0}^{t}\sigma Y\_{s}\Big(\rho dW\_{s}+\sqrt{1-\rho^{2}}dB\_{s}\Big),\\ X\_{0}&=\frac{\pi}{2},\\ -dY\_{t}&=\bigg[-rY\_{t}+\frac{1}{2}e^{-3r(T-t)}\sigma^{2}\lambda^{3}\sin^{3}\Big(X\_{t}+\int\_{0}^{t}\sin(V\_{s})ds\Big)\\ &\quad-\sin(V\_{t})e^{-r(T-t)}\lambda\cos\Big(X\_{t}+\int\_{0}^{t}\sin(V\_{s})ds\Big)\bigg]dt-Z\_{t}dW\_{t}-\tilde{Z\_{t}}dB\_{t},\\ Y\_{T}&=\lambda\sin\Big(X\_{T}+\int\_{0}^{T}\sin{(V\_{s})}ds\Big).\end{cases} |  | (6.2) |

The analytical solution of the above coupled FBSDE is known to be:

|  |  |  |
| --- | --- | --- |
|  | Yt=e‚àír‚Äã(T‚àít)‚ÄãŒª‚Äãsin‚Å°(Xt+‚à´0tsin‚Å°(Vs)‚Äãùëës).Y\_{t}=e^{-r(T-t)}\lambda\sin{\Big(X\_{t}+\int\_{0}^{t}\sin{(V\_{s})}ds\Big)}. |  |

In particular, at t=0t=0 and X0=œÄ2X\_{0}=\frac{\pi}{2}, this yields the exact value:

|  |  |  |
| --- | --- | --- |
|  | Y0‚âà7.6098354.Y\_{0}\approx 7.6098354. |  |

For numerical simulations, we set the parameters as follows: T=1T=1, œÅ=‚àí0.9\rho=-0.9, r=0.05r=0.05, Œª=8\lambda=8, Œº=0.1\mu=0.1, œÉ=0.2\sigma=0.2, H=0.07H=0.07, Œ∑=1.9\eta=1.9, and Œæt=0.09\xi\_{t}=0.09. We apply both of our algorithms using various time discretizations with N‚àà{5,10,20,30}N\in\{5,10,20,30\}.

We train the model using the Adam optimizer on 64000 samples for 20 epochs. Performance is evaluated after each epoch on a separate test set of 16,000 samples. A batch size of 64 is used for both training and testing, implying 1,000 training updates per epoch. The neural networks consist of 3 hidden layers with ReLU activation and include batch normalization before the hidden layers. The neural network approximation at time step jj is denoted by

|  |  |  |
| --- | --- | --- |
|  | ùí≥jùí©‚Äã(‚ãÖ;Œ∏j)=(ùí¥jùí©‚Äã(‚ãÖ;Œ∏j),ùíµjùí©‚Äã(‚ãÖ;Œ∏j))\mathcal{X}\_{j}^{\mathcal{N}}(\cdot;\theta\_{j})=\big(\mathcal{Y}\_{j}^{\mathcal{N}}(\cdot;\theta\_{j}),\mathcal{Z}\_{j}^{\mathcal{N}}(\cdot;\theta\_{j})\big) |  |

where the network input is taken as (XtjœÄ,{Vti}i=0j)\big(X\_{t\_{j}}^{\pi},\{V\_{t\_{i}}\}\_{i=0}^{j}\big).

Algorithm 1 Results: 
Table¬†[1](https://arxiv.org/html/2511.08735v1#S6.T1 "Table 1 ‚Ä£ 6.1 Example 1 ‚Ä£ 6 Numerical Examples with Applications ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications") shows the results from Algorithm 1, reporting the mean approximation of Y0Y\_{0}, relative error with respect to the true solution, and the empirical variance across 20 independent runs. The approximation improves with increasing NN, demonstrating the algorithm‚Äôs effectiveness and consistency.

Table 1: Value of Y0Y\_{0} from Algorithm 1 (Example 1)

| N | Approximation | Relative Error | Variance |
| --- | --- | --- | --- |
| 5 | 7.79166896 | 2.389 % | 0.000098 |
| 10 | 7.67836123 | 0.900% | 0.000054 |
| 20 | 7.64265652 | 0.431% | 0.000063 |
| 30 | 7.63218625 | 0.294% | 0.000069 |

Algorithm 2 Results: Table¬†[2](https://arxiv.org/html/2511.08735v1#S6.T2 "Table 2 ‚Ä£ 6.1 Example 1 ‚Ä£ 6 Numerical Examples with Applications ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications") presents results from Algorithm 2 under the same setup. Although the variance remains low and the estimates are consistent, the accuracy is slightly inferior to Algorithm 1, with a persistent relative error around 1.8‚Äì2.0%.

Table 2: Value of Y0Y\_{0} from Algorithm 2 (Example 1)

| N | Approximation | Relative Error | Variance |
| --- | --- | --- | --- |
| 5 | 7.76642227 | 2.058% | 0.000091 |
| 10 | 7.75322528 | 1.884% | 0.000146 |
| 20 | 7.76028776 | 1.977% | 0.000070 |
| 30 | 7.74844878 | 1.822% | 0.000080 |

### 6.2 Example 2: Utility Maximization with Fixed Consumption Rate

In this example, we consider a utility maximization problem for an agent (or investor) who allocates their wealth between risky and risk-free assets, with a consumption rate influenced by an external process. The risky asset StS\_{t} follows a stochastic volatility model driven by a two-dimensional Weiner process W=(W~,B)W=(\widetilde{W},B):

|  |  |  |
| --- | --- | --- |
|  | {d‚ÄãSt=(r+Œ∏‚ÄãVt)‚ÄãSt‚Äãd‚Äãt+St‚ÄãVt‚Äã(œÅ‚Äãd‚ÄãW~t+1‚àíœÅ2‚Äãd‚ÄãBt),S0=s0,\begin{cases}dS\_{t}&=(r+\theta V\_{t})S\_{t}dt+S\_{t}\sqrt{V\_{t}}(\rho d\widetilde{W}\_{t}+\sqrt{1-\rho^{2}}dB\_{t}),\\ S\_{0}&=s\_{0},\end{cases} |  |

where œÅ‚àà[‚àí1,1]\rho\in[-1,1] is the correlation coefficient, r>0r>0 is the risk-free interest rate, and Œ∏‚ÄãVt;Œ∏‚â†0\theta\sqrt{V\_{t}};\theta\neq 0 is the market price of risk (risk premium).

Let œÄt\pi\_{t} denote the proportion of the wealth invested in the risky asset, and define ut:=Vt‚ÄãœÄtu\_{t}:=\sqrt{V\_{t}}\pi\_{t}. We assume the consumption rate is given by ct‚ÄãVt‚ÄãXtc\_{t}\sqrt{V\_{t}}X\_{t}, where ct‚â•0c\_{t}\geq 0 is a bounded deterministic function representing the agent‚Äôs personal consumption preference. Then, the wealth process XtX\_{t} evolves according to the SDE:

|  |  |  |  |
| --- | --- | --- | --- |
|  | d‚ÄãXt=(r+Œ∏‚ÄãVt‚Äãut‚àíct‚ÄãVt)‚ÄãXt‚Äãd‚Äãt+ut‚ÄãXt‚Äã(œÅ‚Äãd‚ÄãW~t+1‚àíœÅ2‚Äãd‚ÄãBt),X0=x0>0.\displaystyle dX\_{t}=\left(r+\theta\sqrt{V\_{t}}u\_{t}-c\_{t}\sqrt{V\_{t}}\right)X\_{t}\,dt+u\_{t}X\_{t}\,\left(\rho\,d\widetilde{W}\_{t}+\sqrt{1-\rho^{2}}\,dB\_{t}\right),\quad X\_{0}=x\_{0}>0. |  | (6.3) |

The agent aims to maximize their expected utility over the interval [0,T][0,T], with the utility functional:

|  |  |  |  |
| --- | --- | --- | --- |
|  | U‚Äã(0,x0)=maxu‚Å°ùîº‚Äã[e‚àír‚ÄãT‚ÄãXTŒ≥+‚à´0TF‚Äã(t,Xt,ut)‚Äãùëët],X0=x0,‚ÄÑ0<Œ≥<1,U(0,x\_{0})=\max\_{u}\mathbb{E}\bigg[e^{-rT}X^{\gamma}\_{T}+\int\_{0}^{T}F(t,X\_{t},u\_{t})dt\bigg],\quad X\_{0}=x\_{0},\;0<\gamma<1, |  | (6.4) |

where the running utility function FF is given by:

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | F‚Äã(t,Xt,ut)\displaystyle F(t,X\_{t},u\_{t}) | =e‚àír‚Äãt‚Äã(at‚ÄãVt‚Äã(1‚àíœÄt)‚ÄãXt+(ct‚ÄãVt‚ÄãXt)Œ≥)\displaystyle=e^{-rt}\bigg(a\_{t}\sqrt{V\_{t}}\Big(1-\pi\_{t}\Big)X\_{t}+\Big(c\_{t}\sqrt{V\_{t}}X\_{t}\Big)^{\gamma}\bigg) |  | (6.5) |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | =e‚àír‚Äãt‚Äã(at‚Äã(Vt‚àíut)‚ÄãXt+(ct‚ÄãVt‚ÄãXt)Œ≥).\displaystyle=e^{-rt}\bigg(a\_{t}\Big(\sqrt{V\_{t}}-u\_{t}\Big)X\_{t}+\Big(c\_{t}\sqrt{V\_{t}}X\_{t}\Big)^{\gamma}\bigg). |  | (6.6) |

Here at‚â•0a\_{t}\geq 0 rewards deviations from investment in the risky asset. When at=0a\_{t}=0, the problem reduces to the classical power utility maximization with consumption.

By It√¥-Doeblin formula, we obtain

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùîº‚Äã[XTŒ≥]=x0Œ≥+ùîº‚Äã‚à´0TŒ≥‚ÄãXtŒ≥‚Äã(r+Œ∏‚ÄãVt‚Äãut‚àíct‚ÄãVt+Œ≥‚àí12‚Äãut2)‚Äãùëët.\mathbb{E}\bigg[X^{\gamma}\_{T}\bigg]=x^{\gamma}\_{0}+\mathbb{E}\int\_{0}^{T}\gamma X^{\gamma}\_{t}\bigg(r+\theta\sqrt{V\_{t}}u\_{t}-c\_{t}\sqrt{V\_{t}}+\frac{\gamma-1}{2}u\_{t}^{2}\bigg)dt. |  | (6.7) |

Thus, the agent‚Äôs utility maximization problem in ([6.4](https://arxiv.org/html/2511.08735v1#S6.E4 "In 6.2 Example 2: Utility Maximization with Fixed Consumption Rate ‚Ä£ 6 Numerical Examples with Applications ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) is equivalent to the following optimization problem:

|  |  |  |  |
| --- | --- | --- | --- |
|  | maxuùîº[‚à´0T(e‚àír‚ÄãTŒ≥XtŒ≥(r+Œ∏Vtut‚àíctVt+Œ≥‚àí12ut2)+e‚àír‚Äãt(at(Vt‚àíut)Xt+(ctVtXt)Œ≥)dt].\max\_{u}\mathbb{E}\bigg[\int\_{0}^{T}\bigg(e^{-rT}\gamma X^{\gamma}\_{t}\Big(r+\theta\sqrt{V\_{t}}u\_{t}-c\_{t}\sqrt{V\_{t}}+\frac{\gamma-1}{2}u\_{t}^{2}\Big)+e^{-rt}\bigg(a\_{t}\Big(\sqrt{V\_{t}}-u\_{t}\Big)X\_{t}+\Big(c\_{t}\sqrt{V\_{t}}X\_{t}\Big)^{\gamma}\bigg)dt\bigg]. |  | (6.8) |

Let ut‚àóu\_{t}^{\*} (or equivalently œÄt‚àó=ut‚àóVt\pi^{\*}\_{t}=\frac{u^{\*}\_{t}}{\sqrt{V\_{t}}}) denote the optimal control. Applying Pontryagin‚Äôs maximum principle, we arrive at the following coupled FBSDE system for t‚àà[0,T]t\in[0,T]:

|  |  |  |  |
| --- | --- | --- | --- |
|  | {d‚ÄãXt=(r+Œ∏Vtut‚àó‚àíctVt)Xtdt+ut‚àóXt(œÅdW~t+1‚àíœÅ2dBt,X0=x0,‚àíd‚ÄãYt=((r+Œ∏Vtut‚àó‚àíctVt)Yt+ut‚àó(œÅZ~t+1‚àíœÅ2Zt)+e‚àír‚ÄãT‚ÄãŒ≥2‚ÄãXtŒ≥‚àí1‚Äã(rt+Œ∏‚ÄãVt‚Äãut‚àó‚àíct‚ÄãVt+Œ≥‚àí12‚Äã|ut‚àó|2)+e‚àír‚Äãt(at(Vt‚àíut)+(ctVt)Œ≥Œ≥XtŒ≥‚àí1))dt‚àíZtdBt‚àíZ~tdW~t,YT=0,ut‚àó=Œ∏‚ÄãVt‚Äã(Xt1‚àíŒ≥‚ÄãYt+e‚àír‚ÄãT‚ÄãŒ≥)+Xt1‚àíŒ≥‚Äã(1‚àíœÅ2‚ÄãZt+œÅ‚ÄãZ~t)‚àíe‚àír‚Äãt‚Äãat‚ÄãXt1‚àíŒ≥Œ≥‚Äã(1‚àíŒ≥)‚Äãe‚àír‚ÄãT.\begin{cases}dX\_{t}&=(r+\theta\sqrt{V\_{t}}u\_{t}^{\*}-c\_{t}\sqrt{V\_{t}})X\_{t}dt+u\_{t}^{\*}X\_{t}(\rho d\widetilde{W}\_{t}+\sqrt{1-\rho^{2}}dB\_{t},\\ X\_{0}&=x\_{0},\\ -dY\_{t}&=\Bigg(\big(r+\theta\sqrt{V\_{t}}u\_{t}^{\*}-c\_{t}\sqrt{V\_{t}}\big)Y\_{t}+u\_{t}^{\*}\big(\rho\widetilde{Z}\_{t}+\sqrt{1-\rho^{2}}Z\_{t}\big)\\ &\quad+e^{-rT}\gamma^{2}X\_{t}^{\gamma-1}\Big(r\_{t}+\theta\sqrt{V\_{t}}u\_{t}^{\*}-c\_{t}\sqrt{V\_{t}}+\frac{\gamma-1}{2}\left|{u\_{t}^{\*}}\right|^{2}\Big)\\ &\quad+e^{-rt}\Big(a\_{t}(\sqrt{V\_{t}}-u\_{t})+\Big(c\_{t}\sqrt{V\_{t}}\Big)^{\gamma}\gamma X\_{t}^{\gamma-1}\Big)\Bigg)dt-Z\_{t}dB\_{t}-\widetilde{Z}\_{t}d\widetilde{W}\_{t},\\ Y\_{T}&=0,\\ u\_{t}^{\*}&=\frac{\theta\sqrt{V\_{t}}(X\_{t}^{1-\gamma}Y\_{t}+e^{-rT}\gamma)+X\_{t}^{1-\gamma}\big(\sqrt{1-\rho^{2}}Z\_{t}+\rho\widetilde{Z}\_{t}\big)-e^{-rt}a\_{t}X\_{t}^{1-\gamma}}{\gamma(1-\gamma)e^{-rT}}.\end{cases} |  | (6.9) |

To solve for the optimal strategy, optimal wealth, and the optimal expected utility, we must numerically solve the coupled FBSDE in ([6.9](https://arxiv.org/html/2511.08735v1#S6.E9 "In 6.2 Example 2: Utility Maximization with Fixed Consumption Rate ‚Ä£ 6 Numerical Examples with Applications ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")). For this purpose, we set c=0.02c=0.02 and use the same parameter values as in Example 1, namely, T=1T=1, r=0.05r=0.05, œÅ=‚àí0.9\rho=-0.9, H=0.07H=0.07, Œ∑=1.9\eta=1.9, and Œæt=0.09\xi\_{t}=0.09. We then employ Algorithm 1 to compute the optimal strategy œÄt‚àó\pi^{\*}\_{t}, the optimal wealth process XtX\_{t}, and consequently the optimal expected utility.

To improve generalization and avoid overfitting, we reduce the complexity of our neural network by using only two hidden layers and applying l1l\_{1}-regularization. Additionally, we incorporate gradient clipping to prevent exploding gradients. The model is trained over 30 epochs.

Figure [1](https://arxiv.org/html/2511.08735v1#S6.F1 "Figure 1 ‚Ä£ 6.2 Example 2: Utility Maximization with Fixed Consumption Rate ‚Ä£ 6 Numerical Examples with Applications ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications") illustrates the average training and testing loss over epochs for N=100N=100 with parameters Œ≥=0.8,Œ∏=0.3,a=0.025,x0=1\gamma=0.8,\theta=0.3,a=0.025,x\_{0}=1, and s0=1s\_{0}=1. The results indicate that the neural network performs consistently well on both training and testing data.

![Refer to caption](ex2N_loss_N_100_gam_08_theta_03_a_0025_c002_X0_1.png)


Figure 1: Train and Test Loss During Epochs (Œ≥=0.8,Œ∏=0.3,a=0.025\gamma=0.8,\theta=0.3,a=0.025)

Figure [2](https://arxiv.org/html/2511.08735v1#S6.F2 "Figure 2 ‚Ä£ 6.2 Example 2: Utility Maximization with Fixed Consumption Rate ‚Ä£ 6 Numerical Examples with Applications ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")-[5](https://arxiv.org/html/2511.08735v1#S6.F5 "Figure 5 ‚Ä£ 6.2 Example 2: Utility Maximization with Fixed Consumption Rate ‚Ä£ 6 Numerical Examples with Applications ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications") presents sample paths of the processes St,œÄt‚àóS\_{t},\pi\_{t}^{\*} and XtX\_{t} under different choices of parameters Œ≥,Œ∏\gamma,\theta and aa, while all cases share the same rough volatility process VtV\_{t}. Finally, Figure [6](https://arxiv.org/html/2511.08735v1#S6.F6 "Figure 6 ‚Ä£ 6.2 Example 2: Utility Maximization with Fixed Consumption Rate ‚Ä£ 6 Numerical Examples with Applications ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications") displays the corresponding consumption rates for all four parameter configurations.

![Refer to caption](ex2N_V_N_100_gam_08_theta_03_a_0025_c002_X0_1.png)


(a) Rough Volatility Process VtV\_{t}

![Refer to caption](ex2N_S_N_100_gam_08_theta_03_a_0025_c002_X0_1.png)


(b) Stock Price Process StS\_{t}

![Refer to caption](ex2N_Pi_N_100_gam_08_theta_03_a_0_c002_X0_1.png)


(c) Optimal Strategy œÄt‚àó\pi^{\*}\_{t}

![Refer to caption](ex2N_X_N_100_gam_08_theta_03_a_0_c002_X0_1.png)


(d) Optimal Wealth XtX\_{t}

Figure 2: Sample Paths with Œ≥=0.8,Œ∏=0.3,a=0.0\gamma=0.8,\theta=0.3,a=0.0



![Refer to caption](ex2N_V_N_100_gam_08_theta_03_a_0025_c002_X0_1.png)


(a) Rough Volatility Process VtV\_{t}

![Refer to caption](ex2N_S_N_100_gam_08_theta_03_a_0025_c002_X0_1.png)


(b) Stock Price Process StS\_{t}

![Refer to caption](ex2N_Pi_N_100_gam_08_theta_03_a_0025_c002_X0_1.png)


(c) Optimal Strategy œÄt‚àó\pi^{\*}\_{t}

![Refer to caption](ex2N_X_N_100_gam_08_theta_03_a_0025_c002_X0_1.png)


(d) Optimal Wealth XtX\_{t}

Figure 3: Sample Paths with Œ≥=0.8,Œ∏=0.3,a=0.025\gamma=0.8,\theta=0.3,a=0.025



![Refer to caption](ex2N_V_N_100_gam_08_theta_03_a_0025_c002_X0_1.png)


(a) Rough Volatility Process VtV\_{t}

![Refer to caption](ex2N_S_N_100_gam_08_theta_03_a_0025_c002_X0_1.png)


(b) Stock Price Process StS\_{t}

![Refer to caption](ex2N_Pi_N_100_gam_02_theta_03_a_0025_c002_X0_1.png)


(c) Optimal Strategy œÄt‚àó\pi^{\*}\_{t}

![Refer to caption](ex2N_X_N_100_gam_02_theta_03_a_0025_c002_X0_1.png)


(d) Optimal Wealth XtX\_{t}

Figure 4: Sample Paths with Œ≥=0.2,Œ∏=0.3,a=0.025\gamma=0.2,\theta=0.3,a=0.025



![Refer to caption](ex2N_V_N_100_gam_08_theta_03_a_0025_c002_X0_1.png)


(a) Rough Volatility Process VtV\_{t}

![Refer to caption](ex2N_S_N_100_gam_08_theta_01_a_0025_c002_X0_1.png)


(b) Stock Price Process StS\_{t}

![Refer to caption](ex2N_Pi_N_100_gam_08_theta_01_a_0025_c002_X0_1.png)


(c) Optimal Strategy œÄt‚àó\pi^{\*}\_{t}

![Refer to caption](ex2N_X_N_100_gam_08_theta_01_a_0025_c002_X0_1.png)


(d) Optimal Wealth XtX\_{t}

Figure 5: Sample Paths with Œ≥=0.8,Œ∏=0.1,a=0.025\gamma=0.8,\theta=0.1,a=0.025



![Refer to caption](ex2N_C_N_100_gam_08_theta_03_a_0_c002_X0_1.png)


(a) Œ≥=0.8,Œ∏=0.3,a=0.0\gamma=0.8,\theta=0.3,a=0.0

![Refer to caption](ex2N_C_N_100_gam_08_theta_03_a_0025_c002_X0_1.png)


(b) Œ≥=0.8,Œ∏=0.3,a=0.025\gamma=0.8,\theta=0.3,a=0.025

![Refer to caption](ex2N_C_N_100_gam_08_theta_01_a_0025_c002_X0_1.png)


(c) Œ≥=0.8,Œ∏=0.1,a=0.025\gamma=0.8,\theta=0.1,a=0.025

![Refer to caption](ex2N_C_N_100_gam_02_theta_03_a_0025_c002_X0_1.png)


(d) Œ≥=0.2,Œ∏=0.3,a=0.025\gamma=0.2,\theta=0.3,a=0.025

Figure 6: Sample Paths of Consumption Rate

## 7 Conclusion

We developed a deep learning-based framework for fully coupled non-Markovian FBSDEs and established error bounds and convergence results for this class of equations. Our analysis extends the scope of Deep BSDE methods by allowing the coefficients of the forward process to be random and depending on all the backward components. The effectiveness of the scheme was demonstrated through utility maximization problems under rough volatility, illustrating its accuracy and applicability to complex, non-Markovian models.

## Appendix A Proof of Lemma [2.4](https://arxiv.org/html/2511.08735v1#S2.Thmthm4 "Lemma 2.4. ‚Ä£ 2.2 Solutions to FBSDEs ‚Ä£ 2 Preliminaries ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")

###### Proof.

For œï=b,œÉ\phi=b,\sigma, define the differences:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Œî‚Äãœï:=œï‚Äã(ti,Vti,Xti1,Yti1,Zti1)‚àíœï‚Äã(ti,Vti,Xti2,Yti2,Zti2).\Delta\phi:=\phi(t\_{i},V\_{t\_{i}},X\_{t\_{i}}^{1},Y\_{t\_{i}}^{1},Z\_{t\_{i}}^{1})-\phi(t\_{i},V\_{t\_{i}},X\_{t\_{i}}^{2},Y\_{t\_{i}}^{2},Z\_{t\_{i}}^{2}). |  | (A.1) |

Then the forward process difference satisfies:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Œî‚ÄãXti+1=Œî‚ÄãXti+Œî‚Äãb‚Äãh+‚à´titi+1Œî‚ÄãŒ±t‚Äãùëët+Œî‚ÄãœÉ‚ÄãŒî‚ÄãWti+‚à´titi+1Œî‚ÄãŒ≤t‚ÄãùëëWt.\Delta X\_{t\_{i+1}}=\Delta X\_{t\_{i}}+\Delta bh+\int\_{t\_{i}}^{t\_{i+1}}\Delta\alpha\_{t}dt+\Delta\sigma\Delta W\_{{t\_{i}}}+\int\_{t\_{i}}^{t\_{i+1}}\Delta\beta\_{t}dW\_{t}. |  | (A.2) |

Rearranging terms and squaring on both sides yields that

|  |  |  |
| --- | --- | --- |
|  | |Œî‚ÄãXti+1‚àíŒî‚ÄãXti|2=|Œî‚Äãb‚Äãh+Œî‚ÄãœÉ‚ÄãŒî‚ÄãWti+‚à´titi+1Œî‚ÄãŒ±t‚Äãùëët+‚à´titi+1Œî‚ÄãŒ≤t‚ÄãùëëWt|2.|\Delta X\_{t\_{i+1}}-\Delta X\_{t\_{i}}|^{2}=\bigg|\Delta bh+\Delta\sigma\Delta W\_{t\_{i}}+\int\_{t\_{i}}^{t\_{i+1}}\Delta\alpha\_{t}dt+\int\_{t\_{i}}^{t\_{i+1}}\Delta\beta\_{t}dW\_{t}\bigg|^{2}. |  |

Hence, expanding |Œî‚ÄãXti+1|2|\Delta X\_{t\_{i+1}}|^{2}:

|  |  |  |
| --- | --- | --- |
|  | |Œî‚ÄãXti+1|2\displaystyle|\Delta X\_{t\_{i+1}}|^{2} |  |
|  |  |  |
| --- | --- | --- |
|  | =‚àí|Œî‚ÄãXti|2+2‚ÄãŒî‚ÄãXti‚ÄãŒî‚ÄãXti+1+|Œî‚Äãb‚Äãh+Œî‚ÄãœÉ‚ÄãŒî‚ÄãWti+‚à´titi+1Œî‚ÄãŒ±t‚Äãùëët+‚à´titi+1Œî‚ÄãŒ≤t‚ÄãùëëWt|2\displaystyle=-|\Delta X\_{t\_{i}}|^{2}+2\Delta X\_{t\_{i}}\Delta X\_{t\_{i+1}}+\bigg|\Delta bh+\Delta\sigma\Delta W\_{t\_{i}}+\int\_{t\_{i}}^{t\_{i+1}}\Delta\alpha\_{t}dt+\int\_{t\_{i}}^{t\_{i+1}}\Delta\beta\_{t}dW\_{t}\bigg|^{2} |  |
|  |  |  |
| --- | --- | --- |
|  | =‚àí|Œî‚ÄãXti|2+2‚ÄãŒî‚ÄãXti‚Äã(Œî‚ÄãXti+Œî‚Äãb‚Äãh+Œî‚ÄãœÉ‚ÄãŒî‚ÄãWti+‚à´titi+1Œî‚ÄãŒ±t‚Äãùëët+‚à´titi+1Œî‚ÄãŒ≤t‚ÄãùëëWt)\displaystyle=-|\Delta X\_{t\_{i}}|^{2}+2\Delta X\_{t\_{i}}\bigg(\Delta X\_{t\_{i}}+\Delta bh+\Delta\sigma\Delta W\_{{t\_{i}}}+\int\_{t\_{i}}^{t\_{i+1}}\Delta\alpha\_{t}dt+\int\_{t\_{i}}^{t\_{i+1}}\Delta\beta\_{t}dW\_{t}\bigg) |  |
|  |  |  |
| --- | --- | --- |
|  | +|Œî‚Äãb|2‚Äãh2+|Œî‚ÄãœÉ|2‚Äã|Œî‚ÄãWti|2+|‚à´titi+1Œî‚ÄãŒ±t‚Äãùëët|2+|‚à´titi+1Œî‚ÄãŒ≤t‚ÄãùëëWt|2\displaystyle\quad+|\Delta b|^{2}h^{2}+|\Delta\sigma|^{2}|\Delta W\_{t\_{i}}|^{2}+\bigg|\int\_{t\_{i}}^{t\_{i+1}}\Delta\alpha\_{t}dt\bigg|^{2}+\bigg|\int\_{t\_{i}}^{t\_{i+1}}\Delta\beta\_{t}dW\_{t}\bigg|^{2} |  |
|  |  |  |
| --- | --- | --- |
|  | +2‚Äãh‚ÄãŒî‚Äãb‚ÄãŒî‚ÄãœÉ‚ÄãŒî‚ÄãWti+2‚Äãh‚ÄãŒî‚Äãb‚Äã‚à´titi+1Œî‚ÄãŒ±t‚Äãùëët+2‚Äãh‚ÄãŒî‚Äãb‚Äã‚à´titi+1Œî‚ÄãŒ≤t‚ÄãùëëWt+2‚ÄãŒî‚ÄãœÉ‚ÄãŒî‚ÄãWti‚Äã‚à´titi+1Œî‚ÄãŒ±t‚Äãùëët\displaystyle\quad+2h\Delta b\Delta\sigma\Delta W\_{t\_{i}}+2h\Delta b\int\_{t\_{i}}^{t\_{i+1}}\Delta\alpha\_{t}dt+2h\Delta b\int\_{t\_{i}}^{t\_{i+1}}\Delta\beta\_{t}dW\_{t}+2\Delta\sigma\Delta W\_{t\_{i}}\int\_{t\_{i}}^{t\_{i+1}}\Delta\alpha\_{t}dt |  |
|  |  |  |
| --- | --- | --- |
|  | +2‚ÄãŒî‚ÄãœÉ‚ÄãŒî‚ÄãWti‚Äã‚à´titi+1Œî‚ÄãŒ≤t‚ÄãùëëWt+2‚Äã‚à´titi+1Œî‚ÄãŒ±t‚Äãùëët‚Äã‚à´titi+1Œî‚ÄãŒ≤t‚ÄãùëëWt.\displaystyle\quad+2\Delta\sigma\Delta W\_{t\_{i}}\int\_{t\_{i}}^{t\_{i+1}}\Delta\beta\_{t}dW\_{t}+2\int\_{t\_{i}}^{t\_{i+1}}\Delta\alpha\_{t}dt\int\_{t\_{i}}^{t\_{i+1}}\Delta\beta\_{t}dW\_{t}. |  |

Taking conditional expectation ùîºti[‚ãÖ]=ùîº[‚ãÖ|‚Ñ±ti]\mathbb{E}\_{t\_{i}}[\cdot]=\mathbb{E}[\cdot|\mathcal{F}\_{t\_{i}}] on both sides and using the fact that Xtil,Ytil,ZtilX\_{t\_{i}}^{l},Y\_{t\_{i}}^{l},Z\_{t\_{i}}^{l} are ‚Ñ±ti\mathcal{F}\_{t\_{i}}-measurable and so are Œî‚Äãb,Œî‚ÄãœÉ\Delta b,\Delta\sigma, gives

|  |  |  |
| --- | --- | --- |
|  | ùîºti‚Äã|Œî‚ÄãXti+1|2\displaystyle\mathbb{E}\_{t\_{i}}|\Delta X\_{t\_{i+1}}|^{2} |  |
|  |  |  |
| --- | --- | --- |
|  | ‚â§|Œî‚ÄãXti|2+2‚ÄãŒî‚ÄãXti‚Äã(Œî‚Äãb‚Äãh+ùîºti‚Äã‚à´titi+1Œî‚ÄãŒ±t‚Äãùëët)\displaystyle\leq|\Delta X\_{t\_{i}}|^{2}+2\Delta X\_{t\_{i}}\bigg(\Delta bh+\mathbb{E}\_{t\_{i}}\int\_{t\_{i}}^{t\_{i+1}}\Delta\alpha\_{t}dt\bigg) |  |
|  |  |  |
| --- | --- | --- |
|  | +|Œî‚Äãb|2‚Äãh2+|Œî‚ÄãœÉ|2‚Äãh+h‚Äãùîºti‚Äã‚à´titi+1|Œî‚ÄãŒ±t|2‚Äãùëët+ùîºti‚Äã‚à´titi+1|Œî‚ÄãŒ≤t|2‚Äãùëët\displaystyle\quad+|\Delta b|^{2}h^{2}+|\Delta\sigma|^{2}h+h\mathbb{E}\_{t\_{i}}\int\_{t\_{i}}^{t\_{i+1}}|\Delta\alpha\_{t}|^{2}dt+\mathbb{E}\_{t\_{i}}\int\_{t\_{i}}^{t\_{i+1}}|\Delta\beta\_{t}|^{2}dt |  |
|  |  |  |
| --- | --- | --- |
|  | +2‚Äãh‚ÄãŒî‚Äãb‚Äãùîºti‚Äã‚à´titi+1Œî‚ÄãŒ±t‚Äãùëët+2‚Äãùîºti‚Äã[Œî‚ÄãœÉ‚ÄãŒî‚ÄãWti‚Äã‚à´titi+1Œî‚ÄãŒ±t‚Äãùëët]\displaystyle\quad+2h\Delta b\mathbb{E}\_{t\_{i}}\int\_{t\_{i}}^{t\_{i+1}}\Delta\alpha\_{t}dt+2\mathbb{E}\_{t\_{i}}\bigg[\Delta\sigma\Delta W\_{t\_{i}}\int\_{t\_{i}}^{t\_{i+1}}\Delta\alpha\_{t}dt\bigg] |  |
|  |  |  |
| --- | --- | --- |
|  | +2‚Äãùîºti‚Äã[Œî‚ÄãœÉ‚ÄãŒî‚ÄãWti‚Äã‚à´titi+1Œî‚ÄãŒ≤t‚ÄãùëëWt]+2‚Äãùîºti‚Äã[‚à´titi+1Œî‚ÄãŒ±t‚Äãùëët‚Äã‚à´titi+1Œî‚ÄãŒ≤t‚ÄãùëëWt]\displaystyle\quad+2\mathbb{E}\_{t\_{i}}\bigg[\Delta\sigma\Delta W\_{t\_{i}}\int\_{t\_{i}}^{t\_{i+1}}\Delta\beta\_{t}dW\_{t}\bigg]+2\mathbb{E}\_{t\_{i}}\bigg[\int\_{t\_{i}}^{t\_{i+1}}\Delta\alpha\_{t}dt\int\_{t\_{i}}^{t\_{i+1}}\Delta\beta\_{t}dW\_{t}\bigg] |  |
|  |  |  |
| --- | --- | --- |
|  | ‚â§|Œî‚ÄãXti|2+Œª12‚Äãh‚Äã|Œî‚ÄãXti|2+2‚ÄãhŒª1‚Äã|Œî‚Äãb|2+Œª12‚Äãh‚Äã|Œî‚ÄãXti|2+2Œª1‚Äãh‚Äã|ùîºti‚Äã‚à´titi+1Œî‚ÄãŒ±t‚Äãùëët|2\displaystyle\leq|\Delta X\_{t\_{i}}|^{2}+\frac{\lambda\_{1}}{2}h|\Delta X\_{t\_{i}}|^{2}+\frac{2h}{\lambda\_{1}}|\Delta b|^{2}+\frac{\lambda\_{1}}{2}h|\Delta X\_{t\_{i}}|^{2}+\frac{2}{\lambda\_{1}h}\bigg|\mathbb{E}\_{t\_{i}}\int\_{t\_{i}}^{t\_{i+1}}\Delta\alpha\_{t}dt\bigg|^{2} |  |
|  |  |  |
| --- | --- | --- |
|  | +|Œî‚Äãb|2‚Äãh2+|Œî‚ÄãœÉ|2‚Äãh+h‚Äãùîºti‚Äã‚à´titi+1|Œî‚ÄãŒ±t|2‚Äãùëët+ùîºti‚Äã‚à´titi+1|Œî‚ÄãŒ≤t|2‚Äãùëët\displaystyle\quad+|\Delta b|^{2}h^{2}+|\Delta\sigma|^{2}h+h\mathbb{E}\_{t\_{i}}\int\_{t\_{i}}^{t\_{i+1}}|\Delta\alpha\_{t}|^{2}dt+\mathbb{E}\_{t\_{i}}\int\_{t\_{i}}^{t\_{i+1}}|\Delta\beta\_{t}|^{2}dt |  |
|  |  |  |
| --- | --- | --- |
|  | +Œª2‚Äãh2‚Äã|Œî‚Äãb|2+hŒª2‚Äãùîºti‚Äã‚à´titi+1|Œî‚ÄãŒ±t|2‚Äãùëët+Œª12‚Äãùîºti‚Äã|Œî‚ÄãœÉ‚ÄãŒî‚ÄãWti|2+2‚ÄãhŒª1‚Äãùîºti‚Äã‚à´titi+1|Œî‚ÄãŒ±t|2‚Äãùëët\displaystyle\quad+\lambda\_{2}h^{2}|\Delta b|^{2}+\frac{h}{\lambda\_{2}}\mathbb{E}\_{t\_{i}}\int\_{t\_{i}}^{t\_{i+1}}|\Delta\alpha\_{t}|^{2}dt+\frac{\lambda\_{1}}{2}\mathbb{E}\_{t\_{i}}|\Delta\sigma\Delta W\_{t\_{i}}|^{2}+\frac{2h}{\lambda\_{1}}\mathbb{E}\_{t\_{i}}\int\_{t\_{i}}^{t\_{i+1}}|\Delta\alpha\_{t}|^{2}dt |  |
|  |  |  |
| --- | --- | --- |
|  | +Œª12‚Äãùîºti‚Äã|Œî‚ÄãœÉ‚ÄãŒî‚ÄãWti|2+2Œª1‚Äãùîºti‚Äã‚à´titi+1|Œî‚ÄãŒ≤t|2‚Äãùëët+Œª2‚Äãh‚Äãùîºti‚Äã‚à´titi+1|Œî‚ÄãŒ±t|2‚Äãùëët+1Œª2‚Äãùîºti‚Äã‚à´titi+1|Œî‚ÄãŒ≤t|2‚Äãùëët\displaystyle\quad+\frac{\lambda\_{1}}{2}\mathbb{E}\_{t\_{i}}|\Delta\sigma\Delta W\_{t\_{i}}|^{2}+\frac{2}{\lambda\_{1}}\mathbb{E}\_{t\_{i}}\int\_{t\_{i}}^{t\_{i+1}}|\Delta\beta\_{t}|^{2}dt+\lambda\_{2}h\mathbb{E}\_{t\_{i}}\int\_{t\_{i}}^{t\_{i+1}}|\Delta\alpha\_{t}|^{2}dt+\frac{1}{\lambda\_{2}}\mathbb{E}\_{t\_{i}}\int\_{t\_{i}}^{t\_{i+1}}|\Delta\beta\_{t}|^{2}dt |  |
|  |  |  |
| --- | --- | --- |
|  | ‚â§(1+Œª1‚Äãh)‚Äã|Œî‚ÄãXti|2+(2‚ÄãhŒª1+h2+Œª2‚Äãh2)‚Äã|Œî‚Äãb|2+(h+Œª1‚Äãh)‚Äã|Œî‚ÄãœÉ|2\displaystyle\leq(1+\lambda\_{1}h)|\Delta X\_{t\_{i}}|^{2}+\bigg(\frac{2h}{\lambda\_{1}}+h^{2}+\lambda\_{2}h^{2}\bigg)|\Delta b|^{2}+(h+\lambda\_{1}h)|\Delta\sigma|^{2} |  |
|  |  |  |
| --- | --- | --- |
|  | +(2Œª1+h+hŒª2+2‚ÄãhŒª1+Œª2‚Äãh)‚Äãùîºti‚Äã‚à´titi+1|Œî‚ÄãŒ±t|2‚Äãùëët+(1+2Œª1+1Œª2)‚Äãùîºti‚Äã‚à´titi+1|Œî‚ÄãŒ≤t|2‚Äãùëët\displaystyle\quad+\bigg(\frac{2}{\lambda\_{1}}+h+\frac{h}{\lambda\_{2}}+\frac{2h}{\lambda\_{1}}+\lambda\_{2}h\bigg)\mathbb{E}\_{t\_{i}}\int\_{t\_{i}}^{t\_{i+1}}|\Delta\alpha\_{t}|^{2}dt+\bigg(1+\frac{2}{\lambda\_{1}}+\frac{1}{\lambda\_{2}}\bigg)\mathbb{E}\_{t\_{i}}\int\_{t\_{i}}^{t\_{i+1}}|\Delta\beta\_{t}|^{2}dt |  |
|  |  |  |
| --- | --- | --- |
|  | ‚â§(1+Œª1‚Äãh)‚Äã|Œî‚ÄãXti|2+(2‚ÄãhŒª1+h2+Œª2‚Äãh2)‚Äã(K‚Äã|Œî‚ÄãXti|2+by‚Äã|Œî‚ÄãYti|2+bz‚Äã|Œî‚ÄãZti|2)\displaystyle\leq(1+\lambda\_{1}h)|\Delta X\_{t\_{i}}|^{2}+\bigg(\frac{2h}{\lambda\_{1}}+h^{2}+\lambda\_{2}h^{2}\bigg)\big(K|\Delta X\_{t\_{i}}|^{2}+b\_{y}|\Delta Y\_{t\_{i}}|^{2}+b\_{z}|\Delta Z\_{t\_{i}}|^{2}\big) |  |
|  |  |  |
| --- | --- | --- |
|  | +(h+Œª1‚Äãh)‚Äã(œÉx‚Äã|Œî‚ÄãXti|2+œÉy‚Äã|Œî‚ÄãYti|2+œÉz‚Äã|Œî‚ÄãZti|2)\displaystyle\quad+(h+\lambda\_{1}h)\bigg(\sigma\_{x}|\Delta X\_{t\_{i}}|^{2}+\sigma\_{y}|\Delta Y\_{t\_{i}}|^{2}+\sigma\_{z}|\Delta Z\_{t\_{i}}|^{2}\bigg) |  |
|  |  |  |
| --- | --- | --- |
|  | +(2Œª1+h+hŒª2+2‚ÄãhŒª1+Œª2‚Äãh)‚Äãùîºti‚Äã‚à´titi+1|Œî‚ÄãŒ±t|2‚Äãùëët+(1+2Œª1+1Œª2)‚Äãùîºti‚Äã‚à´titi+1|Œî‚ÄãŒ≤t|2‚Äãùëët\displaystyle\quad+\bigg(\frac{2}{\lambda\_{1}}+h+\frac{h}{\lambda\_{2}}+\frac{2h}{\lambda\_{1}}+\lambda\_{2}h\bigg)\mathbb{E}\_{t\_{i}}\int\_{t\_{i}}^{t\_{i+1}}|\Delta\alpha\_{t}|^{2}dt+\bigg(1+\frac{2}{\lambda\_{1}}+\frac{1}{\lambda\_{2}}\bigg)\mathbb{E}\_{t\_{i}}\int\_{t\_{i}}^{t\_{i+1}}|\Delta\beta\_{t}|^{2}dt |  |
|  |  |  |
| --- | --- | --- |
|  | ‚â§(1+Œª1‚Äãh)‚Äã|Œî‚ÄãXti|2+(2‚ÄãhŒª1+h2+Œª2‚Äãh2)‚Äã(K‚Äã|Œî‚ÄãXti|2+by‚Äã|Œî‚ÄãYti|2+bz‚Äã|Œî‚ÄãZti|2)\displaystyle\leq(1+\lambda\_{1}h)|\Delta X\_{t\_{i}}|^{2}+\bigg(\frac{2h}{\lambda\_{1}}+h^{2}+\lambda\_{2}h^{2}\bigg)\big(K|\Delta X\_{t\_{i}}|^{2}+b\_{y}|\Delta Y\_{t\_{i}}|^{2}+b\_{z}|\Delta Z\_{t\_{i}}|^{2}\big) |  |
|  |  |  |
| --- | --- | --- |
|  | +(h+Œª1‚Äãh)‚Äã(œÉx‚Äã|Œî‚ÄãXti|2+œÉy‚Äã|Œî‚ÄãYti|2+œÉz‚Äã|Œî‚ÄãZti|2)\displaystyle\quad+(h+\lambda\_{1}h)\bigg(\sigma\_{x}|\Delta X\_{t\_{i}}|^{2}+\sigma\_{y}|\Delta Y\_{t\_{i}}|^{2}+\sigma\_{z}|\Delta Z\_{t\_{i}}|^{2}\bigg) |  |
|  |  |  |
| --- | --- | --- |
|  | +(1+4Œª1+1Œª2+Œª2)‚Äãùîºti‚Äã‚à´titi+1(|Œî‚ÄãŒ±t|2+|Œî‚ÄãŒ≤t|2)‚Äãùëët,\displaystyle\quad+\bigg(1+\frac{4}{\lambda\_{1}}+\frac{1}{\lambda\_{2}}+\lambda\_{2}\bigg)\mathbb{E}\_{t\_{i}}\int\_{t\_{i}}^{t\_{i+1}}\bigg(|\Delta\alpha\_{t}|^{2}+|\Delta\beta\_{t}|^{2}\bigg)dt, |  |

for Œª1,Œª2>0\lambda\_{1},\lambda\_{2}>0, which completes the proof.
‚àé

## Appendix B Proof of Lemma [2.5](https://arxiv.org/html/2511.08735v1#S2.Thmthm5 "Lemma 2.5. ‚Ä£ 2.2 Solutions to FBSDEs ‚Ä£ 2 Preliminaries ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")

###### Proof.

We define the difference

|  |  |  |  |
| --- | --- | --- | --- |
|  | Œî‚Äãf:=f‚Äã(ti,Vti,Xti1,Yti1,Z^ti1)‚àíf‚Äã(ti,Vti,Xti2,Yti2,Z^ti2).\Delta f:=f(t\_{i},V\_{t\_{i}},X\_{t\_{i}}^{1},Y\_{t\_{i}}^{1},\hat{Z}\_{t\_{i}}^{1})-f(t\_{i},V\_{t\_{i}},X\_{t\_{i}}^{2},Y\_{t\_{i}}^{2},\hat{Z}\_{t\_{i}}^{2}). |  | (B.1) |

Then the backward process difference satisfies

|  |  |  |  |
| --- | --- | --- | --- |
|  | Œî‚ÄãYti+‚à´titi+1Œî‚ÄãZt‚ÄãùëëWt=Œî‚ÄãYti+1+Œî‚Äãf‚Äãh+‚à´titi+1Œî‚ÄãŒ≥t‚Äãùëët.\Delta Y\_{t\_{i}}+\int\_{t\_{i}}^{t\_{i+1}}\Delta Z\_{t}dW\_{t}=\Delta Y\_{t\_{i+1}}+\Delta fh+\int\_{t\_{i}}^{t\_{i+1}}\Delta\gamma\_{t}dt. |  | (B.2) |

Squaring both sides yields

|  |  |  |
| --- | --- | --- |
|  | |Œî‚ÄãYti|2+|‚à´titi+1Œî‚ÄãZt‚ÄãùëëWt|2+2‚ÄãŒî‚ÄãYti‚ãÖ‚à´titi+1Œî‚ÄãZt‚ÄãùëëWt\displaystyle|\Delta Y\_{t\_{i}}|^{2}+\bigg|\int\_{t\_{i}}^{t\_{i+1}}\Delta Z\_{t}dW\_{t}\bigg|^{2}+2\Delta Y\_{t\_{i}}\cdot\int\_{t\_{i}}^{t\_{i+1}}\Delta Z\_{t}dW\_{t} |  |
|  |  |  |
| --- | --- | --- |
|  | =|Œî‚ÄãYti+1+Œî‚Äãf‚Äãh|2+2‚Äã(Œî‚ÄãYti+1+Œî‚Äãf‚Äãh)‚Äã‚à´titi+1Œî‚ÄãŒ≥t‚Äãùëët+|‚à´titi+1Œî‚ÄãŒ≥t‚Äãùëët|2\displaystyle=\big|\Delta Y\_{t\_{i+1}}+\Delta fh\big|^{2}+2(\Delta Y\_{t\_{i+1}}+\Delta fh)\int\_{t\_{i}}^{t\_{i+1}}\Delta\gamma\_{t}dt+\bigg|\int\_{t\_{i}}^{t\_{i+1}}\Delta\gamma\_{t}dt\bigg|^{2} |  |
|  |  |  |
| --- | --- | --- |
|  | ‚â§(1+Œª4‚Äãh)‚Äã|Œî‚ÄãYti+1+Œî‚Äãf‚Äãh|2+(1+1Œª4‚Äãh)‚Äã|‚à´titi+1Œî‚ÄãŒ≥t‚Äãùëët|2.\displaystyle\leq(1+\lambda\_{4}h)\big|\Delta Y\_{t\_{i+1}}+\Delta fh\big|^{2}+\left(1+\frac{1}{\lambda\_{4}h}\right)\bigg|\int\_{t\_{i}}^{t\_{i+1}}\Delta\gamma\_{t}dt\bigg|^{2}. |  |

Then, taking conditional expectations on both sides, we obtain

|  |  |  |  |
| --- | --- | --- | --- |
|  |  | |Œî‚ÄãYti|2+ùîºti‚Äã‚à´titi+1|Œî‚ÄãZt|2‚Äãùëët\displaystyle|\Delta Y\_{t\_{i}}|^{2}+\mathbb{E}\_{t\_{i}}\int\_{t\_{i}}^{t\_{i+1}}|\Delta Z\_{t}|^{2}dt |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | ‚â§(1+Œª4‚Äãh)‚Äãùîºti‚Äã[|Œî‚ÄãYti+1|2+|Œî‚Äãf‚Äãh|2+2‚ÄãŒî‚ÄãYti+1‚ÄãŒî‚Äãf‚Äãh]+(1+1Œª4‚Äãh)‚Äãùîºti‚Äã|‚à´titi+1Œî‚ÄãŒ≥t‚Äãùëët|2.\displaystyle\leq(1+\lambda\_{4}h)\mathbb{E}\_{t\_{i}}\bigg[|\Delta Y\_{t\_{i+1}}|^{2}+|\Delta fh|^{2}+2\Delta Y\_{t\_{i+1}}\Delta fh\bigg]+\bigg(1+\frac{1}{\lambda\_{4}h}\bigg)\mathbb{E}\_{t\_{i}}\bigg|\int\_{t\_{i}}^{t\_{i+1}}\Delta\gamma\_{t}dt\bigg|^{2}. |  | (B.3) |

Note that,

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùîºti‚Äã‚à´titi+1|Œî‚ÄãZt|2‚Äãùëët\displaystyle\mathbb{E}\_{t\_{i}}\int\_{t\_{i}}^{t\_{i+1}}|\Delta Z\_{t}|^{2}dt | ‚â•1h‚Äã|ùîºti‚Äã‚à´titi+1Œî‚ÄãZt‚Äãùëët|2\displaystyle\geq\frac{1}{h}\bigg|\mathbb{E}\_{t\_{i}}\int\_{t\_{i}}^{t\_{i+1}}\Delta Z\_{t}dt\bigg|^{2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =1h‚Äã|ùîºti‚Äã[(Œî‚ÄãYti+1‚àíŒî‚ÄãYti+Œî‚Äãf‚Äãh+‚à´titi+1Œî‚ÄãŒ≥t‚Äãùëët)‚ÄãŒî‚ÄãWti]|2\displaystyle=\frac{1}{h}\bigg|\mathbb{E}\_{t\_{i}}\bigg[\bigg(\Delta Y\_{t\_{i+1}}-\Delta Y\_{t\_{i}}+\Delta fh+\int\_{t\_{i}}^{t\_{i+1}}\Delta\gamma\_{t}dt\bigg)\Delta W\_{t\_{i}}\bigg]\bigg|^{2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =1h‚Äã|h‚ÄãŒî‚ÄãZ^ti+ùîºti‚Äã[Œî‚ÄãWti‚Äã‚à´titi+1Œî‚ÄãŒ≥t‚Äãùëët]|2\displaystyle=\frac{1}{h}\bigg|h\Delta\hat{Z}\_{t\_{i}}+\mathbb{E}\_{t\_{i}}\bigg[\Delta W\_{t\_{i}}\int\_{t\_{i}}^{t\_{i+1}}\Delta\gamma\_{t}dt\bigg]\bigg|^{2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â•1h‚Äã(h2‚Äã|Œî‚ÄãZ^ti|2‚àí2‚Äãh‚Äã|ùîºti‚Äã[Œî‚ÄãZ^ti‚ÄãŒî‚ÄãWti‚Äã‚à´titi+1Œî‚ÄãŒ≥t‚Äãùëët]|)\displaystyle\geq\frac{1}{h}\bigg(h^{2}|\Delta\hat{Z}\_{t\_{i}}|^{2}-2h\bigg|\mathbb{E}\_{t\_{i}}\bigg[\Delta\hat{Z}\_{t\_{i}}\Delta W\_{t\_{i}}\int\_{t\_{i}}^{t\_{i+1}}\Delta\gamma\_{t}dt\bigg]\bigg|\bigg) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =h‚Äã|Œî‚ÄãZ^ti|2‚àí2‚Äã|ùîºti‚Äã[Œî‚ÄãZ^ti‚ÄãŒî‚ÄãWti‚Äã‚à´titi+1Œî‚ÄãŒ≥t‚Äãùëët]|\displaystyle=h|\Delta\hat{Z}\_{t\_{i}}|^{2}-2\bigg|\mathbb{E}\_{t\_{i}}\bigg[\Delta\hat{Z}\_{t\_{i}}\Delta W\_{t\_{i}}\int\_{t\_{i}}^{t\_{i+1}}\Delta\gamma\_{t}dt\bigg]\bigg| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â•(1‚àíŒª3)‚Äãh‚Äã|Œî‚ÄãZ^ti|2‚àí1Œª3‚Äãùîºti‚Äã|‚à´titi+1Œî‚ÄãŒ≥t‚Äãùëët|2\displaystyle\geq(1-\lambda\_{3})h|\Delta\hat{Z}\_{t\_{i}}|^{2}-\frac{1}{\lambda\_{3}}\mathbb{E}\_{t\_{i}}\bigg|\int\_{t\_{i}}^{t\_{i+1}}\Delta\gamma\_{t}dt\bigg|^{2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â•(1‚àíŒª3)‚Äãh‚Äã|Œî‚ÄãZ^ti|2‚àí1Œª3‚Äãùîºti‚Äã|‚à´titi+1Œî‚ÄãŒ≥t‚Äãùëët|2,\displaystyle\geq(1-\lambda\_{3})h|\Delta\hat{Z}\_{t\_{i}}|^{2}-\frac{1}{\lambda\_{3}}\mathbb{E}\_{t\_{i}}\bigg|\int\_{t\_{i}}^{t\_{i+1}}\Delta\gamma\_{t}dt\bigg|^{2}, |  |

and

|  |  |  |  |
| --- | --- | --- | --- |
|  | 2‚Äãh‚ÄãŒî‚ÄãYti+1‚ÄãŒî‚Äãf\displaystyle 2h\Delta Y\_{t\_{i+1}}\Delta f | =2‚Äãh‚ÄãŒî‚ÄãYti+1‚Äã|f‚Äã(ti,Vti,Xti1,Yti1,Z^ti1)‚àíf‚Äã(ti,Vti,Xti2,Yti2,Z^ti2)|\displaystyle=2h\Delta Y\_{t\_{i+1}}\big|f(t\_{i},V\_{t\_{i}},X\_{t\_{i}}^{1},Y\_{t\_{i}}^{1},\hat{Z}\_{t\_{i}}^{1})-f(t\_{i},V\_{t\_{i}},X\_{t\_{i}}^{2},Y\_{t\_{i}}^{2},\hat{Z}\_{t\_{i}}^{2})\big| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§hŒª5‚Äã|Œî‚ÄãYti+1|2+h‚ÄãŒª5‚Äã|f‚Äã(ti,Vti,Xti1,Yti1,Z^ti1)‚àíf‚Äã(ti,Vti,Xti2,Yti2,Z^ti2)|2\displaystyle\leq\frac{h}{\lambda\_{5}}\big|\Delta Y\_{t\_{i+1}}\big|^{2}+h\lambda\_{5}\big|f(t\_{i},V\_{t\_{i}},X\_{t\_{i}}^{1},Y\_{t\_{i}}^{1},\hat{Z}\_{t\_{i}}^{1})-f(t\_{i},V\_{t\_{i}},X\_{t\_{i}}^{2},Y\_{t\_{i}}^{2},\hat{Z}\_{t\_{i}}^{2})\big|^{2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§hŒª5‚Äã|Œî‚ÄãYti+1|2+Œª5‚Äãh‚Äã(fx‚Äã|Œî‚ÄãXti|2+K‚Äã|Œî‚ÄãYti|2+fz‚Äã|Œî‚ÄãZ^ti|2).\displaystyle\leq\frac{h}{\lambda\_{5}}|\Delta Y\_{t\_{i+1}}|^{2}+\lambda\_{5}h\bigg(f\_{x}|\Delta X\_{t\_{i}}|^{2}+K|\Delta Y\_{t\_{i}}|^{2}+f\_{z}|\Delta\hat{Z}\_{t\_{i}}|^{2}\bigg). |  |

Using above two estimates in ([B](https://arxiv.org/html/2511.08735v1#A2.Ex4 "Appendix B Proof of Lemma 2.5 ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")), we finally get

|  |  |  |  |
| --- | --- | --- | --- |
|  | |Œî‚ÄãYti|2+(1‚àíŒª3)‚Äãh‚Äã|Œî‚ÄãZ^ti|2\displaystyle|\Delta Y\_{t\_{i}}|^{2}+(1-\lambda\_{3})h|\Delta\hat{Z}\_{t\_{i}}|^{2} | ‚â§(1+Œª4‚Äãh)‚Äãùîºti‚Äã[|Œî‚ÄãYti+1|2+|Œî‚Äãf‚Äãh|2+2‚ÄãŒî‚ÄãYti+1‚ÄãŒî‚Äãf‚Äãh]\displaystyle\leq(1+\lambda\_{4}h)\mathbb{E}\_{t\_{i}}\bigg[|\Delta Y\_{t\_{i+1}}|^{2}+|\Delta fh|^{2}+2\Delta Y\_{t\_{i+1}}\Delta fh\bigg] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +(1+1Œª4‚Äãh)‚Äãùîºti‚Äã|‚à´titi+1Œî‚ÄãŒ≥t‚Äãùëët|2+1Œª3‚Äãùîºti‚Äã|‚à´titi+1Œî‚ÄãŒ≥t‚Äãùëët|2\displaystyle+\left(1+\frac{1}{\lambda\_{4}h}\right)\mathbb{E}\_{t\_{i}}\bigg|\int\_{t\_{i}}^{t\_{i+1}}\Delta\gamma\_{t}dt\bigg|^{2}+\frac{1}{\lambda\_{3}}\mathbb{E}\_{t\_{i}}\bigg|\int\_{t\_{i}}^{t\_{i+1}}\Delta\gamma\_{t}dt\bigg|^{2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§(1+Œª4h)ùîºti[|ŒîYti+1|2+h2(fx|ŒîXti|2+K|ŒîYti|2+fz|ŒîZ^ti|2)\displaystyle\leq(1+\lambda\_{4}h)\mathbb{E}\_{t\_{i}}\bigg[|\Delta Y\_{t\_{i+1}}|^{2}+h^{2}\big(f\_{x}|\Delta X\_{t\_{i}}|^{2}+K|\Delta Y\_{t\_{i}}|^{2}+f\_{z}|\Delta\hat{Z}\_{t\_{i}}|^{2}\big) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +hŒª5ùîºti|ŒîYti+1|2+Œª5h(fx|ŒîXti|2+K|ŒîYti|2+fz|ŒîZ^ti|2)]\displaystyle+\frac{h}{\lambda\_{5}}\mathbb{E}\_{t\_{i}}|\Delta Y\_{t\_{i+1}}|^{2}+\lambda\_{5}h\bigg(f\_{x}|\Delta X\_{t\_{i}}|^{2}+K|\Delta Y\_{t\_{i}}|^{2}+f\_{z}|\Delta\hat{Z}\_{t\_{i}}|^{2}\bigg)\bigg] |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | +(1+1Œª4‚Äãh+1Œª3)‚Äãùîºti‚Äã|‚à´titi+1Œî‚ÄãŒ≥t‚Äãùëët|2,\displaystyle+\left(1+\frac{1}{\lambda\_{4}h}+\frac{1}{\lambda\_{3}}\right)\mathbb{E}\_{t\_{i}}\bigg|\int\_{t\_{i}}^{t\_{i+1}}\Delta\gamma\_{t}dt\bigg|^{2}, |  | (B.4) |

which completes the proof.
‚àé

## Appendix C Proof of Theorem [2.6](https://arxiv.org/html/2511.08735v1#S2.Thmthm6 "Theorem 2.6. ‚Ä£ 2.2 Solutions to FBSDEs ‚Ä£ 2 Preliminaries ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")

###### Proof.

Step 1: From the forward equation of ([1.1](https://arxiv.org/html/2511.08735v1#S1.E1 "In 1 Introduction ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) we can have

|  |  |  |  |
| --- | --- | --- | --- |
|  | Xti+1\displaystyle X\_{t\_{i+1}} | =Xti+b‚Äã(ti,Vti,Xti,Yti,Z^ti)‚Äãh+‚à´titi+1(b‚Äã(t,Vt,Xt,Yt,Zt)‚àíb‚Äã(ti,Vti,Xti,Yti,Z^ti))‚Äãùëët\displaystyle=X\_{t\_{i}}+b(t\_{i},V\_{t\_{i}},X\_{t\_{i}},Y\_{t\_{i}},\hat{Z}\_{t\_{i}})h+\int\_{t\_{i}}^{t\_{i+1}}\Big(b(t,V\_{t},X\_{t},Y\_{t},Z\_{t})-b(t\_{i},V\_{t\_{i}},X\_{t\_{i}},Y\_{t\_{i}},\hat{Z}\_{t\_{i}})\Big)dt |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | +œÉ‚Äã(ti,Vti,Xti,Yti,Z^ti)‚ÄãŒî‚ÄãWti+‚à´titi+1(œÉ‚Äã(t,Vt,Xt,Yt,Zt)‚àíœÉ‚Äã(ti,Vti,Xti,Yti,Z^ti))‚ÄãùëëWt,\displaystyle+\sigma(t\_{i},V\_{t\_{i}},X\_{t\_{i}},Y\_{t\_{i}},\hat{Z}\_{t\_{i}})\Delta W\_{t\_{i}}+\int\_{t\_{i}}^{t\_{i+1}}\Big(\sigma(t,V\_{t},X\_{t},Y\_{t},Z\_{t})-\sigma(t\_{i},V\_{t\_{i}},X\_{t\_{i}},Y\_{t\_{i}},\hat{Z}\_{t\_{i}})\Big)dW\_{t}, |  | (C.1) |

with Z^ti=1h‚Äãùîºti‚Äã[Yti+1‚ÄãŒî‚ÄãWti]\hat{Z}\_{t\_{i}}=\frac{1}{h}\mathbb{E}\_{t\_{i}}\big[Y\_{t\_{i+1}}\Delta W\_{t\_{i}}\big].
And from ([2.23](https://arxiv.org/html/2511.08735v1#S2.E23 "In 2.2 Solutions to FBSDEs ‚Ä£ 2 Preliminaries ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) we have,

|  |  |  |  |
| --- | --- | --- | --- |
|  | X¬Øti+1œÄ=X¬ØtiœÄ+b‚Äã(ti,Vti‚ÄãX¬ØtiœÄ,Y¬ØtiœÄ,Z¬ØtiœÄ)‚Äãh+œÉ‚Äã(ti,Vti‚ÄãX¬ØtiœÄ,Y¬ØtiœÄ,Z¬ØtiœÄ)‚ÄãŒî‚ÄãWti.\displaystyle\overline{X}\_{t\_{i+1}}^{\pi}=\overline{X}\_{t\_{i}}^{\pi}+b\big(t\_{i},V\_{t\_{i}}\overline{X}\_{t\_{i}}^{\pi},\overline{Y}\_{t\_{i}}^{\pi},\overline{Z}\_{t\_{i}}^{\pi}\big)h+\sigma\big(t\_{i},V\_{t\_{i}}\overline{X}\_{t\_{i}}^{\pi},\overline{Y}\_{t\_{i}}^{\pi},\overline{Z}\_{t\_{i}}^{\pi}\big)\Delta W\_{t\_{i}}. |  | (C.2) |

Now applying Lemma [2.4](https://arxiv.org/html/2511.08735v1#S2.Thmthm4 "Lemma 2.4. ‚Ä£ 2.2 Solutions to FBSDEs ‚Ä£ 2 Preliminaries ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications") on XX and X¬ØœÄ\overline{X}^{\pi} from ([C](https://arxiv.org/html/2511.08735v1#A3.Ex1 "Appendix C Proof of Theorem 2.6 ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) and ([C.2](https://arxiv.org/html/2511.08735v1#A3.E2 "In Appendix C Proof of Theorem 2.6 ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) respectively, we have

|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ùîº‚Äã[|Xti+1‚àíX¬Øti+1œÄ|2]\displaystyle\mathbb{E}\big[|X\_{t\_{i+1}}-\overline{X}\_{t\_{i+1}}^{\pi}|^{2}\big] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§(1+A1‚Äãh)‚Äãùîº‚Äã|Xti‚àíX¬ØtiœÄ|2+A2‚Äãh‚Äãùîº‚Äã|Yti‚àíY¬ØtiœÄ|2+A3‚Äãh‚Äãùîº‚Äã|Z^ti‚àíZ¬ØtiœÄ|2\displaystyle\leq(1+A\_{1}h)\mathbb{E}|X\_{t\_{i}}-\overline{X}\_{t\_{i}}^{\pi}|^{2}+A\_{2}h\mathbb{E}|Y\_{t\_{i}}-\overline{Y}\_{t\_{i}}^{\pi}|^{2}+A\_{3}h\mathbb{E}|\hat{Z}\_{t\_{i}}-\overline{Z}\_{t\_{i}}^{\pi}|^{2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +C‚Äãùîº‚Äã‚à´titi+1|b‚Äã(t,Vt,Xt,Yt,Zt)‚àíb‚Äã(ti,Vti,Xti,Yti,Z^ti)|2‚Äãùëët\displaystyle+C\mathbb{E}\int\_{t\_{i}}^{t\_{i+1}}\big|b(t,V\_{t},X\_{t},Y\_{t},Z\_{t})-b(t\_{i},V\_{t\_{i}},X\_{t\_{i}},Y\_{t\_{i}},\hat{Z}\_{t\_{i}})\big|^{2}dt |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +C‚Äãùîº‚Äã‚à´titi+1|œÉ‚Äã(t,Vt,Xt,Yt,Zt)‚àíœÉ‚Äã(ti,Vti,Xti,Yti,Z^ti)|2‚Äãùëët\displaystyle+C\mathbb{E}\int\_{t\_{i}}^{t\_{i+1}}\big|\sigma(t,V\_{t},X\_{t},Y\_{t},Z\_{t})-\sigma(t\_{i},V\_{t\_{i}},X\_{t\_{i}},Y\_{t\_{i}},\hat{Z}\_{t\_{i}})\big|^{2}dt |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§(1+A1‚Äãh)‚Äãùîº‚Äã|Xti‚àíX¬ØtiœÄ|2+A2‚Äãh‚Äãùîº‚Äã|Yti‚àíY¬ØtiœÄ|2+A3‚Äãh‚Äãùîº‚Äã|Z^ti‚àíZ¬ØtiœÄ|2\displaystyle\leq(1+A\_{1}h)\mathbb{E}|X\_{t\_{i}}-\overline{X}\_{t\_{i}}^{\pi}|^{2}+A\_{2}h\mathbb{E}|Y\_{t\_{i}}-\overline{Y}\_{t\_{i}}^{\pi}|^{2}+A\_{3}h\mathbb{E}|\hat{Z}\_{t\_{i}}-\overline{Z}\_{t\_{i}}^{\pi}|^{2} |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | +C‚Äãùîº‚Äã‚à´titi+1(œÅ‚Äã(h)+|Xt‚àíXti|2+|Yt‚àíYti|2+|Zt‚àíZ^ti|2)‚Äãùëët.\displaystyle+C\mathbb{E}\int\_{t\_{i}}^{t\_{i+1}}\bigg(\rho(h)+|X\_{t}-X\_{t\_{i}}|^{2}+|Y\_{t}-Y\_{t\_{i}}|^{2}+|Z\_{t}-\hat{Z}\_{t\_{i}}|^{2}\bigg)dt. |  | (C.3) |

Now since,

|  |  |  |
| --- | --- | --- |
|  | h2‚Äãùîº‚Äã|Z~ti‚àíZ^ti|2\displaystyle h^{2}\mathbb{E}|\tilde{Z}\_{t\_{i}}-\hat{Z}\_{t\_{i}}|^{2} |  |
|  |  |  |
| --- | --- | --- |
|  | =ùîº‚Äã[|ùîºti‚Äã[‚à´titi+1Zt‚Äãùëët]‚àíùîºti‚Äã[(Yti‚àí‚à´titi+1f‚Äã(t,Vt,Xt,Yt,Zt)‚Äãùëët+‚à´titi+1Zt‚ÄãùëëWt)‚ÄãŒî‚ÄãWti]|2]\displaystyle=\mathbb{E}\bigg[\bigg|\mathbb{E}\_{t\_{i}}\bigg[\int\_{t\_{i}}^{t\_{i+1}}Z\_{t}dt\bigg]-\mathbb{E}\_{t\_{i}}\bigg[\bigg(Y\_{t\_{i}}-\int\_{t\_{i}}^{t\_{i+1}}f(t,V\_{t},X\_{t},Y\_{t},Z\_{t})dt+\int\_{t\_{i}}^{t\_{i+1}}Z\_{t}dW\_{t}\bigg)\Delta W\_{t\_{i}}\bigg]\bigg|^{2}\bigg] |  |
|  |  |  |
| --- | --- | --- |
|  | =ùîº‚Äã[|ùîºti‚Äã[(‚à´titi+1f‚Äã(t,Vt,Xt,Yt,Zt)‚Äãùëët)‚ÄãŒî‚ÄãWti]|2]\displaystyle=\mathbb{E}\bigg[\bigg|\mathbb{E}\_{t\_{i}}\bigg[\bigg(\int\_{t\_{i}}^{t\_{i+1}}f(t,V\_{t},X\_{t},Y\_{t},Z\_{t})dt\bigg)\Delta W\_{t\_{i}}\bigg]\bigg|^{2}\bigg] |  |
|  |  |  |
| --- | --- | --- |
|  | ‚â§ùîº‚Äã[ùîºti‚Äã|‚à´titi+1f‚Äã(t,Vt,Xt,Yt,Zt)‚Äãùëët|2‚ãÖùîºti‚Äã|Œî‚ÄãWti|2]\displaystyle\leq\mathbb{E}\left[\mathbb{E}\_{t\_{i}}\bigg|\int\_{t\_{i}}^{t\_{i+1}}f(t,V\_{t},X\_{t},Y\_{t},Z\_{t})dt\bigg|^{2}\cdot\mathbb{E}\_{t\_{i}}\big|\Delta W\_{t\_{i}}\big|^{2}\right] |  |
|  |  |  |
| --- | --- | --- |
|  | ‚â§h2‚Äãùîº‚Äã‚à´titi+1|f‚Äã(t,Vt,Xt,Yt,Zt)|2‚Äãùëët\displaystyle\leq h^{2}\mathbb{E}\int\_{t\_{i}}^{t\_{i+1}}\bigg|f(t,V\_{t},X\_{t},Y\_{t},Z\_{t})\bigg|^{2}dt |  |
|  |  |  |
| --- | --- | --- |
|  | ‚â§C‚Äãh2‚Äãùîº‚Äã[‚à´titi+1(|f‚Äã(t,Vt,0,0,0)|2+fx‚Äã|Xt|2+K‚Äã|Yt|2+fz‚Äã|Zt|2)‚Äãùëët]\displaystyle\leq Ch^{2}\mathbb{E}\bigg[\int\_{t\_{i}}^{t\_{i+1}}\Big(|f(t,V\_{t},0,0,0)|^{2}+f\_{x}|X\_{t}|^{2}+K|Y\_{t}|^{2}+f\_{z}|Z\_{t}|^{2}\Big)dt\bigg] |  |
|  |  |  |
| --- | --- | --- |
|  | ‚â§C‚Äã(I02+ùîº‚Äã|x0|2)‚Äãh3+C‚Äãh2‚Äãùîº‚Äã[‚à´titi+1|Zs|2‚Äãùëës+‚à´titi+1|f|2‚Äã(s,Vs,0,0,0)‚Äãùëës],\displaystyle\leq C\big(I\_{0}^{2}+\mathbb{E}|x\_{0}|^{2}\big)h^{3}+Ch^{2}\mathbb{E}\left[\int\_{t\_{i}}^{t\_{i+1}}|Z\_{s}|^{2}ds+\int\_{t\_{i}}^{t\_{i+1}}|f|^{2}(s,V\_{s},0,0,0)ds\right], |  |

we can deduce that,

|  |  |  |  |
| --- | --- | --- | --- |
|  | ùîº\displaystyle\mathbb{E} | ‚à´titi+1|Zt‚àíZ^ti|2‚Äãùëët‚â§2‚Äãùîº‚Äã‚à´titi+1(|Zt‚àíZ~ti|2+|Z~ti‚àíZ^ti|2)‚Äãùëët\displaystyle\int\_{t\_{i}}^{t\_{i+1}}|Z\_{t}-\hat{Z}\_{t\_{i}}|^{2}dt\leq 2\mathbb{E}\int\_{t\_{i}}^{t\_{i+1}}\Big(|Z\_{t}-\tilde{Z}\_{t\_{i}}|^{2}+|\tilde{Z}\_{t\_{i}}-\hat{Z}\_{t\_{i}}|^{2}\Big)dt |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =2‚Äãùîº‚Äã‚à´titi+1|Zt‚àíZ~ti|2‚Äãùëët+h‚Äãùîº‚Äã|Z~ti‚àíZ^ti|2\displaystyle=2\mathbb{E}\int\_{t\_{i}}^{t\_{i+1}}|Z\_{t}-\tilde{Z}\_{t\_{i}}|^{2}dt+h\mathbb{E}|\tilde{Z}\_{t\_{i}}-\hat{Z}\_{t\_{i}}|^{2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§C‚Äãùîº‚Äã‚à´titi+1|Zt‚àíZ~ti|2‚Äãùëët+Ch‚Äã((I02+ùîº‚Äã|x0|2)‚Äãh3+h2‚Äãùîº‚Äã[‚à´titi+1|Zt|2‚Äãùëët+‚à´titi+1|f|2‚Äã(t,Vt,0,0,0)‚Äãùëët])\displaystyle\leq C\mathbb{E}\int\_{t\_{i}}^{t\_{i+1}}|Z\_{t}-\tilde{Z}\_{t\_{i}}|^{2}dt+\frac{C}{h}\left(\big(I\_{0}^{2}+\mathbb{E}|x\_{0}|^{2}\big)h^{3}+h^{2}\mathbb{E}\bigg[\int\_{t\_{i}}^{t\_{i+1}}|Z\_{t}|^{2}dt+\int\_{t\_{i}}^{t\_{i+1}}|f|^{2}(t,V\_{t},0,0,0)dt\bigg]\right) |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | ‚â§C‚Äã(I02+ùîº‚Äã|x0|2)‚Äãh2+C‚Äãùîº‚Äã‚à´titi+1|Zt‚àíZ~ti|2‚Äãùëët+C‚Äãh‚Äãùîº‚Äã[‚à´titi+1|Zt|2‚Äãùëët+‚à´titi+1|f|2‚Äã(t,Vt,0,0,0)‚Äãùëët].\displaystyle\leq C\big(I\_{0}^{2}+\mathbb{E}|x\_{0}|^{2}\big)h^{2}+C\mathbb{E}\int\_{t\_{i}}^{t\_{i+1}}|Z\_{t}-\tilde{Z}\_{t\_{i}}|^{2}dt+Ch\mathbb{E}\bigg[\int\_{t\_{i}}^{t\_{i+1}}|Z\_{t}|^{2}dt+\int\_{t\_{i}}^{t\_{i+1}}|f|^{2}(t,V\_{t},0,0,0)dt\bigg]. |  | (C.4) |

Then, using estimate from Theorem [2.3](https://arxiv.org/html/2511.08735v1#S2.Thmthm3 "Theorem 2.3 (Continuity of Solution). ‚Ä£ 2.2 Solutions to FBSDEs ‚Ä£ 2 Preliminaries ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications") and equation ([C](https://arxiv.org/html/2511.08735v1#A3.Ex14 "Appendix C Proof of Theorem 2.6 ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) into ([C](https://arxiv.org/html/2511.08735v1#A3.Ex2 "Appendix C Proof of Theorem 2.6 ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) we have,

|  |  |  |
| --- | --- | --- |
|  | ùîº‚Äã[|Xti+1‚àíX¬Øti+1œÄ|2]\displaystyle\mathbb{E}\Big[|X\_{t\_{i+1}}-\overline{X}\_{t\_{i+1}}^{\pi}|^{2}\Big] |  |
|  |  |  |
| --- | --- | --- |
|  | ‚â§(1+A1‚Äãh)‚Äãùîº‚Äã|Xti‚àíX¬ØtiœÄ|2+A2‚Äãh‚Äãùîº‚Äã|Yti‚àíY¬ØtiœÄ|2+A3‚Äãh‚Äãùîº‚Äã|Z^ti‚àíZ¬ØtiœÄ|2\displaystyle\leq\hskip 5.69054pt(1+A\_{1}h)\mathbb{E}|X\_{t\_{i}}-\overline{X}\_{t\_{i}}^{\pi}|^{2}+A\_{2}h\mathbb{E}|Y\_{t\_{i}}-\overline{Y}\_{t\_{i}}^{\pi}|^{2}+A\_{3}h\mathbb{E}|\hat{Z}\_{t\_{i}}-\overline{Z}\_{t\_{i}}^{\pi}|^{2} |  |
|  |  |  |
| --- | --- | --- |
|  | +C‚Äã((I02+ùîº‚Äã|x0|2)‚Äãh+œÅ‚Äã(h))‚Äãh+C‚Äãùîº‚Äã‚à´titi+1|Zt‚àíZ~ti|2‚Äãùëët\displaystyle+C\Big(\big(I\_{0}^{2}+\mathbb{E}|x\_{0}|^{2}\big)h+\rho(h)\Big)h+C\mathbb{E}\int\_{t\_{i}}^{t\_{i+1}}|Z\_{t}-\tilde{Z}\_{t\_{i}}|^{2}dt |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | +C‚Äãh‚Äãùîº‚Äã[‚à´titi+1|Zt|2‚Äãùëët+‚à´titi+1(|f|2+|œÉ|2)‚Äã(t,Vt,0,0,0)‚Äãùëët].\displaystyle+Ch\mathbb{E}\bigg[\int\_{t\_{i}}^{t\_{i+1}}|Z\_{t}|^{2}dt+\int\_{t\_{i}}^{t\_{i+1}}\big(|f|^{2}+|\sigma|^{2}\big)(t,V\_{t},0,0,0)dt\bigg]. |  | (C.5) |

Finally, by induction we obtain that, for 1‚â§n‚â§N1\leq n\leq N,

|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ùîº‚Äã|Xtn‚àíX¬ØtnœÄ|2\displaystyle\mathbb{E}|X\_{t\_{n}}-\overline{X}\_{t\_{n}}^{\pi}|^{2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§‚àëi=0n‚àí1eA1‚Äãh‚Äã(n‚àíi‚àí1)‚ÄãA2‚Äãh‚Äãùîº‚Äã|Yti‚àíY¬ØtiœÄ|2+‚àëi=0n‚àí1eA1‚Äãh‚Äã(n‚àíi‚àí1)‚ÄãA3‚Äãh‚Äãùîº‚Äã|Z^ti‚àíZ¬ØtiœÄ|2\displaystyle\leq\hskip 5.69054pt\sum\_{i=0}^{n-1}e^{A\_{1}h(n-i-1)}A\_{2}h\mathbb{E}|Y\_{t\_{i}}-\overline{Y}\_{t\_{i}}^{\pi}|^{2}+\sum\_{i=0}^{n-1}e^{A\_{1}h(n-i-1)}A\_{3}h\mathbb{E}|\hat{Z}\_{t\_{i}}-\overline{Z}\_{t\_{i}}^{\pi}|^{2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +C‚àëk=0n‚àí1eA1‚Äãh‚Äã(n‚àík‚àí1)(((I02+ùîº|x0|2)h+œÅ(h))h+sup0‚â§i‚â§nùîº‚à´titi+1|Zt‚àíZ~ti|2dt\displaystyle+C\sum\_{k=0}^{n-1}e^{A\_{1}h(n-k-1)}\bigg(\Big(\big(I\_{0}^{2}+\mathbb{E}|x\_{0}|^{2}\big)h+\rho(h)\Big)h+\sup\_{0\leq i\leq n}\mathbb{E}\int\_{t\_{i}}^{t\_{i+1}}|Z\_{t}-\tilde{Z}\_{t\_{i}}|^{2}dt |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | +hsup0‚â§i‚â§nùîº[‚à´titi+1|Zt|2dt+‚à´titi+1(|f|2+|œÉ|2)(t,Vt,0,0,0)dt]).\displaystyle+h\sup\_{0\leq i\leq n}\mathbb{E}\bigg[\int\_{t\_{i}}^{t\_{i+1}}|Z\_{t}|^{2}dt+\int\_{t\_{i}}^{t\_{i+1}}\big(|f|^{2}+|\sigma|^{2}\big)(t,V\_{t},0,0,0)dt\bigg]\bigg). |  | (C.6) |

Step 2: From the backward equation of ([1.1](https://arxiv.org/html/2511.08735v1#S1.E1 "In 1 Introduction ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) we can have

|  |  |  |  |
| --- | --- | --- | --- |
|  | Yti=\displaystyle Y\_{t\_{i}}= | Yti+1+f‚Äã(ti,Vti,Xti,Yti,Z^ti)‚Äãh‚àí‚à´titi+1Zt‚ÄãùëëWt\displaystyle Y\_{t\_{i+1}}+f(t\_{i},V\_{t\_{i}},X\_{t\_{i}},Y\_{t\_{i}},\hat{Z}\_{t\_{i}})h-\int\_{t\_{i}}^{t\_{i+1}}Z\_{t}dW\_{t} |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | +‚à´titi+1(f‚Äã(t,Vt,Xt,Yt,Zt)‚àíf‚Äã(ti,Vti,Xti,Yti,Z^ti))‚Äãùëët,\displaystyle+\int\_{t\_{i}}^{t\_{i+1}}\Big(f(t,V\_{t},X\_{t},Y\_{t},Z\_{t})-f(t\_{i},V\_{t\_{i}},X\_{t\_{i}},Y\_{t\_{i}},\hat{Z}\_{t\_{i}})\Big)dt, |  | (C.7) |

with Z^ti=1h‚Äãùîºti‚Äã[Yti+1‚ÄãŒî‚ÄãWti]\hat{Z}\_{t\_{i}}=\frac{1}{h}\mathbb{E}\_{t\_{i}}\big[Y\_{t\_{i+1}}\Delta W\_{t\_{i}}\big].
And from ([2.23](https://arxiv.org/html/2511.08735v1#S2.E23 "In 2.2 Solutions to FBSDEs ‚Ä£ 2 Preliminaries ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) we have,

|  |  |  |
| --- | --- | --- |
|  | Y¬ØtiœÄ=ùîº‚Äã[Y¬Øti+1œÄ+f‚Äã(ti,Vti,X¬ØtiœÄ,Y¬ØtiœÄ,Z¬ØtiœÄ)‚Äãh|‚Ñ±ti].\displaystyle\overline{Y}\_{t\_{i}}^{\pi}=\mathbb{E}\bigg[\overline{Y}\_{t\_{i+1}}^{\pi}+f\big(t\_{i},V\_{t\_{i}},\overline{X}\_{t\_{i}}^{\pi},\overline{Y}\_{t\_{i}}^{\pi},\overline{Z}\_{t\_{i}}^{\pi}\big)h\big|\mathcal{F}\_{t\_{i}}\bigg]. |  |

By the martingale representation theorem there exists an ‚Ñ±t\mathcal{F}\_{t}-adapted square-integrable process {Z¬Øt}ti‚â§t‚â§ti+1\{\bar{Z}\_{t}\}\_{t\_{i}\leq t\leq t\_{i+1}} such that

|  |  |  |  |
| --- | --- | --- | --- |
|  | Y¬Øti+1œÄ\displaystyle\overline{Y}\_{t\_{i+1}}^{\pi} | =ùîºti‚Äã[Y¬Øti+1œÄ]+‚à´titi+1Z¬Øt‚ÄãùëëWt=Y¬ØtiœÄ‚àíf‚Äã(ti,Vti,X¬ØtiœÄ,Y¬ØtiœÄ,Z¬ØtiœÄ)‚Äãh+‚à´titi+1Z¬Øt‚ÄãùëëWt,\displaystyle=\mathbb{E}\_{t\_{i}}[\overline{Y}\_{t\_{i+1}}^{\pi}]+\int\_{t\_{i}}^{t\_{i+1}}\bar{Z}\_{t}dW\_{t}=\overline{Y}\_{t\_{i}}^{\pi}-f\big(t\_{i},V\_{t\_{i}},\overline{X}\_{t\_{i}}^{\pi},\overline{Y}\_{t\_{i}}^{\pi},\overline{Z}\_{t\_{i}}^{\pi}\big)h+\int\_{t\_{i}}^{t\_{i+1}}\bar{Z}\_{t}dW\_{t}, |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | ‚üπY¬ØtiœÄ\displaystyle\implies\overline{Y}\_{t\_{i}}^{\pi} | =Y¬Øti+1œÄ+f‚Äã(ti,Vti,X¬ØtiœÄ,Y¬ØtiœÄ,Z¬ØtiœÄ)‚Äãh‚àí‚à´titi+1Z¬Øt‚ÄãùëëWt.\displaystyle=\overline{Y}\_{t\_{i+1}}^{\pi}+f\big(t\_{i},V\_{t\_{i}},\overline{X}\_{t\_{i}}^{\pi},\overline{Y}\_{t\_{i}}^{\pi},\overline{Z}\_{t\_{i}}^{\pi}\big)h-\int\_{t\_{i}}^{t\_{i+1}}\bar{Z}\_{t}dW\_{t}. |  | (C.8) |

Now applying Lemma [2.5](https://arxiv.org/html/2511.08735v1#S2.Thmthm5 "Lemma 2.5. ‚Ä£ 2.2 Solutions to FBSDEs ‚Ä£ 2 Preliminaries ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications") on YY and Y¬ØœÄ\overline{Y}^{\pi} from ([C](https://arxiv.org/html/2511.08735v1#A3.Ex23 "Appendix C Proof of Theorem 2.6 ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) and ([C](https://arxiv.org/html/2511.08735v1#A3.Ex25 "Appendix C Proof of Theorem 2.6 ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) respectively, we have

|  |  |  |
| --- | --- | --- |
|  | (1‚àíA6‚Äãh)‚Äãùîº‚Äã|Yti‚àíY¬ØtiœÄ|2+A7‚Äãh‚Äãùîº‚Äã|Z^ti‚àíZ¬ØtiœÄ|2\displaystyle(1-A\_{6}h)\mathbb{E}|Y\_{t\_{i}}-\overline{Y}\_{t\_{i}}^{\pi}|^{2}+A\_{7}h\mathbb{E}|\hat{Z}\_{t\_{i}}-\overline{Z}\_{t\_{i}}^{\pi}|^{2} |  |
|  |  |  |
| --- | --- | --- |
|  | ‚â§eA4‚Äãh‚Äãùîº‚Äã|Yti+1‚àíY¬Øti+1œÄ|2+A5‚Äãh‚Äãùîº‚Äã|Xti‚àíX¬ØtiœÄ|2\displaystyle\leq e^{A\_{4}h}\mathbb{E}|Y\_{t\_{i+1}}-\overline{Y}\_{t\_{i+1}}^{\pi}|^{2}+A\_{5}h\mathbb{E}|X\_{t\_{i}}-\overline{X}\_{t\_{i}}^{\pi}|^{2} |  |
|  |  |  |
| --- | --- | --- |
|  | +(1+1Œª4‚Äãh+1Œª3)‚Äãùîº‚Äã|‚à´titi+1(f‚Äã(t,Vt,Xt,Yt,Zt)‚àíf‚Äã(ti,Vti,Xti,Yti,Z^ti))‚Äãùëët|2\displaystyle+\left(1+\frac{1}{\lambda\_{4}h}+\frac{1}{\lambda\_{3}}\right)\mathbb{E}\bigg|\int\_{t\_{i}}^{t\_{i+1}}\Big(f(t,V\_{t},X\_{t},Y\_{t},Z\_{t})-f(t\_{i},V\_{t\_{i}},X\_{t\_{i}},Y\_{t\_{i}},\hat{Z}\_{t\_{i}})\Big)dt\bigg|^{2} |  |
|  |  |  |
| --- | --- | --- |
|  | ‚â§eA4‚Äãh‚Äãùîº‚Äã|Yti+1‚àíY¬Øti+1œÄ|2+A5‚Äãh‚Äãùîº‚Äã|Xti‚àíX¬ØtiœÄ|2\displaystyle\leq e^{A\_{4}h}\mathbb{E}|Y\_{t\_{i+1}}-\overline{Y}\_{t\_{i+1}}^{\pi}|^{2}+A\_{5}h\mathbb{E}|X\_{t\_{i}}-\overline{X}\_{t\_{i}}^{\pi}|^{2} |  |
|  |  |  |
| --- | --- | --- |
|  | +(1+1Œª4‚Äãh+1Œª3)‚Äãh‚Äãùîº‚Äã‚à´titi+1|f‚Äã(t,Vt,Xt,Yt,Zt)‚àíf‚Äã(ti,Vti,Xti,Yti,Z^ti)|2‚Äãùëët\displaystyle+\left(1+\frac{1}{\lambda\_{4}h}+\frac{1}{\lambda\_{3}}\right)h\mathbb{E}\int\_{t\_{i}}^{t\_{i+1}}\big|f(t,V\_{t},X\_{t},Y\_{t},Z\_{t})-f(t\_{i},V\_{t\_{i}},X\_{t\_{i}},Y\_{t\_{i}},\hat{Z}\_{t\_{i}})\big|^{2}dt |  |
|  |  |  |
| --- | --- | --- |
|  | ‚â§eA4‚Äãh‚Äãùîº‚Äã|Yti+1‚àíY¬Øti+1œÄ|2+A5‚Äãh‚Äãùîº‚Äã|Xti‚àíX¬ØtiœÄ|2\displaystyle\leq e^{A\_{4}h}\mathbb{E}|Y\_{t\_{i+1}}-\overline{Y}\_{t\_{i+1}}^{\pi}|^{2}+A\_{5}h\mathbb{E}|X\_{t\_{i}}-\overline{X}\_{t\_{i}}^{\pi}|^{2} |  |
|  |  |  |
| --- | --- | --- |
|  | +C‚Äãùîº‚Äã‚à´titi+1(œÅ‚Äã(h)+|Xt‚àíXti|2+|Yt‚àíYti|2+|Zt‚àíZ^ti|2)‚Äãùëët\displaystyle+C\mathbb{E}\int\_{t\_{i}}^{t\_{i+1}}\Big(\rho(h)+|X\_{t}-X\_{t\_{i}}|^{2}+|Y\_{t}-Y\_{t\_{i}}|^{2}+|Z\_{t}-\hat{Z}\_{t\_{i}}|^{2}\Big)dt |  |
|  |  |  |
| --- | --- | --- |
|  | ‚â§eA4‚Äãh‚Äãùîº‚Äã|Yti+1‚àíY¬Øti+1œÄ|2+A5‚Äãh‚Äãùîº‚Äã|Xti‚àíX¬ØtiœÄ|2+C‚Äã((I02+ùîº‚Äã|x0|2)‚Äãh+œÅ‚Äã(h))‚Äãh\displaystyle\leq e^{A\_{4}h}\mathbb{E}|Y\_{t\_{i+1}}-\overline{Y}\_{t\_{i+1}}^{\pi}|^{2}+A\_{5}h\mathbb{E}|X\_{t\_{i}}-\overline{X}\_{t\_{i}}^{\pi}|^{2}+C\Big(\big(I\_{0}^{2}+\mathbb{E}|x\_{0}|^{2}\big)h+\rho(h)\Big)h |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | +C‚Äãùîº‚Äã‚à´titi+1|Zt‚àíZ^ti|2‚Äãùëët+C‚Äãh‚Äãùîº‚Äã[‚à´titi+1|Zt|2‚Äãùëët+‚à´titi+1|œÉ|2‚Äã(t,Vt,0,0,0)‚Äãùëët].\displaystyle+C\mathbb{E}\int\_{t\_{i}}^{t\_{i+1}}|Z\_{t}-\hat{Z}\_{t\_{i}}|^{2}dt+Ch\mathbb{E}\bigg[\int\_{t\_{i}}^{t\_{i+1}}|Z\_{t}|^{2}dt+\int\_{t\_{i}}^{t\_{i+1}}|\sigma|^{2}(t,V\_{t},0,0,0)dt\bigg]. |  | (C.9) |

Now using estimate from ([C](https://arxiv.org/html/2511.08735v1#A3.Ex14 "Appendix C Proof of Theorem 2.6 ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) we further have

|  |  |  |
| --- | --- | --- |
|  | (1‚àíA6‚Äãh)‚Äãùîº‚Äã|Yti‚àíY¬ØtiœÄ|2+A7‚Äãh‚Äãùîº‚Äã|Z^ti‚àíZ¬ØtiœÄ|2\displaystyle(1-A\_{6}h)\mathbb{E}|Y\_{t\_{i}}-\overline{Y}\_{t\_{i}}^{\pi}|^{2}+A\_{7}h\mathbb{E}|\hat{Z}\_{t\_{i}}-\overline{Z}\_{t\_{i}}^{\pi}|^{2} |  |
|  |  |  |
| --- | --- | --- |
|  | ‚â§eA4‚Äãh‚Äãùîº‚Äã|Yti+1‚àíY¬Øti+1œÄ|2+A5‚Äãh‚Äãùîº‚Äã|Xti‚àíX¬ØtiœÄ|2+C‚Äã((I02+ùîº‚Äã|x0|2)‚Äãh+œÅ‚Äã(h))‚Äãh\displaystyle\leq e^{A\_{4}h}\mathbb{E}|Y\_{t\_{i+1}}-\overline{Y}\_{t\_{i+1}}^{\pi}|^{2}+A\_{5}h\mathbb{E}|X\_{t\_{i}}-\overline{X}\_{t\_{i}}^{\pi}|^{2}+C\Big(\big(I\_{0}^{2}+\mathbb{E}|x\_{0}|^{2}\big)h+\rho(h)\Big)h |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | +C‚Äãùîº‚Äã‚à´titi+1|Zt‚àíZ~ti|2‚Äãùëët+C‚Äãh‚Äãùîº‚Äã[‚à´titi+1|Zt|2‚Äãùëët+‚à´titi+1(|f|2+|œÉ|2)‚Äã(t,Vt,0,0,0)‚Äãùëët].\displaystyle+C\mathbb{E}\int\_{t\_{i}}^{t\_{i+1}}|Z\_{t}-\tilde{Z}\_{t\_{i}}|^{2}dt+Ch\mathbb{E}\bigg[\int\_{t\_{i}}^{t\_{i+1}}|Z\_{t}|^{2}dt+\int\_{t\_{i}}^{t\_{i+1}}\big(|f|^{2}+|\sigma|^{2}\big)(t,V\_{t},0,0,0)dt\bigg]. |  | (C.10) |

Then, by induction we obtain that, for 0‚â§n‚â§N‚àí10\leq n\leq N-1,

|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ùîº‚Äã|Ytn‚àíY¬ØtnœÄ|2+‚àëi=nN‚àí1eA4‚Äã(i‚àín)‚Äãh‚ÄãA7‚Äãh‚Äã(1‚àíA6‚Äãh)‚àí(i+1)‚Äãùîº‚Äã|Z^ti‚àíZ¬ØtiœÄ|2\displaystyle\mathbb{E}|Y\_{t\_{n}}-\overline{Y}\_{t\_{n}}^{\pi}|^{2}+\sum\_{i=n}^{N-1}e^{A\_{4}(i-n)h}A\_{7}h(1-A\_{6}h)^{-(i+1)}\mathbb{E}|\hat{Z}\_{t\_{i}}-\overline{Z}\_{t\_{i}}^{\pi}|^{2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§eA4‚Äã(N‚àín)‚Äãh‚Äã(1‚àíA6‚Äãh)‚àíN‚Äãùîº‚Äã|YtN‚àíY¬ØtNœÄ|2+‚àëi=nN‚àí1eA4‚Äã(i‚àín)‚Äãh‚ÄãA5‚Äãh‚Äã(1‚àíA6‚Äãh)‚àí(i+1)‚Äãùîº‚Äã|Xti‚àíX¬ØtiœÄ|2\displaystyle\leq e^{A\_{4}(N-n)h}(1-A\_{6}h)^{-N}\mathbb{E}|Y\_{t\_{N}}-\overline{Y}\_{t\_{N}}^{\pi}|^{2}+\sum\_{i=n}^{N-1}e^{A\_{4}(i-n)h}A\_{5}h(1-A\_{6}h)^{-(i+1)}\mathbb{E}|X\_{t\_{i}}-\overline{X}\_{t\_{i}}^{\pi}|^{2} |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | +C‚Äã‚àëi=nN‚àí1eA4‚Äã(i‚àín)‚Äãh‚Äã(1‚àíA6‚Äãh)‚àí(i+1)‚Äãh‚ÄãMnN‚àí1,\displaystyle+C\sum\_{i=n}^{N-1}e^{A\_{4}(i-n)h}(1-A\_{6}h)^{-(i+1)}hM\_{n}^{N-1}, |  | (C.11) |

where,

|  |  |  |  |
| --- | --- | --- | --- |
|  | MnN‚àí1=\displaystyle M\_{n}^{N-1}= | (I02+ùîº‚Äã|x0|2)‚Äãh+œÅ‚Äã(h)+1h‚Äãsupn‚â§k‚â§N‚àí1ùîº‚Äã‚à´tktk+1|Zt‚àíZ~tk|2‚Äãùëët\displaystyle\big(I\_{0}^{2}+\mathbb{E}|x\_{0}|^{2}\big)h+\rho(h)+\frac{1}{h}\sup\_{n\leq k\leq N-1}\mathbb{E}\int\_{t\_{k}}^{t\_{k+1}}|Z\_{t}-\tilde{Z}\_{t\_{k}}|^{2}dt |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | +supn‚â§k‚â§N‚àí1ùîº‚Äã[‚à´tktk+1|Zt|2‚Äãùëët+‚à´tktk+1(|f|2+|œÉ|2)‚Äã(t,Vt,0,0,0)‚Äãùëët].\displaystyle+\sup\_{n\leq k\leq N-1}\mathbb{E}\bigg[\int\_{t\_{k}}^{t\_{k+1}}|Z\_{t}|^{2}dt+\int\_{t\_{k}}^{t\_{k+1}}\big(|f|^{2}+|\sigma|^{2}\big)(t,V\_{t},0,0,0)dt\bigg]. |  | (C.12) |

Step 3:  Note that ùîº‚Äã|YtN‚àíY¬ØtNœÄ|2=ùîº‚Äã|g‚Äã(VT,XT)‚àíg‚Äã(VT,X¬ØTœÄ)|2‚â§gx‚Äã|XT‚àíX¬ØTœÄ|2\mathbb{E}|Y\_{t\_{N}}-\overline{Y}\_{t\_{N}}^{\pi}|^{2}=\mathbb{E}|g(V\_{T},X\_{T})-g(V\_{T},\overline{X}^{\pi}\_{T})|^{2}\leq g\_{x}|X\_{T}-\overline{X}\_{T}^{\pi}|^{2}.
  
Now first denote

|  |  |  |
| --- | --- | --- |
|  | Œî‚ÄãX:=X‚àíX¬ØœÄ,Œî‚ÄãY:=Y‚àíY¬ØœÄ,Œî‚ÄãZ:=Z^‚àíZ¬ØœÄ,\Delta X:=X-\overline{X}^{\pi},\hskip 5.69054pt\Delta Y:=Y-\overline{Y}^{\pi},\hskip 5.69054pt\Delta Z:=\hat{Z}-\overline{Z}^{\pi}, |  |

and then

|  |  |  |  |
| --- | --- | --- | --- |
|  | P\displaystyle P | :=max0‚â§n‚â§N‚Å°e‚àíA1‚Äãn‚Äãh‚Äãùîº‚Äã|Œî‚ÄãXtn|2,\displaystyle:=\max\_{0\leq n\leq N}e^{-A\_{1}nh}\hskip 5.69054pt\mathbb{E}|\Delta X\_{t\_{n}}|^{2}, |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | S\displaystyle S | :=max‚Å°{max0‚â§n‚â§N‚Å°eA4‚Äãn‚Äãh‚Äãùîº‚Äã|Œî‚ÄãYtn|2,‚àëi=0N‚àí1eA4‚Äãi‚Äãh‚ÄãA7‚Äãh‚Äã(1‚àíA6‚Äãh)‚àí(i+1)‚Äãùîº‚Äã|Œî‚ÄãZti|2}.\displaystyle:=\max\bigg\{\max\_{0\leq n\leq N}e^{A\_{4}nh}\hskip 5.69054pt\mathbb{E}|\Delta Y\_{t\_{n}}|^{2},\sum\_{i=0}^{N-1}e^{A\_{4}ih}A\_{7}h(1-A\_{6}h)^{-(i+1)}\hskip 5.69054pt\mathbb{E}|\Delta Z\_{t\_{i}}|^{2}\bigg\}. |  |

Then from ([C](https://arxiv.org/html/2511.08735v1#A3.Ex20 "Appendix C Proof of Theorem 2.6 ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")), we have for 1‚â§n‚â§N1\leq n\leq N,

|  |  |  |  |
| --- | --- | --- | --- |
|  | e‚àíA1‚Äãn‚Äãh‚Äãùîº‚Äã|Œî‚ÄãXtn|2\displaystyle e^{-A\_{1}nh}\mathbb{E}|\Delta X\_{t\_{n}}|^{2} | ‚â§‚àëi=0n‚àí1e‚àíA1‚Äãh‚Äã(i+1)‚ÄãA2‚Äãh‚Äãùîº‚Äã|Œî‚ÄãYti|2+‚àëi=0n‚àí1e‚àíA1‚Äãh‚Äã(i+1)‚ÄãA3‚Äãh‚Äãùîº‚Äã|Œî‚ÄãZti|2+C‚Äã‚àëi=0n‚àí1e‚àíA1‚Äãh‚Äã(i+1)‚Äãh‚ÄãM0n\displaystyle\leq\hskip 5.69054pt\sum\_{i=0}^{n-1}e^{-A\_{1}h(i+1)}A\_{2}h\mathbb{E}|\Delta Y\_{t\_{i}}|^{2}+\sum\_{i=0}^{n-1}e^{-A\_{1}h(i+1)}A\_{3}h\mathbb{E}|\Delta Z\_{t\_{i}}|^{2}+C\sum\_{i=0}^{n-1}e^{-A\_{1}h(i+1)}hM\_{0}^{n} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§(‚àëi=0n‚àí1e‚àíA1‚Äã(i+1)‚Äãh‚àíA4‚Äãi‚Äãh‚ÄãA2‚Äãh+A3A7‚Äãe‚àíA1‚Äãh)‚ÄãS+C‚Äã‚àëi=0n‚àí1e‚àíA1‚Äãh‚Äã(i+1)‚Äãh‚ÄãM0n\displaystyle\leq\bigg(\sum\_{i=0}^{n-1}e^{-A\_{1}(i+1)h-A\_{4}ih}A\_{2}h+\frac{A\_{3}}{A\_{7}}e^{-A\_{1}h}\bigg)S+C\sum\_{i=0}^{n-1}e^{-A\_{1}h(i+1)}hM\_{0}^{n} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ‚â§(e‚àíA1‚Äãh‚ÄãA2‚Äãh‚Äã1‚àíe‚àí(A1+A4)‚ÄãT1‚àíe‚àí(A1+A4)‚Äãh+A3A7‚Äãe‚àíA1‚Äãh)‚ÄãS+C‚Äã‚àëi=0n‚àí1e‚àíA1‚Äãh‚Äã(i+1)‚Äãh‚ÄãM0n\displaystyle\leq\bigg(e^{-A\_{1}h}A\_{2}h\frac{1-e^{-(A\_{1}+A\_{4})T}}{1-e^{-(A\_{1}+A\_{4})h}}+\frac{A\_{3}}{A\_{7}}e^{-A\_{1}h}\bigg)S+C\sum\_{i=0}^{n-1}e^{-A\_{1}h(i+1)}hM\_{0}^{n} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =A~1‚Äã(h)‚ÄãS+C‚Äã‚àëi=0n‚àí1e‚àíA1‚Äãh‚Äã(i+1)‚Äãh‚ÄãM0n,\displaystyle=\tilde{A}\_{1}(h)S+C\sum\_{i=0}^{n-1}e^{-A\_{1}h(i+1)}hM\_{0}^{n}, |  |

where

|  |  |  |  |
| --- | --- | --- | --- |
|  | M0n=\displaystyle M\_{0}^{n}= | (I02+ùîº‚Äã|x0|2)‚Äãh+œÅ‚Äã(h)+1h‚Äãsup0‚â§k‚â§nùîº‚Äã‚à´tktk+1|Zt‚àíZ~tk|2‚Äãùëët\displaystyle\big(I\_{0}^{2}+\mathbb{E}|x\_{0}|^{2}\big)h+\rho(h)+\frac{1}{h}\sup\_{0\leq k\leq n}\mathbb{E}\int\_{t\_{k}}^{t\_{k+1}}|Z\_{t}-\tilde{Z}\_{t\_{k}}|^{2}dt |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | +sup0‚â§k‚â§nùîº‚Äã[‚à´tktk+1|Zt|2‚Äãùëët+‚à´tktk+1(|f|2+|œÉ|2)‚Äã(t,Vt,0,0,0)‚Äãùëët],\displaystyle+\sup\_{0\leq k\leq n}\mathbb{E}\bigg[\int\_{t\_{k}}^{t\_{k+1}}|Z\_{t}|^{2}dt+\int\_{t\_{k}}^{t\_{k+1}}\big(|f|^{2}+|\sigma|^{2}\big)(t,V\_{t},0,0,0)dt\bigg], |  | (C.13) |

and

|  |  |  |  |
| --- | --- | --- | --- |
|  | A~1‚Äã(h)=e‚àíA1‚Äãh‚ÄãA2‚Äãh‚Äã1‚àíe‚àí(A1+A4)‚ÄãT1‚àíe‚àí(A1+A4)‚Äãh+A3A7‚Äãe‚àíA1‚Äãh.\tilde{A}\_{1}(h)=e^{-A\_{1}h}A\_{2}h\frac{1-e^{-(A\_{1}+A\_{4})T}}{1-e^{-(A\_{1}+A\_{4})h}}+\frac{A\_{3}}{A\_{7}}e^{-A\_{1}h}. |  | (C.14) |

Thus we have,

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | P\displaystyle P | ‚â§A~1‚Äã(h)‚ÄãS+C‚Äã‚àëi=0N‚àí1e‚àíA1‚Äãh‚Äã(i+1)‚Äãh‚ÄãM0N‚àí1.\displaystyle\leq\tilde{A}\_{1}(h)S+C\sum\_{i=0}^{N-1}e^{-A\_{1}h(i+1)}hM\_{0}^{N-1}. |  | (C.15) |

Similarly from ([C](https://arxiv.org/html/2511.08735v1#A3.Ex36 "Appendix C Proof of Theorem 2.6 ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")), we have for 0‚â§n‚â§N‚àí10\leq n\leq N-1,

|  |  |  |
| --- | --- | --- |
|  | eA4‚Äãn‚Äãh‚Äãùîº‚Äã|Œî‚ÄãYtn|2+‚àëi=nN‚àí1eA4‚Äãi‚Äãh‚ÄãA7‚Äãh‚Äã(1‚àíA6‚Äãh)‚àí(i+1)‚Äãùîº‚Äã|Œî‚ÄãZti|2\displaystyle e^{A\_{4}nh}\mathbb{E}|\Delta Y\_{t\_{n}}|^{2}+\sum\_{i=n}^{N-1}e^{A\_{4}ih}A\_{7}h(1-A\_{6}h)^{-(i+1)}\mathbb{E}|\Delta Z\_{t\_{i}}|^{2} |  |
|  |  |  |
| --- | --- | --- |
|  | ‚â§eA4‚ÄãN‚Äãh‚Äã(1‚àíA6‚Äãh)‚àíN‚Äãùîº‚Äã|Œî‚ÄãYtN|2+‚àëi=nN‚àí1(eA4‚Äãi‚Äãh‚ÄãA5‚Äãh‚Äãùîº‚Äã|Œî‚ÄãXti|2+C‚ÄãeA4‚Äãi‚Äãh‚Äãh‚ÄãMnN‚àí1)‚Äã(1‚àíA6‚Äãh)‚àí(i+1)\displaystyle\leq e^{A\_{4}Nh}(1-A\_{6}h)^{-N}\mathbb{E}|\Delta Y\_{t\_{N}}|^{2}+\sum\_{i=n}^{N-1}\bigg(e^{A\_{4}ih}A\_{5}h\mathbb{E}|\Delta X\_{t\_{i}}|^{2}+Ce^{A\_{4}ih}hM\_{n}^{N-1}\bigg)(1-A\_{6}h)^{-(i+1)} |  |
|  |  |  |
| --- | --- | --- |
|  | ‚â§eA4‚ÄãT‚Äã(1‚àíA6‚Äãh)‚àíN‚Äãgx‚Äãùîº‚Äã|Œî‚ÄãXtN|2+‚àëi=nN‚àí1(eA4‚Äãi‚Äãh‚ÄãA5‚Äãh‚Äãùîº‚Äã|Œî‚ÄãXti|2+C‚ÄãeA4‚Äãi‚Äãh‚Äãh‚ÄãMnN‚àí1)‚Äã(1‚àíA6‚Äãh)‚àí(i+1)\displaystyle\leq e^{A\_{4}T}(1-A\_{6}h)^{-N}g\_{x}\mathbb{E}|\Delta X\_{t\_{N}}|^{2}+\sum\_{i=n}^{N-1}\bigg(e^{A\_{4}ih}A\_{5}h\mathbb{E}|\Delta X\_{t\_{i}}|^{2}+Ce^{A\_{4}ih}hM\_{n}^{N-1}\bigg)(1-A\_{6}h)^{-(i+1)} |  |
|  |  |  |
| --- | --- | --- |
|  | ‚â§eA4‚ÄãT+A1‚ÄãT‚Äã(1‚àíA6‚Äãh)‚àíN‚Äãgx‚Äãe‚àíA1‚ÄãT‚Äãùîº‚Äã|Œî‚ÄãXtN|2\displaystyle\leq e^{A\_{4}T+A\_{1}T}(1-A\_{6}h)^{-N}g\_{x}e^{-A\_{1}T}\mathbb{E}|\Delta X\_{t\_{N}}|^{2} |  |
|  |  |  |
| --- | --- | --- |
|  | +‚àëi=nN‚àí1(e(A4+A1)‚Äãi‚Äãh‚ÄãA5‚Äãh‚Äãe‚àíA1‚Äãi‚Äãh‚Äãùîº‚Äã|Œî‚ÄãXti|2+C‚ÄãeA4‚Äãi‚Äãh‚Äãh‚ÄãMnN‚àí1)‚Äã(1‚àíA6‚Äãh)‚àí(i+1)\displaystyle\quad+\sum\_{i=n}^{N-1}\bigg(e^{(A\_{4}+A\_{1})ih}A\_{5}he^{-A\_{1}ih}\mathbb{E}|\Delta X\_{t\_{i}}|^{2}+Ce^{A\_{4}ih}hM\_{n}^{N-1}\bigg)(1-A\_{6}h)^{-(i+1)} |  |
|  |  |  |
| --- | --- | --- |
|  | ‚â§(eA4‚ÄãT+A1‚ÄãT‚Äã(1‚àíA6‚Äãh)‚àíN‚Äãgx+‚àëi=nN‚àí1e(A4+A1)‚Äãi‚Äãh‚ÄãA5‚Äãh‚Äã(1‚àíA6‚Äãh)‚àí(i+1))‚ÄãP\displaystyle\leq\bigg(e^{A\_{4}T+A\_{1}T}(1-A\_{6}h)^{-N}g\_{x}+\sum\_{i=n}^{N-1}e^{(A\_{4}+A\_{1})ih}A\_{5}h(1-A\_{6}h)^{-(i+1)}\bigg)P |  |
|  |  |  |
| --- | --- | --- |
|  | +C‚Äã‚àëi=nN‚àí1eA4‚Äãi‚Äãh‚Äã(1‚àíA6‚Äãh)‚àí(i+1)‚Äãh‚ÄãMnN‚àí1\displaystyle\quad+C\sum\_{i=n}^{N-1}e^{A\_{4}ih}(1-A\_{6}h)^{-(i+1)}hM\_{n}^{N-1} |  |
|  |  |  |
| --- | --- | --- |
|  | ‚â§(e(A4+A1)‚ÄãT‚Äã(1‚àíA6‚Äãh)‚àíN‚Äãgx+A5‚Äãh‚Äã(1‚àíA6‚Äãh)‚àíN‚Äãe(A4+A1)‚ÄãT‚àí1e(A4+A1)‚Äãh‚àí1)‚ÄãP\displaystyle\leq\bigg(e^{(A\_{4}+A\_{1})T}(1-A\_{6}h)^{-N}g\_{x}+A\_{5}h(1-A\_{6}h)^{-N}\frac{e^{(A\_{4}+A\_{1})T}-1}{e^{(A\_{4}+A\_{1})h}-1}\bigg)P |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | +(1‚àíA6‚Äãh)‚àíN‚ÄãC‚Äã‚àëi=0N‚àí1eA4‚Äãi‚Äãh‚Äãh‚ÄãM0N‚àí1.\displaystyle\quad+(1-A\_{6}h)^{-N}C\sum\_{i=0}^{N-1}e^{A\_{4}ih}hM\_{0}^{N-1}. |  | (C.16) |

And thus we have,

|  |  |  |  |
| --- | --- | --- | --- |
|  | S‚â§A~2‚Äã(h)‚ÄãP+(1‚àíA6‚Äãh)‚àíN‚ÄãC‚Äã‚àëi=0N‚àí1eA4‚Äãi‚Äãh‚Äãh‚ÄãM0N‚àí1,\displaystyle S\leq\tilde{A}\_{2}(h)P+(1-A\_{6}h)^{-N}C\sum\_{i=0}^{N-1}e^{A\_{4}ih}hM\_{0}^{N-1}, |  | (C.17) |

where,

|  |  |  |  |
| --- | --- | --- | --- |
|  | A~2‚Äã(h)=e(A4+A1)‚ÄãT‚Äã(1‚àíA6‚Äãh)‚àíN‚Äãgx+A5‚Äãh‚Äã(1‚àíA6‚Äãh)‚àíN‚Äãe(A4+A1)‚ÄãT‚àí1e(A4+A1)‚Äãh‚àí1.\displaystyle\tilde{A}\_{2}(h)=e^{(A\_{4}+A\_{1})T}(1-A\_{6}h)^{-N}g\_{x}+A\_{5}h(1-A\_{6}h)^{-N}\frac{e^{(A\_{4}+A\_{1})T}-1}{e^{(A\_{4}+A\_{1})h}-1}. |  | (C.18) |

Now combining inequalities ([C.15](https://arxiv.org/html/2511.08735v1#A3.E15 "In Appendix C Proof of Theorem 2.6 ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) and ([C.17](https://arxiv.org/html/2511.08735v1#A3.E17 "In Appendix C Proof of Theorem 2.6 ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) we can have

|  |  |  |  |
| --- | --- | --- | --- |
|  | S\displaystyle S | ‚â§A~2‚Äã(h)‚Äã(A~1‚Äã(h)‚ÄãS+C‚Äã‚àëi=0N‚àí1e‚àíA1‚Äãh‚Äã(i+1)‚Äãh‚ÄãM0N‚àí1)+(1‚àíA6‚Äãh)‚àíN‚ÄãC‚Äã‚àëi=0N‚àí1eA4‚Äãi‚Äãh‚Äãh‚ÄãM0N‚àí1\displaystyle\leq\tilde{A}\_{2}(h)\bigg(\tilde{A}\_{1}(h)S+C\sum\_{i=0}^{N-1}e^{-A\_{1}h(i+1)}hM\_{0}^{N-1}\bigg)+(1-A\_{6}h)^{-N}C\sum\_{i=0}^{N-1}e^{A\_{4}ih}hM\_{0}^{N-1} |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | =A~1‚Äã(h)‚ÄãA~2‚Äã(h)‚ÄãS+C‚Äã‚àëi=0N‚àí1(h‚ÄãA~2‚Äã(h)‚Äãe‚àíA1‚Äãh‚Äã(i+1)+(1‚àíA6‚Äãh)‚àíN‚ÄãeA4‚Äãi‚Äãh‚Äãh)‚ÄãM0N‚àí1.\displaystyle=\tilde{A}\_{1}(h)\tilde{A}\_{2}(h)S+C\sum\_{i=0}^{N-1}\bigg(h\tilde{A}\_{2}(h)e^{-A\_{1}h(i+1)}+(1-A\_{6}h)^{-N}e^{A\_{4}ih}h\bigg)M\_{0}^{N-1}. |  | (C.19) |

Combining inequalities ([C.15](https://arxiv.org/html/2511.08735v1#A3.E15 "In Appendix C Proof of Theorem 2.6 ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) and ([C.17](https://arxiv.org/html/2511.08735v1#A3.E17 "In Appendix C Proof of Theorem 2.6 ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) we can also have

|  |  |  |  |
| --- | --- | --- | --- |
|  | P\displaystyle P | ‚â§A~1‚Äã(h)‚Äã(A~2‚Äã(h)‚ÄãP+(1‚àíA6‚Äãh)‚àíN‚ÄãC‚Äã‚àëi=0N‚àí1eA4‚Äãi‚Äãh‚Äãh‚ÄãM0N‚àí1)+C‚Äã‚àëi=0N‚àí1e‚àíA1‚Äãh‚Äã(i+1)‚Äãh‚ÄãM0N‚àí1\displaystyle\leq\tilde{A}\_{1}(h)\bigg(\tilde{A}\_{2}(h)P+(1-A\_{6}h)^{-N}C\sum\_{i=0}^{N-1}e^{A\_{4}ih}hM\_{0}^{N-1}\bigg)+C\sum\_{i=0}^{N-1}e^{-A\_{1}h(i+1)}hM\_{0}^{N-1} |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | =A~1‚Äã(h)‚ÄãA~2‚Äã(h)‚ÄãP+C‚Äã‚àëi=0N‚àí1(A~1‚Äã(h)‚Äã(1‚àíA6‚Äãh)‚àíN‚ÄãeA4‚Äãi‚Äãh‚Äãh+h‚Äãe‚àíA1‚Äãh‚Äã(i+1))‚ÄãM0N‚àí1,\displaystyle=\tilde{A}\_{1}(h)\tilde{A}\_{2}(h)P+C\sum\_{i=0}^{N-1}\bigg(\tilde{A}\_{1}(h)(1-A\_{6}h)^{-N}e^{A\_{4}ih}h+he^{-A\_{1}h(i+1)}\bigg)M\_{0}^{N-1}, |  | (C.20) |

Note that,

|  |  |  |  |
| --- | --- | --- | --- |
|  | A~¬Ø1=limh‚Üí0A~1‚Äã(h)\displaystyle\overline{\tilde{A}}\_{1}=\lim\_{h\rightarrow 0}\tilde{A}\_{1}(h) | =limh‚Üí0(e‚àíA1‚Äãh‚ÄãA2‚Äãh‚Äã1‚àíe‚àí(A1+A4)‚ÄãT1‚àíe‚àí(A1+A4)‚Äãh+A3A7‚Äãe‚àíA1‚Äãh)\displaystyle=\lim\_{h\rightarrow 0}\bigg(e^{-A\_{1}h}A\_{2}h\frac{1-e^{-(A\_{1}+A\_{4})T}}{1-e^{-(A\_{1}+A\_{4})h}}+\frac{A\_{3}}{A\_{7}}e^{-A\_{1}h}\bigg) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =A¬Ø2‚Äã(1‚àíe‚àí(A¬Ø1+A¬Ø4)‚ÄãT)A¬Ø1+A¬Ø4+A¬Ø3A¬Ø7,\displaystyle=\frac{\overline{A}\_{2}(1-e^{-(\overline{A}\_{1}+\overline{A}\_{4})T})}{\overline{A}\_{1}+\overline{A}\_{4}}+\frac{\overline{A}\_{3}}{\overline{A}\_{7}}, |  |

and

|  |  |  |  |
| --- | --- | --- | --- |
|  | A~¬Ø2=limh‚Üí0A~2‚Äã(h)\displaystyle\overline{\tilde{A}}\_{2}=\lim\_{h\rightarrow 0}\tilde{A}\_{2}\left(h\right) | =limh‚Üí0(e(A4+A1)‚ÄãT‚Äã(1‚àíA6‚Äãh)‚àíN‚Äãgx+A5‚Äãh‚Äã(1‚àíA6‚Äãh)‚àíN‚Äãe(A4+A1)‚ÄãT‚àí1e(A4+A1)‚Äãh‚àí1)\displaystyle=\lim\_{h\rightarrow 0}\bigg(e^{(A\_{4}+A\_{1})T}(1-A\_{6}h)^{-N}g\_{x}+A\_{5}h(1-A\_{6}h)^{-N}\frac{e^{(A\_{4}+A\_{1})T}-1}{e^{(A\_{4}+A\_{1})h}-1}\bigg) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =e(A¬Ø1+A¬Ø4+A¬Ø6)‚ÄãT‚Äãgx+A¬Ø5‚Äã(e(A¬Ø1+A¬Ø4+A¬Ø6)‚ÄãT‚àí1)A¬Ø1+A¬Ø4,\displaystyle=e^{(\overline{A}\_{1}+\overline{A}\_{4}+\overline{A}\_{6})T}g\_{x}+\frac{\overline{A}\_{5}(e^{(\overline{A}\_{1}+\overline{A}\_{4}+\overline{A}\_{6})T}-1)}{\overline{A}\_{1}+\overline{A}\_{4}}, |  |

and as a result

|  |  |  |  |
| --- | --- | --- | --- |
|  | limh‚Üí0A~1‚Äã(h)‚ÄãA~2‚Äã(h)=A¬Ø0=(A¬Ø2‚Äã(1‚àíe‚àí(A¬Ø1+A¬Ø4)‚ÄãT)A¬Ø1+A¬Ø4+A¬Ø3A¬Ø7)‚Äã(e(A¬Ø1+A¬Ø4+A¬Ø6)‚ÄãT‚Äãgx+A¬Ø5‚Äã(e(A¬Ø1+A¬Ø4+A¬Ø6)‚ÄãT‚àí1)A¬Ø1+A¬Ø4).\displaystyle\lim\_{h\rightarrow 0}\tilde{A}\_{1}(h)\tilde{A}\_{2}(h)=\overline{A}\_{0}=\bigg(\frac{\overline{A}\_{2}(1-e^{-(\overline{A}\_{1}+\overline{A}\_{4})T})}{\overline{A}\_{1}+\overline{A}\_{4}}+\frac{\overline{A}\_{3}}{\overline{A}\_{7}}\bigg)\bigg(e^{(\overline{A}\_{1}+\overline{A}\_{4}+\overline{A}\_{6})T}g\_{x}+\frac{\overline{A}\_{5}(e^{(\overline{A}\_{1}+\overline{A}\_{4}+\overline{A}\_{6})T}-1)}{\overline{A}\_{1}+\overline{A}\_{4}}\bigg). |  | (C.21) |

Now when A¬Ø0<1\overline{A}\_{0}<1, for any œµ>0\epsilon>0 and sufficiently small h, we can have from ([C](https://arxiv.org/html/2511.08735v1#A3.Ex55 "Appendix C Proof of Theorem 2.6 ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) and ([C](https://arxiv.org/html/2511.08735v1#A3.Ex56 "Appendix C Proof of Theorem 2.6 ‚Ä£ A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications")) that

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | S¬Ø\displaystyle\overline{S} | ‚â§(1+œµ)‚Äã[1‚àíA¬Ø0]‚àí1‚Äã(A~¬Ø2‚Äã1‚àíe‚àíA¬Ø1‚ÄãTA¬Ø1+eA¬Ø6‚ÄãT‚Äã(eA¬Ø4‚ÄãT‚àí1)A¬Ø4)‚ÄãC‚ÄãM0N‚àí1,\displaystyle\leq(1+\epsilon)[1-\overline{A}\_{0}]^{-1}\bigg(\overline{\tilde{A}}\_{2}\frac{1-e^{-\overline{A}\_{1}T}}{\overline{A}\_{1}}+\frac{e^{\overline{A}\_{6}T}(e^{\overline{A}\_{4}T}-1)}{\overline{A}\_{4}}\bigg)CM\_{0}^{N-1}, |  | (C.22) |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | P¬Ø\displaystyle\overline{P} | ‚â§(1+œµ)‚Äã[1‚àíA¬Ø0]‚àí1‚Äã(1‚àíe‚àíA¬Ø1‚ÄãTA¬Ø1+A~¬Ø1‚ÄãeA¬Ø6‚ÄãT‚Äã(eA¬Ø4‚ÄãT‚àí1)A¬Ø4)‚ÄãC‚ÄãM0N‚àí1,\displaystyle\leq(1+\epsilon)[1-\overline{A}\_{0}]^{-1}\bigg(\frac{1-e^{-\overline{A}\_{1}T}}{\overline{A}\_{1}}+\overline{\tilde{A}}\_{1}\frac{e^{\overline{A}\_{6}T}(e^{\overline{A}\_{4}T}-1)}{\overline{A}\_{4}}\bigg)CM\_{0}^{N-1}, |  | (C.23) |

where,

|  |  |  |  |
| --- | --- | --- | --- |
|  | P¬Ø\displaystyle\overline{P} | =max0‚â§n‚â§N‚Å°e‚àíA¬Ø1‚Äãn‚Äãh‚Äãùîº‚Äã|Œî‚ÄãXn|2,\displaystyle=\max\_{0\leq n\leq N}e^{-\overline{A}\_{1}nh}\hskip 5.69054pt\mathbb{E}|\Delta X\_{n}|^{2}, |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | S¬Ø\displaystyle\overline{S} | =max‚Å°{max0‚â§n‚â§N‚Å°eA¬Ø4‚Äãn‚Äãh‚Äãùîº‚Äã|Œî‚ÄãYn|2,‚àëi=0N‚àí1eA¬Ø4‚Äãi‚Äãh‚ÄãeA¬Ø6‚ÄãT‚ÄãA¬Ø7‚Äãh‚Äãùîº‚Äã|Œî‚ÄãZi|2}.\displaystyle=\max\bigg\{\max\_{0\leq n\leq N}e^{\overline{A}\_{4}nh}\hskip 5.69054pt\mathbb{E}|\Delta Y\_{n}|^{2},\sum\_{i=0}^{N-1}e^{\overline{A}\_{4}ih}e^{\overline{A}\_{6}T}\overline{A}\_{7}h\hskip 5.69054pt\mathbb{E}|\Delta Z\_{i}|^{2}\bigg\}. |  |

By fixing œµ=1\epsilon=1, we obtain our error of ùîº‚Äã|Œî‚ÄãXn|2,ùîº‚Äã|Œî‚ÄãYn|2\mathbb{E}|\Delta X\_{n}|^{2},\mathbb{E}|\Delta Y\_{n}|^{2} and ùîº‚Äã|Œî‚ÄãZn|2\mathbb{E}|\Delta Z\_{n}|^{2} as

|  |  |  |
| --- | --- | --- |
|  | max0‚â§n‚â§N‚Å°ùîº‚Äã|Œî‚ÄãXn|2‚â§eA¬Ø1‚ÄãT‚à®0‚ÄãP¬Ø‚â§C‚ÄãM0N‚àí1,\max\_{0\leq n\leq N}\mathbb{E}|\Delta X\_{n}|^{2}\leq e^{\overline{A}\_{1}T\vee 0}\overline{P}\leq CM\_{0}^{N-1}, |  |

and

|  |  |  |
| --- | --- | --- |
|  | max‚Å°{max0‚â§n‚â§N‚Å°ùîº‚Äã|Œî‚ÄãYn|2,‚àëi=0N‚àí1ùîº‚Äã|Œî‚ÄãZi|2‚Äãh}‚â§e‚àíA¬Ø4‚ÄãT‚à®0‚Äã(1+A¬Ø7‚àí1)‚ÄãS¬Ø‚â§C‚ÄãM0N‚àí1.\max\left\{\max\_{0\leq n\leq N}\mathbb{E}|\Delta Y\_{n}|^{2},\,\,\sum\_{i=0}^{N-1}\mathbb{E}|\Delta Z\_{i}|^{2}h\right\}\leq e^{-\overline{A}\_{4}T\vee 0}(1+\overline{A}\_{7}^{-1})\overline{S}\leq CM\_{0}^{N-1}. |  |

Thus,

|  |  |  |  |
| --- | --- | --- | --- |
|  | supt‚àà[0,T]ùîº‚Äã[|Xt‚àíX¬ØtœÄ|2+|Yt‚àíY¬ØtœÄ|2]+ùîº‚Äã‚à´0T|Z^t‚àíZ¬ØtœÄ|2‚Äãùëët‚â§C‚ÄãM.\begin{split}\sup\_{t\in[0,T]}\mathbb{E}\bigg[|X\_{t}-\overline{X}\_{t}^{\pi}|^{2}+|Y\_{t}-\overline{Y}\_{t}^{\pi}|^{2}\bigg]+\mathbb{E}\int\_{0}^{T}|\hat{Z}\_{t}-\overline{Z}\_{t}^{\pi}|^{2}dt&\leq CM.\end{split} |  | (C.24) |

‚àé