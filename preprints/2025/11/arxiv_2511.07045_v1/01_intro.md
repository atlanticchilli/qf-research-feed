---
authors:
- John Armstrong
- Cristin Buescu
- James Dalby
- Rohan Hobbs
doc_id: arxiv:2511.07045v1
family_id: arxiv:2511.07045
is_current: true
taxonomy:
  alpha_families: []
  asset_classes: []
  horizons: []
  themes: []
title: Machine-learning a family of solutions to an optimal pension investment problem
url_abs: http://arxiv.org/abs/2511.07045v1
url_html: https://arxiv.org/html/2511.07045v1
venue: arXiv q-fin
version: 1
year: 2025
---


John Armstrong, Cristin Buescu, James Dalby, Rohan Hobbs

###### Abstract

We use a neural network to identify the optimal solution
to a family of optimal investment problems, where the parameters determining
an investorâ€™s risk and consumption preferences are given as inputs to the neural network in addition to economic variables. This is used to develop a practical tool that can be used to explore how pension outcomes vary with preference parameters. We use a Black-Scholes economic model so that we may validate the accuracy of network using a classical and provably convergent numerical method
developed using the duality approach.

## Introduction

The von Neumannâ€“Morgenstern utility theorem implies that, under mild assumptions, an individualâ€™s preferences at a single time can be represented by an expected utility [[31](https://arxiv.org/html/2511.07045v1#bib.bib31)].
Nevertheless, even if one accepts that utility functions provide the best available tool to model preferences, it remains difficult to identify an
individualâ€™s preferences and this is a recurring criticism in the literature [[1](https://arxiv.org/html/2511.07045v1#bib.bib1), [28](https://arxiv.org/html/2511.07045v1#bib.bib28), [30](https://arxiv.org/html/2511.07045v1#bib.bib30)].
When considering an investorâ€™s preferences over
time, the space of possible preferences is still larger. We
seek to provide a practical tool to assist
in identifying a reasonable approximation to an investorâ€™s preferences
for the purpose of pension investment.

A standard practical approach taken when providing guidance on Defined Contribution (DC) investments is to
give a
questionnaire to identify risk preferences [[11](https://arxiv.org/html/2511.07045v1#bib.bib11)].
A pension professional designing products
can provide a menu of options to potential
customers and use tools such as a risk questionnaire to
advise them on the best selection from this menu. It
is in developing this menu of options that utility functions
can provide a useful framework. They allow preferences
to be operationalised mathematically and then used
to identify coherent investment strategies. Designs that are not based
on optimization may even prove to be stochastically dominated by other strategies
and this is clearly undesirable.

Our goal in this paper
is to write a tool a pension professional might use in order
to identify good candidate gain functions. It allows the pensions professional to
vary the parameters within a family of utility functions and quickly view the resulting outcomes. They can then
use their professional expertise to perform the
subjective task of matching these outcomes to investors.
Utility-function inference from behaviour has been studied mathematically [[13](https://arxiv.org/html/2511.07045v1#bib.bib13), [14](https://arxiv.org/html/2511.07045v1#bib.bib14)], but we do not seek to do anything more sophisticated than
inferring utility-functions by selecting from a menu of options.

To produce an appropriate set of choices we first need a sufficiently rich family of gain functions to capture the key differences between different types of investor while remaining easy to interpret. We require gain functions that capture preferences for consumption over time, while allowing individuals to distinguish between their risk-aversion and the diminishing marginal utility of consumption at any given point in time (which we call satiation). This distinction is necessary to resolve many asset pricing puzzles [[8](https://arxiv.org/html/2511.07045v1#bib.bib8), [7](https://arxiv.org/html/2511.07045v1#bib.bib7), [9](https://arxiv.org/html/2511.07045v1#bib.bib9)]. Epsteinâ€“Zin preferences, exhibit these features and offer analytically tractable solutions that can often be analysed with full rigour [[18](https://arxiv.org/html/2511.07045v1#bib.bib18), [17](https://arxiv.org/html/2511.07045v1#bib.bib17), [21](https://arxiv.org/html/2511.07045v1#bib.bib21)]. However, the positive homogeneity of such utility functions can produce some unrealistic solutions [[6](https://arxiv.org/html/2511.07045v1#bib.bib6)]. This motivates us to sacrifice analyticity and use a preference model given by Exponential Kihlstromâ€“Mirman preferences (also used in this context in [[4](https://arxiv.org/html/2511.07045v1#bib.bib4)]). Although general Kihlstromâ€“Mirman preferences
are not time consistent in the sense of [[22](https://arxiv.org/html/2511.07045v1#bib.bib22)] the exponential case without discounting that we are using is time consistent. This can be used
to provide a justification for restricting attention to
preferences of this form [[12](https://arxiv.org/html/2511.07045v1#bib.bib12)].

The specific problem we solve is optimal investment with
idiosyncratic risk insured using a tontine structure. There is extensive literature on tontines, see [[25](https://arxiv.org/html/2511.07045v1#bib.bib25)] for an extensive review of both the history of tontines and more recent literature. Other works of note would be: [[26](https://arxiv.org/html/2511.07045v1#bib.bib26)] who look at optimal investment with a bond-only tontine or [[10](https://arxiv.org/html/2511.07045v1#bib.bib10)] who study a pooled annuity fund that utilizes the tontine mechanism. Although tontines have been studied heavily, the literature on the optimal control approach to making best use of a tontine is surprisingly limited. The problem is solved for power utility in [[29](https://arxiv.org/html/2511.07045v1#bib.bib29)]
for Epsteinâ€“Zin utility and with systematic longevity risk in [[2](https://arxiv.org/html/2511.07045v1#bib.bib2)].

We will identify the optimal investment and consumption strategies by using a machine-learning approach. The central
task of solving optimal control problems by machine learning is well studie, see for example [[20](https://arxiv.org/html/2511.07045v1#bib.bib20), [15](https://arxiv.org/html/2511.07045v1#bib.bib15)] and [[19](https://arxiv.org/html/2511.07045v1#bib.bib19)] for a recent survey of this fast-moving field.
We have consciously chosen the most direct approach
of a forward method because we believe this will be the simplest for industry
to understand and adapt to their needs without complex mathematics. Backward methods may be more efficient and scale better for higher-dimensional problems.

The preference model used in this paper is studied in [[5](https://arxiv.org/html/2511.07045v1#bib.bib5)] also using
machine-learning methods, but under the assumption that the parameters of the utility function were fixed. The
contribution of this paper is to extend this by solving for multiple utility functions simultaneously. Learning optimal controls across a range of objective function parameters has been done before [[23](https://arxiv.org/html/2511.07045v1#bib.bib23)], but is more challenging for us because of scaling issues with our utility function. We resolve this by using additional neural networks to scale the utility function given the preference parameters. This is the central contribution of our paper.

We have chosen to perform the optimization using a simple Blackâ€“Scholes model
to focus attention on the issue of the parameterization of the utility function.
This has the additional advantage that we can solve the optimal investment problem using
alternative numerical methods and so validate the success of our approach.
The second contribution of our paper is
to validate the numerical method for our preference model using a provably convergent scheme that exploits duality.

## 1 Discrete-time investment problem

We consider an optimal investment and consumption problem set within a Black-Scholes framework. The dynamics of the risky asset price, denoted SS, follow a geometric Brownian motion described by

|  |  |  |
| --- | --- | --- |
|  | dâ€‹St=Î¼â€‹Stâ€‹dâ€‹t+Ïƒâ€‹Stâ€‹dâ€‹Wt,\mathrm{d}S\_{t}=\mu S\_{t}\mathrm{d}t+\sigma S\_{t}\,\mathrm{d}W\_{t}, |  |

where Î¼âˆˆâ„\mu\in\mathbb{R} represents the drift, Ïƒâˆˆâ„+\sigma\in\mathbb{R}\_{+} the volatility and WW a standard Brownian motion. Investment and consumption decisions are assumed to be made at discrete intervals, defined by the set ğ’¯:={0,Î´â€‹t,2â€‹Î´â€‹t,â€¦,T}{\cal T}:=\{0,\delta t,2\delta t,\ldots,T\} for some time-step Î´â€‹t\delta t and final time TT. Between these times, investments are made following a fixed-weight strategy. Let Ï€t\pi\_{t} be the proportion of wealth allocated to the risky asset at time tâˆˆğ’¯t\in{\cal T}. This proprtion is maintained throughout the interval [t,t+Î´â€‹t)[t,t+\delta t) with the remainder of the portfolio allocated to a risk-free asset, growing at a constant rate rr. The wealth evolves according to the stochastic differential equation (SDE)

|  |  |  |  |
| --- | --- | --- | --- |
|  | dâ€‹ws=wsâ€‹(Ï€tâ€‹Î¼+(1âˆ’Ï€t)â€‹r)â€‹dâ€‹s+wsâ€‹Ï€tâ€‹Ïƒâ€‹dâ€‹Ws.\mathrm{d}w\_{s}=w\_{s}(\pi\_{t}\mu+(1-\pi\_{t})r)\mathrm{d}s+w\_{s}\pi\_{t}\sigma\,\mathrm{d}W\_{s}. |  | (1) |

on the interval [wt,wt+Î´â€‹t)[w\_{t},w\_{t+\delta t}). We denote the limit from the left of wealth at the end of the period by w(t+Î´â€‹t)âˆ’w\_{(t+\delta t)-}.
Applying ItÃ´â€™s lemma gives the log wealth process

|  |  |  |
| --- | --- | --- |
|  | dâ€‹(logâ¡(ws))=(Ï€tâ€‹Î¼+(1âˆ’Ï€t)â€‹râˆ’12â€‹(Ï€tâ€‹Ïƒ)2)â€‹dâ€‹s+Ï€tâ€‹Ïƒâ€‹dâ€‹Ws.\mathrm{d}(\log(w\_{s}))=\left(\pi\_{t}\mu+(1-\pi\_{t})r-\frac{1}{2}(\pi\_{t}\sigma)^{2}\right)\mathrm{d}s+\pi\_{t}\sigma\mathrm{d}W\_{s}. |  |

This yields the expression

|  |  |  |
| --- | --- | --- |
|  | logâ¡(w(t+Î´â€‹t)âˆ’)âˆ’logâ¡(wt)=(Ï€tâ€‹Î¼+(1âˆ’Ï€t)â€‹râˆ’12â€‹(Ï€tâ€‹Ïƒ)2)â€‹Î´â€‹t+Ï€tâ€‹Ïƒâ€‹(Wt+Î´â€‹tâˆ’Wt).\log(w\_{(t+\delta t)-})-\log(w\_{t})=\left(\pi\_{t}\mu+(1-\pi\_{t})r-\frac{1}{2}(\pi\_{t}\sigma)^{2}\right)\delta t+\pi\_{t}\sigma(W\_{t+\delta t}-W\_{t}). |  |

For simplicity and simulation, we define

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ïµt:=Wt+Î´â€‹tâˆ’WtÎ´â€‹t,\epsilon\_{t}:=\frac{W\_{t+\delta t}-W\_{t}}{\sqrt{\delta t}}, |  | (2) |

so Ïµt\epsilon\_{t} is distributed according to standard normal. Thus we are able to simulate the log wealth process using the Gaussian increments Ïµt\epsilon\_{t}, and this log simulation, combined with the fixed-weight strategies, automatically ensures that strategies that put one into debt are removed.

To obtain wtw\_{t} from wtâˆ’w\_{t-}, we incorporate contributions, consumption and longevity payments via the equation

|  |  |  |  |
| --- | --- | --- | --- |
|  | wt=Î·â€‹stâ€‹ğŸ™t<tRA+(1âˆ’ctâ€‹ğŸ™tâ‰¥tRA)â€‹(1+Pâˆ,tâ€‹ğŸ™t>tRA)â€‹wtâˆ’,w\_{t}=\eta s\_{t}\mathbbm{1}\_{t<t\_{\mathrm{RA}}}+(1-c\_{t}\mathbbm{1}\_{t\geq t\_{\mathrm{RA}}})(1+P\_{\infty,t}\mathbbm{1}\_{t>t\_{\mathrm{RA}}})w\_{t-}, |  | (3) |

where the first term describes the fraction, Î·\eta, of an individuals salary, sts\_{t}, that is contributed before retirement (tRAt\_{\mathrm{RA}} is the time of retirement) and the second term removes the consumption, ctâ€‹wtâˆ’c\_{t}w\_{t-}, and adds on any longevity payment, Pâˆ,tâ€‹wtâˆ’P\_{\infty,t}w\_{t-}, that one may receive in retirement. The longevity payment satisfies

|  |  |  |
| --- | --- | --- |
|  | Pâˆ,tâ€‹wtâˆ’:=(pt1âˆ’pt)â€‹wtâˆ’,P\_{\infty,t}w\_{t-}:=\left(\frac{p\_{t}}{1-p\_{t}}\right)w\_{t-}, |  |

where ptp\_{t} is the probability of dying in year tt, given you were alive in year tâˆ’1t-1. The infinity in the subscript is there to signify the size of the fund is infinite, and we use it to maintain notation with our other papers. The longevity payment can be achieved using a tontine structure, for a discussion of how this can be implemented in practice see [[5](https://arxiv.org/html/2511.07045v1#bib.bib5)].

The optimization procedure is based on a stylized gain function which
we call Exponential Khilstrom-Mirman utility [[5](https://arxiv.org/html/2511.07045v1#bib.bib5)]. Specifically, the agents seek to maximise

|  |  |  |
| --- | --- | --- |
|  | Uâ€‹(C):=ğ”¼â€‹(âˆ’expâ€‹(âˆ’Î±â€‹âˆ‘j=tRAj<Ï„uâ€‹(Cj)â€‹Î´â€‹t)),U(C):=\mathbb{E}\left(-\text{exp}\left(-\alpha\sum\_{j=t\_{\text{RA}}}^{j<\tau}u(C\_{j})\delta t\right)\right), |  |

where Î±>0\alpha>0 is a risk aversion parameter, Ï„\tau is the individualâ€™s time of death and Î´â€‹t\delta t defines the time step between consumption decisions. The function uâ€‹(Ct,a)u(C\_{t},a) is given by

|  |  |  |
| --- | --- | --- |
|  | uâ€‹(Ct):=CtÏÏâˆ’aÏÏ,u(C\_{t}):=\frac{C\_{t}^{\rho}}{\rho}-\frac{a^{\rho}}{\rho}, |  |

where Ï\rho is a satiation parameter, aa is the adequacy level and CtC\_{t} defines the individualâ€™s consumption amount (consumption proportion multiplied by current wealth) relative to their final salary at a time point tt. In order to compute the gain function, we assume that consumption and individual longevity risk are independent. We also assume there is no systematic longevity risk and that the probability an individual dies in a given year ss is given by pÂ¯s\overline{p}\_{s}. So, we compute

|  |  |  |
| --- | --- | --- |
|  | U=âˆ’ğ”¼Investâ€‹[âˆ‘s=tRATpÂ¯sâ€‹expâ€‹(âˆ’Î±â€‹âˆ‘t=tRAsuâ€‹(Ct,a)â€‹Î´â€‹t)].U=-\mathbb{E}\_{\mathrm{Invest}}\left[\sum\_{s=t\_{\mathrm{RA}}}^{T}\overline{p}\_{s}\,\text{exp}\left(-\alpha\sum\_{t=t\_{\mathrm{RA}}}^{s}u(C\_{t},a)\delta t\right)\right]. |  |

In this formula, ğ”¼Invest{\mathbb{E}}\_{\mathrm{Invest}} denotes the expectation across investment scenarios
and so excludes the mortality component of our probability model, which is accounted for by the term pÂ¯s{\overline{p}}\_{s}. TT is the maximum time of death, which is finite for the mortality model we are using. If we generate NN investment scenarios and label the consumption in each case c(j)c^{(j)} with 1â‰¤jâ‰¤N1\leq j\leq N, we may estimate the gain function using

|  |  |  |
| --- | --- | --- |
|  | U^:=âˆ’1Nâ€‹âˆ‘j=1N[âˆ‘s=tRATpÂ¯sâ€‹expâ€‹(âˆ’Î±â€‹âˆ‘t=tRAsuâ€‹(Ct(j))â€‹Î´â€‹t)].\hat{U}:=-\frac{1}{N}\sum\_{j=1}^{N}\left[\sum\_{s=t\_{\mathrm{RA}}}^{T}\overline{p}\_{s}\,\text{exp}\left(-\alpha\sum\_{t=t\_{\mathrm{RA}}}^{s}u(C\_{t}^{(j)})\delta t\right)\right]. |  |

## 2 Training the Fixed Parameter Network

Due to the inhomogeneous gain function, solving the optimization using classical techniques is reasonably computationally expensive (it takes several minutes for the decumulation-only problem with a single fixed set of parameter values), and will become infeasible if one uses richer economic models. It is also requires considerable programming if one changes economic model or loss function. For these reasons, we a neural network to solve the problem numerically. For this Blackâ€“Scholes model, we take the standard Gaussians Ïµt\epsilon\_{t} in equation ([2](https://arxiv.org/html/2511.07045v1#S1.E2 "In 1 Discrete-time investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")) as the input to the recurrent neural network that we train. Since we use a gated recurrent unit (GRU), we also include the time points. Note that we use years as time points in our simulation, so we set Î´â€‹t=1\delta t=1. While training and evaluating a fixed parameter network, the gain function parameters Î±\alpha, Ï\rho and aa remain fixed throughout, across scenarios. We have a visual representation of the network architecture in Figure [1](https://arxiv.org/html/2511.07045v1#S2.F1 "Figure 1 â€£ 2 Training the Fixed Parameter Network â€£ Machine-learning a family of solutions to an optimal pension investment problem").

![Refer to caption](varying_parameter_paper/RNN_for_varying_params_paper.drawio.png)

Figure 1: Architecture for the Recurrent Neural Network. The arrows are purely for demonstrative purposes and all the layers are dense. The label â€˜FFâ€™ denotes a feedforward layer.

The neural network outputs an investment strategy Ï€tÎ¸\pi\_{t}^{\theta} and a consumption strategy ctÎ¸c\_{t}^{\theta} (both as proportions) at each time step.
The superscript Î¸\theta indicates the dependence of this strategy on the neural networkâ€™s parameter values, and so
changes as the network is trained.
If the time tt is less than the time of retirement, then the consumption strategy is simply ignored. We take these strategies for the whole time period tâˆˆ[0,T]t\in[0,T], and compute our loss function. Since neural networks seek to minimize their loss function, we take the loss function to be the (logarithm of the) negative of the gain function. Sparing some detail and computations outlined in [[5](https://arxiv.org/html/2511.07045v1#bib.bib5)], we compute the loss function of the network to equal

|  |  |  |  |
| --- | --- | --- | --- |
|  | L=logâ¡(âˆ‘s=1Nexpâ¡(logâ¡(âˆ‘t=tRATexpâ¡(logâ¡(p~t)âˆ’Î±â€‹âˆ‘j=tRAtuâ€‹(Cj(s))â€‹Î´â€‹t))))âˆ’logâ¡(N),L=\log\left(\sum^{N}\_{s=1}\exp\left(\log\left(\sum\_{t=t\_{\text{RA}}}^{T}\exp\left(\log(\tilde{p}\_{t})-\alpha\sum^{t}\_{j=t\_{\text{RA}}}u(C\_{j}^{(s)})\delta t\right)\right)\right)\right)\\ -\log(N), |  | (4) |

where NN is the number of scenarios across which we average, TT is the maximum lifetime and pÂ¯t\bar{p}\_{t} is the probability an individual dies in a given year tt after retirement. We compute this expression using the logsumexp function to reduce excessive rounding errors and ensure numerical stability.

As a reference point for the success of training, we plot a graph of the percentiles of the replacement ratio of an individual across many scenarios. The replacement ratio is defined to be the ratio of pension payments to index-linked final salary. Essentially, it shows the consumption of an individual, relative to their final salary and adjusted for by inflation. As such, it can be considered as a direct consequence of the consumption and investment strategies, and so it can be taken as a way of comparing and thus rating the strategies. For further details on replacement ratios, see [[5](https://arxiv.org/html/2511.07045v1#bib.bib5)], and for plots of the actual investment and consumption strategies see Appendix [B](https://arxiv.org/html/2511.07045v1#A2 "Appendix B Strategy Plots to Match Outcome Plots â€£ Machine-learning a family of solutions to an optimal pension investment problem").

We use the parameters in Table [1](https://arxiv.org/html/2511.07045v1#S2.T1 "Table 1 â€£ 2 Training the Fixed Parameter Network â€£ Machine-learning a family of solutions to an optimal pension investment problem"), which we will call the default parameters from hereafter, and the trained network produces strategies that lead to the outcomes in Figure [2](https://arxiv.org/html/2511.07045v1#S2.F2 "Figure 2 â€£ 2 Training the Fixed Parameter Network â€£ Machine-learning a family of solutions to an optimal pension investment problem").

| Parameters | Value |
| --- | --- |
| Î±\alpha | 5Ã—10âˆ’55\times 10^{-5} |
| Ï\rho | -2 |
| aa | 0.4 |

Table 1: Default fixed-parameter values used to train the â€˜fixed-parameterâ€™ network.



![Refer to caption](varying_parameter_paper/fixed_parameter_plot.png)

Figure 2: Retirement outcome deciles for the â€˜fixed-parameterâ€™ trained neural network.

## 3 Verification of the optimal strategy

As a test of the validity of our network
we compare the results obtained using a provably
convergent classical algorithm.
In Appendix [C](https://arxiv.org/html/2511.07045v1#A3 "Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem") we use a duality
approach to derive a numerical algorithm for
solving the decumulation-only problem with discrete-time consumption
and continuous-time investment. As is well-known,
in complete markets it is easier to prove rigorous results for duality methods
than for primal methods. Proofs using primal approaches typically require restrictive
growth conditions or more delicate arguments. See [[16](https://arxiv.org/html/2511.07045v1#bib.bib16)] for
an explanation of the challenges of primal methods and a review of the literature. Our algorithm proceeds by using a duality method to solve the
problem when consumption is restricted to one-period. We then solve the discrete-time consumption, continuous-time investment problem recursively.

We considered a solution to have achieved reasonable accuracy for investment purposes if the value of the expected utility is within 1% of the standard error of the utility. In Figure [3](https://arxiv.org/html/2511.07045v1#S3.F3 "Figure 3 â€£ 3 Verification of the optimal strategy â€£ Machine-learning a family of solutions to an optimal pension investment problem") we plot the outcome from our neural
network against a solution of the decumulation only
problem obtained using the method of [C](https://arxiv.org/html/2511.07045v1#A3 "Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem"). Not only are the plots similar, but the neural network is within 1% of the utility with its predictions. It does this across various different
tested gain function parameters.
The neural network is, of course, able to compute the result much faster. The classical method takes several minutes
to run even though this is just a one-dimensional problem.

![Refer to caption](varying_parameter_paper/neural_network_decumulation_vs_convergent.png)

Figure 3: Comparison of retirement outcomes for the neural network approach compared to the provably convergent method in a decumulation only setting.

We were also able to validate our approach for the decumulation problem with power-utility problem and discrete-time consumption and continuous-time investment using the analytic solution of [[2](https://arxiv.org/html/2511.07045v1#bib.bib2)], Theorem 2.2.

We have tested that the method is also capable of producing good strategies in richer economic models, but we will defer more detailed discussion to another paper.

## 4 Issues with a Naive Method of Training the Neural Network

We now proceed to training an RNN within the problem discussed in section [2](https://arxiv.org/html/2511.07045v1#S2 "2 Training the Fixed Parameter Network â€£ Machine-learning a family of solutions to an optimal pension investment problem") across a range of gain function parameters, rather than for a fixed set. Note that the parameters that we will allow to vary are the risk aversion parameter, the satiation parameter and the adequacy level in the loss function in [4](https://arxiv.org/html/2511.07045v1#S2.E4 "In 2 Training the Fixed Parameter Network â€£ Machine-learning a family of solutions to an optimal pension investment problem").

A naive approach to learning optimal strategies for various parameter combinations would be to randomly sample values from an acceptable range for each parameter in each scenario and include them as inputs to the RNN. They would also therefore be used in the computation of the loss function. This would in theory allow the network to train on a varied set of gain functions, minimising the loss across all of them. However, the principal issue with this method arises from the loss function itself. Since we employ an exponential gain function, the loss values can vary significantly across differing parameter values. As a result, when averaging the loss function across scenarios, certain parameter combinations can dominate as they provide more extreme utility values with more extreme variance. This in turn leads to excessive focus on reducing the loss of these specific scenarios, which distorts training, preventing the network from learning an accurate solution across all parameter combinations.

We test the success of training using this naive method by comparing the trained RNNâ€™s prediction for fixed parameter values against the results obtained when training the fixed-parameter RNN with the corresponding parameter values. The naive method performs so poorly that the expected utility from its strategy is practically incomparable to that of the fixed-parameter RNN across differing gain function parameters.

## 5 Two Alternative Architectures

We now introduce two alternative architectures that overcome the high-variance issues outlined in section [4](https://arxiv.org/html/2511.07045v1#S4 "4 Issues with a Naive Method of Training the Neural Network â€£ Machine-learning a family of solutions to an optimal pension investment problem"), and learn optimal strategies effectively across a wide range of parameter values. These approaches both use separate neural networks to modify the loss function of the main RNN, such that the loss value for each scenario is scaled for the given parameter combination. In this sense, the variance of the loss function is normalized, allowing the main RNN to train effectively across the entire parameter space. Note that from here onward the â€˜mainâ€™ RNN is the network learning the optimal investment and consumption strategies and the architecture is the same as in Figure [1](https://arxiv.org/html/2511.07045v1#S2.F1 "Figure 1 â€£ 2 Training the Fixed Parameter Network â€£ Machine-learning a family of solutions to an optimal pension investment problem"), except that the input dimension is increased by one in order to accommodate the parameters as inputs.

The problem we need to address is that the appropriate scale for the problem depends on the parameter values. The two methods we introduce here differ in how these scaling factors are produced.

We also considered the possibility of scaling
the networks by computing a certainty-equivalent value for
our loss functions to ensure the value is comparable across
different parameter ranges. However, the computation of
a certainty-equivalent is not analytically tractable for our gain function and numerical root-finding methods come with their own challenges when the scale of the problem is unknown. As a result we felt that designing an algorithm using this approach would be more complex than scaling based on the observed mean and variance and was likely to be less effective.

### 5.1 A Two-Step Iterative Algorithm

In this method, we use a two-step iterative algorithm that makes use of a secondary neural network, called the â€˜scalingâ€™ network, to estimate the scaling factors. In short, the main RNN is trained to minimize a scaled loss function using the current estimates from the scaling network. Once trained, this RNN is then used to generate data under the unscaled loss, which is used to update the scaling network. This iterative, alternating procedure allows the two networks to improve each other: the scaling network enables more stable and effective training of the RNN, while the improved RNN provides better data for refining the scaling estimates.

We will use an index i=0,1,2,â€¦i=0,1,2,\ldots as our iteration counter. At each iteration, we will assume that we have the function Ïƒi:ğ’«iâ†’â„\sigma\_{i}:{\mathcal{P}\_{i}}\to{\mathbb{R}} which estimates the standard deviation of the utility function, conditioned on the parameter values pâˆˆğ’«ip\in{\mathcal{P}\_{i}}. Here, pp denotes the triplet (Î±,Ï,a\alpha,\rho,a). To initialize the algorithm we define

|  |  |  |
| --- | --- | --- |
|  | Ïƒ0â€‹(p)=1.\sigma\_{0}(p)=1. |  |

The first step in each iteration is to train an RNN to learn an approximate optimal investment and consumption strategy

|  |  |  |
| --- | --- | --- |
|  | fi:(ğ’«i)NÃ—(â„)Nâ†’(ğ’ª)N.f\_{i}:({\mathcal{P}\_{i}})^{N}\times({\cal I})^{N}\to({\cal O})^{N}. |  |

Define the log utility for each scenario, conditional on a given parameter set pp, as

|  |  |  |  |
| --- | --- | --- | --- |
|  | vâ€‹(p,s)=logâ¡(âˆ‘t=0Texpâ¡(logâ¡(p~t)âˆ’Î±â€‹âˆ‘j=0tupâ€‹(cj(s))â€‹Î´â€‹t)).v(p,s)=\log\left(\sum\_{t=0}^{T}\exp\left(\log(\tilde{p}\_{t})-\alpha\sum^{t}\_{j=0}u\_{p}(c\_{j}^{(s)})\delta t\right)\right). |  | (5) |

The subscript pp in upu\_{p} highlights the dependency of the uu on the parameter set.
The RNN learns the strategy fif\_{i} by minimizing the following scaled loss function:

|  |  |  |
| --- | --- | --- |
|  | â„“iâ€‹(p,Ïƒiâ€‹(p))=logâ¡(âˆ‘s=1Nexpâ¡(vâ€‹(p,s))Ïƒiâ€‹(p))âˆ’logâ¡(N).\ell\_{i}(p,\sigma\_{i}(p))=\log\left(\sum\_{s=1}^{N}\frac{\exp\left(v(p,s)\right)}{\sigma\_{i}(p)}\right)-\log(N). |  |

We compute this using the log-sum-exp function:

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„“iâ€‹(p,Ïƒiâ€‹(p))=logâ¡(âˆ‘s=1Nexpâ¡[vâ€‹(p,s)âˆ’logâ¡(Ïƒiâ€‹(p))])âˆ’logâ¡(N).\ell\_{i}(p,\sigma\_{i}(p))=\log\left(\sum\_{s=1}^{N}\exp\left[v(p,s)-\log\left(\sigma\_{i}(p)\right)\right]\right)-\log(N). |  | (6) |

Again, we do this to reduce excessive rounding errors when computing the average across scenarios.

Next, we train a separate neural network, the â€˜scaling networkâ€™, to estimate the standard deviation of the utility function conditional on the parameters. That is, expâ¡(vâ€‹(p,s))\exp(v(p,s)), evaluated using the strategy fif\_{i} and conditioned on the parameter values pâˆˆğ’«i+1p\in\mathcal{P}\_{i+1}. This network, trained using supervised learning, learns the mapping Ïƒi+1â€‹(p):ğ’«i+1â†’â„+\sigma\_{i+1}(p):\mathcal{P}\_{i+1}\to\mathbb{R}\_{+}. The training dataset consists of 50,00050,000 samples of parameter combinations along with the corresponding empirical standard deviations of the loss function (computed using the current strategy fif\_{i}). The parameters are the inputs and the standard deviations are the labels. If ğ’«i+1\mathcal{P}\_{i+1} is a richer set of parameters than ğ’«i\mathcal{P}\_{i}, then the strategy fif\_{i} essentially extrapolates its learned function across the whole parameter set.

Once this training is complete, we have the new scaling network and thus the new scaling factors, and so we increment the iteration counter and repeat the process using the updated estimates of Ïƒi+1â€‹(p)\sigma\_{i+1}(p).

In practice, we found that only three full iterations of this process were necessary to solve our problem with sufficient accuracy. The final RNN, using the strategy f3f\_{3} from the fourth iteration, was used as our end result. In each of the iterations, we increased the size of the parameter space such that ğ’«3âŠƒğ’«2âŠƒğ’«1\mathcal{P}\_{3}\supset\mathcal{P}\_{2}\supset\mathcal{P}\_{1} (=ğ’«0=\mathcal{P}\_{0}), and full details can be found in Appendix [A](https://arxiv.org/html/2511.07045v1#A1 "Appendix A Neural Network Architecture Details â€£ Machine-learning a family of solutions to an optimal pension investment problem"). As such, when producing the loss function data needed to train the scaling network for the next iteration, the trained RNN simply extrapolated its predictions from the smaller space ğ’«iâˆ’1\mathcal{P}\_{i-1} to the full range ğ’«i\mathcal{P}\_{i}.

Figure [4a](https://arxiv.org/html/2511.07045v1#S5.F4.sf1 "In Figure 4 â€£ 5.1 A Two-Step Iterative Algorithm â€£ 5 Two Alternative Architectures â€£ Machine-learning a family of solutions to an optimal pension investment problem") illustrates the impact of this alternative architecture on training outcomes. Once again, we used the parameters in Table [1](https://arxiv.org/html/2511.07045v1#S2.T1 "Table 1 â€£ 2 Training the Fixed Parameter Network â€£ Machine-learning a family of solutions to an optimal pension investment problem") as input to the trained network.

![Refer to caption](varying_parameter_paper/comaprison_varying_with_fixed_plot.png)


a

![Refer to caption](varying_parameter_paper/comparison_simulatenous_scaling_with_fixed.png)


b

Figure 4: Panel ([4a](https://arxiv.org/html/2511.07045v1#S5.F4.sf1 "In Figure 4 â€£ 5.1 A Two-Step Iterative Algorithm â€£ 5 Two Alternative Architectures â€£ Machine-learning a family of solutions to an optimal pension investment problem")) shows retirement outcomes for the RNN produced by the â€˜two-step iterative method as compared to the fixed parameter network. Panel ([4b](https://arxiv.org/html/2511.07045v1#S5.F4.sf2 "In Figure 4 â€£ 5.1 A Two-Step Iterative Algorithm â€£ 5 Two Alternative Architectures â€£ Machine-learning a family of solutions to an optimal pension investment problem")) shows the same comparison but between the RNN from the â€˜one-stepâ€™ algorithm and the fixed parameter network.

Figure [4a](https://arxiv.org/html/2511.07045v1#S5.F4.sf1 "In Figure 4 â€£ 5.1 A Two-Step Iterative Algorithm â€£ 5 Two Alternative Architectures â€£ Machine-learning a family of solutions to an optimal pension investment problem") demonstrates the improved ability of the network to generalize across a wide range of parameters. The similarities between this new methodâ€™s strategies and the fixed parameter RNNâ€™s strategies are corroborated by Figure [11](https://arxiv.org/html/2511.07045v1#A2.F11 "Figure 11 â€£ Appendix B Strategy Plots to Match Outcome Plots â€£ Machine-learning a family of solutions to an optimal pension investment problem") in the Appendix. Notably, the differences in strategy are largest in the later years, where survival probabilities are minimal, and hence the contribution to the loss function is negligible. Furthermore, the expected utility obtained by the varying-parameter network is within 1% of a standard error of the value obtained by the fixed-parameter network for this default set of parameters. Since training a number of fixed parameter networks would be time consuming, we only tested on two other parameter combinations, the edge cases, where the optimal solutions differ the most. Specifically, these cases are described by an individual who is more easily satiated and highly risk averse111(Î±,Ï,a)=(0.2,âˆ’2,0.4)(\alpha,\rho,a)=(0.2,-2,0.4), or an individual who is less easily satiated and is almost risk neutral222(Î±,Ï,a)=(5Ã—10âˆ’5,âˆ’0.1,0.4)(\alpha,\rho,a)=(5\times 10^{-5},-0.1,0.4). The varying-parameter network was also within 1% under these parameters.

### 5.2 A One-Step Algorithm

The second method we introduce also leverages multiple neural networks to scale the loss function, but, unlike the two-step iterative approach, the networks are trained simultaneously, side-by-side. This allows the main RNN to learn the optimal strategy across the entire range of parameter values far more efficiently.

Again, the main difficulty here is estimating the standard deviation of the loss function for a given set of parameter values, to obtain the appropriate scaling for the loss function. To address this, we introduce two auxiliary networks. The first network, the â€˜mean-estimating networkâ€™, estimates the expectation of the main loss function conditional on parameter values and the strategy produced by the main RNN. The second, the â€˜scaling networkâ€™, predicts the conditional variance of the loss, using the error from the mean estimate as input to its loss function. We then use this scaling factor exactly as before in ([6](https://arxiv.org/html/2511.07045v1#S5.E6 "In 5.1 A Two-Step Iterative Algorithm â€£ 5 Two Alternative Architectures â€£ Machine-learning a family of solutions to an optimal pension investment problem")) to scale the main RNNâ€™s loss.

The mean network is trained by minimizing the average squared difference between the utility from each scenario and its predicted mean, Î¼Î¸â€‹(p)\mu\_{\theta}(p). The scaling network is then trained to predict the variance by minimizing the squared difference between these squared errors and its output, ÏƒÎ¸2â€‹(p)\sigma^{2}\_{\theta}(p). For numerical stability, we again use logarithms, taking the outputs of both networks as the logarithms of the mean and variance. We therefore obtain the following loss functions:

|  |  |  |  |
| --- | --- | --- | --- |
|  | LÎ¼â€‹(p)\displaystyle L\_{\mu}(p) | =logâ¡(âˆ‘s=1Nexpâ¡(logâ¡((expâ¡(vâ€‹(p,s))âˆ’expâ¡(logâ¡(Î¼Î¸â€‹(p))))2)âŸds2â€‹Â (log squared differences)))âˆ’logâ¡(N),\displaystyle=\log\Bigg(\sum\_{s=1}^{N}\exp\bigg(\underbrace{\log\bigg(\big(\exp(v(p,s))-\exp(\log(\mu\_{\theta}(p)))\big)^{2}\bigg)}\_{d\_{s}^{2}\text{ (log squared differences)}}\bigg)\Bigg)-\log(N), |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =logâ¡(1Nâ€‹âˆ‘s=1N(expâ¡(vâ€‹(p,s))âˆ’Î¼Î¸â€‹(p))2),\displaystyle=\log\Bigg(\frac{1}{N}\sum\_{s=1}^{N}\big(\exp(v(p,s))-\mu\_{\theta}(p)\big)^{2}\Bigg), |  |

and

|  |  |  |  |
| --- | --- | --- | --- |
|  | LÏƒâ€‹(p)\displaystyle L\_{\sigma}(p) | =logâ¡(âˆ‘s=1Nexpâ¡(logâ¡((expâ¡(ds2)âˆ’expâ¡(logâ¡(ÏƒÎ¸2â€‹(p))))2)))âˆ’logâ¡(N),\displaystyle=\log\Bigg(\sum\_{s=1}^{N}\exp\bigg(\log\bigg(\big(\exp(d\_{s}^{2})-\exp(\log(\sigma\_{\theta}^{2}(p)))\big)^{2}\bigg)\bigg)\Bigg)-\log(N), |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =logâ¡(1Nâ€‹âˆ‘s=1N(expâ¡(ds2)âˆ’ÏƒÎ¸2â€‹(p))2),\displaystyle=\log\Bigg(\frac{1}{N}\sum\_{s=1}^{N}\big(\exp(d\_{s}^{2})-\sigma^{2}\_{\theta}(p)\big)^{2}\Bigg), |  |

where vâ€‹(p,s)v(p,s) is defined in ([5](https://arxiv.org/html/2511.07045v1#S5.E5 "In 5.1 A Two-Step Iterative Algorithm â€£ 5 Two Alternative Architectures â€£ Machine-learning a family of solutions to an optimal pension investment problem")). For
numerical stability, the squared differences are computed using the identity

|  |  |  |
| --- | --- | --- |
|  | logâ¡((eaâˆ’eb)2)=2â€‹[maxâ¡(a,b)+log1â€‹pâ¡(âˆ’eminâ¡(a,b)âˆ’maxâ¡(a,b))]\log\left((e^{a}-e^{b})^{2}\right)=2\left[\,\max(a,b)+\log\_{1\text{p}}\bigl(-e^{\min(a,b)-\max(a,b)}\bigr)\right] |  |

where log1â€‹pâ¡(x):=logâ¡(1+x)\log\_{1\text{p}}(x):=\log(1+x).

The training procedure jointly optimizes all three networks using the mean and scaling network losses defined above as well as the main RNNâ€™s scaled loss, analogous to ([6](https://arxiv.org/html/2511.07045v1#S5.E6 "In 5.1 A Two-Step Iterative Algorithm â€£ 5 Two Alternative Architectures â€£ Machine-learning a family of solutions to an optimal pension investment problem")). At each step of training (each gradient update), all three networks make a prediction given the dataset and parameters. The main RNNâ€™s consumption and investment strategy, along with the scaling network output, logâ¡(ÏƒÎ¸2â€‹(p))\log(\sigma\_{\theta}^{2}(p)), are used as input into the main RNNâ€™s (scaled) loss function. The two auxiliary networksâ€™ losses are computed using the (unscaled) log utilities from the main RNN, as per the loss functions LÎ¼â€‹(p)L\_{\mu}(p) and LÏƒâ€‹(p)L\_{\sigma}(p) respectively. Finally, all three networksâ€™ parameters are updated using the corresponding gradients from their loss functions. The full training procedure is given in Algorithm [1](https://arxiv.org/html/2511.07045v1#alg1 "Algorithm 1 â€£ 5.2 A One-Step Algorithm â€£ 5 Two Alternative Architectures â€£ Machine-learning a family of solutions to an optimal pension investment problem"):

Algorithm 1  Joint Optimization Training Procedure

Initialize: dataset ğ’Ÿ\mathcal{D}, parameters ğ’«\mathcal{P}, network parameters Î¸RNN,Î¸Î¼,Î¸Ïƒ\theta\_{\text{RNN}},\theta\_{\mu},\theta\_{\sigma}, number of epochs EE, and split ğ’Ÿ\mathcal{D} into batches BB

for epoch in range EE do

for batch in BB do

1. 1.

   Sample scenarios sâˆˆğ’Ÿs\in\mathcal{D} and parameters pâˆˆğ’«p\in\mathcal{P} from the batch
2. 2.

   (Ï€tÎ¸,ctÎ¸)t=1Tâ†RNNâ€‹(s,p)(\pi\_{t}^{\theta},c\_{t}^{\theta})\_{t=1}^{T}\leftarrow\text{RNN}(s,p)
3. 3.

   logâ¡(Î¼Î¸â€‹(p))â†MeanNNâ€‹(p)\log(\mu\_{\theta}(p))\leftarrow\text{MeanNN}(p)
4. 4.

   logâ¡(ÏƒÎ¸2â€‹(p))â†ScalingNNâ€‹(p)\log(\sigma\_{\theta}^{2}(p))\leftarrow\text{ScalingNN}(p)
5. 5.

   Compute wealth process from (Ï€tÎ¸,ctÎ¸)t=1T(\pi\_{t}^{\theta},c\_{t}^{\theta})\_{t=1}^{T} to get consumption amounts (Ct)t=tRAT(C\_{t})\_{t=t\_{\text{RA}}}^{T} in retirement.
6. 6.

   Input the consumption amounts and the scaling factor into the scaled RNN loss â„“â€‹(p,ÏƒÎ¸â€‹(p))\ell(p,\sigma\_{\theta}(p)), and store unscaled log utilities vâ€‹(p,s)v(p,s)
7. 7.

   Compute mean-estimating network loss LÎ¼â€‹(p)L\_{\mu}(p) using stored vâ€‹(p,s)v(p,s) and store log squared differences ds2d^{2}\_{s}
8. 8.

   Compute scaling network loss LÏƒâ€‹(p)L\_{\sigma}(p) using stored log squared differences ds2d^{2}\_{s}
9. 9.

   Update Î¸RNN,Î¸Î¼,Î¸Ïƒ\theta\_{\text{RNN}},\theta\_{\mu},\theta\_{\sigma} using gradients

end for

end for

In this method, training is performed across the full parameter range, equivalent to ğ’«3\mathcal{P}\_{3}, removing the need for iterations through larger parameter sets to improve accuracy. This increases the efficiency of the algorithm, requiring at most a fifth of the runtime that the two-step iterative algorithm requires.

Figure [4b](https://arxiv.org/html/2511.07045v1#S5.F4.sf2 "In Figure 4 â€£ 5.1 A Two-Step Iterative Algorithm â€£ 5 Two Alternative Architectures â€£ Machine-learning a family of solutions to an optimal pension investment problem") illustrates the success of the training procedure. We can also see the similarities in the consumption and investment strategies in Figure [12](https://arxiv.org/html/2511.07045v1#A2.F12 "Figure 12 â€£ Appendix B Strategy Plots to Match Outcome Plots â€£ Machine-learning a family of solutions to an optimal pension investment problem"). This method produces a network that achieves a utility within 1% of a standard error of the fixed network for these default parameters. The network also achieves an expected utility of within 1% of a standard error of the fixed parameter network for the two edge cases described previously. Not only does this show the success of this architecture, but it also shows that it is at least as effective as the two-step iterative approach and is more efficient in reaching accurate solutions.

## 6 Allowing for Real-Time Comparisons

The primary objective of training this varying-parameter network was to enable real-time comparisons of retirement outcomes, allowing users to explore and select their preferred investment and consumption strategies.

Although the varying-parameter networks generate predictions over the full parameter range without requiring further training, it remains time-consuming to simulate sufficient stratetgy outcomes to produce a fan diagram of outcomes. To address this issue, we train an additional feed-forward neural network, referred to as the â€˜replacement-ratio percentileâ€™ network. This network approximates the mapping from percentile, input parameters, and time point to the corresponding replacement ratio, as predicted by the principal RNN. Specifically, let â„:={1,2,â€¦,9}\mathcal{I}:=\{1,2,\ldots,9\} denote the set of percentiles. Then the replacement-ratio percentile network learns the mapping

|  |  |  |
| --- | --- | --- |
|  | g:ğ’«Ã—â„Ã—ğ’¯â†’â„,g:\mathcal{P}\times\mathcal{I}\times\mathcal{T}\to\mathbb{R}, |  |

which returns the value of the ii-th percentile at time tt, corresponding to the replacement ratio produced by the optimal strategy obtained from the principal network. The training was performed in a supervised environment and the dataset was generated by randomly sampling 50,00050,000 combinations of parameters and passing them through the principal RNN to obtain the corresponding nine percentiles for each input. Each percentile contains 5656 time points to account for each year from retirement until the last time point an individual may still be alive. We can see in Figure [5](https://arxiv.org/html/2511.07045v1#S6.F5 "Figure 5 â€£ 6 Allowing for Real-Time Comparisons â€£ Machine-learning a family of solutions to an optimal pension investment problem") the accuracy of this network in mimicking the principal network.

![Refer to caption](varying_parameter_paper/graph_network_plot.png)

Figure 5: Retirement outcomes for the replacement-ratio percentile neural network compared to the main RNN.

The replacement-ratio percentile network produces an almost identical output to the principal RNN. The replacement-ratio percentile network is also able to produce the plot in approximately 1/3 of a second, more than 10 times faster than recalculating by Monte Carlo. This allowed us to create
a far more interactive and responsive user interface.

## 7 Sensitivity Analysis of Optimal Strategies

The ability to learn the optimal strategy across a range of parameter values allows us to perform sensitivity analysis with respect to the parameters. Unless otherwise stated, all parameters are kept constant as specified in Table [1](https://arxiv.org/html/2511.07045v1#S2.T1 "Table 1 â€£ 2 Training the Fixed Parameter Network â€£ Machine-learning a family of solutions to an optimal pension investment problem"). We will analyse the outcomes in retirement and the strategies themselves. The plots for outcomes will be included in this section, but the full plots of the strategies will be included in Appendix [B](https://arxiv.org/html/2511.07045v1#A2 "Appendix B Strategy Plots to Match Outcome Plots â€£ Machine-learning a family of solutions to an optimal pension investment problem").

We begin by examining how retirement outcomes vary with the risk-aversion parameter, Î±\alpha. Figures [6a](https://arxiv.org/html/2511.07045v1#S7.F6.sf1 "In Figure 6 â€£ 7 Sensitivity Analysis of Optimal Strategies â€£ Machine-learning a family of solutions to an optimal pension investment problem") & [6b](https://arxiv.org/html/2511.07045v1#S7.F6.sf2 "In Figure 6 â€£ 7 Sensitivity Analysis of Optimal Strategies â€£ Machine-learning a family of solutions to an optimal pension investment problem") below illustrate how the outcomes differ relative to this parameter. Note that the strategies we have been examining so far are already quite risky, so the low risk aversion strategies we look at now are not drastically different.

![Refer to caption](varying_parameter_paper/high_risk_aversion_plot.png)


a

![Refer to caption](varying_parameter_paper/low_risk_aversion_plot.png)


b

Figure 6: Comparison of retirement outcomes for different values of Î±\alpha. We have subfigure ([6a](https://arxiv.org/html/2511.07045v1#S7.F6.sf1 "In Figure 6 â€£ 7 Sensitivity Analysis of Optimal Strategies â€£ Machine-learning a family of solutions to an optimal pension investment problem")) with Î±=0.01\alpha=0.01 (high risk aversion) and subfigure ([6b](https://arxiv.org/html/2511.07045v1#S7.F6.sf2 "In Figure 6 â€£ 7 Sensitivity Analysis of Optimal Strategies â€£ Machine-learning a family of solutions to an optimal pension investment problem")) with Î±=10âˆ’7\alpha=10^{-7} (low risk aversion).

As expected, increasing the risk aversion parameter leads to more conservative investment behavior during the accumulation phase. Specifically, individuals with higher risk aversion allocate a lower proportion to the risky asset, resulting in narrower spreads between replacement-ratio percentiles. This conservative strategy typically yields lower replacement ratios at retirement but provides greater protection in adverse market scenarios. In terms of consumption strategies, less risk-averse individuals tend to consume more, reflecting a reduced concern for the depletion of their funds. On the contrary, the more risk averse individuals fear the prospect of running out of funds and so are less likely to consume as much in retirement.

We can also consider the affect the satiation parameter, Ï\rho, has on retirement outcomes. Figures [7a](https://arxiv.org/html/2511.07045v1#S7.F7.sf1 "In Figure 7 â€£ 7 Sensitivity Analysis of Optimal Strategies â€£ Machine-learning a family of solutions to an optimal pension investment problem") & [7b](https://arxiv.org/html/2511.07045v1#S7.F7.sf2 "In Figure 7 â€£ 7 Sensitivity Analysis of Optimal Strategies â€£ Machine-learning a family of solutions to an optimal pension investment problem") provide an illustration.

![Refer to caption](varying_parameter_paper/more_easily_satiated_plot.png)


a

![Refer to caption](varying_parameter_paper/less_easily_satiated_plot.png)


b

Figure 7: Comparison of retirement outcomes for different values of Ï\rho. We have Subfigure ([7a](https://arxiv.org/html/2511.07045v1#S7.F7.sf1 "In Figure 7 â€£ 7 Sensitivity Analysis of Optimal Strategies â€£ Machine-learning a family of solutions to an optimal pension investment problem")) with Ï=âˆ’2.0\rho=-2.0 (more easily satiated) and Subfigure ([7b](https://arxiv.org/html/2511.07045v1#S7.F7.sf2 "In Figure 7 â€£ 7 Sensitivity Analysis of Optimal Strategies â€£ Machine-learning a family of solutions to an optimal pension investment problem")) with Ï=âˆ’0.1\rho=-0.1 (less easily satiated).

Note that Ï=âˆ’2.0\rho=-2.0 is the default value for the parameter in the strategies we have looked at before. Again, the figures show that the outcomes follow an intuitive pattern. When Ï=âˆ’2.0\rho=-2.0, the individual becomes satiated much more quickly. As a result, an individual will reduce their investment in the risky asset during the accumulation period in comparison, as they do not seek such a high replacement ratio in retirement. This is what leads to the tighter percentiles that can be seen in Figure [7a](https://arxiv.org/html/2511.07045v1#S7.F7.sf1 "In Figure 7 â€£ 7 Sensitivity Analysis of Optimal Strategies â€£ Machine-learning a family of solutions to an optimal pension investment problem"), and by the opposite reasoning, why the looser percentiles can be seen in Figure [7b](https://arxiv.org/html/2511.07045v1#S7.F7.sf2 "In Figure 7 â€£ 7 Sensitivity Analysis of Optimal Strategies â€£ Machine-learning a family of solutions to an optimal pension investment problem"). Within the consumption strategy, Ï=âˆ’2.0\rho=-2.0 induces reduced overall consumption throughout retirement, since the individual is more satisfied with the amount they are consuming. In contrast, when Ï=âˆ’0.1\rho=-0.1, the individual remains far from satiated and therefore tends to seek to consume more throughout retirement.

The impact of the adequacy parameter is much harder to see than the other two parameters. To understand its effect, we examine the behaviour with extremely high risk aversion (Î±=0.2)(\alpha=0.2). Let VadequateV^{\text{adequate}} be the level of funds at retirement that allow an individual to to consume at the adequacy level for the whole of their retirement if they invest in risk-free bonds. If a highly risk-averse individual has more than VadequateV^{\text{adequate}} at retirement, their strategy will approximate consuming all funds above VadequateV^{\text{adequate}} in the first year and consume at the adequacy level thereafter. This is because this is the only risk-free strategy available. If they have less than VadequateV^{\text{adequate}} funds, the situation is reversed. An individual who is risk averse will choose to reduce consumption initially below the level they could sustain for the whole of retirement in order to reduce their risk levels later in retirement.
This is illustrated in
Figure [8](https://arxiv.org/html/2511.07045v1#S7.F8 "Figure 8 â€£ 7 Sensitivity Analysis of Optimal Strategies â€£ Machine-learning a family of solutions to an optimal pension investment problem"), where we deploy the provably convergent numerical scheme outlined in Section [3](https://arxiv.org/html/2511.07045v1#S3 "3 Verification of the optimal strategy â€£ Machine-learning a family of solutions to an optimal pension investment problem").

![Refer to caption](varying_parameter_paper/c++_high_risk_high_wealth_adequacy_plot.png)


a

![Refer to caption](varying_parameter_paper/c++_high_risk_low_wealth_adequacy_plot.png)


b

Figure 8: Comparison of retirement outcomes using the provably convergent numerical method from Section [3](https://arxiv.org/html/2511.07045v1#S3 "3 Verification of the optimal strategy â€£ Machine-learning a family of solutions to an optimal pension investment problem"), for highly risk averse individuals in decumulation only. We have Subfigure ([8a](https://arxiv.org/html/2511.07045v1#S7.F8.sf1 "In Figure 8 â€£ 7 Sensitivity Analysis of Optimal Strategies â€£ Machine-learning a family of solutions to an optimal pension investment problem")) with high initial wealth and Subfigure ([8b](https://arxiv.org/html/2511.07045v1#S7.F8.sf2 "In Figure 8 â€£ 7 Sensitivity Analysis of Optimal Strategies â€£ Machine-learning a family of solutions to an optimal pension investment problem")) with low initial wealth.

This explains the importance of adequacy in the decumulation phase. If one considers accumulation, as we have done throughout this dissertation, a highly risk averse individual will invest almost entirely in the risk-free asset during accumulation. This results in an essentially deterministic level of income at retirement, and their consumption throughout retirement will again be determined by whether this is greater or less than VadequateV^{\text{adequate}}.

When we view the optimal strategies computed using machine learning, this pattern is somewhat obscured by the fact it is very difficult to train the network to find the optimal strategy over the age of about 100 as consumption after this age has only a negligible effect upon utility. The optimal strategies computed using machine learning for decumulation-only are shown in panels ([9a](https://arxiv.org/html/2511.07045v1#S7.F9.sf1 "In Figure 9 â€£ 7 Sensitivity Analysis of Optimal Strategies â€£ Machine-learning a family of solutions to an optimal pension investment problem")) and ([9b](https://arxiv.org/html/2511.07045v1#S7.F9.sf2 "In Figure 9 â€£ 7 Sensitivity Analysis of Optimal Strategies â€£ Machine-learning a family of solutions to an optimal pension investment problem")) in Figure [9](https://arxiv.org/html/2511.07045v1#S7.F9 "Figure 9 â€£ 7 Sensitivity Analysis of Optimal Strategies â€£ Machine-learning a family of solutions to an optimal pension investment problem") and the corresponding accumulation problem in panel ([9c](https://arxiv.org/html/2511.07045v1#S7.F9.sf3 "In Figure 9 â€£ 7 Sensitivity Analysis of Optimal Strategies â€£ Machine-learning a family of solutions to an optimal pension investment problem")). Notice that both decumulation-only strategies differ from the optimum shown in Figure [8](https://arxiv.org/html/2511.07045v1#S7.F8 "Figure 8 â€£ 7 Sensitivity Analysis of Optimal Strategies â€£ Machine-learning a family of solutions to an optimal pension investment problem"). The utility values of the low initial wealth problem are equal within 1%, but the machine learning solution to the high initial wealth problem is not as close to the other numerical method. This is due to the fact that we are using extreme parameters and the gain function becomes hard to compute numerically in these regions. In the accumulation problem, an individual with this level of risk aversion never reaches a wealth above VadequateV^{\text{adequate}}. As a result, we do not observe the pattern of high initial consumption followed by consumption at the adequacy level. If the individualâ€™s contribution rate (or salary) were sufficiently high, wealth would exceed VadequateV^{\text{adequate}} and this behaviour would then emerge. Such contribution rates are, however, somewhat unrealistic in practice.

![Refer to caption](varying_parameter_paper/NN_high_initial_wealth_adequacy_plot.png)


a

![Refer to caption](varying_parameter_paper/NN_low_initial_wealth_adequacy_plot.png)


b

![Refer to caption](varying_parameter_paper/NN_accumulation_adequacy_plot.png)


c

Figure 9: Comparison of retirement outcomes when using the neural network for highly risk averse individuals. For the decumulation only setting, we have Subfigure ([9a](https://arxiv.org/html/2511.07045v1#S7.F9.sf1 "In Figure 9 â€£ 7 Sensitivity Analysis of Optimal Strategies â€£ Machine-learning a family of solutions to an optimal pension investment problem")) with high initial wealth and Subfigure ([9b](https://arxiv.org/html/2511.07045v1#S7.F9.sf2 "In Figure 9 â€£ 7 Sensitivity Analysis of Optimal Strategies â€£ Machine-learning a family of solutions to an optimal pension investment problem")) with low initial wealth. For an accumulation problem, we have Subfigure ([9c](https://arxiv.org/html/2511.07045v1#S7.F9.sf3 "In Figure 9 â€£ 7 Sensitivity Analysis of Optimal Strategies â€£ Machine-learning a family of solutions to an optimal pension investment problem")).

If we use realistic values for the risk-aversion parameter, the impact of the adequacy parameter becomes much harder to see. When realistic parameter values are chosen, adjusting the parameter has little visual impact on the optimal strategy. Both the choice of risk-aversion and the choice of adequacy break the positive homogeneity of the problem and it seems that one can use either varying adequacy levels or exponential risk-aversion to avoid the unreasonable strategies found in [[2](https://arxiv.org/html/2511.07045v1#bib.bib2)] using homogeneous Epsteinâ€“Zin preferences.

## 8 Conclusions

We have created a tool which allows pensions to be designed by choosing a family of loss functions and then tuning the
parameters interactively.

We have shown how to overcome the issue of the high variance of an exponential utility function for varied parameters when learning the optimal control in a given setting. We expect
that this architecture could be re-used for more general loss functions and richer economic models.

## References

* [1]

  S.Â N. Afriat.
  The construction of utility functions from expenditure data.
  International Economic Review, 8(1):67â€“77, 1967.
* [2]

  J.Â Armstrong, C.Â Buescu, and J.Â Dalby.
  Optimal post-retirement investment and consumption under longevity risk in collective funds.
  Scandinavian Actuarial Journal, 2025.
* [3]

  John Armstrong.
  Classifying markets up to isomorphism.
  arXiv preprint arXiv:1810.03546, 2018.
* [4]

  John Armstrong and Cristin Buescu.
  Collectivised pension investment with exponential kihlstromâ€“mirman preferences, 2019.
* [5]

  John Armstrong, James Dalby, and Rohan Hobbs.
  Collective defined contribution schemes without intergenerational cross-subsidies, 2025.
* [6]

  John Armstrong and JamesÂ Luke Dalby.
  Optimal mutual insurance against systematic longevity risk.
  Scandinavian Actuarial Journal, 0(0):1â€“19, 2025.
* [7]

  Ravi Bansal.
  Long-run risks and financial markets.
  Review, 89(Jul):283â€“300, None 2007.
* [8]

  Ravi Bansal and Amir Yaron.
  Risks for the long run: A potential resolution of asset pricing puzzles.
  The Journal of Finance, 59(4):1481â€“1509, 2004.
* [9]

  Luca Benzoni, Pierre Collin-Dufresne, and RobertÂ S. Goldstein.
  Explaining asset pricing puzzles associated with the 1987 market crash.
  Journal of Financial Economics, 101(3):552â€“573, September 2011.
* [10]

  Thomas Bernhardt and Catherine Donnelly.
  Modern tontine with bequest: Innovation in pooled annuity products.
  Insurance: Mathematics and Economics, 86:168â€“188, 2019.
* [11]

  DavidÂ P. Blake, Mel Duffield, Ian Tonks, Alistair Haig, Dean Blower, and Laura MacPhee.
  Grouping individual investment preferences in retirement savings: A cluster analysis of a USS members risk attitude survey.
  Discussion Paper PI-2003, Pensions Institute, City, University of London, February 2020.
* [12]

  Antoine Bommier.
  Uncertain lifetime and intertemporal choice: risk aversion as a rationale for time discounting.
  International Economic Review, 47(4):1223â€“1246, 2006.
* [13]

  Alexander M.Â G. Cox, David Hobson, and Jan Obloj.
  Utility theory front to back - inferring utility from agentsâ€™ choices, 2012.
* [14]

  Marta Grzeskiewicz.
  Uncovering utility functions from observed outcomes, 2025.
* [15]

  Jiequn Han etÂ al.
  Deep learning approximation for stochastic control problems.
  arXiv preprint arXiv:1611.07422, 2016.
* [16]

  Martin Herdegen, David Hobson, and Joseph Jerome.
  An elementary approach to the merton problem.
  Mathematical Finance, 31(4):1218â€“1239, 2021.
* [17]

  Martin Herdegen, David Hobson, and Joseph Jerome.
  The infinite-horizon investment-consumption problem for epstein-zin stochastic differential utility. ii.
  Finance and Stochastics, 27(1):159â€“188, January 2023.
* [18]

  Martin Herdegen, DavidÂ G. Hobson, and Joseph Jerome.
  The infinite horizon investment-consumption problem for epstein-zin stochastic differential utility. i : Foundations.
  Finance and Stochastics, 27:127â€“158, January 2023.
* [19]

  Ruimeng Hu and Mathieu Lauriere.
  Recent developments in machine learning methods for stochastic control and games.
  arXiv preprint arXiv:2303.10257, 2023.
* [20]

  CÃ´me HurÃ©, HuyÃªn Pham, Achref Bachouch, and Nicolas LangrenÃ©.
  Deep neural networks algorithms for stochastic control problems on finite horizon: convergence analysis.
  SIAM Journal on Numerical Analysis, 59(1):525â€“557, 2021.
* [21]

  Holger Kraft, Thomas Seiferling, and FrankÂ Thomas Seifried.
  Optimal consumption and investment with epsteinâ€“zin recursive utility.
  Finance and Stochastics, 21(1):187â€“226, 2017.
* [22]

  DavidÂ M Kreps and EvanÂ L Porteus.
  Temporal resolution of uncertainty and dynamic choice theory.
  Econometrica, pages 185â€“200, 1978.
* [23]

  L.Â Leal, M.Â Lauriere, and C.-A. Lehalle.
  Learning a functional control for high-frequency finance.
  Quantitative Finance, 22(11):1973â€“1987, 2022.
* [24]

  DavidÂ G Luenberger.
  Optimization by vector space methods.
  John Wiley & Sons, 1997.
* [25]

  MosheÂ A. Milevsky.
  King Williamâ€™s Tontine: Why the Retirement Annuity of the Future Should Resemble its Past.
  Cambridge University Press, 2015.
* [26]

  MosheÂ A. Milevsky and ThomasÂ S. Salisbury.
  Optimal retirement income tontines.
  Insurance: Mathematics and Economics, 64:91â€“105, 2015.
* [27]

  RalphÂ Tyrell Rockafellar.
  Convex Analysis.
  Princeton University Press, 2015.
* [28]

  P.Â A. Samuelson.
  A note on the pure theory of consumerâ€™s behaviour.
  Economica, 5(17):61â€“71, 1938.
* [29]

  MichaelÂ Z Stamos.
  Optimal consumption and portfolio choice for pooled annuity funds.
  Insurance: Mathematics and Economics, 43(1):56â€“68, 2008.
* [30]

  HalÂ R. Varian.
  The nonparametric approach to demand analysis.
  Econometrica, 50(4):945â€“973, 1982.
* [31]

  John von Neumann, Oskar Morgenstern, and Ariel Rubinstein.
  Theory of Games and Economic Behavior (60th Anniversary Commemorative Edition).
  Princeton University Press, 1944.

## Appendix A Neural Network Architecture Details

Our code is written in Python using the Tensorflow package.

### A.1 RNN Architecture

The main RNNâ€™s used in the fixed-parameter method, and both the one and two step iterative methods are identical aside from the inputs. We mark in brackets the additional inputs to the varying-parameter networks. The networks consist of six layers:

* â€¢

  The first input layer has two (five) nodes, representing: the Gaussian increments and the time points (and the three parameter values).
* â€¢

  The second layer is a dense layer with 8080 nodes and a ReLU activation function.
* â€¢

  The third layer is a gated recurrent unit (GRU) with 2525 nodes. This is the recurrent layer in our network. The activation function is the hyperbolic tangent function and the recurrent activation function is sigmoid. The GRU ensures that the network returns an output at each time point.
* â€¢

  The fourth and fifth layers are identical to the second layer.
* â€¢

  The final output layer has two nodes, which represent the proportion of wealth to invest in the risky asset and the proportion of wealth to consume. We use the linear activation function for both outputs, and perform a transformation of the consumption proportion so that it remains in the interval [0,1][0,1]. We obtain investment and consumption decisions in each year of our simulation as a result of the GRU.

We used the Adam optimizer with an initial learning rate of 0.0010.001. Each time, training was carried out over 500500 epochs, each consisting of 131,072131,072 scenarios with a batch size of 4,0964,096. A validation set of 10,00010,000 separately generated scenarios was evaluated at the end of each epoch. We use a large number of epochs and simply extract the weights for which the validation and training loss was least.

### A.2 The Scaling Network

The â€˜scalingâ€™ network in both methods is a feedforward neural network with a much simpler architecture. The mean estimating network in the one-step iterative method also has the same architecture. They consist of an input layer of three nodes, that takes the three parameter values and three hidden layers each with 6464 nodes. All of the hidden layers use the ReLU activation function. The output layer has one node, representing predictions for standard deviation (mean in the case of the mean estimating network) of the loss function. The output layer uses the linear activation function since we take the scaling factors as logarithms.

For the one-step method, these networks are trained in the same loop as the main RNN, and so have the same training parameters as the main RNN.

For the two-step iterative method, training consisted of 100100 epochs, where the training data was 80%80\% of the 50,00050,000 data points and the validation data was the remainder. We use a batch size of 1,0001,000 and the mean-square-error loss function. We use the Adam optimizer with an initial learning rate of 0.0010.001. We break up the parameter sets for each iteration. The smallest parameter ranges ğ’«0=ğ’«1\mathcal{P}\_{0}=\mathcal{P}\_{1}, used for training the first â€˜scalingâ€™ network, is given by Table [2](https://arxiv.org/html/2511.07045v1#A1.T2 "Table 2 â€£ A.2 The Scaling Network â€£ Appendix A Neural Network Architecture Details â€£ Machine-learning a family of solutions to an optimal pension investment problem").

| Parameters | Min | Max |
| --- | --- | --- |
| Î±\alpha | 10âˆ’510^{-5} | 10âˆ’410^{-4} |
| Ï\rho | âˆ’2-2 | âˆ’1-1 |
| aa | 0.20.2 | 0.70.7 |

Table 2: The smaller parameter range used to train the first iteration scaling network in the two-step iterative algorithm.

The parameter range ğ’«2\mathcal{P}\_{2}, used for training the second â€˜scalingâ€™ network, is given in Table [3](https://arxiv.org/html/2511.07045v1#A1.T3 "Table 3 â€£ A.2 The Scaling Network â€£ Appendix A Neural Network Architecture Details â€£ Machine-learning a family of solutions to an optimal pension investment problem").

| Parameters | Min | Max |
| --- | --- | --- |
| Î±\alpha | 10âˆ’610^{-6} | 10âˆ’310^{-3} |
| Ï\rho | âˆ’2-2 | âˆ’0.1-0.1 |
| aa | 0.20.2 | 1.01.0 |

Table 3: The second parameter range used to train the second scaling network in the two-step iterative algorithm.

The full parameter range ğ’«3\mathcal{P}\_{3}, used for training the third â€˜scalingâ€™ network, is given in Table [4](https://arxiv.org/html/2511.07045v1#A1.T4 "Table 4 â€£ A.2 The Scaling Network â€£ Appendix A Neural Network Architecture Details â€£ Machine-learning a family of solutions to an optimal pension investment problem").

| Parameters | Min | Max |
| --- | --- | --- |
| Î±\alpha | 10âˆ’710^{-7} | 10âˆ’210^{-2} |
| Ï\rho | âˆ’2-2 | âˆ’0.1-0.1 |
| aa | 0.10.1 | 1.01.0 |

Table 4: The third parameter range used to train the third scaling network in the two-step iterative algorithm.

### A.3 Replacement Ratio Percentile Network

The replacement ratio percentile network is a feedforward neural network. It consists of an input layer with five nodes, to represent the time point, the percentile and the three varying parameters. So the replacement ratio percentile network learns the value of the replacement ratio for a given time point, in a given percentile, for a given set of parameters. There are three hidden layers consisting of 64 nodes, all with the ReLU activation function. The output layer has a single node as the network only makes one prediction per data point. The output layer is governed by the sigmoid activation function since we transform both inputs and outputs to the interval [0,1][0,1].

We follow the same training procedure as with the â€˜scalingâ€™ network in the two-step iterative approach, but note that we obtain a larger data set since each data point consists of nine percentiles, each 5656 time points long. This therefore means we only need 5050 epochs to find the minimum.

## Appendix B Strategy Plots to Match Outcome Plots

Here, we show the strategy produced by the respective networks to produce the outcome plots in the main text of the paper.

![Refer to caption](varying_parameter_paper/fixed_parameter_consumption_proportion.png)


a

![Refer to caption](varying_parameter_paper/fixed_parameter_investment_proportion.png)


b

Figure 10: Panel ([10a](https://arxiv.org/html/2511.07045v1#A2.F10.sf1 "In Figure 10 â€£ Appendix B Strategy Plots to Match Outcome Plots â€£ Machine-learning a family of solutions to an optimal pension investment problem")) shows the consumption strategy for the outcomes plotted in Figure [2](https://arxiv.org/html/2511.07045v1#S2.F2 "Figure 2 â€£ 2 Training the Fixed Parameter Network â€£ Machine-learning a family of solutions to an optimal pension investment problem"), for the fixed-parameter RNN. Panel ([10b](https://arxiv.org/html/2511.07045v1#A2.F10.sf2 "In Figure 10 â€£ Appendix B Strategy Plots to Match Outcome Plots â€£ Machine-learning a family of solutions to an optimal pension investment problem")) shows the corresponding investment strategy.



![Refer to caption](varying_parameter_paper/comaprison_strategies_varying_with_fixed_plot.png)

Figure 11: Consumption and investment strategies for the outcomes plotted in Figure [4a](https://arxiv.org/html/2511.07045v1#S5.F4.sf1 "In Figure 4 â€£ 5.1 A Two-Step Iterative Algorithm â€£ 5 Two Alternative Architectures â€£ Machine-learning a family of solutions to an optimal pension investment problem"), for the two-step iterative approach as compared to the fixed network.



![Refer to caption](varying_parameter_paper/comparison_simultaneous_scaling_strategies.png)

Figure 12: Consumption and investment strategies for the outcomes plotted in Figure [4b](https://arxiv.org/html/2511.07045v1#S5.F4.sf2 "In Figure 4 â€£ 5.1 A Two-Step Iterative Algorithm â€£ 5 Two Alternative Architectures â€£ Machine-learning a family of solutions to an optimal pension investment problem"), for the one-step approach as compared to the fixed network.



![Refer to caption](varying_parameter_paper/strategy_plots_same_y_axis/high_risk_aversion_consumption_strategy.png)


a

![Refer to caption](varying_parameter_paper/strategy_plots_same_y_axis/high_risk_aversion_investment_strategy.png)


b

Figure 13: Panel ([13a](https://arxiv.org/html/2511.07045v1#A2.F13.sf1 "In Figure 13 â€£ Appendix B Strategy Plots to Match Outcome Plots â€£ Machine-learning a family of solutions to an optimal pension investment problem")) shows the consumption strategy for the outcomes plotted in Figure [6a](https://arxiv.org/html/2511.07045v1#S7.F6.sf1 "In Figure 6 â€£ 7 Sensitivity Analysis of Optimal Strategies â€£ Machine-learning a family of solutions to an optimal pension investment problem"), for high risk aversion. Panel ([13b](https://arxiv.org/html/2511.07045v1#A2.F13.sf2 "In Figure 13 â€£ Appendix B Strategy Plots to Match Outcome Plots â€£ Machine-learning a family of solutions to an optimal pension investment problem")) shows the corresponding investment strategy.



![Refer to caption](varying_parameter_paper/strategy_plots_same_y_axis/low_risk_aversion_consumption_strategy.png)


a

![Refer to caption](varying_parameter_paper/strategy_plots_same_y_axis/low_risk_aversion_investment_strategy.png)


b

Figure 14: Panel ([14a](https://arxiv.org/html/2511.07045v1#A2.F14.sf1 "In Figure 14 â€£ Appendix B Strategy Plots to Match Outcome Plots â€£ Machine-learning a family of solutions to an optimal pension investment problem")) shows the consumption strategy for the outcomes plotted in Figure [6b](https://arxiv.org/html/2511.07045v1#S7.F6.sf2 "In Figure 6 â€£ 7 Sensitivity Analysis of Optimal Strategies â€£ Machine-learning a family of solutions to an optimal pension investment problem"), for low risk aversion. Panel ([14b](https://arxiv.org/html/2511.07045v1#A2.F14.sf2 "In Figure 14 â€£ Appendix B Strategy Plots to Match Outcome Plots â€£ Machine-learning a family of solutions to an optimal pension investment problem")) shows the corresponding investment strategy.



![Refer to caption](varying_parameter_paper/strategy_plots_same_y_axis/more_easily_satiated_consumption_strategy.png)


a

![Refer to caption](varying_parameter_paper/strategy_plots_same_y_axis/more_easily_satiated_investment_strategy.png)


b

Figure 15: Panel ([15a](https://arxiv.org/html/2511.07045v1#A2.F15.sf1 "In Figure 15 â€£ Appendix B Strategy Plots to Match Outcome Plots â€£ Machine-learning a family of solutions to an optimal pension investment problem")) shows the consumption strategy for the outcomes plotted in Figure [7a](https://arxiv.org/html/2511.07045v1#S7.F7.sf1 "In Figure 7 â€£ 7 Sensitivity Analysis of Optimal Strategies â€£ Machine-learning a family of solutions to an optimal pension investment problem"), for more easily satiated preferences. Panel ([15b](https://arxiv.org/html/2511.07045v1#A2.F15.sf2 "In Figure 15 â€£ Appendix B Strategy Plots to Match Outcome Plots â€£ Machine-learning a family of solutions to an optimal pension investment problem")) shows the corresponding investment strategy.



![Refer to caption](varying_parameter_paper/strategy_plots_same_y_axis/less_easily_satiated_consumption_strategy.png)


a

![Refer to caption](varying_parameter_paper/strategy_plots_same_y_axis/less_easily_satiated_investment_strategy.png)


b

Figure 16: Panel ([16a](https://arxiv.org/html/2511.07045v1#A2.F16.sf1 "In Figure 16 â€£ Appendix B Strategy Plots to Match Outcome Plots â€£ Machine-learning a family of solutions to an optimal pension investment problem")) shows the consumption strategy for the outcomes plotted in Figure [7b](https://arxiv.org/html/2511.07045v1#S7.F7.sf2 "In Figure 7 â€£ 7 Sensitivity Analysis of Optimal Strategies â€£ Machine-learning a family of solutions to an optimal pension investment problem"), for less easily satiated preferences. Panel ([16b](https://arxiv.org/html/2511.07045v1#A2.F16.sf2 "In Figure 16 â€£ Appendix B Strategy Plots to Match Outcome Plots â€£ Machine-learning a family of solutions to an optimal pension investment problem")) shows the corresponding investment strategy.

## Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem

Recall that the dynamics of ww are determined by equations ([1](https://arxiv.org/html/2511.07045v1#S1.E1 "In 1 Discrete-time investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")) and ([3](https://arxiv.org/html/2511.07045v1#S1.E3 "In 1 Discrete-time investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")).
We define an admissible control
to be a progressively measurable process ((Ct)tâˆˆğ’¯,(Ï€t)tâˆˆ[0,T])((C\_{t})\_{t\in{\cal T}},(\pi\_{t})\_{t\in[0,T]}) such that wtâˆ’â‰¥0w\_{t-}\geq 0
and wtâ‰¥0w\_{t}\geq 0 for all time.
We write ğ’œ{\cal A} for the set of admissible controls.

Our objective is to compute

|  |  |  |  |
| --- | --- | --- | --- |
|  | v:=sup(C,Ï€)âˆˆğ’œUâ€‹(C),v:=\sup\_{(C,\pi)\in{\cal A}}U(C), |  | (7) |

and to find (C,Ï€)(C,\pi) achieving (or if necessary, approximating) this supremum.

Our strategy is to solve the one-period problem using a duality method which will
allow us to identify the solution with minimal assumptions on the form of our utility
function. To simplify the duality argument, we use the theory of isomorphic markets to recast the problem in a particularly simple form. Having obtained this solution, we
will recursively solve the multi-period problem.
Our goal in this appendix is to give
all details needed to implement the resulting algorithm and a proof of its convergence.

### C.1 Solution to the one-period problem

Write ğ’œw,t{\cal A}\_{w,t} for the admissible consumption-investment
strategies that start with wealth ww at time tt.
Define the value function vv, as
a function of initial wealth, ww at time t1âˆˆğ’¯t\_{1}\in{\cal T} by

|  |  |  |  |
| --- | --- | --- | --- |
|  | vtâ€‹(w)\displaystyle v\_{t}(w) | :=supC,Ï€âˆˆğ’œw,tğ”¼â€‹(âˆ’expâ¡(âˆ’Î±â€‹âˆ‘jâˆˆğ’¯,tâ‰¤j<Ï„uâ€‹(Cj)â€‹Î´â€‹t))\displaystyle:=\sup\_{{C,\pi}\in{\cal A}\_{w,t}}\mathbb{E}\left(-\exp\left(-\alpha\sum\_{j\in{\cal T},\,{t\leq j<\tau}}u(C\_{j})\delta t\right)\right) |  |

To make the limits in the sum easier to read, we will write the sum
using the following integral notation

|  |  |  |  |
| --- | --- | --- | --- |
|  | vtâ€‹(w)\displaystyle v\_{t}(w) | =supC,Ï€âˆˆğ’œw,tğ”¼â€‹(âˆ’expâ¡(âˆ’Î±â€‹âˆ«tÏ„uâ€‹(Cs)â€‹dğ’¯â€‹(s))).\displaystyle=\sup\_{{C,\pi}\in{\cal A}\_{w,t}}\mathbb{E}\left(-\exp\left(-\alpha\int\_{t}^{\tau}u(C\_{s})\mathrm{d}{\cal T}(s)\right)\right). |  |

Given vtv\_{t}, we wish to compute vtâˆ’Î´â€‹tv\_{t-\delta t}, we will then be able
to recursively compute vtv\_{t} for all tâˆˆğ’¯t\in{\cal T}. Our next theorem
shows how to compute vtâˆ’Î´â€‹tv\_{t-\delta t}, but
in order to state our results concisely we first make the following definitions.

###### Definition C.1.

Let f:â„â†’â„âˆª{Â±âˆ}f:\mathbb{R}\to\mathbb{R}\cup\{\pm\infty\} be concave and increasing. Define

|  |  |  |
| --- | --- | --- |
|  | fâ€ â€‹(p):â„>0â†’â„f^{\dagger}(p):\mathbb{R}\_{>0}\to\mathbb{R} |  |

by

|  |  |  |
| --- | --- | --- |
|  | fâ€ â€‹(p)=inf{xâˆ£pâˆˆâˆ‚fâ€‹(x)}f^{\dagger}(p)=\inf\{x\mid p\in\partial f(x)\} |  |

where âˆ‚fâ€‹(x)\partial f(x) is the sub-differential of ff at xx.

For sufficiently regular
functions uu, we have fâ€ =(fâ€²)âˆ’1f^{\dagger}=(f^{\prime})^{-1}, or, equivalently, fâ€ f^{\dagger}
is the derivative of the Legendre transform of uu.

###### Definition C.2.

Define

|  |  |  |  |
| --- | --- | --- | --- |
|  | Qâ€‹(z):=Î¦â€‹(M+Î¦âˆ’1â€‹(z)),Q(z):=\Phi\left(M+\Phi^{-1}(z)\right), |  | (8) |

where Î¦\Phi is the cumulative distribution function of the
standard normal distribution and

|  |  |  |
| --- | --- | --- |
|  | M:=|Î¼âˆ’r|â€‹Î´â€‹tÏƒ.M:=\frac{\left|\mu-r\right|\sqrt{\delta t}}{\sigma}. |  |

Define

|  |  |  |  |
| --- | --- | --- | --- |
|  | qBSAâ€‹(z)=dâ€‹Qdâ€‹z.q^{A}\_{\mathrm{BS}}(z)=\frac{\mathrm{d}Q}{\mathrm{d}z}. |  | (9) |

As we will show in Lemma [C.5](https://arxiv.org/html/2511.07045v1#A3.Thmtheorem5 "Lemma C.5. â€£ C.1 Solution to the one-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem") below, the quantity qBSAâ€‹(z)q^{A}\_{\mathrm{BS}}(z) can be related to the pricing kernel of the Blackâ€“Scholes model.

We may now state the following result which allows us to solve the one period problem.

###### Proposition C.3.

Suppose that t1=t0+Î´â€‹tt\_{1}=t\_{0}+\delta t and that vâ€‹(w):=vt1â€‹(w)v(w):=v\_{t\_{1}}(w) is known, concave and increasing
for w>0w>0, equal to âˆ’âˆ-\infty for wâ‰¤0w\leq 0,
and satisfies vâ€‹(w)â‰¤0v(w)\leq 0. Define st=(1âˆ’pt)s\_{t}=(1-p\_{t}) for tâˆˆğ’¯t\in{\cal T}, so sts\_{t} denotes the survival
probability over the period [t,t+Î´â€‹t)[t,t+\delta t).

1. (a)

   vt0â€‹(w)v\_{t\_{0}}(w) is itself concave and increasing for w>0w>0, equal to âˆ’âˆ-\infty for wâ‰¤0w\leq 0
   and satisfies vâ€‹(w)â‰¤0v(w)\leq 0.
2. (b)

   For each Î·>0\eta>0 define a function on
   fÎ·:(0,1)â†’â„â‰¥0f^{\eta}:(0,1)\to\mathbb{R}\_{\geq 0} by

   |  |  |  |  |
   | --- | --- | --- | --- |
   |  | fÎ·â€‹(s)=vâ€ â€‹(Î·â€‹eâˆ’râ€‹Î´â€‹tâ€‹qBSAâ€‹(s)).f^{\eta}(s)=v^{\dagger}\left(\eta e^{-r\delta t}q^{A}\_{\mathrm{BS}}(s)\right). |  | (10) |

   Define CÎ·âˆˆâ„â‰¥0C^{\eta}\in\mathbb{R}\_{\geq 0} by

   |  |  |  |  |
   | --- | --- | --- | --- |
   |  | CÎ·=uâ€ â€‹(âˆ’Î·Î´â€‹tâ€‹(âˆ’1+st0â€‹âˆ«01(1+vâ€‹(fÎ·â€‹(s)))â€‹ds)âˆ’1).C^{\eta}=u^{\dagger}\left(-\frac{\eta}{\delta t}\left(-1+s\_{t\_{0}}\int\_{0}^{1}(1+v(f^{\eta}(s)))\,\mathrm{d}s\right)^{-1}\right). |  | (11) |

   Define wÎ·w^{\eta} by

   |  |  |  |  |
   | --- | --- | --- | --- |
   |  | wÎ·=CÎ·+st0â€‹âˆ«01eâˆ’râ€‹Î´â€‹tâ€‹qBSAâ€‹(s)â€‹fÎ·â€‹(s).w^{\eta}=C^{\eta}+s\_{t\_{0}}\int\_{0}^{1}e^{-r\delta t}q^{A}\_{\mathrm{BS}}(s)f^{\eta}(s). |  | (12) |

   If there exists Î·wt0\eta\_{w\_{t\_{0}}} such that wÎ·wt0=wt0w^{\eta\_{w\_{t\_{0}}}}=w\_{t\_{0}} then we have

   |  |  |  |
   | --- | --- | --- |
   |  | vt0â€‹(wt0)=expâ¡(âˆ’uâ€‹(Î³Î·X0)â€‹Î´â€‹t)â€‹(âˆ’1+st0â€‹âˆ«01(1+vâ€‹(fÎ·wt0â€‹(s)))â€‹ds)v\_{t\_{0}}(w\_{t\_{0}})=\exp(-u(\gamma^{\eta\_{X\_{0}}})\delta t)\left(-1+s\_{t\_{0}}\int\_{0}^{1}(1+v(f^{\eta\_{w\_{t\_{0}}}}(s)))\,\mathrm{d}s\right) |  |

   and CÎ·wt0C^{\eta\_{w\_{t\_{0}}}} is the optimal consumption at time t0t\_{0}.

Part [(a)](https://arxiv.org/html/2511.07045v1#A3.I1.i1 "item (a) â€£ Proposition C.3. â€£ C.1 Solution to the one-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem") is trivial. For example the
statement about concavity follows from [[24](https://arxiv.org/html/2511.07045v1#bib.bib24)] Proposition 8.3.1.
The proof strategy for Part [(b)](https://arxiv.org/html/2511.07045v1#A3.I1.i2 "item (b) â€£ Proposition C.3. â€£ C.1 Solution to the one-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem") is as follows:

1. (i)

   Use the dynamic programming principle to obtain a recursive formulation of the problem. This is done in Lemma [C.4](https://arxiv.org/html/2511.07045v1#A3.Thmtheorem4 "Lemma C.4. â€£ C.1 Solution to the one-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")
2. (ii)

   Reduce the continuous time investment problem of the recursion step to a calculus of variations problem using the classification of one-period complete markets. This is done in Lemma [C.6](https://arxiv.org/html/2511.07045v1#A3.Thmtheorem6 "Lemma C.6. â€£ C.1 Solution to the one-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem").
3. (iii)

   Solve the resulting calculus of variations problem. This is done in Lemma [C.7](https://arxiv.org/html/2511.07045v1#A3.Thmtheorem7 "Lemma C.7. â€£ C.1 Solution to the one-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem").

Let us first see how to compute
vt0â€‹(X0)v\_{t\_{0}}(X\_{0}) as the solution to a one period optimal investment problem.

###### Lemma C.4.

Assume the conditions of Proposition [C.3](https://arxiv.org/html/2511.07045v1#A3.Thmtheorem3 "Proposition C.3. â€£ C.1 Solution to the one-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem").
Let ğ’œwt0,t0,t1{\cal A}\_{w\_{t\_{0}},t\_{0},t\_{1}} denote the set of pairs (Ct0,Ï€)(C\_{t\_{0}},\pi)
where Ï€\pi is an admissible investment strategy for the period [t0,t1][t\_{0},t\_{1}] and Ct0âˆˆâ„C\_{t\_{0}}\in\mathbb{R} is the consumption at time t0t\_{0}
and satisfies Ct0<wt0C\_{t\_{0}}<w\_{t\_{0}}. Then

|  |  |  |  |
| --- | --- | --- | --- |
|  | vt0â€‹(wt0):=supCt0,Î±âˆˆğ’œwt0,t0,t1{expâ¡(âˆ’uâ€‹(Ct0)â€‹Î´â€‹t)â€‹(âˆ’1+st0â€‹ğ”¼â€‹(1+vt1â€‹(wt1(Ct0,Ï€))))}\begin{split}v\_{t\_{0}}(w\_{t\_{0}}):=\sup\_{{C\_{t\_{0}},\alpha}\in{\cal A}\_{w\_{t\_{0}},t\_{0},t\_{1}}}\Big\{\exp\left(-u(C\_{t\_{0}})\delta t\right)\left(-1+s\_{t\_{0}}\mathbb{E}\left(1+v\_{t\_{1}}(w^{(C\_{t\_{0}},\pi)}\_{t\_{1}})\right)\right)\Big\}\end{split} |  | (13) |

where wt1(Ct0,Ï€)w^{(C\_{t\_{0}},\pi)}\_{t\_{1}} is the value obtained by
following the investment strategy Ï€\pi from t0t\_{0} to t1t\_{1}
with an initial wealth of stâˆ’1â€‹(wt0âˆ’Ct0)s\_{t}^{-1}(w\_{t\_{0}}-C\_{t\_{0}}).

###### Proof.

We calculate

|  |  |  |  |
| --- | --- | --- | --- |
|  | vt0â€‹(wt0)\displaystyle v\_{t\_{0}}(w\_{t\_{0}}) | =supC,Ï€âˆˆğ’œwt0,t0{ğ”¼(âˆ’exp(âˆ’u(Ct0)Î´t)â„™(Ï„<t1âˆ£Ï„â‰¥t0))\displaystyle=\sup\_{{C,\pi}\in{\cal A}\_{w\_{t\_{0}},t\_{0}}}\Big\{\mathbb{E}\left(-\exp\left(-u(C\_{t\_{0}})\delta t\right)\mathbb{P}(\tau<t\_{1}\mid\tau\geq t\_{0})\right) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +ğ”¼(âˆ’exp(âˆ’u(Ct0)Î´tâˆ’âˆ«t1Ï„u(Ct)dğ’¯(t))âˆ£Ï„â‰¥t1)â„™(Ï„â‰¥t1âˆ£Ï„â‰¥t0)}\displaystyle\quad\quad\quad+\mathbb{E}\left(-\exp\left(-u(C\_{t\_{0}})\delta t-\int\_{t\_{1}}^{\tau}u(C\_{t})\,\mathrm{d}{\cal T}(t)\right)\mid\tau\geq t\_{1}\right)\mathbb{P}(\tau\geq t\_{1}\mid\tau\geq t\_{0})\Big\} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =supC,Ï€âˆˆğ’œwt0,t0{âˆ’(1âˆ’st0)exp(âˆ’u(Ct0)Î´t)\displaystyle=\sup\_{{C,\pi}\in{\cal A}\_{w\_{t\_{0}},t\_{0}}}\Big\{-(1-s\_{t\_{0}})\exp\left(-u(C\_{t\_{0}})\delta t\right) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +st0exp(âˆ’u(Ct0)Î´t)ğ”¼(âˆ’exp(âˆ’âˆ«t1Ï„u(Ct)dğ’¯(t))âˆ£Ï„â‰¥t1)}\displaystyle\quad\quad\quad+s\_{t\_{0}}\exp(-u(C\_{t\_{0}})\delta t)\mathbb{E}\left(-\exp\left(-\int\_{t\_{1}}^{\tau}u(C\_{t})\,\mathrm{d}{\cal T}(t)\right)\mid\tau\geq t\_{1}\right)\Big\} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =supÎ³,Ï€âˆˆğ’œwt0,t0{exp(âˆ’u(Ct0)Î´t)Ã—\displaystyle=\sup\_{{\gamma,\pi}\in{\cal A}\_{w\_{t\_{0}},t\_{0}}}\Bigg\{\exp\left(-u(C\_{t\_{0}})\delta t\right)\times |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | (âˆ’1+st0ğ”¼(1âˆ’exp(âˆ’âˆ«t1Ï„u(Ct)dğ’¯(t))âˆ£Ï„â‰¥t1))}\displaystyle\quad\quad\quad\left(-1+s\_{t\_{0}}\mathbb{E}\left(1-\exp\left(-\int\_{t\_{1}}^{\tau}u(C\_{t})\,\mathrm{d}{\cal T}(t)\right)\mid\tau\geq t\_{1}\right)\right)\Bigg\} |  |

The result now follows by the dynamic
programming principle.
âˆ

Equation ([13](https://arxiv.org/html/2511.07045v1#A3.E13 "In Lemma C.4. â€£ C.1 Solution to the one-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")) is a one-period investment problem in a complete market.
Complete one-period markets are classified
in [[3](https://arxiv.org/html/2511.07045v1#bib.bib3)]. This allows us to find
a more convenient, but isomorphic, representation
of our market.
For complete one-period markets, we may
say that two markets are isomorphic if they have the same
risk-free rate and if there is a map which acts as a
probability space isomorphism for both the â„™\mathbb{P} and â„š\mathbb{Q}
measures simultaneously.

Let Î©A\Omega^{A} be the probability space given by [0,1]Ã—[0,1][0,1]\times[0,1]
equipped with the Lebesgue measure. Let qA:[0,1]â†’â„>0q^{A}:[0,1]\to\mathbb{R}\_{>0} be a
measurable function of integral 11. We may define an
abstract financial market
(Î©A,qA,r)(\Omega^{A},q^{A},r) whose assets consist of random variables ff
(representing the payoff of the asset)
defined on Î©A\Omega^{A}. The cost of asset ff is given by

|  |  |  |
| --- | --- | --- |
|  | PAâ€‹(f):=âˆ«[0,1]Ã—[0,1]eâˆ’râ€‹Î´â€‹tâ€‹fâ€‹(x,y)â€‹qAâ€‹(x)â€‹dxâ€‹dyP^{A}(f):=\int\_{[0,1]\times[0,1]}e^{-r\delta t}f(x,y)\,q^{A}(x)\,\mathrm{d}x\,\mathrm{d}y |  |

if this integral exists. Assets of positively infinite or undefined cost
cannot be purchased. Assets of infinitely negative cost can
be purchased at any price. The AA in our superscripts
stands for abstract. Notice that in this abstract market
the random variable UU defined by Uâ€‹(x,y)=xU(x,y)=x is uniform
in the â„™A\mathbb{P}^{A} measure and has density qAq^{A} in the â„šA\mathbb{Q}^{A} measure.

###### Lemma C.5.

As a one period market, the Blackâ€“Scholesâ€“Merton market
from time t0t\_{0} to time t1t\_{1}
is isomorphic to the market (Î©A,qBSA,r)(\Omega^{A},q^{A}\_{\mathrm{BS}},r).

We defer the proof to appendix [D](https://arxiv.org/html/2511.07045v1#A4 "Appendix D Proof of Lemma C.5 â€£ Machine-learning a family of solutions to an optimal pension investment problem").

Having found a simple isomorphic representative of our market, we can rewrite
the equation ([13](https://arxiv.org/html/2511.07045v1#A3.E13 "In Lemma C.4. â€£ C.1 Solution to the one-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")) in terms of the abstract market Î©A\Omega^{A}.

###### Lemma C.6.

Assume the conditions of Proposition [C.3](https://arxiv.org/html/2511.07045v1#A3.Thmtheorem3 "Proposition C.3. â€£ C.1 Solution to the one-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem").
The value function vt0â€‹(wt0)v\_{t\_{0}}(w\_{t\_{0}}) can be calculated by
solving the optimisation problem

|  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- |
|  |  | maximizeCâˆˆâ„,fâˆˆL0â€‹[0,1]\displaystyle\underset{C\in\mathbb{R},f\in L^{0}[0,1]}{\mathrm{maximize}} |  | expâ¡(âˆ’uâ€‹(C)â€‹Î´â€‹t)â€‹(âˆ’1+st0â€‹âˆ«01(1+vâ€‹(fâ€‹(s)))â€‹ds)\displaystyle\exp(-u(C)\delta t)\left(-1+s\_{t\_{0}}\int\_{0}^{1}(1+v(f(s)))\,\mathrm{d}s\right) |  | (14) |
|  |  | subject to |  | C+st0â€‹âˆ«01eâˆ’râ€‹Î´â€‹tâ€‹qBSAâ€‹(s)â€‹fâ€‹(s)â€‹dsâ‰¤wt0.\displaystyle C+s\_{t\_{0}}\int\_{0}^{1}e^{-r\delta t}q^{A}\_{\mathrm{BS}}(s)f(s)\,\mathrm{d}s\leq w\_{t\_{0}}. |  |

taking v=vt1v=v\_{t\_{1}}.

###### Proof.

Let us write (Ct0,f)(C\_{t\_{0}},f) for a pair of a consumption
Ct0âˆˆâ„C\_{t\_{0}}\in\mathbb{R} and an
investment fâˆˆL0â€‹(Î©A)f\in L^{0}(\Omega^{A}).
We denote by â„¬wt0{\cal B}\_{w\_{t\_{0}}} the set
of consumptions and investments that are available with
a budget of wt0{w\_{t\_{0}}}

|  |  |  |
| --- | --- | --- |
|  | â„¬wt0={(Ct0,f)âˆˆâ„Ã—L0â€‹(Î©A)âˆ£Î³t0+st0â€‹PBSAâ€‹(f)â‰¤X0}.{\cal B}\_{w\_{t\_{0}}}=\{(C\_{t\_{0}},f)\in\mathbb{R}\times L^{0}(\Omega^{A})\mid\gamma\_{t\_{0}}+s\_{t\_{0}}P^{A}\_{\mathrm{BS}}(f)\leq X\_{0}\}. |  |

If we also write

|  |  |  |
| --- | --- | --- |
|  | Ut0Aâ€‹(Ct0,f):=expâ¡(âˆ’uâ€‹(Ct0)â€‹Î´â€‹t)â€‹(âˆ’1+st0â€‹âˆ«[0,1]Ã—[0,1](1+vt1â€‹(fâ€‹(x,y)))â€‹dxâ€‹dy)\begin{split}U^{A}\_{t\_{0}}(C\_{t\_{0}},f):=&\exp(-u(C\_{t\_{0}})\delta t)\left(-1+s\_{t\_{0}}\int\_{[0,1]\times[0,1]}\,(1+v\_{t\_{1}}(f(x,y)))\,\mathrm{d}x\,\mathrm{d}y\right)\end{split} |  |

to accord with equation ([13](https://arxiv.org/html/2511.07045v1#A3.E13 "In Lemma C.4. â€£ C.1 Solution to the one-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")),
then the fact that our markets are isomorphic allows
us to deduce that

|  |  |  |  |
| --- | --- | --- | --- |
|  | vt0â€‹(wt0):=sup(C,f)âˆˆâ„¬wt0UAâ€‹(Ct0,f).v\_{t\_{0}}(w\_{t\_{0}}):=\sup\_{(C,f)\in{\cal B}\_{w\_{t\_{0}}}}U^{A}(C\_{t\_{0}},f). |  | (15) |

Since vt1v\_{t\_{1}} is assumed to be concave we may average an
investment fâ€‹(x,y)f(x,y) over the factor yy to obtain a new investment
fÂ¯\overline{f} which achieves a higher value for the gain
function UAU^{A}. Thus we may restrict our attention
to investments fâ€‹(x,y)f(x,y) which depend only upon xx. The result follows.
âˆ

Note that an investment fâˆˆL1f\in L^{1} for this abstract market
model corresponds to a derivative with payoff given by
the random variable fâ€‹(Fdâ€‹â„šdâ€‹â„™â€‹(dâ€‹â„šdâ€‹â„™))f(F\_{\frac{\mathrm{d}\mathbb{Q}}{\mathrm{d}\mathbb{P}}}(\frac{\mathrm{d}\mathbb{Q}}{\mathrm{d}\mathbb{P}})) in the original Blackâ€“Scholesâ€“Merton market (or indeed in any isomorphic market). This derivative can then be replicated by delta hedging
in the Blackâ€“Scholesâ€“Merton market. So the solution to the
abstract investment problem ([14](https://arxiv.org/html/2511.07045v1#A3.E14 "In Lemma C.6. â€£ C.1 Solution to the one-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem"))
can be straightforwardly mapped to a solution of the original problem.

###### Lemma C.7.

Assume the conditions and definitions of Proposition [C.3](https://arxiv.org/html/2511.07045v1#A3.Thmtheorem3 "Proposition C.3. â€£ C.1 Solution to the one-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem"). If an Î·wt0\eta\_{w\_{t\_{0}}} exists with wÎ·wt0=wt0w^{\eta\_{w\_{t\_{0}}}}=w\_{t\_{0}},
then the solution of ([14](https://arxiv.org/html/2511.07045v1#A3.E14 "In Lemma C.6. â€£ C.1 Solution to the one-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")) is given by fÎ·wt0f^{\eta\_{w\_{t\_{0}}}}
and Î³Î·wt0\gamma^{\eta\_{w\_{t\_{0}}}}.

###### Proof.

We will now solve ([14](https://arxiv.org/html/2511.07045v1#A3.E14 "In Lemma C.6. â€£ C.1 Solution to the one-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")) using
the method of Lagrange multipliers. We define a vector space V=â„âŠ•L0â€‹([0,1])âŠ•â„V=\mathbb{R}\oplus L^{0}([0,1])\oplus\mathbb{R}
For Î»âˆˆâ„\lambda\in\mathbb{R},
we define the Lagrangian
L:Vâ†’â„L:V\to\mathbb{R} by

|  |  |  |  |
| --- | --- | --- | --- |
|  | Lâ€‹(C,f,Î»):=expâ¡(âˆ’uâ€‹(C)â€‹Î´â€‹t)â€‹(âˆ’1+st0â€‹âˆ«01(1+vâ€‹(fâ€‹(s)))â€‹ds)+Î»â€‹(âˆ’X0+C+st0â€‹âˆ«01eâˆ’râ€‹Î´â€‹tâ€‹qBSAâ€‹(s)â€‹fâ€‹(s)â€‹ds).\begin{split}L(C,f,\lambda):=&\exp(-u(C)\delta t)\left(-1+s\_{t\_{0}}\int\_{0}^{1}(1+v(f(s)))\,\mathrm{d}s\right)\\ &+\lambda\left(-X\_{0}+C+s\_{t\_{0}}\int\_{0}^{1}e^{-r\delta t}q^{A}\_{\mathrm{BS}}(s)f(s)\,\mathrm{d}s\right).\end{split} |  | (16) |

Computing the directional derivatives of Lâ€‹(C,f)L(C,f) we find the following
necessary and sufficient conditions for (C,f)(C,f) to be a saddle point of Lâ€‹(Î³,f,Î»)L(\gamma,f,\lambda)
for the given Î»\lambda. Firstly

|  |  |  |  |
| --- | --- | --- | --- |
|  | 0âˆˆâˆ’âˆ‚uâ€‹(C)â€‹Î´â€‹tâ€‹expâ¡(âˆ’uâ€‹(C)â€‹Î´â€‹t)â€‹(âˆ’1+st0â€‹âˆ«01(1+vâ€‹(fâ€‹(s)))â€‹ds)+Î»0\in-\partial u(C)\delta t\exp(-u(C)\delta t)\left(-1+s\_{t\_{0}}\int\_{0}^{1}(1+v(f(s)))\,\mathrm{d}s\right)+\lambda |  | (17) |

where âˆ‚uâ€‹(C)\partial u(C) is the subdifferential of uu at CC.
Secondly

|  |  |  |
| --- | --- | --- |
|  | 0=âˆ«01(expâ¡(âˆ’uâ€‹(C)â€‹Î´â€‹t)â€‹st0â€‹(âˆ‚v)â€‹(fâ€‹(s))+Î»â€‹st0â€‹eâˆ’râ€‹Î´â€‹tâ€‹qBSAâ€‹(s))â€‹gâ€‹(s)â€‹ds.0=\int\_{0}^{1}\left(\exp(-u(C)\delta t)s\_{t\_{0}}(\partial v)(f(s))+\lambda s\_{t\_{0}}e^{-r\delta t}q^{A}\_{\mathrm{BS}}(s)\right)g(s)\,\mathrm{d}s. |  |

The integral is well-defined since âˆ‚v\partial v will
be single-valued almost everywhere.
This must hold for all gâ€‹(s)g(s) so this is equivalent to requiring

|  |  |  |  |
| --- | --- | --- | --- |
|  | (âˆ‚v)â€‹(fâ€‹(z))=âˆ’Î»â€‹expâ¡(uâ€‹(C)â€‹Î´â€‹t)â€‹st0â€‹eâˆ’râ€‹Î´â€‹tâ€‹qBSAâ€‹(z).(\partial v)(f(z))=-\lambda\exp(u(C)\delta t)s\_{t\_{0}}e^{-r\delta t}q^{A}\_{\mathrm{BS}}(z). |  | (18) |

for almost all zâˆˆ(0,1)z\in(0,1).

If the Kuhn-Tucker
conditions ([17](https://arxiv.org/html/2511.07045v1#A3.E17 "In C.1 Solution to the one-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")) and ([18](https://arxiv.org/html/2511.07045v1#A3.E18 "In C.1 Solution to the one-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem"))
are satisfied, ((C,f),Î»)((C,f),\lambda) will be a saddle point of the Lagrangian.
The theory of Lagrange multipliers (see [[27](https://arxiv.org/html/2511.07045v1#bib.bib27)] Theorem 28.3)
now shows that if we can
find (C,f)(C,f) satisfying the Kuhnâ€“Tucker conditions ([17](https://arxiv.org/html/2511.07045v1#A3.E17 "In C.1 Solution to the one-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem"))
and ([18](https://arxiv.org/html/2511.07045v1#A3.E18 "In C.1 Solution to the one-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")) then this will yield a maximizer
for the problem ([14](https://arxiv.org/html/2511.07045v1#A3.E14 "In Lemma C.6. â€£ C.1 Solution to the one-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")) in the case
where the initial budget satisfies

|  |  |  |  |
| --- | --- | --- | --- |
|  | wt0=C+st0â€‹âˆ«01eâˆ’râ€‹Î´â€‹tâ€‹qBSAâ€‹(s)â€‹fâ€‹(s)â€‹ds.w\_{t\_{0}}=C+s\_{t\_{0}}\int\_{0}^{1}e^{-r\delta t}q^{A}\_{\mathrm{BS}}(s)f(s)\,\mathrm{d}s. |  | (19) |

We remark that the theory of Lagrange multipliers given in [[27](https://arxiv.org/html/2511.07045v1#bib.bib27)] is
stated in terms of finite-dimensional spaces. We may, nevertheless, apply it
by noting that if (C,f)(C,f) satisfies the Kuhnâ€“Tucker conditions yet
is not a maximizer then there must be some direction
in which we can perturb (C,f)(C,f) to obtain a higher value for the gain. We may
now apply the finite-dimensional theory to the vector space generated by this perturbation
to obtain a contradiction.

The result now follows by introducing a variable

|  |  |  |
| --- | --- | --- |
|  | Î·:=âˆ’Î»â€‹expâ¡(uâ€‹(C)â€‹Î´â€‹t)\eta:=-\lambda\exp(u(C)\delta t) |  |

to simplify the equations.
âˆ

This completes the proof of Proposition [C.3](https://arxiv.org/html/2511.07045v1#A3.Thmtheorem3 "Proposition C.3. â€£ C.1 Solution to the one-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem").

The outstanding difficulty is proving that an Î·\eta solving wÎ·=wt0w^{\eta}=w\_{t\_{0}} exists.
One might attempt to use general duality theory to do this. Theorem 8.3.1 of [[24](https://arxiv.org/html/2511.07045v1#bib.bib24)]
ensures that so long as wt0w\_{t\_{0}} is chosen to satisfy the Slater condition we can guarantee
the existence of a Î»\lambda minimizing the dual problem. However, this theorem does not
guarantee the existence of a maximizer for the primal problem. As a result, even if one knows
the value of Î»\lambda it is still unclear whether a solution to ([17](https://arxiv.org/html/2511.07045v1#A3.E17 "In C.1 Solution to the one-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem"))
and ([18](https://arxiv.org/html/2511.07045v1#A3.E18 "In C.1 Solution to the one-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")) exists. When one introduces the variable Î·\eta, this ensures
that CÎ·C^{\eta} and fÎ·f^{\eta} are well-defined once Î·\eta is known and so the problem
shifts to finding the correct value of Î·\eta. We will resolve this issue in
the cases of interest using a continuity argument in the next section.

### C.2 Numerical approximation of the multi-period problem

The results of the previous section immediately suggests a numerical method for
solving our investment problems with exponential utility.

We define the minimum acceptable consumption to be

|  |  |  |
| --- | --- | --- |
|  | Cmin:=inf{Câˆˆâ„âˆ£uâ€‹(C)>âˆ’âˆ}.C\_{\min}:=\inf\{C\in\mathbb{R}\mid u(C)>-\infty\}. |  |

In addition to the previous assumptions that uu is concave and increasing, we assume

|  |  |  |  |
| --- | --- | --- | --- |
|  | uâ€ â€‹Â is continuous onÂ â€‹(0,âˆ)u^{\dagger}\text{ is continuous on }(0,\infty) |  | (20) |

and

|  |  |  |  |
| --- | --- | --- | --- |
|  | limpâ†’0uâ€ â€‹(p)=âˆ.\lim\_{p\to 0}u^{\dagger}(p)=\infty. |  | (21) |

We note that our assumption that uu is concave and increasing also ensures that

|  |  |  |  |
| --- | --- | --- | --- |
|  | limpâ†’âˆuâ€ â€‹(p)=Cmin.\lim\_{p\to\infty}u^{\dagger}(p)=C\_{\min}. |  | (22) |

###### Algorithm C.8.

Choose a grid of points
ğ’³={x1,x2â€‹â€¦,xN}{\cal X}=\{x\_{1},x\_{2}\ldots,x\_{N}\} on which we will approximate the value function vtv\_{t}.
We will write v~t\tilde{v}\_{t} for our approximate value function.
This will be a concave increasing
piecewise linear function equal to âˆ’âˆ-\infty on (âˆ’âˆ,x1)(-\infty,x\_{1}), linear on [xi,xi+1][x\_{i},x\_{i+1}]
and constant on [xN,âˆ)[x\_{N},\infty). We will simply need
to store the values v~tâ€‹(xi){\tilde{v}}\_{t}(x\_{i}) at the grid points.

To avoid numerical overflow issues we define a function
â„“â€‹(x):=âˆ’logâ¡(âˆ’x)\ell(x):=-\log(-x) and store the values â„“â€‹(v~tâ€‹(xi))\ell(\tilde{v}\_{t}(x\_{i}))
at each grid point rather than storing v~tâ€‹(xi)\tilde{v}\_{t}(x\_{i}) itself.

1. (i)

   Choose the values at the final time point Tâˆ’Î´â€‹tT-\delta t by

   |  |  |  |
   | --- | --- | --- |
   |  | v~Tâˆ’Î´â€‹tâ€‹(xi):=vTâˆ’Î´â€‹tâ€‹(xi)=âˆ’expâ¡(âˆ’uâ€‹(xi)â€‹Î´â€‹t).\tilde{v}\_{T-\delta t}(x\_{i}):=v\_{T-\delta t}(x\_{i})=-\exp(-u(x\_{i})\delta t). |  |

   Or equivalently

   |  |  |  |
   | --- | --- | --- |
   |  | â„“â€‹(v~Tâˆ’Î´â€‹tâ€‹(xi))=â„“â€‹(vTâˆ’Î´â€‹tâ€‹(xi))=uâ€‹(xi)â€‹Î´â€‹t.\ell(\tilde{v}\_{T-\delta t}(x\_{i}))=\ell(v\_{T-\delta t}(x\_{i}))=u(x\_{i})\delta t. |  |
2. (ii)

   Suppose that v~t\tilde{v}\_{t} is known. Set
   v~tâˆ’Î´â€‹tâ€‹(xi)\tilde{v}\_{t-\delta t}(x\_{i}) to be the solution of ([14](https://arxiv.org/html/2511.07045v1#A3.E14 "In Lemma C.6. â€£ C.1 Solution to the one-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem"))
   with vt1=v~tv\_{t\_{1}}=\tilde{v}\_{t} and initial budget xix\_{i}. We describe in detail how
   to solve this problem in Proposition [C.10](https://arxiv.org/html/2511.07045v1#A3.Thmtheorem10 "Proposition C.10. â€£ C.2 Numerical approximation of the multi-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem") below.

Since vTâˆ’Î´â€‹tv\_{T-\delta t} is concave and increasing and
v~Tâˆ’Î´â€‹t\tilde{v}\_{T-\delta t} is piecewise linear v~Tâˆ’Î´â€‹tâ€‹(w)â‰¤vTâˆ’Î´â€‹tâ€‹(w)\tilde{v}\_{T-\delta t}(w)\leq v\_{T-\delta t}(w). Let v^tâ€‹(w)\hat{v}\_{t}(w)
be defined to be the solution of ([14](https://arxiv.org/html/2511.07045v1#A3.E14 "In Lemma C.6. â€£ C.1 Solution to the one-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem"))
with vt1=v~tv\_{t\_{1}}=\tilde{v}\_{t} and initial budget ww. We see
that v~Tâˆ’Î´â€‹tâ€‹(w)â‰¤v^Tâˆ’Î´â€‹tâ€‹(w)â‰¤vTâˆ’Î´â€‹tâ€‹(w)\tilde{v}\_{T-\delta t}(w)\leq\hat{v}\_{T-\delta t}(w)\leq v\_{T-\delta t}(w).

Let
ğ’³1âŠ†ğ’³2âŠ†ğ’³3â€‹â€¦{\cal X}\_{1}\subseteq{\cal X}\_{2}\subseteq{\cal X}\_{3}\ldots
be an increasing sequence of grids
with ğ’³âˆ:=âˆªj=1âˆğ’³i{\cal X}\_{\infty}:=\cup\_{j=1}^{\infty}{\cal X}\_{i} being dense
in (0,âˆ)(0,\infty). Write v~tj\tilde{v}^{j}\_{t}
for the approximations with respect to ğ’³i{\cal X}\_{i}.
We see by repeating the argument above that v~tjâ€‹(w)â‰¤vtâ€‹(w)\tilde{v}^{j}\_{t}(w)\leq v\_{t}(w) at all points wâˆˆ(0,âˆ)w\in(0,\infty).
Hence we may define

|  |  |  |
| --- | --- | --- |
|  | v~tâ€‹(w)=limjâ†’âˆv~tjâ€‹(w).\tilde{v}\_{t}(w)=\lim\_{j\to\infty}\tilde{v}^{j}\_{t}(w). |  |

###### Theorem C.9 (Convergence of Algorithm [C.8](https://arxiv.org/html/2511.07045v1#A3.Thmtheorem8 "Algorithm C.8. â€£ C.2 Numerical approximation of the multi-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")).

Define

|  |  |  |
| --- | --- | --- |
|  | Wmin,t=sup{wâˆ£vtâ€‹(w)=âˆ’âˆ}.W\_{\min,t}=\sup\{w\mid v\_{t}(w)=-\infty\}. |  |

For w>Wmin,tw>W\_{\min,t} we have

|  |  |  |
| --- | --- | --- |
|  | v~tâ€‹(w)=vtâ€‹(w).\tilde{v}\_{t}(w)=v\_{t}(w). |  |

###### Proof.

Let ğ’±{\cal V} denote the space of concave, increasing functions vâ€‹(w)v(w) which satisfy vâ€‹(w)=âˆ’âˆv(w)=-\infty for w<0w<0 and where vâ€‹(w)v(w) is bounded above by 0.
For two adjacent times t0,t1=t0+Î´â€‹tt\_{0},t\_{1}=t\_{0}+\delta t in our grid
we define a solution function Ï•t0,t1,w:ğ’±â†’â„\phi\_{t\_{0},t\_{1},w}:{\cal V}\to\mathbb{R} by
setting

|  |  |  |
| --- | --- | --- |
|  | Ï•t0,t1,wâ€‹(vt1)\phi\_{t\_{0},t\_{1},w}(v\_{t\_{1}}) |  |

to equal the supremum in ([13](https://arxiv.org/html/2511.07045v1#A3.E13 "In Lemma C.4. â€£ C.1 Solution to the one-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")). By composing these
solution functions in the obvious way, we obtain a solution function Ï•t0,t1,w\phi\_{t\_{0},t\_{1},{w}}
for any times in the grid with t0â‰¤t1t\_{0}\leq t\_{1}.

We define a corresponding minimum budget as follows:

|  |  |  |
| --- | --- | --- |
|  | Wmin,t0,t1â€‹(v)=sup{wâˆ£Ï•t0,t1,wâ€‹(v)=âˆ}.W\_{\min,t\_{0},t\_{1}}(v)=\sup\{w\mid\phi\_{t\_{0},t\_{1},w}(v)=\infty\}. |  |

Let t0,t1t\_{0},t\_{1} be adjacent times in the grid.
Given vâˆˆğ’±v\in{\cal V} with Ï•t0,t1,wâ€‹(v)\phi\_{t\_{0},t\_{1},w}(v) finite,
let (Ct0,Ï€)âˆˆğ’œw,t0,t1(C\_{t\_{0}},\pi)\in{\cal A}\_{w,t\_{0},t\_{1}}
be a maximizing strategy for the problem ([13](https://arxiv.org/html/2511.07045v1#A3.E13 "In Lemma C.4. â€£ C.1 Solution to the one-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")) with vt1=vv\_{t\_{1}}=v.
Suppose vâ€²âˆˆğ’±v^{\prime}\in{\cal V}. We have

|  |  |  |
| --- | --- | --- |
|  | |exp(âˆ’u(Ct0)Î´t)(âˆ’1+st0ğ”¼(1+v(w1(Ct0,Î±))))âˆ’exp(âˆ’u(Î³t0)Î´t)(âˆ’1+st0ğ”¼(1+vâ€²(w1(Î³t0,Î±))))|â‰¤Aâ€‹expâ¡(âˆ’uÎ³t0)â€‹â€–vâˆ’vâ€²â€–âˆ\Big|\exp\left(-u(C\_{t\_{0}})\delta t\right)\left(-1+s\_{t\_{0}}\mathbb{E}\left(1+v(w^{(C\_{t\_{0}},\alpha)}\_{1})\right)\right)-\\ \exp\left(-u(\gamma\_{t\_{0}})\delta t\right)\left(-1+s\_{t\_{0}}\mathbb{E}\left(1+v^{\prime}(w^{(\gamma\_{t\_{0}},\alpha)}\_{1})\right)\right)\Big|\\ \leq A\exp(-u\_{\gamma\_{t\_{0}}})\|v-v^{\prime}\|\_{\infty} |  |

for some constant AA.
Hence for any Ïµ>0\epsilon>0 we can find Î´1>0\delta\_{1}>0 such that â€–vâˆ’vâ€²â€–âˆ<Î´1\|v-v^{\prime}\|\_{\infty}<\delta\_{1} implies

|  |  |  |
| --- | --- | --- |
|  | Ï•t0,t1,wâ€‹(vâ€²)â‰¥Ï•t0,t1,wâ€‹(v)âˆ’Ïµ.\phi\_{t\_{0},t\_{1},w}(v^{\prime})\geq\phi\_{t\_{0},t\_{1},w}(v)-\epsilon. |  |

We have shown Ï•t0,t1,w\phi\_{t\_{0},t\_{1},w} is lower semi-continuous in the sup\sup norm for adjacent
times t0t\_{0} and t1t\_{1}. It follows that Ï•t0,t1,w\phi\_{t\_{0},t\_{1},w} is lower semi-continuous for all t0<t1t\_{0}<t\_{1}.

Given vâˆˆğ’±v\in{\cal V} and hâˆˆâ„h\in\mathbb{R}, define the translation

|  |  |  |
| --- | --- | --- |
|  | vhâ€‹(x)={vâ€‹(xâˆ’h)xâˆ’hâ‰¥0âˆ’âˆxâˆ’h<0.=minâ¡{vâ€‹(xâˆ’h),(supv)â€‹ğŸxâˆ’h<0}v\_{h}(x)=\begin{cases}v(x-h)&x-h\geq 0\\ -\infty&x-h<0.\end{cases}=\min\{v(x-h),(\sup v){\bf 1}\_{x-h<0}\} |  |

Define ft0,t1,w,vâ€‹(h)=Ï•t0,t1,wâ€‹(vh)f\_{t\_{0},t\_{1},w,v}(h)=\phi\_{t\_{0},t\_{1},w}(v\_{h}). The function vâ€‹(x,h)=vhâ€‹(x)v(x,h)=v\_{h}(x)
is concave.
Hence ft0,t1,w,vf\_{t\_{0},t\_{1},w,v}
is concave as a function of hh. If w>Wmin,t0,t1â€‹(v)w>W\_{\min,t\_{0},t\_{1}}(v) then
0âˆˆriâ¡ft0,t1,w,v0\in\operatorname{ri}f\_{t\_{0},t\_{1},w,v}, where riâ¡f\operatorname{ri}f denotes the relative interior of ff. Hence
ft0,t1,w,vf\_{t\_{0},t\_{1},w,v} is continuous in hh at 0.

Combining this with the lower semi-continuity result, we see that if w>Wmin,t0,t1â€‹(v)w>W\_{\min,t\_{0},t\_{1}}(v)
then given Ïµ>0\epsilon>0, we can find Î´1>0\delta\_{1}>0 and Î´2>0\delta\_{2}>0 such that

|  |  |  |
| --- | --- | --- |
|  | Ï•t0,t1,wâ€‹(vÎ´1â€‹(x)âˆ’Î´2)â‰¥Ï•t0,t1,wâ€‹(v)âˆ’Ïµ.\phi\_{t\_{0},t\_{1},w}(v\_{\delta\_{1}}(x)-\delta\_{2})\geq\phi\_{t\_{0},t\_{1},w}(v)-\epsilon. |  |

Let us write vÏµâ€‹(x)v\_{\epsilon}(x) for the function vÎ´1â€‹(x)âˆ’Î´2v\_{\delta\_{1}}(x)-\delta\_{2}. Given a function ff
let us write Î“f\Gamma\_{f} for the hypograph of ff, that is to say the set of points on or below the graph. We have Î“vâŠ‡Î“vÏµ\Gamma\_{v}\supseteq\Gamma\_{v\_{\epsilon}}. For any function vâ€²âˆˆğ’±v^{\prime}\in{\cal V} satisfying
Î“vâŠ‡Î“vâ€²âŠ‡Î“vÏµ\Gamma\_{v}\supseteq\Gamma\_{v^{\prime}}\supseteq\Gamma\_{v\_{\epsilon}} we will have

|  |  |  |
| --- | --- | --- |
|  | Ï•t0,t1,wâ€‹(v)â‰¥Ï•t0,t1,wâ€‹(vâ€²)â‰¥Ï•t0,t1,wâ€‹(vÏµ)â‰¥Ï•t0,t1,wâ€‹(v)âˆ’Ïµ.\phi\_{t\_{0},t\_{1},w}(v)\geq\phi\_{t\_{0},t\_{1},w}(v^{\prime})\geq\phi\_{t\_{0},t\_{1},w}(v\_{\epsilon})\geq\phi\_{t\_{0},t\_{1},w}(v)-\epsilon. |  |

since it is clear that Î“vâŠ‡Î“vâ€²\Gamma\_{v}\supseteq\Gamma\_{v^{\prime}} implies Ï•t0,t1,wâ€‹(v)â‰¥Ï•t0,t1,wâ€‹(vâ€²)\phi\_{t\_{0},t\_{1},w}(v)\geq\phi\_{t\_{0},t\_{1},w}(v^{\prime}). Note that we can always find a piecewise linear approximation
between Î“v\Gamma\_{v} and Î“vÏµ\Gamma\_{v\_{\epsilon}}.

Given a value for Ïµ0\epsilon\_{0}, we may inductively extend this to a sequence of
positive Ïµt{\epsilon\_{t}} for tâˆˆğ’¯t\in{\cal T}
such that if our approximation v~t\tilde{v}\_{t} satisfies
Î“vtâŠ‡Î“v~tâ€‹Î“(vt)Ïµt\Gamma\_{v\_{t}}\supseteq\Gamma\_{\tilde{v}\_{t}}\Gamma\_{(v\_{t})\_{\epsilon\_{t}}}
then it will automatically satisfy
Î“vtâˆ’Î´â€‹tâŠ‡Î“v~tâˆ’Î´â€‹tâŠ‡Î“(vt)Ïµtâˆ’Î´â€‹t\Gamma\_{v\_{t-\delta t}}\supseteq\Gamma\_{\tilde{v}\_{t-\delta t}}\supseteq\Gamma\_{(v\_{t})\_{\epsilon\_{t-\delta t}}}.
By choosing a sufficiently fine grid we can ensure this condition is satisfied at time Tâˆ’Î´â€‹tT-\delta t.
By further refinements we may ensure that it is satisfied at all times.
âˆ

Let us now describe in full detail how to
solve ([14](https://arxiv.org/html/2511.07045v1#A3.E14 "In Lemma C.6. â€£ C.1 Solution to the one-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem"))
given that vt1v\_{t\_{1}} is of the form used in our algorithm.
In Proposition [C.10](https://arxiv.org/html/2511.07045v1#A3.Thmtheorem10 "Proposition C.10. â€£ C.2 Numerical approximation of the multi-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem"), we will give the formulae
necessary to solve the problem on a computer in a format that addresses
numerical overflow issues. Terms on the left hand side of the equations in the Proposition
should be stored in computer memory and can be computed without overflow issues from the terms on the right. We use infinite values for some terms as a convenient shorthand, terms such as an exponential of âˆ’âˆ-\infty should be interpreted in the obvious way.

To store probability values we define a bijection L:[0,1]â†’â„âˆª{Â±âˆ}L:[0,1]\to\mathbb{R}\cup\{\pm\infty\}
by

|  |  |  |
| --- | --- | --- |
|  | Lâ€‹(u)={logâ¡(2â€‹u)uâ‰¤0.5âˆ’logâ¡(2âˆ’2â€‹u)u>0.5.L(u)=\begin{cases}\log(2u)&u\leq 0.5\\ -\log(2-2u)&u>0.5.\end{cases} |  |

We note that the GNU scientific library contains a function gsl\_sf\_log\_erfc
which computes the logarithm of the complementary error function which we can then use to compute Lâ€‹(Î¦)L(\Phi).

We define a function

|  |  |  |
| --- | --- | --- |
|  | u~â€‹(y)=logâ¡(uâ€ â€‹(ey)).\tilde{u}(y)=\log(u^{\dagger}(e^{y})). |  |

For the specific functional form

|  |  |  |
| --- | --- | --- |
|  | uâ€‹(x)={aâ€‹(xâˆ’x0)n+bxâ‰¥0âˆ’âˆotherwiseu(x)=\begin{cases}a(x-x\_{0})^{n}+b&x\geq 0\\ -\infty&\text{otherwise}\end{cases} |  |

which we will use in our numerical examples, we may compute
u~\tilde{u} without experiencing
overflow errors using the formulae

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | u~0â€‹(p)\displaystyle\tilde{u}\_{0}(p) | :=1nâˆ’1â€‹(pâˆ’logâ¡(aâ€‹n)),\displaystyle:=\frac{1}{n-1}(p-\log(a\,n)), |  | (23) |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | u~â€‹(y)\displaystyle\tilde{u}(y) | ={logâ¡(eu~0â€‹(p))x0=0logâ¡(eu~0â€‹(p)+elogâ¡(x0))x0>0logâ¡(eu~0â€‹(p)âˆ’elogâ¡(âˆ’x0))u~0â€‹(p)>logâ¡(âˆ’x0)â€‹Â andÂ â€‹x0<0âˆ’âˆu~0â€‹(p)â‰¤logâ¡(âˆ’x0)â€‹Â andÂ â€‹x0<0.\displaystyle=\begin{cases}\log(e^{\tilde{u}\_{0}(p)})&x\_{0}=0\\ \log(e^{\tilde{u}\_{0}(p)}+e^{\log(x\_{0})})&x\_{0}>0\\ \log(e^{\tilde{u}\_{0}(p)}-e^{\log(-x\_{0})})&\tilde{u}\_{0}(p)>\log(-x\_{0})\text{ and }x\_{0}<0\\ -\infty&\tilde{u}\_{0}(p)\leq\log(-x\_{0})\text{ and }x\_{0}<0.\end{cases} |  | (24) |

We note the standard approach to computing the log of sums and differences of exponentials
without overflow issues should be used when evaluating
expressions such as this.

###### Proposition C.10.

Let vv be a concave, non-positive, increasing function
which is linear between grid points in ğ’³={x1,x2,â€¦â€‹xN}{\cal X}=\{x\_{1},x\_{2},\ldots x\_{N}\}
with xix\_{i} strictly increasing. Suppose also that vv is equal
to âˆ’âˆ-\infty on (âˆ’âˆ,x1)(-\infty,x\_{1}) and constant on (xN,âˆ)(x\_{N},\infty).
Suppose that uâ€ u^{\dagger} is continuous and
satisfies equations ([20](https://arxiv.org/html/2511.07045v1#A3.E20 "In C.2 Numerical approximation of the multi-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")) and ([21](https://arxiv.org/html/2511.07045v1#A3.E21 "In C.2 Numerical approximation of the multi-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")).

Define a decreasing sequence of points logâ¡(pi)\log(p\_{i}) by

|  |  |  |  |
| --- | --- | --- | --- |
|  | logâ¡(pi)={âˆi=0logâ¡(eâˆ’â„“â€‹(vâ€‹(xi))âˆ’eâˆ’â„“â€‹(vâ€‹(xi+1)))âˆ’logâ¡(xi+1âˆ’xi)0<i<Nâˆ’âˆi=N.\log(p\_{i})=\begin{cases}\infty&i=0\\ \log(e^{-\ell(v(x\_{i}))}-e^{-\ell(v(x\_{i+1}))})-\log(x\_{i+1}-x\_{i})&0<i<N\\ -\infty&i=N.\end{cases} |  | (25) |

For a given value of logâ¡Î·\log\eta, define Lâ€‹(UiÎ·)L(U^{\eta}\_{i}) and Lâ€‹(QiÎ·)L(Q^{\eta}\_{i}) for 0<i<N0<i<N by

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | Lâ€‹(UiÎ·)\displaystyle L(U^{\eta}\_{i}) | =Lâ€‹(Î¦â€‹(âˆ’12â€‹M+1Mâ€‹(logâ¡(Î·)âˆ’râ€‹Î´â€‹tâˆ’logâ¡(pi)))),\displaystyle=L\left(\Phi\left(-\frac{1}{2}M+\frac{1}{M}\left(\log(\eta)-r\delta t-\log(p\_{i})\right)\right)\right), |  | (26) |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | Lâ€‹(QiÎ·)\displaystyle L(Q^{\eta}\_{i}) | =Lâ€‹(Î¦â€‹(12â€‹M+1Mâ€‹(logâ¡(Î·)âˆ’râ€‹Î´â€‹tâˆ’logâ¡(pi)))).\displaystyle=L\left(\Phi\left(\frac{1}{2}M+\frac{1}{M}\left(\log(\eta)-r\delta t-\log(p\_{i})\right)\right)\right). |  | (27) |

Define Lâ€‹(U0Î·)=Lâ€‹(Q0Î·)=âˆ’âˆL(U^{\eta}\_{0})=L(Q^{\eta}\_{0})=-\infty and Lâ€‹(UNÎ·)=Lâ€‹(QNÎ·)=âˆL(U^{\eta}\_{N})=L(Q^{\eta}\_{N})=\infty.
We may then define the quantity AÎ·A^{\eta} by

|  |  |  |
| --- | --- | --- |
|  | AÎ·=logâ¡(elogâ¡(1âˆ’st0)+âˆ‘i=1Nelogâ¡(st0)+logâ¡(âˆ’vâ€‹(xi))+logâ¡(elogâ¡UiÎ·âˆ’elogâ¡Uiâˆ’1Î·)).A^{\eta}=\log\left(e^{\log(1-s\_{t\_{0}})}+\sum\_{i=1}^{N}e^{\log(s\_{t\_{0}})+\log(-v(x\_{i}))+\log\left(e^{\log U^{\eta}\_{i}}-e^{\log U^{\eta}\_{i-1}}\right)}\right). |  |

We then have that

|  |  |  |  |
| --- | --- | --- | --- |
|  | logâ¡(CÎ·)=u~â€‹(logâ¡(Î·)âˆ’logâ¡(Î´â€‹t)âˆ’AÎ·)\log(C^{\eta})=\tilde{u}(\log(\eta)-\log(\delta t)-A^{\eta}) |  | (28) |

where CÎ·C^{\eta} is as defined in ([11](https://arxiv.org/html/2511.07045v1#A3.E11 "In item (b) â€£ Proposition C.3. â€£ C.1 Solution to the one-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")).
We have

|  |  |  |  |
| --- | --- | --- | --- |
|  | logâ¡(wÎ·)=logâ¡(elogâ¡(CÎ·)+âˆ‘i=1Nelogâ¡(st0)âˆ’râ€‹Î´â€‹t+logâ¡(xi)+logâ¡(elogâ¡QiÎ·âˆ’elogâ¡Qiâˆ’1Î·))\log(w^{\eta})=\log\left(e^{\log(C^{\eta})}+\sum\_{i=1}^{N}e^{\log(s\_{t\_{0}})-r\delta t+\log(x\_{i})+\log\left(e^{\log Q^{\eta}\_{i}}-e^{\log Q^{\eta}\_{i-1}}\right)}\right) |  | (29) |

and wÎ·w^{\eta} depends continuously upon Î·\eta.
If wt0>st0â€‹eâˆ’râ€‹Î´â€‹tâ€‹x1+Î³minw\_{t\_{0}}>s\_{t\_{0}}e^{-r\delta t}x\_{1}+\gamma\_{\min},
we may find the value of Î·wt0\eta\_{w\_{t\_{0}}} by finding
logâ¡(Î·)\log(\eta) such that logâ¡(wÎ·)=logâ¡(wt0)\log(w^{\eta})=\log(w\_{t\_{0}}).
We then have

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„“â€‹(vâ€‹(t0,wt0))=uâ€‹(Î³Î·)â€‹Î´â€‹tâˆ’AÎ·.\displaystyle\ell(v(t\_{0},w\_{t\_{0}}))=u(\gamma^{\eta})\delta t-A^{\eta}. |  | (30) |

If wt0<st0â€‹eâˆ’râ€‹Î´â€‹tâ€‹x1w\_{t\_{0}}<s\_{t\_{0}}e^{-r\delta t}x\_{1}, the maximum in ([14](https://arxiv.org/html/2511.07045v1#A3.E14 "In Lemma C.6. â€£ C.1 Solution to the one-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")) is âˆ’âˆ-\infty which is achieved by the negative consumption Î³=wt0âˆ’st0â€‹eâˆ’râ€‹Î´â€‹tâ€‹x1\gamma=w\_{t\_{0}}-s\_{t\_{0}}e^{-r\delta t}x\_{1}.

###### Proof.

Corresponding to ([25](https://arxiv.org/html/2511.07045v1#A3.E25 "In Proposition C.10. â€£ C.2 Numerical approximation of the multi-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")) we have a decreasing sequence of points pip\_{i} given
by

|  |  |  |  |
| --- | --- | --- | --- |
|  | pi={âˆi=0vâ€‹(xi+1)âˆ’vâ€‹(xi)xi+1âˆ’xi0<i<N0i=N.p\_{i}=\begin{cases}\infty&i=0\\ \frac{v(x\_{i+1})-v(x\_{i})}{x\_{i+1}-x\_{i}}&0<i<N\\ 0&i=N.\end{cases} |  | (31) |

We will then have

|  |  |  |
| --- | --- | --- |
|  | vâ€ â€‹(p)=âˆ‘i=1Nxiâ€‹ğŸ[pi,piâˆ’1)â€‹(p).v^{\dagger}(p)=\sum\_{i=1}^{N}x\_{i}{\bf 1}\_{[p\_{i},p\_{i-1})}(p). |  |

From ([10](https://arxiv.org/html/2511.07045v1#A3.E10 "In item (b) â€£ Proposition C.3. â€£ C.1 Solution to the one-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem"))

|  |  |  |
| --- | --- | --- |
|  | fÎ·â€‹(u)=âˆ‘i=1Nxiâ€‹ğŸ[pi,piâˆ’1)â€‹(Î·â€‹eâˆ’râ€‹Î´â€‹tâ€‹qBSAâ€‹(u)).f^{\eta}(u)=\sum\_{i=1}^{N}x\_{i}{\bf 1}\_{[p\_{i},p\_{i-1})}\left(\eta e^{-r\delta t}q^{A}\_{\mathrm{BS}}(u)\right). |  |

Hence we will be able to deduce that

|  |  |  |  |
| --- | --- | --- | --- |
|  | fÎ·â€‹(U)=âˆ‘i=1Nxiâ€‹ğŸ(Uiâˆ’1Î·,UiÎ·]â€‹(U)f^{\eta}(U)=\sum\_{i=1}^{N}x\_{i}{\bf 1}\_{(U^{\eta}\_{i-1},U^{\eta}\_{i}]}\left(U\right) |  | (32) |

if we can show ([26](https://arxiv.org/html/2511.07045v1#A3.E26 "In Proposition C.10. â€£ C.2 Numerical approximation of the multi-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")) ensures that

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î·â€‹st0â€‹eâˆ’râ€‹Î´â€‹tâ€‹qBSAâ€‹(UiÎ·)=pi.\eta s\_{t\_{0}}e^{-r\delta t}q^{A}\_{\mathrm{BS}}(U^{\eta}\_{i})=p\_{i}. |  | (33) |

Writing Ï•\phi for the pdf of the standard normal we compute

|  |  |  |  |
| --- | --- | --- | --- |
|  | qBSAâ€‹(u)\displaystyle q^{A}\_{\mathrm{BS}}(u) | =Ï•â€‹(M+Î¦âˆ’1â€‹(u))Ï•â€‹(Î¦âˆ’1â€‹(u))\displaystyle=\frac{\phi(M+\Phi^{-1}(u))}{\phi(\Phi^{-1}(u))} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =expâ¡(12â€‹(Î¦âˆ’1â€‹(u)2âˆ’(M+Î¦âˆ’1â€‹(u))2))\displaystyle=\exp\left(\frac{1}{2}(\Phi^{-1}(u)^{2}-(M+\Phi^{-1}(u))^{2})\right) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =expâ¡(âˆ’12â€‹M2âˆ’Mâ€‹Î¦âˆ’1â€‹(u)).\displaystyle=\exp\left(-\frac{1}{2}M^{2}-M\Phi^{-1}(u)\right). |  |

Hence equation ([33](https://arxiv.org/html/2511.07045v1#A3.E33 "In C.2 Numerical approximation of the multi-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")) is equivalent to

|  |  |  |  |
| --- | --- | --- | --- |
|  | UiÎ·=Î¦â€‹(âˆ’12â€‹Mâˆ’1Mâ€‹logâ¡(1Î·â€‹erâ€‹Î´â€‹tâ€‹pi)).U^{\eta}\_{i}=\Phi\left(-\frac{1}{2}M-\frac{1}{M}\log\left(\frac{1}{\eta}e^{r\delta t}p\_{i}\right)\right). |  | (34) |

for 0<i<N0<i<N, which will hold due to our definition ([26](https://arxiv.org/html/2511.07045v1#A3.E26 "In Proposition C.10. â€£ C.2 Numerical approximation of the multi-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")).
From ([11](https://arxiv.org/html/2511.07045v1#A3.E11 "In item (b) â€£ Proposition C.3. â€£ C.1 Solution to the one-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")) and ([32](https://arxiv.org/html/2511.07045v1#A3.E32 "In C.2 Numerical approximation of the multi-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")) we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | CÎ·\displaystyle C^{\eta} | =uâ€ â€‹(âˆ’Î·Î´â€‹tâ€‹(âˆ’1+st0â€‹âˆ«01(1+vâ€‹(âˆ‘i=1Nxiâ€‹ğŸ(Uiâˆ’1Î·,UiÎ·]â€‹(s)))â€‹ds)âˆ’1)\displaystyle=u^{\dagger}\left(-\frac{\eta}{\delta t}\left(-1+s\_{t\_{0}}\int\_{0}^{1}\left(1+v\left(\sum\_{i=1}^{N}x\_{i}{\bf 1}\_{(U^{\eta}\_{i-1},U^{\eta}\_{i}]}(s)\right)\right)\,\mathrm{d}s\right)^{-1}\right) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =uâ€ â€‹(âˆ’Î·Î´â€‹tâ€‹(âˆ’1+st0â€‹âˆ«01(1+âˆ‘i=1Nvâ€‹(xi)â€‹ğŸ(Uiâˆ’1Î·,UiÎ·]â€‹(s))â€‹ds)âˆ’1)\displaystyle=u^{\dagger}\left(-\frac{\eta}{\delta t}\left(-1+s\_{t\_{0}}\int\_{0}^{1}\left(1+\sum\_{i=1}^{N}v(x\_{i}){\bf 1}\_{(U^{\eta}\_{i-1},U^{\eta}\_{i}]}(s)\right)\,\mathrm{d}s\right)^{-1}\right) |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | =uâ€ â€‹(âˆ’Î·Î´â€‹tâ€‹(âˆ’1+st0â€‹(1+âˆ‘i=1Nvâ€‹(xi)â€‹(UiÎ·âˆ’Uiâˆ’1Î·)))âˆ’1).\displaystyle=u^{\dagger}\left(-\frac{\eta}{\delta t}\left(-1+s\_{t\_{0}}\left(1+\sum\_{i=1}^{N}v(x\_{i})(U^{\eta}\_{i}-U^{\eta}\_{i-1})\right)\right)^{-1}\right). |  | (35) |

Equation ([28](https://arxiv.org/html/2511.07045v1#A3.E28 "In Proposition C.10. â€£ C.2 Numerical approximation of the multi-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")) follows immediately.

Use ([12](https://arxiv.org/html/2511.07045v1#A3.E12 "In item (b) â€£ Proposition C.3. â€£ C.1 Solution to the one-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")) and ([8](https://arxiv.org/html/2511.07045v1#A3.E8 "In Definition C.2. â€£ C.1 Solution to the one-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")) to see that

|  |  |  |  |
| --- | --- | --- | --- |
|  | wÎ·\displaystyle w^{\eta} | =CÎ·+st0â€‹âˆ‘i=1Nâˆ«Uiâˆ’1Î·UiÎ·eâˆ’râ€‹Î´â€‹tâ€‹qBSAâ€‹(s)â€‹xiâ€‹ds\displaystyle=C^{\eta}+s\_{t\_{0}}\sum\_{i=1}^{N}\int\_{U^{\eta}\_{i-1}}^{U^{\eta}\_{i}}e^{-r\delta t}q^{A}\_{\mathrm{BS}}(s)x\_{i}\,\mathrm{d}s |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =CÎ·+st0â€‹âˆ‘i=1Neâˆ’râ€‹Î´â€‹tâ€‹xiâ€‹(Qâ€‹(UiÎ·)âˆ’Qâ€‹(Uiâˆ’1Î·))\displaystyle=C^{\eta}+s\_{t\_{0}}\sum\_{i=1}^{N}e^{-r\delta t}x\_{i}(Q(U^{\eta}\_{i})-Q(U^{\eta}\_{i-1})) |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | =CÎ·+st0âˆ‘i=1Neâˆ’râ€‹Î´â€‹txi(QiÎ·âˆ’Qiâˆ’1Î·))\displaystyle=C^{\eta}+s\_{t\_{0}}\sum\_{i=1}^{N}e^{-r\delta t}x\_{i}(Q^{\eta}\_{i}-Q^{\eta}\_{i-1})) |  | (36) |

The last line follows directly from our definitions of QQ, UiÎ·U^{\eta}\_{i} and QiÎ·Q^{\eta}\_{i}.
We now see that equation ([36](https://arxiv.org/html/2511.07045v1#A3.E36 "In C.2 Numerical approximation of the multi-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")) is equivalent to
([29](https://arxiv.org/html/2511.07045v1#A3.E29 "In Proposition C.10. â€£ C.2 Numerical approximation of the multi-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")).

Our explicit formula, ([34](https://arxiv.org/html/2511.07045v1#A3.E34 "In C.2 Numerical approximation of the multi-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")), for CÎ·C^{\eta} shows that it depends continuously Î·\eta given the assumption ([20](https://arxiv.org/html/2511.07045v1#A3.E20 "In C.2 Numerical approximation of the multi-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")).
It then follows from equation ([36](https://arxiv.org/html/2511.07045v1#A3.E36 "In C.2 Numerical approximation of the multi-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")) that wÎ·w^{\eta} depends continuously on Î·\eta.
Lemmas ([C.11](https://arxiv.org/html/2511.07045v1#A3.Thmtheorem11 "Lemma C.11. â€£ C.2 Numerical approximation of the multi-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")) and ([C.13](https://arxiv.org/html/2511.07045v1#A3.Ex68 "Lemma C.13. â€£ C.2 Numerical approximation of the multi-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")) below then establish
that we can solve for Î·\eta in wÎ·=wt0w^{\eta}=w\_{t\_{0}} under the conditions of the proposition.

The value function
is then given by

|  |  |  |  |
| --- | --- | --- | --- |
|  | vâ€‹(t0,wÎ·)\displaystyle v(t\_{0},w^{\eta}) | =expâ¡(âˆ’uâ€‹(CÎ·)â€‹Î´â€‹t)â€‹(âˆ’1+st0â€‹âˆ«01(1+vâ€‹(âˆ‘i=1Nxiâ€‹ğŸ(Uiâˆ’1Î·,UiÎ·]â€‹(s)))â€‹ds)\displaystyle=\exp(-u(C^{\eta})\delta t)\left(-1+s\_{t\_{0}}\int\_{0}^{1}(1+v(\sum\_{i=1}^{N}x\_{i}{\bf 1}\_{(U^{\eta}\_{i-1},U^{\eta}\_{i}]}\left(s\right)))\,\mathrm{d}s\right) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =expâ¡(âˆ’uâ€‹(CÎ·)â€‹Î´â€‹t)â€‹(âˆ’1+st0â€‹(1+âˆ‘i=1Nvâ€‹(xi)â€‹(UiÎ·âˆ’Uiâˆ’1Î·)))\displaystyle=\exp(-u(C^{\eta})\delta t)\left(-1+s\_{t\_{0}}(1+\sum\_{i=1}^{N}v(x\_{i})(U^{\eta}\_{i}-U^{\eta}\_{i-1}))\right) |  |

and so ([30](https://arxiv.org/html/2511.07045v1#A3.E30 "In Proposition C.10. â€£ C.2 Numerical approximation of the multi-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")) also follows.
âˆ

###### Lemma C.11.

Under the assumptions of Proposition [C.10](https://arxiv.org/html/2511.07045v1#A3.Thmtheorem10 "Proposition C.10. â€£ C.2 Numerical approximation of the multi-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem"),

|  |  |  |
| --- | --- | --- |
|  | limÎ·â†’0wÎ·=âˆ.\lim\_{\eta\to 0}w^{\eta}=\infty. |  |

###### Proof.

Our assumptions on vv ensure that

|  |  |  |
| --- | --- | --- |
|  | âˆ’1+st0â€‹âˆ«01(1+vâ€‹(fÎ·â€‹(s)))â€‹dsâ‰¤âˆ’1+st0<0.-1+s\_{t\_{0}}\int\_{0}^{1}(1+v(f^{\eta}(s)))\,\mathrm{d}s\leq-1+s\_{t\_{0}}<0. |  |

Hence

|  |  |  |
| --- | --- | --- |
|  | 0>(âˆ’1+st0â€‹âˆ«01(1+vâ€‹(fÎ·â€‹(s))))âˆ’1<1âˆ’1+st0.0>\left(-1+s\_{t\_{0}}\int\_{0}^{1}(1+v(f^{\eta}(s)))\right)^{-1}<\frac{1}{-1+s\_{t\_{0}}}. |  |

It now follows from our equation ([21](https://arxiv.org/html/2511.07045v1#A3.E21 "In C.2 Numerical approximation of the multi-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")) coupled with equation ([11](https://arxiv.org/html/2511.07045v1#A3.E11 "In item (b) â€£ Proposition C.3. â€£ C.1 Solution to the one-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem"))
that

|  |  |  |
| --- | --- | --- |
|  | limÎ·â†’0CÎ·=âˆ.\lim\_{\eta\to 0}C^{\eta}=\infty. |  |

The result now follows from ([12](https://arxiv.org/html/2511.07045v1#A3.E12 "In item (b) â€£ Proposition C.3. â€£ C.1 Solution to the one-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")).
âˆ

###### Lemma C.12.

Under the assumptions of Proposition [C.10](https://arxiv.org/html/2511.07045v1#A3.Thmtheorem10 "Proposition C.10. â€£ C.2 Numerical approximation of the multi-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem"),

|  |  |  |
| --- | --- | --- |
|  | limÎ·â†’âˆCÎ·=Cmin.\lim\_{\eta\to\infty}C^{\eta}=C\_{\min}. |  |

###### Proof.

Our assumptions on vv ensure that

|  |  |  |
| --- | --- | --- |
|  | (âˆ’1+st0â€‹âˆ«01(1+vâ€‹(fÎ·â€‹(s)))â€‹ds)âˆ’1\left(-1+s\_{t\_{0}}\int\_{0}^{1}(1+v(f^{\eta}(s)))\,\mathrm{d}s\right)^{-1} |  |

is bounded. Hence using the expression ([11](https://arxiv.org/html/2511.07045v1#A3.E11 "In item (b) â€£ Proposition C.3. â€£ C.1 Solution to the one-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem"))
combined with assumption ([22](https://arxiv.org/html/2511.07045v1#A3.E22 "In C.2 Numerical approximation of the multi-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")) we find CÎ·â†’0C^{\eta}\to 0
as Î·â†’âˆ\eta\to\infty.
âˆ

###### Lemma C.13.

Under the assumptions of Proposition [C.10](https://arxiv.org/html/2511.07045v1#A3.Thmtheorem10 "Proposition C.10. â€£ C.2 Numerical approximation of the multi-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem"),

|  |  |  |
| --- | --- | --- |
|  | limÎ·â†’âˆwÎ·=Î³min+st0â€‹eâˆ’râ€‹Î´â€‹tâ€‹x1.\lim\_{\eta\to\infty}w^{\eta}=\gamma\_{\min}+s\_{t\_{0}}e^{-r\delta t}x\_{1}. |  |

###### Proof.

Define

|  |  |  |
| --- | --- | --- |
|  | pâˆ—=infâˆ‚vâ€‹(x1).p^{\*}=\inf\partial v(x\_{1}). |  |

For Î·>0\eta>0, define

|  |  |  |  |
| --- | --- | --- | --- |
|  | sÎ·âˆ—=qBSAâ€‹(pâˆ—â€‹Î·âˆ’1â€‹erâ€‹t),s^{\*}\_{\eta}=q^{A}\_{\mathrm{BS}}(p^{\*}\eta^{-1}e^{rt}), |  | (37) |

which ensures that

|  |  |  |  |
| --- | --- | --- | --- |
|  | sâ‰¥sâˆ—â‡”Î·â€‹erâ€‹tâ€‹qBSAâ€‹(s)<pâ‹†.s\geq s^{\*}\iff\eta e^{rt}q^{A}\_{\mathrm{BS}}(s)<p^{\star}. |  | (38) |

We compute

|  |  |  |  |
| --- | --- | --- | --- |
|  | âˆ«01qBSAâ€‹(s)â€‹fÎ·â€‹(s)â€‹ds\displaystyle\int\_{0}^{1}q^{A}\_{\mathrm{BS}}(s)f^{\eta}(s)\mathrm{d}s | =âˆ«01qBSAâ€‹(s)â€‹vâ€ â€‹(Î·â€‹eâˆ’râ€‹tâ€‹qBSAâ€‹(s))â€‹ds\displaystyle=\int\_{0}^{1}q^{A}\_{\mathrm{BS}}(s)v^{\dagger}(\eta e^{-rt}q^{A}\_{\mathrm{BS}}(s))\,\mathrm{d}s |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =âˆ«0sÎ·âˆ—qBSAâ€‹(s)â€‹vâ€ â€‹(Î·â€‹eâˆ’râ€‹tâ€‹qBSAâ€‹(s))â€‹ds\displaystyle=\int\_{0}^{s^{\*}\_{\eta}}q^{A}\_{\mathrm{BS}}(s)v^{\dagger}(\eta e^{-rt}q^{A}\_{\mathrm{BS}}(s))\,\mathrm{d}s |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +1Î·â€‹eâˆ’râ€‹tâ€‹âˆ«sÎ·âˆ—1Î·â€‹eâˆ’râ€‹tâ€‹qBSAâ€‹(s)â€‹vâ€ â€‹(Î·â€‹eâˆ’râ€‹tâ€‹qBSAâ€‹(s))â€‹ds\displaystyle\qquad+\frac{1}{\eta e^{-rt}}\int\_{s^{\*}\_{\eta}}^{1}\eta e^{-rt}\,q^{A}\_{\mathrm{BS}}(s)v^{\dagger}(\eta e^{-rt}q^{A}\_{\mathrm{BS}}(s))\,\mathrm{d}s |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤âˆ«0sÎ·âˆ—qBSAâ€‹(s)â€‹x1â€‹ds\displaystyle\leq\int\_{0}^{s^{\*}\_{\eta}}q^{A}\_{\mathrm{BS}}(s)x\_{1}\,\mathrm{d}s |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | +1Î·â€‹eâˆ’râ€‹tâ€‹âˆ«sÎ·âˆ—1Î·â€‹eâˆ’râ€‹tâ€‹qBSAâ€‹(s)â€‹vâ€ â€‹(Î·â€‹eâˆ’râ€‹tâ€‹qBSAâ€‹(s))â€‹ds\displaystyle\qquad+\frac{1}{\eta e^{-rt}}\int\_{s^{\*}\_{\eta}}^{1}\eta e^{-rt}\,q^{A}\_{\mathrm{BS}}(s)v^{\dagger}(\eta e^{-rt}q^{A}\_{\mathrm{BS}}(s))\,\mathrm{d}s |  | (39) |

We note that pâˆˆâˆ‚vâ€‹(vâ€ â€‹(p))p\in\partial v(v^{\dagger}(p)).
By the definition of the subdifferential at vâ€ â€‹(p)v^{\dagger}(p)

|  |  |  |
| --- | --- | --- |
|  | vâ€‹(x)â‰¤vâ€‹(vâ€ â€‹(p))+pâ€‹(xâˆ’vâ€ â€‹(p)).v(x)\leq v(v^{\dagger}(p))+p(x-v^{\dagger}(p)). |  |

Rearranging yields

|  |  |  |
| --- | --- | --- |
|  | pâ€‹vâ€ â€‹(p)â‰¤pâ€‹x+vâ€‹(vâ€ â€‹(p))âˆ’vâ€‹(x).pv^{\dagger}(p)\leq px+v(v^{\dagger}(p))-v(x). |  |

Using the fact vv is increasing and substituting x1x\_{1} for xx we find that for all pp

|  |  |  |
| --- | --- | --- |
|  | pâ€‹vâ€ â€‹(p)â‰¤pâ€‹x1+vâ€‹(xN)âˆ’vâ€‹(x1).pv^{\dagger}(p)\leq px\_{1}+v(x\_{N})-v(x\_{1}). |  |

Using this inequality in ([39](https://arxiv.org/html/2511.07045v1#A3.E39 "In C.2 Numerical approximation of the multi-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")) we find

|  |  |  |  |
| --- | --- | --- | --- |
|  | âˆ«01qBSAâ€‹(s)â€‹fÎ·â€‹(s)â€‹ds\displaystyle\int\_{0}^{1}q^{A}\_{\mathrm{BS}}(s)f^{\eta}(s)\mathrm{d}s | â‰¤âˆ«0sÎ·âˆ—qBSAâ€‹(s)â€‹x1â€‹ds\displaystyle\leq\int\_{0}^{s^{\*}\_{\eta}}q^{A}\_{\mathrm{BS}}(s)x\_{1}\,\mathrm{d}s |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +1Î·â€‹eâˆ’râ€‹tâ€‹âˆ«sÎ·âˆ—1(Î·â€‹eâˆ’râ€‹tâ€‹qBSAâ€‹(s)â€‹x1+vâ€‹(xN)âˆ’vâ€‹(x1))â€‹ds\displaystyle\qquad+\frac{1}{\eta e^{-rt}}\int\_{s^{\*}\_{\eta}}^{1}(\eta e^{-rt}\,q^{A}\_{\mathrm{BS}}(s)x\_{1}+v(x\_{N})-v(x\_{1}))\,\mathrm{d}s |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤âˆ«0sÎ·âˆ—qBSAâ€‹(s)â€‹x1â€‹ds\displaystyle\leq\int\_{0}^{s^{\*}\_{\eta}}q^{A}\_{\mathrm{BS}}(s)x\_{1}\,\mathrm{d}s |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | +1Î·â€‹eâˆ’râ€‹tâ€‹âˆ«sÎ·âˆ—1(pâˆ—â€‹x1+vâ€‹(xN)âˆ’vâ€‹(x1))â€‹ds.\displaystyle\qquad+\frac{1}{\eta e^{-rt}}\int\_{s^{\*}\_{\eta}}^{1}(p^{\*}x\_{1}+v(x\_{N})-v(x\_{1}))\,\mathrm{d}s. |  | (41) |

by ([38](https://arxiv.org/html/2511.07045v1#A3.E38 "In C.2 Numerical approximation of the multi-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")). From ([37](https://arxiv.org/html/2511.07045v1#A3.E37 "In C.2 Numerical approximation of the multi-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem"))

|  |  |  |
| --- | --- | --- |
|  | limÎ·â†’âˆsÎ·âˆ—=1.\lim\_{\eta\to\infty}s^{\*}\_{\eta}=1. |  |

We may therefore take the limit of the inequality ([41](https://arxiv.org/html/2511.07045v1#A3.E41 "In C.2 Numerical approximation of the multi-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem"))
to find

|  |  |  |
| --- | --- | --- |
|  | lim infÎ·>0âˆ«01qBSAâ€‹(s)â€‹fÎ·â€‹(s)â€‹dsâ‰¤x1.\liminf\_{\eta>0}\int\_{0}^{1}q^{A}\_{\mathrm{BS}}(s)f^{\eta}(s)\mathrm{d}s\leq x\_{1}. |  |

Using this, Lemma [C.12](https://arxiv.org/html/2511.07045v1#A3.Ex66 "Lemma C.12. â€£ C.2 Numerical approximation of the multi-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem") and the definition of wÎ·w^{\eta} in
equation ([12](https://arxiv.org/html/2511.07045v1#A3.E12 "In item (b) â€£ Proposition C.3. â€£ C.1 Solution to the one-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")) we find

|  |  |  |
| --- | --- | --- |
|  | lim infÎ·>0wÎ·â‰¤Cmin+st0â€‹eâˆ’râ€‹Î´â€‹tâ€‹x1.\liminf\_{\eta>0}w^{\eta}\leq C\_{\min}+s\_{t\_{0}}e^{-r\delta t}x\_{1}. |  |

From ([11](https://arxiv.org/html/2511.07045v1#A3.E11 "In item (b) â€£ Proposition C.3. â€£ C.1 Solution to the one-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")) and ([12](https://arxiv.org/html/2511.07045v1#A3.E12 "In item (b) â€£ Proposition C.3. â€£ C.1 Solution to the one-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")) one sees that, on the other hand,
for all Î·>0\eta>0 we have

|  |  |  |
| --- | --- | --- |
|  | wÎ·â‰¥Cmin+st0â€‹eâˆ’râ€‹Î´â€‹tâ€‹x1.w^{\eta}\geq C\_{\min}+s\_{t\_{0}}e^{-r\delta t}x\_{1}. |  |

The result follows.
âˆ

###### Remark C.14.

We note that that if we follow the optimal investment
strategy at time tt, then the optimal investment strategy will result
in a wealth at time t+Î´â€‹tt+\delta t which takes values in the grid {x1,â€¦,xn}\{x\_{1},\ldots,x\_{n}\}.
We may then approximate the value function on the space-time grid
{x1,â€¦â€‹xn}Ã—{0,Î´â€‹t,2â€‹Î´â€‹t,â€¦,T}\{x\_{1},\ldots x\_{n}\}\times\{0,\delta t,2\delta t,\ldots,T\}.
One can then obtain a simulation of the optimal strategy by first simulating
the stock price on the time grid and then computing the corresponding dynamics
of xtx\_{t} in the grid {x1,â€¦â€‹xn}\{x\_{1},\ldots x\_{n}\} using this approximation to the value
function. Since the wealth process never leaves a fixed space-time grid, we can use
the same approximation of the value function for all the scenarios.

###### Remark C.15.

When implementing this algorithm we notice that many values
of UiÎ·U\_{i}^{\eta} will be extremely close to either 0 or 1, and so including
these terms will have a negligible effect on the values of the sums
in the equations ([28](https://arxiv.org/html/2511.07045v1#A3.E28 "In Proposition C.10. â€£ C.2 Numerical approximation of the multi-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")), ([29](https://arxiv.org/html/2511.07045v1#A3.E29 "In Proposition C.10. â€£ C.2 Numerical approximation of the multi-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")).
Financially this is equivalent to ignoring extreme events of
very low probability where the
â„™\mathbb{P} and â„š\mathbb{Q} disagree by a large amount. Since our payoff functions ff
take values in ğ’³{\cal X}, and so are bounded and positive, ignoring
these extreme events will have no material impact upon either the price or the expected utility.
The value we chose in our numerical calculations was Ïµ=10âˆ’10â€‹maxâ¡|vâ€‹(xi)|âˆ’1\epsilon=10^{-10}\max{|v(x\_{i})|}^{-1}.

This can be used to speed up the algorithm. When
calculating wÎ·w^{\eta}, choose some small Ïµ\epsilon and define

|  |  |  |  |
| --- | --- | --- | --- |
|  | imin\displaystyle i\_{\min} | :=maxâ¡{1}âˆª{iâˆ£Ui<Ïµ}\displaystyle:=\max\{1\}\cup\{i\mid U\_{i}<\epsilon\} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | imax\displaystyle i\_{\max} | :=minâ¡{N}âˆª{iâˆ£Ui>1âˆ’Ïµ}.\displaystyle:=\min\{N\}\cup\{i\mid U\_{i}>1-\epsilon\}. |  |

To compute these values and the values of UiU\_{i},
first use the method of bisection to find some
iâˆ—i^{\*} where Ïµ<Uiâˆ—<1âˆ’Ïµ\epsilon<U\_{i^{\*}}<1-\epsilon. Then compute the values of UiU\_{i}
from iâˆ—i^{\*} down to imini\_{\min}, stopping when Ui<ÏµU\_{i}<\epsilon. Similarly compute the values of UiU\_{i} from iâˆ—i^{\*} up to imaxi\_{\max}, stopping when Ui>1âˆ’ÏµU\_{i}>1-\epsilon.
No other values of UiU\_{i} outside the range iminâˆ’1â‰¤iâ‰¤imaxi\_{\min-1}\leq i\leq i\_{\max} are then needed in the computation of wÎ·w^{\eta}.

When computing the values of the sums in ([28](https://arxiv.org/html/2511.07045v1#A3.E28 "In Proposition C.10. â€£ C.2 Numerical approximation of the multi-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")), ([29](https://arxiv.org/html/2511.07045v1#A3.E29 "In Proposition C.10. â€£ C.2 Numerical approximation of the multi-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")) use indices running from imini\_{\min} to imaxi\_{\max} rather than form 11 to nn.

## Appendix D Proof of Lemma [C.5](https://arxiv.org/html/2511.07045v1#A3.Thmtheorem5 "Lemma C.5. â€£ C.1 Solution to the one-period problem â€£ Appendix C A convergent algorithm for the discrete-consumption, continuous-investment problem â€£ Machine-learning a family of solutions to an optimal pension investment problem")

###### Proof.

If Î¼=r\mu=r, then the result is trivial. We will consider
the case Î¼>r\mu>r, the case Î¼<r\mu<r is similar.

The classification of complete markets already shows that
the Blackâ€“Scholesâ€“Merton market over the time period [t0,t1][t\_{0},t\_{1}]
is isomorphic to a
market of this form for an appropriate choice of qAq^{A}
which we will call qBSAq^{A}\_{\mathrm{BS}}. Let
dâ€‹â„šdâ€‹â„™\frac{\mathrm{d}\mathbb{Q}}{\mathrm{d}\mathbb{P}} denote the Radonâ€“Nikodym derivative
of the measures â„š\mathbb{Q} and â„™\mathbb{P} in the Blackâ€“Scholesâ€“Merton market.
Let Fdâ€‹â„šdâ€‹â„™F\_{\frac{\mathrm{d}\mathbb{Q}}{\mathrm{d}\mathbb{P}}} denote the â„™\mathbb{P}-measure distribution
function of the Radonâ€“Nikodym derivative.
The classification theorem moreover gives us
an isomorphism
for both the â„™\mathbb{P} and â„š\mathbb{Q} measures which maps the uniformly distributed random variable Uâ€²:=Fdâ€‹â„šdâ€‹â„™â€‹(dâ€‹Qdâ€‹P)U^{\prime}:=F\_{\frac{\mathrm{d}\mathbb{Q}}{\mathrm{d}\mathbb{P}}}(\frac{\mathrm{d}Q}{\mathrm{d}P})
to UU. In particular this tells us that

|  |  |  |  |
| --- | --- | --- | --- |
|  | âˆ«0wqBSAâ€‹(s)â€‹ds=â„™â„šAâ€‹(Uâ‰¤w)=â„™â„šâ€‹(Fdâ€‹â„šdâ€‹â„™â€‹(Uâ€²â‰¤w))\int\_{0}^{w}q^{A}\_{\mathrm{BS}}(s)\mathrm{d}s=\mathbb{P}\_{\mathbb{Q}^{A}}(U\leq w)=\mathbb{P}\_{\mathbb{Q}}(F\_{\frac{\mathrm{d}\mathbb{Q}}{\mathrm{d}\mathbb{P}}}(U^{\prime}\leq w)) |  | (42) |

Differentiating this, we may obtain an expression for qBSAq^{A}\_{\mathrm{BS}}.

The â„™\mathbb{P} measure distribution function of the log stock price, zt1=logâ¡(St1)z\_{t\_{1}}=\log(S\_{t\_{1}}) given the log stock price zt1z\_{t\_{1}}
in the Blackâ€“Scholesâ€“Merton model is

|  |  |  |
| --- | --- | --- |
|  | pâ€‹(z)=12â€‹Ï€â€‹Ïƒâ€‹Î´â€‹tâ€‹expâ¡(âˆ’(zâˆ’(zt0+(Î¼âˆ’12â€‹Ïƒ2)â€‹Î´â€‹t))22â€‹Ïƒ2â€‹Î´â€‹t).p(z)=\frac{1}{\sqrt{2\pi\sigma\delta t}}\exp\left(-\frac{(z-(z\_{t\_{0}}+(\mu-\frac{1}{2}\sigma^{2})\delta t))^{2}}{2\sigma^{2}\delta t}\right). |  |

Similarly the
â„š\mathbb{Q} measure distribution function of zt1z\_{t\_{1}} is

|  |  |  |
| --- | --- | --- |
|  | qâ€‹(z)=12â€‹Ï€â€‹Ïƒâ€‹Î´â€‹tâ€‹expâ¡(âˆ’(zâˆ’(zt0+(râˆ’12â€‹Ïƒ2)â€‹Î´â€‹t))22â€‹Ïƒ2â€‹Î´â€‹t).q(z)=\frac{1}{\sqrt{2\pi\sigma\delta t}}\exp\left(-\frac{(z-(z\_{t\_{0}}+(r-\frac{1}{2}\sigma^{2})\delta t))^{2}}{2\sigma^{2}\delta t}\right). |  |

The standard computation of the â„š\mathbb{Q} measure using Girsanovâ€™s theorem
shows that

|  |  |  |  |
| --- | --- | --- | --- |
|  | dâ€‹â„šdâ€‹â„™â€‹(z)\displaystyle\frac{\mathrm{d}\mathbb{Q}}{\mathrm{d}\mathbb{P}}(z) | =qâ€‹(z)pâ€‹(z).\displaystyle=\frac{q(z)}{p(z)}. |  |

Hence

|  |  |  |  |
| --- | --- | --- | --- |
|  | dâ€‹â„šdâ€‹â„™â€‹(z)\displaystyle\frac{\mathrm{d}\mathbb{Q}}{\mathrm{d}\mathbb{P}}(z) | =expâ¡(âˆ’(zâˆ’(zt0+(râˆ’12â€‹Ïƒ2)â€‹Î´â€‹t))2âˆ’(zâˆ’(zt0+(Î¼âˆ’12â€‹Ïƒ2)â€‹Î´â€‹t))22â€‹Ïƒ2â€‹Î´â€‹t).\displaystyle=\exp\left(-\frac{(z-(z\_{t\_{0}}+(r-\frac{1}{2}\sigma^{2})\delta t))^{2}-(z-(z\_{t\_{0}}+(\mu-\frac{1}{2}\sigma^{2})\delta t))^{2}}{2\sigma^{2}\delta t}\right). |  |

Note that the term in side the exp\exp is linear in zz, so dâ€‹â„šdâ€‹â„™\frac{\mathrm{d}\mathbb{Q}}{\mathrm{d}\mathbb{P}}
is decreasing. Hence
Uâ€²â€‹(z)U^{\prime}(z) is decreasing, and we recall that Uâ€²U^{\prime} is uniformly distributed. Hence, Uâ€²â€‹(z)=1âˆ’Fzâ€‹(z)U^{\prime}(z)=1-F\_{z}(z) where FzF\_{z} is the â„™\mathbb{P}-measure
distribution function of ztz\_{t}. But conditioned on zt0z\_{t\_{0}}, zt1z\_{t\_{1}} is normally distributed with mean Î¼âˆ’12â€‹Ïƒ2\mu-\tfrac{1}{2}\sigma^{2} and
standard deviation Ïƒâ€‹Î´â€‹t\sigma\sqrt{\delta t}. Hence

|  |  |  |
| --- | --- | --- |
|  | zt1=zt0+(Î¼âˆ’12â€‹Ïƒ2)â€‹Î´â€‹t+Ïƒâ€‹Î´â€‹tâ€‹Î¦âˆ’1â€‹(Uâ€²)z\_{t\_{1}}=z\_{t\_{0}}+(\mu-\tfrac{1}{2}\sigma^{2})\delta t+\sigma\sqrt{\delta t}\,\Phi^{-1}(U^{\prime}) |  |

where Î¦\Phi is the inverse distribution function of the standard normal
distribution.

We now compute

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„™â„šâ€‹(Uâ€²â‰¤w)\displaystyle\mathbb{P}\_{\mathbb{Q}}(U^{\prime}\leq w) | =â„™â„šâ€‹(zt1â‰¤zt0+(Î¼âˆ’12â€‹Ïƒ2)â€‹Î´â€‹t+Ïƒâ€‹Î´â€‹tâ€‹Î¦âˆ’1â€‹(w))\displaystyle=\mathbb{P}\_{\mathbb{Q}}(z\_{t\_{1}}\leq z\_{t\_{0}}+(\mu-\tfrac{1}{2}\sigma^{2})\delta t+\sigma\sqrt{\delta t}\Phi^{-1}(w)) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =â„™â„šâ€‹(zt1â‰¤zt0+(râˆ’12â€‹Ïƒ2)â€‹Î´â€‹t+(Î¼âˆ’r)â€‹Î´â€‹t+Ïƒâ€‹Î´â€‹tâ€‹Î¦âˆ’1â€‹(w)).\displaystyle=\mathbb{P}\_{\mathbb{Q}}(z\_{t\_{1}}\leq z\_{t\_{0}}+(r-\tfrac{1}{2}\sigma^{2})\delta t+(\mu-r)\delta t+\sigma\sqrt{\delta t}\Phi^{-1}(w)). |  |

Since zt1z\_{t\_{1}} is normally distributed in the â„š\mathbb{Q} measure with
mean râˆ’12â€‹Ïƒ2r-\tfrac{1}{2}\sigma^{2} and standard deviation Ïƒâ€‹Î´â€‹t\sigma\sqrt{\delta t}
we find

|  |  |  |
| --- | --- | --- |
|  | â„™â„šâ€‹(Uâ€²â‰¤w)=Î¦â€‹(|(Î¼âˆ’r)â€‹Î´â€‹tÏƒ|+Î¦âˆ’1â€‹(w)).\mathbb{P}\_{\mathbb{Q}}(U^{\prime}\leq w)=\Phi\left(\left|\frac{(\mu-r)\sqrt{\delta t}}{\sigma}\right|+\Phi^{-1}(w)\right). |  |

Combining this with ([42](https://arxiv.org/html/2511.07045v1#A4.E42 "In Appendix D Proof of Lemma C.5 â€£ Machine-learning a family of solutions to an optimal pension investment problem")), we get the result.
âˆ