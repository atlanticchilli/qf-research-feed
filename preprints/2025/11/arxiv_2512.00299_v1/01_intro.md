---
authors:
- Zeyun Hu
- Yang Liu
doc_id: arxiv:2512.00299v1
family_id: arxiv:2512.00299
is_current: true
taxonomy:
  alpha_families: []
  asset_classes: []
  horizons: []
  themes: []
title: 'Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region
  Algorithm and Neural Network'
url_abs: http://arxiv.org/abs/2512.00299v1
url_html: https://arxiv.org/html/2512.00299v1
venue: arXiv q-fin
version: 1
year: 2025
---


Zeyun Hu
School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, China. Email: zeyunhu@link.cuhk.edu.cn
â€ƒâ€ƒ
Yang Liu
Corresponding author. School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, China. Email: yangliu16@cuhk.edu.cn

###### Abstract

We investigate the static portfolio selection problem of S-shaped and non-concave utility maximization under first-order and second-order stochastic dominance (SD) constraints. In many S-shaped utility optimization problems, one should require a liquidation boundary to guarantee the existence of a finite concave envelope function. A first-order SD (FSD) constraint can replace this requirement and provide an alternative for risk management. We explicitly solve the optimal solution under a general S-shaped utility function with a first-order stochastic dominance constraint. However, the second-order SD (SSD) constrained problem under non-concave utilities is difficult to solve analytically due to the invalidity of Sionâ€™s maxmin theorem.
For this sake, we propose a numerical algorithm to obtain a plausible and sub-optimal solution for general non-concave utilities. The key idea is to detect the poor performance region with respect to the SSD constraints, characterize its structure and modify the distribution on that region to obtain (sub-)optimality. A key financial insight is that the decision maker should follow the SD constraint on the poor performance scenario while conducting the unconstrained optimal strategy otherwise. We provide numerical experiments to show that our algorithm effectively finds a sub-optimal solution in many cases. Finally, we develop an algorithm-guided piecewise-neural-network framework to learn the solution of the SSD problem, which demonstrates accelerated convergence compared to standard neural network approaches.

Keywords: Non-concave utility, portfolio selection, risk constraints, first-order stochastic dominance (FSD), second-order stochastic dominance (SSD), numerical method, neural network

## 1 Introduction

S-shaped utility functions, formalized in cumulative prospect theory (Tversky and Kahneman ([1992](https://arxiv.org/html/2512.00299v1#bib.bib16))) and surveyed in behavioral finance (Barberis and Thaler ([2003](https://arxiv.org/html/2512.00299v1#bib.bib1))), capture two salient features of investor behavior: risk seeking in losses and risk aversion in gains. In portfolio selection problems, this non-concavity can induce aggressive tail-risk taking (Carpenter ([2000](https://arxiv.org/html/2512.00299v1#bib.bib2)); He and Kou ([2018](https://arxiv.org/html/2512.00299v1#bib.bib5)); Liang and Liu ([2020](https://arxiv.org/html/2512.00299v1#bib.bib10))). In particular, when the pricing kernel has a heavy right tail, unconstrained S-shaped optimization typically generates a heavy left tail in terminal wealth, leading to large probabilities of extreme losses, which are the risks that classical VaR/ES-type constraints do not reliably mitigate.

We adopt stochastic dominance (SD) constraints as an implementable approach to control such downside risk under non-concave utilities. First-order SD (FSD) relative to a benchmark wealth X0X\_{0} enforces a quantile-by-quantile floor, offering a distribution-level safety guarantee that often aligns better with practice than hard liquidation boundaries. Second-order SD (SSD) controls cumulative quantiles, providing a flexible, model-free form of downside protection from the pathological risk-seeking induced by the convex (loss) region of S-shaped preferences.

Analytically, SD-constrained utility maximization is well understood for strictly concave utilities via quantile reformulations, duality, and saddle-point arguments (FÃ¶llmer and Schied ([2016](https://arxiv.org/html/2512.00299v1#bib.bib3)); Wang and Xia ([2021](https://arxiv.org/html/2512.00299v1#bib.bib18))). However, extending these tools to non-concave utilities under SSD is challenging. Sionâ€™s max-min theorem no longer applies in the key saddle-point step, concavification is not guaranteed to be valid because optimizers can fall in regions where the utility and its concave envelope differ (Liang and Liu ([2024](https://arxiv.org/html/2512.00299v1#bib.bib11))), and closed-form solutions are scarce beyond special, technically constrained settings (Wang and Xia ([2021](https://arxiv.org/html/2512.00299v1#bib.bib18))). This paper contributes on three aspects.

First, we derive an explicit optimal solution for portfolio selection with a general S-shaped utility under an FSD constraint relative to a benchmark. Crucially, the FSD constraint obviates the need for a liquidation boundary: the optimal wealth is counter-comonotone with the pricing kernel and exhibits a two-regime structure, coinciding with the classical solution in favorable states and binding to the benchmark quantile in adverse states. This clarifies how FSD serves as a â€œsoftâ€ and interpretable left-tail boundary.

Second, we propose the Poor-Performance-Region Algorithm (PPRA), a numerical method that constructs a feasible, high-quality suboptimal solution, and in many cases a numerically optimal one. The key idea is to identify the â€œpoor performance regionâ€, namely quantile levels where the unconstrained classical optimizer violates SSD relative to the benchmark. The algorithm partitions this region and applies a state-dependent correction to enforce SSD, while reverting to the unconstrained policy elsewhere. Financially, the resulting rule is intuitive: track the benchmark in poor states to satisfy SSD; otherwise follow the classical optimizer.

Third, we develop an algorithm-guided piecewise-neural-network framework that embeds the PPRA-derived partition and analytic priors into the architecture. This design drastically narrows the functional search space, accelerates convergence, satisfies budget and SSD constraints more quickly, and attains higher objective values than a standard monolithic network, especially in non-concave settings where regular training struggles with infeasibility and local minima.

Methodologically, our approach combines duality and concavification insights (Karatzas et al. ([1987](https://arxiv.org/html/2512.00299v1#bib.bib7)); Carpenter ([2000](https://arxiv.org/html/2512.00299v1#bib.bib2)); Liang and Liu ([2020](https://arxiv.org/html/2512.00299v1#bib.bib10))) with the quantile formulation of utility maximization (He and Zhou ([2011](https://arxiv.org/html/2512.00299v1#bib.bib6)); FÃ¶llmer and Schied ([2016](https://arxiv.org/html/2512.00299v1#bib.bib3))) and SD theory (Wang and Xia ([2021](https://arxiv.org/html/2512.00299v1#bib.bib18)); Wang, Wei and Xia ([2024](https://arxiv.org/html/2512.00299v1#bib.bib17))). For FSD, we provide a closed-form solution without a liquidation boundary and interpret FSD as a distributional safety floor. For SSD, we translate feasibility into integral inequalities in the quantile domain and design a correction that â€œrepairsâ€ exactly where the unconstrained policy underperforms the benchmark.

We validate our methods in a complete-market (Black-Scholes) setting across diverse utilities (CRRA, exponential, log, S-shaped, and piecewise) and benchmarks (log-normal, normal, exponential, uniform). For FSD, the explicit solution confirms that dominance constraints can substitute for liquidation boundaries. For SSD, PPRA consistently produces feasible, interpretable solutions that often match known optima in concave cases. The piecewise neural network guided by PPRA converges substantially faster and to better solutions than a monolithic network, particularly under non-concavity.

Financial implications are immediate.
First, FSD guarantees that all terminal-wealth quantiles exceed those of a benchmark, giving a realistic and implementable floor.
Second, the optimal/near-optimal policy is to adhere to the benchmark in bad states and follow the classical policy otherwise, yielding transparent risk management. Third, SD constraints significantly reduce the probability of extreme losses induced by S-shaped preferences, beyond what standard VaR/ES controls typically achieve.

Scope and limitations. Our analysis focuses on complete markets and static terminal-wealth problems, leveraging their equivalence to dynamic continuous-time settings via the pricing kernel. While PPRA is broadly applicable and robust in experiments, it provides suboptimality guarantees rather than universal optimality in the non-concave SSD case; establishing general sharp optimality conditions remains a promising direction for future research.

The structure of this paper is as follows. Section [2](https://arxiv.org/html/2512.00299v1#S2 "2 Model Settings â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") presents the model. Section [3](https://arxiv.org/html/2512.00299v1#S3 "3 FSD Problem and Analytical Solution â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") derives the explicit FSD solution for general S-shaped utilities and explains how FSD replaces liquidation boundaries. Section [4](https://arxiv.org/html/2512.00299v1#S4 "4 SSD Problem â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") explains the analytical hurdles of SSD under non-concavity and introduces PPRA. Section [5](https://arxiv.org/html/2512.00299v1#S5 "5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") provides numerical studies across utilities and benchmarks. Section [6](https://arxiv.org/html/2512.00299v1#S6 "6 Algorithm-Guided Piecewise-Neural-Network Framework â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") develops the algorithm-guided, piecewise neural-network framework and contrasts it with standard networks. Section [7](https://arxiv.org/html/2512.00299v1#S7 "7 Conclusion â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") concludes.

## 2 Model Settings

Fix an atomless probability space (Î©,â„±,â„™)(\Omega,\mathcal{F},\mathbb{P}). Let L0L^{0} be the set of all random variables on (Î©,â„±,â„™)(\Omega,\mathcal{F},\mathbb{P}). Let L1âŠ‚L0L^{1}\subset L^{0} be the set of all integrable random variables.
Denote the pricing kernel by a continuously-distributed random variable Ï:Î©â†’(0,âˆ)\rho:\Omega\to(0,\infty) and ÏâˆˆL1\rho\in L^{1}.
For an initial capital xÂ¯âˆˆâ„\overline{x}\in\mathbb{R}, the static version of the classic Merton ([1969](https://arxiv.org/html/2512.00299v1#bib.bib13))â€™s problem is given by

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | maxXâˆˆL0â¡ğ”¼â€‹[Uâ€‹(X)]â€‹Â s.t.Â â€‹ğ”¼â€‹[Ïâ€‹X]â©½xÂ¯,\displaystyle\max\_{X\in L^{0}}\mathbb{E}\left[U(X)\right]\text{ s.t. }\mathbb{E}[\rho X]\leqslant\overline{x}, |  | (1) |

where U:â„â†’â„U:\mathbb{R}\to\mathbb{R} is a utility function to be specified in the following different sections. The constraint is called the budget constraint. For a strictly concave utility, the solution of Problem ([1](https://arxiv.org/html/2512.00299v1#S2.E1 "In 2 Model Settings â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")) is given by

|  |  |  |  |
| --- | --- | --- | --- |
|  | Xcla=Iâ€‹(Î»claâ€‹Ï),X\_{\text{cla}}=I(\lambda\_{\text{cla}}\rho), |  | (2) |

where the conjugate function I:(0,âˆ)â†’â„I:(0,\infty)\to\mathbb{R} is given by Iâ€‹(y)â‰œargâ€‹supxâˆˆâ„[Uâ€‹(x)âˆ’xâ€‹y],y>0I(y)\triangleq\arg\sup\_{x\in\mathbb{R}}\left[U(x)-xy\right],~y>0
(We will revisit the definition if UU is non-concave)
and Î»cla>0\lambda\_{\text{cla}}>0 is a Lagrange multiplier solved from
ğ”¼â€‹[Ïâ€‹Iâ€‹(Î»claâ€‹Ï)]=xÂ¯.\mathbb{E}[\rho I(\lambda\_{\text{cla}}\rho)]=\overline{x}.
The problem ([1](https://arxiv.org/html/2512.00299v1#S2.E1 "In 2 Model Settings â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")) can be seen as the terminal wealth optimization of the classic continuous-time Mertonâ€™s problem in a complete market; see Appendix A of Liang and Liu ([2024](https://arxiv.org/html/2512.00299v1#bib.bib11)) for details.
In the classic Mertonâ€™s problem, the utility function is chosen as a smooth and strictly concave function, including power/log (CRRA, constant relative risk aversion) or exponential (CARA, constant absolute risk aversion) functions.

Now we introduce the concept of stochastic dominance (SD). For a random variable XâˆˆL0X\in L^{0}, the (upper) quantile function QX:[0,1]â†’â„âˆª{Â±âˆ}Q\_{X}:[0,1]\to\mathbb{R}\cup\{\pm\infty\} is defined by

|  |  |  |
| --- | --- | --- |
|  | QXâ€‹(s)=inf{xâˆˆâ„|â„™â€‹(Xâ©½x)>s},sâˆˆ[0,1].Q\_{X}(s)=\inf\{x\in\mathbb{R}\big|\mathbb{P}(X\leqslant x)>s\},~~s\in[0,1]. |  |

Denote by ğ’¬\mathcal{Q} the set of all quantile functions:

|  |  |  |
| --- | --- | --- |
|  | ğ’¬â‰œ{Q:[0,1]â†’â„âˆª{Â±âˆ}|Qâ€‹Â is increasing and right-continuous}.\mathcal{Q}\triangleq\left\{Q:[0,1]\to\mathbb{R}\cup\{\pm\infty\}\big|Q\text{ is increasing and right-continuous}\right\}. |  |

###### Definition 1 (Stochastic dominance).

1. (1)

   Fix X,YâˆˆL0X,Y\in L^{0}. XX is greater than YY in first-order stochastic dominance (FSD) if QXâ€‹(s)â©¾QYâ€‹(s)Q\_{X}(s)\geqslant Q\_{Y}(s) for all sâˆˆ(0,1)s\in(0,1), which is denoted by Xâª°(1)YX\succeq\_{(1)}Y.
2. (2)

   Fix X,YâˆˆL1X,Y\in L^{1}. XX is greater than YY in second-order stochastic dominance (SSD) if âˆ«0tQXâ€‹(s)â€‹dsâ©¾âˆ«0tQYâ€‹(s)â€‹ds\int\_{0}^{t}Q\_{X}(s)\mathrm{d}s\geqslant\int\_{0}^{t}Q\_{Y}(s)\mathrm{d}s for all tâˆˆ(0,1)t\in(0,1), which is denoted by Xâª°(2)YX\succeq\_{(2)}Y.

We specify a given benchmark wealth X0âˆˆL0X\_{0}\in L^{0}.
We proceed to study the problem with the first-order stochastic dominance (FSD) or second-order stochastic dominance (SSD) constraints:

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | (FSD Problem) | maxXâˆˆL0â¡ğ”¼â€‹[Uâ€‹(X)]â€‹Â s.t.Â â€‹ğ”¼â€‹[Ïâ€‹X]â©½xÂ¯â€‹Â andÂ â€‹Xâª°(1)X0,\displaystyle\max\_{X\in L^{0}}\mathbb{E}[U(X)]\text{ s.t. }\mathbb{E}[\rho X]\leqslant\overline{x}\text{ and }X\succeq\_{(1)}X\_{0}, |  | (3) |

and

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | (SSD Problem) | maxXâˆˆL1â¡ğ”¼â€‹[Uâ€‹(X)]â€‹Â s.t.Â â€‹ğ”¼â€‹[Ïâ€‹X]â©½xÂ¯â€‹Â andÂ â€‹Xâª°(2)X0.\displaystyle\max\_{X\in L^{1}}\mathbb{E}[U(X)]\text{ s.t. }\mathbb{E}[\rho X]\leqslant\overline{x}\text{ and }X\succeq\_{(2)}X\_{0}. |  | (4) |

We denote the quantile function of X0X\_{0} by Q0âˆˆğ’¬Q\_{0}\in\mathcal{Q}. Further, we define a minimal budget value:

|  |  |  |
| --- | --- | --- |
|  | xQ0â‰œğ”¼â€‹[Ïâ€‹X0]=âˆ«01Q0â€‹(s)â€‹QÏâ€‹(1âˆ’s)â€‹ds.x\_{Q\_{0}}\triangleq\mathbb{E}[\rho X\_{0}]=\int\_{0}^{1}Q\_{0}(s)Q\_{\rho}(1-s)\mathrm{d}s. |  |

Throughout, we assume xÂ¯â©¾xQ0\overline{x}\geqslant x\_{Q\_{0}}. As a result, both problems ([3](https://arxiv.org/html/2512.00299v1#S2.E3 "In 2 Model Settings â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network"))-([4](https://arxiv.org/html/2512.00299v1#S2.E4 "In 2 Model Settings â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")) have at least one feasible solution X0X\_{0}.

Wang and Xia ([2021](https://arxiv.org/html/2512.00299v1#bib.bib18)) propose and solve the FSD and SSD problems ([3](https://arxiv.org/html/2512.00299v1#S2.E3 "In 2 Model Settings â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network"))-([4](https://arxiv.org/html/2512.00299v1#S2.E4 "In 2 Model Settings â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")) with smooth and concave utilities; see Wang, Wei and Xia ([2024](https://arxiv.org/html/2512.00299v1#bib.bib17)) for a mean-stochastic-dominance problem. In the following, we investigate the corresponding general non-concave utility optimization, particularly, S-shaped utility optimization.

## 3 FSD Problem and Analytical Solution

In this section, we apply the general S-shaped utility in Definition [2](https://arxiv.org/html/2512.00299v1#Thmdefinition2 "Definition 2 (General S-shaped utility). â€£ 3 FSD Problem and Analytical Solution â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") (see Liang and Liu ([2020](https://arxiv.org/html/2512.00299v1#bib.bib10))) and proceed to study Problem ([3](https://arxiv.org/html/2512.00299v1#S2.E3 "In 2 Model Settings â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")). Now we define a general S-shaped utility function UU.

###### Definition 2 (General S-shaped utility).

A general S-shaped utility function U:â„â†’â„U:\mathbb{R}\rightarrow\mathbb{R} with the reference point Bâˆˆâ„B\in\mathbb{R} has the expression

|  |  |  |  |
| --- | --- | --- | --- |
|  | U(x)={U1â€‹(x),xâ©¾B,U2â€‹(x),x<B,U(x)=\left\{\begin{aligned} &U\_{1}(x),&&x\geqslant B,\\ &U\_{2}(x),&&x<B,\end{aligned}\right. |  | (5) |

and satisfies the following properties:

1. (i)

   UU is increasing on â„\mathbb{R}, U=U1U=U\_{1} is strictly concave on (B,âˆ)(B,\infty), and U=U2U=U\_{2} is convex on (âˆ’âˆ,B)(-\infty,B).
2. (ii)

   U1â€‹(B)=U2â€‹(B)U\_{1}(B)=U\_{2}(B) and U1â€²â€‹(B+)=U2â€²â€‹(Bâˆ’)=âˆU\_{1}^{\prime}(B+)=U\_{2}^{\prime}(B-)=\infty.
3. (iii)

   Inada condition: U1â€²â€‹(âˆ)=0U\_{1}^{\prime}(\infty)=0.

The conjugate function I:(0,âˆ)â†’â„I:(0,\infty)\rightarrow\mathbb{R} is given by Iâ€‹(y)=argâ€‹supxâ©¾B[Uâ€‹(x)âˆ’xâ€‹y]=(U1â€²)âˆ’1â€‹(y)I(y)=\arg\sup\_{x\geqslant B}\left[U(x)-xy\right]=(U\_{1}^{\prime})^{-1}(y). Note that in Definition [2](https://arxiv.org/html/2512.00299v1#Thmdefinition2 "Definition 2 (General S-shaped utility). â€£ 3 FSD Problem and Analytical Solution â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network"), there is no requirement for a finite left endpoint of the domain of the utility function (known as the liquidation boundary). We solve the FSD problem ([3](https://arxiv.org/html/2512.00299v1#S2.E3 "In 2 Model Settings â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")) with a general S-shaped utility and hence illustrate that using the FSD constraint can replace the liquidation boundary for risk management.

###### Theorem 1.

With a general S-shaped utility in Definition [2](https://arxiv.org/html/2512.00299v1#Thmdefinition2 "Definition 2 (General S-shaped utility). â€£ 3 FSD Problem and Analytical Solution â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network"), the optimal solution of Problem ([3](https://arxiv.org/html/2512.00299v1#S2.E3 "In 2 Model Settings â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")) is given by

|  |  |  |  |
| --- | --- | --- | --- |
|  | XFSDâˆ—={(U1â€²)âˆ’1â€‹(Î»â€‹Ï),ifÂ â€‹{Ï<1Î»â€‹U1â€²â€‹(Q0â€‹(1âˆ’FÏâ€‹(Ï)))â€‹Â andÂ â€‹Q0â€‹(1âˆ’FÏâ€‹(Ï))â©¾B}â€‹Â orÂ â€‹{Ïâ©½1Î»â€‹U1â€²â€‹(C)â€‹Â andÂ â€‹Q0â€‹(1âˆ’FÏâ€‹(Ï))<B};Q0â€‹(1âˆ’FÏâ€‹(Ï)),ifÂ â€‹{Ïâ©¾1Î»â€‹U1â€²â€‹(Q0â€‹(1âˆ’FÏâ€‹(Ï)))â€‹Â andÂ â€‹Q0â€‹(1âˆ’FÏâ€‹(Ï))â©¾B}â€‹Â orÂ â€‹{Ï>1Î»â€‹U1â€²â€‹(C)â€‹Â andÂ â€‹Q0â€‹(1âˆ’FÏâ€‹(Ï))<B}.\scriptsize X\_{\text{FSD}}^{\*}=\left\{\begin{aligned} &(U\_{1}^{\prime})^{-1}(\lambda\rho),\text{if }\left\{\rho<\frac{1}{\lambda}U\_{1}^{\prime}\left(Q\_{0}(1-F\_{\rho}(\rho))\right)\text{ and }Q\_{0}\left(1-F\_{\rho}(\rho)\right)\geqslant B\right\}\text{ or }\left\{\rho\leqslant\frac{1}{\lambda}U\_{1}^{\prime}(C)\text{ and }Q\_{0}\left(1-F\_{\rho}(\rho)\right)<B\right\};\\ &Q\_{0}(1-F\_{\rho}(\rho)),\text{if }\left\{\rho\geqslant\frac{1}{\lambda}U\_{1}^{\prime}\left(Q\_{0}(1-F\_{\rho}(\rho))\right)\text{ and }Q\_{0}\left(1-F\_{\rho}(\rho)\right)\geqslant B\right\}\text{ or }\left\{\rho>\frac{1}{\lambda}U\_{1}^{\prime}(C)\text{ and }Q\_{0}\left(1-F\_{\rho}(\rho)\right)<B\right\}.\end{aligned}\right. |  | (6) |

where (i) the Lagrange multiplier Î»>0\lambda>0 is solved from the binding budget constraint ğ”¼â€‹[Ïâ€‹XFSDâˆ—]=xÂ¯\mathbb{E}[\rho X\_{\text{FSD}}^{\*}]=\overline{x}, and (ii) for any Q0â€‹(1âˆ’FÏâ€‹(Ï))<BQ\_{0}\left(1-F\_{\rho}(\rho)\right)<B, the (state-dependent) tangent point Câˆˆ(B,âˆ)C\in(B,\infty) is solved from

|  |  |  |  |
| --- | --- | --- | --- |
|  | U1â€‹(C)âˆ’U2â€‹(Q0â€‹(1âˆ’FÏâ€‹(Ï)))Câˆ’Q0â€‹(1âˆ’FÏâ€‹(Ï))=U1â€²â€‹(C).\frac{U\_{1}(C)-U\_{2}(Q\_{0}(1-F\_{\rho}(\rho)))}{C-Q\_{0}(1-F\_{\rho}(\rho))}=U\_{1}^{\prime}(C). |  | (7) |

###### Proof of Theorem [1](https://arxiv.org/html/2512.00299v1#Thmtheorem1 "Theorem 1. â€£ 3 FSD Problem and Analytical Solution â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network").

As the objective in Problem ([3](https://arxiv.org/html/2512.00299v1#S2.E3 "In 2 Model Settings â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")) is an increasing function of XX, the optimal wealth XFSDâˆ—X\_{\text{FSD}}^{\*} of Problem ([3](https://arxiv.org/html/2512.00299v1#S2.E3 "In 2 Model Settings â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")) is counter-comonotonic to the pricing kernel Ï\rho (see, e.g., He and Zhou ([2011](https://arxiv.org/html/2512.00299v1#bib.bib6))). Denote by Î¾\xi the uniform transformation of Ï\rho such that Î¾\xi has the uniform distribution on [0,1][0,1] and QÏâ€‹(Î¾)=ÏQ\_{\rho}(\xi)=\rho. Hence, the optimal wealth XFSDâˆ—X\_{\text{FSD}}^{\*} under the first-order stochastic dominance constraint should satisfy

|  |  |  |
| --- | --- | --- |
|  | XFSDâˆ—â©¾Q0â€‹(1âˆ’Î¾),Qâˆˆğ’¬.X\_{\text{FSD}}^{\*}\geqslant Q\_{0}(1-\xi),~~Q\in\mathcal{Q}. |  |

Hence, Problem ([3](https://arxiv.org/html/2512.00299v1#S2.E3 "In 2 Model Settings â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")) is translated to the following problem:

|  |  |  |  |
| --- | --- | --- | --- |
|  | maxXâ©¾Q0â€‹(1âˆ’Î¾),ğ”¼â€‹[Ïâ€‹X]=xâ¡ğ”¼â€‹[Uâ€‹(X)].\max\_{X\geqslant Q\_{0}(1-\xi),\mathbb{E}\left[\rho X\right]=x}\mathbb{E}\left[U(X)\right]. |  | (8) |

Further, Problem ([8](https://arxiv.org/html/2512.00299v1#S3.E8 "In 3 FSD Problem and Analytical Solution â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")) is converted to a state-dependent pointwise optimization problem:

|  |  |  |  |
| --- | --- | --- | --- |
|  | maxXâ©¾Q0â€‹(1âˆ’Î¾)â¡{Uâ€‹(X)âˆ’Î»â€‹Ïâ€‹X},\max\_{X\geqslant Q\_{0}(1-\xi)}\left\{U(X)-\lambda\rho X\right\}, |  | (9) |

where Î»>0\lambda>0 is a to-be-determined Lagrange multiplier such that ğ”¼â€‹[Ïâ€‹X]=xÂ¯\mathbb{E}[\rho X]=\overline{x}. To solve Problem ([9](https://arxiv.org/html/2512.00299v1#S3.E9 "In 3 FSD Problem and Analytical Solution â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")), we have the following two cases:

1. (i)

   for any Ï‰âˆˆÎ©\omega\in\Omega satisfying Q0â€‹(1âˆ’Î¾â€‹(Ï‰))<BQ\_{0}(1-\xi(\omega))<B, we solve the tangent point Câ€‹(Ï‰)C(\omega) from ([7](https://arxiv.org/html/2512.00299v1#S3.E7 "In Theorem 1. â€£ 3 FSD Problem and Analytical Solution â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")) and have
   X\_FSD^\* = {
   (U1â€™)-1(Î»Ï), if (U1â€™)-1(Î»Ï) Â¿ C and Q0(1-Î¾) Â¡ B;Q0(1-Î¾), if (U1â€™)-1(Î»Ï) â©½C and Q0(1-Î¾) Â¡ B.
2. (ii)

   for any Ï‰âˆˆÎ©\omega\in\Omega satisfying Q0â€‹(1âˆ’Î¾â€‹(Ï‰))â©¾BQ\_{0}(1-\xi(\omega))\geqslant B, we have
   X\_FSD^\* = {
   (U1â€™)-1(Î»Ï), if (U1â€™)-1(Î»Ï) Â¿ Q0(1-Î¾) and Q0(1-Î¾) â©¾B;Q0(1-Î¾), if (U1â€™)-1(Î»Ï) â©½Q0(1-Î¾) and Q0(1-Î¾) â©¾B.

Further, as Î¾=FÏâ€‹(Ï)\xi=F\_{\rho}(\rho), we derive the optimal solution XFSDâˆ—X\_{\text{FSD}}^{\*} given by ([6](https://arxiv.org/html/2512.00299v1#S3.E6 "In Theorem 1. â€£ 3 FSD Problem and Analytical Solution â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")).
âˆ

In the literature, a liquidation boundary is needed for the optimization of the S-shaped utility, otherwise the problem has no solution (mathematically, it is because one cannot establish a concave envelope for the S-shaped utility without a lower bound in the domain). In the problem of FSD, we do not require the liquidation boundary for the S-shaped utility. The solution is twofold. In some good scenarios, it behaves like the classic solution. In some bad scenarios, it behaves like the benchmark quantile. From the solution, we can see that the FSD constraint acts as a good substitute of the liquidation boundary.

## 4 SSD Problem

### 4.1 SSD Problem under Non-concavity: Analytical Difficulty

Let us restate the results of Wang and Xia ([2021](https://arxiv.org/html/2512.00299v1#bib.bib18)) on strictly concave utilities.

###### Theorem 2 (Theorem 5.10 of Wang and Xia ([2021](https://arxiv.org/html/2512.00299v1#bib.bib18))).

Let xÂ¯>x0\overline{x}>x\_{0}. For a strictly concave utility UU with appropriate regularity conditions, the optimal solution to Problem ([4](https://arxiv.org/html/2512.00299v1#S2.E4 "In 2 Model Settings â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")) is XSSDâˆ—=QSSDâˆ—â€‹(1âˆ’FÏâ€‹(Ï))X\_{\text{SSD}}^{\*}=Q\_{\text{SSD}}^{\*}(1-F\_{\rho}(\rho)) with

|  |  |  |  |
| --- | --- | --- | --- |
|  | QSSDâˆ—â€‹(s)=Iâ€‹(Î»â€‹(QÏâ€‹(1âˆ’s)âˆ’ySSDâˆ—â€‹(1âˆ’s))),sâˆˆ(0,1),Q\_{\text{SSD}}^{\*}(s)=I\left(\lambda\left(Q\_{\rho}(1-s)-y\_{\text{SSD}}^{\*}(1-s)\right)\right),~s\in(0,1), |  | (10) |

where ySSDâˆ—:[0,1]â†’[0,âˆ)y\_{\text{SSD}}^{\*}:[0,1]\to[0,\infty) is a function
given by the system

|  |  |  |  |
| --- | --- | --- | --- |
|  | {ySSDâˆ—â€‹Â is right-continuous andÂ â€‹0â©½dâ€‹ySSDâˆ—â€‹(t)dâ€‹QÏâ€‹(t)â©½1,Â â€‹QÏâ€‹(t)âˆ’ySSDâˆ—â€‹(t)>0â€‹Â for allÂ â€‹tâˆˆ(0,1);zSSDâˆ—â€‹(s)â‰œâˆ’âˆ«s1(Iâ€‹(Î»â€‹(QÏâ€‹(t)âˆ’ySSDâˆ—â€‹(t)))âˆ’Q0â€‹(1âˆ’t))â€‹dt,sâˆˆ[0,1];dâ€‹ySSDâˆ—â€‹(t)dâ€‹QÏâ€‹(t){âˆˆ[0,1],zSSDâˆ—â€‹(t)=0;=0,zSSDâˆ—â€‹(t)<0,dQÏ-a.e.,\left\{\begin{aligned} &y\_{\text{SSD}}^{\*}\text{ is right-continuous and }0\leqslant\frac{\mathrm{d}y\_{\text{SSD}}^{\*}(t)}{\mathrm{d}Q\_{\rho}(t)}\leqslant 1,\text{ }Q\_{\rho}(t)-y\_{\text{SSD}}^{\*}(t)>0\text{ for all }t\in(0,1);\\ &z\_{\text{SSD}}^{\*}(s)\triangleq-\int\_{s}^{1}\left(I(\lambda(Q\_{\rho}(t)-y\_{\text{SSD}}^{\*}(t)))-Q\_{0}(1-t)\right)\mathrm{d}t,~s\in[0,1];\\ &\frac{\mathrm{d}y\_{\text{SSD}}^{\*}(t)}{\mathrm{d}Q\_{\rho}(t)}\left\{\begin{aligned} &\in[0,1],&&z\_{\text{SSD}}^{\*}(t)=0;\\ &=0,&&z\_{\text{SSD}}^{\*}(t)<0,\end{aligned}\right.\quad dQ\_{\rho}\text{-a.e.,}\end{aligned}\right. |  | (11) |

and the Lagrange multiplier Î»>0\lambda>0 is determined by the binding budget constraint equation

|  |  |  |  |
| --- | --- | --- | --- |
|  | âˆ«01QSSDâˆ—â€‹(s)â€‹QÏâ€‹(1âˆ’s)â€‹ds=xÂ¯.\displaystyle\int\_{0}^{1}Q\_{\text{SSD}}^{\*}(s)Q\_{\rho}(1-s)\mathrm{d}s=\overline{x}. |  | (12) |

In the proof of Wang and Xia ([2021](https://arxiv.org/html/2512.00299v1#bib.bib18)), the procedure of solving Problem ([4](https://arxiv.org/html/2512.00299v1#S2.E4 "In 2 Model Settings â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")) starts with a view of quantile formulation (He and Zhou ([2011](https://arxiv.org/html/2512.00299v1#bib.bib6)); Xia and Zhou ([2016](https://arxiv.org/html/2512.00299v1#bib.bib20)); Xu ([2016](https://arxiv.org/html/2512.00299v1#bib.bib21))).
Specifically, we define

|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ğ’²â‰œ{w:[0,1]â†’[0,âˆ)|wâ€‹(0)=0,wâ€‹Â is increasing and concave},\displaystyle\mathcal{W}\triangleq\{w:[0,1]\to[0,\infty)|w(0)=0,w\text{ is increasing and concave}\}, |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ğ’²1â‰œ{wâˆˆğ’²|wâ€‹(1)=1}.\displaystyle\mathcal{W}\_{1}\triangleq\{w\in\mathcal{W}|w(1)=1\}. |  |

Define

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | ğ’¬2â€‹(Q0)\displaystyle\mathcal{Q}\_{2}(Q\_{0}) | â‰œ{Qâˆˆğ’¬|âˆ«[0,1]Qâ€‹(s)â€‹dwâ€‹(s)â©¾âˆ«[0,1]Q0â€‹(s)â€‹dwâ€‹(s)â€‹Â for allÂ â€‹wâˆˆğ’²}\displaystyle\triangleq\left\{Q\in\mathcal{Q}\big|\int\_{[0,1]}Q(s)\mathrm{d}w(s)\geqslant\int\_{[0,1]}Q\_{0}(s)\mathrm{d}w(s)\text{ for all }w\in\mathcal{W}\right\} |  | (13) |
|  |  | ={Qâˆˆğ’¬|âˆ«[0,1]Qâ€‹(s)â€‹dwâ€‹(s)â©¾âˆ«[0,1]Q0â€‹(s)â€‹dwâ€‹(s)â€‹Â for allÂ â€‹wâˆˆğ’²1}.\displaystyle=\left\{Q\in\mathcal{Q}\big|\int\_{[0,1]}Q(s)\mathrm{d}w(s)\geqslant\int\_{[0,1]}Q\_{0}(s)\mathrm{d}w(s)\text{ for all }w\in\mathcal{W}\_{1}\right\}. |  |

In view of quantile formulation approach, we are going to change the optimization among the random variable XX to its quantile function QXQ\_{X}. We can hence express the objective in SSD Problem ([4](https://arxiv.org/html/2512.00299v1#S2.E4 "In 2 Model Settings â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")) as
ğ”¼â€‹[Uâ€‹(X)]=âˆ«01Uâ€‹(QXâ€‹(s))â€‹ds.\mathbb{E}[U(X)]=\int\_{0}^{1}U(Q\_{X}(s))\mathrm{d}s.
Based on the counter-monotonic dependence between the optimal solution XX and the pricing kernel Ï\rho (see He and Zhou ([2011](https://arxiv.org/html/2512.00299v1#bib.bib6)) and the proof of Theorem [1](https://arxiv.org/html/2512.00299v1#Thmtheorem1 "Theorem 1. â€£ 3 FSD Problem and Analytical Solution â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")), we can express the budget constraint as
ğ”¼â€‹[Ïâ€‹X]=âˆ«01QXâ€‹(s)â€‹QÏâ€‹(1âˆ’s)â€‹ds.\mathbb{E}[\rho X]=\int\_{0}^{1}Q\_{X}(s)Q\_{\rho}(1-s)\mathrm{d}s.
According to FÃ¶llmer and Schied ([2016](https://arxiv.org/html/2512.00299v1#bib.bib3)), SSD Problem ([4](https://arxiv.org/html/2512.00299v1#S2.E4 "In 2 Model Settings â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")) reads as
an optimal quantile problem

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | maxQâˆˆğ’¬2â€‹(Q0)â€‹âˆ«01Uâ€‹(Qâ€‹(s))â€‹dsâ€‹s.t.Â â€‹âˆ«01Qâ€‹(s)â€‹QÏâ€‹(1âˆ’s)â€‹dsâ©½xÂ¯.\displaystyle\max\_{Q\in\mathcal{Q}\_{2}(Q\_{0})}\int\_{0}^{1}U(Q(s))\mathrm{d}s~~\text{s.t. }\int\_{0}^{1}Q(s)Q\_{\rho}(1-s)\mathrm{d}s\leqslant\overline{x}. |  | (14) |

The next step of solving Problem ([4](https://arxiv.org/html/2512.00299v1#S2.E4 "In 2 Model Settings â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")) is a conversion from Problem ([14](https://arxiv.org/html/2512.00299v1#S4.E14 "In 4.1 SSD Problem under Non-concavity: Analytical Difficulty â€£ 4 SSD Problem â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")) to

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | maxQâˆˆğ’¬âˆ«01U(Q(s))dsÂ s.t.Â {âˆ«01Qâ€‹(s)â€‹QÏâ€‹(1âˆ’s)â€‹dsâ©½x,infwâˆˆğ’²1(âˆ«[0,1](Qâ€‹(s)âˆ’Q0â€‹(s))â€‹dwâ€‹(s))â©¾0.\displaystyle\max\_{Q\in\mathcal{Q}}\int\_{0}^{1}U(Q(s))\mathrm{d}s~~\text{ s.t. }\left\{\begin{aligned} &\int\_{0}^{1}Q(s)Q\_{\rho}(1-s)\mathrm{d}s\leqslant x,\\ &\inf\_{w\in\mathcal{W}\_{1}}\left(\int\_{[0,1]}(Q(s)-Q\_{0}(s))\mathrm{d}w(s)\right)\geqslant 0.\end{aligned}\right. |  | (15) |

If the optimal solution of the original problem satisfies the second constraint in Problem ([15](https://arxiv.org/html/2512.00299v1#S4.E15 "In 4.1 SSD Problem under Non-concavity: Analytical Difficulty â€£ 4 SSD Problem â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")), then the optimal solution of Problem ([15](https://arxiv.org/html/2512.00299v1#S4.E15 "In 4.1 SSD Problem under Non-concavity: Analytical Difficulty â€£ 4 SSD Problem â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")) is the same as that of the original problem and it is considered as a trivial case. In the non-trivial case, for any Î»>0\lambda>0, Qâˆˆğ’¬Q\in\mathcal{Q} and wâˆˆğ’²w\in\mathcal{W}, we let

|  |  |  |
| --- | --- | --- |
|  | Kâ€‹(Q,w;Î»)=âˆ«01Uâ€‹(Qâ€‹(s))â€‹dsâˆ’Î»â€‹âˆ«01Qâ€‹(s)â€‹QÏâ€‹(1âˆ’s)â€‹ds+âˆ«[0,1](Qâ€‹(s)âˆ’Q0â€‹(s))â€‹dwâ€‹(s).K(Q,w;\lambda)=\int\_{0}^{1}U(Q(s))\mathrm{d}s-\lambda\int\_{0}^{1}Q(s)Q\_{\rho}(1-s)\mathrm{d}s+\int\_{[0,1]}(Q(s)-Q\_{0}(s))\mathrm{d}w(s). |  |

One needs to consider the following max-min problem for KK:

|  |  |  |  |
| --- | --- | --- | --- |
|  | maxQâˆˆğ’¬â¡minwâˆˆğ’²â¡Kâ€‹(Q,w;Î»).\max\_{Q\in\mathcal{Q}}\min\_{w\in\mathcal{W}}K(Q,w;\lambda). |  | (16) |

Unfortunately, the solution procedure of the SSD Problem under non-concave utility is stuck at this step, because the desired Sionâ€™s max-min theorem requires that KK is concave in QQ, which does not hold generally. In an alternative clue of concavifying UU (see Liang and Liu ([2020](https://arxiv.org/html/2512.00299v1#bib.bib10))), one cannot guarantee that the concavification principle is valid (i.e., the optimal wealth variable under the concave envelope is almost surely not located in the region where the original utility and its concave envelope do not coincide). Even if a similar form of Theorem [2](https://arxiv.org/html/2512.00299v1#Thmtheorem2 "Theorem 2 (Theorem 5.10 of Wang and Xia (2021)). â€£ 4.1 SSD Problem under Non-concavity: Analytical Difficulty â€£ 4 SSD Problem â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") is established for general non-concave utilities, one needs to solve the optimal pair (yâˆ—,zâˆ—)(y^{\*},z^{\*}) from the system ([11](https://arxiv.org/html/2512.00299v1#S4.E11 "In Theorem 2 (Theorem 5.10 of Wang and Xia (2021)). â€£ 4.1 SSD Problem under Non-concavity: Analytical Difficulty â€£ 4 SSD Problem â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")), which is an infinite-dimensional optimization problem over the functional space. Wang and Xia ([2021](https://arxiv.org/html/2512.00299v1#bib.bib18)) propose some explicit optimal solutions based on specific and technical assumptions on UU, QÏQ\_{\rho} and Q0Q\_{0}. Beyond these, there is no general analytical expression for optimal solutions.

Nevertheless, Theorem 5.10 of Wang and Xia ([2021](https://arxiv.org/html/2512.00299v1#bib.bib18)) provides an important idea that the optimal solution may be characterized by the optimal pair (yâˆ—,zâˆ—)(y^{\*},z^{\*}) from the system ([11](https://arxiv.org/html/2512.00299v1#S4.E11 "In Theorem 2 (Theorem 5.10 of Wang and Xia (2021)). â€£ 4.1 SSD Problem under Non-concavity: Analytical Difficulty â€£ 4 SSD Problem â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")), and hence we can obtain some heuristics to construct suboptimal solutions based on numerical algorithms and even neural networks. Specifically, we define a concept of the poor performance region:

|  |  |  |
| --- | --- | --- |
|  | Câ‰œ{tâˆˆ(0,1)|Iâ€‹(Î»â€‹QÏâ€‹(t))âˆ’Q0â€‹(1âˆ’t)<0}.C\triangleq\left\{t\in(0,1)|I(\lambda Q\_{\rho}(t))-Q\_{0}(1-t)<0\right\}. |  |

Here for the non-concave utility UU, we define
Iâ€‹(y)â‰œinf{argâ€‹supxâˆˆâ„[Uâ€‹(x)âˆ’xâ€‹y]},y>0I(y)\triangleq\inf\{\arg\sup\_{x\in\mathbb{R}}\left[U(x)-xy\right]\},~y>0 whenever applicable. For example, if UU is a S-shaped utility with the domain [L,âˆ)[L,\infty) where Lâˆˆâ„L\in\mathbb{R}, we have Iâ€‹(â‹…)I(\cdot) is well-defined, while if the domain is â„\mathbb{R}, we have Iâ€‹(â‹…)â‰¡âˆ’âˆI(\cdot)\equiv-\infty.

Remark: â€Whenever applicableâ€ means that the function II is finite for any yâˆˆ(0,âˆ)y\in(0,\infty). That is, the utility UU has a finite concave envelope function (the smallest concave function dominating UU; see Liang and Liu ([2020](https://arxiv.org/html/2512.00299v1#bib.bib10))). In this case, Iâ€‹(â‹…)I(\cdot) is right-continuous and decreasing on (0,+âˆ)(0,+\infty).

In the set CC, we compare the unconstrained classic solution Iâ€‹(Î»â€‹QÏâ€‹(t))I(\lambda Q\_{\rho}(t)) with the SSD benchmark Q0â€‹(1âˆ’t)Q\_{0}(1-t). If C=âˆ…C=\emptyset, then it means that the unconstrained solution automatically satisfies the SSD constraint and QSSDâˆ—Q\_{\text{SSD}}^{\*} should be the same as the unconstrained solution. Next, we discuss the non-trivial case (i.e., Câ‰ âˆ…C\neq\emptyset). For the scenario tt on this region, the unconstrained solution Iâ€‹(Î»â€‹QÏâ€‹(t))I(\lambda Q\_{\rho}(t)) is smaller (i.e., performing worse) than the SSD benchmark Q0â€‹(1âˆ’t)Q\_{0}(1-t).
From Definition [1](https://arxiv.org/html/2512.00299v1#Thmdefinition1 "Definition 1 (Stochastic dominance). â€£ 2 Model Settings â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network"), the SSD constraint QSSDâˆ—âª°(2)Q0Q\_{\text{SSD}}^{\*}\succeq\_{(2)}Q\_{0} reads as

|  |  |  |
| --- | --- | --- |
|  | âˆ«0sQSSDâˆ—â€‹(t)â€‹dtâ©¾âˆ«0sQ0â€‹(t)â€‹dtâ€‹Â for anyÂ â€‹sâˆˆ(0,1),\int\_{0}^{s}Q\_{\text{SSD}}^{\*}(t)\mathrm{d}t\geqslant\int\_{0}^{s}Q\_{0}(t)\mathrm{d}t~~\text{ for any }s\in(0,1), |  |

which is reflected in the system ([11](https://arxiv.org/html/2512.00299v1#S4.E11 "In Theorem 2 (Theorem 5.10 of Wang and Xia (2021)). â€£ 4.1 SSD Problem under Non-concavity: Analytical Difficulty â€£ 4 SSD Problem â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")):

|  |  |  |
| --- | --- | --- |
|  | zSSDâˆ—â€‹(s)=âˆ’âˆ«s1(QSSDâˆ—â€‹(1âˆ’t)âˆ’Q0â€‹(1âˆ’t))â€‹dtâ©½0â€‹Â for anyÂ â€‹sâˆˆ(0,1).z\_{\text{SSD}}^{\*}(s)=-\int\_{s}^{1}\left(Q\_{\text{SSD}}^{\*}(1-t)-Q\_{0}(1-t)\right)\mathrm{d}t\leqslant 0~~\text{ for any }s\in(0,1). |  |

which translates to

|  |  |  |  |
| --- | --- | --- | --- |
|  | zSSDâˆ—â€‹(s)=âˆ’âˆ«s1(Iâ€‹(Î»â€‹(QÏâ€‹(t)âˆ’ySSDâˆ—â€‹(t)))âˆ’Q0â€‹(1âˆ’t))â€‹dtâ©½0â€‹Â for anyÂ â€‹sâˆˆ(0,1).z\_{\text{SSD}}^{\*}(s)=-\int\_{s}^{1}\left(I(\lambda(Q\_{\rho}(t)-y\_{\text{SSD}}^{\*}(t)))-Q\_{0}(1-t)\right)\mathrm{d}t\leqslant 0~~\text{ for any }s\in(0,1). |  | (17) |

The theorem inspires that some correction function ySSDâˆ—y\_{\text{SSD}}^{\*} should be added to satisfy the constraint ([17](https://arxiv.org/html/2512.00299v1#S4.E17 "In 4.1 SSD Problem under Non-concavity: Analytical Difficulty â€£ 4 SSD Problem â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")). In the next subsection, we provide a numerical algorithm and design the correction function to obtain a sub-optimal solution.

### 4.2 SSD Problem: Numerical Algorithm

Algorithm 1  Poor-Performance-Region Algorithm for SSD Problem ([4](https://arxiv.org/html/2512.00299v1#S2.E4 "In 2 Model Settings â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")) with general utilities

1:We solve the classic problem without the SSD constraint and obtain the optimal quantile QclaQ\_{\text{cla}}. The Lagrange multiplier is denoted by Î»claâˆˆ(0,âˆ)\lambda\_{\text{cla}}\in(0,\infty), which is solved from the following equation

|  |  |  |  |
| --- | --- | --- | --- |
|  | xÂ¯\displaystyle\overline{x} | =âˆ«01QÏâ€‹(s)â€‹Iâ€‹(Î»claâ€‹QÏâ€‹(s))â€‹ds.\displaystyle=\int\_{0}^{1}Q\_{\rho}(s)I(\lambda\_{\text{cla}}Q\_{\rho}(s))\mathrm{d}s. |  |

If

|  |  |  |
| --- | --- | --- |
|  | âˆ’âˆ«t1(Iâ€‹(Î»claâ€‹QÏâ€‹(s))âˆ’Q0â€‹(1âˆ’s))â€‹dsâ©½0-\int\_{t}^{1}\left(I(\lambda\_{\text{cla}}Q\_{\rho}(s))-Q\_{0}(1-s)\right)\mathrm{d}s\leqslant 0 |  |

holds for any tâˆˆ[0,1]t\in[0,1], then the optimal solution is Qcla(â‹…)â‰œI(Î»claQÏ(1âˆ’â‹…))Q\_{\text{cla}}(\cdot)\triangleq I(\lambda\_{\text{cla}}Q\_{\rho}(1-\cdot)). Otherwise, we start the procedure below.

2:The Lagrange multiplier is initially set as the above Î»\lambda (to be determined at last). Compute the set

|  |  |  |
| --- | --- | --- |
|  | C={tâˆˆ(0,1)|Iâ€‹(Î»â€‹QÏâ€‹(t))âˆ’Q0â€‹(1âˆ’t)<0}.C=\left\{t\in(0,1)|I(\lambda Q\_{\rho}(t))-Q\_{0}(1-t)<0\right\}. |  |

3:If C=âˆ…C=\emptyset, then the optimal solution is QclaQ\_{\text{cla}}. Otherwise, specify an appropriate nâˆˆâ„•n\in\mathbb{N} and write C=âˆªi=1n(ai,bi)C=\cup\_{i=1}^{n}(a\_{i},b\_{i}) with ai<bia\_{i}<b\_{i}. Further we set an+1=1a\_{n+1}=1.

4:For i=n,(nâˆ’1),â€¦,1i=n,(n-1),\dots,1 (Steps 4-6), we compute

|  |  |  |  |
| --- | --- | --- | --- |
|  | y0â€‹(s)=inf{yâ©¾0|Iâ€‹(Î»â€‹(QÏâ€‹(s)âˆ’y))âˆ’Q0â€‹(1âˆ’s)â©¾0},sâˆˆ(ai,bi).y\_{0}(s)=\inf\{y\geqslant 0|I\left(\lambda(Q\_{\rho}(s)-y)\right)-Q\_{0}(1-s)\geqslant 0\},~~\text{$s\in(a\_{i},b\_{i})$}. |  | (18) |

Define

|  |  |  |  |
| --- | --- | --- | --- |
|  | giâ€‹(s)=âˆ’âˆ«sai+1(Iâ€‹(Î»â€‹(QÏâ€‹(t)âˆ’y0â€‹(s)))âˆ’Q0â€‹(1âˆ’t))â€‹dt,sâˆˆ(ai,bi).g\_{i}(s)=-\int\_{s}^{a\_{i+1}}\left(I\left(\lambda\left(Q\_{\rho}(t)-y\_{0}(s)\right)\right)-Q\_{0}(1-t)\right)\mathrm{d}t,~~s\in(a\_{i},b\_{i}). |  | (19) |

5:We compute

|  |  |  |  |
| --- | --- | --- | --- |
|  | ti=sup{tâˆˆ[ai,bi]|giâ€‹(t)+zsubâ€‹(ai+1)â€‹ğŸ™{iâ‰ n}>0},t\_{i}=\sup\Big\{\,t\in[a\_{i},b\_{i}]\;\big|\;g\_{i}(t)+z\_{\text{sub}}(a\_{i+1})\mathds{1}\_{\{i\neq n\}}>0\Big\}, |  | (20) |

where zsubâ€‹(â‹…)z\_{\text{sub}}(\cdot) will be determined in Step 6.
If {tâˆˆ[ai,bi]|giâ€‹(t)+zsubâ€‹(ai+1)â€‹ğŸ™{iâ‰ n}>0}=âˆ…\{t\in[a\_{i},b\_{i}]|g\_{i}(t)+z\_{\text{sub}}(a\_{i+1})\mathds{1}\_{\{i\neq n\}}>0\}=\emptyset, set ti=ait\_{i}=a\_{i}.

6:Set

|  |  |  |  |
| --- | --- | --- | --- |
|  | ysubâ€‹(â‹…)â‰¡y0â€‹(ti)â€‹Â onÂ â€‹(ti,ai+1)â€‹Â andÂ â€‹ysubâ€‹(â‹…)=y0â€‹(â‹…)â€‹Â onÂ â€‹(ai,ti).y\_{\text{sub}}(\cdot)\equiv y\_{0}(t\_{i})\text{ on }(t\_{i},a\_{i+1})~~\text{ and }~~y\_{\text{sub}}(\cdot)=y\_{0}(\cdot)~\text{ on }(a\_{i},t\_{i}). |  | (21) |

Define zsubâ€‹(â‹…)z\_{\text{sub}}(\cdot) as follows:

|  |  |  |  |
| --- | --- | --- | --- |
|  | zsubâ€‹(s)â‰œâˆ’âˆ«s1(Iâ€‹(Î»â€‹(QÏâ€‹(t)âˆ’ysubâ€‹(t)))âˆ’Q0â€‹(1âˆ’t))â€‹dt,sâˆˆ[ai,ai+1].z\_{\text{sub}}(s)\triangleq-\int\_{s}^{1}\left(I(\lambda(Q\_{\rho}(t)-y\_{\text{sub}}(t)))-Q\_{0}(1-t)\right)\mathrm{d}t,~s\in[a\_{i},a\_{i+1}]. |  | (22) |

7:Set t0=0t\_{0}=0 and y0â€‹(t0)=0y\_{0}(t\_{0})=0. After the iteration, we have Eq. ([21](https://arxiv.org/html/2512.00299v1#S4.E21 "In 6 â€£ Algorithm 1 â€£ 4.2 SSD Problem: Numerical Algorithm â€£ 4 SSD Problem â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")) for i=n,â€¦,1i=n,\dots,1
and ysubâ€‹(â‹…)â‰¡y0â€‹(t0)y\_{\text{sub}}(\cdot)\equiv y\_{0}(t\_{0}) on (t0,a1](t\_{0},a\_{1}]. We then verify whether ysubâ€‹(â‹…)y\_{\textbf{sub}}(\cdot) satisfies the monotonicity condition (non-decreasing over (0,1)(0,1)). If the condition holds, proceed to Step 14; otherwise, apply the correction procedure and proceed to Step 8.

8:For i=n,(nâˆ’1),â€¦,2i=n,(n-1),\dots,2 (Steps 8-12), check whether y0â€‹(â‹…)y\_{0}(\cdot) is increasing (non-decreasing) over (aiâˆ’1,ai+1)(a\_{i-1},a\_{i+1}). If yes, skip and proceed to next iteration; if not, proceed to Step 11.

9:Compute y0â€‹(â‹…)y\_{0}(\cdot) over (aiâˆ’1,biâˆ’1)(a\_{i-1},b\_{i-1}) and (ai,bi)(a\_{i},b\_{i}) by Eq. ([18](https://arxiv.org/html/2512.00299v1#S4.E18 "In 4 â€£ Algorithm 1 â€£ 4.2 SSD Problem: Numerical Algorithm â€£ 4 SSD Problem â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")).

10:Redefine

|  |  |  |
| --- | --- | --- |
|  | giâ€‹(s)=âˆ’âˆ«ssÂ¯(Iâ€‹(Î»â€‹(QÏâ€‹(t)âˆ’y0â€‹(s)))âˆ’Q0â€‹(1âˆ’t))â€‹dt,sâˆˆ(aiâˆ’1,biâˆ’1),g\_{i}(s)=-\int\_{s}^{\bar{s}}\left(I\left(\lambda\left(Q\_{\rho}(t)-y\_{0}(s)\right)\right)-Q\_{0}(1-t)\right)\mathrm{d}t,~~s\in(a\_{i-1},b\_{i-1}), |  |

where sÂ¯=inf{tâ©¾ai|y0â€‹(t)âˆ’y0â€‹(s)â©¾0},tâˆˆ[ai,bi).\bar{s}=\inf\{t\geqslant a\_{i}|y\_{0}(t)-y\_{0}(s)\geqslant 0\},t\in[a\_{i},b\_{i}).

11:Compute

|  |  |  |  |
| --- | --- | --- | --- |
|  | tleft=sup{tâˆˆ[a1,b1]|g1â€‹(t)>0},tright=sÂ¯.t\_{\text{left}}=\sup\{t\in[a\_{1},b\_{1}]|g\_{1}(t)>0\},\;\;t\_{\text{right}}=\bar{s}. |  | (23) |



12:Replace the initial ysubâ€‹(â‹…)y\_{\text{sub}}(\cdot) over (tleft,tright)(t\_{\text{left}},t\_{\text{right}}) and set

|  |  |  |
| --- | --- | --- |
|  | ysubâ€‹(â‹…)â‰¡y0â€‹(tleft)â€‹Â onÂ â€‹(tleft,tright).y\_{\text{sub}}(\cdot)\equiv y\_{0}(t\_{\text{left}})\text{ on }(t\_{\text{left}},t\_{\text{right}}). |  |

13:After the iteration, check whether ysubâ€‹(â‹…)y\_{\text{sub}}(\cdot) satisfy the monotonicity condition. If yes, proceed to next step; otherwise, the algorithm may fail.

14:Hence, we design the sub-optimal solution by

|  |  |  |  |
| --- | --- | --- | --- |
|  | Qsub(s)={Iâ€‹(Î»â€‹(QÏâ€‹(1âˆ’s)âˆ’y0â€‹(tn))),sâˆˆ(1âˆ’an+1,1âˆ’tn];Iâ€‹(Î»â€‹(QÏâ€‹(1âˆ’s)âˆ’y0â€‹(1âˆ’s)))â‰¡Q0â€‹(s),sâˆˆ(1âˆ’tn,1âˆ’an];â€¦Iâ€‹(Î»â€‹(QÏâ€‹(1âˆ’s)âˆ’y0â€‹(t0))),sâˆˆ(1âˆ’a1,1âˆ’t0).Q\_{\text{sub}}(s)=\left\{\begin{aligned} &I\left(\lambda\left(Q\_{\rho}(1-s)-y\_{0}(t\_{n})\right)\right),&&s\in(1-a\_{n+1},1-t\_{n}];\\ &I\left(\lambda\left(Q\_{\rho}(1-s)-y\_{0}(1-s)\right)\right)\equiv Q\_{0}(s),&&s\in(1-t\_{n},1-a\_{n}];\\ &\dots\\ &I\left(\lambda\left(Q\_{\rho}(1-s)-y\_{0}(t\_{0})\right)\right),&&s\in(1-a\_{1},1-t\_{0}).\\ \end{aligned}\right. |  | (24) |

15:Set

|  |  |  |
| --- | --- | --- |
|  | ziâˆ—â€‹(t)=âˆ’âˆ«t1(Iâ€‹(Î»â€‹(QÏâ€‹(s)âˆ’y0â€‹(ti)))âˆ’Q0â€‹(1âˆ’s))â€‹ds,tâˆˆ(maxâ¡{ai,ti},ai+1).z^{\*}\_{i}(t)=-\int\_{t}^{1}\left(I\left(\lambda\left(Q\_{\rho}(s)-y\_{0}(t\_{i})\right)\right)-Q\_{0}(1-s)\right)\mathrm{d}s,~~t\in(\max\{a\_{i},t\_{i}\},a\_{i+1}). |  |

If for any i=1,â€¦,ni=1,\dots,n, ziâˆ—â€‹(â‹…)â©½0z^{\*}\_{i}(\cdot)\leqslant 0 always holds on the interval (maxâ¡{ai,ti},ai+1)(\max\{a\_{i},t\_{i}\},a\_{i+1}), this
QsubQ\_{\text{sub}} is sub-optimal.

16:Using a bisection method, we determine Î»subâˆˆ(0,âˆ)\lambda\_{\text{sub}}\in(0,\infty) from the following equation

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | xÂ¯\displaystyle\overline{x} | =âˆ«01QÏâ€‹(s)â€‹Qsubâ€‹(1âˆ’s)â€‹ds.\displaystyle=\int\_{0}^{1}Q\_{\rho}(s)Q\_{\text{sub}}(1-s)\mathrm{d}s. |  | (25) |

Inspired by the structure of the optimal solution ([10](https://arxiv.org/html/2512.00299v1#S4.E10 "In Theorem 2 (Theorem 5.10 of Wang and Xia (2021)). â€£ 4.1 SSD Problem under Non-concavity: Analytical Difficulty â€£ 4 SSD Problem â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")), We proceed to present a numerical algorithm to propose a suboptimal solution QsubQ\_{\text{sub}}, where we design a correction function ysuby\_{\text{sub}} based on the value of zsubz\_{\text{sub}} in different sections of the poor performance region CC.

Detecting the structure of the poor performance region is the key task. We first define a function

|  |  |  |  |
| --- | --- | --- | --- |
|  | y0â€‹(s)â‰œinf{yâ©¾0|Iâ€‹(Î»â€‹(QÏâ€‹(s)âˆ’y))âˆ’Q0â€‹(1âˆ’s)â©¾0},sâˆˆ(0,1).y\_{0}(s)\triangleq\inf\{y\geqslant 0|I\left(\lambda(Q\_{\rho}(s)-y)\right)-Q\_{0}(1-s)\geqslant 0\},~~s\in(0,1). |  | (26) |

Hence, we alternatively write the poor performance region as

|  |  |  |  |
| --- | --- | --- | --- |
|  | C={tâˆˆ(0,1)|y0â€‹(t)>0}.C=\{t\in(0,1)|y\_{0}(t)>0\}. |  | (27) |

For some very poorly-performed scenarios tâˆˆCt\in C, the function y0y\_{0} is adopted such that Qsubâ€‹(1âˆ’t)=Iâ€‹(Î»â€‹(QÏâ€‹(t)âˆ’y0â€‹(t)))=Q0â€‹(1âˆ’t)Q\_{\text{sub}}(1-t)=I(\lambda(Q\_{\rho}(t)-y\_{0}(t)))=Q\_{0}(1-t).

The general idea in the construction of the sub-optimal solution to SSD Problem ([4](https://arxiv.org/html/2512.00299v1#S2.E4 "In 2 Model Settings â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")) is: When zsubâ©½0z\_{\text{sub}}\leqslant 0, we use the classic solution to achieve optimality (now ysub=0y\_{\text{sub}}=0 or constant, and QSSDQ\_{\text{SSD}} is the form of II); When zsub>0z\_{\text{sub}}>0, we set ysub=y0y\_{\text{sub}}=y\_{0} such that Qsub=Q0Q\_{\text{sub}}=Q\_{0} to satisfy the SSD constraint. Throughout the algorithm design, we need to guarantee that ysuby\_{\text{sub}} is increasing and non-negative.

We therefore propose Algorithm [1](https://arxiv.org/html/2512.00299v1#alg1 "Algorithm 1 â€£ 4.2 SSD Problem: Numerical Algorithm â€£ 4 SSD Problem â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") below, named the Poor-Performance-Region Algorithm. Based on Theorem [2](https://arxiv.org/html/2512.00299v1#Thmtheorem2 "Theorem 2 (Theorem 5.10 of Wang and Xia (2021)). â€£ 4.1 SSD Problem under Non-concavity: Analytical Difficulty â€£ 4 SSD Problem â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network"), we design a closed-form sub-optimal solution to Problem ([14](https://arxiv.org/html/2512.00299v1#S4.E14 "In 4.1 SSD Problem under Non-concavity: Analytical Difficulty â€£ 4 SSD Problem â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")):

|  |  |  |  |
| --- | --- | --- | --- |
|  | Qsubâ€‹(s)=âˆ‘i=0nIâ€‹(Î»subâ€‹(QÏâ€‹(1âˆ’s)âˆ’y0â€‹(ti)))â€‹ğŸ™{sâˆˆ(1âˆ’ai+1,1âˆ’ti)}+âˆ‘i=1nQ0â€‹(s)â€‹ğŸ™{sâˆˆ[1âˆ’ti,1âˆ’ai]},sâˆˆ(0,1).Q\_{\text{sub}}(s)=\sum\_{i=0}^{n}I\left(\lambda\_{\text{sub}}\left(Q\_{\rho}(1-s)-y\_{0}(t\_{i})\right)\right)\mathds{1}\_{\{s\in(1-a\_{i+1},1-t\_{i})\}}+\sum\_{i=1}^{n}Q\_{0}(s)\mathds{1}\_{\{s\in[1-t\_{i},1-a\_{i}]\}},\;\;s\in(0,1). |  | (28) |

This quantile function is also given in Eq. ([24](https://arxiv.org/html/2512.00299v1#S4.E24 "In 14 â€£ 4.2 SSD Problem: Numerical Algorithm â€£ 4 SSD Problem â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")). The key idea is to use an increasing step function ySSDâˆ—â€‹(â‹…)y^{\*}\_{\text{SSD}}(\cdot) for approximation. The financial insight is that the decision maker should follow the SSD benchmark quantile Q0Q\_{0} on some poor performance scenario and conduct the unconstrained optimal strategy Iâ€‹(Î»subâ€‹(QÏâ€‹(1âˆ’s)âˆ’y0â€‹(ti)))I\left(\lambda\_{\text{sub}}\left(Q\_{\rho}(1-s)-y\_{0}(t\_{i})\right)\right) otherwise.

Here are some explanations of the algorithm design.

1. a.

   In this algorithm, the partition points {ai}i=1n{\{a\_{i}\}}^{n}\_{i=1} and {ti}i=1n{\{t\_{i}\}}^{n}\_{i=1} of the poor performance region play an essential role in determining the structure of the optimal solution.
2. b.

   In Step 2: In the whole procedure, we are solving out the structure of the optimal solution and the Lagrange multiplier. For the latter, note that the initial Lagrange multiplier may not satisfy the budget constraint. But it is a good initial point to start the algorithm. It will be determined in the last step.
3. c.

   In Step 3: Because Iâ€‹(Î»â€‹QÏâ€‹(â‹…))I(\lambda Q\_{\rho}(\cdot)) and Q0(1âˆ’â‹…)Q\_{0}(1-\cdot) are both nonincreasing, the set CC can be written as the union of disjoint intervals âˆªi=1n(ai,bi)\cup\_{i=1}^{n}(a\_{i},b\_{i}) or âˆªi=1âˆ(ai,bi)\cup\_{i=1}^{\infty}(a\_{i},b\_{i}). In the latter case, to construct a numerically tractable solution, we use the union of the first nn disjoint intervals, where nn can be specified based on oneâ€™s computational capability.
4. d.

   In Steps 4-5: For any tâˆˆ[bi,ai+1)t\in[b\_{i},a\_{i+1}), we have
   I(Î»^\* (Q\_Ï(s) - y\_0(t) ) ) - Q\_0(1-s) â©¾I(Î»^\* (Q\_Ï(s) ) ) - Q\_0(1-s) â©¾0, Â Â  s âˆˆ(t, a\_i+1].
   We then compute
   g\_i(t) = - âˆ«\_t^a\_i+1 ( I(Î»^\* (Q\_Ï(s) - y\_0(t) ) ) - Q\_0(1-s) ) ds â©½0.
   Hence, any tâˆˆ[bi,ai+1)t\in[b\_{i},a\_{i+1}) satisfies the SSD constraint. We desire to search the first point tit\_{i} on (ai,b1)(a\_{i},b\_{1}) which does not satisfy the constraint.
5. e.

   In Steps 8-12, we need to check whether the constructed ysubâ€‹(â‹…)y\_{\text{sub}}(\cdot) is non-decreasing on (0,1) to make the solution valid. It holds in many cases. However, in some extreme cases, we could still ensure the monotonicity by redesigning the construction of tit\_{i} defined in Eq. ([20](https://arxiv.org/html/2512.00299v1#S4.E20 "In 5 â€£ Algorithm 1 â€£ 4.2 SSD Problem: Numerical Algorithm â€£ 4 SSD Problem â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")), which is named tleftt\_{\text{left}} in Eq. ([23](https://arxiv.org/html/2512.00299v1#S4.E23 "In 11 â€£ Algorithm 1 â€£ 4.2 SSD Problem: Numerical Algorithm â€£ 4 SSD Problem â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")). The specific case will be shown in Section [5](https://arxiv.org/html/2512.00299v1#S5 "5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network").
6. f.

   In Step 14, in many cases, one can design that Qsub(â‹…)=I(Î»(QÏ(1âˆ’â‹…)âˆ’y0(â‹…)))=Q0(â‹…)Q\_{\text{sub}}(\cdot)=I(\lambda(Q\_{\rho}(1-\cdot)-y\_{0}(\cdot)))=Q\_{0}(\cdot) on (1âˆ’tn,1âˆ’an](1-t\_{n},1-a\_{n}].
7. g.

   In Step 15, we need to check the condition holds numerically. It holds in many cases.

In the later sections, based on Algorithm [1](https://arxiv.org/html/2512.00299v1#alg1 "Algorithm 1 â€£ 4.2 SSD Problem: Numerical Algorithm â€£ 4 SSD Problem â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network"), we are able to provide the numerical illustration for the SSD Problem ([4](https://arxiv.org/html/2512.00299v1#S2.E4 "In 2 Model Settings â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")) with various concrete settings.

## 5 Numerical Results

Our study is motivated by the Black-Scholes model in a complete market. A classic Black-Scholes model consists of one riskless bond (dâ€‹BtBt=râ€‹dâ€‹t,tâˆˆ[0,T)\frac{\mathrm{d}B\_{t}}{B\_{t}}=r\mathrm{d}t,~t\in[0,T), where the risk-free rate is r=0.05r=0.05) and one stock (dâ€‹StSt=Î¼Sâ€‹dâ€‹t+ÏƒSâ€‹dâ€‹Wt,tâˆˆ[0,T)\frac{\mathrm{d}S\_{t}}{S\_{t}}=\mu\_{\text{S}}\mathrm{d}t+\sigma\_{\text{S}}\mathrm{d}W\_{t},~t\in[0,T), which is a geometric Brownian motion with the expected return rate Î¼S=0.086\mu\_{\text{S}}=0.086 and the volatility parameter ÏƒS=0.3\sigma\_{\text{S}}=0.3 and {Wt}0â©½tâ©½T\{W\_{t}\}\_{0\leqslant t\leqslant T} is a standard Brownian motion).
The wealth process is given by
dâ€‹Xt=(râ€‹Xt+(Î¼Sâˆ’r)â€‹Ï€t)â€‹dâ€‹t+ÏƒSâ€‹Ï€tâ€‹dâ€‹Wt,tâˆˆ[0,T)\mathrm{d}X\_{t}=\left(rX\_{t}+(\mu\_{S}-r)\pi\_{t}\right)\mathrm{d}t+\sigma\_{S}\pi\_{t}\mathrm{d}W\_{t},~t\in[0,T)
and
X0=xÂ¯X\_{0}=\overline{x},
where {Ï€t}0â©½t<T\{\pi\_{t}\}\_{0\leqslant t<T} is the control process representing the investment amount in the stock and Tâˆˆ(0,âˆ)T\in(0,\infty) is the evaluation time of investment. We define the pricing kernel process {Ït}0â©½tâ©½T\{\rho\_{t}\}\_{0\leqslant t\leqslant T} by

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | dâ€‹ÏtÏt=âˆ’râ€‹dâ€‹tâˆ’Î¸â€‹dâ€‹Wt,tâˆˆ[0,T],\displaystyle\frac{\mathrm{d}\rho\_{t}}{\rho\_{t}}=-r\mathrm{d}t-\theta\mathrm{d}W\_{t},~~t\in[0,T], |  | (29) |

where we denote the market price of risk by Î¸â‰œ(Î¼Sâˆ’r)/ÏƒS\theta\triangleq(\mu\_{S}-r)/\sigma\_{S}.

As the market is complete and one can use the martingale method to duplicate the optimal portfolio process, it is sufficient to solve the optimal terminal wealth variable via the static problem ([1](https://arxiv.org/html/2512.00299v1#S2.E1 "In 2 Model Settings â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")) (see, e.g., Liang and Liu ([2024](https://arxiv.org/html/2512.00299v1#bib.bib11))). Hence, our focus is solving the optimal wealth variable in Problem ([1](https://arxiv.org/html/2512.00299v1#S2.E1 "In 2 Model Settings â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")).
Adapting to Problem ([1](https://arxiv.org/html/2512.00299v1#S2.E1 "In 2 Model Settings â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")), we denote the terminal variables of pricing kernel and wealth by Ï:=ÏT\rho:=\rho\_{T} and X:=XTX:=X\_{T}, with a slight abuse of notation. Solving from Eq. ([29](https://arxiv.org/html/2512.00299v1#S5.E29 "In 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")), Ï\rho follows the log-normal distribution (i.e., logâ¡(Ï)âˆ¼Nâ€‹(âˆ’(r+Î¸2/2)â€‹T,(Î¸â€‹T)2)\log(\rho)\sim\text{N}(-(r+\theta^{2}/2)T,(\theta\sqrt{T})^{2})) and has a quantile function

|  |  |  |  |
| --- | --- | --- | --- |
|  | QÏ(t)=exp{Î¸TÎ¦âˆ’1(t)âˆ’(r+Î¸2/2)T)}â‰œexp{ÏƒÎ¦âˆ’1(t)+Î¼},tâˆˆ(0,1),Q\_{\rho}(t)=\exp\left\{\theta\sqrt{T}\Phi^{-1}(t)-(r+\theta^{2}/2)T)\right\}\triangleq\exp\left\{\sigma\Phi^{-1}(t)+\mu\right\},~t\in(0,1), |  | (30) |

where we denote by Î¦âˆ’1\Phi^{-1} the standard normal quantile function and define Ïƒâ‰œÎ¸â€‹T\sigma\triangleq\theta\sqrt{T} and Î¼â‰œâˆ’(r+Î¸2/2)T)\mu\triangleq-(r+\theta^{2}/2)T).
In Sections [5](https://arxiv.org/html/2512.00299v1#S5 "5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")-[6](https://arxiv.org/html/2512.00299v1#S6 "6 Algorithm-Guided Piecewise-Neural-Network Framework â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network"), we will mainly consider this QÏQ\_{\rho} in Eq. ([30](https://arxiv.org/html/2512.00299v1#S5.E30 "In 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")) and specify different benchmark quantile functions Q0Q\_{0} and various utility functions.

We specify the parameters:
the risk-free rate is r=0.05r=0.05;
the expected return rate Î¼S=0.086\mu\_{\text{S}}=0.086;
the volatility parameter ÏƒS=0.3\sigma\_{\text{S}}=0.3;
The evaluation time of investment is T=20T=20 (years).
It follows that the market price of risk is Î¸=(Î¼âˆ’r)/Ïƒ=0.12\theta=(\mu-r)/\sigma=0.12. We compute that Ïƒ=0.5367\sigma=0.5367 and Î¼=âˆ’1.1440\mu=-1.1440.
We numerically illustrate our result by using the Black-Scholes model above.

### 5.1 FSD Problem: S-shaped Utility (Theorem [1](https://arxiv.org/html/2512.00299v1#Thmtheorem1 "Theorem 1. â€£ 3 FSD Problem and Analytical Solution â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network"))

We begin by specifying a S-shaped utility U:[L,âˆ)â†’â„U:[L,\infty)\rightarrow\mathbb{R}, following the general S-shaped utility formulation in Definition [2](https://arxiv.org/html/2512.00299v1#Thmdefinition2 "Definition 2 (General S-shaped utility). â€£ 3 FSD Problem and Analytical Solution â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network"):

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | U(x)={xpp,xâ©¾0,âˆ’kâ€‹(âˆ’x)q,Lâ©½x<0,\displaystyle U(x)=\left\{\begin{aligned} &\frac{x^{p}}{p},&&x\geqslant 0,\\ &-k(-x)^{q},&&L\leqslant x<0,\end{aligned}\right. |  | (31) |

where the parameters are set to p=0.6,q=0.5p=0.6,q=0.5, and k=2k=2.

We then consider two portfolio selection problems. The first is Problem ([3](https://arxiv.org/html/2512.00299v1#S2.E3 "In 2 Model Settings â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")) with the general setting of S-shaped utility in Definition [2](https://arxiv.org/html/2512.00299v1#Thmdefinition2 "Definition 2 (General S-shaped utility). â€£ 3 FSD Problem and Analytical Solution â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") (In particular, Eq. ([31](https://arxiv.org/html/2512.00299v1#S5.E31 "In 5.1 FSD Problem: S-shaped Utility (Theorem 1) â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network"))) and the benchmark quantile
Q0â€‹(t)=10â€‹t2âˆ’1,tâˆˆ[0,1].Q\_{0}(t)=10t^{2}-1,\;t\in[0,1].
The second is the Merton problem ([1](https://arxiv.org/html/2512.00299v1#S2.E1 "In 2 Model Settings â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")) using the S-shaped utility setting in Eq. ([31](https://arxiv.org/html/2512.00299v1#S5.E31 "In 5.1 FSD Problem: S-shaped Utility (Theorem 1) â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")):

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | maxXâˆˆL0â¡ğ”¼â€‹[Uâ€‹(X)],Â s.t.Â â€‹ğ”¼â€‹[Ïâ€‹X]â©½xÂ¯,Xâ©¾Lâ€‹Â a.s.Â ,\displaystyle\max\_{X\in L^{0}}\mathbb{E}[U(X)],\text{ s.t. }\mathbb{E}[\rho X]\leqslant\overline{x},\;X\geqslant L\text{ a.s. }, |  | (32) |

where the liquidation boundary is given by L=âˆ’5L=-5. Here we add the liquidation boundary in order to make the second problem well defined and compare with the first problem.

In this example, the first-order SD constraint acts a similar role as the liquidation constraint: if Q0â€‹(1âˆ’FÏâ€‹(Ï))=Q0â€‹(0)Q\_{0}(1-F\_{\rho}(\rho))=Q\_{0}(0), then the optimal solution Xâˆ—X^{\*} locates at the boundary Q0â€‹(1âˆ’FÏâ€‹(Ï))Q\_{0}(1-F\_{\rho}(\rho)), otherwise Xâˆ—X^{\*} is the same as the classic solution (U1â€²)âˆ’1â€‹(Î»â€‹Ï)(U\_{1}^{\prime})^{-1}(\lambda\rho).

![Refer to caption](x1.png)


Figure 1: First-order SD constraint: xÂ¯=5\bar{x}=5

### 5.2 SSD Problem: Power Utility

We first assume the decision maker has a CRRA utility

|  |  |  |  |
| --- | --- | --- | --- |
|  | Uâ€‹(x)=1pâ€‹xp,x>0,U(x)=\frac{1}{p}x^{p},~~x>0, |  | (33) |

where p=0.6p=0.6. We suppose that the benchmark quantile Q0Q\_{0} also follows the log-normal distribution:

|  |  |  |
| --- | --- | --- |
|  | Q0â€‹(t)=expâ¡{Ïƒ0â€‹Î¦âˆ’1â€‹(t)+Î¼0},tâˆˆ[0,1].Q\_{0}(t)=\exp\left\{\sigma\_{0}\Phi^{-1}(t)+\mu\_{0}\right\},\;t\in[0,1]. |  |

The settings are given in Table [1](https://arxiv.org/html/2512.00299v1#S5.T1 "Table 1 â€£ 5.2 SSD Problem: Power Utility â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network").

|  |  |  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| rr | Î¼S\mu\_{\text{S}} | ÏƒS\sigma\_{\text{S}} | Î¸\theta | TT | Î¼\mu | Ïƒ\sigma | xÂ¯\overline{x} | pp |
| 0.05 | 0.086 | 0.3 | 0.12 | 20 | -1.1440 | 0.5367 | 10 | 0.6 |

Table 1: Parameter setting in the numerical illustration.

For the concave utility specified in Eq. ([33](https://arxiv.org/html/2512.00299v1#S5.E33 "In 5.2 SSD Problem: Power Utility â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")), the applicability of Proposition 6.8 in Wang and Xia ([2021](https://arxiv.org/html/2512.00299v1#bib.bib18)) to power utility with a log-normal pricing kernel provides a theoretical benchmark. Our algorithmâ€™s result coincides with the characterization in Wang and Xia ([2021](https://arxiv.org/html/2512.00299v1#bib.bib18)). Consequently, the algorithm attains the optimal solution in the following cases in Figure [2](https://arxiv.org/html/2512.00299v1#S5.F2 "Figure 2 â€£ 5.2 SSD Problem: Power Utility â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network").
For comparison, we also compute the classical optimal solution obtained without the SSD constraint and plot the figure.

|  |  |  |
| --- | --- | --- |
|  | Refer to captionRefer to caption\displaystyle\tiny\begin{matrix}\includegraphics[width=226.16321pt]{PowerU\_result/3\_1.eps}&\includegraphics[width=226.16321pt]{PowerU\_result/3\_0.6.eps}\end{matrix} |  |

|  |  |  |
| --- | --- | --- |
|  | Refer to captionRefer to caption\displaystyle\tiny\begin{matrix}\includegraphics[width=226.16321pt]{PowerU\_result/3\_1.4.eps}&\includegraphics[width=226.16321pt]{PowerU\_result/3.2\_1.eps}\end{matrix} |  |

|  |  |  |
| --- | --- | --- |
|  | Refer to captionRefer to caption\displaystyle\tiny\begin{matrix}\includegraphics[width=226.16321pt]{PowerU\_result/2.3\_2.eps}&\includegraphics[width=226.16321pt]{PowerU\_result/1.5\_2.5.eps}\end{matrix} |  |

Figure 2: Impacts of Î¼0\mu\_{0} and Ïƒ0\sigma\_{0}.



|  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- |
|  | parametrization | budget of Q0Q\_{0} | poor performance region CC | Î»\lambda | Î»cla\lambda\_{\text{cla}} | partition parameter |
| (a) | (Î¼0,Ïƒ0)=(3,1)(\mu\_{0},\sigma\_{0})=(3,1) | 7.1231 | (0.6092,1)(0.6092,1) | 0.91040.9104 | 0.9003 | t1=1t\_{1}=1 |
| (b) | (Î¼0,Ïƒ0)=(3,0.6)(\mu\_{0},\sigma\_{0})=(3,0.6) | 6.4109 | (0.4978,1)(0.4978,1) | 0.9471 | 0.9003 | t1=1t\_{1}=1 |
| (c) | (Î¼0,Ïƒ0)=(3,1.4)(\mu\_{0},\sigma\_{0})=(3,1.4) | 9.2876 | (0, 0.0179) | 0.9003 | 0.9003 | t1=0t\_{1}=0 |
| (d) | (Î¼0,Ïƒ0)=(3.2,1)(\mu\_{0},\sigma\_{0})=(3.2,1) | 8.7002 | (0.2858, 1) | 0.9430 | 0.9003 | t1=1t\_{1}=1 |
| (e) | (Î¼0,Ïƒ0)=(2.3,2)(\mu\_{0},\sigma\_{0})=(2.3,2) | 9.2691 | (0, 0.4309) | 1.1951 | 0.9003 | t1=0.0057t\_{1}=0.0057 |
| (f) | (Î¼0,Ïƒ0)=(1.5,2.5)(\mu\_{0},\sigma\_{0})=(1.5,2.5) | 9.8096 | (0, 0.6248) | 1.9965 | 0.9003 | t1=0.0654t\_{1}=0.0654 |

Table 2: Numerical results in Figure [2](https://arxiv.org/html/2512.00299v1#S5.F2 "Figure 2 â€£ 5.2 SSD Problem: Power Utility â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network").

The basic logic is that:

1. (i)

   If the value of the pricing kernel is small, then the optimal wealth value is larger. Hence, the pricing kernel value is a signal of the market state: A small value shows a good market scenario.
2. (ii)

   If the poor performance region CC is smaller, then the optimal wealth is better (compared to the SSD constraint). This is because the SSD constraint is easier to achieve and the optimal wealth is closer to the classic unconstrained solution XclaX\_{\text{cla}}.

We apply the Poor-Performance-Region Algorithm (PPRA), the explanations and financial insights from Figure [2](https://arxiv.org/html/2512.00299v1#S5.F2 "Figure 2 â€£ 5.2 SSD Problem: Power Utility â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") and Table [2](https://arxiv.org/html/2512.00299v1#S5.T2 "Table 2 â€£ 5.2 SSD Problem: Power Utility â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") are as follows:

1. 1.

   Figure [2](https://arxiv.org/html/2512.00299v1#S5.F2 "Figure 2 â€£ 5.2 SSD Problem: Power Utility â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") (a) vs (b) vs (d): When the market performs poorly, the optimal wealth must coincide with the benchmark. This is because, in such adverse scenario, the benchmark provides a large value, and the SSD constraint serves to guarantee a minimum safety level and reduce risk.
2. 2.

   Figure [2](https://arxiv.org/html/2512.00299v1#S5.F2 "Figure 2 â€£ 5.2 SSD Problem: Power Utility â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") (c): The poor performance region is very small, which implies the classical solution inherently satisfies the SSD constraint. As a result, we observe that Î»=Î»cla\lambda=\lambda\_{\text{cla}} and the optimal solution essentially coincides with the classical solution. This indicates that the benchmark plays only a limited role in shaping the optimal wealth.
3. 3.

   Figure [2](https://arxiv.org/html/2512.00299v1#S5.F2 "Figure 2 â€£ 5.2 SSD Problem: Power Utility â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") (e) vs (f): The budget of Q0Q\_{0} is close to xÂ¯\bar{x}, to ensure the SSD constraint, the optimal wealth would behave similarly to the benchmark. However, due to a different Î»\lambda and a correction function ysubâ€‹(â‹…)y\_{\text{sub}}(\cdot) in the SSD problem, the solution between the SSD problem and the classical problem differs.

### 5.3 SSD Problem: S-shaped Utility

We use Algorithm [1](https://arxiv.org/html/2512.00299v1#alg1 "Algorithm 1 â€£ 4.2 SSD Problem: Numerical Algorithm â€£ 4 SSD Problem â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") to study the sub-optimal solution of the SSD problem with S-shaped utility. First, we adopt the S-shaped utility setting in Eq. ([31](https://arxiv.org/html/2512.00299v1#S5.E31 "In 5.1 FSD Problem: S-shaped Utility (Theorem 1) â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")).
We then suppose that the benchmark quantile Q0Q\_{0} also follows a log-normal distribution:

|  |  |  |
| --- | --- | --- |
|  | Q0â€‹(t)=expâ¡{Ïƒ0â€‹Î¦âˆ’1â€‹(t)+Î¼0},tâˆˆ[0,1].Q\_{0}(t)=\exp\left\{\sigma\_{0}\Phi^{-1}(t)+\mu\_{0}\right\},\;t\in[0,1]. |  |

We also compute the classic optimal solution without the SSD constraint. The settings are provided in Table [3](https://arxiv.org/html/2512.00299v1#S5.T3 "Table 3 â€£ 5.3 SSD Problem: S-shaped Utility â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network").

|  |  |  |  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| rr | Î¼S\mu\_{\text{S}} | ÏƒS\sigma\_{\text{S}} | Î¸\theta | TT | Î¼\mu | Ïƒ\sigma | xÂ¯\overline{x} | pp | k |
| 0.05 | 0.086 | 0.3 | 0.12 | 20 | -1.1440 | 0.5367 | 10 | 0.6 | 2 |

Table 3: Parameter setting in Figure [3](https://arxiv.org/html/2512.00299v1#S5.F3 "Figure 3 â€£ 5.3 SSD Problem: S-shaped Utility â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network").



|  |  |  |
| --- | --- | --- |
|  | Refer to captionRefer to caption\displaystyle\tiny\begin{matrix}\includegraphics[width=197.8937pt,height=191.42567pt]{3\_1.eps}&\includegraphics[width=197.8937pt,height=191.42567pt]{3\_0.6.eps}\end{matrix} |  |

|  |  |  |
| --- | --- | --- |
|  | Refer to captionRefer to caption\displaystyle\tiny\begin{matrix}\includegraphics[width=197.8937pt,height=191.42567pt]{3\_0.8.eps}&\includegraphics[width=197.8937pt,height=191.42567pt]{3.2\_1.eps}\end{matrix} |  |

|  |  |  |
| --- | --- | --- |
|  | Refer to captionRefer to caption\displaystyle\tiny\begin{matrix}\includegraphics[width=197.8937pt,height=191.42567pt]{2.3\_2.eps}&\includegraphics[width=197.8937pt,height=191.42567pt]{1.5\_2.5.eps}\end{matrix} |  |

Figure 3: Impacts of Î¼0\mu\_{0} and Ïƒ0\sigma\_{0}.



|  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- |
|  | parametrization | budget of Q0Q\_{0} | poor performance region CC | Î»\lambda | Î»cla\lambda\_{\text{cla}} | partition parameter |
| (a) | (Î¼0,Ïƒ0)=(3,1)(\mu\_{0},\sigma\_{0})=(3,1) | 7.1231 | (0.6089,1)(0.6089,1) | 0.91050.9105 | 0.8979 | t1=1t\_{1}=1 |
| (b) | (Î¼0,Ïƒ0)=(3,0.6)(\mu\_{0},\sigma\_{0})=(3,0.6) | 6.4109 | (0.4978,1)(0.4978,1) | 0.9471 | 0.8979 | t1=1t\_{1}=1 |
| (c) | (Î¼0,Ïƒ0)=(3,0.8)(\mu\_{0},\sigma\_{0})=(3,0.8) | 6.6238 | (0.5394,1)(0.5394,1) | 0.9255 | 0.8979 | t1=1t\_{1}=1 |
| (d) | (Î¼0,Ïƒ0)=(3.2,1)(\mu\_{0},\sigma\_{0})=(3.2,1) | 8.7002 | (0.2858, 1) | 0.9430 | 0.8979 | t1=1t\_{1}=1 |
| (e) | (Î¼0,Ïƒ0)=(2.3,2)(\mu\_{0},\sigma\_{0})=(2.3,2) | 9.2691 | (0,0.4355)âˆª(0.9669,1)(0,0.4355)\cup(0.9669,1) | 1.1987 | 0.8979 | t2=1,t1=0.0061t\_{2}=1,t\_{1}=0.0061 |
| (f) | (Î¼0,Ïƒ0)=(1.5,2.5)(\mu\_{0},\sigma\_{0})=(1.5,2.5) | 9.8087 | (0,0.6840)âˆª(0.7726,1)(0,0.6840)\cup(0.7726,1) | 2.1508 | 0.8979 | t2=1,t1=0.0957t\_{2}=1,t\_{1}=0.0957 |

Table 4: Numerical results in Figure [3](https://arxiv.org/html/2512.00299v1#S5.F3 "Figure 3 â€£ 5.3 SSD Problem: S-shaped Utility â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network").

We apply the Poor-Performance-Region Algorithm (PPRA), the explanations and financial insights from Figure [3](https://arxiv.org/html/2512.00299v1#S5.F3 "Figure 3 â€£ 5.3 SSD Problem: S-shaped Utility â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") and Table [4](https://arxiv.org/html/2512.00299v1#S5.T4 "Table 4 â€£ 5.3 SSD Problem: S-shaped Utility â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") are summarized as follows:

1. 1.

   Figure [3](https://arxiv.org/html/2512.00299v1#S5.F3 "Figure 3 â€£ 5.3 SSD Problem: S-shaped Utility â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") (a) vs (b) vs (c) vs (d): These cases correspond to scenarios in which the poor performance region consists of a single interval. Numerically, a smaller Î¼0\mu\_{0} reduces the size of the poor performance region CC whereas a smaller Ïƒ0\sigma\_{0} enlarges it.
2. 2.

   Figure [3](https://arxiv.org/html/2512.00299v1#S5.F3 "Figure 3 â€£ 5.3 SSD Problem: S-shaped Utility â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network"): Across the S-shaped utility cases, we observe that when the market performs poorly, relative to the classical solution, the PPRA wealth exhibits a clear improvement, driven by the SSD constraint. Therefore, in adverse market scenario, the SSD constraint effectively performs as a risk-control mechanism, ensuring that the PPRA wealth remains at least as high as the benchmark wealth.
3. 3.

   Figure [3](https://arxiv.org/html/2512.00299v1#S5.F3 "Figure 3 â€£ 5.3 SSD Problem: S-shaped Utility â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") (e) vs (f): In these examples, the poor performance region splits into two disjoint intervals. We further observe that when the budget of Q0Q\_{0} is close to the bound xÂ¯\bar{x}, the PPRA wealth becomes increasingly close to the benchmark. This phenomenon arises because a high level of the budget of Q0Q\_{0} make the SSD constraint dominate the optimization.
4. 4.

   Figure [3](https://arxiv.org/html/2512.00299v1#S5.F3 "Figure 3 â€£ 5.3 SSD Problem: S-shaped Utility â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") (a) vs (b) vs (d): A deeper examination of the impact of the budget of Q0Q\_{0} shows a consistent pattern: as the budget of Q0Q\_{0} approaches the bar xÂ¯\bar{x}, the PPRA wealth converges towards the benchmark. This illustrates how the budget level critically shapes the structure of the PPRA wealth.

### 5.4 SSD Problem: Various Utilities and Benchmark Quantiles

Based on the proposed algorithm, we further extend its applicability to a broader class of the SSD problem.
To assess the generality of our approach, we conduct numerical experiments using different utilities and benchmark quantiles Q0Q\_{0}.

For the utility function Uâ€‹(x)U(x), we consider several representative forms capturing different risk preference, as shown in Table [5](https://arxiv.org/html/2512.00299v1#S5.T5 "Table 5 â€£ 5.4 SSD Problem: Various Utilities and Benchmark Quantiles â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network").

|  |  |  |  |
| --- | --- | --- | --- |
|  | exponential | log | piecewise |
| Uâ€‹(â‹…)U(\cdot) | âˆ’1pâ€‹expâ¡(âˆ’pâ‹…x),x>0-\dfrac{1}{p}\exp(-p\cdot x),\;x>0 | logâ¡(x),x>0\log(x),\;x>0 | {(xâˆ’1)p1,xâ©¾2âˆ’Î»1â€‹(1âˆ’x)q1,1â©½x<2xp2+C,0â©½x<1Câˆ’Î»2â€‹(âˆ’x)q2,âˆ’1â©½x<0\begin{cases}(x-1)^{p\_{1}},&x\geqslant 2\\ -\lambda\_{1}(1-x)^{q\_{1}},&1\leqslant x<2\\ x^{p\_{2}}+C,&0\leqslant x<1\\ C-\lambda\_{2}(-x)^{q\_{2}},&-1\leqslant x<0\end{cases} |

Table 5: Setting of various utilities.

For the benchmark quantile Q0Q\_{0}, we consider the following four cases, as shown in Table [6](https://arxiv.org/html/2512.00299v1#S5.T6 "Table 6 â€£ 5.4 SSD Problem: Various Utilities and Benchmark Quantiles â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network").

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | exponential | log-normal | normal | uniform |
| Q0â€‹(â‹…)Q\_{0}(\cdot) | âˆ’logâ¡(1âˆ’t)Î±+k0-\dfrac{\log(1-t)}{\alpha}+k\_{0} | expâ¡(Ïƒ0â‹…Î¦âˆ’1â€‹(t)+Î¼0)+k0\exp\left(\sigma\_{0}\cdot\Phi^{-1}(t)+\mu\_{0}\right)+k\_{0} | Ïƒ0â‹…Î¦âˆ’1â€‹(t)+Î¼0\sigma\_{0}\cdot\Phi^{-1}(t)+\mu\_{0} | kâ€‹t+k0kt+k\_{0} |

Table 6: Benchmark quantiles Q0â€‹(t)Q\_{0}(t), tâˆˆ(0,1)t\in(0,1).

The general settings coincide with the previous numerical examples and are given in Table [7](https://arxiv.org/html/2512.00299v1#S5.T7 "Table 7 â€£ 5.4 SSD Problem: Various Utilities and Benchmark Quantiles â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network").

|  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- |
| rr | Î¼S\mu\_{\text{S}} | ÏƒS\sigma\_{\text{S}} | Î¸\theta | TT | Î¼\mu | Ïƒ\sigma |
| 0.05 | 0.086 | 0.3 | 0.12 | 20 | -1.1440 | 0.5367 |

Table 7: General setting in Figures [4](https://arxiv.org/html/2512.00299v1#S5.F4 "Figure 4 â€£ 5.4 SSD Problem: Various Utilities and Benchmark Quantiles â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")-[5](https://arxiv.org/html/2512.00299v1#S5.F5 "Figure 5 â€£ 5.4 SSD Problem: Various Utilities and Benchmark Quantiles â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network").

We investigate several combinations of the utilities and the benchmark quantiles. To illustrate the effectiveness of our proposed algorithm, we present the most representative cases under the parameter settings in Table [8](https://arxiv.org/html/2512.00299v1#S5.T8 "Table 8 â€£ 5.4 SSD Problem: Various Utilities and Benchmark Quantiles â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network").

|  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- |
|  | utility | distribution of Q0Q\_{0} | xÂ¯\overline{x} | parameters of Q0Q\_{0} | parameters of utility |
| (a) | exponential | uniform | 0.3 | k=1,k0=0k=1,k\_{0}=0 | p=0.6p=0.6 |
| (b) | exponential | exponential | 0.3 | Î±=1.5,k0=0\alpha=1.5,k\_{0}=0 | p=0.6p=0.6 |
| (c) | log | normal | 1.8 | Î¼0=5,Ïƒ0=1\mu\_{0}=5,\sigma\_{0}=1 | â€“ |
| (d) | log | uniform | 1.4 | k=10,k0=0k=10,k\_{0}=0 | â€“ |
| (e) | piecewise | log-normal | 3.5 | Î¼0=âˆ’1,Ïƒ0=3,k0=2.3\mu\_{0}=-1,\sigma\_{0}=3,k\_{0}=2.3 | |  | | --- | | p1=q1=0.6,p2=0.8,p\_{1}=q\_{1}=0.6,p\_{2}=0.8, | | q2=0.9,Î»1=1,Î»2=2q\_{2}=0.9,\lambda\_{1}=1,\lambda\_{2}=2 | |
| (f) | piecewise | exponential | 1.3 | Î±=0.7,k0=2.3\alpha=0.7,k\_{0}=2.3 | |  | | --- | | p1=q1=0.6,p2=0.8,p\_{1}=q\_{1}=0.6,p\_{2}=0.8, | | q2=0.9,Î»1=1,Î»2=2q\_{2}=0.9,\lambda\_{1}=1,\lambda\_{2}=2 | |

Table 8: Parameter setting in Figures [4](https://arxiv.org/html/2512.00299v1#S5.F4 "Figure 4 â€£ 5.4 SSD Problem: Various Utilities and Benchmark Quantiles â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")-[5](https://arxiv.org/html/2512.00299v1#S5.F5 "Figure 5 â€£ 5.4 SSD Problem: Various Utilities and Benchmark Quantiles â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network").

Applying the Algorithm [1](https://arxiv.org/html/2512.00299v1#alg1 "Algorithm 1 â€£ 4.2 SSD Problem: Numerical Algorithm â€£ 4 SSD Problem â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network"), we obtain the numerical results in Table [9](https://arxiv.org/html/2512.00299v1#S5.T9 "Table 9 â€£ 5.4 SSD Problem: Various Utilities and Benchmark Quantiles â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") and plot Figures [4](https://arxiv.org/html/2512.00299v1#S5.F4 "Figure 4 â€£ 5.4 SSD Problem: Various Utilities and Benchmark Quantiles â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")-[5](https://arxiv.org/html/2512.00299v1#S5.F5 "Figure 5 â€£ 5.4 SSD Problem: Various Utilities and Benchmark Quantiles â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network"), where each panel illustrates the structure of the correction function ysubâ€‹(â‹…)y\_{\text{sub}}(\cdot) and the Poor-Performance-Region Algorithm (PPRA) solution.

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | poor performance region CC | partition parameter | Î»\lambda | Î»cla\lambda\_{\text{cla}} |
| (a) | (0.8803,1)(0.8803,1) | t1=1t\_{1}=1 | Î»=1.5540\lambda=1.5540 | Î»cla=1.4429\lambda\_{\text{cla}}=1.4429 |
| (b) | (0.8904,1)(0.8904,1) | t1=1t\_{1}=1 | Î»=1.5498\lambda=1.5498 | Î»cla=1.4429\lambda\_{\text{cla}}=1.4429 |
| (c) | (0.4608,1)(0.4608,1) | t1=1t\_{1}=1 | Î»=0.6497\lambda=0.6497 | Î»cla=0.5556\lambda\_{\text{cla}}=0.5556 |
| (d) | (0.0426,0.7236)(0.0426,0.7236) | t1=0.1766t\_{1}=0.1766 | Î»=0.8260\lambda=0.8260 | Î»cla=0.7143\lambda\_{\text{cla}}=0.7143 |
| (e) | (0,0.2419)âˆª(0.8930,1)(0,0.2419)\cup(0.8930,1) | t2=1,t1=0.0049t\_{2}=1,t\_{1}=0.0049 | Î»=1.7002\lambda=1.7002 | Î»cla=0.9005\lambda\_{\text{cla}}=0.9005 |
| (f) | (0.3902,1)(0.3902,1) | t1=1t\_{1}=1 | Î»=1.7930\lambda=1.7930 | Î»cla=1.5516\lambda\_{\text{cla}}=1.5516 |

Table 9: Numerical results in Figures [4](https://arxiv.org/html/2512.00299v1#S5.F4 "Figure 4 â€£ 5.4 SSD Problem: Various Utilities and Benchmark Quantiles â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")-[5](https://arxiv.org/html/2512.00299v1#S5.F5 "Figure 5 â€£ 5.4 SSD Problem: Various Utilities and Benchmark Quantiles â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network").



|  |  |  |
| --- | --- | --- |
|  | Refer to captionRefer to caption\displaystyle\tiny\begin{matrix}\includegraphics[width=212.02846pt,height=189.88449pt]{exp2\_y.eps}&\includegraphics[width=212.02846pt,height=193.18452pt]{exp2\_Q.eps}\end{matrix} |  |

|  |  |  |
| --- | --- | --- |
|  | Refer to captionRefer to caption\displaystyle\tiny\begin{matrix}\includegraphics[width=212.02846pt,height=189.88449pt]{exp3\_y.eps}&\includegraphics[width=212.02846pt,height=193.18452pt]{exp3\_Q.eps}\end{matrix} |  |

|  |  |  |
| --- | --- | --- |
|  | Refer to captionRefer to caption\displaystyle\tiny\begin{matrix}\includegraphics[width=212.02846pt,height=189.88449pt]{log1\_y.eps}&\includegraphics[width=212.02846pt,height=193.18452pt]{log1\_Q.eps}\end{matrix} |  |

Figure 4: Setting (a)-(b)-(c).



|  |  |  |
| --- | --- | --- |
|  | Refer to captionRefer to caption\displaystyle\tiny\begin{matrix}\includegraphics[width=212.02846pt,height=189.88449pt]{log2\_y.eps}&\includegraphics[width=212.02846pt,height=193.18452pt]{log2\_Q.eps}\end{matrix} |  |

|  |  |  |
| --- | --- | --- |
|  | Refer to captionRefer to caption\displaystyle\tiny\begin{matrix}\includegraphics[width=212.02846pt,height=189.88449pt]{hard3\_y.eps}&\includegraphics[width=212.02846pt,height=193.18452pt]{hard3\_Q.eps}\end{matrix} |  |

|  |  |  |
| --- | --- | --- |
|  | Refer to captionRefer to caption\displaystyle\tiny\begin{matrix}\includegraphics[width=212.02846pt,height=189.88449pt]{hard1\_y.eps}&\includegraphics[width=212.02846pt,height=193.18452pt]{hard1\_Q.eps}\end{matrix} |  |

Figure 5: Setting (d)-(e)-(f).

Later, we would clarify the structural behavior of the correction function ysubâ€‹(â‹…)y\_{\text{sub}}(\cdot). The numerical experiments yield the following explanations and financial insights:

1. 1.

   Figure [4](https://arxiv.org/html/2512.00299v1#S5.F4 "Figure 4 â€£ 5.4 SSD Problem: Various Utilities and Benchmark Quantiles â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") (a)-(b)-(c): In bad market scenario, the correction function ysubâ€‹(â‹…)y\_{\text{sub}}(\cdot) satisfies ysubâ€‹(â‹…)=y0â€‹(â‹…)y\_{\text{sub}}(\cdot)=y\_{0}(\cdot), therefore, the PPRA wealth will coincide with the benchmark wealth. In this scenario, the benchmark serves as a risk-control mechanism, which is similar to Figure [2](https://arxiv.org/html/2512.00299v1#S5.F2 "Figure 2 â€£ 5.2 SSD Problem: Power Utility â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") (a)-(b)-(d).
2. 2.

   Figure [4](https://arxiv.org/html/2512.00299v1#S5.F4 "Figure 4 â€£ 5.4 SSD Problem: Various Utilities and Benchmark Quantiles â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") (c)-(f): Compared with Figure [4](https://arxiv.org/html/2512.00299v1#S5.F4 "Figure 4 â€£ 5.4 SSD Problem: Various Utilities and Benchmark Quantiles â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") (a)-(b), the region where ysubâ€‹(â‹…)=y0â€‹(â‹…)y\_{\text{sub}}(\cdot)=y\_{0}(\cdot) expands. Hence, under relatively stagnant or bad market scenario, the PPRA wealth tends to coincide with the benchmark wealth. In such scenario, the SSD constraint plays a more prominent role by ensuring a benchmark wealth to reduce risk.
3. 3.

   Figure [5](https://arxiv.org/html/2512.00299v1#S5.F5 "Figure 5 â€£ 5.4 SSD Problem: Various Utilities and Benchmark Quantiles â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") (d): The correction function ysubâ€‹(â‹…)=y0â€‹(â‹…)y\_{\text{sub}}(\cdot)=y\_{0}(\cdot) over certain intermediate intervals, causing the PPRA wealth to coincide with the benchmark in mid-range market scenario, as shown in (d)-Right.
4. 4.

   Figure [5](https://arxiv.org/html/2512.00299v1#S5.F5 "Figure 5 â€£ 5.4 SSD Problem: Various Utilities and Benchmark Quantiles â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") (e): The correction function ysubâ€‹(â‹…)=y0â€‹(â‹…)y\_{\text{sub}}(\cdot)=y\_{0}(\cdot) at both endpoints of the interval (0,1)(0,1). This implies the PPRA wealth must coincide with the benchmark wealth in both extremely bad and exceptionally favorable market scenarios.

In Figure [5](https://arxiv.org/html/2512.00299v1#S5.F5 "Figure 5 â€£ 5.4 SSD Problem: Various Utilities and Benchmark Quantiles â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") (e), we can observe how Steps 8-12 of AlgorithmÂ [1](https://arxiv.org/html/2512.00299v1#alg1 "Algorithm 1 â€£ 4.2 SSD Problem: Numerical Algorithm â€£ 4 SSD Problem â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") ensure that the constructed function ysubâ€‹(â‹…)y\_{\text{sub}}(\cdot) remains non-decreasing on (0,1)(0,1). The key idea is to modify the construction of giâ€‹(t)g\_{i}(t) in Eq.Â ([19](https://arxiv.org/html/2512.00299v1#S4.E19 "In 4 â€£ Algorithm 1 â€£ 4.2 SSD Problem: Numerical Algorithm â€£ 4 SSD Problem â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")) so that both monotonicity requirement of ysubâ€‹(â‹…)y\_{\text{sub}}(\cdot) and the condition zâˆ—â€‹(â‹…)â©½0z^{\*}(\cdot)\leqslant 0 is satisfied throughout the region in which ysubâ€‹(â‹…)y\_{\text{sub}}(\cdot) is updated. In this example, the interval where ysubâ€‹(â‹…)=ysubâ€‹(t1)y\_{\text{sub}}(\cdot)=y\_{\text{sub}}(t\_{1}) is extended into the region covered by the previous iteration (i.e., the first iteration), updating the original ysubâ€‹(â‹…)y\_{\text{sub}}(\cdot) in this region.

Across various combinations of the utilities and the benchmark quantiles Q0Q\_{0}, we observe that the poor-performance region typically consists of one or two disjoint intervals. The proposed algorithm exhibits strong adaptability under these diverse settings, effectively identifying the sub-optimal solution in most cases. At the same time, our algorithm is able to handle certain special cases in which the original piecewise construction fails to guarantee monotonicity. By modifying the construction of giâ€‹(t)g\_{i}(t), the algorithm provides a valid sub-optimal solution and, as a result, restores monotonicity and extends the applicability of our approach to more complex configurations.

## 6 Algorithm-Guided Piecewise-Neural-Network Framework

In this section, we propose a novel approach to solve the SSD Problem ([14](https://arxiv.org/html/2512.00299v1#S4.E14 "In 4.1 SSD Problem under Non-concavity: Analytical Difficulty â€£ 4 SSD Problem â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")) by designing a piecewise-neural-network-framework combined with Algorithm [1](https://arxiv.org/html/2512.00299v1#alg1 "Algorithm 1 â€£ 4.2 SSD Problem: Numerical Algorithm â€£ 4 SSD Problem â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network"). The key observation is that in Algorithm [1](https://arxiv.org/html/2512.00299v1#alg1 "Algorithm 1 â€£ 4.2 SSD Problem: Numerical Algorithm â€£ 4 SSD Problem â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network"), the poor performance region CC defined in Eq. ([27](https://arxiv.org/html/2512.00299v1#S4.E27 "In 4.2 SSD Problem: Numerical Algorithm â€£ 4 SSD Problem â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")) and the construction of the correction function ysubâ€‹(â‹…)y\_{\text{sub}}(\cdot) provide valuable structural information about the optimal solution, thereby guiding the design of an effective neural network framework.

We begin by applying Algorithm [1](https://arxiv.org/html/2512.00299v1#alg1 "Algorithm 1 â€£ 4.2 SSD Problem: Numerical Algorithm â€£ 4 SSD Problem â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network"), which yields a correction function ysubâ€‹(â‹…)y\_{\text{sub}}(\cdot). Using ysubâ€‹(â‹…)y\_{\text{sub}}(\cdot), we derive a sub-optimal solution Qsubâ€‹(â‹…)Q\_{\text{sub}}(\cdot) as presented in Eq. ([24](https://arxiv.org/html/2512.00299v1#S4.E24 "In 14 â€£ 4.2 SSD Problem: Numerical Algorithm â€£ 4 SSD Problem â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")).
To design a neural network framework, the key idea is that we use the structure information of the derived Qsubâ€‹(â‹…)Q\_{\text{sub}}(\cdot) to build a piecewise-neural-network framework. Then we apply this framework to train a solution for Problem ([14](https://arxiv.org/html/2512.00299v1#S4.E14 "In 4.1 SSD Problem under Non-concavity: Analytical Difficulty â€£ 4 SSD Problem â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")).

### 6.1 Model Setting

Algorithm 2  Algorithm-guided piecewise-neural-network framework for SSD Problem ([39](https://arxiv.org/html/2512.00299v1#S6.E39 "In 6.1 Model Setting â€£ 6 Algorithm-Guided Piecewise-Neural-Network Framework â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network"))

1:Implement Algorithm [1](https://arxiv.org/html/2512.00299v1#alg1 "Algorithm 1 â€£ 4.2 SSD Problem: Numerical Algorithm â€£ 4 SSD Problem â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") and get the sub-optimal solution structure as Eq. ([24](https://arxiv.org/html/2512.00299v1#S4.E24 "In 14 â€£ 4.2 SSD Problem: Numerical Algorithm â€£ 4 SSD Problem â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")). Using the partition intervals (sk,sk+1](s\_{k},s\_{k+1}] defined in Eq. ([40](https://arxiv.org/html/2512.00299v1#S6.E40 "In 6.1 Model Setting â€£ 6 Algorithm-Guided Piecewise-Neural-Network Framework â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")), initialize the neural network architecture as follows:

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | QÎ¸â€‹(s)\displaystyle Q\_{\theta}(s) | ={fÎ¸0(0)â€‹(ğ¬featâ€‹(s)),sâˆˆ(s0,s1],fÎ¸1(1)â€‹(ğ¬featâ€‹(s)),sâˆˆ(s1,s2],â‹®fÎ¸K(K)â€‹(ğ¬featâ€‹(s)),sâˆˆ(sK,sK+1),\displaystyle= |  | (34) |

where fÎ¸(k)f^{(k)}\_{\theta} denotes the kk-th neural sub-network parameterized by Î¸k\theta\_{k}, the parameters of the integrated network QQ are denoted as Î¸={Î¸0,Î¸1,â€¦,Î¸K}\theta=\{\theta\_{0},\theta\_{1},\dots,\theta\_{K}\}. ğ¬featâ€‹(s)\mathbf{s}\_{\text{feat}}(s) represents the Fourier feature embedding of the scalar input ss defined as

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ¬featâ€‹(s)=[sinâ¡(2â€‹Ï€â€‹s),sinâ¡(4â€‹Ï€â€‹s),cosâ¡(2â€‹Ï€â€‹s),cosâ¡(4â€‹Ï€â€‹s)]âŠ¤âˆˆâ„4.\mathbf{s}\_{\text{feat}}(s)=[\,\sin(2\pi s),\;\sin(4\pi s),\;\cos(2\pi s),\;\cos(4\pi s)\,]^{\top}\in\mathbb{R}^{4}. |  | (35) |

2:Define a prior function as follows:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ï•â€‹(s)={Iâ€‹(QÏâ€‹(1âˆ’s)),sâˆˆ(s0,s1],Q0â€‹(s),sâˆˆ(s1,s2],â‹®Iâ€‹(QÏâ€‹(1âˆ’s)),sâˆˆ(sK,sK+1).\displaystyle\phi(s)= |  | (36) |

3:For each interval, update the sub-network output with the analytic prior term and activate the integrated network QÎ¸â€‹(â‹…)Q\_{\theta}(\cdot) as follows:

|  |  |  |
| --- | --- | --- |
|  | QÎ¸â€‹(s)â†QÎ¸â€‹(s)+Ï•â€‹(s),QÎ¸â€‹(s)â†ReLUâ€‹(QÎ¸â€‹(s)).Q\_{\theta}(s)\leftarrow Q\_{\theta}(s)+\phi(s),\;\;Q\_{\theta}(s)\leftarrow\text{ReLU}(Q\_{\theta}(s)). |  |

4:Define the objective function as the expected utility from Eq. ([39](https://arxiv.org/html/2512.00299v1#S6.E39 "In 6.1 Model Setting â€£ 6 Algorithm-Guided Piecewise-Neural-Network Framework â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")).
Since the integral cannot be computed analytically, we approximate it by uniformly sampling nn points siâˆˆ(0,1)s\_{i}\in(0,1) and compute

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„’objâ€‹(Î¸)=1nâ€‹âˆ‘i=1nUâ€‹(QÎ¸â€‹(si)).\mathcal{L}\_{\text{obj}}(\theta)=\frac{1}{n}\sum\_{i=1}^{n}U\big(Q\_{\theta}(s\_{i})\big). |  | (37) |

5:Take the budget constraint (C1C\_{1}) and SSD constraint (C2C\_{2}) as the penalty of the loss function. Define

|  |  |  |
| --- | --- | --- |
|  | C1=1nâ€‹âˆ‘i=1nQÎ¸â€‹(si)â€‹QÏâ€‹(1âˆ’si),C2=maxâ¡{0,maxk=1,â€¦,nâ¡[1nâ€‹âˆ‘i=1kQ0â€‹(si)âˆ’1nâ€‹âˆ‘i=1kQÎ¸â€‹(si)]}.C\_{1}=\frac{1}{n}\sum\_{i=1}^{n}Q\_{\theta}(s\_{i})Q\_{\rho}(1-s\_{i}),\;\;C\_{2}=\max\left\{0,\max\_{k=1,\dots,n}\left[\frac{1}{n}\sum\_{i=1}^{k}Q\_{0}(s\_{i})-\frac{1}{n}\sum\_{i=1}^{k}Q\_{\theta}(s\_{i})\right]\right\}. |  |

6:Add weights w1,w2w\_{1},w\_{2} to C1,C2C\_{1},C\_{2} and calculate the loss function as follows

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„’p1=w1â€‹(C1âˆ’xÂ¯)2,â„’p2=w2â‹…C2,\mathcal{L}\_{\text{p1}}=w\_{1}\,(C\_{1}-\bar{x})^{2},\;\;\mathcal{L}\_{\text{p2}}=w\_{2}\cdot C\_{2}, |  | (38) |

|  |  |  |
| --- | --- | --- |
|  | â„’totalâ€‹(Î¸)=âˆ’â„’objâ€‹(Î¸)+â„’p1+â„’p2.\mathcal{L}\_{\text{total}}(\theta)=-\mathcal{L}\_{\text{obj}}(\theta)+\mathcal{L}\_{\text{p1}}+\mathcal{L}\_{\text{p2}}. |  |

7:Next, start the training process.

8:Neural sub-networks {fÎ¸(k)}k=0K\{f\_{\theta}^{(k)}\}\_{k=0}^{K}, pricing kernel function QÏâ€‹(â‹…)Q\_{\rho}(\cdot), benchmark function Q0â€‹(â‹…)Q\_{0}(\cdot), utility function Uâ€‹(â‹…)U(\cdot), budget xÂ¯\bar{x}, sample size nn, learning rate Î·\eta, number of Adam steps AsA\_{s}, penalty weights w1,w2w\_{1},w\_{2}.

9:Trained network QÎ¸â€‹(s)Q\_{\theta}(s).

10:Sample siâˆˆ(0,1)s\_{i}\in(0,1), i=1,â€¦,ni=1,\dots,n.

11:Compute Fourier features ğ¬feat\mathbf{s}\_{\text{feat}} as Eq. ([35](https://arxiv.org/html/2512.00299v1#S6.E35 "In 1 â€£ Algorithm 2 â€£ 6.1 Model Setting â€£ 6 Algorithm-Guided Piecewise-Neural-Network Framework â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")).

12:for k=0k=0 to KK do

13:â€ƒâ€„Initialize each sub-network: fÎ¸k(k)â€‹(ğ¬featâ€‹(si))f\_{\theta\_{k}}^{(k)}(\mathbf{s}\_{\text{feat}}(s\_{i})).

14:â€ƒâ€„Compute analytic prior Ï•â€‹(si)\phi(s\_{i}) as Eq. ([36](https://arxiv.org/html/2512.00299v1#S6.E36 "In 2 â€£ Algorithm 2 â€£ 6.1 Model Setting â€£ 6 Algorithm-Guided Piecewise-Neural-Network Framework â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")).

15:end for

16:Merge the sub-networks to an integrated network QÎ¸â€‹(s)Q\_{\theta}(s) as Eq. ([34](https://arxiv.org/html/2512.00299v1#S6.E34 "In 1 â€£ Algorithm 2 â€£ 6.1 Model Setting â€£ 6 Algorithm-Guided Piecewise-Neural-Network Framework â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")).

17:Activate the network: QÎ¸â€‹(s)â†ReLUâ€‹(QÎ¸â€‹(s))Q\_{\theta}(s)\leftarrow\text{ReLU}(Q\_{\theta}(s)).

18:for i=0i=0 to AsA\_{s} do

19:â€ƒâ€„Compute the objective function: â„’objâ€‹(Î¸)=1nâ€‹âˆ‘i=1nUâ€‹(QÎ¸â€‹(si))\mathcal{L}\_{\text{obj}}(\theta)=\frac{1}{n}\sum\_{i=1}^{n}U\big(Q\_{\theta}(s\_{i})\big).

20:â€ƒâ€„Compute constraint penalties â„’p1\mathcal{L}\_{\text{p1}} and â„’p2\mathcal{L}\_{\text{p2}} as Eq. ([38](https://arxiv.org/html/2512.00299v1#S6.E38 "In 6 â€£ Algorithm 2 â€£ 6.1 Model Setting â€£ 6 Algorithm-Guided Piecewise-Neural-Network Framework â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")).

21:â€ƒâ€„Compute the total loss in current iteration: â„’totalâ€‹(Î¸)=âˆ’â„’objâ€‹(Î¸)+â„’p1+â„’p2\mathcal{L}\_{\text{total}}(\theta)=-\mathcal{L}\_{\text{obj}}(\theta)+\mathcal{L}\_{\text{p1}}+\mathcal{L}\_{\text{p2}}.

22:â€ƒâ€„Adam update: Î¸â†Î¸âˆ’Î·â€‹âˆ‡Î¸â„’totalâ€‹(Î¸)\theta\leftarrow\theta-\eta\nabla\_{\theta}\mathcal{L}\_{\text{total}}(\theta).

23:end for

24:return Trained network QÎ¸â€‹(s)Q\_{\theta}(s).

The optimization problem to be approximated by the neural network is formulated as follows:

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | maxQâˆˆğ’¬2â€‹(Q0)â€‹âˆ«01Uâ€‹(Qâ€‹(s))â€‹dsâ€‹s.t.Â â€‹âˆ«01Qâ€‹(s)â€‹QÏâ€‹(1âˆ’s)â€‹dsâ©½xÂ¯,\displaystyle\max\_{Q\in\mathcal{Q}\_{2}(Q\_{0})}\int\_{0}^{1}U(Q(s))\mathrm{d}s~~\text{s.t. }\int\_{0}^{1}Q(s)Q\_{\rho}(1-s)\mathrm{d}s\leqslant\overline{x}, |  | (39) |

which is identical to Problem ([15](https://arxiv.org/html/2512.00299v1#S4.E15 "In 4.1 SSD Problem under Non-concavity: Analytical Difficulty â€£ 4 SSD Problem â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")).

Based on Algorithm [1](https://arxiv.org/html/2512.00299v1#alg1 "Algorithm 1 â€£ 4.2 SSD Problem: Numerical Algorithm â€£ 4 SSD Problem â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network"), we obtain the suboptimal solution Qsubâ€‹(â‹…)Q\_{\text{sub}}(\cdot) as given in Eq. ([24](https://arxiv.org/html/2512.00299v1#S4.E24 "In 14 â€£ 4.2 SSD Problem: Numerical Algorithm â€£ 4 SSD Problem â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")), which exhibits a piecewise structure. Specifically, Qsubâ€‹(â‹…)Q\_{\text{sub}}(\cdot) can be expressed as

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | Qsubâ€‹(s)\displaystyle Q\_{\text{sub}}(s) | ={I(Î»(QÏ(1âˆ’s)âˆ’y(â‹…)),sâˆˆ(s0,s1],Q0â€‹(s),sâˆˆ(s1,s2],â‹®I(Î»(QÏ(1âˆ’s)âˆ’y(â‹…)),sâˆˆ(sK,sK+1),\displaystyle= |  | (40) |

where s0=0,sK+1=1.s\_{0}=0,s\_{K+1}=1.

Following this structure, we construct a set of neural sub-networks
{fÎ¸k(k)}k=0K\{f^{(k)}\_{\theta\_{k}}\}\_{k=0}^{K} to approximate each interval respectively. The main design considerations are summarized as follows:

1. a

   In Step 1, the piecewise formulation ensures that QÎ¸â€‹(s)Q\_{\theta}(s) inherits the partition interval of Qsubâ€‹(â‹…)Q\_{\text{sub}}(\cdot), enabling the network to better capture the structural behavior of the optimal solution. Specifically, each sub-network fÎ¸k(k)f^{(k)}\_{\theta\_{k}} is a fully connected feedforward network with 8 hidden layers, each consisting of 256 neurons with Tanh activations.
2. b

   In Step 2, we introduce an analytical prior term into each sub-network fÎ¸k(k)f\_{\theta\_{k}}^{(k)}. The design of Ï•â€‹(s)\phi(s) is inspired by the structure of Qsubâ€‹(â‹…)Q\_{\text{sub}}(\cdot), preserving essential information from the solution of AlgorithmÂ [1](https://arxiv.org/html/2512.00299v1#alg1 "Algorithm 1 â€£ 4.2 SSD Problem: Numerical Algorithm â€£ 4 SSD Problem â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") as a prior function and helps the network to capture the intrinsic information of the optimal solution.
3. c

   In Steps 5-6, we square the violation of the budget constraint to represent its penalty, ensuring the budget stays close to xÂ¯\overline{x}.
   For the SSD constraint, we penalize the maximal violation to strictly enforce its satisfaction. The relative importance of satisfying the constraint during training can be adjusted by tuning its associated weight w1,w2w\_{1},w\_{2}.
4. d

   In Steps 8-22, the approximation accuracy of the objective function and constraints can be improved by increasing the number of sampled points nn. The Adam optimizer is used to update the network parameters, and the number of Adam steps AsA\_{s} determines how many gradient-based updates are performed during training, thus controlling the convergence of the network.

### 6.2 Experimental Results

To validate the model, we first conduct experiments under conditions in which the optimal solution Qâˆ—â€‹(â‹…)Q^{\*}(\cdot) is available. The optimality is because Algorithm [1](https://arxiv.org/html/2512.00299v1#alg1 "Algorithm 1 â€£ 4.2 SSD Problem: Numerical Algorithm â€£ 4 SSD Problem â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") results numerically satisfy the characterization in Theorem [2](https://arxiv.org/html/2512.00299v1#Thmtheorem2 "Theorem 2 (Theorem 5.10 of Wang and Xia (2021)). â€£ 4.1 SSD Problem under Non-concavity: Analytical Difficulty â€£ 4 SSD Problem â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network"). We follow the setup in Section [5.4](https://arxiv.org/html/2512.00299v1#S5.SS4 "5.4 SSD Problem: Various Utilities and Benchmark Quantiles â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network"), which considers both the exponential utility and log utility under a linear SSD constraint quantile Q0Q\_{0} (Table [8](https://arxiv.org/html/2512.00299v1#S5.T8 "Table 8 â€£ 5.4 SSD Problem: Various Utilities and Benchmark Quantiles â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") (a)(c)).

We follow the steps in Algorithm [2](https://arxiv.org/html/2512.00299v1#alg2 "Algorithm 2 â€£ 6.1 Model Setting â€£ 6 Algorithm-Guided Piecewise-Neural-Network Framework â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") to get a trained neural network QÎ¸Q\_{\theta}, and the results are summarized in Table [10](https://arxiv.org/html/2512.00299v1#S6.T10 "Table 10 â€£ 6.2 Experimental Results â€£ 6 Algorithm-Guided Piecewise-Neural-Network Framework â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network").

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | utility | distribution of Q0Q\_{0} | optimal value | neural network value |
| (i) | exponential | uniform | -0.8965 | -0.8990 |
| (ii) | log | uniform | 1.4781 | 1.4686 |

Table 10: Objective value for neural network approximation.



|  |  |  |
| --- | --- | --- |
|  | Refer to captionRefer to caption\displaystyle\tiny\begin{matrix}\includegraphics[width=212.02846pt,height=204.63057pt]{nn\_graphs/nn\_exp\_Q.eps}&\includegraphics[width=212.02846pt,height=204.63057pt]{nn\_graphs/nn\_exp\_obj.eps}\end{matrix} |  |

|  |  |  |
| --- | --- | --- |
|  | Refer to captionRefer to caption\displaystyle\tiny\begin{matrix}\includegraphics[width=212.02846pt,height=204.63057pt]{nn\_graphs/nn\_log\_Q.eps}&\includegraphics[width=212.02846pt,height=204.63057pt]{nn\_graphs/nn\_log\_obj.eps}\end{matrix} |  |

Figure 6: Neural network approximation results.

Here are some illustrations of the neural network results:

1. 1.

   Figure [6](https://arxiv.org/html/2512.00299v1#S6.F6 "Figure 6 â€£ 6.2 Experimental Results â€£ 6 Algorithm-Guided Piecewise-Neural-Network Framework â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") (i)-(ii) Left: Compared with Figure [4](https://arxiv.org/html/2512.00299v1#S5.F4 "Figure 4 â€£ 5.4 SSD Problem: Various Utilities and Benchmark Quantiles â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") (a) and Figure [5](https://arxiv.org/html/2512.00299v1#S5.F5 "Figure 5 â€£ 5.4 SSD Problem: Various Utilities and Benchmark Quantiles â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") (d), the algorithm-guided piecewise-neural-network framework successfully preserves the structure features of the optimal solution, achieving a close match.
2. 2.

   Convergence behavior: The network typically converges within 100-2000 steps. As shown in Table [10](https://arxiv.org/html/2512.00299v1#S6.T10 "Table 10 â€£ 6.2 Experimental Results â€£ 6 Algorithm-Guided Piecewise-Neural-Network Framework â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network"), our algorithm achieves high numerical accuracy relative to the optimal solution, which is reflected in the objective value âˆ«01Uâ€‹(Qâ€‹(s))â€‹ds\int\_{0}^{1}U(Q(s))\mathrm{d}s in Problem ([39](https://arxiv.org/html/2512.00299v1#S6.E39 "In 6.1 Model Setting â€£ 6 Algorithm-Guided Piecewise-Neural-Network Framework â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")).
3. 3.

   Figure [6](https://arxiv.org/html/2512.00299v1#S6.F6 "Figure 6 â€£ 6.2 Experimental Results â€£ 6 Algorithm-Guided Piecewise-Neural-Network Framework â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") (i)-(ii) Right: The convergence speed depends on the number of sub-networks: the exponential case uses 2 sub-networks, while the log case uses 3, which also reflects the problem complexity. In simpler cases (Figure [6](https://arxiv.org/html/2512.00299v1#S6.F6 "Figure 6 â€£ 6.2 Experimental Results â€£ 6 Algorithm-Guided Piecewise-Neural-Network Framework â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") (i)), the convergence curves are smooth and rapid because the two constraints in Problem ([39](https://arxiv.org/html/2512.00299v1#S6.E39 "In 6.1 Model Setting â€£ 6 Algorithm-Guided Piecewise-Neural-Network Framework â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")) are quickly satisfied. In more complex cases (Figure [6](https://arxiv.org/html/2512.00299v1#S6.F6 "Figure 6 â€£ 6.2 Experimental Results â€£ 6 Algorithm-Guided Piecewise-Neural-Network Framework â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") (ii)), the network spends more time mitigating constraint penalties, resulting in slower convergence.

Here we come to the conclusion that our approach, first deriving a sub-optimal solution via the proposed algorithm and then leveraging it to guide the neural network design, effectively captures the essential properties of the optimal solution and provides stable, high-quality performance for the SSD problem.

Extending to non-concave utility functions introduces a substantially more challenging problem. The difficulty stems from several factors. First, the optimization is infinite-dimensional over the space of admissible allocation functions, meaning that the solution is a functional rather than a finite-dimensional vector. Second, in the presence of SSD constraint and budget constraint, the feasible set becomes highly restricted and non-convex, which further complicates convergence.

Remarkably, our algorithm-guided piecewise-neural-network framework remains stable under these challenging conditions and achieves rapid convergence. By leveraging sub-optimal solutions obtained from our proposed Algorithm [1](https://arxiv.org/html/2512.00299v1#alg1 "Algorithm 1 â€£ 4.2 SSD Problem: Numerical Algorithm â€£ 4 SSD Problem â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") as analytic priors and structuring the network in a piecewise manner, the framework effectively reduces the functional search space and guides the optimization toward regions that respect the SSD constraints. In contrast, a standard monolithic-neural-network (non-piecewise) framework exhibits substantially slower convergence, often requiring tens of times more training steps to reach convergence.

To illustrate this, we consider an S-shaped utility following the setup in Section [5.3](https://arxiv.org/html/2512.00299v1#S5.SS3 "5.3 SSD Problem: S-shaped Utility â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") (Table [4](https://arxiv.org/html/2512.00299v1#S5.T4 "Table 4 â€£ 5.3 SSD Problem: S-shaped Utility â€£ 5 Numerical Results â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") (b)). Solutions obtained with our Algorithm [2](https://arxiv.org/html/2512.00299v1#alg2 "Algorithm 2 â€£ 6.1 Model Setting â€£ 6 Algorithm-Guided Piecewise-Neural-Network Framework â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") are compared against those from a standard monolithic-neural-network framework using identical parameter settings.

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | network | neural network value | C1C\_{1} satisfied steps | C2C\_{2} satisfied steps |
| (i) | piecewise | 14.7531 | 83 | 10 |
| (ii) | standard | 12.0320 | 10950 | 4279 |

Table 11: Convergence step and constraint satisfied step.



|  |  |  |
| --- | --- | --- |
|  | Refer to captionRefer to caption\displaystyle\tiny\begin{matrix}\includegraphics[width=212.02846pt,height=198.03316pt]{nn\_graphs/convergence\_graph\_prior.eps}&\includegraphics[width=212.02846pt,height=198.03316pt]{nn\_graphs/prior\_penalty.eps}\end{matrix} |  |

|  |  |  |
| --- | --- | --- |
|  | Refer to captionRefer to caption\displaystyle\tiny\begin{matrix}\includegraphics[width=212.02846pt,height=198.03316pt]{nn\_graphs/convergence\_graph\_naive.eps}&\includegraphics[width=212.02846pt,height=194.72942pt]{nn\_graphs/naive\_penalty.eps}\end{matrix} |  |

Figure 7: Convergence behavior and penalty updates.

Here, we illustrate the convergence behavior of different neural network designs:

1. 1.

   Table [11](https://arxiv.org/html/2512.00299v1#S6.T11 "Table 11 â€£ 6.2 Experimental Results â€£ 6 Algorithm-Guided Piecewise-Neural-Network Framework â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network"): The neural network value corresponds to the objective value âˆ«01Uâ€‹(Qâ€‹(s))â€‹ds\int\_{0}^{1}U(Q(s))\,\mathrm{d}s in Problem ([39](https://arxiv.org/html/2512.00299v1#S6.E39 "In 6.1 Model Setting â€£ 6 Algorithm-Guided Piecewise-Neural-Network Framework â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network")). The table also reports the minimum number of steps required to satisfy the SSD and budget constraints for each network design.
2. 2.

   Figure [7](https://arxiv.org/html/2512.00299v1#S6.F7 "Figure 7 â€£ 6.2 Experimental Results â€£ 6 Algorithm-Guided Piecewise-Neural-Network Framework â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network"): Our algorithm-guided piecewise-neural-network framework reaches near-convergence in roughly 300 Adam steps, whereas a standard monolithic-neural-network requires approximately 10,000 steps to achieve comparable near-convergence. This dramatic acceleration highlights the efficiency of our approach. Furthermore, our piecewise framework achieves a higher sub-optimal objective value compared with the standard framework, indicating that the standard framework is prone to getting trapped in local optima, while our piecewise framework can avoid such traps and attain superior performance.
3. 3.

   Table [11](https://arxiv.org/html/2512.00299v1#S6.T11 "Table 11 â€£ 6.2 Experimental Results â€£ 6 Algorithm-Guided Piecewise-Neural-Network Framework â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") (i) vs. (ii): The significant speedup under the piecewise framework primarily stems from rapid satisfaction of the penalty terms. By incorporating analytic prior inspired from Algorithm [1](https://arxiv.org/html/2512.00299v1#alg1 "Algorithm 1 â€£ 4.2 SSD Problem: Numerical Algorithm â€£ 4 SSD Problem â€£ Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network") into the network design, our piecewise framework enforces the SSD and budget constraints more efficiently, allowing the neural network to converge quickly while maintaining better approximation of the sub-optimal solution.

In summary, our study demonstrates the effectiveness of the algorithm-guided piecewise-neural-network framework for solving the SSD problem.
For cases where an analytical optimal solution exists, our framework achieves results that closely match the optimal solution both numerically and structurally, indicating its high precision.
From the perspective of convergence, our framework significantly accelerates the optimization process and is capable of escaping local optima, thereby attaining superior objective value compared with the standard monolithic-neural-network framework.
These findings highlight the efficiency and robustness of our approach across different problem settings, and suggest strong potential for extension to more complex scenarios.

## 7 Conclusion

We study a utility maximization problem under stochastic dominance constraints. Starting from an S-shaped utility function, we derive the explicit optimal solution without a liquidation boundary under first-order stochastic dominance (FSD) constraints.
For the more challenging SSD problem, particularly under non-concavity, obtaining an explicit optimal solution is inherently difficult. Motivated by the structural properties of the analytical theorem in the concave case, we introduce a Poor-Performance-Region Algorithm (PPRA). This algorithm efficiently identifies the candidate structure that a potential optimal solution should satisfy and proves effective in the vast majority of cases. In addition, extensive numerical experiments illustrate how the algorithm operates, confirm its broad applicability, and demonstrate its capability to handle the few exceptional cases.

Building on the structural insights provided by the PPRA, we further recognize the potential of neural networks in tackling SSD problems under non-concavity. While neural networks offer strong approximation capabilities, their direct application often suffers from slow convergence and severe sensitivity to local optima induced by non-concavity. By leveraging the PPRAâ€™s ability to capture the essential structure of the optimal solution, we develop an algorithm-guided piecewise-neural-network framework that integrates these structural cues into the learning process. Compared with a standard neural-network approach, this framework achieves significantly faster convergence and effectively avoids undesirable local minima, thereby delivering consistently superior solution quality.

#### Acknowledgement

The authors are grateful to Jianming Xia and members of the research group on financial mathematics and risk management at The Chinese University of Hong Kong, Shenzhen for their insightful discussions and conversations.
Y. Liu acknowledges financial support from the National Natural Science Foundation of China (Grant No. 12401624), The Chinese University of Hong Kong (Shenzhen) University Development Fund (Grant No. UDF01003336) and Shenzhen Science and Technology Program (Grant No. RCBS20231211090814028, JCYJ20250604141203005, 2025TC0010) and is partly supported by the Guangdong Provincial Key Laboratory of Mathematical Foundations for Artificial Intelligence (Grant No. 2023B1212010001).

## References

* Barberis and Thaler (2003)
   Barberis, N., Thaler, R. (2003). A Survey of Behavioral Finance, in Handbook of the Economics of Finance: Vol. 1. Financial Markets and Asset Pricing, M. H. G. M. Constantinides, and R. Stulz, eds., Elsevier, Kidlington, 1053-1128.
* Carpenter (2000)
   Carpenter, J. N. (2000). Does option compensation increase managerial risk appetite? Journal of Finance, 55, 2311-2331.
* FÃ¶llmer and Schied (2016)
   FÃ¶llmer, H., Schied, A. (2016). *Stochastic Finance. An Introduction in Discrete Time*. Fourth Edition. Walter de Gruyter, Berlin.
* Ghossoub and Zhu (2025)
   Ghossoub, M., Zhu, M. B. (2025). Risk-constrained portfolio choice under rank-dependent utility. Finance and Stochastics, 29, 399-442.
* He and Kou (2018)

  He, X., Kou, S. (2018). Profit sharing in hedge funds. Mathematical Finance, 28, 50-81.
* He and Zhou (2011)
   He, X., Zhou, X. (2011). Portfolio choice under cumulative prospect theory: An analytical treatment. Management Science, 57, 315-331.
* Karatzas et al. (1987)
   Karatzas, I., Lehoczky, J. P., Shreve, S. E. (1987). Optimal portfolio and consumption decisions for a â€œsmall investorâ€ on a finite horizon. SIAM Journal on Control and Optimization, 25, 1557-1586.
* Karatzas and Shreve (1998)
   Karatzas, I., Shreve, S. E. (1998). Methods of Mathematical Finance. Springer, New York.
* Kahneman and Tversky (1979)
   Kahneman, D., Tversky, A. (1979). Prospect Theory: an analysis of decision under risk. Econometrica, 47, 263-291.
* Liang and Liu (2020)
   Liang, Z., Liu, Y. (2020). A classification approach to general S-shaped utility optimization with principalsâ€™ constraints.
  SIAM Journal on Control and Optimization, 58, 3734-3762.
* Liang and Liu (2024)
   Liang, Z., Liu, Y. (2024). An asymptotic approach to centrally planned portfolio selection. Advances in Applied Probability, 56, 757-784.
* Liang, Liu and Zhang (2025)
   Liang, Z., Liu, Y., Zhang, L. (2025). A framework of state-dependent utility optimization with general benchmarks. Finance and Stochastics, 29, 469-518.
* Merton (1969)
   Merton, R. C. (1969). Lifetime portfolio selection under uncertainty: The continuous-time case. The Review of Economics and Statistics, 51, 247-257.
* Rockafellar (1970)
   Rockafellar, R. T. (1970). Convex Analysis. Princeton University Press, 1st edition.
* Scarselli and Tsoi (1998)
   Scarselli, F., Tsoi, A. C. (1998). Universal approximation using feedforward neural networks: A survey of some existing methods, and some new results. Neural Networks, 11(1), 15-37.
* Tversky and Kahneman (1992)
  Tversky, A., Kahneman, D. (1992). Advances in prospect theory: cumulative representation of uncertainty. Journal of Risk and Uncertainty, 5, 297-323.
* Wang, Wei and Xia (2024)
   Wang, Y., Wei, J., Xia, J. (2024). Mean-Stochastic-Dominance portfolio selection in continuous time. SIAM Journal on Financial Mathematics, 15, SC80-SC90.
* Wang and Xia (2021)
   Wang, X., Xia, J. (2021). Expected utility maximization with stochastic dominance constraints in complete markets. SIAM Journal on Financial Mathematics, 12, 1054-1111.
* Wei (2018)
   Wei, P. (2018). Risk management with weighted VaR. Mathematical Finance, 28, 1020-1060.
* Xia and Zhou (2016)
   Xia, J., Zhou, X. (2016). Arrow-Debreu equilibria for rank-dependent utilities. Mathematical Finance, 26, 558-588.
* Xu (2016)
   Xu, Z. (2016). A note on the quantile formulation. Mathematical Finance, 26, 589-601.
* Zhang et al. (2018)
   Zhang, L., Wang, F., Sun, T., Xu, B. (2018). A constrained optimization method based on BP neural network. Neural Computing and Applications, 29(2), 413-421.