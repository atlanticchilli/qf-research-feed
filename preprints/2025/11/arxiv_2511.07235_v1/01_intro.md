---
authors:
- Erhan Bayraktar
- Qi Feng
- Zecheng Zhang
- Zhaoyu Zhang
doc_id: arxiv:2511.07235v1
family_id: arxiv:2511.07235
is_current: true
taxonomy:
  alpha_families: []
  asset_classes: []
  horizons: []
  themes: []
title: Deep Neural Operator Learning for Probabilistic Models
url_abs: http://arxiv.org/abs/2511.07235v1
url_html: https://arxiv.org/html/2511.07235v1
venue: arXiv q-fin
version: 1
year: 2025
---


Erhan
Bayraktar Â Â Â Qi Feng
Â Â Â  Zecheng ZhangÂ Â Â  Zhaoyu Zhang
Department of
Mathematics, University of Michigan, Ann Arbor, 48109; email: erhan@umich.edu. This author is supported in part the Susan M. Smith Professorship and in part by by the National Science Foundation under grant #DMS-2507940.Department of
Mathematics, Florida State University, Tallahassee, 32306; email: qfeng2@fsu.edu. This author is partially supported by the National Science Foundation under grant #DMS-2420029.Department of Applied Computational Mathematics and Statistics, University of Notre Dame, Notre Dame 46556; zzhang48@nd.edu. This author is partially supported by the Department of Energy DE-SC0025440 Department of
Mathematics, University of California, Los Angeles, CA, 90095;
email: zhaoyu@math.ucla.edu.

###### Abstract

We propose a deep neural-operator framework for a general class of probability models. Under global Lipschitz conditions on the operator over the entire Euclidean spaceâ€”and for a broad class of probabilistic modelsâ€”we establish a universal approximation theorem with explicit network-size bounds for the proposed architecture. The underlying stochastic processes are required only to satisfy integrability and general tail-probability conditions. We verify these assumptions for both European and American option-pricing problems within the forwardâ€“backward SDE (FBSDE) framework, which in turn covers a broad class of operators arising from parabolic PDEs, with or without free boundaries. Finally, we present a numerical example for a basket of American options, demonstrating that the learned model produces optimal stopping boundaries for new strike prices without retraining.

## 1 Introduction

Operator learning [[13](https://arxiv.org/html/2511.07235v1#bib.bib13), [32](https://arxiv.org/html/2511.07235v1#bib.bib32), [35](https://arxiv.org/html/2511.07235v1#bib.bib35), [31](https://arxiv.org/html/2511.07235v1#bib.bib31), [30](https://arxiv.org/html/2511.07235v1#bib.bib30), [42](https://arxiv.org/html/2511.07235v1#bib.bib42)] uses deep neural networks to approximate mappings between functions or function spaces, enabling efficient solutions to a wide range of computational science problems.
For instance, it can learn the mapping from the initial condition of a partial differential equation (PDE) to its corresponding solution.
Another example is to learn the mapping from fine-scale PDE solutions to coarse-scale ones [[21](https://arxiv.org/html/2511.07235v1#bib.bib21), [29](https://arxiv.org/html/2511.07235v1#bib.bib29)], effectively performing model upscaling.
A key advantage of operator learning is its ability to handle parametric PDEs.
For example, when the PDE initial condition is parameterized by free variables, the operator learning framework can learn the mapping between the space formed by all initial conditions and the space of their corresponding solution space.
Once trained, a deep neural operator (DNO) can instantly predict the solution for any new initial condition within the same function space.
Compared to the standard numerical solvers, operator learning offers much faster and more cost-efficient computations.
This approach has been widely applied to inverse PDE problems, where it is often integrated with standard numerical solversâ€”either providing an initial approximation refined by numerical methods or serving as a rapid surrogate model to accelerate the overall solution process.

Many popular deep neural operators (DNOs) have been proposed [[13](https://arxiv.org/html/2511.07235v1#bib.bib13), [32](https://arxiv.org/html/2511.07235v1#bib.bib32), [35](https://arxiv.org/html/2511.07235v1#bib.bib35), [31](https://arxiv.org/html/2511.07235v1#bib.bib31), [30](https://arxiv.org/html/2511.07235v1#bib.bib30), [42](https://arxiv.org/html/2511.07235v1#bib.bib42)], and operator learning has been successfully applied to solving practical problems [[36](https://arxiv.org/html/2511.07235v1#bib.bib36), [10](https://arxiv.org/html/2511.07235v1#bib.bib10)], making it an important machine learning framework for large-scale computational applications.
Theoretically, the first operator learning framework was proposed in the seminal works [[13](https://arxiv.org/html/2511.07235v1#bib.bib13), [12](https://arxiv.org/html/2511.07235v1#bib.bib12)], where the authors introduced a shallow universal approximation architecture for nonlinear operators. This foundational theory has directly inspired the design of several modern DNOs, such as DeepONet [[35](https://arxiv.org/html/2511.07235v1#bib.bib35)].
Later, operator learningâ€”viewed as a mapping between infinite-dimensional function spacesâ€”has been analyzed in [[9](https://arxiv.org/html/2511.07235v1#bib.bib9), [33](https://arxiv.org/html/2511.07235v1#bib.bib33)], where the approximation error was quantified with respect to the discretization size of the input function, network complexity, and related parameters.
In [[23](https://arxiv.org/html/2511.07235v1#bib.bib23), [24](https://arxiv.org/html/2511.07235v1#bib.bib24)], the authors generalized the framework of several neural operators, including the Fourier Neural Operator (FNO), and analyzed their universal approximation properties in shallow network settings, though without establishing convergence rates.
In [[26](https://arxiv.org/html/2511.07235v1#bib.bib26), [34](https://arxiv.org/html/2511.07235v1#bib.bib34)], the convergence rate of DeepONet was established for a class of PDE operators.
In [[27](https://arxiv.org/html/2511.07235v1#bib.bib27)], the author studied the lower bound of the convergence rate for PCA-Net, with potential generalizations to other DNOs, while an upper bound result for PCA-Net was given in [[25](https://arxiv.org/html/2511.07235v1#bib.bib25)].
Across the existing literature, establishing rigorous convergence rates for general operatorsâ€”without restricting to specific PDE formulationsâ€”remains one of the central theoretical challenges in operator learning.
One notable contribution in this direction is provided by [[34](https://arxiv.org/html/2511.07235v1#bib.bib34)], which unifies various formulations of neural operators and rigorously establishes convergence rates as the network depth and width increase, for general operators not tied to any specific PDE.
Specifically, the total number of trainable parameters to reach Îµ\displaystyle\varepsilon error in Lâˆ\displaystyle L^{\infty} norm is scaled as Îµâˆ’Îµâˆ’d2\displaystyle\varepsilon^{-\varepsilon^{-d\_{2}}}.

However, all the aforementioned studies address deterministic problems on bounded domains, without involving any stochastic processes or probabilistic models.
To the best of our knowledge, the neural operator approach has only recently been extended to Forwardâ€“Backward Stochastic Differential Equations (FBSDEs) [[17](https://arxiv.org/html/2511.07235v1#bib.bib17)], which can be applied to solve European-type option pricing problems, and to Dynamic Stackelberg Games [[1](https://arxiv.org/html/2511.07235v1#bib.bib1)]. In this paper, we develop a neural-operator framework under general Lipschitz conditions for broad classes of stochastic processes satisfying integrability and tail-probability assumptions. In particular, we employ our neural operator to address the American option pricing problem and its associated optimal stopping boundary problem. Recently, American option pricing and optimal stopping problems have been investigated using deep neural networks in [[39](https://arxiv.org/html/2511.07235v1#bib.bib39), [28](https://arxiv.org/html/2511.07235v1#bib.bib28), [22](https://arxiv.org/html/2511.07235v1#bib.bib22), [7](https://arxiv.org/html/2511.07235v1#bib.bib7), [8](https://arxiv.org/html/2511.07235v1#bib.bib8), [37](https://arxiv.org/html/2511.07235v1#bib.bib37), [5](https://arxiv.org/html/2511.07235v1#bib.bib5), [18](https://arxiv.org/html/2511.07235v1#bib.bib18), [6](https://arxiv.org/html/2511.07235v1#bib.bib6), [19](https://arxiv.org/html/2511.07235v1#bib.bib19), [20](https://arxiv.org/html/2511.07235v1#bib.bib20)]. Theoretical results on the continuity property of optimal stopping boundary have been investigated in [[40](https://arxiv.org/html/2511.07235v1#bib.bib40)], while a more general Stefan-type problem for partial differential equations (PDEs) with free boundaries has been studied in [[38](https://arxiv.org/html/2511.07235v1#bib.bib38)]. In these existing works, the methods are typically designed for a fixed terminal payoff function, requiring retraining of the network when the terminal function changes. In contrast, by adopting the operator learning perspective, our trained model can directly generate the optimal stopping boundary for a new terminal payoff function without retraining. Our general neural-operator approximation results encompass both European and American options in the FBSDE setting. Through the Feynmanâ€“Kac correspondence (and its optimal-stopping/variational-inequality version for American options), the same guarantees apply to the corresponding PDEs and free-boundary PDEs.

The paper is organized as follows. Section [2](https://arxiv.org/html/2511.07235v1#S2 "2 Assumptions â€£ Deep Neural Operator Learning for Probabilistic Models") states the standing assumptions on the underlying probabilistic models and the conditions imposed on the operators. Section [3](https://arxiv.org/html/2511.07235v1#S3 "3 Deep Operator Learning Architecture â€£ Deep Neural Operator Learning for Probabilistic Models") presents the deep neural operator architecture, including its construction and size bounds. Section [4](https://arxiv.org/html/2511.07235v1#S4 "4 Neural scaling of operator learning â€£ Deep Neural Operator Learning for Probabilistic Models") establishes a universal approximation theorem for functions, functionals, and operators under these assumptions. Section [5](https://arxiv.org/html/2511.07235v1#S5 "5 European Option Pricing Operator â€£ Deep Neural Operator Learning for Probabilistic Models") and Section [6](https://arxiv.org/html/2511.07235v1#S6 "6 Deep neural operator for American option pricing and PDE with free boundary â€£ Deep Neural Operator Learning for Probabilistic Models") verifies that European- and American-style option pricing problems within the FBSDE framework satisfy the assumptions; by the Feynmanâ€“Kac representation (and its variational-inequality form for optimal stopping), the theorem then applies to the associated parabolic PDEs, with or without free boundaries. Section [7](https://arxiv.org/html/2511.07235v1#S7 "7 Algorithm â€£ Deep Neural Operator Learning for Probabilistic Models") provides a numerical example for a basket of American options, demonstrating that the learned model produces optimal stopping boundaries for new strike prices without retraining.

## 2 Assumptions

Let
(Î©,â„±,{â„±t}tâˆˆ[0,T],â„™)\displaystyle(\Omega,\mathcal{F},\{\mathcal{F}\_{t}\}\_{t\in[0,T]},\mathbb{P})
be a filtered probability space satisfying the usual conditions and right-continuity. In this paper, we denote

|  |  |  |
| --- | --- | --- |
|  | X=(Xt)tâˆˆ[0,T]X=(X\_{t})\_{t\in[0,T]} |  |

such that X0=x\displaystyle X\_{0}=x, as an â„d1\displaystyle\mathbb{R}^{d\_{1}}-valued adapted process progressively measurable with respect to {â„±t}tâˆˆ[0,T]\displaystyle\{\mathcal{F}\_{t}\}\_{t\in[0,T]}.

###### Definition 2.1.

For xâˆˆâ„d\displaystyle x\in\mathbb{R}^{d}, |x|:=âˆ‘i=1dxi2\displaystyle|x|:=\sqrt{\sum\_{i=1}^{d}x\_{i}^{2}}, and â€–xâ€–âˆ:=max1â‰¤iâ‰¤dâ¡|xi|.\displaystyle\|x\|\_{\infty}:=\max\_{1\leq i\leq d}|x\_{i}|.

###### Assumption 1.

For any p>0\displaystyle p>0, there exists a constant Cp>0\displaystyle C\_{p}>0, such that

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â€‹[sup0â‰¤tâ‰¤T|Xt|p]â‰¤Cp.\displaystyle\displaystyle\mathbb{E}\left[\sup\_{0\leq t\leq T}|X\_{t}|^{p}\right]\leq C\_{p}. |  | (2.1) |

###### Assumption 2.

There exists a constant CT\displaystyle C\_{T} depending on time T\displaystyle T, and a constant c\displaystyle c such that for any r>0\displaystyle r>0,

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„™â€‹(suptâˆˆ[0,T]|Xtâˆ’x|â‰¥r)â‰¤expâ¡(âˆ’câ€‹rÎ±CT).\displaystyle\displaystyle\mathbb{P}\left(\sup\_{t\in[0,T]}|X\_{t}-x|\geq r\right)\leq\exp\left(-\frac{cr^{\alpha}}{C\_{T}}\right). |  | (2.2) |

###### Definition 2.2.

For any r>0\displaystyle r>0, we define

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î©r:={xâˆˆâ„d1:|x|â‰¤r}andÎ©rC:=â„d1âˆ–Î©r,\displaystyle\displaystyle\Omega\_{r}:=\{x\in\mathbb{R}^{d\_{1}}:|x|\leq r\}\quad\text{and}\quad\Omega\_{r}^{C}:=\mathbb{R}^{d\_{1}}\setminus\Omega\_{r}, |  | (2.3) |

where , and we define the cube correspondingly as follows

|  |  |  |
| --- | --- | --- |
|  | Qr:=[âˆ’r,r]d1={xâˆˆâ„d1:â€–xâ€–âˆâ‰¤r}.Q\_{r}:=[-r,r]^{d\_{1}}\;=\;\left\{x\in\mathbb{R}^{d\_{1}}:\|x\|\_{\infty}\leq r\right\}. |  |

###### Definition 2.3 (Input space ğ’¢\displaystyle\mathcal{G}).

Define the input space as below

|  |  |  |
| --- | --- | --- |
|  | ğ’¢:={g:â„d1â†’â„|g(Xt)Â is progressively measurable andÂ ğ”¼[sup0â‰¤sâ‰¤T|g(Xs)|2]<âˆ}.\mathcal{G}:=\left\{g:\mathbb{R}^{d\_{1}}\to\mathbb{R}\ \Big|\ g(X\_{t})\text{ is progressively measurable and }\mathbb{E}\!\left[\sup\_{0\leq s\leq T}|g(X\_{s})|^{2}\right]<\infty\right\}. |  |

The space ğ’¢\displaystyle\mathcal{G} is equipped with the norm

|  |  |  |
| --- | --- | --- |
|  | â€–gâ€–ğ’®2:=(ğ”¼â€‹[sup0â‰¤sâ‰¤T|gâ€‹(Xs)|2])1/2.\|g\|\_{\mathcal{S}^{2}}:=\left(\mathbb{E}\!\left[\sup\_{0\leq s\leq T}|g(X\_{s})|^{2}\right]\right)^{1/2}. |  |

###### Definition 2.4 (Output space ğ’°\displaystyle\mathcal{U}).

Define the output space as below

|  |  |  |
| --- | --- | --- |
|  | ğ’°:={u:[0,T]Ã—â„d2â†’â„|u(t,Xt)Â is progressively measurable andÂ ğ”¼[sup0â‰¤sâ‰¤T|u(s,Xs)|2]<âˆ},\mathcal{U}:=\left\{u:[0,T]\times\mathbb{R}^{d\_{2}}\to\mathbb{R}\ \Big|\ u(t,X\_{t})\text{ is progressively measurable and }\mathbb{E}\!\left[\sup\_{0\leq s\leq T}|u(s,X\_{s})|^{2}\right]<\infty\right\}, |  |

with the norm

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–uâ€–ğ’®2:=(ğ”¼â€‹[sup0â‰¤sâ‰¤T|uâ€‹(s,Xs)|2])1/2.\displaystyle\displaystyle\|u\|\_{\mathcal{S}^{2}}:=\left(\mathbb{E}\!\left[\sup\_{0\leq s\leq T}|u(s,X\_{s})|^{2}\right]\right)^{1/2}. |  | (2.4) |

###### Remark 2.5.

We denote X=(Xt)tâˆˆ[0,T]\displaystyle X=(X\_{t})\_{t\in[0,T]} for a generic stochastic process. Depending on the context, XâˆˆDomainâ€‹(ğ’¢)\displaystyle X\in\text{Domain}(\mathcal{G}) implies Xtâˆˆâ„d1\displaystyle X\_{t}\in\mathbb{R}^{d\_{1}}, while XâˆˆDomainâ€‹(ğ’°)\displaystyle X\in\text{Domain}(\mathcal{U}) implies Xtâˆˆâ„d2\displaystyle X\_{t}\in\mathbb{R}^{d\_{2}}.

We impose the following polynomial growth condition and Lipschitz condition.

###### Assumption 3.

For any function gâˆˆğ’¢\displaystyle g\in\mathcal{G}, and xâˆˆâ„d1\displaystyle x\in\mathbb{R}^{d\_{1}}, there there exists a constant Cg\displaystyle C\_{g}, such that

|  |  |  |  |
| --- | --- | --- | --- |
|  | gâ€‹(x)â‰¤Cgâ€‹(1+|x|p),forp>0.\displaystyle\displaystyle g(x)\leq C\_{g}(1+|x|^{p}),\quad\text{for}\quad p>0. |  | (2.5) |

###### Assumption 4.

Any function gâˆˆğ’¢\displaystyle g\in\mathcal{G} is Lipschitz with a Lipschitz constant no more than Lg>0\displaystyle L\_{g}>0:

|  |  |  |
| --- | --- | --- |
|  | |gâ€‹(x1)âˆ’gâ€‹(x2)|â‰¤Lgâ€‹|x1âˆ’x2|2|g(x\_{1})-g(x\_{2})|\leq L\_{g}|x\_{1}-x\_{2}|\_{2} |  |

for any x1,x2âˆˆâ„d1\displaystyle x\_{1},x\_{2}\in\mathbb{R}^{d\_{1}}.

###### Assumption 5.

Any function uâˆˆğ’°\displaystyle u\in\mathcal{U} is Lipschitz with a Lipschitz constant no more than Lu>0\displaystyle L\_{u}>0:

|  |  |  |
| --- | --- | --- |
|  | |uâ€‹(x1)âˆ’uâ€‹(x2)|â‰¤Luâ€‹|x1âˆ’x2|,|u(x\_{1})-u(x\_{2})|\leq L\_{u}|x\_{1}-x\_{2}|, |  |

for any x1,x2âˆˆâ„d2\displaystyle x\_{1},x\_{2}\in\mathbb{R}^{d\_{2}}.

###### Assumption 6.

Assume the operator

|  |  |  |
| --- | --- | --- |
|  | Î“:ğ’¢âŸ¶ğ’°,gâŸ¼u=Î“â€‹(g),\Gamma:\mathcal{G}\longrightarrow\mathcal{U},\qquad g\longmapsto u=\Gamma(g), |  |

from ğ’¢\displaystyle\mathcal{G} to ğ’°\displaystyle\mathcal{U} is Lipschitz if : there exists LÎ“\displaystyle L\_{\Gamma} such that for any g1,g2âˆˆğ’¢\displaystyle g\_{1},g\_{2}\in\mathcal{G}, we have

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[sup0â‰¤tâ‰¤T|Î“â€‹(g1)â€‹(Xt)âˆ’Î“â€‹(g2)â€‹(Xt)|2]â‰¤LÎ“2â€‹ğ”¼â€‹[sup0â‰¤tâ‰¤T|g1â€‹(Xt)âˆ’g2â€‹(Xt)|2],âˆ€g1,g2âˆˆğ’¢.\mathbb{E}\!\left[\sup\_{0\leq t\leq T}|\Gamma(g\_{1})(X\_{t})-\Gamma(g\_{2})(X\_{t})|^{2}\right]\leq L\_{\Gamma}^{2}\,\mathbb{E}\!\left[\sup\_{0\leq t\leq T}|g\_{1}(X\_{t})-g\_{2}(X\_{t})|^{2}\right],\ \forall g\_{1},g\_{2}\in\mathcal{G}. |  |

or equivalently,

|  |  |  |
| --- | --- | --- |
|  | â€–Î“â€‹(g1)âˆ’Î“â€‹(g2)â€–S2â‰¤LÎ“â€‹â€–g1âˆ’g2â€–S2.\|\Gamma(g\_{1})-\Gamma(g\_{2})\|\_{S^{2}}\leq L\_{\Gamma}\|g\_{1}-g\_{2}\|\_{S^{2}}. |  |

As mentioned in Remark [2.5](https://arxiv.org/html/2511.07235v1#S2.Thmtheorem5 "Remark 2.5. â€£ 2 Assumptions â€£ Deep Neural Operator Learning for Probabilistic Models"), the process Xâˆˆ\displaystyle X\in Domain(ğ’°\displaystyle\mathcal{U}), i.e. Xtâˆˆâ„d2\displaystyle X\_{t}\in\mathbb{R}^{d\_{2}}.

###### Definition 2.6 (Lipschitz functional).

We say a functional ğ–¥:ğ’±â†’â„\displaystyle\mathsf{F}:\mathcal{V}\rightarrow\mathbb{R}, where ğ’±\displaystyle\mathcal{V} could either be the input space ğ’¢\displaystyle\mathcal{G} or the output space ğ’°\displaystyle\mathcal{U} is Lipschitz with Lipschitz constant Lğ–¥\displaystyle L\_{\mathsf{F}} if

|  |  |  |
| --- | --- | --- |
|  | |ğ–¥â€‹(v1)âˆ’ğ–¥â€‹(v2)|â‰¤Lğ–¥â€‹â€–v1âˆ’v2â€–S2,âˆ€v1,v2âˆˆğ’±.|\mathsf{F}(v\_{1})-\mathsf{F}(v\_{2})|\leq L\_{\mathsf{F}}\|v\_{1}-v\_{2}\|\_{S^{2}},\ \forall v\_{1},v\_{2}\in\mathcal{V}. |  |

###### Lemma 2.7 (Theorem 13.7(ii) of [[41](https://arxiv.org/html/2511.07235v1#bib.bib41)]).

Let {Î©k}k=1M\displaystyle\{\Omega\_{k}\}\_{k=1}^{M} be an open cover of a compact smooth manifold â„³\displaystyle\mathcal{M} . There exists a Câˆ\displaystyle C^{\infty} partition of unity {Ï‰k}k=1M\displaystyle\{\omega\_{k}\}\_{k=1}^{M} that subordinates to {Î©k}k=1M\displaystyle\{\Omega\_{k}\}\_{k=1}^{M} such that sâ€‹uâ€‹pâ€‹pâ€‹oâ€‹râ€‹tâ€‹(Ï‰k)âŠ‚Î©k\displaystyle support(\omega\_{k})\subset\Omega\_{k} for any k\displaystyle k.

## 3 Deep Operator Learning Architecture

Operator learning aims to approximate mappings between infinite-dimensional function spaces, distinguishing itself from traditional neural networks, which approximate functions directly. Specifically, given a nonlinear operator Î“\displaystyle\Gamma that maps an input function g\displaystyle g to an output function Î“â€‹(g)\displaystyle\Gamma(g), the objective is to learn Î“\displaystyle\Gamma using a neural network architecture.
In this work, the nonlinear operator Î“\displaystyle\Gamma represents a pricing operator, and the goal is to approximate it via a neural network-based approach.
We will use the notations used in [[33](https://arxiv.org/html/2511.07235v1#bib.bib33)], we will define the fully connected ReLU neural network.
we define a feedforward ReLU network q:â„d1â†’â„\displaystyle{q}:\mathbb{R}^{d\_{1}}\rightarrow\mathbb{R} as

|  |  |  |  |
| --- | --- | --- | --- |
|  | qâ€‹(ğ±)=WLâ‹…ReLUâ€‹(WLâˆ’1â€‹â‹¯â€‹ReLUâ€‹(W1â€‹x+b1)+â‹¯+bLâˆ’1)+bL,\displaystyle\displaystyle q(\mathbf{x})=W\_{L}\cdot\text{ReLU}\left(W\_{L-1}\cdots\text{ReLU}(W\_{1}x+b\_{1})+\cdots+b\_{L-1}\right)+b\_{L}, |  | (3.1) |

where Wl\displaystyle W\_{l}â€™s are weight matrices, bl\displaystyle b\_{l}â€™s are bias vectors, ReLUâ€‹(a)=maxâ¡{a,0}\displaystyle\text{ReLU}(a)=\max\{a,0\} is the rectified linear unit activation (ReLU) applied element-wise, and Î©\displaystyle\Omega is the domain.
We define the network class â„±NN:â„d1â†’â„d2:\displaystyle\mathcal{F}\_{\rm NN}:\mathbb{R}^{d\_{1}}\rightarrow\mathbb{R}^{d\_{2}}:

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„±NN(d1,d2,â„’,ğ”­,K,Îº,R)={\displaystyle\displaystyle\mathcal{F}\_{\rm NN}(d\_{1},d\_{2},\mathcal{L},\mathfrak{p},K,\kappa,R)=\{ | [q1,q2,â€¦,qd2]âŠºâˆˆâ„d2:Â for eachÂ â€‹k=1,â€¦,d2,\displaystyle\displaystyle[q\_{1},q\_{2},...,q\_{d\_{2}}]^{\intercal}\in\mathbb{R}^{d\_{2}}:\mbox{ for each }k=1,...,d\_{2}, |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | qk:â„d1â†’â„â€‹Â is in the form of ([3.1](https://arxiv.org/html/2511.07235v1#S3.E1 "In 3 Deep Operator Learning Architecture â€£ Deep Neural Operator Learning for Probabilistic Models")) withÂ â€‹â„’â€‹Â layers, width bounded byÂ â€‹ğ”­,\displaystyle\displaystyle q\_{k}:\mathbb{R}^{d\_{1}}\rightarrow\mathbb{R}\mbox{ is in the form of (\ref{eqn\_relu\_net}) with }\mathcal{L}\mbox{ layers, width bounded by }\mathfrak{p}, |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | âˆ¥qkâˆ¥Lâˆâ‰¤R,âˆ¥Wlâˆ¥âˆ,âˆâ‰¤Îº,âˆ¥blâˆ¥âˆâ‰¤Îº,âˆ‘l=1Lâˆ¥Wlâˆ¥0+âˆ¥blâˆ¥0â‰¤K,âˆ€l},\displaystyle\displaystyle\|q\_{k}\|\_{L^{\infty}}\leq R,\ \|W\_{l}\|\_{\infty,\infty}\leq\kappa,\ \|b\_{l}\|\_{\infty}\leq\kappa,\ \sum\_{l=1}^{L}\|W\_{l}\|\_{0}+\|b\_{l}\|\_{0}\leq K,\ \forall l\}, |  | (3.2) |

where
â€–qâ€–Lâˆâ€‹(Î©)=supğ±âˆˆÎ©|qâ€‹(ğ±)|,â€–Wlâ€–âˆ,âˆ=maxi,jâ¡|Wi,j|,â€–bâ€–âˆ=maxiâ¡|bi|\displaystyle\|q\|\_{L^{\infty}(\Omega)}=\sup\limits\_{\mathbf{x}\in\Omega}|q(\mathbf{x})|,\ \|W\_{l}\|\_{\infty,\infty}=\max\limits\_{i,j}|W\_{i,j}|,\ \|b\|\_{\infty}=\max\limits\_{i}|b\_{i}|,
and âˆ¥â‹…âˆ¥0\displaystyle\|\cdot\|\_{0} denotes the number of nonzero elements of its argument.
The network class above has input dimension d1\displaystyle d\_{1}, output dimension d2\displaystyle d\_{2}, â„’\displaystyle\mathcal{L} layers, width ğ”­\displaystyle\mathfrak{p}, and the number of nonzero parameters no larger than K\displaystyle K.
All parameters are bounded by Îº\displaystyle\kappa, and each element in the output is bounded by R\displaystyle R.

The objective of operator learning is to construct an operator network Î“pâ€‹[â‹…;Î¸]\displaystyle\Gamma\_{p}[\cdot;\theta] that approximates Î“\displaystyle\Gamma.
One approximation structure [[13](https://arxiv.org/html/2511.07235v1#bib.bib13), [35](https://arxiv.org/html/2511.07235v1#bib.bib35)] which is widely adopted in designing DNOs is the following.
To better demonstrate the idea of the approximation, we denote y\displaystyle y as the independent variable of the output function of the operator Î“\displaystyle\Gamma, and denote Î“â€‹(g;Î¸)â€‹(y)\displaystyle\Gamma(g;\theta)(y) as the neural operator approximation to Î“\displaystyle\Gamma, we then have the approximation,

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î“â€‹(g)â€‹(y)â‰ˆÎ“â€‹(g;Î¸)â€‹(y):=âˆ‘k=1N1a~kâ€‹(ğ’ˆ;Î¸^)â€‹q~kâ€‹(y;Î¸~)\Gamma(g)(y)\approx\Gamma(g;\theta)(y):=\sum\_{k=1}^{N\_{1}}\widetilde{a}\_{k}(\bm{g};\widehat{\theta})\widetilde{q}\_{k}(y;\widetilde{\theta}) |  | (3.3) |

where g\displaystyle g is the input function with a discretization denoted as ğ’ˆ\displaystyle\bm{g}, Î“â€‹(g)\displaystyle\Gamma(g) is the output function depends on y\displaystyle y, and Î¸={Î¸^,Î¸~}\displaystyle\theta=\{\widehat{\theta},\widetilde{\theta}\} represents the set of trainable parameters of the operator network consisted of a~k\displaystyle\widetilde{a}\_{k} and q~k\displaystyle\widetilde{q}\_{k}.
The framework was first proposed in [[13](https://arxiv.org/html/2511.07235v1#bib.bib13), [12](https://arxiv.org/html/2511.07235v1#bib.bib12)] as a two-layer universal approximation scheme for nonlinear operators.
It was later extended computationally to deep neural network architectures in [[35](https://arxiv.org/html/2511.07235v1#bib.bib35)], and was finally rigorously analyzed in terms of error convergence and generalization in [[34](https://arxiv.org/html/2511.07235v1#bib.bib34)].
Following the terminology widely adopted in recent literature, we use the notation introduced in [[35](https://arxiv.org/html/2511.07235v1#bib.bib35)] to name the substructure of the network.
Specifically, the architecture consists of two components:

* â€¢

  Branch network: a~â€‹(ğ ;Î¸^)=(a~1â€‹(ğ ;Î¸^1),â€¦,a~N1â€‹(ğ ;Î¸^N1))T\displaystyle\widetilde{a}(\mathbf{g};\widehat{\theta})=(\widetilde{a}\_{1}(\mathbf{g};\widehat{\theta}\_{1}),\dots,\widetilde{a}\_{N\_{1}}(\mathbf{g};\widehat{\theta}\_{N\_{1}}))^{T} encode the input function g\displaystyle g and the operator Î“\displaystyle\Gamma.
  Each component a~k:â„N2â†’â„\displaystyle\widetilde{a}\_{k}:\mathbb{R}^{{N\_{2}}}\rightarrow\mathbb{R} is implemented as a sum of fully connected neural networks.
  Specifically, a~k=âˆ‘n=1Hanâ€‹b~n\displaystyle\widetilde{a}\_{k}=\sum\_{n=1}^{H}a\_{n}\widetilde{b}\_{n}, where N2\displaystyle N\_{2} is the size of the discretization ğ \displaystyle\mathbf{g} , H\displaystyle H is the number of basis represented as a network b~n\displaystyle\widetilde{b}\_{n} in a neural network class â„±2=â„±NNâ€‹(N2,1,â„’2,ğ”­2,K2,Îº2)\displaystyle\mathcal{F}\_{2}=\mathcal{F}\_{\rm NN}(N\_{2},1,\mathcal{L}\_{2},\mathfrak{p}\_{2},K\_{2},\kappa\_{2}), an\displaystyle a\_{n} are constants.
  The size of the entire network Î²\displaystyle\beta (containing all b~k\displaystyle\widetilde{b}\_{k}) is specified in Theorem [4.5](https://arxiv.org/html/2511.07235v1#S4.Thmtheorem5 "Theorem 4.5. â€£ 4.3 Operator Approximation â€£ 4 Neural scaling of operator learning â€£ Deep Neural Operator Learning for Probabilistic Models").
* â€¢

  Trunk network: q~â€‹(y;Î¸~)=(q~1,â€¦,q~N1)\displaystyle\widetilde{q}(y;\widetilde{\theta})=(\widetilde{q}\_{1},...,\widetilde{q}\_{N\_{1}}).
  Here each q~k:â„d2â†’â„\displaystyle\widetilde{q}\_{k}:\mathbb{R}^{d\_{2}}\rightarrow\mathbb{R} is a network in class â„±1=â„±NNâ€‹(d2,1,â„’1,ğ”­1,K1,Îº1)\displaystyle\mathcal{F}\_{1}=\mathcal{F}\_{\rm NN}(d\_{2},1,\mathcal{L}\_{1},\mathfrak{p}\_{1},K\_{1},\kappa\_{1}) with size to be specified in Theorem [4.5](https://arxiv.org/html/2511.07235v1#S4.Thmtheorem5 "Theorem 4.5. â€£ 4.3 Operator Approximation â€£ 4 Neural scaling of operator learning â€£ Deep Neural Operator Learning for Probabilistic Models").

The following lemma from [[44](https://arxiv.org/html/2511.07235v1#bib.bib44)][Proposition 3] shows that a function of the product can be approximated by a network with arbitrary accuracy.

###### Lemma 3.1.

Given M>0\displaystyle M>0 and Îµ>0\displaystyle\varepsilon>0, there is a ReLU network Ã—~:â„2â†’â„\displaystyle\widetilde{\times}:\mathbb{R}^{2}\rightarrow\mathbb{R} in â„±NNâ€‹(2,1,â„’,ğ”­,K,Îº,R)\displaystyle\mathcal{F}\_{\rm NN}(2,1,\mathcal{L},\mathfrak{p},K,\kappa,R) such that for any |x|â‰¤M,|y|â‰¤M\displaystyle|x|\leq M,|y|\leq M, we have

|  |  |  |
| --- | --- | --- |
|  | |Ã—~â€‹(x,y)âˆ’xâ€‹y|<Îµ.|\widetilde{\times}(x,y)-xy|<\varepsilon. |  |

The network architecture has

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„’=Oâ€‹(logâ¡Îµâˆ’1),ğ”­=6,K=Oâ€‹(logâ¡Îµâˆ’1),Îº=Oâ€‹(Îµâˆ’1),R=M2.\displaystyle\displaystyle\mathcal{L}=O(\log\varepsilon^{-1}),\ \mathfrak{p}=6,\ K=O(\log\varepsilon^{-1}),\ \kappa=O(\varepsilon^{-1}),\ R=M^{2}. |  | (3.4) |

The constant hidden in O\displaystyle O depends on M\displaystyle M.

## 4 Neural scaling of operator learning

### 4.1 Function Approximation

We will first prove the function approximation and establish the convergence rate.
The results will be used in proving the operator approximation Theorem [4.5](https://arxiv.org/html/2511.07235v1#S4.Thmtheorem5 "Theorem 4.5. â€£ 4.3 Operator Approximation â€£ 4 Neural scaling of operator learning â€£ Deep Neural Operator Learning for Probabilistic Models").

###### Theorem 4.1.

Assume Assumptions [1](https://arxiv.org/html/2511.07235v1#Thmassumption1 "Assumption 1. â€£ 2 Assumptions â€£ Deep Neural Operator Learning for Probabilistic Models"), [2](https://arxiv.org/html/2511.07235v1#Thmassumption2 "Assumption 2. â€£ 2 Assumptions â€£ Deep Neural Operator Learning for Probabilistic Models"), [3](https://arxiv.org/html/2511.07235v1#Thmassumption3 "Assumption 3. â€£ 2 Assumptions â€£ Deep Neural Operator Learning for Probabilistic Models"), [4](https://arxiv.org/html/2511.07235v1#Thmassumption4 "Assumption 4. â€£ 2 Assumptions â€£ Deep Neural Operator Learning for Probabilistic Models") hold.
For any Îµ>0\displaystyle\varepsilon>0, set r=âŒˆâˆ’2â€‹CTcâ€‹logâ¡Îµ4â€‹C0âŒ‰1Î±\displaystyle r=\left\lceil-\frac{2C\_{T}}{c}\log\frac{\varepsilon}{4C\_{0}}\right\rceil^{\frac{1}{\alpha}} in Assumption [2](https://arxiv.org/html/2511.07235v1#Thmassumption2 "Assumption 2. â€£ 2 Assumptions â€£ Deep Neural Operator Learning for Probabilistic Models"),
and set N=Câ€‹d1â€‹Îµâˆ’1/2â€‹(logâ¡(Îµâˆ’1))1Î±\displaystyle N=C\sqrt{d\_{1}}\varepsilon^{-1/2}(\log(\varepsilon^{-1}))^{\frac{1}{\alpha}}, with constant C\displaystyle C depends on CT,C0,c,Lg\displaystyle C\_{T},C\_{0},c,L\_{g}.
Let {ğœk}k=1Nd1\displaystyle\{\mathbf{c}\_{k}\}\_{k=1}^{N^{d\_{1}}} be a uniform grid on Qr\displaystyle Q\_{r} (covering Î©r\displaystyle\Omega\_{r}) with spacing 2â€‹r/N\displaystyle 2r/N along each dimension.
There exists a network architecture â„±NNâ€‹(d1,1,â„’,ğ”­,K,Îº,R)\displaystyle\mathcal{F}\_{\rm NN}(d\_{1},1,\mathcal{L},\mathfrak{p},K,\kappa,R) and networks {q~k}k=1Nd1\displaystyle\{\widetilde{q}\_{k}\}\_{k=1}^{N^{d\_{1}}} with q~kâˆˆâ„±NNâ€‹(d1,1,â„’,ğ”­,K,Îº,R)\displaystyle\widetilde{q}\_{k}\in\mathcal{F}\_{\rm NN}(d\_{1},1,\mathcal{L},\mathfrak{p},K,\kappa,R) for k=1,â€¦,Nd1\displaystyle k=1,...,N^{d\_{1}}, such that for any gâˆˆğ’¢\displaystyle g\in\mathcal{G}, we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â€‹[sup0â‰¤tâ‰¤T|gâ€‹(Xt)âˆ’âˆ‘k=1Nd1gâ€‹(ğœk)â€‹q~kâ€‹(Xt)|2]â‰¤Îµ.\displaystyle\displaystyle\mathbb{E}\Big[\sup\_{0\leq t\leq T}\Big|g(X\_{t})-\sum\_{k=1}^{N^{d\_{1}}}g(\mathbf{c}\_{k})\widetilde{q}\_{k}(X\_{t})\Big|^{2}\Big]\leq\varepsilon. |  | (4.1) |

Such a network architecture has

|  |  |  |
| --- | --- | --- |
|  | â„’=Oâ€‹(d12â€‹logâ¡d1+d12+d12â€‹logâ¡(Îµâˆ’1)+d12+d1â€‹pÎ±â€‹logâ¡logâ¡(Îµâˆ’1)),ğ”­=Oâ€‹(1),\displaystyle\displaystyle{\color[rgb]{0,0,0}\mathcal{L}=O\left(d\_{1}^{2}\log d\_{1}+\frac{d\_{1}^{2}+d\_{1}}{2}\log(\varepsilon^{-1})+\frac{d\_{1}^{2}+d\_{1}p}{\alpha}\log\log(\varepsilon^{-1})\right)},\mathfrak{p}=O(1), |  |
|  |  |  |
| --- | --- | --- |
|  | K=Oâ€‹(d12â€‹logâ¡d1+d12+d12â€‹logâ¡(Îµâˆ’1)+d12+d1â€‹pÎ±â€‹logâ¡logâ¡(Îµâˆ’1)),\displaystyle\displaystyle K=O\left(d\_{1}^{2}\log d\_{1}+\frac{d\_{1}^{2}+d\_{1}}{2}\log(\varepsilon^{-1})+\frac{d\_{1}^{2}+d\_{1}p}{\alpha}\log\log(\varepsilon^{-1})\right), |  |
|  |  |  |
| --- | --- | --- |
|  | Îº=Oâ€‹(d1âˆ’d12â€‹Îµâˆ’d1+12â€‹(logâ¡(Îµâˆ’1))d1+pÎ±),R=1.\displaystyle\displaystyle{\color[rgb]{0,0,0}\kappa=O(d\_{1}^{-\frac{d\_{1}}{2}}\varepsilon^{-\frac{d\_{1}+1}{2}}(\log(\varepsilon^{-1}))^{\frac{d\_{1}+p}{\alpha}})},R=1. |  |

Here, the network order is determined by the constants CT,C0,c,Î±,Lg\displaystyle C\_{T},C\_{0},c,\alpha,L\_{g} specified in Assumptions [1](https://arxiv.org/html/2511.07235v1#Thmassumption1 "Assumption 1. â€£ 2 Assumptions â€£ Deep Neural Operator Learning for Probabilistic Models"), [2](https://arxiv.org/html/2511.07235v1#Thmassumption2 "Assumption 2. â€£ 2 Assumptions â€£ Deep Neural Operator Learning for Probabilistic Models"), [3](https://arxiv.org/html/2511.07235v1#Thmassumption3 "Assumption 3. â€£ 2 Assumptions â€£ Deep Neural Operator Learning for Probabilistic Models"), and [4](https://arxiv.org/html/2511.07235v1#Thmassumption4 "Assumption 4. â€£ 2 Assumptions â€£ Deep Neural Operator Learning for Probabilistic Models"), which concern the function g\displaystyle g and process {Xt}0â‰¤tâ‰¤T\displaystyle\{X\_{t}\}\_{0\leq t\leq T}. In particular, we denote C0\displaystyle C\_{0} as the upper bound of ğ”¼â€‹[sup0â‰¤tâ‰¤T|1+|Xt|p|4]1/2\displaystyle\mathbb{E}\Big[\sup\_{0\leq t\leq T}\Big|1+|X\_{t}|^{p}\Big|^{4}\Big]^{1/2}.

###### Proof.

Recall Î©r\displaystyle\Omega\_{r} and Î©rC\displaystyle\Omega\_{r}^{C} from the Definition [2.2](https://arxiv.org/html/2511.07235v1#S2.Thmtheorem2 "Definition 2.2. â€£ 2 Assumptions â€£ Deep Neural Operator Learning for Probabilistic Models"). Without loss of generality, we assume that the origin 0âˆˆÎ©r\displaystyle 0\in\Omega\_{r}. We then decompose gâ€‹(Xt)=gâ€‹(Xt)â€‹ğŸÎ©r+gâ€‹(Xt)â€‹ğŸÎ©rC\displaystyle g(X\_{t})=g(X\_{t})\mathbf{1}\_{\Omega\_{r}}+g(X\_{t})\mathbf{1}\_{\Omega\_{r}^{C}}
For the compact domain Î©râŠ‚Qr\displaystyle\Omega\_{r}\subset Q\_{r}, we apply a partition to Qr\displaystyle Q\_{r} covered by Nd1\displaystyle N^{d\_{1}} subcubes for some N\displaystyle N to be specified later. We first approximate gâ€‹(Xt)â€‹ğŸÎ©r\displaystyle g(X\_{t})\mathbf{1}\_{\Omega\_{r}} on each cube by a constant function and then assemble them together to get an approximation of {gâ€‹(Xt)}0â‰¤tâ‰¤T\displaystyle\{g(X\_{t})\}\_{0\leq t\leq T} on Î©r\displaystyle\Omega\_{r}. Denote the centers of the subcubes by {ğœk}k=1Nd1\displaystyle\{\mathbf{c}\_{k}\}\_{k=1}^{N^{d\_{1}}} with ğœk=[ck,1,ck,2,â€¦,ck,d1]âŠº\displaystyle\mathbf{c}\_{k}=[c\_{k,1},c\_{k,2},...,c\_{k,d\_{1}}]^{\intercal}.
Let {ğœk}k=1Nd1\displaystyle\{\mathbf{c}\_{k}\}\_{k=1}^{N^{d\_{1}}} be a uniform grid on Qr\displaystyle Q\_{r} (covering Î©r\displaystyle\Omega\_{r}), so that each ğœkâˆˆ{âˆ’r,âˆ’r+2â€‹rNâˆ’1,â€¦,r}d1\displaystyle\mathbf{c}\_{k}\in\left\{-r,-r+\frac{2r}{N-1},...,r\right\}^{d\_{1}} for each k\displaystyle k.
Define

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ïˆâ€‹(a)={1,|a|<1,0,|a|>2,2âˆ’|a|,1â‰¤|a|â‰¤2,\displaystyle\displaystyle\psi(a)=\begin{cases}1,|a|<1,\\ 0,|a|>2,\\ 2-|a|,1\leq|a|\leq 2,\end{cases} |  | (4.2) |

with aâˆˆâ„\displaystyle a\in\mathbb{R}, and

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ï•ğœkâ€‹(ğ±)=âˆj=1d1Ïˆâ€‹(3â€‹(Nâˆ’1)2â€‹râ€‹(xjâˆ’ck,j)),ğ±âˆˆQr.\displaystyle\displaystyle\phi\_{\mathbf{c}\_{k}}(\mathbf{x})=\prod\_{j=1}^{d\_{1}}\psi\left(\frac{3(N-1)}{2r}(x\_{j}-c\_{k,j})\right),\quad\mathbf{x}\in Q\_{r}. |  | (4.3) |

In this definition, we have suppâ€‹(Ï•ğœk)={ğ±:â€–ğ±âˆ’ğœkâ€–âˆâ‰¤4â€‹r3â€‹(Nâˆ’1)}âŠ‚{ğ±:â€–ğ±âˆ’ğœkâ€–âˆâ‰¤2â€‹r(Nâˆ’1)}\displaystyle\mathrm{supp\,}(\phi\_{\mathbf{c}\_{k}})=\left\{\mathbf{x}:\|\mathbf{x}-\mathbf{c}\_{k}\|\_{\infty}\leq\frac{4r}{3(N-1)}\right\}\subset\left\{\mathbf{x}:\|\mathbf{x}-\mathbf{c}\_{k}\|\_{\infty}\leq\frac{2r}{(N-1)}\right\} and for the constraint space Î©r\displaystyle\Omega\_{r}, we have

|  |  |  |
| --- | --- | --- |
|  | â€–Ï•ğœkâ€–Lâˆâ€‹(Qr)=1,âˆ‘k=1Nd1Ï•ğœk=1.\|\phi\_{\mathbf{c}\_{k}}\|\_{L^{\infty}(Q\_{r})}=1,\quad\sum\_{k=1}^{N^{d\_{1}}}\phi\_{\mathbf{c}\_{k}}=1. |  |

For any gâ€‹(Xt)\displaystyle g(X\_{t}) with XtâˆˆÎ©r\displaystyle X\_{t}\in\Omega\_{r}, we construct a piecewise constant approximation as below,

|  |  |  |
| --- | --- | --- |
|  | gÂ¯â€‹(Xt)=âˆ‘k=1Nd1gâ€‹(ğœk)â€‹Ï•ğœkâ€‹(Xt),XtâˆˆÎ©r.\bar{g}(X\_{t})=\sum\_{k=1}^{N^{d\_{1}}}g(\mathbf{c}\_{k})\phi\_{\mathbf{c}\_{k}}(X\_{t}),\quad X\_{t}\in\Omega\_{r}. |  |

Based on the decomposition of the domain â„d1=Î©râˆªÎ©rC\displaystyle\mathbb{R}^{d\_{1}}=\Omega\_{r}\cup\Omega\_{r}^{C}, for any Tâ‰¥0\displaystyle T\geq 0, we have

|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ğ”¼â€‹[sup0â‰¤tâ‰¤T|gâ€‹(Xt)âˆ’gÂ¯â€‹(Xt)|2]\displaystyle\displaystyle\mathbb{E}\Big[\sup\_{0\leq t\leq T}\Big|g(X\_{t})-\bar{g}(X\_{t})\Big|^{2}\Big] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | =\displaystyle\displaystyle= | ğ”¼â€‹[sup0â‰¤tâ‰¤T|gâ€‹(Xt)|2â€‹ğŸÎ©rCâ€‹(Xt)]âŸâ„1+ğ”¼â€‹[sup0â‰¤tâ‰¤T|gâ€‹(Xt)âˆ’gÂ¯â€‹(Xt)|2â€‹ğŸÎ©râ€‹(Xt)]âŸâ„2.\displaystyle\displaystyle\underbrace{\mathbb{E}\Big[\sup\_{0\leq t\leq T}\Big|g(X\_{t})\Big|^{2}\mathbf{1}\_{\Omega\_{r}^{C}}(X\_{t})\Big]}\_{\mathcal{I}\_{1}}+\underbrace{\mathbb{E}\Big[\sup\_{0\leq t\leq T}\Big|g(X\_{t})-\bar{g}(X\_{t})\Big|^{2}\mathbf{1}\_{\Omega\_{r}}(X\_{t})\Big]}\_{\mathcal{I}\_{2}}. |  |

For the first term â„1\displaystyle\mathcal{I}\_{1}, applying the polynomial growth Assumption [3](https://arxiv.org/html/2511.07235v1#Thmassumption3 "Assumption 3. â€£ 2 Assumptions â€£ Deep Neural Operator Learning for Probabilistic Models") for function g\displaystyle g and uniform bound Assumption [1](https://arxiv.org/html/2511.07235v1#Thmassumption1 "Assumption 1. â€£ 2 Assumptions â€£ Deep Neural Operator Learning for Probabilistic Models"), and Cauchy-Schwartz inequality, we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„1\displaystyle\displaystyle\mathcal{I}\_{1} | =ğ”¼â€‹[sup0â‰¤tâ‰¤T|gâ€‹(Xt)|2â€‹ğŸÎ©rCâ€‹(Xt)]\displaystyle\displaystyle=\mathbb{E}\Big[\sup\_{0\leq t\leq T}\Big|g(X\_{t})\Big|^{2}\mathbf{1}\_{\Omega\_{r}^{C}}(X\_{t})\Big] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤ğ”¼â€‹[sup0â‰¤tâ‰¤T|gâ€‹(Xt)|4]1/2â€‹ğ”¼â€‹[sup0â‰¤tâ‰¤T(ğŸÎ©rCâ€‹(Xt))2]1/2\displaystyle\displaystyle\leq\mathbb{E}\Big[\sup\_{0\leq t\leq T}\Big|g(X\_{t})\Big|^{4}\Big]^{1/2}\mathbb{E}\Big[\sup\_{0\leq t\leq T}(\mathbf{1}\_{\Omega\_{r}^{C}}(X\_{t}))^{2}\Big]^{1/2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤ğ”¼â€‹[sup0â‰¤tâ‰¤T|1+â€–Xtâ€–p|4]1/2â€‹(â„™â€‹(sup0â‰¤tâ‰¤T|Xt|â‰¥r))1/2\displaystyle\displaystyle\leq\mathbb{E}\Big[\sup\_{0\leq t\leq T}\Big|1+\|X\_{t}\|^{p}\Big|^{4}\Big]^{1/2}\Big(\mathbb{P}\Big(\sup\_{0\leq t\leq T}|X\_{t}|\geq r\Big)\Big)^{1/2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤C0â€‹(â„™â€‹(sup0â‰¤tâ‰¤T|Xt|â‰¥r))1/2.\displaystyle\displaystyle\leq C\_{0}\Big(\mathbb{P}\Big(\sup\_{0\leq t\leq T}|X\_{t}|\geq r\Big)\Big)^{1/2}. |  |

According to the tail bound Assumption [2](https://arxiv.org/html/2511.07235v1#Thmassumption2 "Assumption 2. â€£ 2 Assumptions â€£ Deep Neural Operator Learning for Probabilistic Models"), by taking the initial point X0\displaystyle X\_{0} as the center of the domain, and selecting
 r=âŒˆâˆ’2â€‹CTcâ€‹logâ¡Îµ4â€‹C0âŒ‰1Î±\displaystyle r=\left\lceil-\frac{2C\_{T}}{c}\log\frac{\varepsilon}{4C\_{0}}\right\rceil^{\frac{1}{\alpha}},
we get the following bound

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | â„1\displaystyle\displaystyle\mathcal{I}\_{1} | â‰¤C0â€‹eâˆ’câ€‹rÎ±2â€‹CTâ‰¤Îµ4,\displaystyle\displaystyle\leq C\_{0}e^{-\frac{cr^{\alpha}}{2C\_{T}}}\leq\frac{\varepsilon}{4}, |  | (4.4) |

where C0\displaystyle C\_{0} denotes the upper bound of ğ”¼â€‹[sup0â‰¤tâ‰¤T|1+â€–Xtâ€–p|4]1/2\displaystyle\mathbb{E}\Big[\sup\_{0\leq t\leq T}\Big|1+\|X\_{t}\|^{p}\Big|^{4}\Big]^{1/2}, which follows from Assumption [1](https://arxiv.org/html/2511.07235v1#Thmassumption1 "Assumption 1. â€£ 2 Assumptions â€£ Deep Neural Operator Learning for Probabilistic Models"). 
For the second term â„2\displaystyle\mathcal{I}\_{2}, by applying the partition of unity property âˆ‘k=1Nd1Ï•ğœk=1\displaystyle\sum\_{k=1}^{N^{d\_{1}}}\phi\_{\mathbf{c}\_{k}}=1, we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„2=\displaystyle\displaystyle\mathcal{I}\_{2}= | ğ”¼â€‹[sup0â‰¤tâ‰¤T|âˆ‘k=1Nd1[gâ€‹(Xt)âˆ’gâ€‹(ğœk)]â€‹Ï•ğœkâ€‹(Xt)|2â€‹ğŸÎ©râ€‹(Xt)]\displaystyle\displaystyle\mathbb{E}\Big[\sup\_{0\leq t\leq T}\Big|\sum\_{k=1}^{N^{d\_{1}}}[g(X\_{t})-g(\mathbf{c}\_{k})]\phi\_{\mathbf{c}\_{k}}(X\_{t})\Big|^{2}\mathbf{1}\_{\Omega\_{r}}(X\_{t})\Big] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤\displaystyle\displaystyle\leq | ğ”¼â€‹[sup0â‰¤tâ‰¤T(âˆ‘k=1Nd1|gâ€‹(Xt)âˆ’gâ€‹(ğœk)|â€‹|Ï•ğœkâ€‹(Xt)|)2â€‹ğŸÎ©râ€‹(Xt)]\displaystyle\displaystyle\mathbb{E}\Big[\sup\_{0\leq t\leq T}\Big(\sum\_{k=1}^{N^{d\_{1}}}|g(X\_{t})-g(\mathbf{c}\_{k})||\phi\_{\mathbf{c}\_{k}}(X\_{t})|\Big)^{2}\mathbf{1}\_{\Omega\_{r}}(X\_{t})\Big] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | =\displaystyle\displaystyle= | ğ”¼â€‹[sup0â‰¤tâ‰¤T(âˆ‘k:â€–ğœkâˆ’Xtâ€–âˆâ‰¤2â€‹r(Nâˆ’1)|gâ€‹(Xt)âˆ’gâ€‹(ğœk)|â€‹Ï•ğœkâ€‹(Xt))2â€‹ğŸÎ©râ€‹(Xt)],\displaystyle\displaystyle\mathbb{E}\Big[\sup\_{0\leq t\leq T}\Big(\sum\_{k:\|\mathbf{c}\_{k}-X\_{t}\|\_{\infty}\leq\frac{2r}{(N-1)}}|g(X\_{t})-g(\mathbf{c}\_{k})|\phi\_{\mathbf{c}\_{k}}(X\_{t})\Big)^{2}\mathbf{1}\_{\Omega\_{r}}(X\_{t})\Big], |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤\displaystyle\displaystyle\leq | ğ”¼â€‹[sup0â‰¤tâ‰¤T(maxk:â€–ğœkâˆ’Xtâ€–âˆâ‰¤2â€‹r(Nâˆ’1)â¡|gâ€‹(Xt)âˆ’gâ€‹(ğœk)|â€‹(âˆ‘k:â€–ğœkâˆ’Xtâ€–âˆâ‰¤2â€‹r(Nâˆ’1)Ï•ğœkâ€‹(Xt)))2â€‹ğŸÎ©râ€‹(Xt)]\displaystyle\displaystyle\mathbb{E}\Big[\sup\_{0\leq t\leq T}\Big(\max\_{k:\|\mathbf{c}\_{k}-X\_{t}\|\_{\infty}\leq\frac{2r}{(N-1)}}|g(X\_{t})-g(\mathbf{c}\_{k})|\Big(\sum\_{k:\|\mathbf{c}\_{k}-X\_{t}\|\_{\infty}\leq\frac{2r}{(N-1)}}\phi\_{\mathbf{c}\_{k}}(X\_{t})\Big)\Big)^{2}\mathbf{1}\_{\Omega\_{r}}(X\_{t})\Big] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤\displaystyle\displaystyle\leq | ğ”¼â€‹[sup0â‰¤tâ‰¤Tmaxk:â€–ğœkâˆ’Xtâ€–âˆâ‰¤2â€‹r(Nâˆ’1)â¡|gâ€‹(Xt)âˆ’gâ€‹(ğœk)|2â€‹ğŸÎ©râ€‹(Xt)]\displaystyle\displaystyle\mathbb{E}\Big[\sup\_{0\leq t\leq T}\max\_{k:\|\mathbf{c}\_{k}-X\_{t}\|\_{\infty}\leq\frac{2r}{(N-1)}}|g(X\_{t})-g(\mathbf{c}\_{k})|^{2}\mathbf{1}\_{\Omega\_{r}}(X\_{t})\Big] |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | â‰¤\displaystyle\displaystyle\leq | (2â€‹râ€‹Lg)2â€‹d1(Nâˆ’1)2,\displaystyle\displaystyle\frac{(2rL\_{g})^{2}d\_{1}}{(N-1)^{2}}, |  | (4.5) |

where we use the Lipschitz assumption of g\displaystyle g in the last inequality and the uniform bound given {Xt}0â‰¤tâ‰¤TâˆˆÎ©r\displaystyle\{X\_{t}\}\_{0\leq t\leq T}\in\Omega\_{r}.
Let N=âŒˆ4â€‹d1â€‹log1/Î±â¡(Îµâˆ’1)â€‹LgÎµâŒ‰+1\displaystyle N=\left\lceil\frac{4\sqrt{d\_{1}}\log^{1/\alpha}(\varepsilon^{-1})L\_{g}}{\sqrt{\varepsilon}}\right\rceil+1, we get

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„2â‰¤Îµ/4.\displaystyle\displaystyle\mathcal{I}\_{2}\leq\varepsilon/4. |  | (4.6) |

Combining the estimates ([4.4](https://arxiv.org/html/2511.07235v1#S4.E4 "In 4.1 Function Approximation â€£ 4 Neural scaling of operator learning â€£ Deep Neural Operator Learning for Probabilistic Models")) and ([4.5](https://arxiv.org/html/2511.07235v1#S4.E5 "In 4.1 Function Approximation â€£ 4 Neural scaling of operator learning â€£ Deep Neural Operator Learning for Probabilistic Models")), we get

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â€‹[sup0â‰¤tâ‰¤T|gâ€‹(Xt)âˆ’gÂ¯â€‹(Xt)|2]â‰¤Îµ2.\displaystyle\displaystyle\mathbb{E}\Big[\sup\_{0\leq t\leq T}\Big|g(X\_{t})-\bar{g}(X\_{t})\Big|^{2}\Big]\leq\frac{\varepsilon}{2}. |  | (4.7) |

We then show that Ï•ğœk\displaystyle\phi\_{\mathbf{c}\_{k}} can be approximated by a network with arbitrary accuracy on the compact domain Î©r\displaystyle\Omega\_{r} with a fixed parameter r\displaystyle r. Notice that for a compact domain Î©r\displaystyle\Omega\_{r}, this type of approximation has already been established in [[34](https://arxiv.org/html/2511.07235v1#bib.bib34)]. Since a different norm is employed here, we provide the proof for completeness.
Note that Ï•ğœk\displaystyle\phi\_{\mathbf{c}\_{k}} is the product of d1\displaystyle d\_{1} functions, each of which is piecewise linear and can be realized by the constant depth ReLU networks.
Let Ã—~\displaystyle\widetilde{\times} be the network defined in Lemma [3.1](https://arxiv.org/html/2511.07235v1#S3.Thmtheorem1 "Lemma 3.1. â€£ 3 Deep Operator Learning Architecture â€£ Deep Neural Operator Learning for Probabilistic Models") with accuracy Î´>0\displaystyle\delta>0.
For any ğ±âˆˆQr\displaystyle\mathbf{x}\in Q\_{r}, we approximate Ï•ğœk\displaystyle\phi\_{\mathbf{c}\_{k}} with q~k\displaystyle\widetilde{q}\_{k} defined as follows,

|  |  |  |
| --- | --- | --- |
|  | q~kâ€‹(ğ±)=Ã—~â€‹(Ïˆâ€‹(3â€‹(Nâˆ’1)2â€‹râ€‹(x1âˆ’ck,1)),Ã—~â€‹(Ïˆâ€‹(3â€‹(Nâˆ’1)2â€‹râ€‹(x2âˆ’ck,2)),â‹¯)).\displaystyle\displaystyle\widetilde{q}\_{k}(\mathbf{x})=\widetilde{\times}\left(\psi\left(\frac{3(N-1)}{2r}(x\_{1}-c\_{k,1})\right),\widetilde{\times}\left(\psi\left(\frac{3(N-1)}{2r}(x\_{2}-c\_{k,2})\right),\cdot\cdot\cdot\right)\right). |  |

For each k\displaystyle k, define q~kâˆˆâ„±NNâ€‹(d1,1,â„’,ğ”­,K,Îº,R)\displaystyle\widetilde{q}\_{k}\in\mathcal{F}\_{\rm NN}(d\_{1},1,\mathcal{L},\mathfrak{p},K,\kappa,R) with sizes to be specified later.
For any XtâˆˆÎ©r\displaystyle X\_{t}\in\Omega\_{r} which is equivalent to considering Xtâ‹…ğŸÎ©r\displaystyle X\_{t}\cdot\mathbf{1}\_{\Omega\_{r}}, we may simply take ğ±âˆˆÎ©r\displaystyle\mathbf{x}\in\Omega\_{r}. That is, by viewing ğ±\displaystyle\mathbf{x} as an arbitrary point in the domain Î©r\displaystyle\Omega\_{r}, we have

|  |  |  |  |
| --- | --- | --- | --- |
|  |  | |q~kâ€‹(ğ±)âˆ’Ï•ğœkâ€‹(ğ±)|\displaystyle\displaystyle|\widetilde{q}\_{k}(\mathbf{x})-\phi\_{\mathbf{c}\_{k}}(\mathbf{x})| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤\displaystyle\displaystyle\leq | |Ã—~â€‹(Ïˆâ€‹(3â€‹(Nâˆ’1)2â€‹râ€‹(x1âˆ’ck,1)),Ã—~â€‹(Ïˆâ€‹(3â€‹(Nâˆ’1)2â€‹râ€‹(x2âˆ’ck,2)),â‹¯))âˆ’Ï•ğœkâ€‹(ğ±)|\displaystyle\displaystyle\left|\widetilde{\times}\left(\psi\left(\frac{3(N-1)}{2r}(x\_{1}-c\_{k,1})\right),\widetilde{\times}\left(\psi\left(\frac{3(N-1)}{2r}(x\_{2}-c\_{k,2})\right),\cdot\cdot\cdot\right)\right)-\phi\_{\mathbf{c}\_{k}}(\mathbf{x})\right| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤\displaystyle\displaystyle\leq | |Ã—~(Ïˆ(3â€‹(Nâˆ’1)2â€‹r(x1âˆ’ck,1)),Ã—~(Ïˆ(3â€‹(Nâˆ’1)2â€‹r(x2âˆ’ck,2)),â‹¯))\displaystyle\displaystyle\bigg|\widetilde{\times}\left(\psi\left(\frac{3(N-1)}{2r}(x\_{1}-c\_{k,1})\right),\widetilde{\times}\left(\psi\left(\frac{3(N-1)}{2r}(x\_{2}-c\_{k,2})\right),\cdot\cdot\cdot\right)\right) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | âˆ’Ïˆ(3â€‹(Nâˆ’1)2â€‹r(x1âˆ’ck,1))Ã—~(Ïˆ(3â€‹(Nâˆ’1)2â€‹r(x2âˆ’ck,2)),â‹¯)|\displaystyle\displaystyle-\psi\left(\frac{3(N-1)}{2r}(x\_{1}-c\_{k,1})\right)\widetilde{\times}\left(\psi\left(\frac{3(N-1)}{2r}(x\_{2}-c\_{k,2})\right),\cdot\cdot\cdot\right)\bigg| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +|Ïˆâ€‹(3â€‹(Nâˆ’1)2â€‹râ€‹(x1âˆ’ck,1))â€‹Ã—~â€‹(Ïˆâ€‹(3â€‹(Nâˆ’1)2â€‹râ€‹(x2âˆ’ck,2)),â‹¯)âˆ’Ï•ğœkâ€‹(ğ±)|\displaystyle\displaystyle+\bigg|\psi\left(\frac{3(N-1)}{2r}(x\_{1}-c\_{k,1})\right)\widetilde{\times}\left(\psi\left(\frac{3(N-1)}{2r}(x\_{2}-c\_{k,2})\right),\cdot\cdot\cdot\right)-\phi\_{\mathbf{c}\_{k}}(\mathbf{x})\bigg| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤\displaystyle\displaystyle\leq | Î´+â„°2,\displaystyle\displaystyle\delta+\mathcal{E}\_{2}, |  |

where

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„°2\displaystyle\displaystyle\mathcal{E}\_{2} | =|Ïˆâ€‹(3â€‹(Nâˆ’1)2â€‹râ€‹(x1âˆ’ck,1))â€‹Ã—~â€‹(Ïˆâ€‹(3â€‹(Nâˆ’1)2â€‹râ€‹(x2âˆ’ck,2)),â‹¯)âˆ’Ï•ğœkâ€‹(ğ±)|\displaystyle\displaystyle=\bigg|\psi\left(\frac{3(N-1)}{2r}(x\_{1}-c\_{k,1})\right)\widetilde{\times}\left(\psi\left(\frac{3(N-1)}{2r}(x\_{2}-c\_{k,2})\right),\cdot\cdot\cdot\right)-\phi\_{\mathbf{c}\_{k}}(\mathbf{x})\bigg| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =|Ïˆâ€‹(3â€‹(Nâˆ’1)2â€‹râ€‹(x1âˆ’ck,1))|â€‹|Ã—~â€‹(Ïˆâ€‹(3â€‹(Nâˆ’1)2â€‹râ€‹(x2âˆ’ck,2)),â‹¯)âˆ’âˆj=2d1Ïˆâ€‹(3â€‹(Nâˆ’1)2â€‹râ€‹(xjâˆ’ck,j))|\displaystyle\displaystyle=\bigg|\psi\left(\frac{3(N-1)}{2r}(x\_{1}-c\_{k,1})\right)\bigg|\bigg|\widetilde{\times}\left(\psi\left(\frac{3(N-1)}{2r}(x\_{2}-c\_{k,2})\right),\cdot\cdot\cdot\right)-\prod\_{j=2}^{d\_{1}}\psi\left(\frac{3(N-1)}{2r}(x\_{j}-c\_{k,j})\right)\bigg| |  |

Repeat this process to estimate â„°2,â„°3,â€¦,â„°d1+1\displaystyle\mathcal{E}\_{2},\mathcal{E}\_{3},...,\mathcal{E}\_{d\_{1}+1}, where â„°d1+1=âˆk=1d1Ïˆâ€‹(3â€‹(Nâˆ’1)2â€‹Î³1â€‹(x2âˆ’ck,2))âˆ’Ï•ğœk=0\displaystyle\mathcal{E}\_{d\_{1}+1}=\prod\limits\_{k=1}^{d\_{1}}\psi\left(\frac{3(N-1)}{2\gamma\_{1}}(x\_{2}-c\_{k,2})\right)-\phi\_{\mathbf{c}\_{k}}=0.
This implies that â€–Ï•ğœkâˆ’q~kâ€–Lâˆâ€‹(Î©r)â‰¤d1â€‹Î´\displaystyle\|\phi\_{\mathbf{c}\_{k}}-\widetilde{q}\_{k}\|\_{L^{\infty}(\Omega\_{r})}\leq d\_{1}\delta.
Thus, we have

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[sup0â‰¤tâ‰¤T|Ï•ğœkâ€‹(Xt)âˆ’q~kâ€‹(Xt)|2â€‹ğŸÎ©râ€‹(Xt)]â‰¤d12â€‹Î´2.\displaystyle\displaystyle\mathbb{E}\left[\sup\_{0\leq t\leq T}|\phi\_{\mathbf{c}\_{k}}(X\_{t})-\widetilde{q}\_{k}(X\_{t})|^{2}\mathbf{1}\_{\Omega\_{r}}(X\_{t})\right]\leq d^{2}\_{1}\delta^{2}. |  |

Applying Cauchyâ€“Schwarz inequality and the Assumption [3](https://arxiv.org/html/2511.07235v1#Thmassumption3 "Assumption 3. â€£ 2 Assumptions â€£ Deep Neural Operator Learning for Probabilistic Models"), we have

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | ğ”¼â€‹[sup0â‰¤tâ‰¤T|âˆ‘k=1Nd1gâ€‹(ğœk)â€‹q~kâ€‹(Xt)âˆ’gÂ¯â€‹(Xt)|2â€‹ğŸÎ©râ€‹(Xt)]\displaystyle\displaystyle\mathbb{E}\left[\sup\_{0\leq t\leq T}\left|\sum\_{k=1}^{N^{d\_{1}}}g(\mathbf{c}\_{k})\widetilde{q}\_{k}(X\_{t})-\bar{g}(X\_{t})\right|^{2}\mathbf{1}\_{\Omega\_{r}}(X\_{t})\right] |  | (4.8) |
|  |  |  |  |
| --- | --- | --- | --- |
|  | =\displaystyle\displaystyle= | ğ”¼â€‹[sup0â‰¤tâ‰¤T|âˆ‘k=1Nd1gâ€‹(ğœk)â€‹q~kâ€‹(Xt)âˆ’âˆ‘k=1Nd1gâ€‹(ğœk)â€‹Ï•ğœkâ€‹(Xt)|2â€‹ğŸÎ©râ€‹(Xt)]\displaystyle\displaystyle\mathbb{E}\left[\sup\_{0\leq t\leq T}\left|\sum\_{k=1}^{N^{d\_{1}}}g(\mathbf{c}\_{k})\widetilde{q}\_{k}(X\_{t})-\sum\_{k=1}^{N^{d\_{1}}}g(\mathbf{c}\_{k})\phi\_{\mathbf{c}\_{k}}(X\_{t})\right|^{2}\mathbf{1}\_{\Omega\_{r}}(X\_{t})\right] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤\displaystyle\displaystyle\leq | ğ”¼â€‹[âˆ‘k=1Nd1|gâ€‹(ğœk)|2â€‹sup0â‰¤tâ‰¤Tâˆ‘k=1Nd1|q~kâ€‹(Xt)âˆ’Ï•ğœkâ€‹(Xt)|2â€‹ğŸÎ©râ€‹(Xt)]\displaystyle\displaystyle\mathbb{E}\left[\sum\_{k=1}^{N^{d\_{1}}}|g(\mathbf{c}\_{k})|^{2}\sup\_{0\leq t\leq T}\sum\_{k=1}^{N^{d\_{1}}}|\widetilde{q}\_{k}(X\_{t})-\phi\_{\mathbf{c}\_{k}}(X\_{t})|^{2}\mathbf{1}\_{\Omega\_{r}}(X\_{t})\right] |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | â‰¤\displaystyle\displaystyle\leq | d12â€‹N2â€‹d1â€‹Cg2â€‹(1+rp)2â€‹Î´2â‰¤Îµ2,\displaystyle\displaystyle d\_{1}^{2}N^{2d\_{1}}C\_{g}^{2}(1+r^{p})^{2}\delta^{2}\leq\frac{\varepsilon}{2}, |  | (4.9) |

where the last inequality follows from the fact XtâˆˆÎ©r\displaystyle X\_{t}\in\Omega\_{r} and the polynomial growth of g\displaystyle g.
By selecting Î´=Îµ2â€‹d12â€‹N2â€‹d1â€‹Cg2â€‹(1+rp)2=O(d1âˆ’d12Îµd1+12(log(Îµâˆ’1))âˆ’d1+pÎ±\displaystyle\delta=\sqrt{\frac{\varepsilon}{2d\_{1}^{2}N^{2d\_{1}}C\_{g}^{2}(1+r^{p})^{2}}}=O(d\_{1}^{-\frac{d\_{1}}{2}}\varepsilon^{\frac{d\_{1}+1}{2}}\left(\log(\varepsilon^{-1})\right)^{-\frac{d\_{1}+p}{\alpha}} ).
Thus, we have

|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ğ”¼â€‹[sup0â‰¤tâ‰¤T|gâ€‹(Xt)âˆ’âˆ‘k=1Nd1gâ€‹(ğœk)â€‹q~k|2]\displaystyle\displaystyle\mathbb{E}\left[\sup\_{0\leq t\leq T}\left|g(X\_{t})-\sum\_{k=1}^{N^{d\_{1}}}g(\mathbf{c}\_{k})\widetilde{q}\_{k}\right|^{2}\right] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤\displaystyle\displaystyle\leq | ğ”¼â€‹[sup0â‰¤tâ‰¤T|gâ€‹(Xt)âˆ’gÂ¯â€‹(Xt)|2]+ğ”¼â€‹[sup0â‰¤tâ‰¤T|gÂ¯â€‹(Xt)âˆ’âˆ‘k=1Nd1uâ€‹(ğœk)â€‹q~k|2]\displaystyle\displaystyle\mathbb{E}\Big[\sup\_{0\leq t\leq T}\left|g(X\_{t})-\bar{g}(X\_{t})\right|^{2}\Big]+\mathbb{E}\left[\sup\_{0\leq t\leq T}\left|\bar{g}(X\_{t})-\sum\_{k=1}^{N^{d\_{1}}}u(\mathbf{c}\_{k})\widetilde{q}\_{k}\right|^{2}\right] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤\displaystyle\displaystyle\leq | Îµ2+Îµ2=Îµ.\displaystyle\displaystyle\frac{\varepsilon}{2}+\frac{\varepsilon}{2}=\varepsilon. |  |

The network architecture is then specified in the theorem.
âˆ

### 4.2 Functional Approximations

###### Theorem 4.2.

Let ğ–¥\displaystyle\mathsf{F} be defined in Definition [2.6](https://arxiv.org/html/2511.07235v1#S2.Thmtheorem6 "Definition 2.6 (Lipschitz functional). â€£ 2 Assumptions â€£ Deep Neural Operator Learning for Probabilistic Models"), and assume Assumptions [1](https://arxiv.org/html/2511.07235v1#Thmassumption1 "Assumption 1. â€£ 2 Assumptions â€£ Deep Neural Operator Learning for Probabilistic Models"), [2](https://arxiv.org/html/2511.07235v1#Thmassumption2 "Assumption 2. â€£ 2 Assumptions â€£ Deep Neural Operator Learning for Probabilistic Models"), [3](https://arxiv.org/html/2511.07235v1#Thmassumption3 "Assumption 3. â€£ 2 Assumptions â€£ Deep Neural Operator Learning for Probabilistic Models"), and [4](https://arxiv.org/html/2511.07235v1#Thmassumption4 "Assumption 4. â€£ 2 Assumptions â€£ Deep Neural Operator Learning for Probabilistic Models") hold.
For any Îµ>0\displaystyle\varepsilon>0,
set r=âŒˆâˆ’2â€‹CTcâ€‹logâ¡Îµ28â€‹C0â€‹Lğ–¥2âŒ‰1Î±+1\displaystyle r=\left\lceil-\frac{2C\_{T}}{c}\log\frac{\varepsilon^{2}}{8C\_{0}L\_{\mathsf{F}}^{2}}\right\rceil^{\frac{1}{\alpha}}+1.
Let {ğœm}m=1NÎ´âŠ‚Qr\displaystyle\{\mathbf{c}\_{m}\}\_{m=1}^{N^{\delta}}\subset Q\_{r} so that {â„¬Î´â€‹(ğœm)}m=1NÎ´\displaystyle\{\mathcal{B}\_{\delta}(\mathbf{c}\_{m})\}\_{m=1}^{N^{\delta}} is a cover of Qr\displaystyle Q\_{r} for some NÎ´>0\displaystyle N^{\delta}>0 to be estimated in Remark [4.4](https://arxiv.org/html/2511.07235v1#S4.Thmtheorem4 "Remark 4.4. â€£ 4.2 Functional Approximations â€£ 4 Neural scaling of operator learning â€£ Deep Neural Operator Learning for Probabilistic Models"),
and with ball radius Î´=Câ€‹Îµâ€‹(Lğ–¥â€‹Lg)âˆ’1â€‹d1âˆ’12\displaystyle\delta=C\varepsilon(L\_{\mathsf{F}}L\_{g})^{-1}d\_{1}^{-\frac{1}{2}}, with C\displaystyle C a constant.
Let H=Oâ€‹(NÎ´â€‹Îµâˆ’NÎ´)\displaystyle H=O(\sqrt{N^{\delta}}\varepsilon^{-N^{\delta}}), and set the network â„±NNâ€‹(N,1,L,ğ”­,K,Îº,R)\displaystyle\mathcal{F}\_{\rm NN}(N,1,L,\mathfrak{p},K,\kappa,R)
with

|  |  |  |
| --- | --- | --- |
|  | â„’=Oâ€‹((NÎ´)2â€‹logâ¡(NÎ´)+(NÎ´)2â€‹logâ¡(Îµâˆ’1)),ğ”­=Oâ€‹(1),K=Oâ€‹((NÎ´)2â€‹logâ¡NÎ´+(NÎ´)2â€‹logâ¡(Îµâˆ’1)),\displaystyle\displaystyle\mathcal{L}=O\left((N^{\delta})^{2}\log(N^{\delta})+(N^{\delta})^{2}\log(\varepsilon^{-1})\right),\mathfrak{p}=O(1),K=O\left((N^{\delta})^{2}\log N^{\delta}+(N^{\delta})^{2}\log(\varepsilon^{-1})\right), |  |
|  |  |  |
| --- | --- | --- |
|  | Îº=Oâ€‹((NÎ´)âˆ’NÎ´2â€‹Îµâˆ’NÎ´âˆ’12),R=1.\displaystyle\displaystyle\kappa=O((N^{\delta})^{-\frac{N^{\delta}}{2}}\varepsilon^{\frac{-N^{\delta}-1}{2}}),R=1. |  |

There are
{q~k}k=1H\displaystyle\{\widetilde{q}\_{k}\}\_{k=1}^{H} with q~kâˆˆâ„±NNâ€‹(NÎ´,1,â„’,ğ”­,K,Îº,R)\displaystyle\widetilde{q}\_{k}\in\mathcal{F}\_{\rm NN}(N^{\delta},1,\mathcal{L},\mathfrak{p},K,\kappa,R) for any k\displaystyle k, such that we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | supgâˆˆğ’¢|ğ–¥â€‹gâˆ’âˆ‘k=1Hakâ€‹q~kâ€‹(ğ’ˆ)|â‰¤Îµ,\displaystyle\displaystyle\sup\_{g\in\mathcal{G}}|\mathsf{F}g-\sum\_{k=1}^{H}a\_{k}\widetilde{q}\_{k}(\bm{g})|\leq\varepsilon, |  | (4.10) |

where ğ =[gâ€‹(ğœ1),gâ€‹(ğœ2),â€¦,gâ€‹(ğœNÎ´)]âŠ¤\displaystyle\bm{g}=[g(\mathbf{c}\_{1}),g(\mathbf{c}\_{2}),...,g(\mathbf{c}\_{N^{\delta}})]^{\top}, ak\displaystyle a\_{k}â€™s are coefficients depending on ğ–¥\displaystyle\mathsf{F}.
The constant hidden in O\displaystyle O and all constants C\displaystyle C depend on the constants Lğ–¥,Lg,CT,C0,c,Î±\displaystyle L\_{\mathsf{F}},L\_{g},C\_{T},C\_{0},c,\alpha in the assumptions.

###### Proof.

For r>0\displaystyle r>0, define the cube as before,

|  |  |  |
| --- | --- | --- |
|  | Qr:=[âˆ’r,r]d1={xâˆˆâ„d1:â€–xâ€–âˆ<r},whereÂ â€‹â€–xâ€–âˆ:=max1â‰¤jâ‰¤dâ¡|xj|.Q\_{r}:=[-r,r]^{d\_{1}}\;=\;\left\{x\in\mathbb{R}^{d\_{1}}:\|x\|\_{\infty}<r\right\},\qquad\mbox{where }\|x\|\_{\infty}:=\max\_{1\leq j\leq d}|x\_{j}|. |  |

Here r\displaystyle r is chosen so that tail probability is small as we did in the function approximation. Let {BÎ´â€‹(ck)}k=1NÎ´\displaystyle\{B\_{\delta}(c\_{k})\}\_{k=1}^{N^{\delta}} be a finite cover of Qr\displaystyle Q\_{r} by NÎ´\displaystyle N^{\delta} Euclidean balls.
By the Lemma [2.7](https://arxiv.org/html/2511.07235v1#S2.Thmtheorem7 "Lemma 2.7 (Theorem 13.7(ii) of [41]). â€£ 2 Assumptions â€£ Deep Neural Operator Learning for Probabilistic Models"), there exists a partition of unity {wkâ€‹(x)}k=1N\displaystyle\{w\_{k}(x)\}\_{k=1}^{N} subordinate to the cover {BÎ´â€‹(ck)}k=1NÎ´\displaystyle\{B\_{\delta}(c\_{k})\}\_{k=1}^{N^{\delta}}.

For any gâˆˆğ’°\displaystyle g\in\mathcal{U}, define ğ’ˆ=[gâ€‹(c1),gâ€‹(c2),â€¦,gâ€‹(cNÎ´)]ğ–³\displaystyle\bm{g}=[g(c\_{1}),g(c\_{2}),\ldots,g(c\_{N^{\delta}})]^{\mathsf{T}}, and define,

|  |  |  |  |
| --- | --- | --- | --- |
|  | gwâ€‹(x)={âˆ‘k=1NÎ´gâ€‹(ck)â€‹wkâ€‹(x),xâˆˆÎ©r,0,xâˆˆÎ©rC.\displaystyle\displaystyle g\_{w}(x)=\begin{cases}\displaystyle\sum\_{k=1}^{N^{\delta}}g(c\_{k})\,w\_{k}(x),&x\in\Omega\_{r},\\[3.44444pt] 0,&x\in\Omega\_{r}^{\,C}\,.\end{cases} |  | (4.11) |

Then, similar to the estimates in ([4.7](https://arxiv.org/html/2511.07235v1#S4.E7 "In 4.1 Function Approximation â€£ 4 Neural scaling of operator learning â€£ Deep Neural Operator Learning for Probabilistic Models")), as NÎ´\displaystyle N^{\delta} is the covering number of all d1\displaystyle d\_{1} dimensions, we have

|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ğ”¼â€‹[suptâ‰¤sâ‰¤T|gâ€‹(Xs)âˆ’gwâ€‹(Xs)|2]\displaystyle\displaystyle\mathbb{E}\!\left[\sup\_{t\leq s\leq T}\bigl|g(X\_{s})-g\_{w}(X\_{s})\bigr|^{2}\right] |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | =\displaystyle\displaystyle= | ğ”¼â€‹[suptâ‰¤sâ‰¤T|gâ€‹(Xs)|2â€‹ğŸÎ©rCâ€‹(Xs)]âŸğ’¥1+ğ”¼â€‹[suptâ‰¤sâ‰¤T|gâ€‹(Xs)âˆ’gwâ€‹(Xs)|2â€‹ğŸÎ©râ€‹(Xt)]âŸğ’¥2.\displaystyle\displaystyle\underbrace{\mathbb{E}\Big[\sup\_{t\leq s\leq T}\Big|g(X\_{s})\Big|^{2}\mathbf{1}\_{\Omega\_{r}^{C}}(X\_{s})\Big]}\_{\mathcal{J}\_{1}}+\underbrace{\mathbb{E}\Big[\sup\_{t\leq s\leq T}\Big|g(X\_{s})-g\_{w}(X\_{s})\Big|^{2}\mathbf{1}\_{\Omega\_{r}}(X\_{t})\Big]}\_{\mathcal{J}\_{2}}. |  | (4.12) |

Similar to ([4.4](https://arxiv.org/html/2511.07235v1#S4.E4 "In 4.1 Function Approximation â€£ 4 Neural scaling of operator learning â€£ Deep Neural Operator Learning for Probabilistic Models")), we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ’¥1â‰¤C0â€‹eâˆ’câ€‹rÎ±2â€‹CT.\displaystyle\displaystyle\mathcal{J}\_{1}\leq C\_{0}e^{-\frac{cr^{\alpha}}{2C\_{T}}}. |  | (4.13) |

Similar to ([4.5](https://arxiv.org/html/2511.07235v1#S4.E5 "In 4.1 Function Approximation â€£ 4 Neural scaling of operator learning â€£ Deep Neural Operator Learning for Probabilistic Models")), taking into consideration of the radius of the covering ball being Î´\displaystyle\delta, we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ’¥2=\displaystyle\displaystyle\mathcal{J}\_{2}= | ğ”¼â€‹[suptâ‰¤sâ‰¤T|âˆ‘k=1NÎ´[gâ€‹(Xs)âˆ’gâ€‹(ğœk)]â€‹wkâ€‹(Xs)|2â€‹ğŸÎ©râ€‹(Xs)]\displaystyle\displaystyle\mathbb{E}\Big[\sup\_{t\leq s\leq T}\Big|\sum\_{k=1}^{N^{\delta}}[g(X\_{s})-g(\mathbf{c}\_{k})]w\_{k}(X\_{s})\Big|^{2}\mathbf{1}\_{\Omega\_{r}}(X\_{s})\Big] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤\displaystyle\displaystyle\leq | ğ”¼â€‹[suptâ‰¤sâ‰¤T(âˆ‘k=1NÎ´|gâ€‹(Xs)âˆ’gâ€‹(ğœk)|â€‹|wkâ€‹(Xs)|)2â€‹ğŸÎ©râ€‹(Xs)]\displaystyle\displaystyle\mathbb{E}\Big[\sup\_{t\leq s\leq T}\Big(\sum\_{k=1}^{N^{\delta}}|g(X\_{s})-g(\mathbf{c}\_{k})||w\_{k}(X\_{s})|\Big)^{2}\mathbf{1}\_{\Omega\_{r}}(X\_{s})\Big] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | =\displaystyle\displaystyle= | ğ”¼â€‹[suptâ‰¤sâ‰¤T(âˆ‘k:â€–ğœkâˆ’Xsâ€–âˆâ‰¤Î´|gâ€‹(Xs)âˆ’gâ€‹(ğœk)|â€‹wkâ€‹(Xs))2â€‹ğŸÎ©râ€‹(Xs)],\displaystyle\displaystyle\mathbb{E}\Big[\sup\_{t\leq s\leq T}\Big(\sum\_{k:\|\mathbf{c}\_{k}-X\_{s}\|\_{\infty}\leq\delta}|g(X\_{s})-g(\mathbf{c}\_{k})|w\_{k}(X\_{s})\Big)^{2}\mathbf{1}\_{\Omega\_{r}}(X\_{s})\Big], |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤\displaystyle\displaystyle\leq | ğ”¼â€‹[suptâ‰¤sâ‰¤T(maxk:â€–ğœkâˆ’Xsâ€–âˆâ‰¤Î´â¡|gâ€‹(Xs)âˆ’gâ€‹(ğœk)|â€‹(âˆ‘k:|ğœkâˆ’Xs|â‰¤Î´wkâ€‹(Xs)))2â€‹ğŸÎ©râ€‹(Xs)]\displaystyle\displaystyle\mathbb{E}\Big[\sup\_{t\leq s\leq T}\Big(\max\_{k:\|\mathbf{c}\_{k}-X\_{s}\|\_{\infty}\leq\delta}|g(X\_{s})-g(\mathbf{c}\_{k})|\Big(\sum\_{k:|\mathbf{c}\_{k}-X\_{s}|\leq\delta}w\_{k}(X\_{s})\Big)\Big)^{2}\mathbf{1}\_{\Omega\_{r}}(X\_{s})\Big] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤\displaystyle\displaystyle\leq | ğ”¼â€‹[suptâ‰¤sâ‰¤Tmaxk:â€–ğœkâˆ’Xsâ€–âˆâ‰¤Î´â¡|gâ€‹(Xs)âˆ’gâ€‹(ğœk)|2â€‹ğŸÎ©râ€‹(Xs)]\displaystyle\displaystyle\mathbb{E}\Big[\sup\_{t\leq s\leq T}\max\_{k:\|\mathbf{c}\_{k}-X\_{s}\|\_{\infty}\leq\delta}|g(X\_{s})-g(\mathbf{c}\_{k})|^{2}\mathbf{1}\_{\Omega\_{r}}(X\_{s})\Big] |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | â‰¤\displaystyle\displaystyle\leq | (Lgâ€‹Î´)2â€‹d1.\displaystyle\displaystyle(L\_{g}\delta)^{2}d\_{1}. |  | (4.14) |

From the Lipschitz property of the functional ğ–¥\displaystyle\mathsf{F} defined in Definition [2.6](https://arxiv.org/html/2511.07235v1#S2.Thmtheorem6 "Definition 2.6 (Lipschitz functional). â€£ 2 Assumptions â€£ Deep Neural Operator Learning for Probabilistic Models"), we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | |ğ–¥â€‹(g)âˆ’ğ–¥â€‹(gw)|2\displaystyle\displaystyle\bigl|\mathsf{F}(g)-\mathsf{F}(g\_{w})\bigr|^{2} | â‰¤Lğ–¥2â€‹â€–gâˆ’gwâ€–S22\displaystyle\displaystyle\leq L\_{\mathsf{F}}^{2}\,\|g-g\_{w}\|\_{S^{2}}^{2} |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | â‰¤Lğ–¥2â€‹((Lgâ€‹Î´)2â€‹d1+C0â€‹eâˆ’câ€‹rÎ±2â€‹CT)<Îµ2/4,\displaystyle\displaystyle\leq\;L\_{\mathsf{F}}^{2}\left((L\_{g}\delta)^{2}d\_{1}+\;C\_{0}e^{-\frac{cr^{\alpha}}{2C\_{T}}}\right)\ <\varepsilon^{2}/4, |  | (4.15) |

i.e.

|  |  |  |  |
| --- | --- | --- | --- |
|  | |ğ–¥â€‹(g)âˆ’ğ–¥â€‹(gw)|<Îµ/2,\displaystyle\displaystyle\bigl|\mathsf{F}(g)-\mathsf{F}(g\_{w})\bigr|<\varepsilon/2, |  | (4.16) |

which follows by selecting

|  |  |  |  |
| --- | --- | --- | --- |
|  | r=âŒˆâˆ’2â€‹CTcâ€‹logâ¡Îµ28â€‹C0â€‹Lğ–¥2âŒ‰1Î±+1,Î´=Îµ2â€‹2â€‹Lğ–¥â€‹Lgâ€‹d112.\displaystyle\displaystyle r=\left\lceil-\frac{2C\_{T}}{c}\log\frac{\varepsilon^{2}}{8C\_{0}L\_{\mathsf{F}}^{2}}\right\rceil^{\frac{1}{\alpha}}+1,\quad\delta=\frac{\varepsilon}{2\sqrt{2}L\_{\mathsf{F}}L\_{g}d\_{1}^{\frac{1}{2}}}. |  | (4.17) |

Now, for any g,g~âˆˆğ’¢\displaystyle g,\widetilde{g}\in\mathcal{G}, define gw\displaystyle g\_{w} and g~w\displaystyle\widetilde{g}\_{w} as in ([4.11](https://arxiv.org/html/2511.07235v1#S4.E11 "In 4.2 Functional Approximations â€£ 4 Neural scaling of operator learning â€£ Deep Neural Operator Learning for Probabilistic Models")), and set

|  |  |  |
| --- | --- | --- |
|  | ğ’ˆ=[gâ€‹(c1),â€¦,gâ€‹(cNÎ´)]ğ–³,ğ’ˆ~=[g~â€‹(c1),â€¦,g~â€‹(cNÎ´)]ğ–³.\bm{g}=\bigl[g(c\_{1}),\ldots,g(c\_{N^{\delta}})\bigr]^{\mathsf{T}},\qquad\widetilde{\bm{g}}=\bigl[\widetilde{g}(c\_{1}),\ldots,\widetilde{g}(c\_{N^{\delta}})\bigr]^{\mathsf{T}}. |  |

Define the function hâ€‹(ğ’ˆ):=ğ–¥â€‹(gw)\displaystyle h(\bm{g}):=\mathsf{F}(g\_{w}).
Then

|  |  |  |  |
| --- | --- | --- | --- |
|  | |hâ€‹(ğ’ˆ)âˆ’hâ€‹(ğ’ˆ~)|2\displaystyle\displaystyle|h(\bm{g})-h(\widetilde{\bm{g}})|^{2} | =|ğ–¥â€‹(gw)âˆ’ğ–¥â€‹(g~w)|2\displaystyle\displaystyle=\bigl|\mathsf{F}(g\_{w})-\mathsf{F}(\widetilde{g}\_{w})\bigr|^{2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤Lğ–¥2â€‹â€–gwâˆ’g~wâ€–S22\displaystyle\displaystyle\leq L\_{\mathsf{F}}^{2}\,\|g\_{w}-\widetilde{g}\_{w}\|\_{S^{2}}^{2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =Lğ–¥2â€‹ğ”¼â€‹[suptâ‰¤sâ‰¤T(âˆ‘k=1NÎ´(gâ€‹(ck)âˆ’g~â€‹(ck))â€‹wkâ€‹(Xs))2]\displaystyle\displaystyle=L\_{\mathsf{F}}^{2}\,\mathbb{E}\!\left[\sup\_{t\leq s\leq T}\!\!\Bigl(\sum\_{k=1}^{N^{\delta}}\bigl(g(c\_{k})-\widetilde{g}(c\_{k})\bigr)\,w\_{k}(X\_{s})\Bigr)^{2}\right] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤Lğ–¥2â€‹(âˆ‘k=1NÎ´|gâ€‹(ck)âˆ’g~â€‹(ck)|2)â€‹ğ”¼â€‹[suptâ‰¤sâ‰¤Tâˆ‘k=1NÎ´wkâ€‹(Xs)2]\displaystyle\displaystyle\leq L\_{\mathsf{F}}^{2}\,\Bigl(\sum\_{k=1}^{N^{\delta}}|g(c\_{k})-\widetilde{g}(c\_{k})|^{2}\Bigr)\,\mathbb{E}\!\left[\sup\_{t\leq s\leq T}\sum\_{k=1}^{N^{\delta}}w\_{k}(X\_{s})^{2}\right] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤Lğ–¥2â€‹(âˆ‘k=1NÎ´|gâ€‹(ck)âˆ’g~â€‹(ck)|2)\displaystyle\displaystyle\leq L\_{\mathsf{F}}^{2}\,\,\Bigl(\sum\_{k=1}^{N^{\delta}}|g(c\_{k})-\widetilde{g}(c\_{k})|^{2}\Bigr) |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | â‰¤Lğ–¥2â€‹|ğ’ˆâˆ’ğ’ˆ~|2,\displaystyle\displaystyle\leq L\_{\mathsf{F}}^{2}\,\,|\bm{g}-\widetilde{\bm{g}}|^{2}, |  | (4.18) |

where we use the fact that {wkâ€‹(x)}k=1NÎ´\displaystyle\{w\_{k}(x)\}\_{k=1}^{N^{\delta}} is a partition of unity. Thus we show that hâ€‹(ğ’ˆ):=ğ–¥â€‹(gw)\displaystyle h(\bm{g}):=\mathsf{F}(g\_{w}) is a Lipchitz function on ğ’¢\displaystyle\mathcal{G} according to ([4.18](https://arxiv.org/html/2511.07235v1#S4.E18 "In 4.2 Functional Approximations â€£ 4 Neural scaling of operator learning â€£ Deep Neural Operator Learning for Probabilistic Models")).Besides, according to the assumption on ğ–¥\displaystyle\mathsf{F} and the definition of h\displaystyle h, h\displaystyle h is bounded.
Also, the domain of h\displaystyle h is bounded by the range of g\displaystyle g.
Consequently, h\displaystyle h satisfies the approximation rate estimate in [[34](https://arxiv.org/html/2511.07235v1#bib.bib34)][Theorem 5], it follows that, for Îµ>0\displaystyle\varepsilon>0, if we set H=Câ€‹NÎ´â€‹Îµâˆ’NÎ´\displaystyle H=C\sqrt{N^{\delta}}\varepsilon^{-N^{\delta}} for some C\displaystyle C, then there exists a
network architecture â„±NNâ€‹(NÎ´,1,â„’,ğ”­,K,Îº,R)\displaystyle\mathcal{F}\_{\rm NN}(N^{\delta},1,\mathcal{L},\mathfrak{p},K,\kappa,R) and {q~k}k=1H\displaystyle\{\widetilde{q}\_{k}\}\_{k=1}^{H} with q~kâˆˆâ„±NNâ€‹(NÎ´,1,â„’,ğ”­,K,Îº,R)\displaystyle\widetilde{q}\_{k}\in\mathcal{F}\_{\rm NN}(N^{\delta},1,\mathcal{L},\mathfrak{p},K,\kappa,R) for k=1,â€¦,H\displaystyle k=1,\ldots,H such that

|  |  |  |  |
| --- | --- | --- | --- |
|  | supgâˆˆğ’¢|ğ–¥â€‹(gw)âˆ’âˆ‘k=1Hakâ€‹q~kâ€‹(ğ’ˆ)|â‰¤Îµ2,\displaystyle\displaystyle\sup\_{g\in\mathcal{G}}\left|\mathsf{F}(g\_{w})-\sum\_{k=1}^{H}a\_{k}\widetilde{q}\_{k}(\bm{g})\right|\leq\frac{\varepsilon}{2}, |  | (4.19) |

where ak\displaystyle a\_{k} are constants depending on f\displaystyle f.
Such an architecture has

|  |  |  |
| --- | --- | --- |
|  | â„’=Oâ€‹((NÎ´)2â€‹logâ¡(NÎ´)+(NÎ´)2â€‹logâ¡(Îµâˆ’1)),ğ”­=Oâ€‹(1),K=Oâ€‹((NÎ´)2â€‹logâ¡NÎ´+(NÎ´)2â€‹logâ¡(Îµâˆ’1)),\displaystyle\displaystyle\mathcal{L}=O\left((N^{\delta})^{2}\log(N^{\delta})+(N^{\delta})^{2}\log(\varepsilon^{-1})\right),\mathfrak{p}=O(1),K=O\left((N^{\delta})^{2}\log N^{\delta}+(N^{\delta})^{2}\log(\varepsilon^{-1})\right), |  |
|  |  |  |
| --- | --- | --- |
|  | Îº=Oâ€‹((NÎ´)âˆ’NÎ´2â€‹Îµâˆ’NÎ´âˆ’12),R=1.\displaystyle\displaystyle\kappa=O((N^{\delta})^{-\frac{N^{\delta}}{2}}\varepsilon^{\frac{-N^{\delta}-1}{2}}),R=1. |  |

Finally, We have, for any gâˆˆğ’¢\displaystyle g\in\mathcal{G} and ğ’ˆ=[gâ€‹(c1),â€¦,gâ€‹(cNÎ´)]âŠ¤\displaystyle\bm{g}=[g(c\_{1}),...,g(c\_{N^{\delta}})]^{\top}

|  |  |  |  |
| --- | --- | --- | --- |
|  | supgâˆˆğ’¢|ğ–¥â€‹(g)âˆ’âˆ‘k=1Hakâ€‹q~kâ€‹(ğ’ˆ)|â‰¤\displaystyle\displaystyle\sup\_{g\in\mathcal{G}}\left|\mathsf{F}(g)-\sum\_{k=1}^{H}a\_{k}\widetilde{q}\_{k}(\bm{g})\right|\leq | supgâˆˆğ’¢|ğ–¥â€‹(g)âˆ’hâ€‹(ğ’ˆ)|+supğ’ˆ|hâ€‹(ğ’ˆ)âˆ’âˆ‘k=1Hakâ€‹q~kâ€‹(ğ’ˆ)|\displaystyle\displaystyle\sup\_{g\in\mathcal{G}}\left|\mathsf{F}(g)-h(\bm{g})\right|+\sup\_{\bm{g}}\left|h(\bm{g})-\sum\_{k=1}^{H}a\_{k}\widetilde{q}\_{k}(\bm{g})\right| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤\displaystyle\displaystyle\leq | Îµ2+Îµ2=Îµ.\displaystyle\displaystyle\frac{\varepsilon}{2}+\frac{\varepsilon}{2}=\varepsilon. |  |

âˆ

The next lemma and remark are used to estimate NÎ´\displaystyle N^{\delta} and H\displaystyle H.

###### Lemma 4.3.

Let ğ”‡=[âˆ’Î³,Î³]d\displaystyle\mathfrak{D}=[-\gamma,\gamma]^{d} for some Î³>0\displaystyle\gamma>0. For any Î´>0\displaystyle\delta>0, there exists a cover {â„¬Î´â€‹(ğœm)}m=1M\displaystyle\{\mathcal{B}\_{\delta}(\mathbf{c}\_{m})\}\_{m=1}^{M} of ğ”‡\displaystyle\mathfrak{D} with

|  |  |  |  |
| --- | --- | --- | --- |
|  | Mâ‰¤Câ€‹Î´âˆ’d,\displaystyle\displaystyle{\color[rgb]{0,0,0}M\leq C\delta^{-d}}, |  | (4.20) |

where C\displaystyle C is a constant depending on Î³\displaystyle\gamma and d\displaystyle d.

###### Proof of Lemma [4.3](https://arxiv.org/html/2511.07235v1#S4.Thmtheorem3 "Lemma 4.3. â€£ 4.2 Functional Approximations â€£ 4 Neural scaling of operator learning â€£ Deep Neural Operator Learning for Probabilistic Models").

By [[14](https://arxiv.org/html/2511.07235v1#bib.bib14), Chapter 2], we have,

|  |  |  |  |
| --- | --- | --- | --- |
|  | câ‰¤âŒˆ2â€‹Î³Î´âŒ‰d+7â€‹dâ€‹logâ¡dâ‰¤Câ€‹(Î³Î´)d\displaystyle\displaystyle{\color[rgb]{0,0,0}c\leq\left\lceil\frac{2\gamma}{\delta}\right\rceil^{d}+7d\log d\leq C\left(\frac{\gamma}{\delta}\right)^{d}} |  | (4.21) |

for some C\displaystyle C depending on Î³\displaystyle\gamma and d\displaystyle d.
âˆ

###### Remark 4.4.

In this remark, we estimate the number of covering NÎ´\displaystyle N^{\delta} and hence the number of basis H\displaystyle H needed.
By Lemma [4.3](https://arxiv.org/html/2511.07235v1#S4.Thmtheorem3 "Lemma 4.3. â€£ 4.2 Functional Approximations â€£ 4 Neural scaling of operator learning â€£ Deep Neural Operator Learning for Probabilistic Models") and Equation [4.17](https://arxiv.org/html/2511.07235v1#S4.E17 "In 4.2 Functional Approximations â€£ 4 Neural scaling of operator learning â€£ Deep Neural Operator Learning for Probabilistic Models") in the proof, it follows that,

|  |  |  |  |
| --- | --- | --- | --- |
|  | NÎ´â‰¤Câ€‹(Î³Î´)d1\displaystyle\displaystyle N^{\delta}\leq C\left(\frac{\gamma}{\delta}\right)^{d\_{1}} | â‰¤Câ€‹(âˆ’2â€‹CTc)d1Î±â€‹(logâ¡Îµ28â€‹C0â€‹Lğ–¥2)d1Î±â€‹Îµâˆ’d1â€‹d1d12\displaystyle\displaystyle\leq C(-\frac{2C\_{T}}{c})^{\frac{d\_{1}}{\alpha}}\left(\log\frac{\varepsilon^{2}}{8C\_{0}L\_{\mathsf{F}}^{2}}\right)^{\frac{d\_{1}}{\alpha}}\varepsilon^{-d\_{1}}d\_{1}^{\frac{d\_{1}}{2}} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤Câ€‹(logâ¡(Îµâˆ’2))d1Î±â€‹Îµâˆ’d1,\displaystyle\displaystyle\leq C\left(\log(\varepsilon^{-2})\right)^{\frac{d\_{1}}{\alpha}}\varepsilon^{-d\_{1}}, |  |

where the constant C\displaystyle C depends on CT\displaystyle C\_{T}, c\displaystyle c, C0\displaystyle C\_{0}, Î±\displaystyle\alpha, Lğ–¥,Lg\displaystyle L\_{\mathsf{F}},L\_{g} and d1\displaystyle d\_{1}.
Dropping the lower order term in Equation [4.4](https://arxiv.org/html/2511.07235v1#S4.Ex52 "Remark 4.4. â€£ 4.2 Functional Approximations â€£ 4 Neural scaling of operator learning â€£ Deep Neural Operator Learning for Probabilistic Models"), it follows that

|  |  |  |
| --- | --- | --- |
|  | H=Câ€‹(NÎ´)12â€‹Îµâˆ’NÎ´=Câ€‹Îµâˆ’d12â€‹Îµâˆ’Îµâˆ’d1,\displaystyle\displaystyle H=C(N^{\delta})^{\frac{1}{2}}\varepsilon^{-N^{\delta}}=C\varepsilon^{-\frac{d\_{1}}{2}}\varepsilon^{-\varepsilon^{-d\_{1}}}, |  |

or H=ğ’ªâ€‹(Îµâˆ’Îµâˆ’d1)\displaystyle H=\mathcal{O}(\varepsilon^{-\varepsilon^{-d\_{1}}}).

### 4.3 Operator Approximation

###### Theorem 4.5.

[Operator]
Let Assumptions [1](https://arxiv.org/html/2511.07235v1#Thmassumption1 "Assumption 1. â€£ 2 Assumptions â€£ Deep Neural Operator Learning for Probabilistic Models"), [2](https://arxiv.org/html/2511.07235v1#Thmassumption2 "Assumption 2. â€£ 2 Assumptions â€£ Deep Neural Operator Learning for Probabilistic Models"), [3](https://arxiv.org/html/2511.07235v1#Thmassumption3 "Assumption 3. â€£ 2 Assumptions â€£ Deep Neural Operator Learning for Probabilistic Models"), [4](https://arxiv.org/html/2511.07235v1#Thmassumption4 "Assumption 4. â€£ 2 Assumptions â€£ Deep Neural Operator Learning for Probabilistic Models"), [5](https://arxiv.org/html/2511.07235v1#Thmassumption5 "Assumption 5. â€£ 2 Assumptions â€£ Deep Neural Operator Learning for Probabilistic Models"), and [6](https://arxiv.org/html/2511.07235v1#Thmassumption6 "Assumption 6. â€£ 2 Assumptions â€£ Deep Neural Operator Learning for Probabilistic Models") hold.
For any Îµ>0\displaystyle\varepsilon>0,
set N1=Câ€‹Îµâˆ’d2\displaystyle N\_{1}=C\varepsilon^{-d\_{2}}, and N2=Oâ€‹(Îµâˆ’d1â€‹d2âˆ’d1)\displaystyle N\_{2}=O(\varepsilon^{-d\_{1}d\_{2}-d\_{1}}).
Define the network architecture â„±1=â„±NNâ€‹(d2,1,â„’1,ğ”­1,K1,Îº1)\displaystyle\mathcal{F}\_{1}=\mathcal{F}\_{\rm NN}(d\_{2},1,\mathcal{L}\_{1},\mathfrak{p}\_{1},K\_{1},\kappa\_{1}) and â„±2=â„±NNâ€‹(N2,1,â„’2,ğ”­2,K2,Îº2)\displaystyle\mathcal{F}\_{2}=\mathcal{F}\_{\rm NN}(N\_{2},1,\mathcal{L}\_{2},\mathfrak{p}\_{2},K\_{2},\kappa\_{2}) with

|  |  |  |
| --- | --- | --- |
|  | â„’1=Oâ€‹(logâ¡(Îµâˆ’2)),ğ”­1=Oâ€‹(1),K1=Oâ€‹(logâ¡(Îµâˆ’2)),Îº1=Oâ€‹(Îµâˆ’d2),R=1,\displaystyle\displaystyle\mathcal{L}\_{1}=O\left(\log(\varepsilon^{-2})\right),\ \mathfrak{p}\_{1}=O(1),\ K\_{1}=O\left(\log(\varepsilon^{-2})\right),\ \kappa\_{1}=O(\varepsilon^{-d\_{2}}),R=1, |  |

and

|  |  |  |
| --- | --- | --- |
|  | â„’2=Oâ€‹(N22â€‹logâ¡N2+N22â€‹logâ¡(Îµâˆ’d2âˆ’1)),ğ”­2=Oâ€‹(N212â€‹Îµâˆ’N2â€‹d2âˆ’N2),\displaystyle\displaystyle\mathcal{L}\_{2}=O\left(N\_{2}^{2}\log N\_{2}+N\_{2}^{2}\log(\varepsilon^{-d\_{2}-1})\right),\ \mathfrak{p}\_{2}=O(N\_{2}^{\frac{1}{2}}\varepsilon^{-N\_{2}d\_{2}-N\_{2}}), |  |
|  |  |  |
| --- | --- | --- |
|  | K2=Oâ€‹(N212â€‹Îµâˆ’N2â€‹d2âˆ’N2â€‹(N22â€‹logâ¡N2+N22â€‹logâ¡(Îµâˆ’d2âˆ’1))),\displaystyle\displaystyle K\_{2}=O\left(N\_{2}^{\frac{1}{2}}\varepsilon^{-N\_{2}d\_{2}-N\_{2}}\left(N\_{2}^{2}\log N\_{2}+N\_{2}^{2}\log(\varepsilon^{-d\_{2}-1})\right)\right), |  |
|  |  |  |
| --- | --- | --- |
|  | Îº2=Oâ€‹(N2âˆ’N22â€‹Îµâˆ’N2â€‹d22âˆ’N2).\displaystyle\displaystyle\kappa\_{2}=O(N\_{2}^{-\frac{N\_{2}}{2}}\varepsilon^{-\frac{N\_{2}d\_{2}}{2}-N\_{2}}). |  |

For any operator Î“:ğ’¢â†’ğ’°\displaystyle\Gamma:\mathcal{G}\rightarrow\mathcal{U} satisfying Assumption [6](https://arxiv.org/html/2511.07235v1#Thmassumption6 "Assumption 6. â€£ 2 Assumptions â€£ Deep Neural Operator Learning for Probabilistic Models"), there are {q~k}k=1N1\displaystyle\{\widetilde{q}\_{k}\}\_{k=1}^{N\_{1}} with q~kâˆˆâ„±1\displaystyle\widetilde{q}\_{k}\in\mathcal{F}\_{1} and {a~k}k=1N1\displaystyle\{\widetilde{a}\_{k}\}\_{k=1}^{N\_{1}} with a~kâˆˆâ„±2\displaystyle\widetilde{a}\_{k}\in\mathcal{F}\_{2} such that

|  |  |  |  |
| --- | --- | --- | --- |
|  | supgâˆˆğ’¢ğ”¼â€‹[sup0â‰¤tâ‰¤T|Î“â€‹(g)â€‹(Xt)âˆ’âˆ‘k=1N1a~kâ€‹(ğ’ˆ)â€‹q~kâ€‹(Xt)|]â‰¤Îµ.\displaystyle\displaystyle\sup\_{g\in\mathcal{G}}\mathbb{E}\bigg[\sup\_{0\leq t\leq T}\left|\Gamma(g)(X\_{t})-\sum\_{k=1}^{N\_{1}}\widetilde{a}\_{k}(\bm{g})\widetilde{q}\_{k}(X\_{t})\right|\bigg]\leq\varepsilon. |  | (4.22) |

###### Proof of Theorem [4.5](https://arxiv.org/html/2511.07235v1#S4.Thmtheorem5 "Theorem 4.5. â€£ 4.3 Operator Approximation â€£ 4 Neural scaling of operator learning â€£ Deep Neural Operator Learning for Probabilistic Models").

By Assumption [5](https://arxiv.org/html/2511.07235v1#Thmassumption5 "Assumption 5. â€£ 2 Assumptions â€£ Deep Neural Operator Learning for Probabilistic Models") and [6](https://arxiv.org/html/2511.07235v1#Thmassumption6 "Assumption 6. â€£ 2 Assumptions â€£ Deep Neural Operator Learning for Probabilistic Models"), Î“â€‹(g)â€‹(â‹…)\displaystyle\Gamma(g)(\cdot) satisfies the assumptions of Theorem [4.1](https://arxiv.org/html/2511.07235v1#S4.Thmtheorem1 "Theorem 4.1. â€£ 4.1 Function Approximation â€£ 4 Neural scaling of operator learning â€£ Deep Neural Operator Learning for Probabilistic Models").
It follows that, for Îµ1>0\displaystyle\varepsilon\_{1}>0 which will be specified,
set r=âŒˆâˆ’2â€‹CTcâ€‹logâ¡Îµ14â€‹C0âŒ‰1Î±\displaystyle r=\left\lceil-\frac{2C\_{T}}{c}\log\frac{\varepsilon\_{1}}{4C\_{0}}\right\rceil^{\frac{1}{\alpha}}, and define Qr,Î©r\displaystyle Q\_{r},\Omega\_{r} as in Definition [2.2](https://arxiv.org/html/2511.07235v1#S2.Thmtheorem2 "Definition 2.2. â€£ 2 Assumptions â€£ Deep Neural Operator Learning for Probabilistic Models"),
there exists a constant N1=Câ€‹Îµ1âˆ’d2\displaystyle N\_{1}=C\varepsilon\_{1}^{-d\_{2}} for some constant C\displaystyle C depending on d2,Lg\displaystyle d\_{2},L\_{g} and r\displaystyle r, a network architecture â„±1=â„±NNâ€‹(d2,1,â„’1,ğ”­1,K1,Îº1,R1)\displaystyle\mathcal{F}\_{1}=\mathcal{F}\_{\rm NN}(d\_{2},1,\mathcal{L}\_{1},\mathfrak{p}\_{1},K\_{1},\kappa\_{1},R\_{1}) and {q~k}k=1N1\displaystyle\{\widetilde{q}\_{k}\}\_{k=1}^{N\_{1}} with q~kâˆˆâ„±1\displaystyle\widetilde{q}\_{k}\in\mathcal{F}\_{1}, and {ğœk}k=1N1âŠ‚Qr\displaystyle\{\mathbf{c}\_{k}\}\_{k=1}^{N\_{1}}\subset Q\_{r} such that for any gâˆˆğ’¢\displaystyle g\in\mathcal{G}, we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â€‹[sup0â‰¤tâ‰¤T|Î“â€‹(g)â€‹(Xt)âˆ’âˆ‘k=1N1Î“â€‹(g)â€‹(ğœk)â€‹q~kâ€‹(Xt)|2]1/2â‰¤Îµ1.\displaystyle\displaystyle\mathbb{E}\bigg[\sup\_{0\leq t\leq T}\left|\Gamma(g)(X\_{t})-\sum\_{k=1}^{N\_{1}}\Gamma(g)(\mathbf{c}\_{k})\widetilde{q}\_{k}(X\_{t})\right|^{2}\bigg]^{1/2}\leq\varepsilon\_{1}. |  | (4.23) |

Such a network has size

|  |  |  |
| --- | --- | --- |
|  | â„’1=Oâ€‹(logâ¡(Îµ1âˆ’2)),ğ”­1=Oâ€‹(1),K1=Oâ€‹(logâ¡(Îµ1âˆ’2)),Îº1=Oâ€‹(Îµ1âˆ’d2).\displaystyle\displaystyle\mathcal{L}\_{1}=O\left(\log(\varepsilon\_{1}^{-2})\right),\ \mathfrak{p}\_{1}=O(1),\ K\_{1}=O\left(\log(\varepsilon\_{1}^{-2})\right),\ \kappa\_{1}=O(\varepsilon\_{1}^{-d\_{2}}). |  |

For each k\displaystyle k, define the functional

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ–¥kâ€‹(Î“â€‹g):=Î“â€‹gâ€‹(ğœk).\displaystyle\displaystyle\mathsf{F}\_{k}(\Gamma g):=\Gamma g(\mathbf{c}\_{k}). |  | (4.24) |

For any g1,g2âˆˆğ’¢\displaystyle g\_{1},g\_{2}\in\mathcal{G}, and the forward process at t\displaystyle t starts at ğœk\displaystyle\mathbf{c}\_{k}, we have

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | |ğ–¥kâ€‹(Î“â€‹g1)âˆ’ğ–¥kâ€‹(Î“â€‹g2)|2\displaystyle\displaystyle|\mathsf{F}\_{k}(\Gamma g\_{1})-\mathsf{F}\_{k}(\Gamma g\_{2})|^{2} |  | (4.25) |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤\displaystyle\displaystyle\leq | ğ”¼â€‹[sup0â‰¤sâ‰¤T|Î“â€‹(g1)â€‹(Xs)âˆ’Î“â€‹(g2)â€‹(Xs)|2]\displaystyle\displaystyle\mathbb{E}\bigg[\sup\_{0\leq s\leq T}|\Gamma(g\_{1})(X\_{s})-\Gamma(g\_{2})(X\_{s})|^{2}\bigg] |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | â‰¤\displaystyle\displaystyle\leq | LÎ“2â€‹ğ”¼â€‹[sup0â‰¤sâ‰¤T|g1â€‹(Xs)âˆ’g2â€‹(Xs)|2]\displaystyle\displaystyle L\_{\Gamma}^{2}\mathbb{E}\bigg[\sup\_{0\leq s\leq T}|g\_{1}(X\_{s})-g\_{2}(X\_{s})|^{2}\bigg] |  | (4.26) |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | =\displaystyle\displaystyle= | LÎ“2â€‹â€–g1âˆ’g2â€–S22,\displaystyle\displaystyle L\_{\Gamma}^{2}\|g\_{1}-g\_{2}\|\_{S^{2}}^{2}, |  | (4.27) |

where the last inequality follows from Assumption [6](https://arxiv.org/html/2511.07235v1#Thmassumption6 "Assumption 6. â€£ 2 Assumptions â€£ Deep Neural Operator Learning for Probabilistic Models").
By Theorem [4.2](https://arxiv.org/html/2511.07235v1#S4.Thmtheorem2 "Theorem 4.2. â€£ 4.2 Functional Approximations â€£ 4 Neural scaling of operator learning â€£ Deep Neural Operator Learning for Probabilistic Models"), for any Îµ2>0\displaystyle\varepsilon\_{2}>0, there exist N2\displaystyle N\_{2} and H\displaystyle H with values estimated later, and a network architecture â„±2=â„±NNâ€‹(N2,1,â„’2,ğ”­2,K2,Îº2,R2)\displaystyle\mathcal{F}\_{2}=\mathcal{F}\_{\rm NN}(N\_{2},1,\mathcal{L}\_{2},\mathfrak{p}\_{2},K\_{2},\kappa\_{2},R\_{2}) with

|  |  |  |
| --- | --- | --- |
|  | â„’2=Oâ€‹(N22â€‹logâ¡N2+N22â€‹logâ¡(Îµ2âˆ’1)),ğ”­2=Oâ€‹(1),K2=Oâ€‹(N22â€‹logâ¡N2+N22â€‹logâ¡(Îµ2âˆ’1)),\displaystyle\displaystyle\mathcal{L}\_{2}=O\left(N\_{2}^{2}\log N\_{2}+N\_{2}^{2}\log(\varepsilon\_{2}^{-1})\right),\ \mathfrak{p}\_{2}=O(1),\ K\_{2}=O\left(N\_{2}^{2}\log N\_{2}+N\_{2}^{2}\log(\varepsilon\_{2}^{-1})\right), |  |
|  |  |  |
| --- | --- | --- |
|  | Îº2=Oâ€‹(N2âˆ’N22â€‹Îµ2âˆ’N2âˆ’1),R=1.\displaystyle\displaystyle\kappa\_{2}=O(N\_{2}^{-\frac{N\_{2}}{2}}\varepsilon\_{2}^{-N\_{2}-1}),R=1. |  |

Such a network architecture gives a network a~k\displaystyle\widetilde{a}\_{k} so that

|  |  |  |
| --- | --- | --- |
|  | supg|ğ–¥kâ€‹(Î“â€‹(g))âˆ’a~kâ€‹(ğ’ˆ)|â‰¤Îµ2.\displaystyle\displaystyle\sup\_{g}|\mathsf{F}\_{k}(\Gamma(g))-\widetilde{a}\_{k}(\bm{g})|\leq\varepsilon\_{2}. |  |

Therefore,

|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ğ”¼â€‹[sup0â‰¤sâ‰¤T|âˆ‘k=1N1ğ–¥kâ€‹(Î“â€‹g)â€‹q~kâ€‹(Xs)âˆ’âˆ‘k=1N1a~kâ€‹(ğ’ˆ)â€‹q~kâ€‹(Xs)|]\displaystyle\displaystyle\mathbb{E}\bigg[\sup\_{0\leq s\leq T}\left|\sum\_{k=1}^{N\_{1}}\mathsf{F}\_{k}(\Gamma g)\widetilde{q}\_{k}(X\_{s})-\sum\_{k=1}^{N\_{1}}\widetilde{a}\_{k}(\bm{g})\widetilde{q}\_{k}(X\_{s})\right|\bigg] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | =\displaystyle\displaystyle= | ğ”¼â€‹[sup0â‰¤sâ‰¤T|âˆ‘k=1N1(ğ–¥kâ€‹(Î“â€‹g)âˆ’a~kâ€‹(ğ’ˆ))â€‹q~kâ€‹(Xs)|]\displaystyle\displaystyle\mathbb{E}\bigg[\sup\_{0\leq s\leq T}\left|\sum\_{k=1}^{N\_{1}}\left(\mathsf{F}\_{k}(\Gamma g)-\widetilde{a}\_{k}(\bm{g})\right)\widetilde{q}\_{k}(X\_{s})\right|\bigg] |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | â‰¤\displaystyle\displaystyle\leq | âˆ‘k=1N1supğ’ˆ|ğ–¥kâ€‹(Î“â€‹g)âˆ’a~kâ€‹(ğ’ˆ)|=N1â€‹Îµ2.\displaystyle\displaystyle\sum\_{k=1}^{N\_{1}}\sup\_{\bm{g}}|\mathsf{F}\_{k}(\Gamma g)-\widetilde{a}\_{k}(\bm{g})|=N\_{1}\varepsilon\_{2}. |  | (4.28) |

Applying the Cauchyâ€“Schwarz inequality, using ([4.23](https://arxiv.org/html/2511.07235v1#S4.E23 "In 4.3 Operator Approximation â€£ 4 Neural scaling of operator learning â€£ Deep Neural Operator Learning for Probabilistic Models")) and ([4.28](https://arxiv.org/html/2511.07235v1#S4.E28 "In 4.3 Operator Approximation â€£ 4 Neural scaling of operator learning â€£ Deep Neural Operator Learning for Probabilistic Models")), we have,

|  |  |  |  |
| --- | --- | --- | --- |
|  |  | supgâˆˆğ’¢ğ”¼â€‹[sup0â‰¤sâ‰¤T|Î“â€‹gâ€‹(Xs)âˆ’âˆ‘k=1N1a~kâ€‹(ğ’ˆ)â€‹q~kâ€‹(Xs)|]\displaystyle\displaystyle\sup\_{g\in\mathcal{G}}\mathbb{E}\bigg[\sup\_{0\leq s\leq T}\left|\Gamma g(X\_{s})-\sum\_{k=1}^{N\_{1}}\widetilde{a}\_{k}(\bm{g})\widetilde{q}\_{k}(X\_{s})\right|\bigg] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤\displaystyle\displaystyle\leq | supgâˆˆğ’¢ğ”¼â€‹[sup0â‰¤sâ‰¤T|Î“â€‹(g)â€‹(Xs)âˆ’âˆ‘k=1N1ğ–¥kâ€‹(Î“â€‹g)â€‹q~kâ€‹(Xs)|]\displaystyle\displaystyle\sup\_{g\in\mathcal{G}}\mathbb{E}\bigg[\sup\_{0\leq s\leq T}\left|\Gamma(g)(X\_{s})-\sum\_{k=1}^{N\_{1}}\mathsf{F}\_{k}(\Gamma g)\widetilde{q}\_{k}(X\_{s})\right|\bigg] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +supgâˆˆğ’¢ğ”¼â€‹[sup0â‰¤sâ‰¤T|âˆ‘k=1N1ğ–¥kâ€‹(Î“â€‹g)â€‹q~kâ€‹(Xs)âˆ’âˆ‘k=1N1a~kâ€‹(ğ’ˆ)â€‹q~kâ€‹(Xs)|]\displaystyle\displaystyle+\sup\_{g\in\mathcal{G}}\mathbb{E}\bigg[\sup\_{0\leq s\leq T}\left|\sum\_{k=1}^{N\_{1}}\mathsf{F}\_{k}(\Gamma g)\widetilde{q}\_{k}(X\_{s})-\sum\_{k=1}^{N\_{1}}\widetilde{a}\_{k}(\bm{g})\widetilde{q}\_{k}(X\_{s})\right|\bigg] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤\displaystyle\displaystyle\leq | supgâˆˆğ’¢ğ”¼â€‹[sup0â‰¤sâ‰¤T|Î“â€‹(g)â€‹(Xs)âˆ’âˆ‘k=1N1ğ–¥kâ€‹(Î“â€‹g)â€‹q~kâ€‹(Xs)|2]1/2\displaystyle\displaystyle\sup\_{g\in\mathcal{G}}\mathbb{E}\bigg[\sup\_{0\leq s\leq T}\left|\Gamma(g)(X\_{s})-\sum\_{k=1}^{N\_{1}}\mathsf{F}\_{k}(\Gamma g)\widetilde{q}\_{k}(X\_{s})\right|^{2}\bigg]^{1/2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +supgâˆˆğ’¢ğ”¼â€‹[sup0â‰¤sâ‰¤T|âˆ‘k=1N1ğ–¥kâ€‹(Î“â€‹g)â€‹q~kâ€‹(Xs)âˆ’âˆ‘k=1Na~kâ€‹(ğ’ˆ)â€‹q~kâ€‹(Xs)|]\displaystyle\displaystyle+\sup\_{g\in\mathcal{G}}\mathbb{E}\bigg[\sup\_{0\leq s\leq T}\left|\sum\_{k=1}^{N\_{1}}\mathsf{F}\_{k}(\Gamma g)\widetilde{q}\_{k}(X\_{s})-\sum\_{k=1}^{N}\widetilde{a}\_{k}(\bm{g})\widetilde{q}\_{k}(X\_{s})\right|\bigg] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤\displaystyle\displaystyle\leq | Îµ1+N1â€‹Îµ2.\displaystyle\displaystyle\varepsilon\_{1}+N\_{1}\varepsilon\_{2}. |  |

Set Îµ2=Îµ1/(2â€‹N1),Îµ1=Îµ2\displaystyle\varepsilon\_{2}=\varepsilon\_{1}/(2N\_{1}),\varepsilon\_{1}=\frac{\varepsilon}{2},
it follows that Îµ2=Oâ€‹(Îµd2+1)\displaystyle\varepsilon\_{2}=O(\varepsilon^{d\_{2}+1}), we then have

|  |  |  |
| --- | --- | --- |
|  | supgâˆˆğ’¢ğ”¼â€‹[sup0â‰¤sâ‰¤T|Î“â€‹gâ€‹(Xs)âˆ’âˆ‘k=1N1a~kâ€‹(ğ’ˆ)â€‹q~kâ€‹(Xs)|]â‰¤Îµ.\displaystyle\displaystyle\sup\_{g\in\mathcal{G}}\mathbb{E}\bigg[\sup\_{0\leq s\leq T}\left|\Gamma g(X\_{s})-\sum\_{k=1}^{N\_{1}}\widetilde{a}\_{k}(\bm{g})\widetilde{q}\_{k}(X\_{s})\right|\bigg]\leq\varepsilon. |  |

By Remark [4.4](https://arxiv.org/html/2511.07235v1#S4.Thmtheorem4 "Remark 4.4. â€£ 4.2 Functional Approximations â€£ 4 Neural scaling of operator learning â€£ Deep Neural Operator Learning for Probabilistic Models"), the resulting network architectures have N2=Oâ€‹(Îµâˆ’d1â€‹d2âˆ’d1)\displaystyle N\_{2}=O(\varepsilon^{-d\_{1}d\_{2}-d\_{1}}), the number of basis H\displaystyle H needed is then estimated as
H=Oâ€‹(N212â€‹Îµ2âˆ’N2)=Oâ€‹(Îµâˆ’(d2+1)â€‹Îµâˆ’d1â€‹d2âˆ’d1)\displaystyle H=O(N\_{2}^{\frac{1}{2}}\varepsilon\_{2}^{-N\_{2}})=O(\varepsilon^{-(d\_{2}+1)\varepsilon^{-d\_{1}d\_{2}-d\_{1}}}), which determines the width of â„±2\displaystyle\mathcal{F}\_{2}, which is p2=H\displaystyle p\_{2}=H.
Consequently, the network size estimate follows,

|  |  |  |
| --- | --- | --- |
|  | â„’2=Oâ€‹(N22â€‹logâ¡N2+N22â€‹logâ¡(Îµâˆ’d2âˆ’1)),ğ”­2=Oâ€‹(N212â€‹Îµâˆ’N2â€‹d2âˆ’N2),\displaystyle\displaystyle\mathcal{L}\_{2}=O\left(N\_{2}^{2}\log N\_{2}+N\_{2}^{2}\log(\varepsilon^{-d\_{2}-1})\right),\ \mathfrak{p}\_{2}=O(N\_{2}^{\frac{1}{2}}\varepsilon^{-N\_{2}d\_{2}-N\_{2}}), |  |
|  |  |  |
| --- | --- | --- |
|  | K2=Oâ€‹(N212â€‹Îµâˆ’N2â€‹d2âˆ’N2â€‹(N22â€‹logâ¡N2+N22â€‹logâ¡(Îµâˆ’d2âˆ’1))),\displaystyle\displaystyle K\_{2}=O\left(N\_{2}^{\frac{1}{2}}\varepsilon^{-N\_{2}d\_{2}-N\_{2}}\left(N\_{2}^{2}\log N\_{2}+N\_{2}^{2}\log(\varepsilon^{-d\_{2}-1})\right)\right), |  |
|  |  |  |
| --- | --- | --- |
|  | Îº2=Oâ€‹(N2âˆ’N22â€‹Îµâˆ’N2â€‹d22âˆ’N2).\displaystyle\displaystyle\kappa\_{2}=O(N\_{2}^{-\frac{N\_{2}}{2}}\varepsilon^{-\frac{N\_{2}d\_{2}}{2}-N\_{2}}). |  |

âˆ

## 5 European Option Pricing Operator

After proving the universal approximation of the operator, we consider the following applications on European and American type option pricing problems in this section and the next section. The relationship of the functional, operator and the solution of the BSDE is decripted in the following table [5](https://arxiv.org/html/2511.07235v1#S5 "5 European Option Pricing Operator â€£ Deep Neural Operator Learning for Probabilistic Models").

|  |  |  |
| --- | --- | --- |
| Symbol | Meaning | Definition / Norm |
| ğ’¢\displaystyle\mathcal{G} | Input space (payoffs) | ğ”¼â€‹[sup0â‰¤sâ‰¤T|gâ€‹(Xs)|2]<âˆ\displaystyle\displaystyle\mathbb{E}\!\left[\sup\_{0\leq s\leq T}|g(X\_{s})|^{2}\right]<\infty |
| ğ’°\displaystyle\mathcal{U} | Output space (pricing functions) | ğ”¼â€‹[sup0â‰¤sâ‰¤T|uâ€‹(s,Xs)|2]<âˆ\displaystyle\displaystyle\mathbb{E}\!\left[\sup\_{0\leq s\leq T}|u(s,X\_{s})|^{2}\right]<\infty |
| Î“\displaystyle\Gamma | Pricing operator | Î“:ğ’¢â†’ğ’°,gâ†¦u=Î“â€‹(g)\displaystyle\Gamma:\mathcal{G}\to\mathcal{U},\quad g\mapsto u=\Gamma(g) |
| ğ–¥t,x\displaystyle\mathsf{F}\_{t,x} | Evaluation functional | ğ–¥t,xâ€‹(u)=uâ€‹(t,x)\displaystyle\mathsf{F}\_{t,x}(u)=u(t,x) |
| Yt\displaystyle Y\_{t} | BSDE solution | Yt=(ğ–¥t,Xtâˆ˜Î“)â€‹(g)\displaystyle Y\_{t}=(\mathsf{F}\_{t,X\_{t}}\circ\Gamma)(g) |

### 5.1 European option pricing

Let (Î©,â„±,{â„±t}tâˆˆ[0,T],â„š)\displaystyle(\Omega,\mathcal{F},\{\mathcal{F}\_{t}\}\_{t\in[0,T]},\mathbb{Q}) be a filtered probability space satisfying the usual conditions, carrying a d\displaystyle d-dimensional Brownian motion B=(B1,â€¦,Bd)\displaystyle B=(B^{1},\dots,B^{d}) under the risk-neutral measure â„š\displaystyle\mathbb{Q}.

The state process Xtâˆˆâ„d1\displaystyle X\_{t}\in\mathbb{R}^{d\_{1}} follows the diffusion

|  |  |  |  |
| --- | --- | --- | --- |
|  | dâ€‹Xt=bâ€‹(t,Xt)â€‹dâ€‹t+Ïƒâ€‹(t,Xt)â€‹dâ€‹Bt,X0=x,dX\_{t}=b(t,X\_{t})\,dt+\sigma(t,X\_{t})\,dB\_{t},\quad X\_{0}=x, |  | (5.1) |

where b:[0,T]Ã—â„d1â†’â„d1\displaystyle b:[0,T]\times\mathbb{R}^{d\_{1}}\to\mathbb{R}^{d\_{1}} and Ïƒ:[0,T]Ã—â„d1â†’â„d1Ã—d\displaystyle\sigma:[0,T]\times\mathbb{R}^{d\_{1}}\to\mathbb{R}^{d\_{1}\times d} are measurable, locally bounded, and Lipschitz in x\displaystyle x.
Let g:â„d1â†’â„\displaystyle g:\mathbb{R}^{d\_{1}}\to\mathbb{R} be the terminal payoff, such that gâ€‹(XT)âˆˆL2â€‹(â„š)\displaystyle g(X\_{T})\in L^{2}(\mathbb{Q}).
The price of the European option is

|  |  |  |  |
| --- | --- | --- | --- |
|  | uâ€‹(t,x)=ğ”¼â„šâ€‹[expâ¡(âˆ’âˆ«tTrâ€‹(s,Xs)â€‹ğ‘‘s)â€‹gâ€‹(XT)|Xt=x].u(t,x)=\mathbb{E}^{\mathbb{Q}}\!\left[\exp\!\left(-\int\_{t}^{T}r(s,X\_{s})\,ds\right)\,g(X\_{T})\,\big|\,X\_{t}=x\right]. |  | (5.2) |

where the risk free rate râ€‹(t,Xt)â‰¥0\displaystyle r(t,X\_{t})\geq 0.

Let â„’\displaystyle\mathcal{L} denote the infinitesimal generator of Xt\displaystyle X\_{t}:

|  |  |  |  |
| --- | --- | --- | --- |
|  | (â„’â€‹Ï•)â€‹(t,x)=âˆ‘i=1nbiâ€‹(t,x)â€‹âˆ‚xiÏ•â€‹(t,x)+12â€‹âˆ‘i,j=1naiâ€‹jâ€‹(t,x)â€‹âˆ‚xiâ€‹xj2Ï•â€‹(t,x).(\mathcal{L}\phi)(t,x)=\sum\_{i=1}^{n}b\_{i}(t,x)\partial\_{x\_{i}}\phi(t,x)+\frac{1}{2}\sum\_{i,j=1}^{n}a\_{ij}(t,x)\partial\_{x\_{i}x\_{j}}^{2}\phi(t,x). |  | (5.3) |

where aâ€‹(t,x)=Ïƒâ€‹(t,x)â€‹Ïƒâ€‹(t,x)âŠ¤\displaystyle a(t,x)=\sigma(t,x)\sigma(t,x)^{\top}.
Then uâ€‹(t,x)\displaystyle u(t,x) satisfies the PDE

|  |  |  |  |
| --- | --- | --- | --- |
|  | âˆ‚tu+â„’â€‹uâˆ’râ€‹u=0,uâ€‹(T,x)=gâ€‹(x).\partial\_{t}u+\mathcal{L}u-ru=0,\qquad u(T,x)=g(x). |  | (5.4) |

In addition, from a probablistic point of view, the price process (Yt,Zt)\displaystyle(Y\_{t},Z\_{t}) satisfies the backward stochastic differential equation (BSDE):

|  |  |  |  |
| --- | --- | --- | --- |
|  | Yt=gâ€‹(XT)âˆ’âˆ«tTrâ€‹(s,Xs)â€‹Ysâ€‹ğ‘‘sâˆ’âˆ«tTZsâ€‹ğ‘‘Bs,Y\_{t}=g(X\_{T})-\int\_{t}^{T}r(s,X\_{s})Y\_{s}\,ds-\int\_{t}^{T}Z\_{s}\,dB\_{s}, |  | (5.5) |

where Yt=uâ€‹(t,Xt)\displaystyle Y\_{t}=u(t,X\_{t}) and Zt=ÏƒâŠ¤â€‹(t,Xt)â€‹âˆ‡xuâ€‹(t,Xt)\displaystyle Z\_{t}=\sigma^{\top}(t,X\_{t})\nabla\_{x}u(t,X\_{t}).

As an example, in the case of Blackâ€“Scholes Model, for a single asset Xt\displaystyle X\_{t} with

|  |  |  |
| --- | --- | --- |
|  | dâ€‹Xt=Xtâ€‹(râ€‹dâ€‹t+Ïƒâ€‹dâ€‹Bt),dX\_{t}=X\_{t}(r\,dt+\sigma\,dB\_{t}), |  |

and payoff gâ€‹(XT)\displaystyle g(X\_{T}), the PDE reduces to

|  |  |  |  |
| --- | --- | --- | --- |
|  | âˆ‚tu+12â€‹Ïƒ2â€‹x2â€‹uxâ€‹x+(râˆ’q)â€‹xâ€‹uxâˆ’râ€‹u=0,uâ€‹(T,x)=gâ€‹(x).\partial\_{t}u+\frac{1}{2}\sigma^{2}x^{2}u\_{xx}+(r-q)xu\_{x}-ru=0,\quad u(T,x)=g(x). |  | (5.6) |

###### Theorem 5.1 (Lipschitz continuity of the European pricing operator in S2\displaystyle S^{2}).

Let (Î©,â„±,{â„±t}tâˆˆ[0,T],â„š)\displaystyle(\Omega,\mathcal{F},\{\mathcal{F}\_{t}\}\_{t\in[0,T]},\mathbb{Q}) support a d\displaystyle d-dimensional
Brownian motion B\displaystyle B, and let the state process X\displaystyle X solve

|  |  |  |  |
| --- | --- | --- | --- |
|  | dâ€‹Xt=bâ€‹(t,Xt)â€‹dâ€‹t+Ïƒâ€‹(t,Xt)â€‹dâ€‹Wt,X0=x,\displaystyle\displaystyle dX\_{t}=b(t,X\_{t})\,dt+\sigma(t,X\_{t})\,dW\_{t},\qquad X\_{0}=x, |  | (5.7) |

with b,Ïƒ\displaystyle b,\sigma Lipschitz in x\displaystyle x and of linear growth. Let the risk free rate
râ€‹(t,Xt)\displaystyle r(t,X\_{t}) be bounded and nonnegative, with 0â‰¤râ€‹(t,x)â‰¤rÂ¯\displaystyle 0\leq r(t,x)\leq\bar{r}.
For any terminal payoff g:â„d1â†’â„\displaystyle g:\mathbb{R}^{d\_{1}}\to\mathbb{R} with
ğ”¼â€‹[|gâ€‹(XT)|2]<âˆ\displaystyle\mathbb{E}[|g(X\_{T})|^{2}]<\infty, let u=Î“Eâ€‹g\displaystyle u=\Gamma^{E}g denote the (unique) solution to

|  |  |  |
| --- | --- | --- |
|  | âˆ‚tu+â„’â€‹uâˆ’râ€‹u=0,uâ€‹(T,â‹…)=gâ€‹(â‹…),\partial\_{t}u+\mathcal{L}u-r\,u=0,\qquad u(T,\cdot)=g(\cdot), |  |

where â„’\displaystyle\mathcal{L} is the generator of X\displaystyle X.
Set Ytg:=uâ€‹(t,Xt)\displaystyle Y^{g}\_{t}:=u(t,X\_{t}) and define the S2\displaystyle S^{2}-norm
â€–Yâ€–S2:=(ğ”¼â€‹[sup0â‰¤tâ‰¤T|Yt|2])1/2\displaystyle\|Y\|\_{S^{2}}:=\big(\mathbb{E}[\sup\_{0\leq t\leq T}|Y\_{t}|^{2}]\big)^{1/2}.

Then for any two terminal payoffs g1,g2\displaystyle g\_{1},g\_{2}, the operator Î“E\displaystyle\Gamma^{E} satisfies the following condition,

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[sup0â‰¤tâ‰¤T|Î“Eâ€‹(g1)â€‹(t,Xt)âˆ’Î“Eâ€‹(g2)â€‹(t,Xt)|2]â‰¤Lâ€‹ğ”¼â€‹[sup0â‰¤tâ‰¤T|g1â€‹(t,Xt)âˆ’g2â€‹(t,Xt)|2],\mathbb{E}\!\left[\sup\_{0\leq t\leq T}\!\big|\Gamma^{E}(g\_{1})(t,X\_{t})-\Gamma^{E}(g\_{2})(t,X\_{t})\big|^{2}\right]\;\leq\;L\,\mathbb{E}\!\left[\sup\_{0\leq t\leq T}\!\big|g\_{1}(t,X\_{t})-g\_{2}(t,X\_{t})\big|^{2}\right], |  |

with Lipschitz constant L=4â€‹e2â€‹rÂ¯â€‹T\displaystyle L=4e^{2\bar{r}T}. Hence the pricing operator Î“\displaystyle\Gamma is Lipschitz on S2\displaystyle S^{2}.

###### Proof.

By Feynmanâ€“Kac, for each g\displaystyle g we have (under â„š\displaystyle\mathbb{Q})

|  |  |  |
| --- | --- | --- |
|  | Î“E(g):=Ytg=u(t,Xt)=ğ”¼[exp(âˆ’âˆ«tTr(s,Xs)ds)g(XT)|â„±t].\Gamma^{E}(g):=Y^{g}\_{t}=u(t,X\_{t})=\mathbb{E}\!\left[\exp\!\Big(-\!\int\_{t}^{T}r(s,X\_{s})\,ds\Big)\,g(X\_{T})\,\middle|\,\mathcal{F}\_{t}\right]. |  |

Fix g1,g2\displaystyle g\_{1},g\_{2} and write Î”â€‹g:=g1âˆ’g2\displaystyle\Delta g:=g\_{1}-g\_{2},
Î”â€‹Yt:=Ytg1âˆ’Ytg2\displaystyle\Delta Y\_{t}:=Y^{g\_{1}}\_{t}-Y^{g\_{2}}\_{t}.
Then

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î”â€‹Yt=\displaystyle\displaystyle\Delta Y\_{t}= | ğ”¼[exp(âˆ’âˆ«tTr(s,Xs)ds)Î”g(XT)|â„±t]\displaystyle\displaystyle\mathbb{E}\!\left[\exp\!\Big(-\!\int\_{t}^{T}r(s,X\_{s})\,ds\Big)\,\Delta g(X\_{T})\,\middle|\,\mathcal{F}\_{t}\right] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | =\displaystyle\displaystyle= | exp(âˆ«0tr(s,Xs)ds)ğ”¼[exp(âˆ’âˆ«0Tr(s,Xs)ds)Î”g(XT)|â„±t]\displaystyle\displaystyle\exp\!\Big(\!\int\_{0}^{t}r(s,X\_{s})\,ds\Big)\,\mathbb{E}\!\left[\exp\!\Big(-\!\int\_{0}^{T}r(s,X\_{s})\,ds\Big)\Delta g(X\_{T})\,\middle|\,\mathcal{F}\_{t}\right] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | =\displaystyle\displaystyle= | expâ¡(âˆ«0trâ€‹(s,Xs)â€‹ğ‘‘s)â€‹Mt,\displaystyle\displaystyle\exp\!\Big(\!\int\_{0}^{t}r(s,X\_{s})\,ds\Big)\,M\_{t}, |  |

where Mt:=ğ”¼[exp(âˆ’âˆ«0Tr(s,Xs)ds)Î”g(XT)|â„±t]\displaystyle M\_{t}:=\mathbb{E}\!\left[\exp\!\Big(-\!\int\_{0}^{T}r(s,X\_{s})\,ds\Big)\Delta g(X\_{T})\,\middle|\,\mathcal{F}\_{t}\right] is a squareâ€‘integrable
martingale.

Because 0â‰¤râ‰¤rÂ¯\displaystyle 0\leq r\leq\bar{r}, we have
sup0â‰¤tâ‰¤Texpâ¡(âˆ«0trâ€‹(s,Xs)â€‹ğ‘‘s)â‰¤erÂ¯â€‹T.\displaystyle\sup\_{0\leq t\leq T}\exp\!\big(\!\int\_{0}^{t}r(s,X\_{s})\,ds\big)\leq e^{\bar{r}T}.
Therefore

|  |  |  |
| --- | --- | --- |
|  | sup0â‰¤tâ‰¤T|Î”â€‹Yt|â‰¤erÂ¯â€‹Tâ€‹sup0â‰¤tâ‰¤T|Mt|.\sup\_{0\leq t\leq T}|\Delta Y\_{t}|\leq e^{\bar{r}T}\,\sup\_{0\leq t\leq T}|M\_{t}|. |  |

Taking expectations and applying Doobâ€™s inequality for martingales,

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â€‹[sup0â‰¤tâ‰¤T|Î”â€‹Yt|2]â‰¤\displaystyle\displaystyle\mathbb{E}\!\left[\sup\_{0\leq t\leq T}|\Delta Y\_{t}|^{2}\right]\leq | e2â€‹rÂ¯â€‹Tâ€‹ğ”¼â€‹[sup0â‰¤tâ‰¤T|Mt|2]\displaystyle\displaystyle e^{2\bar{r}T}\,\mathbb{E}\!\left[\sup\_{0\leq t\leq T}|M\_{t}|^{2}\right] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤\displaystyle\displaystyle\leq | 4â€‹e2â€‹rÂ¯â€‹Tâ€‹ğ”¼â€‹[|MT|2]=4â€‹e2â€‹rÂ¯â€‹Tâ€‹ğ”¼â€‹[expâ¡(âˆ«0Tâˆ’2â€‹râ€‹(s,Xs)â€‹dâ€‹s)â€‹|Î”â€‹gâ€‹(XT)|2].\displaystyle\displaystyle 4e^{2\bar{r}T}\,\mathbb{E}\!\left[|M\_{T}|^{2}\right]=4e^{2\bar{r}T}\,\mathbb{E}\!\left[\exp\!\Big(\!\int\_{0}^{T}-2r(s,X\_{s})\,ds\Big)\,|\Delta g(X\_{T})|^{2}\right]. |  |

Since râ‰¥0\displaystyle r\geq 0, expâ¡(âˆ«0Tâˆ’2â€‹râ€‹(s,Xs)â€‹dâ€‹s)â‰¤1\displaystyle\exp\!\Big(\!\int\_{0}^{T}-2r(s,X\_{s})\,ds\Big)\leq 1 a.s., hence

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[sup0â‰¤tâ‰¤T|Î”â€‹Yt|2]â‰¤4â€‹e2â€‹rÂ¯â€‹Tâ€‹ğ”¼â€‹[|Î”â€‹gâ€‹(XT)|2]â‰¤4â€‹e2â€‹rÂ¯â€‹Tâ€‹ğ”¼â€‹[sup0â‰¤tâ‰¤T|Î”â€‹gâ€‹(t,Xt)|2].\mathbb{E}\!\left[\sup\_{0\leq t\leq T}|\Delta Y\_{t}|^{2}\right]\leq 4e^{2\bar{r}T}\,\mathbb{E}\!\left[|\Delta g(X\_{T})|^{2}\right]\leq 4e^{2\bar{r}T}\,\mathbb{E}\!\left[\sup\_{0\leq t\leq T}|\Delta g(t,X\_{t})|^{2}\right]. |  |

âˆ

Next we verify the tail probability Assumption [2](https://arxiv.org/html/2511.07235v1#Thmassumption2 "Assumption 2. â€£ 2 Assumptions â€£ Deep Neural Operator Learning for Probabilistic Models"). According to
[[2](https://arxiv.org/html/2511.07235v1#bib.bib2)] and [[11](https://arxiv.org/html/2511.07235v1#bib.bib11)], we have the following estimate.

###### Proposition 5.2.

The solution Xt\displaystyle X\_{t} for
the equation ([5.7](https://arxiv.org/html/2511.07235v1#S5.E7 "In Theorem 5.1 (Lipschitz continuity of the European pricing operator in ğ‘†Â²). â€£ 5.1 European option pricing â€£ 5 European Option Pricing Operator â€£ Deep Neural Operator Learning for Probabilistic Models"))
has the following tail probability,

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„™â€‹(suptâˆˆ[0,T]|Xtâˆ’x0|â‰¥r)â‰¤expâ¡(âˆ’câ€‹r2T),\displaystyle\displaystyle\mathbb{P}(\sup\_{t\in[0,T]}|X\_{t}-x\_{0}|\geq r)\leq\exp(-\frac{cr^{2}}{T}), |  | (5.8) |

for some constants c>0\displaystyle c>0, and r>0\displaystyle r>0.

###### Remark 5.3.

For SDEs driven by fractional Brownian motion, such estimates are proved in [[4](https://arxiv.org/html/2511.07235v1#bib.bib4)].
For rough differential equations, the corresponding estimates are established in [[16](https://arxiv.org/html/2511.07235v1#bib.bib16)].

## 6 Deep neural operator for American option pricing and PDE with free boundary

Given a Markov process Xt\displaystyle X\_{t} and exercise payoff gâ€‹(t,Xt)\displaystyle g(t,X\_{t}), the price of an American option is the value function

|  |  |  |  |
| --- | --- | --- | --- |
|  | uâ€‹(t,x)=supÏ„âˆˆğ’¯t,Tğ”¼â„šâ€‹[expâ¡(âˆ’âˆ«tÏ„râ€‹(s,Xs)â€‹ğ‘‘s)â€‹gâ€‹(Ï„,XÏ„)|Xt=x],u(t,x)=\sup\_{\tau\in\mathcal{T}\_{t,T}}\mathbb{E}^{\mathbb{Q}}\!\left[\exp\!\left(-\int\_{t}^{\tau}r(s,X\_{s})\,ds\right)\,g(\tau,X\_{\tau})\Big|X\_{t}=x\right], |  | (6.1) |

where ğ’¯t,T\displaystyle\mathcal{T}\_{t,T} is the set of stopping times with values in [t,T]\displaystyle[t,T]. Thanks to [[15](https://arxiv.org/html/2511.07235v1#bib.bib15)], the triple (Y,Z,K)\displaystyle(Y,Z,K) satisfies the *reflected backward SDE*:

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | Yt\displaystyle Y\_{t} | =gâ€‹(T,XT)âˆ’âˆ«tTrâ€‹(s,Xs)â€‹Ysâ€‹ğ‘‘s+KTâˆ’Ktâˆ’âˆ«tTZsâ€‹ğ‘‘Ws,\displaystyle=g(T,X\_{T})-\int\_{t}^{T}r(s,X\_{s})Y\_{s}\,ds+K\_{T}-K\_{t}-\int\_{t}^{T}Z\_{s}\,dW\_{s}, |  | (6.2) |
|  | Yt\displaystyle Y\_{t} | â‰¥gâ€‹(t,Xt),âˆ«0T(Ysâˆ’gâ€‹(s,Xs))â€‹ğ‘‘Ks=0,\displaystyle\geq g(t,X\_{t}),\quad\int\_{0}^{T}(Y\_{s}-g(s,X\_{s}))\,dK\_{s}=0, |  |

with (Y,Z,K)âˆˆğ’®2Ã—â„‹2Ã—ğ’œ2\displaystyle(Y,Z,K)\in\mathcal{S}^{2}\times\mathcal{H}^{2}\times\mathcal{A}^{2}. Here ğ’®2\displaystyle{\mathcal{S}}^{2} denotes squareâ€‘integrable adapted processes, â„‹2\displaystyle\mathcal{H}^{2} is
the predictable processes with squareâ€‘integrable norm, and ğ’œ2\displaystyle\mathcal{A}^{2}
the increasing, adapted, squareâ€‘integrable processes vanishing at 0.

The price uâ€‹(t,x)\displaystyle u(t,x) can also be formulated using variational inequality. If uâ€‹(t,x)\displaystyle u(t,x) is sufficiently smooth, it satisfies the obstacle problem:

|  |  |  |  |
| --- | --- | --- | --- |
|  | maxâ¡{âˆ‚tu+â„’â€‹uâˆ’râ€‹u+c,gâˆ’u}=0,uâ€‹(T,x)=gâ€‹(T,x).\max\!\left\{\partial\_{t}u+\mathcal{L}u-ru+c,\,g-u\right\}=0,\qquad u(T,x)=g(T,x). |  | (6.3) |

Our resutls thus also provide a deep neural operator approximation for PDE with free boundary.
For the special case of Blackâ€“Scholes American Option, a single asset St\displaystyle S\_{t} under the dynamic,

|  |  |  |
| --- | --- | --- |
|  | dâ€‹Xt=Xtâ€‹(râ€‹dâ€‹t+Ïƒâ€‹dâ€‹Wt).dX\_{t}=X\_{t}(r\,dt+\sigma\,dW\_{t}). |  |

The corresponding PDE becomes

|  |  |  |  |
| --- | --- | --- | --- |
|  | maxâ¡{âˆ‚tu+12â€‹Ïƒ2â€‹x2â€‹uxâ€‹x+râ€‹xâ€‹uxâˆ’râ€‹u,gâ€‹(x)âˆ’u}=0,uâ€‹(T,x)=gâ€‹(x).\max\!\left\{\partial\_{t}u+\frac{1}{2}\sigma^{2}x^{2}u\_{xx}+rxu\_{x}-ru,\;g(x)-u\right\}=0,\qquad u(T,x)=g(x). |  | (6.4) |

In what follows, we denote by Î“A\displaystyle\Gamma^{A}
the American option pricing operator associated with ([6.1](https://arxiv.org/html/2511.07235v1#S6.E1 "In 6 Deep neural operator for American option pricing and PDE with free boundary â€£ Deep Neural Operator Learning for Probabilistic Models")) and ([6.2](https://arxiv.org/html/2511.07235v1#S6.E2 "In 6 Deep neural operator for American option pricing and PDE with free boundary â€£ Deep Neural Operator Learning for Probabilistic Models")), equivalently the PDE free-boundary problem ([6.4](https://arxiv.org/html/2511.07235v1#S6.E4 "In 6 Deep neural operator for American option pricing and PDE with free boundary â€£ Deep Neural Operator Learning for Probabilistic Models")). We first show that Î“A\displaystyle\Gamma^{A} satisfies a Lipschitz condition.

###### Theorem 6.1.

Assume that ğ”¼â€‹[|sup0â‰¤tâ‰¤Tgâ€‹(Xt)|2]<âˆ\displaystyle\mathbb{E}\Big[\Big|\sup\_{0\leq t\leq T}g(X\_{t})\Big|^{2}\Big]<\infty, then we have

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[sup0â‰¤tâ‰¤T|Î“Aâ€‹(g1)tâˆ’Î“Aâ€‹(g2)t|2]â‰¤LÎ“â€‹ğ”¼â€‹[â€–g1â€‹(XT)âˆ’g2â€‹(XT)â€–2]+Cf,g1,g2â€‹(ğ”¼â€‹[|(g1âˆ’g2)Tâˆ—|2])1/2,\mathbb{E}\left[\sup\_{0\leq t\leq T}|\Gamma^{A}(g\_{1})\_{t}-\Gamma^{A}(g\_{2})\_{t}|^{2}\right]\leq L\_{\Gamma}\mathbb{E}\left[\|g\_{1}(X\_{T})-g\_{2}(X\_{T})\|^{2}\right]+C\_{f,g\_{1},g\_{2}}(\mathbb{E}\left[|(g\_{1}-g\_{2})^{\*}\_{T}|^{2}\right])^{1/2}, |  |

where we denote Î“Aâ€‹(gi)t=Yti\displaystyle\Gamma^{A}(g\_{i})\_{t}=Y^{i}\_{t} as the solution for ([6.2](https://arxiv.org/html/2511.07235v1#S6.E2 "In 6 Deep neural operator for American option pricing and PDE with free boundary â€£ Deep Neural Operator Learning for Probabilistic Models")) with terminal and boundary function gi\displaystyle g\_{i}.

###### Proof.

According to [[45](https://arxiv.org/html/2511.07235v1#bib.bib45)][Theorem 6.2.3] with same generator fâ€‹(Y,Z)=râ€‹Y\displaystyle f(Y,Z)=rY in the BSDE ([6.2](https://arxiv.org/html/2511.07235v1#S6.E2 "In 6 Deep neural operator for American option pricing and PDE with free boundary â€£ Deep Neural Operator Learning for Probabilistic Models")), we first have the following estimates,

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[sup0â‰¤tâ‰¤T|Î“Aâ€‹(g1)tâˆ’Î“Aâ€‹(g2)t|2]â‰¤LÎ“â€‹ğ”¼â€‹[â€–g1â€‹(XT)âˆ’g2â€‹(XT)â€–2]+Câ€‹(I1+I2)â€‹ğ”¼â€‹[(sup0â‰¤tâ‰¤T|g1â€‹(Xt)âˆ’g2â€‹(Xt)|)2]1/2â‰¤[LÎ“+Câ€‹(I1+I2)]â€‹(ğ”¼â€‹[(sup0â‰¤tâ‰¤T|g1â€‹(Xt)âˆ’g2â€‹(Xt)|)2])1/2,\begin{split}\mathbb{E}\left[\sup\_{0\leq t\leq T}|\Gamma^{A}(g\_{1})\_{t}-\Gamma^{A}(g\_{2})\_{t}|^{2}\right]&\leq L\_{\Gamma}\mathbb{E}[\|g\_{1}(X\_{T})-g\_{2}(X\_{T})\|^{2}]\\ &+C(I\_{1}+I\_{2})\mathbb{E}[(\sup\_{0\leq t\leq T}|g\_{1}(X\_{t})-g\_{2}(X\_{t})|)^{2}]^{1/2}\\ &\leq[L\_{\Gamma}+C(I\_{1}+I\_{2})](\mathbb{E}[(\sup\_{0\leq t\leq T}|g\_{1}(X\_{t})-g\_{2}(X\_{t})|)^{2}])^{1/2},\end{split} |  |

where

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ii:=ğ”¼[|gi(XT)|2+(âˆ«0T|fti(0,0)|dt)2+|(sup0â‰¤tâ‰¤T(gi(Xt))+|2].I\_{i}:=\mathbb{E}[|g\_{i}(X\_{T})|^{2}+(\int\_{0}^{T}|f^{i}\_{t}(0,0)|dt)^{2}+|(\sup\_{0\leq t\leq T}(g\_{i}(X\_{t}))^{+}|^{2}]. |  | (6.5) |

Denote Cg1,g2\displaystyle C\_{g\_{1},g\_{2}} as the constant depending on g1,g2,LÎ“,f\displaystyle g\_{1},g\_{2},L\_{\Gamma},f, we conclude the proof.
âˆ

### 6.1 Operator approximation for American option pricing operator

In this section, we generalize the operator approximation framework from Section [4](https://arxiv.org/html/2511.07235v1#S4 "4 Neural scaling of operator learning â€£ Deep Neural Operator Learning for Probabilistic Models") to encompass a wider class of operators. The extension is based on a Lipschitz assumption motivated by Theorem [6.1](https://arxiv.org/html/2511.07235v1#S6.Thmtheorem1 "Theorem 6.1. â€£ 6 Deep neural operator for American option pricing and PDE with free boundary â€£ Deep Neural Operator Learning for Probabilistic Models"), which naturally arises in the study of American option pricing problems, reflected FBSDEs, and PDEs with free boundary conditions.

###### Assumption 7.

Assume the operator

|  |  |  |
| --- | --- | --- |
|  | Î“A:ğ’¢âŸ¶ğ’°,gâŸ¼u=Î“Aâ€‹(g),\Gamma^{A}:\mathcal{G}\longrightarrow\mathcal{U},\qquad g\longmapsto u=\Gamma^{A}(g), |  |

from ğ’¢\displaystyle\mathcal{G} to ğ’°\displaystyle\mathcal{U} is Lipschitz if : there exists LÎ“A\displaystyle L\_{\Gamma^{A}} such that for any g1,g2âˆˆğ’¢\displaystyle g\_{1},g\_{2}\in\mathcal{G}, we have

|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ğ”¼â€‹[sup0â‰¤tâ‰¤T|Î“Aâ€‹(g1)â€‹(Xt)âˆ’Î“Aâ€‹(g2)â€‹(Xt)|2]\displaystyle\displaystyle\mathbb{E}\!\left[\sup\_{0\leq t\leq T}|\Gamma^{A}(g\_{1})(X\_{t})-\Gamma^{A}(g\_{2})(X\_{t})|^{2}\right] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤\displaystyle\displaystyle\leq | LÎ“A2â€‹(ğ”¼â€‹[sup0â‰¤tâ‰¤T|g1â€‹(Xt)âˆ’g2â€‹(Xt)|2]+(ğ”¼â€‹[sup0â‰¤tâ‰¤T|g1â€‹(Xt)âˆ’g2â€‹(Xt)|2])1/2).\displaystyle\displaystyle L\_{\Gamma\_{A}}^{2}\left(\,\mathbb{E}\!\left[\sup\_{0\leq t\leq T}|g\_{1}(X\_{t})-g\_{2}(X\_{t})|^{2}\right]+\left(\mathbb{E}\!\left[\sup\_{0\leq t\leq T}|g\_{1}(X\_{t})-g\_{2}(X\_{t})|^{2}\right]\right)^{1/2}\right). |  |

for all g1,g2âˆˆğ’¢\displaystyle g\_{1},g\_{2}\in\mathcal{G}.
Or equivalently,

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–Î“Aâ€‹(g1)âˆ’Î“Aâ€‹(g2)â€–S22â‰¤LÎ“A2â€‹(â€–g1âˆ’g2â€–S2+â€–g1âˆ’g2â€–S22).\displaystyle\displaystyle\|\Gamma^{A}(g\_{1})-\Gamma^{A}(g\_{2})\|\_{S^{2}}^{2}\leq L\_{\Gamma\_{A}}^{2}(\|g\_{1}-g\_{2}\|\_{S^{2}}+\|g\_{1}-g\_{2}\|\_{S^{2}}^{2}). |  | (6.6) |

For notation simplicity, we denote â€–g1âˆ’g2â€–S21,2:=â€–g1âˆ’g2â€–S2+â€–g1âˆ’g2â€–S22\displaystyle\|g\_{1}-g\_{2}\|\_{S^{2}}^{1,2}:=\|g\_{1}-g\_{2}\|\_{S^{2}}+\|g\_{1}-g\_{2}\|\_{S^{2}}^{2}.

We next prove the operator approximation under Assumption [7](https://arxiv.org/html/2511.07235v1#Thmassumption7 "Assumption 7. â€£ 6.1 Operator approximation for American option pricing operator â€£ 6 Deep neural operator for American option pricing and PDE with free boundary â€£ Deep Neural Operator Learning for Probabilistic Models").

###### Theorem 6.2.

[American Option Pricing Operator]
Let Assumptions [1](https://arxiv.org/html/2511.07235v1#Thmassumption1 "Assumption 1. â€£ 2 Assumptions â€£ Deep Neural Operator Learning for Probabilistic Models"), [2](https://arxiv.org/html/2511.07235v1#Thmassumption2 "Assumption 2. â€£ 2 Assumptions â€£ Deep Neural Operator Learning for Probabilistic Models"), [3](https://arxiv.org/html/2511.07235v1#Thmassumption3 "Assumption 3. â€£ 2 Assumptions â€£ Deep Neural Operator Learning for Probabilistic Models"), [4](https://arxiv.org/html/2511.07235v1#Thmassumption4 "Assumption 4. â€£ 2 Assumptions â€£ Deep Neural Operator Learning for Probabilistic Models"), and [7](https://arxiv.org/html/2511.07235v1#Thmassumption7 "Assumption 7. â€£ 6.1 Operator approximation for American option pricing operator â€£ 6 Deep neural operator for American option pricing and PDE with free boundary â€£ Deep Neural Operator Learning for Probabilistic Models") hold.
For any Îµ>0\displaystyle\varepsilon>0,
set N=Oâ€‹(Îµâˆ’2â€‹d2)\displaystyle N=O\left(\varepsilon^{-2d\_{2}}\right), and NÎ´=Oâ€‹(Îµâˆ’4â€‹d1â€‹d2âˆ’2â€‹d1)\displaystyle N^{\delta}=O(\varepsilon^{-4d\_{1}d\_{2}-2d\_{1}}).
Define the network architecture â„±1=â„±NNâ€‹(d2,1,L1,p1,K1,Îº1,R1)\displaystyle\mathcal{F}\_{1}=\mathcal{F}\_{\rm NN}(d\_{2},1,L\_{1},p\_{1},K\_{1},\kappa\_{1},R\_{1}) and â„±2=â„±NNâ€‹(NÎ´,1,L2,p2,K2,Îº2,R2)\displaystyle\mathcal{F}\_{2}=\mathcal{F}\_{\rm NN}(N^{\delta},1,L\_{2},p\_{2},K\_{2},\kappa\_{2},R\_{2}) with

|  |  |  |
| --- | --- | --- |
|  | â„’1=Oâ€‹((12â€‹d2+2â€‹d22)â€‹logâ¡(Îµ1âˆ’2)),ğ”­1=Oâ€‹(1),K1=Oâ€‹((12â€‹d2+2â€‹d22)â€‹logâ¡(Îµ1âˆ’2)),Îº1=Oâ€‹(Îµ1âˆ’2â€‹d2),R1=1.\displaystyle\displaystyle\mathcal{L}\_{1}=O\left((\frac{1}{2}d\_{2}+2d\_{2}^{2})\log(\varepsilon\_{1}^{-2})\right),\mathfrak{p}\_{1}=O(1),\ K\_{1}=O\left((\frac{1}{2}d\_{2}+2d\_{2}^{2})\log(\varepsilon\_{1}^{-2})\right),\kappa\_{1}=O\left(\varepsilon\_{1}^{-2d\_{2}}\right),R\_{1}=1. |  |

and,

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„’2\displaystyle\displaystyle\mathcal{L}\_{2} | =Oâ€‹((NÎ´+12â€‹(NÎ´)2)â€‹logâ¡(NÎ´)+(2â€‹(NÎ´)2+NÎ´)â€‹logâ¡Îµâˆ’(2â€‹d2+1)+NÎ´â€‹logâ¡(r~)),ğ”­2=Oâ€‹(Îµ2âˆ’(4â€‹d1âˆ’2)â€‹NÎ´),\displaystyle\displaystyle=O\left((N^{\delta}+\frac{1}{2}(N^{\delta})^{2})\log(N^{\delta})+(2(N^{\delta})^{2}+N^{\delta})\log\varepsilon^{-(2d\_{2}+1)}+N^{\delta}\log(\widetilde{r})\right),\mathfrak{p}\_{2}=O(\varepsilon\_{2}^{-(4d\_{1}-2)N^{\delta}}), |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | K2\displaystyle\displaystyle K\_{2} | =Oâ€‹((NÎ´+12â€‹(NÎ´)2)â€‹logâ¡(NÎ´)+(2â€‹(NÎ´)2+NÎ´)â€‹logâ¡Îµâˆ’2â€‹(d2+1)+NÎ´â€‹logâ¡(r~)),\displaystyle\displaystyle=O\left((N^{\delta}+\frac{1}{2}(N^{\delta})^{2})\log(N^{\delta})+(2(N^{\delta})^{2}+N^{\delta})\log\varepsilon^{-2(d\_{2}+1)}+N^{\delta}\log(\widetilde{r})\right), |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | Îº2\displaystyle\displaystyle\kappa\_{2} | =Oâ€‹(Îµâˆ’NÎ´â€‹(2â€‹d2+1)),R2=1,\displaystyle\displaystyle=O(\varepsilon^{-N^{\delta}(2d\_{2}+1)}),R\_{2}=1, |  |

where r~\displaystyle\widetilde{r} is a constant.
For any operator Î“A:ğ’¢â†’ğ’°\displaystyle\Gamma^{A}:\mathcal{G}\rightarrow\mathcal{U} satisfying Assumption [7](https://arxiv.org/html/2511.07235v1#Thmassumption7 "Assumption 7. â€£ 6.1 Operator approximation for American option pricing operator â€£ 6 Deep neural operator for American option pricing and PDE with free boundary â€£ Deep Neural Operator Learning for Probabilistic Models"), there are {q~k}k=1N1\displaystyle\{\widetilde{q}\_{k}\}\_{k=1}^{N\_{1}} with q~kâˆˆâ„±1\displaystyle\widetilde{q}\_{k}\in\mathcal{F}\_{1} and {a~k}k=1N1\displaystyle\{\widetilde{a}\_{k}\}\_{k=1}^{N\_{1}} with a~kâˆˆâ„±2\displaystyle\widetilde{a}\_{k}\in\mathcal{F}\_{2} such that

|  |  |  |  |
| --- | --- | --- | --- |
|  | supgâˆˆğ’¢ğ”¼â€‹[sup0â‰¤tâ‰¤T|Î“Aâ€‹(g)â€‹(Xt)âˆ’âˆ‘k=1N1a~kâ€‹(ğ’ˆ)â€‹q~kâ€‹(Xt)|]â‰¤Îµ.\displaystyle\displaystyle\sup\_{g\in\mathcal{G}}\mathbb{E}\bigg[\sup\_{0\leq t\leq T}\left|\Gamma^{A}(g)(X\_{t})-\sum\_{k=1}^{N\_{1}}\widetilde{a}\_{k}(\bm{g})\widetilde{q}\_{k}(X\_{t})\right|\bigg]\leq\varepsilon. |  | (6.7) |

###### Proof of Theorem [6.2](https://arxiv.org/html/2511.07235v1#S6.Thmtheorem2 "Theorem 6.2. â€£ 6.1 Operator approximation for American option pricing operator â€£ 6 Deep neural operator for American option pricing and PDE with free boundary â€£ Deep Neural Operator Learning for Probabilistic Models").

We reproduce the function, functional and operator approximation under the new Assumption [7](https://arxiv.org/html/2511.07235v1#Thmassumption7 "Assumption 7. â€£ 6.1 Operator approximation for American option pricing operator â€£ 6 Deep neural operator for American option pricing and PDE with free boundary â€£ Deep Neural Operator Learning for Probabilistic Models").

Step 1 (function approximation): We first prove the function approximation under Assumption [7](https://arxiv.org/html/2511.07235v1#Thmassumption7 "Assumption 7. â€£ 6.1 Operator approximation for American option pricing operator â€£ 6 Deep neural operator for American option pricing and PDE with free boundary â€£ Deep Neural Operator Learning for Probabilistic Models") which is the building block for the rest of the proof. Following the proof of Theorem [4.1](https://arxiv.org/html/2511.07235v1#S4.Thmtheorem1 "Theorem 4.1. â€£ 4.1 Function Approximation â€£ 4 Neural scaling of operator learning â€£ Deep Neural Operator Learning for Probabilistic Models"), the only difference is that we need to estimate â„2\displaystyle\mathcal{I}\_{2} in the proof of Theorem [4.1](https://arxiv.org/html/2511.07235v1#S4.Thmtheorem1 "Theorem 4.1. â€£ 4.1 Function Approximation â€£ 4 Neural scaling of operator learning â€£ Deep Neural Operator Learning for Probabilistic Models"), which now has the following form due to the new Assumption [7](https://arxiv.org/html/2511.07235v1#Thmassumption7 "Assumption 7. â€£ 6.1 Operator approximation for American option pricing operator â€£ 6 Deep neural operator for American option pricing and PDE with free boundary â€£ Deep Neural Operator Learning for Probabilistic Models").
In specific, the function approximation for the function Î“Aâ€‹(g)\displaystyle\Gamma^{A}(g) following the proof of Theorem [4.1](https://arxiv.org/html/2511.07235v1#S4.Thmtheorem1 "Theorem 4.1. â€£ 4.1 Function Approximation â€£ 4 Neural scaling of operator learning â€£ Deep Neural Operator Learning for Probabilistic Models") has the following form,

|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ğ”¼â€‹[sup0â‰¤tâ‰¤T|Î“Aâ€‹(g)â€‹(Xt)âˆ’âˆ‘k=1Nd2Î“Aâ€‹(g)â€‹(ğœk)â€‹Ï•ğœkâ€‹(Xt)|2]\displaystyle\displaystyle\mathbb{E}\Big[\sup\_{0\leq t\leq T}\Big|\Gamma^{A}(g)(X\_{t})-\sum\_{k=1}^{N^{d\_{2}}}\Gamma^{A}(g)(\mathbf{c}\_{k})\phi\_{\mathbf{c}\_{k}}(X\_{t})\Big|^{2}\Big] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | =\displaystyle\displaystyle= | ğ”¼â€‹[sup0â‰¤tâ‰¤T|Î“Aâ€‹(g)â€‹(Xt)|2â€‹ğŸÎ©rCâ€‹(Xt)]âŸâ„1A+ğ”¼â€‹[sup0â‰¤tâ‰¤T|Î“Aâ€‹(g)â€‹(Xt)âˆ’âˆ‘k=1Nd2Î“Aâ€‹(g)â€‹(ğœk)â€‹Ï•ğœkâ€‹(Xt)|2â€‹ğŸÎ©râ€‹(Xt)]âŸâ„2A.\displaystyle\displaystyle\underbrace{\mathbb{E}\Big[\sup\_{0\leq t\leq T}\Big|\Gamma^{A}(g)(X\_{t})\Big|^{2}\mathbf{1}\_{\Omega\_{r}^{C}}(X\_{t})\Big]}\_{\mathcal{I}\_{1}^{A}}+\underbrace{\mathbb{E}\Big[\sup\_{0\leq t\leq T}\Big|\Gamma^{A}(g)(X\_{t})-\sum\_{k=1}^{N^{d\_{2}}}\Gamma^{A}(g)(\mathbf{c}\_{k})\phi\_{\mathbf{c}\_{k}}(X\_{t})\Big|^{2}\mathbf{1}\_{\Omega\_{r}}(X\_{t})\Big]}\_{\mathcal{I}\_{2}^{A}}. |  |

The estimate of â„1A\displaystyle\mathcal{I}\_{1}^{A} follows the same as in ([4.4](https://arxiv.org/html/2511.07235v1#S4.E4 "In 4.1 Function Approximation â€£ 4 Neural scaling of operator learning â€£ Deep Neural Operator Learning for Probabilistic Models")). For the second term â„2A\displaystyle\mathcal{I}\_{2}^{A}, reproducing the estimates in ([4.5](https://arxiv.org/html/2511.07235v1#S4.E5 "In 4.1 Function Approximation â€£ 4 Neural scaling of operator learning â€£ Deep Neural Operator Learning for Probabilistic Models")) under Assumption [7](https://arxiv.org/html/2511.07235v1#Thmassumption7 "Assumption 7. â€£ 6.1 Operator approximation for American option pricing operator â€£ 6 Deep neural operator for American option pricing and PDE with free boundary â€£ Deep Neural Operator Learning for Probabilistic Models") for the function Î“Aâ€‹(g)âˆˆğ’°\displaystyle\Gamma^{A}(g)\in\mathcal{U}, we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„2A=\displaystyle\displaystyle\mathcal{I}\_{2}^{A}= | ğ”¼â€‹[sup0â‰¤tâ‰¤T|âˆ‘k=1Nd2[Î“Aâ€‹(g)â€‹(Xt)âˆ’Î“Aâ€‹(g)â€‹(ğœk)]â€‹Ï•ğœkâ€‹(Xt)|2â€‹ğŸÎ©râ€‹(Xt)]\displaystyle\displaystyle\mathbb{E}\Big[\sup\_{0\leq t\leq T}\Big|\sum\_{k=1}^{N^{d\_{2}}}[\Gamma^{A}(g)(X\_{t})-\Gamma^{A}(g)(\mathbf{c}\_{k})]\phi\_{\mathbf{c}\_{k}}(X\_{t})\Big|^{2}\mathbf{1}\_{\Omega\_{r}}(X\_{t})\Big] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤\displaystyle\displaystyle\leq | ğ”¼â€‹[sup0â‰¤tâ‰¤T(âˆ‘k=1Nd2|Î“Aâ€‹(g)â€‹(Xt)âˆ’Î“Aâ€‹(g)â€‹(ğœk)|â€‹|Ï•ğœkâ€‹(Xt)|)2â€‹ğŸÎ©râ€‹(Xt)]\displaystyle\displaystyle\mathbb{E}\Big[\sup\_{0\leq t\leq T}\Big(\sum\_{k=1}^{N^{d\_{2}}}|\Gamma^{A}(g)(X\_{t})-\Gamma^{A}(g)(\mathbf{c}\_{k})||\phi\_{\mathbf{c}\_{k}}(X\_{t})|\Big)^{2}\mathbf{1}\_{\Omega\_{r}}(X\_{t})\Big] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | =\displaystyle\displaystyle= | ğ”¼â€‹[sup0â‰¤tâ‰¤T(âˆ‘k:â€–ğœkâˆ’Xtâ€–âˆâ‰¤2â€‹r(Nâˆ’1)|Î“Aâ€‹(g)â€‹(Xt)âˆ’Î“Aâ€‹(g)â€‹(ğœk)|â€‹Ï•ğœkâ€‹(Xt))2â€‹ğŸÎ©râ€‹(Xt)],\displaystyle\displaystyle\mathbb{E}\Big[\sup\_{0\leq t\leq T}\Big(\sum\_{k:\|\mathbf{c}\_{k}-X\_{t}\|\_{\infty}\leq\frac{2r}{(N-1)}}|\Gamma^{A}(g)(X\_{t})-\Gamma^{A}(g)(\mathbf{c}\_{k})|\phi\_{\mathbf{c}\_{k}}(X\_{t})\Big)^{2}\mathbf{1}\_{\Omega\_{r}}(X\_{t})\Big], |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤\displaystyle\displaystyle\leq | ğ”¼â€‹[sup0â‰¤tâ‰¤T(maxk:â€–ğœkâˆ’Xtâ€–âˆâ‰¤2â€‹r(Nâˆ’1)â¡|Î“Aâ€‹(g)â€‹(Xt)âˆ’Î“Aâ€‹(g)â€‹(ğœk)|â€‹(âˆ‘k:â€–ğœkâˆ’Xtâ€–âˆâ‰¤2â€‹r(Nâˆ’1)Ï•ğœkâ€‹(Xt)))2â€‹ğŸÎ©râ€‹(Xt)]\displaystyle\displaystyle\mathbb{E}\Big[\sup\_{0\leq t\leq T}\Big(\max\_{k:\|\mathbf{c}\_{k}-X\_{t}\|\_{\infty}\leq\frac{2r}{(N-1)}}|\Gamma^{A}(g)(X\_{t})-\Gamma^{A}(g)(\mathbf{c}\_{k})|\Big(\sum\_{k:\|\mathbf{c}\_{k}-X\_{t}\|\_{\infty}\leq\frac{2r}{(N-1)}}\phi\_{\mathbf{c}\_{k}}(X\_{t})\Big)\Big)^{2}\mathbf{1}\_{\Omega\_{r}}(X\_{t})\Big] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤\displaystyle\displaystyle\leq | ğ”¼â€‹[sup0â‰¤tâ‰¤Tmaxk:â€–ğœkâˆ’Xtâ€–âˆâ‰¤2â€‹r(Nâˆ’1)â¡|Î“Aâ€‹(g)â€‹(Xt)âˆ’Î“Aâ€‹(g)â€‹(ğœk)|2â€‹ğŸÎ©râ€‹(Xt)]\displaystyle\displaystyle\mathbb{E}\Big[\sup\_{0\leq t\leq T}\max\_{k:\|\mathbf{c}\_{k}-X\_{t}\|\_{\infty}\leq\frac{2r}{(N-1)}}|\Gamma^{A}(g)(X\_{t})-\Gamma^{A}(g)(\mathbf{c}\_{k})|^{2}\mathbf{1}\_{\Omega\_{r}}(X\_{t})\Big] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤\displaystyle\displaystyle\leq | ğ”¼â€‹[sup0â‰¤tâ‰¤Tmaxk:â€–ğœkâˆ’Xtâ€–âˆâ‰¤2â€‹r(Nâˆ’1)â¡|Î“Aâ€‹(g)â€‹(Xt)âˆ’Î“Aâ€‹(g)â€‹(ğœk)|2â€‹ğŸÎ©râ€‹(Xt)]\displaystyle\displaystyle\mathbb{E}\Big[\sup\_{0\leq t\leq T}\max\_{k:\|\mathbf{c}\_{k}-X\_{t}\|\_{\infty}\leq\frac{2r}{(N-1)}}|\Gamma^{A}(g)(X\_{t})-\Gamma^{A}(g)(\mathbf{c}\_{k})|^{2}\mathbf{1}\_{\Omega\_{r}}(X\_{t})\Big] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤\displaystyle\displaystyle\leq | LÎ“A2(ğ”¼[sup0â‰¤tâ‰¤Tmaxk:â€–ğœkâˆ’Xtâ€–âˆâ‰¤2â€‹r(Nâˆ’1)|g(Xt)âˆ’g(ğœk)|2]\displaystyle\displaystyle L\_{\Gamma^{A}}^{2}\left(\,\mathbb{E}\!\left[\sup\_{0\leq t\leq T}\max\_{k:\|\mathbf{c}\_{k}-X\_{t}\|\_{\infty}\leq\frac{2r}{(N-1)}}|g(X\_{t})-g(\mathbf{c}\_{k})|^{2}\right]\right. |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +(ğ”¼[sup0â‰¤tâ‰¤Tmaxk:â€–ğœkâˆ’Xtâ€–âˆâ‰¤2â€‹r(Nâˆ’1)|g(Xt)âˆ’g(ğœk)|2])1/2)\displaystyle\displaystyle\quad\left.+\left(\mathbb{E}\!\left[\sup\_{0\leq t\leq T}\max\_{k:\|\mathbf{c}\_{k}-X\_{t}\|\_{\infty}\leq\frac{2r}{(N-1)}}|g(X\_{t})-g(\mathbf{c}\_{k})|^{2}\right]\right)^{1/2}\right) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤\displaystyle\displaystyle\leq | LÎ“A2â€‹((2â€‹râ€‹Lg)2â€‹d2(Nâˆ’1)2+((2â€‹râ€‹Lg)2â€‹d2(Nâˆ’1)2)1/2)\displaystyle\displaystyle L\_{\Gamma^{A}}^{2}\left(\frac{(2rL\_{g})^{2}d\_{2}}{(N-1)^{2}}+\left(\frac{(2rL\_{g})^{2}d\_{2}}{(N-1)^{2}}\right)^{1/2}\right) |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | â‰¤\displaystyle\displaystyle\leq | Câ€‹LÎ“A2â€‹2â€‹râ€‹Lgâ€‹d21/2(Nâˆ’1)â‰¤14Ã—Îµ124,\displaystyle\displaystyle CL\_{\Gamma^{A}}^{2}\frac{2rL\_{g}d\_{2}^{1/2}}{(N-1)}\leq\frac{1}{4}\times\frac{\varepsilon\_{1}^{2}}{4}, |  | (6.8) |

where we use the Lipschitz assumption of Î“A\displaystyle\Gamma^{A} from Assumption ([7](https://arxiv.org/html/2511.07235v1#Thmassumption7 "Assumption 7. â€£ 6.1 Operator approximation for American option pricing operator â€£ 6 Deep neural operator for American option pricing and PDE with free boundary â€£ Deep Neural Operator Learning for Probabilistic Models")), and ((2â€‹râ€‹Lg)2â€‹d2(Nâˆ’1)2)1/2\displaystyle\left(\frac{(2rL\_{g})^{2}d\_{2}}{(N-1)^{2}}\right)^{1/2} is the dominating term since (2â€‹râ€‹Lg)2â€‹d2(Nâˆ’1)2<1\displaystyle\frac{(2rL\_{g})^{2}d\_{2}}{(N-1)^{2}}<1 by choosing Îµ1\displaystyle\varepsilon\_{1} to be sufficiently small. Following the same idea and estimate in ([4.6](https://arxiv.org/html/2511.07235v1#S4.E6 "In 4.1 Function Approximation â€£ 4 Neural scaling of operator learning â€£ Deep Neural Operator Learning for Probabilistic Models")), in order to get the desired estimate in ([6.11](https://arxiv.org/html/2511.07235v1#S6.E11 "In 6.1 Operator approximation for American option pricing operator â€£ 6 Deep neural operator for American option pricing and PDE with free boundary â€£ Deep Neural Operator Learning for Probabilistic Models")), we make the following selection.

Up to a constant c\displaystyle c change, for Îµ1>0\displaystyle\varepsilon\_{1}>0 which will be specified,
set r=âŒˆâˆ’2â€‹CTcâ€‹logâ¡Îµ1216â€‹C0âŒ‰1Î±\displaystyle r=\left\lceil-\frac{2C\_{T}}{c}\log\frac{\varepsilon\_{1}^{2}}{16C\_{0}}\right\rceil^{\frac{1}{\alpha}}, and define Qr,Î©r\displaystyle Q\_{r},\Omega\_{r} as in Definition [2.2](https://arxiv.org/html/2511.07235v1#S2.Thmtheorem2 "Definition 2.2. â€£ 2 Assumptions â€£ Deep Neural Operator Learning for Probabilistic Models"),
there exists a constant N=Oâ€‹(râ€‹Lgâ€‹d112â€‹Îµ1âˆ’2)\displaystyle N=O\left(rL\_{g}d\_{1}^{\frac{1}{2}}\varepsilon\_{1}^{-2}\right) following from the designed estiamtes in ([6.8](https://arxiv.org/html/2511.07235v1#S6.E8 "In 6.1 Operator approximation for American option pricing operator â€£ 6 Deep neural operator for American option pricing and PDE with free boundary â€£ Deep Neural Operator Learning for Probabilistic Models")), for some constant C\displaystyle C depending on d2,Lg\displaystyle d\_{2},L\_{g} and r\displaystyle r, a network architecture â„±1=â„±NNâ€‹(d2,1,â„’1,ğ”­1,K1,Îº1,R1)\displaystyle\mathcal{F}\_{1}=\mathcal{F}\_{\rm NN}(d\_{2},1,\mathcal{L}\_{1},\mathfrak{p}\_{1},K\_{1},\kappa\_{1},R\_{1}) and {q~k}k=1N1\displaystyle\{\widetilde{q}\_{k}\}\_{k=1}^{N\_{1}} with q~kâˆˆâ„±1\displaystyle\widetilde{q}\_{k}\in\mathcal{F}\_{1}, and {ğœk}k=1N1âŠ‚Qr\displaystyle\{\mathbf{c}\_{k}\}\_{k=1}^{N\_{1}}\subset Q\_{r} such that for any gâˆˆğ’¢\displaystyle g\in\mathcal{G},
we separate the estimates into the following two parts,

|  |  |  |  |
| --- | --- | --- | --- |
|  | (ğ”¼â€‹[sup0â‰¤tâ‰¤T|Î“Aâ€‹(g)â€‹(Xt)âˆ’âˆ‘k=1N1Î“Aâ€‹(g)â€‹(ğœk)â€‹q~kâ€‹(Xt)|2])1/2â‰¤Îµ1/2.\displaystyle\displaystyle\left(\mathbb{E}\bigg[\sup\_{0\leq t\leq T}\left|\Gamma^{A}(g)(X\_{t})-\sum\_{k=1}^{N\_{1}}\Gamma^{A}(g)(\mathbf{c}\_{k})\widetilde{q}\_{k}(X\_{t})\right|^{2}\bigg]\right)^{1/2}\leq\varepsilon\_{1}/2. |  | (6.9) |

For the ease of the notation, we denote N1=Câ€‹Nd2\displaystyle N\_{1}=CN^{d\_{2}} for some constant C\displaystyle C.
Since Îµ1/2<1\displaystyle\varepsilon\_{1}/2<1, this further implies

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â€‹[sup0â‰¤tâ‰¤T|Î“Aâ€‹(g)â€‹(Xt)âˆ’âˆ‘k=1N1Î“Aâ€‹(g)â€‹(ğœk)â€‹q~kâ€‹(Xt)|2]â‰¤Îµ12/4â‰¤Îµ1/2.\displaystyle\displaystyle\mathbb{E}\bigg[\sup\_{0\leq t\leq T}\left|\Gamma^{A}(g)(X\_{t})-\sum\_{k=1}^{N\_{1}}\Gamma^{A}(g)(\mathbf{c}\_{k})\widetilde{q}\_{k}(X\_{t})\right|^{2}\bigg]\leq\varepsilon\_{1}^{2}/4\leq\varepsilon\_{1}/2. |  | (6.10) |

Combining the above two estimates, we get

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–Î“Aâ€‹(g)âˆ’âˆ‘k=1N1Î“Aâ€‹(g)â€‹(ğœk)â€‹q~kâ€–ğ’®21,2â‰¤Îµ1.\displaystyle\displaystyle\|\Gamma^{A}(g)-\sum\_{k=1}^{N\_{1}}\Gamma^{A}(g)(\mathbf{c}\_{k})\widetilde{q}\_{k}\|\_{\mathcal{S}\_{2}}^{1,2}\leq\varepsilon\_{1}. |  | (6.11) |

According to Theorem [4.1](https://arxiv.org/html/2511.07235v1#S4.Thmtheorem1 "Theorem 4.1. â€£ 4.1 Function Approximation â€£ 4 Neural scaling of operator learning â€£ Deep Neural Operator Learning for Probabilistic Models"), and the relations between ([6.9](https://arxiv.org/html/2511.07235v1#S6.E9 "In 6.1 Operator approximation for American option pricing operator â€£ 6 Deep neural operator for American option pricing and PDE with free boundary â€£ Deep Neural Operator Learning for Probabilistic Models")) and ([6.10](https://arxiv.org/html/2511.07235v1#S6.E10 "In 6.1 Operator approximation for American option pricing operator â€£ 6 Deep neural operator for American option pricing and PDE with free boundary â€£ Deep Neural Operator Learning for Probabilistic Models")), the network size â„±1\displaystyle\mathcal{F}\_{1} will be determined by the estimate in ([6.10](https://arxiv.org/html/2511.07235v1#S6.E10 "In 6.1 Operator approximation for American option pricing operator â€£ 6 Deep neural operator for American option pricing and PDE with free boundary â€£ Deep Neural Operator Learning for Probabilistic Models")).
Since the Lipschitz assumption is not used in ([4.9](https://arxiv.org/html/2511.07235v1#S4.E9 "In 4.1 Function Approximation â€£ 4 Neural scaling of operator learning â€£ Deep Neural Operator Learning for Probabilistic Models")), following the same estimate in ([4.9](https://arxiv.org/html/2511.07235v1#S4.E9 "In 4.1 Function Approximation â€£ 4 Neural scaling of operator learning â€£ Deep Neural Operator Learning for Probabilistic Models")) and r=Oâ€‹((logâ¡(Îµ1âˆ’2))1Î±),N=Oâ€‹(râ€‹Lgâ€‹d112â€‹Îµ1âˆ’2)\displaystyle r=O\left((\log(\varepsilon\_{1}^{-2}))^{\frac{1}{\alpha}}\right),N=O\left(rL\_{g}d\_{1}^{\frac{1}{2}}\varepsilon\_{1}^{-2}\right) estimates,
we have Î´=Oâ€‹(d1âˆ’d22âˆ’1â€‹Îµ112+2â€‹d2â€‹(logâ¡Îµ1âˆ’2)âˆ’p+d2Î±)\displaystyle\delta=O\left(d\_{1}^{-\frac{d\_{2}}{2}-1}\varepsilon\_{1}^{\frac{1}{2}+2d\_{2}}(\log\varepsilon\_{1}^{-2})^{-\frac{p+d\_{2}}{\alpha}}\right).
Such a network has size,

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„’1\displaystyle\displaystyle\mathcal{L}\_{1} | =Oâ€‹((12â€‹d2+2â€‹d22)â€‹logâ¡(Îµ1âˆ’2)),ğ”­1=Oâ€‹(1),K1=Oâ€‹((12â€‹d2+2â€‹d22)â€‹logâ¡(Îµ1âˆ’2)),\displaystyle\displaystyle=O\left((\frac{1}{2}d\_{2}+2d\_{2}^{2})\log(\varepsilon\_{1}^{-2})\right),\mathfrak{p}\_{1}=O(1),\ K\_{1}=O\left((\frac{1}{2}d\_{2}+2d\_{2}^{2})\log(\varepsilon\_{1}^{-2})\right), |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | Îº1\displaystyle\displaystyle\kappa\_{1} | =Oâ€‹(Îµ1âˆ’2â€‹d2),R1=1.\displaystyle\displaystyle=O\left(\varepsilon\_{1}^{-2d\_{2}}\right),R\_{1}=1. |  |

Step 2 (functional approximation): For each k\displaystyle k, define the functional induced by the operator Î“A\displaystyle\Gamma^{A} as follows,

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ–¥kâ€‹(Î“Aâ€‹(g))=Î“Aâ€‹(g)â€‹(ğœk).\displaystyle\displaystyle\mathsf{F}\_{k}(\Gamma^{A}(g))=\Gamma^{A}(g)(\mathbf{c}\_{k}). |  | (6.12) |

For any g1,g2âˆˆğ’¢\displaystyle g\_{1},g\_{2}\in\mathcal{G}, and the forward process at t\displaystyle t starts at ğœk\displaystyle\mathbf{c}\_{k}, we have

|  |  |  |  |
| --- | --- | --- | --- |
|  |  | |ğ–¥kâ€‹(Î“Aâ€‹(g1))âˆ’ğ–¥kâ€‹(Î“Aâ€‹(g2))|2\displaystyle\displaystyle|\mathsf{F}\_{k}(\Gamma^{A}(g\_{1}))-\mathsf{F}\_{k}(\Gamma^{A}(g\_{2}))|^{2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤\displaystyle\displaystyle\leq | ğ”¼â€‹[sup0â‰¤sâ‰¤T|Î“Aâ€‹(g1)â€‹(Xs)âˆ’Î“Aâ€‹(g2)â€‹(Xs)|2]\displaystyle\displaystyle\mathbb{E}\bigg[\sup\_{0\leq s\leq T}|\Gamma^{A}(g\_{1})(X\_{s})-\Gamma^{A}(g\_{2})(X\_{s})|^{2}\bigg] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤\displaystyle\displaystyle\leq | LÎ“A2â€‹(ğ”¼â€‹[sup0â‰¤sâ‰¤T|g1â€‹(Xs)âˆ’g2â€‹(Xs)|2]+(ğ”¼â€‹[sup0â‰¤sâ‰¤T|g1â€‹(Xs)âˆ’g2â€‹(Xs)|2])1/2)\displaystyle\displaystyle L\_{\Gamma^{A}}^{2}\left(\mathbb{E}\bigg[\sup\_{0\leq s\leq T}|g\_{1}(X\_{s})-g\_{2}(X\_{s})|^{2}\bigg]+\left(\mathbb{E}\bigg[\sup\_{0\leq s\leq T}|g\_{1}(X\_{s})-g\_{2}(X\_{s})|^{2}\bigg]\right)^{1/2}\right) |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | =\displaystyle\displaystyle= | LÎ“A2â€‹â€–g1âˆ’g2â€–S21,2,\displaystyle\displaystyle L\_{\Gamma^{A}}^{2}\|g\_{1}-g\_{2}\|\_{S^{2}}^{1,2}, |  | (6.13) |

where the last inequality follows from Assumption [7](https://arxiv.org/html/2511.07235v1#Thmassumption7 "Assumption 7. â€£ 6.1 Operator approximation for American option pricing operator â€£ 6 Deep neural operator for American option pricing and PDE with free boundary â€£ Deep Neural Operator Learning for Probabilistic Models").
Recall the function gwâ€‹(x)\displaystyle g\_{w}(x) defined in ([4.11](https://arxiv.org/html/2511.07235v1#S4.E11 "In 4.2 Functional Approximations â€£ 4 Neural scaling of operator learning â€£ Deep Neural Operator Learning for Probabilistic Models")),
for the functional ğ–¥\displaystyle\mathsf{F} induced by the American option pricing operator Î“A\displaystyle\Gamma^{A}, we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | |ğ–¥â€‹(g)âˆ’ğ–¥â€‹(gw)|2\displaystyle\displaystyle\bigl|\mathsf{F}(g)-\mathsf{F}(g\_{w})\bigr|^{2} | â‰¤LÎ“A2â€‹â€–gâˆ’gwâ€–S21,2\displaystyle\displaystyle\leq L\_{\Gamma^{A}}^{2}\,\|g-g\_{w}\|\_{S^{2}}^{1,2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤LÎ“A2â€‹â€–gâˆ’gwâ€–S22+LÎ“Aâ€‹â€–gâˆ’gwâ€–S21,\displaystyle\displaystyle\leq L\_{\Gamma^{A}}^{2}\,\|g-g\_{w}\|\_{S^{2}}^{2}+L\_{\Gamma^{A}}\,\|g-g\_{w}\|\_{S^{2}}^{1}, |  |

which is equivalent to

|  |  |  |
| --- | --- | --- |
|  | |ğ–¥â€‹(g)âˆ’ğ–¥â€‹(gw)|â‰¤(LÎ“A2â€‹â€–gâˆ’gwâ€–S22+LÎ“Aâ€‹â€–gâˆ’gwâ€–S21)1/2,\displaystyle\displaystyle\bigl|\mathsf{F}(g)-\mathsf{F}(g\_{w})\bigr|\leq(L\_{\Gamma^{A}}^{2}\,\|g-g\_{w}\|\_{S^{2}}^{2}+L\_{\Gamma^{A}}\,\|g-g\_{w}\|\_{S^{2}}^{1})^{1/2}, |  |

where we assume that Î“A2â‰¤Î“A\displaystyle\Gamma\_{A}^{2}\leq\Gamma\_{A} to ease the computation which, up to a constant change, does not change the order of the radius size. (Similarly, Î“Aâ‰¤Î“A2\displaystyle\Gamma\_{A}\leq\Gamma\_{A}^{2} implies similar computations).
Following the idea to the derivation of ([6.11](https://arxiv.org/html/2511.07235v1#S6.E11 "In 6.1 Operator approximation for American option pricing operator â€£ 6 Deep neural operator for American option pricing and PDE with free boundary â€£ Deep Neural Operator Learning for Probabilistic Models")) from ([6.9](https://arxiv.org/html/2511.07235v1#S6.E9 "In 6.1 Operator approximation for American option pricing operator â€£ 6 Deep neural operator for American option pricing and PDE with free boundary â€£ Deep Neural Operator Learning for Probabilistic Models")) and ([6.10](https://arxiv.org/html/2511.07235v1#S6.E10 "In 6.1 Operator approximation for American option pricing operator â€£ 6 Deep neural operator for American option pricing and PDE with free boundary â€£ Deep Neural Operator Learning for Probabilistic Models")), up to a constant, for any Îµ2>0\displaystyle\varepsilon\_{2}>0, we can pick r\displaystyle r such that LÎ“A2â€‹â€–gâˆ’gwâ€–S22â‰¤Îµ2464â‰¤Îµ228\displaystyle L\_{\Gamma\_{A}}^{2}\,\|g-g\_{w}\|\_{S^{2}}^{2}\leq\frac{\varepsilon\_{2}^{4}}{64}\leq\frac{\varepsilon\_{2}^{2}}{8}, which thus implies LÎ“Aâ€‹â€–gâˆ’gwâ€–S21â‰¤Îµ228\displaystyle L\_{\Gamma^{A}}\|g-g\_{w}\|\_{S^{2}}^{1}\leq\frac{\varepsilon\_{2}^{2}}{8}, and the following estimate

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | |ğ–¥â€‹(g)âˆ’ğ–¥â€‹(gw)|\displaystyle\displaystyle\bigl|\mathsf{F}(g)-\mathsf{F}(g\_{w})\bigr| | â‰¤(Îµ228+Îµ228)1/2â‰¤Îµ2/2.\displaystyle\displaystyle\leq(\frac{\varepsilon\_{2}^{2}}{8}+\frac{\varepsilon\_{2}^{2}}{8})^{1/2}\leq\varepsilon\_{2}/2. |  | (6.14) |

Thus, LÎ“A2â€‹â€–gâˆ’gwâ€–S22â‰¤Îµ2464\displaystyle L\_{\Gamma\_{A}}^{2}\,\|g-g\_{w}\|\_{S^{2}}^{2}\leq\frac{\varepsilon\_{2}^{4}}{64} determines the following parameters, following ([4.13](https://arxiv.org/html/2511.07235v1#S4.E13 "In 4.2 Functional Approximations â€£ 4 Neural scaling of operator learning â€£ Deep Neural Operator Learning for Probabilistic Models")), ([4.14](https://arxiv.org/html/2511.07235v1#S4.E14 "In 4.2 Functional Approximations â€£ 4 Neural scaling of operator learning â€£ Deep Neural Operator Learning for Probabilistic Models")) and ([4.15](https://arxiv.org/html/2511.07235v1#S4.E15 "In 4.2 Functional Approximations â€£ 4 Neural scaling of operator learning â€£ Deep Neural Operator Learning for Probabilistic Models")), we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | r2=âŒˆâˆ’2â€‹CTcâ€‹logâ¡Îµ24128â€‹C0â€‹Lğ–¥2âŒ‰1Î±+1,Î´2=Câ€‹Îµ228â€‹2â€‹Lğ–¥â€‹Lgâ€‹d112,\displaystyle\displaystyle r\_{2}=\left\lceil-\frac{2C\_{T}}{c}\log\frac{\varepsilon\_{2}^{4}}{128C\_{0}L\_{\mathsf{F}}^{2}}\right\rceil^{\frac{1}{\alpha}}+1,\quad\delta\_{2}=\frac{C\varepsilon\_{2}^{2}}{8\sqrt{2}L\_{\mathsf{F}}L\_{g}d\_{1}^{\frac{1}{2}}}, |  | (6.15) |

where C\displaystyle C is a constant.
Now, for any g,g~âˆˆğ’¢\displaystyle g,\widetilde{g}\in\mathcal{G}, define gw\displaystyle g\_{w} and g~w\displaystyle\widetilde{g}\_{w} as in ([4.11](https://arxiv.org/html/2511.07235v1#S4.E11 "In 4.2 Functional Approximations â€£ 4 Neural scaling of operator learning â€£ Deep Neural Operator Learning for Probabilistic Models")), and set

|  |  |  |
| --- | --- | --- |
|  | ğ’ˆ=[gâ€‹(c1),â€¦,gâ€‹(cNÎ´)]ğ–³,ğ’ˆ~=[g~â€‹(c1),â€¦,g~â€‹(cNÎ´)]ğ–³.\bm{g}=\bigl[g(c\_{1}),\ldots,g(c\_{N^{\delta}})\bigr]^{\mathsf{T}},\qquad\widetilde{\bm{g}}=\bigl[\widetilde{g}(c\_{1}),\ldots,\widetilde{g}(c\_{N^{\delta}})\bigr]^{\mathsf{T}}. |  |

Define the function hâ€‹(ğ’ˆ):=ğ–¥â€‹(gw)\displaystyle h(\bm{g}):=\mathsf{F}(g\_{w}). According to ([6.12](https://arxiv.org/html/2511.07235v1#S6.E12 "In 6.1 Operator approximation for American option pricing operator â€£ 6 Deep neural operator for American option pricing and PDE with free boundary â€£ Deep Neural Operator Learning for Probabilistic Models")) and ([6.1](https://arxiv.org/html/2511.07235v1#S6.Ex26 "6.1 Operator approximation for American option pricing operator â€£ 6 Deep neural operator for American option pricing and PDE with free boundary â€£ Deep Neural Operator Learning for Probabilistic Models")), we have

|  |  |  |  |
| --- | --- | --- | --- |
|  | |hâ€‹(ğ’ˆ)âˆ’hâ€‹(ğ’ˆ~)|2\displaystyle\displaystyle|h(\bm{g})-h(\widetilde{\bm{g}})|^{2} | =|ğ–¥â€‹(gw)âˆ’ğ–¥â€‹(g~w)|2\displaystyle\displaystyle=\bigl|\mathsf{F}(g\_{w})-\mathsf{F}(\widetilde{g}\_{w})\bigr|^{2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤LÎ“A2â€‹â€–gwâˆ’g~wâ€–S21,2\displaystyle\displaystyle\leq L\_{\Gamma\_{A}}^{2}\,\|g\_{w}-\widetilde{g}\_{w}\|\_{S^{2}}^{1,2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | =LÎ“A2â€‹ğ”¼â€‹[suptâ‰¤sâ‰¤T(âˆ‘k=1NÎ´(gâ€‹(ck)âˆ’g~â€‹(ck))â€‹wkâ€‹(Xs))2]\displaystyle\displaystyle=L\_{\Gamma\_{A}}^{2}\,\mathbb{E}\!\left[\sup\_{t\leq s\leq T}\!\!\Bigl(\sum\_{k=1}^{N^{\delta}}\bigl(g(c\_{k})-\widetilde{g}(c\_{k})\bigr)\,w\_{k}(X\_{s})\Bigr)^{2}\right] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +LÎ“A2â€‹(ğ”¼â€‹[suptâ‰¤sâ‰¤T(âˆ‘k=1NÎ´(gâ€‹(ck)âˆ’g~â€‹(ck))â€‹wkâ€‹(Xs))2])1/2\displaystyle\displaystyle+L\_{\Gamma\_{A}}^{2}\,\left(\mathbb{E}\!\left[\sup\_{t\leq s\leq T}\!\!\Bigl(\sum\_{k=1}^{N^{\delta}}\bigl(g(c\_{k})-\widetilde{g}(c\_{k})\bigr)\,w\_{k}(X\_{s})\Bigr)^{2}\right]\right)^{1/2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤LÎ“A2â€‹(âˆ‘k=1NÎ´|gâ€‹(ck)âˆ’g~â€‹(ck)|2)â€‹ğ”¼â€‹[suptâ‰¤sâ‰¤Tâˆ‘k=1NÎ´wkâ€‹(Xs)2]\displaystyle\displaystyle\leq L\_{\Gamma\_{A}}^{2}\,\Bigl(\sum\_{k=1}^{N^{\delta}}|g(c\_{k})-\widetilde{g}(c\_{k})|^{2}\Bigr)\,\mathbb{E}\!\left[\sup\_{t\leq s\leq T}\sum\_{k=1}^{N^{\delta}}w\_{k}(X\_{s})^{2}\right] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +LÎ“A2â€‹(âˆ‘k=1NÎ´|gâ€‹(ck)âˆ’g~â€‹(ck)|2)1/2â€‹ğ”¼â€‹[suptâ‰¤sâ‰¤Tâˆ‘k=1NÎ´wkâ€‹(Xs)2]1/2\displaystyle\displaystyle+L\_{\Gamma\_{A}}^{2}\,\Bigl(\sum\_{k=1}^{N^{\delta}}|g(c\_{k})-\widetilde{g}(c\_{k})|^{2}\Bigr)^{1/2}\,\mathbb{E}\!\left[\sup\_{t\leq s\leq T}\sum\_{k=1}^{N^{\delta}}w\_{k}(X\_{s})^{2}\right]^{1/2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | â‰¤LÎ“A2â€‹(âˆ‘k=1NÎ´|gâ€‹(ck)âˆ’g~â€‹(ck)|2+(âˆ‘k=1NÎ´|gâ€‹(ck)âˆ’g~â€‹(ck)|2)1/2)\displaystyle\displaystyle\leq L\_{\Gamma\_{A}}^{2}\,\,\Bigl(\sum\_{k=1}^{N^{\delta}}|g(c\_{k})-\widetilde{g}(c\_{k})|^{2}+\left(\sum\_{k=1}^{N^{\delta}}|g(c\_{k})-\widetilde{g}(c\_{k})|^{2}\right)^{1/2}\Bigr) |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | â‰¤LÎ“A2â€‹(|ğ’ˆâˆ’ğ’ˆ~|2+|ğ’ˆâˆ’ğ’ˆ~|),\displaystyle\displaystyle\leq L\_{\Gamma\_{A}}^{2}(|\bm{g}-\widetilde{\bm{g}}|^{2}+|\bm{g}-\widetilde{\bm{g}}|), |  | (6.16) |

which is equivalent to

|  |  |  |
| --- | --- | --- |
|  | |hâ€‹(ğ’ˆ)âˆ’hâ€‹(ğ’ˆ~)|â‰¤LÎ“Aâ€‹(|ğ’ˆâˆ’ğ’ˆ~|2+|ğ’ˆâˆ’ğ’ˆ~|)1/2.\displaystyle\displaystyle|h(\bm{g})-h(\widetilde{\bm{g}})|\leq L\_{\Gamma\_{A}}(|\bm{g}-\widetilde{\bm{g}}|^{2}+|\bm{g}-\widetilde{\bm{g}}|)^{1/2}. |  |

By Assumption [3](https://arxiv.org/html/2511.07235v1#Thmassumption3 "Assumption 3. â€£ 2 Assumptions â€£ Deep Neural Operator Learning for Probabilistic Models"), we have the bound of function g\displaystyle g on Qr\displaystyle Q\_{r} as below,

|  |  |  |
| --- | --- | --- |
|  | gâ€‹(x)â‰¤Cgâ€‹(1+|x|p)â‰¤Cgâ€‹(1+|r2|p):=r~,xâˆˆQr2.g(x)\leq C\_{g}(1+|x|^{p})\leq C\_{g}(1+|r\_{2}|^{p}):=\widetilde{r},\quad x\in Q\_{r\_{2}}. |  |

Similarly, according to our definition in ([6.12](https://arxiv.org/html/2511.07235v1#S6.E12 "In 6.1 Operator approximation for American option pricing operator â€£ 6 Deep neural operator for American option pricing and PDE with free boundary â€£ Deep Neural Operator Learning for Probabilistic Models")), h\displaystyle h is also bounded above. Applying Lemma [6.3](https://arxiv.org/html/2511.07235v1#S6.Thmtheorem3 "Lemma 6.3. â€£ 6.1 Operator approximation for American option pricing operator â€£ 6 Deep neural operator for American option pricing and PDE with free boundary â€£ Deep Neural Operator Learning for Probabilistic Models") (which is proved below), we claim that for 12â€‹Îµ2>0\displaystyle\frac{1}{2}\varepsilon\_{2}>0
the function (functional) hâ€‹(ğ’ˆ)=ğ–¥â€‹(gw)\displaystyle h(\bm{g})=\mathsf{F}(g\_{w}) can be approximated by a network â„±2=â„±NNâ€‹(NÎ´,1,â„’2,ğ”­2,K2,Îº2,R2)\displaystyle\mathcal{F}\_{2}=\mathcal{F}\_{\rm NN}(N^{\delta},1,\mathcal{L}\_{2},\mathfrak{p}\_{2},K\_{2},\kappa\_{2},R\_{2}),

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„’2\displaystyle\displaystyle\mathcal{L}\_{2} | =Oâ€‹((NÎ´+12â€‹(NÎ´)2)â€‹logâ¡(NÎ´)+(2â€‹(NÎ´)2+NÎ´)â€‹logâ¡Îµ2âˆ’1+NÎ´â€‹logâ¡(r~)),ğ”­2=Oâ€‹(1),\displaystyle\displaystyle=O\left((N^{\delta}+\frac{1}{2}(N^{\delta})^{2})\log(N^{\delta})+(2(N^{\delta})^{2}+N^{\delta})\log\varepsilon\_{2}^{-1}+N^{\delta}\log(\widetilde{r})\right),\mathfrak{p}\_{2}=O(1), |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | K2\displaystyle\displaystyle K\_{2} | =Oâ€‹((NÎ´+12â€‹(NÎ´)2)â€‹logâ¡(NÎ´)+(2â€‹(NÎ´)2+NÎ´)â€‹logâ¡Îµ2âˆ’1+NÎ´â€‹logâ¡(r~)),\displaystyle\displaystyle=O\left((N^{\delta}+\frac{1}{2}(N^{\delta})^{2})\log(N^{\delta})+(2(N^{\delta})^{2}+N^{\delta})\log\varepsilon\_{2}^{-1}+N^{\delta}\log(\widetilde{r})\right), |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | Îº2\displaystyle\displaystyle\kappa\_{2} | =Oâ€‹(Îµ2âˆ’NÎ´),R2=1.\displaystyle\displaystyle=O(\varepsilon\_{2}^{-N^{\delta}}),R\_{2}=1. |  |

That is

|  |  |  |  |
| --- | --- | --- | --- |
|  | supgâˆˆU|ğ–¥â€‹(gw)âˆ’âˆ‘k=1N2hâ€‹(ğ’ˆk)â€‹q~k|â‰¤12â€‹Îµ2,\displaystyle\displaystyle\sup\_{g\in U}\left|\mathsf{F}(g\_{w})-\sum\_{k=1}^{N\_{2}}h({\bm{g}}\_{k})\widetilde{q}\_{k}\right|\leq\frac{1}{2}\varepsilon\_{2}, |  | (6.17) |

where N2=NNÎ´\displaystyle N\_{2}=N^{N^{\delta}}, with N=Oâ€‹(NÎ´â€‹Îµ2âˆ’2)\displaystyle N=O(\sqrt{N^{\delta}}\varepsilon\_{2}^{-2}).
Combining the above estimates in ([6.14](https://arxiv.org/html/2511.07235v1#S6.E14 "In 6.1 Operator approximation for American option pricing operator â€£ 6 Deep neural operator for American option pricing and PDE with free boundary â€£ Deep Neural Operator Learning for Probabilistic Models")) and ([6.17](https://arxiv.org/html/2511.07235v1#S6.E17 "In 6.1 Operator approximation for American option pricing operator â€£ 6 Deep neural operator for American option pricing and PDE with free boundary â€£ Deep Neural Operator Learning for Probabilistic Models")), such a network architecture gives a network a~k\displaystyle\widetilde{a}\_{k} so that

|  |  |  |
| --- | --- | --- |
|  | supg|ğ–¥kâ€‹(Î“â€‹(g))âˆ’a~kâ€‹(ğ’ˆ)|â‰¤Îµ2.\displaystyle\displaystyle\sup\_{g}|\mathsf{F}\_{k}(\Gamma(g))-\widetilde{a}\_{k}(\bm{g})|\leq\varepsilon\_{2}. |  |

Therefore,

|  |  |  |  |
| --- | --- | --- | --- |
|  |  | ğ”¼â€‹[sup0â‰¤sâ‰¤T|âˆ‘k=1N1ğ–¥kâ€‹(Î“â€‹g)â€‹q~kâ€‹(Xs)âˆ’âˆ‘k=1N1a~kâ€‹(ğ’ˆ)â€‹q~kâ€‹(Xs)|]\displaystyle\displaystyle\mathbb{E}\bigg[\sup\_{0\leq s\leq T}\left|\sum\_{k=1}^{N\_{1}}\mathsf{F}\_{k}(\Gamma g)\widetilde{q}\_{k}(X\_{s})-\sum\_{k=1}^{N\_{1}}\widetilde{a}\_{k}(\bm{g})\widetilde{q}\_{k}(X\_{s})\right|\bigg] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | =\displaystyle\displaystyle= | ğ”¼â€‹[sup0â‰¤sâ‰¤T|âˆ‘k=1N1(ğ–¥kâ€‹(Î“â€‹g)âˆ’a~kâ€‹(ğ’ˆ))â€‹q~kâ€‹(Xs)|]\displaystyle\displaystyle\mathbb{E}\bigg[\sup\_{0\leq s\leq T}\left|\sum\_{k=1}^{N\_{1}}\left(\mathsf{F}\_{k}(\Gamma g)-\widetilde{a}\_{k}(\bm{g})\right)\widetilde{q}\_{k}(X\_{s})\right|\bigg] |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | â‰¤\displaystyle\displaystyle\leq | âˆ‘k=1N1supğ’ˆ|ğ–¥kâ€‹(Î“â€‹g)âˆ’a~kâ€‹(ğ’ˆ)|=N1â€‹Îµ2.\displaystyle\displaystyle\sum\_{k=1}^{N\_{1}}\sup\_{\bm{g}}|\mathsf{F}\_{k}(\Gamma g)-\widetilde{a}\_{k}(\bm{g})|=N\_{1}\varepsilon\_{2}. |  | (6.18) |

Step 3 (American pricing operator approximation):
Applying the Cauchyâ€“Schwarz inequality, using ([6.11](https://arxiv.org/html/2511.07235v1#S6.E11 "In 6.1 Operator approximation for American option pricing operator â€£ 6 Deep neural operator for American option pricing and PDE with free boundary â€£ Deep Neural Operator Learning for Probabilistic Models")) and ([6.18](https://arxiv.org/html/2511.07235v1#S6.E18 "In 6.1 Operator approximation for American option pricing operator â€£ 6 Deep neural operator for American option pricing and PDE with free boundary â€£ Deep Neural Operator Learning for Probabilistic Models")), we have

|  |  |  |  |
| --- | --- | --- | --- |
|  |  | supgâˆˆğ’¢ğ”¼â€‹[sup0â‰¤sâ‰¤T|Î“Aâ€‹gâ€‹(Xs)âˆ’âˆ‘k=1N1a~kâ€‹(ğ’ˆ)â€‹q~kâ€‹(Xs)|]\displaystyle\displaystyle\sup\_{g\in\mathcal{G}}\mathbb{E}\bigg[\sup\_{0\leq s\leq T}\left|\Gamma^{A}g(X\_{s})-\sum\_{k=1}^{N\_{1}}\widetilde{a}\_{k}(\bm{g})\widetilde{q}\_{k}(X\_{s})\right|\bigg] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤\displaystyle\displaystyle\leq | supgâˆˆğ’¢ğ”¼â€‹[sup0â‰¤sâ‰¤T|Î“Aâ€‹(g)â€‹(Xs)âˆ’âˆ‘k=1N1ğ–¥kâ€‹(Î“â€‹g)â€‹q~kâ€‹(Xs)|]\displaystyle\displaystyle\sup\_{g\in\mathcal{G}}\mathbb{E}\bigg[\sup\_{0\leq s\leq T}\left|\Gamma^{A}(g)(X\_{s})-\sum\_{k=1}^{N\_{1}}\mathsf{F}\_{k}(\Gamma g)\widetilde{q}\_{k}(X\_{s})\right|\bigg] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +supgâˆˆğ’¢ğ”¼â€‹[sup0â‰¤sâ‰¤T|âˆ‘k=1N1ğ–¥kâ€‹(Î“Aâ€‹g)â€‹q~kâ€‹(Xs)âˆ’âˆ‘k=1N1a~kâ€‹(ğ’ˆ)â€‹q~kâ€‹(Xs)|]\displaystyle\displaystyle+\sup\_{g\in\mathcal{G}}\mathbb{E}\bigg[\sup\_{0\leq s\leq T}\left|\sum\_{k=1}^{N\_{1}}\mathsf{F}\_{k}(\Gamma^{A}g)\widetilde{q}\_{k}(X\_{s})-\sum\_{k=1}^{N\_{1}}\widetilde{a}\_{k}(\bm{g})\widetilde{q}\_{k}(X\_{s})\right|\bigg] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤\displaystyle\displaystyle\leq | supgâˆˆğ’¢ğ”¼â€‹[sup0â‰¤sâ‰¤T|Î“Aâ€‹(g)â€‹(Xs)âˆ’âˆ‘k=1N1ğ–¥kâ€‹(Î“Aâ€‹g)â€‹q~kâ€‹(Xs)|2]1/2\displaystyle\displaystyle\sup\_{g\in\mathcal{G}}\mathbb{E}\bigg[\sup\_{0\leq s\leq T}\left|\Gamma^{A}(g)(X\_{s})-\sum\_{k=1}^{N\_{1}}\mathsf{F}\_{k}(\Gamma^{A}g)\widetilde{q}\_{k}(X\_{s})\right|^{2}\bigg]^{1/2} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +supgâˆˆğ’¢ğ”¼â€‹[sup0â‰¤sâ‰¤T|âˆ‘k=1N1ğ–¥kâ€‹(Î“Aâ€‹g)â€‹q~kâ€‹(Xs)âˆ’âˆ‘k=1Na~kâ€‹(ğ’ˆ)â€‹q~kâ€‹(Xs)|]\displaystyle\displaystyle+\sup\_{g\in\mathcal{G}}\mathbb{E}\bigg[\sup\_{0\leq s\leq T}\left|\sum\_{k=1}^{N\_{1}}\mathsf{F}\_{k}(\Gamma^{A}g)\widetilde{q}\_{k}(X\_{s})-\sum\_{k=1}^{N}\widetilde{a}\_{k}(\bm{g})\widetilde{q}\_{k}(X\_{s})\right|\bigg] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤\displaystyle\displaystyle\leq | supgâˆˆğ’¢â€–Î“Aâ€‹(g)âˆ’âˆ‘k=1N1Î“Aâ€‹(g)â€‹(ğœk)â€‹q~kâ€–ğ’®21,2+supgâˆˆğ’¢ğ”¼â€‹[sup0â‰¤sâ‰¤T|âˆ‘k=1N1ğ–¥kâ€‹(Î“â€‹g)â€‹q~kâ€‹(Xs)âˆ’âˆ‘k=1N1a~kâ€‹(ğ’ˆ)â€‹q~kâ€‹(Xs)|]\displaystyle\displaystyle\sup\_{g\in\mathcal{G}}\|\Gamma^{A}(g)-\sum\_{k=1}^{N\_{1}}\Gamma^{A}(g)(\mathbf{c}\_{k})\widetilde{q}\_{k}\|\_{\mathcal{S}\_{2}}^{1,2}+\sup\_{g\in\mathcal{G}}\mathbb{E}\bigg[\sup\_{0\leq s\leq T}\left|\sum\_{k=1}^{N\_{1}}\mathsf{F}\_{k}(\Gamma g)\widetilde{q}\_{k}(X\_{s})-\sum\_{k=1}^{N\_{1}}\widetilde{a}\_{k}(\bm{g})\widetilde{q}\_{k}(X\_{s})\right|\bigg] |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤\displaystyle\displaystyle\leq | Îµ1+N1â€‹Îµ2.\displaystyle\displaystyle\varepsilon\_{1}+N\_{1}\varepsilon\_{2}. |  |

Set Îµ2=Îµ1/(2â€‹N1),Îµ1=Îµ2\displaystyle\varepsilon\_{2}=\varepsilon\_{1}/(2N\_{1}),\varepsilon\_{1}=\frac{\varepsilon}{2},
or Îµ2=Oâ€‹(Îµ2â€‹d2+1â€‹(logâ¡Îµâˆ’2)âˆ’d2Î±â€‹d1âˆ’d22)\displaystyle\varepsilon\_{2}=O\left(\varepsilon^{2d\_{2}+1}(\log\varepsilon^{-2})^{-\frac{d\_{2}}{\alpha}}d\_{1}^{-\frac{d\_{2}}{2}}\right),
we then have

|  |  |  |
| --- | --- | --- |
|  | supgâˆˆğ’¢ğ”¼â€‹[sup0â‰¤sâ‰¤T|Î“Aâ€‹gâ€‹(Xs)âˆ’âˆ‘k=1N1a~kâ€‹(ğ’ˆ)â€‹q~kâ€‹(Xs)|]â‰¤Îµ.\displaystyle\displaystyle\sup\_{g\in\mathcal{G}}\mathbb{E}\bigg[\sup\_{0\leq s\leq T}\left|\Gamma^{A}g(X\_{s})-\sum\_{k=1}^{N\_{1}}\widetilde{a}\_{k}(\bm{g})\widetilde{q}\_{k}(X\_{s})\right|\bigg]\leq\varepsilon. |  |

Consequently, the network size is estimated to be,

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„’2\displaystyle\displaystyle\mathcal{L}\_{2} | =Oâ€‹((NÎ´+12â€‹(NÎ´)2)â€‹logâ¡(NÎ´)+(2â€‹(NÎ´)2+NÎ´)â€‹logâ¡Îµâˆ’(2â€‹d2+1)+NÎ´â€‹logâ¡(r~)),ğ”­2=Oâ€‹(Îµ2âˆ’(4â€‹d1âˆ’2)â€‹NÎ´),\displaystyle\displaystyle=O\left((N^{\delta}+\frac{1}{2}(N^{\delta})^{2})\log(N^{\delta})+(2(N^{\delta})^{2}+N^{\delta})\log\varepsilon^{-(2d\_{2}+1)}+N^{\delta}\log(\widetilde{r})\right),\mathfrak{p}\_{2}=O(\varepsilon\_{2}^{-(4d\_{1}-2)N^{\delta}}), |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | K2\displaystyle\displaystyle K\_{2} | =Oâ€‹((NÎ´+12â€‹(NÎ´)2)â€‹logâ¡(NÎ´)+(2â€‹(NÎ´)2+NÎ´)â€‹logâ¡Îµâˆ’2â€‹(d2+1)+NÎ´â€‹logâ¡(r~)),\displaystyle\displaystyle=O\left((N^{\delta}+\frac{1}{2}(N^{\delta})^{2})\log(N^{\delta})+(2(N^{\delta})^{2}+N^{\delta})\log\varepsilon^{-2(d\_{2}+1)}+N^{\delta}\log(\widetilde{r})\right), |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | Îº2\displaystyle\displaystyle\kappa\_{2} | =Oâ€‹(Îµâˆ’NÎ´â€‹(2â€‹d2+1)),R2=1.\displaystyle\displaystyle=O(\varepsilon^{-N^{\delta}(2d\_{2}+1)}),R\_{2}=1. |  |

By Lemma [4.3](https://arxiv.org/html/2511.07235v1#S4.Thmtheorem3 "Lemma 4.3. â€£ 4.2 Functional Approximations â€£ 4 Neural scaling of operator learning â€£ Deep Neural Operator Learning for Probabilistic Models"), and equation [6.15](https://arxiv.org/html/2511.07235v1#S6.E15 "In 6.1 Operator approximation for American option pricing operator â€£ 6 Deep neural operator for American option pricing and PDE with free boundary â€£ Deep Neural Operator Learning for Probabilistic Models"),

|  |  |  |
| --- | --- | --- |
|  | NÎ´â‰¤Câ€‹(logâ¡Îµ2âˆ’4)d1Î±â€‹Îµ2âˆ’2â€‹d1â€‹d1d12.\displaystyle\displaystyle N^{\delta}\leq C(\log\varepsilon\_{2}^{-4})^{\frac{d\_{1}}{\alpha}}\varepsilon\_{2}^{-2d\_{1}}d\_{1}^{\frac{d\_{1}}{2}}. |  |

Substitute back to Îµ\displaystyle\varepsilon, we have NÎ´=Oâ€‹(Îµâˆ’4d1d2âˆ’2d1))\displaystyle N^{\delta}=O(\varepsilon^{-4d\_{1}d\_{2}-2d\_{1})}).
It follows that N2=Oâ€‹(Îµâˆ’(4â€‹d2âˆ’2)â€‹Îµâˆ’4â€‹d1â€‹d2âˆ’2â€‹d1)\displaystyle N\_{2}=O(\varepsilon^{-(4d\_{2}-2)\varepsilon^{-4d\_{1}d\_{2}-2d\_{1}}}).

âˆ

###### Lemma 6.3.

Let Qr~=[âˆ’r~,r~]NÎ´\displaystyle Q\_{\widetilde{r}}=[-\widetilde{r},\widetilde{r}]^{N^{\delta}}, ğ =[g1,â€¦,gNÎ´]ğ–³âˆˆQr~\displaystyle\bm{g}=\bigl[g\_{1},\ldots,g\_{N^{\delta}}\bigr]^{\mathsf{T}}\in Q\_{\widetilde{r}} and Î²ğ =supgâˆˆQr~|hâ€‹(g)|\displaystyle\beta\_{\bm{g}}=\sup\_{g\in Q\_{\widetilde{r}}}|h(g)|.
Assume function h:Qr~â†’â„\displaystyle h:Q\_{\widetilde{r}}\rightarrow\mathbb{R}, with NÎ´\displaystyle N^{\delta} as an integer and some constant r~\displaystyle\widetilde{r}, and h\displaystyle h satisfies the following assumption,

|  |  |  |  |
| --- | --- | --- | --- |
|  | |hâ€‹(g)âˆ’hâ€‹(g~)|â‰¤Lâ€‹(|gâˆ’g~|2+|gâˆ’g~|)1/2,g,g~âˆˆQr~,\displaystyle\displaystyle|h(g)-h(\widetilde{g})|\leq L(|g-\widetilde{g}|^{2}+|g-\widetilde{g}|)^{1/2},g,\widetilde{g}\in Q\_{\widetilde{r}}, |  | (6.19) |

where L\displaystyle L is a constant.
For Îµ2>0\displaystyle\varepsilon\_{2}>0, there exist N2\displaystyle N\_{2}, {ğ k}k=1N2âŠ‚Qr~\displaystyle\{\bm{g}\_{k}\}\_{k=1}^{N\_{2}}\subset Q\_{\widetilde{r}}, and a network â„±2=â„±NNâ€‹(NÎ´,1,â„’2,ğ”­2,K2,Îº2,R2)\displaystyle\mathcal{F}\_{2}=\mathcal{F}\_{\rm NN}(N^{\delta},1,\mathcal{L}\_{2},\mathfrak{p}\_{2},K\_{2},\kappa\_{2},R\_{2}), where

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„’2\displaystyle\displaystyle\mathcal{L}\_{2} | =Oâ€‹((NÎ´+12â€‹(NÎ´)2)â€‹logâ¡(NÎ´)+(2â€‹(NÎ´)2+NÎ´)â€‹logâ¡Îµ2âˆ’1+NÎ´â€‹logâ¡(r~)),ğ”­2=Oâ€‹(1),\displaystyle\displaystyle=O\left((N^{\delta}+\frac{1}{2}(N^{\delta})^{2})\log(N^{\delta})+(2(N^{\delta})^{2}+N^{\delta})\log\varepsilon\_{2}^{-1}+N^{\delta}\log(\widetilde{r})\right),\mathfrak{p}\_{2}=O(1), |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | K2\displaystyle\displaystyle K\_{2} | =Oâ€‹((NÎ´+12â€‹(NÎ´)2)â€‹logâ¡(NÎ´)+(2â€‹(NÎ´)2+NÎ´)â€‹logâ¡Îµ2âˆ’1+NÎ´â€‹logâ¡(r~)),\displaystyle\displaystyle=O\left((N^{\delta}+\frac{1}{2}(N^{\delta})^{2})\log(N^{\delta})+(2(N^{\delta})^{2}+N^{\delta})\log\varepsilon\_{2}^{-1}+N^{\delta}\log(\widetilde{r})\right), |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | Îº2\displaystyle\displaystyle\kappa\_{2} | =Oâ€‹(Îµ2âˆ’NÎ´),R=1,\displaystyle\displaystyle=O(\varepsilon\_{2}^{-N^{\delta}}),R=1, |  |

such that

|  |  |  |
| --- | --- | --- |
|  | supgâˆˆQr~|hâ€‹(g)âˆ’âˆ‘k=1N2hâ€‹(ğ’ˆk)â€‹q~k|â‰¤Îµ2,\displaystyle\displaystyle\sup\_{g\in Q\_{\widetilde{r}}}\left|h(g)-\sum\_{k=1}^{N\_{2}}h({\bm{g}}\_{k})\widetilde{q}\_{k}\right|\leq\varepsilon\_{2}, |  |

where N2=NNÎ´\displaystyle N\_{2}=N^{N^{\delta}}, with N=Oâ€‹(NÎ´â€‹Îµ2âˆ’2)\displaystyle N=O(\sqrt{N^{\delta}}\varepsilon\_{2}^{-2}).

###### Proof of Lemma.

Let us partition Qr~\displaystyle Q\_{\widetilde{r}} into NNÎ´\displaystyle N^{N^{\delta}} subcubes for some N\displaystyle N to be specified later, and NÎ´\displaystyle N^{\delta} follows the Theorem [6.2](https://arxiv.org/html/2511.07235v1#S6.Thmtheorem2 "Theorem 6.2. â€£ 6.1 Operator approximation for American option pricing operator â€£ 6 Deep neural operator for American option pricing and PDE with free boundary â€£ Deep Neural Operator Learning for Probabilistic Models").
Let {ğ’ˆk}k=1NNÎ´\displaystyle\{\bm{g}\_{k}\}\_{k=1}^{N^{N^{\delta}}} be a uniform grid on Qr~\displaystyle Q\_{\widetilde{r}} so that each ğ’ˆkâˆˆ{âˆ’r~,âˆ’r~+2â€‹r~Nâˆ’1,â€¦,r~}NÎ´\displaystyle\bm{g}\_{k}\in\left\{-\widetilde{r},-\widetilde{r}+\frac{2\widetilde{r}}{N-1},...,\widetilde{r}\right\}^{N^{\delta}} for each k\displaystyle k.
Define

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ïˆâ€‹(a)={1,|a|<1,0,|a|>2,2âˆ’|a|,1â‰¤|a|â‰¤2,\displaystyle\displaystyle\psi(a)=\begin{cases}1,|a|<1,\\ 0,|a|>2,\\ 2-|a|,1\leq|a|\leq 2,\end{cases} |  | (6.20) |

with aâˆˆâ„\displaystyle a\in\mathbb{R}, and

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ï•ğ’ˆkâ€‹(ğ±)=âˆj=1NÎ´Ïˆâ€‹(3â€‹(Nâˆ’1)2â€‹r~â€‹(xjâˆ’gk,j)).\displaystyle\displaystyle\phi\_{\bm{g}\_{k}}(\mathbf{x})=\prod\_{j=1}^{N^{\delta}}\psi\left(\frac{3(N-1)}{2\widetilde{r}}(x\_{j}-g\_{k,j})\right). |  | (6.21) |

In this definition, we have suppâ€‹(Ï•ğ’ˆk)={ğ±:â€–ğ±âˆ’ğ’ˆkâ€–âˆâ‰¤4â€‹r~3â€‹(Nâˆ’1)}âŠ‚{ğ±:â€–ğ±âˆ’ğ’ˆkâ€–âˆâ‰¤2â€‹r~(Nâˆ’1)}\displaystyle\mathrm{supp\,}(\phi\_{\bm{g}\_{k}})=\left\{\mathbf{x}:\|\mathbf{x}-\bm{g}\_{k}\|\_{\infty}\leq\frac{4\widetilde{r}}{3(N-1)}\right\}\subset\left\{\mathbf{x}:\|\mathbf{x}-\bm{g}\_{k}\|\_{\infty}\leq\frac{2\widetilde{r}}{(N-1)}\right\} and

|  |  |  |
| --- | --- | --- |
|  | maxkâ¡Ï•ğ’ˆk=1,âˆ‘k=1NNÎ´Ï•ğ’ˆk=1.\max\_{k}\phi\_{\bm{g}\_{k}}=1,\quad\sum\_{k=1}^{N^{N^{\delta}}}\phi\_{\bm{g}\_{k}}=1. |  |

We construct a piecewise constant approximation to h\displaystyle h as

|  |  |  |
| --- | --- | --- |
|  | hÂ¯â€‹(ğ±)=âˆ‘k=1NNÎ´hâ€‹(ğ’ˆk)â€‹Ï•ğ’ˆkâ€‹(ğ±).\bar{h}(\mathbf{x})=\sum\_{k=1}^{N^{N^{\delta}}}h(\bm{g}\_{k})\phi\_{\bm{g}\_{k}}(\mathbf{x}). |  |

It follows that,

|  |  |  |  |
| --- | --- | --- | --- |
|  | |hâ€‹(ğ’ˆ)âˆ’hÂ¯â€‹(ğ’ˆ)|=\displaystyle\displaystyle|h(\bm{g})-\bar{h}(\bm{g})|= | |âˆ‘k=1NNÎ´Ï•ğ’ˆkâ€‹(ğ±)â€‹(hâ€‹(ğ’ˆ)âˆ’hâ€‹(ğ’ˆk))|\displaystyle\displaystyle\left|\sum\_{k=1}^{N^{N^{\delta}}}\phi\_{\bm{g}\_{k}}(\mathbf{x})(h(\bm{g})-h(\bm{g}\_{k}))\right| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤\displaystyle\displaystyle\leq | âˆ‘k=1NNÎ´Ï•ğ’ˆkâ€‹(ğ±)â€‹|hâ€‹(ğ’ˆ)âˆ’hâ€‹(ğ’ˆk)|\displaystyle\displaystyle\sum\_{k=1}^{N^{N^{\delta}}}\phi\_{\bm{g}\_{k}}(\mathbf{x})|h(\bm{g})-h(\bm{g}\_{k})| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | =\displaystyle\displaystyle= | âˆ‘k:â€–ğ’ˆkâˆ’ğ±â€–âˆâ‰¤2â€‹r~(Nâˆ’1)Ï•ğ’ˆkâ€‹(ğ±)â€‹|(hâ€‹(ğ±)âˆ’hâ€‹(ğ’ˆk))|\displaystyle\displaystyle\sum\_{k:\|\bm{g}\_{k}-\mathbf{x}\|\_{\infty}\leq\frac{2\widetilde{r}}{(N-1)}}\phi\_{\bm{g}\_{k}}(\mathbf{x})|(h(\mathbf{x})-h(\bm{g}\_{k}))| |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤\displaystyle\displaystyle\leq | maxk:â€–ğ’ˆkâˆ’ğ±â€–âˆâ‰¤2â€‹r~(Nâˆ’1)â¡|(hâ€‹(ğ±)âˆ’hâ€‹(ğ’ˆk))|â€‹(âˆ‘k:â€–ğ’ˆkâˆ’ğ±â€–âˆâ‰¤2â€‹r~(Nâˆ’1)Ï•ğ’ˆkâ€‹(ğ±))\displaystyle\displaystyle\max\_{k:\|\bm{g}\_{k}-\mathbf{x}\|\_{\infty}\leq\frac{2\widetilde{r}}{(N-1)}}|(h(\mathbf{x})-h(\bm{g}\_{k}))|\left(\sum\_{k:\|\bm{g}\_{k}-\mathbf{x}\|\_{\infty}\leq\frac{2\widetilde{r}}{(N-1)}}\phi\_{\bm{g}\_{k}}(\mathbf{x})\right) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤\displaystyle\displaystyle\leq | maxk:â€–ğ’ˆkâˆ’ğ±â€–âˆâ‰¤2â€‹r~(Nâˆ’1)â¡|(hâ€‹(ğ±)âˆ’hâ€‹(ğ’ˆk))|\displaystyle\displaystyle\max\_{k:\|\bm{g}\_{k}-\mathbf{x}\|\_{\infty}\leq\frac{2\widetilde{r}}{(N-1)}}|(h(\mathbf{x})-h(\bm{g}\_{k}))| |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | â‰¤\displaystyle\displaystyle\leq | LÎ“Aâ€‹((2â€‹NÎ´â€‹r~Nâˆ’1)2+(2â€‹NÎ´â€‹r~Nâˆ’1))12<Îµ22,\displaystyle\displaystyle L\_{\Gamma\_{A}}\left(\left(\frac{2\sqrt{N^{\delta}}\widetilde{r}}{N-1}\right)^{2}+\left(\frac{2\sqrt{N^{\delta}}\widetilde{r}}{N-1}\right)\right)^{\frac{1}{2}}<\frac{\varepsilon\_{2}}{2}, |  | (6.22) |

where the last line follows from the Assumption [6.19](https://arxiv.org/html/2511.07235v1#S6.E19 "In Lemma 6.3. â€£ 6.1 Operator approximation for American option pricing operator â€£ 6 Deep neural operator for American option pricing and PDE with free boundary â€£ Deep Neural Operator Learning for Probabilistic Models").
Setting N=Câ€‹LÎ“A2â€‹Îµ2âˆ’2â€‹(NÎ´)12â€‹r~\displaystyle N=CL\_{\Gamma\_{A}}^{2}\varepsilon\_{2}^{-2}(N^{\delta})^{\frac{1}{2}}\widetilde{r},
where C\displaystyle C is a constant.
Now we adopt neural network with structure q~kâˆˆâ„±2\displaystyle\widetilde{q}\_{k}\in\mathcal{F}\_{2} to be specified later to approximate Ï•ğ’ˆk\displaystyle\phi\_{{\bm{g}}\_{k}} such that â€–Ï•ğ’ˆkâˆ’q~kâ€–Lâˆâ€‹(Qr~)â‰¤NÎ´â€‹Î´~\displaystyle\|\phi\_{{\bm{g}}\_{k}}-\widetilde{q}\_{k}\|\_{L^{\infty}(Q\_{\widetilde{r}})}\leq N^{\delta}\widetilde{\delta}, with Î´~\displaystyle\widetilde{\delta} to be specified later.
We hence have,

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–âˆ‘k=1NNÎ´hâ€‹(ğ’ˆk)â€‹q~kâˆ’hÂ¯â€–Lâˆâ€‹(Qr~)â‰¤\displaystyle\displaystyle\left\|\sum\_{k=1}^{N^{N^{\delta}}}h({\bm{g}}\_{k})\widetilde{q}\_{k}-\bar{h}\right\|\_{L^{\infty}(Q\_{\widetilde{r}})}\leq | âˆ‘k=1NNÎ´|hâ€‹(ğ’ˆk)|â€‹â€–q~kâˆ’Ï•ğ’ˆkâ€–Lâˆâ€‹(Qr~)\displaystyle\displaystyle\sum\_{k=1}^{N^{N^{\delta}}}|h({\bm{g}}\_{k})|\|\widetilde{q}\_{k}-\phi\_{{\bm{g}}\_{k}}\|\_{L^{\infty}(Q\_{\widetilde{r}})} |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | â‰¤\displaystyle\displaystyle\leq | NÎ´â€‹NNÎ´â€‹Î²ğ’ˆâ€‹Î´~.\displaystyle\displaystyle N^{\delta}N^{N^{\delta}}\beta\_{\bm{g}}\widetilde{\delta}. |  | (6.23) |

Now set Î´~=Câ€‹Îµ2NÎ´â€‹NNÎ´â€‹Î²ğ’ˆ\displaystyle\widetilde{\delta}=C\frac{\varepsilon\_{2}}{N^{\delta}N^{N^{\delta}}\beta\_{\bm{g}}}, where C\displaystyle C is a constant.
Consequently the network has an architecture, â„±2=â„±NNâ€‹(NÎ´,1,â„’2,ğ”­2,K2,Îº2,R2)\displaystyle\mathcal{F}\_{2}=\mathcal{F}\_{\rm NN}(N^{\delta},1,\mathcal{L}\_{2},\mathfrak{p}\_{2},K\_{2},\kappa\_{2},R\_{2}), where

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„’2\displaystyle\displaystyle\mathcal{L}\_{2} | =Oâ€‹((NÎ´+12â€‹(NÎ´)2)â€‹logâ¡(NÎ´)+(2â€‹(NÎ´)2+NÎ´)â€‹logâ¡Îµ2âˆ’1+NÎ´â€‹logâ¡(r~)),ğ”­2=Oâ€‹(1),\displaystyle\displaystyle=O\left((N^{\delta}+\frac{1}{2}(N^{\delta})^{2})\log(N^{\delta})+(2(N^{\delta})^{2}+N^{\delta})\log\varepsilon\_{2}^{-1}+N^{\delta}\log(\widetilde{r})\right),\mathfrak{p}\_{2}=O(1), |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | K2\displaystyle\displaystyle K\_{2} | =Oâ€‹((NÎ´+12â€‹(NÎ´)2)â€‹logâ¡(NÎ´)+(2â€‹(NÎ´)2+NÎ´)â€‹logâ¡Îµ2âˆ’1+NÎ´â€‹logâ¡(r~)),\displaystyle\displaystyle=O\left((N^{\delta}+\frac{1}{2}(N^{\delta})^{2})\log(N^{\delta})+(2(N^{\delta})^{2}+N^{\delta})\log\varepsilon\_{2}^{-1}+N^{\delta}\log(\widetilde{r})\right), |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | Îº2\displaystyle\displaystyle\kappa\_{2} | =Oâ€‹(Îµ2âˆ’NÎ´),R2=1,\displaystyle\displaystyle=O(\varepsilon\_{2}^{-N^{\delta}}),R\_{2}=1, |  |

here the hidden constant depends on LÎ“A\displaystyle L\_{\Gamma\_{A}} and Î²ğ’ˆ\displaystyle\beta\_{\bm{g}}.
âˆ

## 7 Algorithm

A unifying view to solve a basket of American options has been studied in
[[3](https://arxiv.org/html/2511.07235v1#bib.bib3)] by using one unified Snell envelope. In this section, we use the proposed deep neural operator to solve this basket of American options problem. In particular, we are able to provide the exercise boundary for new strike prices within the range of our training sets. The precise training process and model specification is presented below.

In the following numerical example, we train a deep operator neural network to obtain the pricing operator of Bermudan put option. Then we plot the exercise boundaries for various terminal payoff functions in Figure [1](https://arxiv.org/html/2511.07235v1#S7.F1 "Figure 1 â€£ 7 Algorithm â€£ Deep Neural Operator Learning for Probabilistic Models").

The ground-truth training data are produced by a fully implicit finite-difference discretization of the Blackâ€“Scholes American pricing PDE in log-price variables on a uniform grid, closely following standard references on PDE methods for options [[43](https://arxiv.org/html/2511.07235v1#bib.bib43)].

Under the riskâ€“neutral measure, the price of an American option with strike K\displaystyle K, volatility Ïƒ>0\displaystyle\sigma>0, and riskâ€“free rate r>0\displaystyle r>0 satisfies the Blackâ€“Scholes PDE with free boundary as in ([6.4](https://arxiv.org/html/2511.07235v1#S6.E4 "In 6 Deep neural operator for American option pricing and PDE with free boundary â€£ Deep Neural Operator Learning for Probabilistic Models"))
with terminal condition uâ€‹(T,x)=gkâ€‹(x)=maxâ¡(Kâˆ’x,0).\displaystyle u(T,x)=g\_{k}(x)=\max(K-x,0).

The spaceâ€“time grid is given by xj=xmin+jâ€‹Î”â€‹x\displaystyle x\_{j}=x\_{\min}+j\Delta x for j=0,â€¦,Nxâˆ’1\displaystyle j=0,\dots,N\_{x}-1 and tn=nâ€‹Î”â€‹t\displaystyle t\_{n}=n\Delta t for n=0,â€¦,Nt\displaystyle n=0,\dots,N\_{t}, with Î”â€‹x=(xmaxâˆ’xmin)/(Nxâˆ’1)\displaystyle\Delta x=(x\_{\max}-x\_{\min})/(N\_{x}-1), Î”â€‹t=T/Nt\displaystyle\Delta t=T/N\_{t}. In our numerical example, we set the risk-free interest rate r=0.1\displaystyle r=0.1, the volatility Ïƒ=0.2\displaystyle\sigma=0.2, the time to maturity T=1\displaystyle T=1, Nt=50\displaystyle N\_{t}=50 and Nx=300\displaystyle N\_{x}=300. The price interval is chosen wide enough to contain the early-exercise region for all strikes in the training range: with Kmin=90\displaystyle K\_{\min}=90 and Kmax=120\displaystyle K\_{\max}=120, we take xmin=Kmin/2=45\displaystyle x\_{\min}=K\_{\min}/2=45 and xmax=1.5â€‹Kmax=180\displaystyle x\_{\max}=1.5K\_{\max}=180.

We first perform a log transformation such that let y=lnâ¡x\displaystyle y=\ln x and define vâ€‹(y,t):=uâ€‹(x,t)=uâ€‹(ey,t)\displaystyle v(y,t):=u(x,t)=u(e^{y},t). Then

|  |  |  |
| --- | --- | --- |
|  | ux=1xâ€‹vy,uxâ€‹x=1x2â€‹(vyâ€‹yâˆ’vy),u\_{x}=\frac{1}{x}v\_{y},\qquad u\_{xx}=\frac{1}{x^{2}}\left(v\_{yy}-v\_{y}\right), |  |

and ([6.4](https://arxiv.org/html/2511.07235v1#S6.E4 "In 6 Deep neural operator for American option pricing and PDE with free boundary â€£ Deep Neural Operator Learning for Probabilistic Models")) becomes the constantâ€“coefficient convectionâ€“diffusion equation

|  |  |  |  |
| --- | --- | --- | --- |
|  | âˆ‚tv+12â€‹Ïƒ2â€‹vyâ€‹y+Î¼â€‹vyâˆ’râ€‹v=0,Î¼:=râˆ’12â€‹Ïƒ2.\partial\_{t}v+\frac{1}{2}\sigma^{2}v\_{yy}+\mu\,v\_{y}-rv=0,\qquad\mu:=r-\tfrac{1}{2}\sigma^{2}. |  | (7.1) |

Denote vjnâ‰ˆvâ€‹(yj,tn)\displaystyle v\_{j}^{n}\approx v(y\_{j},t\_{n}).
Use centered differences in space at time level n\displaystyle n:

|  |  |  |
| --- | --- | --- |
|  | vyâ€‹(yj,tn)â‰ˆvj+1nâˆ’vjâˆ’1n2â€‹Î”â€‹y,vyâ€‹yâ€‹(yj,tn)â‰ˆvj+1nâˆ’2â€‹vjn+vjâˆ’1nÎ”â€‹y2.v\_{y}(y\_{j},t\_{n})\approx\frac{v\_{j+1}^{n}-v\_{j-1}^{n}}{2\Delta y},\qquad v\_{yy}(y\_{j},t\_{n})\approx\frac{v\_{j+1}^{n}-2v\_{j}^{n}+v\_{j-1}^{n}}{\Delta y^{2}}. |  |

The finite difference algorithm runs backward in time from tNt=T\displaystyle t\_{N\_{t}}=T to t0=0\displaystyle t\_{0}=0, the fully implicit step

|  |  |  |  |
| --- | --- | --- | --- |
|  | vjnâˆ’vjn+1Î”â€‹t+12â€‹Ïƒ2â€‹vj+1nâˆ’2â€‹vjn+vjâˆ’1nÎ”â€‹x2+Î¼â€‹vj+1nâˆ’vjâˆ’1n2â€‹Î”â€‹xâˆ’râ€‹vjn=0.\frac{v\_{j}^{n}-v\_{j}^{n+1}}{\Delta t}+\frac{1}{2}\sigma^{2}\frac{v\_{j+1}^{n}-2v\_{j}^{n}+v\_{j-1}^{n}}{\Delta x^{2}}+\mu\frac{v\_{j+1}^{n}-v\_{j-1}^{n}}{2\Delta x}-rv\_{j}^{n}=0. |  | (7.2) |

After rearranging into matrix form, and enforcing the free boundary condition, (v)n\displaystyle(v)^{n} are obtained from (v)n+1\displaystyle(v)^{n+1}.

Our approach uses operator learning for Bermudan-style put options. From sampled spaceâ€“time values of option prices, we train a neural operator that, given a payoff function, reconstructs the full price surface and thereby recovers the optimal exercise (stopping) boundary.

Let Î“\displaystyle\Gamma denote the pricing operator that maps a payoff gk\displaystyle g\_{k} to its price surface uk\displaystyle u\_{k}. On a grid {xj}j=1NxÃ—{tn}n=1Nt\displaystyle\{x\_{j}\}\_{j=1}^{N\_{x}}\times\{t\_{n}\}\_{n=1}^{N\_{t}},

|  |  |  |
| --- | --- | --- |
|  | ukâ€‹(xj,tn)=Î“â€‹gkâ€‹(xj,tn).u\_{k}(x\_{j},t\_{n})=\Gamma g\_{k}(x\_{j},t\_{n}). |  |

The neural operator Î“Î¸\displaystyle\Gamma\_{\theta} with parameters Î¸\displaystyle\theta is trained to approximate the pricing operator by minimizing the empirical mean-squared error

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„’â€‹(Î¸)=1Kâ€‹Ntâ€‹Nxâ€‹âˆ‘k=1Kâˆ‘j=1Nxâˆ‘n=1Nt|Î“Î¸â€‹gkâ€‹(xj,tn)âˆ’ukâ€‹(xj,tn)|2.\mathcal{L}(\theta)=\frac{1}{K\,N\_{t}\,N\_{x}}\sum\_{k=1}^{K}\sum\_{j=1}^{N\_{x}}\sum\_{n=1}^{N\_{t}}\bigl|\Gamma\_{\theta}g\_{k}(x\_{j},t\_{n})-u\_{k}(x\_{j},t\_{n})\bigr|^{2}. |  | (7.3) |

This optimization ensures that the operator network learns an accurate mapping from input payoff functions to their corresponding option price surfaces. Our algorithm is capable of computing the exercise boundary for any strike price between 90 and 120. We select six payoff functions with different strike prices and present their corresponding exercise boundaries in the Figure [1](https://arxiv.org/html/2511.07235v1#S7.F1 "Figure 1 â€£ 7 Algorithm â€£ Deep Neural Operator Learning for Probabilistic Models"). Once trained, the learned operator Î“Î¸\displaystyle\Gamma\_{\theta} can be utilized to recover the entire exercise boundary from the approximated solution surface.

![[Uncaptioned image]](figures/Strike90.png)

![[Uncaptioned image]](figures/Strike96.png)

![[Uncaptioned image]](figures/Strike101.png)

![[Uncaptioned image]](figures/Strike107.png)



![Refer to caption](figures/Strike113.png)

![Refer to caption](figures/Strike117.png)

Figure 1: Exercise boundaries for American put options.

## References

* [1]

  Guillermo Alvarez, Ibrahim Ekren, Anastasis Kratsios, and Xuwei Yang.
  Neural operators can play dynamic stackelberg games.
  arXiv preprint arXiv:2411.09644, 2024.
* [2]

  Robert Azencott.
  Formule de taylor stochastique et dÃ©veloppement asymptotique
  dâ€™intÃ©grales de feynmann.
  In SÃ©minaire de ProbabilitÃ©s XVI, 1980/81
  SupplÃ©ment: GÃ©omÃ©trie DiffÃ©rentielle Stochastique, pages
  237â€“285. Springer, 2006.
* [3]

  Peter Bank and Hans FÃ¶llmer.
  American options, multiâ€“armed bandits, and optimal consumption
  plans: A unifying view.
  In Paris-Princeton Lectures on Mathematical Finance 2002, pages
  1â€“42. Springer, 2003.
* [4]

  Fabrice Baudoin, Eulalia Nualart, Cheng Ouyang, and Samy Tindel.
  On probability laws of solutions to differential systems driven by a
  fractional brownian motion.
  Ann. Probab., 44(4):2554â€“2590, 2016.
* [5]

  Christian Bayer, PaulÂ P Hager, Sebastian Riedel, and John Schoenmakers.
  Optimal stopping with signatures.
  The Annals of Applied Probability, 33(1):238â€“273, 2023.
* [6]

  Erhan Bayraktar, QiÂ Feng, and Zhaoyu Zhang.
  Deep signature algorithm for multidimensional path-dependent options.
  SIAM Journal on Financial Mathematics, 15(1):194â€“214, 2024.
* [7]

  Sebastian Becker, Patrick Cheridito, and Arnulf Jentzen.
  Deep optimal stopping.
  Journal of Machine Learning Research, 20(74):1â€“25, 2019.
* [8]

  Sebastian Becker, Patrick Cheridito, Arnulf Jentzen, and Timo Welti.
  Solving high-dimensional optimal stopping problems using deep
  learning.
  European Journal of Applied Mathematics, 32(3):470â€“514, 2021.
* [9]

  Kaushik Bhattacharya, Bamdad Hosseini, NikolaÂ B Kovachki, and AndrewÂ M Stuart.
  Model reduction and neural networks for parametric pdes.
  The SMAI journal of computational mathematics, 7:121â€“157,
  2021.
* [10]

  Cristian Bodnar, WesselÂ P Bruinsma, Ana Lucic, Megan Stanley, Anna Allen,
  Johannes Brandstetter, Patrick Garvan, Maik Riechert, JonathanÂ A Weyn, Haiyu
  Dong, etÂ al.
  A foundation model for the earth system.
  Nature, pages 1â€“8, 2025.
* [11]

  Fabienne Castell.
  Asymptotic expansion of stochastic flows.
  Probability theory and related fields, 96(2):225â€“239, 1993.
* [12]

  Tianping Chen and Hong Chen.
  Approximations of continuous functionals by neural networks with
  application to dynamic systems.
  IEEE Transactions on Neural networks, 4(6):910â€“918, 1993.
* [13]

  Tianping Chen and Hong Chen.
  Universal approximation to nonlinear operators by neural networks
  with arbitrary activation functions and its application to dynamical systems.
  IEEE Transactions on Neural Networks, 6(4):911â€“917, 1995.
* [14]

  JohnÂ Horton Conway and Neil JamesÂ Alexander Sloane.
  Sphere packings, lattices and groups, volume 290.
  Springer Science & Business Media, 2013.
* [15]

  Nicole ElÂ Karoui, Christophe Kapoudjian, Etienne Pardoux, Shige Peng, and
  Marie-Claire Quenez.
  Reflected solutions of backward sdeâ€™s, and related obstacle problems
  for pdeâ€™s.
  the Annals of Probability, 25(2):702â€“737, 1997.
* [16]

  QiÂ Feng and Xuejing Zhang.
  Taylor expansions and castell estimates for solutions of stochastic
  differential equations driven by rough paths.
  Journal of Stochastic Analysis, 1(2):4, 2020.
* [17]

  Takashi Furuya and Anastasis Kratsios.
  Simultaneously solving fbsdes with neural operators of logarithmic
  depth, constant width, and sub-linear rank.
  arXiv preprint arXiv:2410.14788, 2024.
* [18]

  Chengfan Gao, Siping Gao, Ruimeng Hu, and Zimu Zhu.
  Convergence of the backward deep bsde method with applications to
  optimal stopping problems.
  SIAM Journal on Financial Mathematics, 14(4):1290â€“1303, 2023.
* [19]

  Lukas Gonon.
  Deep neural network expressivity for optimal stopping problems.
  Finance and Stochastics, 28(3):865â€“910, 2024.
* [20]

  Calypso Herrera, Florian Krach, Pierre Ruyssen, and Josef Teichmann.
  Optimal stopping via randomized neural networks.
  Frontiers of Mathematical Finance, 3(1):31â€“77, 2024.
* [21]

  AmandaÂ A Howard, Mauro Perego, GeorgeÂ E Karniadakis, and Panos Stinis.
  Multifidelity deep operator networks.
  arXiv preprint arXiv:2204.09157, 2022.
* [22]

  CÃ´me HurÃ©, HuyÃªn Pham, and Xavier Warin.
  Deep backward schemes for high-dimensional nonlinear pdes.
  Mathematics of Computation, 89(324):1547â€“1579, 2020.
* [23]

  Nikola Kovachki, Samuel Lanthaler, and Siddhartha Mishra.
  On universal approximation and error bounds for fourier neural
  operators.
  Journal of Machine Learning Research, 22(290):1â€“76, 2021.
* [24]

  Nikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik
  Bhattacharya, Andrew Stuart, and Anima Anandkumar.
  Neural operator: Learning maps between function spaces with
  applications to pdes.
  Journal of Machine Learning Research, 24(89):1â€“97, 2023.
* [25]

  Samuel Lanthaler.
  Operator learning with pca-net: upper and lower complexity bounds.
  Journal of Machine Learning Research, 24(318):1â€“67, 2023.
* [26]

  Samuel Lanthaler, Siddhartha Mishra, and GeorgeÂ E Karniadakis.
  Error estimates for deeponets: A deep learning framework in infinite
  dimensions.
  Transactions of Mathematics and Its Applications, 6(1):tnac001,
  2022.
* [27]

  Samuel Lanthaler and AndrewÂ M Stuart.
  The parametric complexity of operator learning.
  IMA Journal of Numerical Analysis, page draf028, 2025.
* [28]

  Bernard Lapeyre and JÃ©rÃ´me Lelong.
  Neural network regression for bermudan option pricing.
  Monte Carlo Methods and Applications, 27(3):227â€“247, 2021.
* [29]

  WingÂ Tat Leung, Guang Lin, and Zecheng Zhang.
  Nh-pinn: Neural homogenization-based physics-informed neural network
  for multiscale problems.
  Journal of Computational Physics, 470:111539, 2022.
* [30]

  Zongyi Li, DanielÂ Zhengyu Huang, Burigede Liu, and Anima Anandkumar.
  Fourier neural operator with learned deformations for pdes on general
  geometries.
  Journal of Machine Learning Research, 24(388):1â€“26, 2023.
* [31]

  Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik
  Bhattacharya, Andrew Stuart, and Anima Anandkumar.
  Fourier neural operator for parametric partial differential
  equations.
  arXiv preprint arXiv:2010.08895, 2020.
* [32]

  Guang Lin, Christian Moya, and Zecheng Zhang.
  B-deeponet: An enhanced bayesian deeponet for solving noisy
  parametric pdes using accelerated replica exchange sgld.
  Journal of Computational Physics, 473:111713, 2023.
* [33]

  Hao Liu, Haizhao Yang, Minshuo Chen, Tuo Zhao, and Wenjing Liao.
  Deep nonparametric estimation of operators between infinite
  dimensional spaces.
  Journal of Machine Learning Research, 25(24):1â€“67, 2024.
* [34]

  Hao Liu, Zecheng Zhang, Wenjing Liao, and Hayden Schaeffer.
  Neural scaling laws of deep relu and deep operator network: A
  theoretical study.
  arXiv preprint arXiv:2410.00357, 2024.
* [35]

  LuÂ Lu, Pengzhan Jin, Guofei Pang, Zhongqiang Zhang, and GeorgeÂ Em Karniadakis.
  Learning nonlinear operators via deeponet based on the universal
  approximation theorem of operators.
  Nature Machine Intelligence, 3(3):218â€“229, 2021.
* [36]

  Jaideep Pathak, Shashank Subramanian, Peter Harrington, Sanjeev Raja, Ashesh
  Chattopadhyay, Morteza Mardani, Thorsten Kurth, David Hall, Zongyi Li, Kamyar
  Azizzadenesheli, etÂ al.
  Fourcastnet: A global data-driven high-resolution weather model using
  adaptive fourier neural operators.
  arXiv preprint arXiv:2202.11214, 2022.
* [37]

  AÂ Max Reppen, HÂ Mete Soner, and Valentin Tissot-Daguette.
  Neural optimal stopping boundary.
  arXiv preprint arXiv:2205.04595, 2022.
* [38]

  Mykhaylo Shkolnikov, HÂ Mete Soner, and Valentin Tissot-Daguette.
  Deep level-set method for stefan problems.
  Journal of Computational Physics, 503:112828, 2024.
* [39]

  Justin Sirignano and Konstantinos Spiliopoulos.
  Dgm: A deep learning algorithm for solving partial differential
  equations.
  Journal of computational physics, 375:1339â€“1364, 2018.
* [40]

  HÂ Mete Soner and Valentin Tissot-Daguette.
  Stopping times of boundaries: Relaxation and continuity.
  SIAM Journal on Control and Optimization, 63(4):2835â€“2855,
  2025.
* [41]

  LoringÂ W Tu.
  Manifolds.
  In An Introduction to Manifolds, pages 47â€“83. Springer, 2011.
* [42]

  Jindong Wang and Wenrui Hao.
  Laplacian eigenfunction-based neural operator for learning nonlinear
  reactionâ€“diffusion dynamics.
  Journal of Computational Physics, page 114400, 2025.
* [43]

  Paul Wilmott, Sam Howison, and Jeff Dewynne.
  The Mathematics of Financial Derivatives: A Student
  Introduction.
  Cambridge University Press, 1995.
* [44]

  Dmitry Yarotsky.
  Error bounds for approximations with deep relu networks.
  Neural networks, 94:103â€“114, 2017.
* [45]

  Jianfeng Zhang and Jianfeng Zhang.
  Backward stochastic differential equations.
  Springer, 2017.