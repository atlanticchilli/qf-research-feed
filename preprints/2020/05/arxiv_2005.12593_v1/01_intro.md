---
authors:
- Bruno Bouchard
- Adil Reghai
- Benjamin Virrion
doc_id: arxiv:2005.12593v1
family_id: arxiv:2005.12593
is_current: true
taxonomy:
  alpha_families: []
  asset_classes: []
  horizons: []
  themes: []
title: '[2005.12593] Computation of Expected Shortfall by fast detection of worst
  scenarios'
url_abs: http://arxiv.org/abs/2005.12593v1
url_html: https://ar5iv.org/html/2005.12593v1
venue: arXiv q-fin
version: 1
year: 2020
---


Bruno Bouchard111CEREMADE, CNRS, UniversitÃ© Paris Dauphine, PSL University., Adil Reghai222Natixis., Benjamin Virrion333Natixis and CEREMADE, CNRS, UniversitÃ© Paris Dauphine, PSL University.,,{}^{\;\>,}444The authors would like to thank Nicolas Baradel for helping with the code, Rida Mahi and Mathieu Bernardo from the Natixis Quantitative Research Teams for providing the first results and ideas on the Fast Detection Algorithm, and finally William Leduc for providing all the necessary data to obtain the different book parameters.

###### Abstract

We consider a multi-step algorithm for the computation of the historical expected shortfall such as defined by the Basel Minimum Capital Requirements for Market Risk.
At each step of the algorithm, we use Monte Carlo simulations to reduce the number of historical scenarios that potentially belong to the set of worst scenarios. The number of simulations increases as the number of candidate scenarios is reduced and the distance between them diminishes. For the most naive scheme, we show that the ğ•ƒpsuperscriptğ•ƒğ‘{{\mathbb{L}}}^{p}-error of the estimator of the Expected Shortfall is bounded by a linear combination of the probabilities of inversion of favorable and unfavorable scenarios at each step, and of the last step Monte Carlo error associated to each scenario. By using concentration inequalities, we then show that, for sub-gamma pricing errors, the probabilities of inversion converge at an exponential rate in the number of simulated paths. We then propose an adaptative version in which the algorithm improves step by step its knowledge on the unknown parameters of interest: mean and variance of the Monte Carlo estimators of the different scenarios. Both schemes can be optimized by using dynamic programming algorithms that can be solved off-line.
To our knowledge, these are the first non-asymptotic bounds for such estimators. Our hypotheses are weak enough to allow for the use of estimators for the different scenarios and steps based on the same random variables, which, in practice, reduces considerably the computational effort. First numerical tests are performed.

Keywords: Expected Shortfall, ranking and selection, sequential design, Bayesian filter.

## 1 Introduction

The Basel Minimum Capital Requirements for Market Risk [[4](#bib.bib4)] has brought two main changes in the way that investment banks need to compute their capital requirements.
Expected Shortfall (ESES{\rm ES}) replaces Value at Risk (VaR) as the main risk indicator for the computation of capital requirements. The advantages of ES over VaR have been brought forward in Artzner et al.Â [[2](#bib.bib2)], and Expected Shortfall is now considered by most researchers and practitioners as superior to VaR as a risk measure, because it respects the sub-additivity axiom, see [[1](#bib.bib1), [2](#bib.bib2), [23](#bib.bib23)].
The second main change is that the number of required daily computations of ESES{\rm ES} has been multiplied by a factor of up to 90. Where banks used to need to compute one VaR per day, they now need to compute up to three ESES{\rm ES} per liquidity horizon and risk class, as well as three ESES{\rm ES} per liquidity horizon for all risk classes combined.
The above has triggered several works on the fast computation of ESES{\rm ES}.

Mathematically, if Vğ‘‰V is a random variable modeling the level of loss555All over this paper, we measure the performances in terms of losses. A positive number is a loss, a negative number is a gain. of a portfolio that will be known at a future time, and 0<Î±<10ğ›¼10<\alpha<1, the expected shortfall of level Î±âˆˆ(0,1)ğ›¼01\alpha\in(0,1) is defined by

|  |  |  |  |
| --- | --- | --- | --- |
|  | ESÎ±:=1Î±â€‹âˆ«0Î±VaRÎ³â€‹(V)â€‹ğ‘‘Î³,assignsubscriptESğ›¼1ğ›¼superscriptsubscript0ğ›¼subscriptVaRğ›¾ğ‘‰differential-dğ›¾{\rm ES}\_{\alpha}:=\frac{1}{\alpha}\int\_{0}^{\alpha}{\rm VaR}\_{\gamma}({V})d\gamma, |  | (1) |

where VaRÎ³subscriptVaRğ›¾{\rm VaR}\_{\gamma} is the Value at Risk at level Î³ğ›¾\gamma, i.e.

|  |  |  |  |
| --- | --- | --- | --- |
|  | VaRÎ³â€‹(V):=maxâ¡{xâˆˆâ„:â„™â€‹[Vâ‰¥x]>Î³}.assignsubscriptVaRğ›¾ğ‘‰:ğ‘¥â„â„™delimited-[]ğ‘‰ğ‘¥ğ›¾{\rm VaR}\_{\gamma}\left({V}\right):={\max\left\{x\in\mathbb{R}:{\mathbb{P}}[V\geq x]>\gamma\right\}}. |  | (2) |

Nearly all of the literature concentrates on studying the ESES{\rm ES} by using parametric, non-parametric or semi-parametric approaches to approximate the distribution of Vğ‘‰{V} based on historical data. See in particular [[9](#bib.bib9), [11](#bib.bib11), [12](#bib.bib12), [15](#bib.bib15), [16](#bib.bib16), [19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21), [24](#bib.bib24), [25](#bib.bib25), [26](#bib.bib26), [27](#bib.bib27)]. Another approach consists in using the fact that Vğ‘‰V is the risk neutral value of a book, and therefore of the form ğ”¼â€‹[P|S]ğ”¼delimited-[]conditionalğ‘ƒğ‘†\mathbb{E}[P|S] in which Sğ‘†S is a random variable associated to market parameters and Pğ‘ƒP represents the future (discounted) payoffs of the book. This suggests using a nested Monte Carlo approach : simulate a set of values in the distribution of Sğ‘†S (outer scenarios), and, for each simulation of Sğ‘†S, compute a Monte Carlo estimator of ğ”¼â€‹[P|S]ğ”¼delimited-[]conditionalğ‘ƒğ‘†{\mathbb{E}}[P|S] by using simulations in the conditional distribution (inner scenarios). This is for instance the approach of [[8](#bib.bib8), [13](#bib.bib13)].

But, as defined in the regulatory document of Basel [[4](#bib.bib4)], the expected shortfall is based on ns=253subscriptğ‘›ğ‘ 253n\_{s}=253 scenarios of market parameters s=(si)iâ‰¤nsssubscriptsuperscriptsğ‘–ğ‘–subscriptğ‘›ğ‘ {\rm s}=({\rm s}^{i})\_{i\leq n\_{s}} that are generated in a deterministic way. Therefore, Sğ‘†S is just uniformly distributed in the sequence ss{\rm s} and there is no need for simulating outer scenarios. Since Vğ‘‰V is defined by a pricing formula ğ”¼â€‹[P|S]ğ”¼delimited-[]conditionalğ‘ƒğ‘†\mathbb{E}[P|S] that is fully described by the value of Sğ‘†S, there is also no room for approximating the law of Vğ‘‰V based on historical data, if we are only interested by the requirements of [[4](#bib.bib4)].
The only issue is to compute in an efficient way the loss impacts (Î¼i)iâ‰¤nssubscriptsuperscriptğœ‡ğ‘–ğ‘–subscriptğ‘›ğ‘ (\mu^{i})\_{i\leq n\_{s}} of the book,

|  |  |  |
| --- | --- | --- |
|  | Î¼i:=(ğ”¼â€‹[P|S=si]âˆ’ğ”¼â€‹[P|S=s0]),i=1â€‹â€¦,ns,formulae-sequenceassignsuperscriptğœ‡ğ‘–ğ”¼delimited-[]conditionalğ‘ƒğ‘†superscriptsğ‘–ğ”¼delimited-[]conditionalğ‘ƒğ‘†superscripts0ğ‘–  1â€¦subscriptğ‘›ğ‘ \mu^{i}:=\left(\mathbb{E}[P|S={\rm s}^{i}]-\mathbb{E}[P|S={\rm s}^{0}]\right),\;i=1\ldots,n\_{s}, |  |

in which s0superscripts0{\rm s}^{0} is the current value of the market parameters, and then compute the average over the nw=6subscriptğ‘›ğ‘¤6n\_{w}=6 worst impacts, say

|  |  |  |  |
| --- | --- | --- | --- |
|  | ES=1nwâ€‹âˆ‘i=1nwÎ¼i,ES1subscriptğ‘›ğ‘¤superscriptsubscriptğ‘–1subscriptğ‘›ğ‘¤superscriptğœ‡ğ‘–{\rm ES}=\frac{1}{n\_{w}}\sum\_{i=1}^{n\_{w}}\mu^{i}, |  | (3) |

if, for ease of notations, we assume that

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î¼1â‰¥Î¼2â‰¥â‹¯â‰¥Î¼nsâˆ’1â‰¥Î¼ns.superscriptğœ‡1superscriptğœ‡2â‹¯superscriptğœ‡subscriptğ‘›ğ‘ 1superscriptğœ‡subscriptğ‘›ğ‘ \mu^{1}\geq\mu^{2}\geq\cdots\geq\mu^{n\_{s}-1}\geq\mu^{n\_{s}}. |  | (4) |

Methods that are in line with the above have also been studied, in particular in [[17](#bib.bib17), [22](#bib.bib22)] in which the authors define a distance on the space of scenarios induced by the distance between their risk factors. Starting with the original outer-level scenarios (called â€œprediction pointsâ€), they determine â€œdesign pointsâ€ that are included in their convex hull. Inner-level paths are simulated in order to evaluate the portfolio value at the design points. These values are then used to establish a metamodel of the portfolio price with respect to the risk factors, and this metamodel is then used to select among the prediction points those that are most likely to be part of the worst scenarios set. They are then added to the design points, and evaluated by using inner-level simulations, after which the metamodel is updated.

These methods are very smart but neglect one important point for practitioners: the cost of launching a pricer is high, as it typically entails instanciating thousands of objects at initialization, as well as volatility surface calibrations and sometimes even graphical interfaces. Furthermore, these pricers usually do not have the flexibility to add dynamically, at each inner-level pricing, new paths to a given scenario. Therefore, we do not allow ourselves to adapt our strategies at such a level of granularity.

Instead, we will consider strategies that only entail Lğ¿L-levels of sets of simulations, where Lğ¿L is typically quite low, so as not to pay too many times the overhead of launching the pricer and/or calibrating the required volatility surfaces.
We also do not use any concept of distance between scenarios induced by their risk factors. Although this enables [[17](#bib.bib17)] and [[22](#bib.bib22)] to obtain better empirical convergence rates, we see at least one problem with this approach: at the scale of a bank, the space of risk factors is both of a very high dimension (a few thousands) and with a very complex geometry (the payoffs of the portfolioâ€™s derivative products are usually non-convex, and path-dependent), so that it is very difficult to establish a model describing the proximity of scenarios in a robust way.

We thus study a relatively simple procedure that also has the advantage of allowing us to establish non-asymptotic bounds on the ğ•ƒpsuperscriptğ•ƒğ‘\mathbb{L}^{p}-error of our estimator, in the spirit of the simplest ranking by mean procedures, see e.g.Â [[3](#bib.bib3), [5](#bib.bib5), [6](#bib.bib6), [14](#bib.bib14)]. It consists in using a first set of simulated paths to provide a crude estimation of the impact factors Î¼isuperscriptğœ‡ğ‘–\mu^{i}. These first estimators are ranked to select the q1<nssubscriptğ‘1subscriptğ‘›ğ‘ q\_{1}<n\_{s} outer-level scenarios with the highest estimated impact values. Then, only the impact values of these q1subscriptğ‘1q\_{1} pre-selected scenarios are estimated again by using the previous estimators together with a new set of simulated paths. Among these new estimators we select the scenarios with the q2<q1subscriptğ‘2subscriptğ‘1q\_{2}<q\_{1} highest estimated impact factors. And so on. After Lâ‰¥2ğ¿2L\geq 2 steps, Lğ¿L being small in practice, we just keep the mean of the six highest estimated impacts.

The rationale behind this is that a first crude estimation should be sufficient to rule out a large part of the scenarios from the candidates of being in the 666 worst ones, because the corresponding values should be far enough. While the number of candidates reduces, one can expect that the differences between the corresponding impacts diminish as well and that more Monte Carlo simulations are needed to differentiate them. Under an indifference zone hypothesis, similar to the one used in the above mentioned paper, and a sub-gamma distribution assumption, the convergence is exponential in the number of simulations used at the different steps and of order 1/2121/2 in the total number of simulations. See Proposition [2.2](#S2.Thmtheorem2 "Proposition 2.2. â€£ 2.2 General a-priori bound on the ğ•ƒ^ğ‘ error â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios") and Corollary [2.3](#S2.Thmtheorem3 "Corollary 2.3. â€£ 2.3 Error bound for Sub-Gamma distributions â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios") below.

The optimal number of additional paths that should be used at each step to minimize the strong estimation error, given a maximal computational cost, can be determined by a simple dynamic programming algorithm, that can be solved off-line, see Section [2.4](#S2.SS4 "2.4 Optimal a-priori allocation by deterministic dynamic programming based on fixed a-priori bounds â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios"). In theory, this requires the a priori knowledge of the means and covariances of our estimators, which are obviously not known in practice. However, one can easily define a version based on a robust specification of the error. One can also take advantage of the different simulation sets to improve our prior on the true hidden parameters. This leads to a natural adaptative algorithm, see Section [3](#S3 "3 Adaptative algorithm â€£ Computation of Expected Shortfall by fast detection of worst scenarios"), for which convergence is also proved, see Proposition [3.3](#S3.Thmtheorem3 "Proposition 3.3. â€£ 3.1 Error bounds and convergence for predictable strategies â€£ 3 Adaptative algorithm â€£ Computation of Expected Shortfall by fast detection of worst scenarios"). Estimating the optimal policy associated to this adaptative algorithm is costly but can be done off-line by using a neural network approximation combined with a backward dynamic programming algorithm. We explain how this can be done in Section [3.3](#S3.SS3 "3.3 Example of numerical implementation using neural networks â€£ 3 Adaptative algorithm â€£ Computation of Expected Shortfall by fast detection of worst scenarios") (further details are in Appendix [B](#A2 "Appendix B Precise implementation of the neural network algorithm â€£ Computation of Expected Shortfall by fast detection of worst scenarios")).

The rest of the paper is organized as follows. Section [2](#S2 "2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios") is dedicated to the most naive deterministic algorithm. In particular, Section [2.5](#S2.SS5 "2.5 Simplified 2-levels algorithm for a linear indifference zoneâ€™s size â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios") gives a very easy to use two levels algorithm for the case where the impacts decrease linearly in the scenariosâ€™ rank order. The adaptative version of the algorithm is presented in Section [3](#S3 "3 Adaptative algorithm â€£ Computation of Expected Shortfall by fast detection of worst scenarios"). Finally, we perform first numerical tests in Section [4](#S4 "4 Numerical Experiments â€£ Computation of Expected Shortfall by fast detection of worst scenarios").

## 2 Algorithm with a deterministic effort allocation

In this section, we describe the simplest version of the algorithm. It uses a pre-defined deterministic number of simulations. We establish a strong ğ•ƒpsuperscriptğ•ƒğ‘{\mathbb{L}}^{p}-error bound and discuss several ways of choosing the optimal strategy for minimizing this error.

### 2.1 The algorithm

From now on, we assume that ğ”¼â€‹[P|S=s0]ğ”¼delimited-[]conditionalğ‘ƒğ‘†superscripts0{\mathbb{E}}[P|S={\rm s}^{0}] is known perfectly and set it to 00 for ease of notations. As explained above, the algorithm relies on the idea of selecting progressively the scenarios that will be used for the computation of the Expected Shortfall. Namely, let P|s:=(P|si)iâ‰¤nsP\_{|\rm s}:=(P\_{|\rm s^{i}})\_{i\leq n\_{s}} be a nssubscriptğ‘›ğ‘ n\_{s}-dimensional random variable such that each P|siP\_{|{\rm s}^{i}} has the law of Pğ‘ƒP given S=siğ‘†superscriptsğ‘–S={\rm s}^{i}.
We first simulate independent copies (Pj1,â€¦,Pjns)jâ‰¥1subscriptsuperscriptsubscriptğ‘ƒğ‘—1â€¦superscriptsubscriptğ‘ƒğ‘—subscriptğ‘›ğ‘ ğ‘—1(P\_{j}^{1},\ldots,P\_{j}^{n\_{s}})\_{j\geq 1} of P|sP\_{|\rm s} and compute the Monte Carlo estimators of ğ”¼â€‹[P|S=si]ğ”¼delimited-[]conditionalğ‘ƒğ‘†superscriptsğ‘–{\mathbb{E}}[P|S={\rm s}^{i}], iâ‰¤nsğ‘–subscriptğ‘›ğ‘ i\leq n\_{s}:

|  |  |  |
| --- | --- | --- |
|  | Î¼^1i:=1N1â€‹âˆ‘j=1N1Pjiâ€‹Â forÂ â€‹iâ‰¤ns,assignsubscriptsuperscript^ğœ‡ğ‘–11subscriptğ‘1superscriptsubscriptğ‘—1subscriptğ‘1superscriptsubscriptğ‘ƒğ‘—ğ‘–Â forÂ ğ‘–subscriptğ‘›ğ‘ \hat{\mu}^{i}\_{1}:=\frac{1}{N\_{1}}\sum\_{j=1}^{N\_{1}}P\_{j}^{i}\;\mbox{ for }i\leq n\_{s}, |  |

for some N1â‰¥1subscriptğ‘11N\_{1}\geq 1.
Among these random variables, we then select the ones that are the most likely to coincide with the worst scenarios s1,â€¦,snw

superscripts1â€¦superscriptssubscriptğ‘›ğ‘¤{\rm s}^{1},\ldots,{\rm s}^{n\_{w}}, for some 1â‰¤nw<ns1subscriptğ‘›ğ‘¤subscriptğ‘›ğ‘ 1\leq n\_{w}<n\_{s}.
To do this, one considers the (random) permutation ğ”ª1subscriptğ”ª1{\mathfrak{m}}\_{1} on [[1,ns]]delimited-[]1subscriptğ‘›ğ‘ [\![1,n\_{s}]\!] such that the components of (Î¼^1ğ”ª1â€‹(i))iâ‰¤nssubscriptsuperscriptsubscript^ğœ‡1subscriptğ”ª1ğ‘–ğ‘–subscriptğ‘›ğ‘ \left(\hat{\mu}\_{1}^{{\mathfrak{m}}\_{1}(i)}\right)\_{i\leq n\_{s}} are in decreasing order:

|  |  |  |
| --- | --- | --- |
|  | {Î¼^1ğ”ª1â€‹(1)â‰¥Î¼^1ğ”ª1â€‹(2)â‰¥â€¦â‰¥Î¼^1ğ”ª1â€‹(ns),ğ”ª1â€‹(i)<ğ”ª1â€‹(iâ€²)â€‹Â ifÂ â€‹Î¼^1ğ”ª1â€‹(i)=Î¼^1ğ”ª1â€‹(iâ€²)â€‹Â forÂ â€‹1â‰¤i<iâ€²â‰¤ns,casessuperscriptsubscript^ğœ‡1subscriptğ”ª11superscriptsubscript^ğœ‡1subscriptğ”ª12â€¦superscriptsubscript^ğœ‡1subscriptğ”ª1subscriptğ‘›ğ‘ subscriptğ”ª1ğ‘–subscriptğ”ª1superscriptğ‘–â€²Â ifÂ superscriptsubscript^ğœ‡1subscriptğ”ª1ğ‘–superscriptsubscript^ğœ‡1subscriptğ”ª1superscriptğ‘–â€²Â forÂ 1ğ‘–superscriptğ‘–â€²subscriptğ‘›ğ‘ \left\{\begin{array}[]{l}\hat{\mu}\_{1}^{{\mathfrak{m}}\_{1}(1)}\geq\hat{\mu}\_{1}^{{\mathfrak{m}}\_{1}(2)}\geq\ldots\geq\hat{\mu}\_{1}^{{\mathfrak{m}}\_{1}(n\_{s})},\\ {\mathfrak{m}}\_{1}(i)<{\mathfrak{m}}\_{1}(i^{\prime})\;\mbox{ if }\hat{\mu}\_{1}^{{\mathfrak{m}}\_{1}(i)}=\hat{\mu}\_{1}^{{\mathfrak{m}}\_{1}(i^{\prime})}\mbox{ for }1\leq i<i^{\prime}\leq n\_{s},\end{array}\right. |  |

and only keep the indexes (ğ”ª1â€‹(â„“))â„“â‰¤q1subscriptsubscriptğ”ª1â„“â„“subscriptğ‘1({\mathfrak{m}}\_{1}(\ell))\_{\ell\leq q\_{1}} of the corresponding q1â‰¥nwsubscriptğ‘1subscriptğ‘›ğ‘¤q\_{1}\geq n\_{w} highest values, i.e.Â the indexes belonging to

|  |  |  |
| --- | --- | --- |
|  | â„‘1:=â„‘0âˆ©ğ”ª1â€‹([[1,q1]])â€‹Â in whichÂ â€‹â„‘0:=[[1,ns]].assignsubscriptâ„‘1subscriptâ„‘0subscriptğ”ª1delimited-[]1subscriptğ‘1Â in whichÂ subscriptâ„‘0assigndelimited-[]1subscriptğ‘›ğ‘ {\mathfrak{I}}\_{1}:={\mathfrak{I}}\_{0}\cap{\mathfrak{m}}\_{1}([\![1,q\_{1}]\!])\;\mbox{ in which }{\mathfrak{I}}\_{0}:=[\![1,n\_{s}]\!]. |  |

We then iterate the above procedure on the scenarios in â„‘1subscriptâ„‘1{\mathfrak{I}}\_{1} and so on. Namely, we fix Lâ‰¥1ğ¿1L\geq 1 different thresholds (qâ„“)â„“=0,â€¦,Lâˆ’1subscriptsubscriptğ‘â„“â„“

0â€¦ğ¿1(q\_{\ell})\_{\ell=0,\ldots,L{-1}} such that

|  |  |  |  |
| --- | --- | --- | --- |
|  | nw=:qLâˆ’1â‰¤â‹¯â‰¤q0:=ns.\displaystyle{n\_{w}=:q\_{L-1}}{\leq}\cdots{\leq}q\_{0}:=n\_{s}. |  | (5) |

Assuming that â„‘â„“âˆ’1subscriptâ„‘â„“1{\mathfrak{I}}\_{\ell-1} is given, for some 1â‰¤â„“âˆ’1â‰¤Lâˆ’11â„“1ğ¿11\leq\ell-1\leq L-{1}, we compute the estimators666Note from the considerations below that only the elements (Î¼^â„“i)iâˆˆâ„‘â„“âˆ’1subscriptsubscriptsuperscript^ğœ‡ğ‘–â„“ğ‘–subscriptâ„‘â„“1(\hat{\mu}^{i}\_{\ell})\_{i\in{\mathfrak{I}}\_{\ell-1}} are needed in practice, the others are only defined here because they will be used in our proofs.

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î¼^â„“i:=1Nâ„“â€‹âˆ‘j=1Nâ„“Pjiâ€‹Â forÂ â€‹iâ‰¤ns,assignsubscriptsuperscript^ğœ‡ğ‘–â„“1subscriptğ‘â„“superscriptsubscriptğ‘—1subscriptğ‘â„“superscriptsubscriptğ‘ƒğ‘—ğ‘–Â forÂ ğ‘–subscriptğ‘›ğ‘ \hat{\mu}^{i}\_{\ell}:=\frac{1}{N\_{\ell}}\sum\_{j=1}^{N\_{\ell}}P\_{j}^{i}\;\mbox{ for }i\leq n\_{s}, |  | (6) |

for some Nâ„“â‰¥Nâ„“âˆ’1subscriptğ‘â„“subscriptğ‘â„“1N\_{\ell}\geq N\_{\ell-1}.
If â„“â‰¤Lâˆ’1â„“ğ¿1\ell\leq L-1, we consider the (random) permutation ğ”ªâ„“:[[1,qâ„“âˆ’1]]â†¦â„‘â„“âˆ’1:subscriptğ”ªâ„“maps-todelimited-[]1subscriptğ‘â„“1subscriptâ„‘â„“1{\mathfrak{m}}\_{\ell}:[\![1,q\_{\ell-1}]\!]\mapsto{\mathfrak{I}}\_{\ell-1} such that the components of (Î¼^â„“i)iâˆˆâ„‘â„“âˆ’1subscriptsuperscriptsubscript^ğœ‡â„“ğ‘–ğ‘–subscriptâ„‘â„“1\left(\hat{\mu}\_{\ell}^{i}\right)\_{i\in{\mathfrak{I}}\_{\ell-1}} are in decreasing order

|  |  |  |  |
| --- | --- | --- | --- |
|  | {Î¼^â„“ğ”ªâ„“â€‹(1)â‰¥Î¼^â„“ğ”ªâ„“â€‹(2)â‰¥â€¦â‰¥Î¼^â„“ğ”ªâ„“â€‹(qâ„“âˆ’1),ğ”ªâ„“â€‹(i)<ğ”ªâ„“â€‹(iâ€²)â€‹Â ifÂ â€‹Î¼^â„“ğ”ªâ„“â€‹(i)=Î¼^â„“ğ”ªâ„“â€‹(iâ€²)â€‹Â forÂ â€‹1â‰¤i<iâ€²â‰¤ns,casessuperscriptsubscript^ğœ‡â„“subscriptğ”ªâ„“1superscriptsubscript^ğœ‡â„“subscriptğ”ªâ„“2â€¦superscriptsubscript^ğœ‡â„“subscriptğ”ªâ„“subscriptğ‘â„“1subscriptğ”ªâ„“ğ‘–subscriptğ”ªâ„“superscriptğ‘–â€²Â ifÂ superscriptsubscript^ğœ‡â„“subscriptğ”ªâ„“ğ‘–superscriptsubscript^ğœ‡â„“subscriptğ”ªâ„“superscriptğ‘–â€²Â forÂ 1ğ‘–superscriptğ‘–â€²subscriptğ‘›ğ‘ \left\{\begin{array}[]{l}\hat{\mu}\_{\ell}^{{\mathfrak{m}}\_{\ell}(1)}\geq\hat{\mu}\_{\ell}^{{\mathfrak{m}}\_{\ell}(2)}\geq\ldots\geq\hat{\mu}\_{\ell}^{{\mathfrak{m}}\_{\ell}(q\_{\ell-1})},\\ {\mathfrak{m}}\_{\ell}(i)<{\mathfrak{m}}\_{\ell}(i^{\prime})\;\mbox{ if }\hat{\mu}\_{\ell}^{{\mathfrak{m}}\_{\ell}(i)}=\hat{\mu}\_{\ell}^{{\mathfrak{m}}\_{\ell}(i^{\prime})}\mbox{ for }1\leq i<i^{\prime}\leq n\_{s},\end{array}\right. |  | (7) |

and only keep the elements in

|  |  |  |
| --- | --- | --- |
|  | â„‘â„“:=â„‘â„“âˆ’1âˆ©ğ”ªâ„“â€‹([[1,qâ„“]])assignsubscriptâ„‘â„“subscriptâ„‘â„“1subscriptğ”ªâ„“delimited-[]1subscriptğ‘â„“{\mathfrak{I}}\_{\ell}:={\mathfrak{I}}\_{\ell-1}\cap{\mathfrak{m}}\_{\ell}([\![1,q\_{\ell}]\!]) |  |

for the next step. If â„“=Lâ„“ğ¿\ell=L, we just compute the final estimator of the ESES{\rm ES} given by

|  |  |  |
| --- | --- | --- |
|  | ES^:=1nwâ€‹âˆ‘i=1nwÎ¼^Lğ”ªLâˆ’1â€‹(i)=1nwâ€‹âˆ‘iâˆˆâ„‘Lâˆ’1Î¼^Li.assign^ES1subscriptğ‘›ğ‘¤superscriptsubscriptğ‘–1subscriptğ‘›ğ‘¤superscriptsubscript^ğœ‡ğ¿subscriptğ”ªğ¿1ğ‘–1subscriptğ‘›ğ‘¤subscriptğ‘–subscriptâ„‘ğ¿1superscriptsubscript^ğœ‡ğ¿ğ‘–\widehat{\rm ES}:=\frac{1}{n\_{w}}\sum\_{i=1}^{n\_{w}}\hat{\mu}\_{L}^{{\mathfrak{m}}\_{{L-1}}(i)}=\frac{1}{n\_{w}}\sum\_{i\in{\mathfrak{I}}\_{{L-1}}}\hat{\mu}\_{L}^{i}. |  |

Note that only the Lâˆ’1ğ¿1L-1-first steps are used to select the worth scenarios, the step Lğ¿L is a pure Monte Carlo step. Again, the general idea is to reduce little by little the number of candidate scenarios to be part of the worst ones. As the number of candidates diminishes, one increases the number of simulated paths so as to reduce the variance of our Monte Carlo estimators and be able to differentiate between potentially closer true values of the associated conditional expectations.

###### Remark 2.1.

Note that, given jğ‘—j, we do not assume that the Pjisubscriptsuperscriptğ‘ƒğ‘–ğ‘—P^{i}\_{j}, iâ‰¤nsğ‘–subscriptğ‘›ğ‘ i\leq n\_{s}, are independent. The simulations associated to different scenarios are in general not independent. Moreover, the Î¼^â„“isubscriptsuperscript^ğœ‡ğ‘–â„“\hat{\mu}^{i}\_{\ell}, â„“â‰¤Lâ„“ğ¿\ell\leq L, use the same simulated paths, only the number of used simulations changes. Both permit to reduce the computational cost, by allowing the use of the same simulations of the underlying processes across scenarios and steps.

### 2.2 General a-priori bound on the ğ•ƒpsuperscriptğ•ƒğ‘{\mathbb{L}}^{p} error

In this section, we first provide a general ğ•ƒpsuperscriptğ•ƒğ‘{\mathbb{L}}^{p} estimate of the error. A more tractable formulation will be provided in Corollary [2.3](#S2.Thmtheorem3 "Corollary 2.3. â€£ 2.3 Error bound for Sub-Gamma distributions â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios") under an additional sub-gamma distribution assumption.

From now on, we assume that P|sâˆˆğ•ƒpP\_{|{\rm s}}\in{\mathbb{L}}^{p} for all pâ‰¥1ğ‘1p\geq 1, and we use the notations777The element qLsubscriptğ‘ğ¿q\_{L} and N0subscriptğ‘0N\_{0} are defined for notational convenience, they never appear in our algorithm. To fix ideas, they can be set to qL=nwsubscriptğ‘ğ¿subscriptğ‘›ğ‘¤q\_{L}=n\_{w} and N0=0subscriptğ‘00N\_{0}=0 all over this paper.

|  |  |  |
| --- | --- | --- |
|  | q:=(q0,q1,â€¦,qL)â€‹Â ,Â â€‹N=(N0,N1,â€¦,NL)assignğ‘subscriptğ‘0subscriptğ‘1â€¦subscriptğ‘ğ¿Â ,Â ğ‘subscriptğ‘0subscriptğ‘1â€¦subscriptğ‘ğ¿\displaystyle q:=({q\_{0}},q\_{1},\ldots,q\_{{L}})\;\mbox{ , }\;N=(N\_{0},N\_{1},\ldots,N\_{L}) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | Î´â€‹qâ„“:=qâ„“âˆ’1âˆ’qâ„“â€‹Â andÂ â€‹Î´â€‹Nâ„“:=Nâ„“âˆ’Nâ„“âˆ’1â€‹, forÂ â€‹1â‰¤â„“â‰¤L,assignğ›¿subscriptğ‘â„“subscriptğ‘â„“1subscriptğ‘â„“Â andÂ ğ›¿subscriptğ‘â„“assignsubscriptğ‘â„“subscriptğ‘â„“1, forÂ 1â„“ğ¿\displaystyle\delta q\_{\ell}:=q\_{\ell-1}-q\_{\ell}\;\mbox{ and }\;\delta N\_{\ell}:=N\_{\ell}-N\_{\ell-1}\mbox{, for }1\leq\ell\leq L, |  | (8) |
|  |  |  |  |
| --- | --- | --- | --- |
|  | Î´â€‹Î¼^â„“i:=âˆ‘j=Nâ„“âˆ’1+1Nâ„“PjiÎ´â€‹Nâ„“=Nâ„“â€‹Î¼^â„“iâˆ’Nâ„“âˆ’1â€‹Î¼^â„“âˆ’1iÎ´â€‹Nâ„“,Â forÂ 1â‰¤iâ‰¤ns,formulae-sequenceassignğ›¿subscriptsuperscript^ğœ‡ğ‘–â„“superscriptsubscriptğ‘—subscriptğ‘â„“11subscriptğ‘â„“subscriptsuperscriptğ‘ƒğ‘–ğ‘—ğ›¿subscriptğ‘â„“subscriptğ‘â„“subscriptsuperscript^ğœ‡ğ‘–â„“subscriptğ‘â„“1subscriptsuperscript^ğœ‡ğ‘–â„“1ğ›¿subscriptğ‘â„“Â forÂ 1â‰¤iâ‰¤ns,\displaystyle\delta\hat{\mu}^{i}\_{\ell}:=\frac{\sum\_{j=N\_{\ell-1}+1}^{N\_{\ell}}P^{i}\_{j}}{\delta N\_{\ell}}=\frac{N\_{\ell}\hat{\mu}^{i}\_{\ell}-N\_{\ell-1}\hat{\mu}^{i}\_{\ell-1}}{\delta N\_{\ell}},\mbox{ for $1\leq i\leq n\_{s}$,} |  | (9) |

with the convention 0/0=00000/0=0.

###### Proposition 2.2.

For all pâ‰¥1ğ‘1p\geq 1,

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â€‹[|ESâˆ’ES^|p]1pâ‰¤ğ”¼superscriptdelimited-[]superscriptES^ESğ‘1ğ‘absent\displaystyle\mathbb{E}\left[\left|{\rm ES}-\widehat{\rm ES}\right|^{p}\right]^{\frac{1}{p}}\leq | âˆ‘â„“=1Lâˆ’1(Î´â€‹qâ„“)1pâ€‹max(i,k)âˆˆ[[1,nw]]Ã—[[qâ„“+1,ns]]â¡(Î¼iâˆ’Î¼k)â€‹â„™â€‹[Î¼^â„“k>Î¼^â„“i]1psuperscriptsubscriptâ„“1ğ¿1superscriptğ›¿subscriptğ‘â„“1ğ‘subscriptğ‘–ğ‘˜delimited-[]1subscriptğ‘›ğ‘¤delimited-[]subscriptğ‘â„“1subscriptğ‘›ğ‘ superscriptğœ‡ğ‘–superscriptğœ‡ğ‘˜â„™superscriptdelimited-[]superscriptsubscript^ğœ‡â„“ğ‘˜superscriptsubscript^ğœ‡â„“ğ‘–1ğ‘\displaystyle\sum\_{\ell=1}^{L-1}(\delta q\_{\ell})^{\frac{1}{p}}\max\_{(i,k)\in[\![1,{n\_{w}}]\!]\times[\![q\_{\ell}+1,n\_{s}]\!]}(\mu^{i}-\mu^{k}){\mathbb{P}}[\hat{\mu}\_{\ell}^{k}>\hat{\mu}\_{\ell}^{i}]^{\frac{1}{p}} |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | +1nwâ€‹Î´â€‹NLNLâ€‹max1â‰¤i1<â‹¯<inwâ‰¤nsâ€‹(âˆ‘j=1nwğ”¼â€‹[|Î´â€‹Î¼^Lijâˆ’Î¼ij|p]1p)1subscriptğ‘›ğ‘¤ğ›¿subscriptğ‘ğ¿subscriptğ‘ğ¿1subscriptğ‘–1â‹¯subscriptğ‘–subscriptğ‘›ğ‘¤subscriptğ‘›ğ‘ superscriptsubscriptğ‘—1subscriptğ‘›ğ‘¤ğ”¼superscriptdelimited-[]superscriptğ›¿subscriptsuperscript^ğœ‡subscriptğ‘–ğ‘—ğ¿superscriptğœ‡subscriptğ‘–ğ‘—ğ‘1ğ‘\displaystyle+\frac{1}{n\_{w}}\frac{\delta N\_{L}}{N\_{L}}\underset{1\leq i\_{1}<\cdots<i\_{{n\_{w}}}\leq n\_{s}}{\max}\left(\sum\_{j=1}^{{n\_{w}}}{\mathbb{E}}\left[\left|\delta\hat{\mu}^{i\_{j}}\_{L}-\mu^{i\_{j}}\right|^{p}\right]^{\frac{1}{p}}\right) |  | (10) |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +1nwâ€‹NLâˆ’1NLâ€‹âˆ‘i=1nsğ”¼â€‹[|Î¼^Lâˆ’1iâˆ’Î¼i|p]1p.1subscriptğ‘›ğ‘¤subscriptğ‘ğ¿1subscriptğ‘ğ¿superscriptsubscriptğ‘–1subscriptğ‘›ğ‘ ğ”¼superscriptdelimited-[]superscriptsuperscriptsubscript^ğœ‡ğ¿1ğ‘–superscriptğœ‡ğ‘–ğ‘1ğ‘\displaystyle+\frac{1}{n\_{w}}\frac{N\_{L-1}}{N\_{L}}\sum\_{i=1}^{{n\_{s}}}{\mathbb{E}}\left[\left|\hat{\mu}\_{L-1}^{{i}}-\mu^{{i}}\right|^{p}\right]^{\frac{1}{p}}. |  |

Before providing the proof of this general estimate, let us make some comments. The last two terms in ([10](#S2.E10 "In Proposition 2.2. â€£ 2.2 General a-priori bound on the ğ•ƒ^ğ‘ error â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios")) are natural as they are due to the Monte Carlo error made on the estimation of the various conditional expectations that can enter, after the (Lâˆ’1)ğ¿1(L-1)-levels selection procedure, in the estimation of ESES{\rm ES}. Note that it corresponds to the estimation errors using the cumulated number of Monte Carlo simulations NLâˆ’1subscriptğ‘ğ¿1N\_{L-1} of step Lâˆ’1ğ¿1L-1 and the number NLâˆ’NLâˆ’1subscriptğ‘ğ¿subscriptğ‘ğ¿1N\_{L}-N\_{L-1} of simulations used only for the last step. In practice, these numbers should be sufficiently large. The first term involves the quantities max(i,k)âˆˆ[[1,nw]]Ã—[[qâ„“+1,ns]]â¡(Î¼iâˆ’Î¼k)â€‹â„™â€‹[Î¼^â„“k>Î¼^â„“i]1psubscriptğ‘–ğ‘˜delimited-[]1subscriptğ‘›ğ‘¤delimited-[]subscriptğ‘â„“1subscriptğ‘›ğ‘ superscriptğœ‡ğ‘–superscriptğœ‡ğ‘˜â„™superscriptdelimited-[]superscriptsubscript^ğœ‡â„“ğ‘˜superscriptsubscript^ğœ‡â„“ğ‘–1ğ‘\max\_{(i,k)\in[\![1,{n\_{w}}]\!]\times[\![q\_{\ell}+1,n\_{s}]\!]}(\mu^{i}-\mu^{k}){\mathbb{P}}[\hat{\mu}\_{\ell}^{k}>\hat{\mu}\_{\ell}^{i}]^{\frac{1}{p}} with â„“=1,â€¦,Lâˆ’1â„“

1â€¦ğ¿1\ell=1,\ldots,L-1. Each term corresponds to the situation in which an element iâˆˆ[[1,nw]]ğ‘–delimited-[]1subscriptğ‘›ğ‘¤i\in[\![1,n\_{w}]\!] gets out the set of selected indexes â„‘â„“subscriptâ„‘â„“{\mathfrak{I}}\_{\ell} exactly at the â„“â„“\ell-th step. In the worst situation, it is replaced by an element of index kğ‘˜k larger than qâ„“subscriptğ‘â„“q\_{\ell} and this can happen only if Î¼^â„“k>Î¼^â„“isuperscriptsubscript^ğœ‡â„“ğ‘˜superscriptsubscript^ğœ‡â„“ğ‘–\hat{\mu}\_{\ell}^{k}>\hat{\mu}\_{\ell}^{i}. The probability of this event is controlled by the number of Monte Carlo simulations Nâ„“subscriptğ‘â„“N\_{\ell} used at the step â„“â„“\ell but also by the distance between the two scenarios. More specifically, for â„“â„“\ell small, one expects that â„™â€‹[Î¼^â„“k>Î¼^â„“i]â„™delimited-[]superscriptsubscript^ğœ‡â„“ğ‘˜superscriptsubscript^ğœ‡â„“ğ‘–{\mathbb{P}}[\hat{\mu}\_{\ell}^{k}>\hat{\mu}\_{\ell}^{i}] is small because the law of P|skP\_{|{{\rm s}\_{k}}} is concentrated far away from where the law of P|siP\_{|{{\rm s}\_{i}}} is. This quantity potentially increases with â„“â„“\ell, as we reduce the number of selected indexes. This should be compensated by an increase in the number of used Monte Carlo simulations. Otherwise stated, we expect to balance the various terms of
([10](#S2.E10 "In Proposition 2.2. â€£ 2.2 General a-priori bound on the ğ•ƒ^ğ‘ error â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios")) by considering a suitable increasing sequence (Nâ„“)â„“â‰¤Lsubscriptsubscriptğ‘â„“â„“ğ¿(N\_{\ell})\_{\ell\leq L}.

Obviously, ([10](#S2.E10 "In Proposition 2.2. â€£ 2.2 General a-priori bound on the ğ•ƒ^ğ‘ error â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios")) implies that the algorithm converges as Nâ„“â†’âˆâ†’subscriptğ‘â„“N\_{\ell}\to\infty for all â„“â‰¤Lâ„“ğ¿\ell\leq L, see Proposition [3.3](#S3.Thmtheorem3 "Proposition 3.3. â€£ 3.1 Error bounds and convergence for predictable strategies â€£ 3 Adaptative algorithm â€£ Computation of Expected Shortfall by fast detection of worst scenarios") below for a proof in a more general framework.

###### Proof of Proposition [2.2](#S2.Thmtheorem2 "Proposition 2.2. â€£ 2.2 General a-priori bound on the ğ•ƒ^ğ‘ error â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios").

We split the error into a permutation and a Monte Carlo error:

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | ğ”¼â€‹[|ESâˆ’ES^|p]1pâ‰¤ğ”¼superscriptdelimited-[]superscriptES^ESğ‘1ğ‘absent\displaystyle\mathbb{E}\left[\left|{\rm ES}-\widehat{\rm ES}\right|^{p}\right]^{\frac{1}{p}}\leq | ğ”¼â€‹[|1nwâ€‹âˆ‘iâ‰¤nwÎ¼iâˆ’Î¼ğ”ªLâˆ’1â€‹(i)|p]1p+ğ”¼â€‹[|1nwâ€‹âˆ‘iâ‰¤nwÎ¼^Lğ”ªLâˆ’1â€‹(i)âˆ’Î¼ğ”ªLâˆ’1â€‹(i)|p]1p.ğ”¼superscriptdelimited-[]superscript1subscriptğ‘›ğ‘¤subscriptğ‘–subscriptğ‘›ğ‘¤superscriptğœ‡ğ‘–superscriptğœ‡subscriptğ”ªğ¿1ğ‘–ğ‘1ğ‘ğ”¼superscriptdelimited-[]superscript1subscriptğ‘›ğ‘¤subscriptğ‘–subscriptğ‘›ğ‘¤superscriptsubscript^ğœ‡ğ¿subscriptğ”ªğ¿1ğ‘–superscriptğœ‡subscriptğ”ªğ¿1ğ‘–ğ‘1ğ‘\displaystyle{\mathbb{E}}\left[\left|\frac{1}{n\_{w}}\sum\_{i\leq n\_{w}}\mu^{i}-\mu^{{\mathfrak{m}}\_{L-1}(i)}\right|^{p}\right]^{\frac{1}{p}}+{\mathbb{E}}\left[\left|\frac{1}{n\_{w}}\sum\_{i\leq n\_{w}}\hat{\mu}\_{L}^{{\mathfrak{m}}\_{L-1}(i)}-\mu^{{\mathfrak{m}}\_{L-1}(i)}\right|^{p}\right]^{\frac{1}{p}}. |  | (11) |

Let us first look at the second term which corresponds to a Monte Carlo error. We have

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â€‹[|1nwâ€‹âˆ‘iâ‰¤nwÎ¼^Lğ”ªLâˆ’1â€‹(i)âˆ’Î¼ğ”ªLâˆ’1â€‹(i)|p]1pâ‰¤ğ”¼superscriptdelimited-[]superscript1subscriptğ‘›ğ‘¤subscriptğ‘–subscriptğ‘›ğ‘¤superscriptsubscript^ğœ‡ğ¿subscriptğ”ªğ¿1ğ‘–superscriptğœ‡subscriptğ”ªğ¿1ğ‘–ğ‘1ğ‘absent\displaystyle{\mathbb{E}}\left[\left|\frac{1}{n\_{w}}\sum\_{i\leq n\_{w}}\hat{\mu}\_{L}^{{\mathfrak{m}}\_{L-1}(i)}-\mu^{{\mathfrak{m}}\_{L-1}(i)}\right|^{p}\right]^{\frac{1}{p}}\leq | NLâˆ’1NLâ€‹1nwâ€‹ğ”¼â€‹[|âˆ‘iâ‰¤nwÎ¼^Lâˆ’1ğ”ªLâˆ’1â€‹(i)âˆ’Î¼ğ”ªLâˆ’1â€‹(i)|p]1psubscriptğ‘ğ¿1subscriptğ‘ğ¿1subscriptğ‘›ğ‘¤ğ”¼superscriptdelimited-[]superscriptsubscriptğ‘–subscriptğ‘›ğ‘¤superscriptsubscript^ğœ‡ğ¿1subscriptğ”ªğ¿1ğ‘–superscriptğœ‡subscriptğ”ªğ¿1ğ‘–ğ‘1ğ‘\displaystyle\frac{N\_{L-1}}{N\_{L}}\frac{1}{n\_{w}}{\mathbb{E}}\left[\left|\sum\_{i\leq n\_{w}}\hat{\mu}\_{L-1}^{{\mathfrak{m}}\_{L-1}(i)}-\mu^{{\mathfrak{m}}\_{L-1}(i)}\right|^{p}\right]^{\frac{1}{p}} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +NLâˆ’NLâˆ’1NLâ€‹1nwâ€‹ğ”¼â€‹[|âˆ‘iâ‰¤nwâˆ‘j=NLâˆ’1+1NLP^jğ”ªLâˆ’1â€‹(i)NLâˆ’NLâˆ’1âˆ’Î¼ğ”ªLâˆ’1â€‹(i)|p]1psubscriptğ‘ğ¿subscriptğ‘ğ¿1subscriptğ‘ğ¿1subscriptğ‘›ğ‘¤ğ”¼superscriptdelimited-[]superscriptsubscriptğ‘–subscriptğ‘›ğ‘¤superscriptsubscriptğ‘—subscriptğ‘ğ¿11subscriptğ‘ğ¿superscriptsubscript^ğ‘ƒğ‘—subscriptğ”ªğ¿1ğ‘–subscriptğ‘ğ¿subscriptğ‘ğ¿1superscriptğœ‡subscriptğ”ªğ¿1ğ‘–ğ‘1ğ‘\displaystyle+\frac{N\_{L}-N\_{L-1}}{N\_{L}}\frac{1}{n\_{w}}{\mathbb{E}}\left[\left|\sum\_{i\leq n\_{w}}\frac{\sum\_{j=N\_{L-1}+1}^{N\_{L}}\hat{P}\_{j}^{{\mathfrak{m}}\_{L-1}(i)}}{N\_{L}-N\_{L-1}}-\mu^{{\mathfrak{m}}\_{L-1}(i)}\right|^{p}\right]^{\frac{1}{p}} |  |

in which

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â€‹[|âˆ‘iâ‰¤nwÎ¼^Lâˆ’1ğ”ªLâˆ’1â€‹(i)âˆ’Î¼ğ”ªLâˆ’1â€‹(i)|p]1pğ”¼superscriptdelimited-[]superscriptsubscriptğ‘–subscriptğ‘›ğ‘¤superscriptsubscript^ğœ‡ğ¿1subscriptğ”ªğ¿1ğ‘–superscriptğœ‡subscriptğ”ªğ¿1ğ‘–ğ‘1ğ‘\displaystyle{\mathbb{E}}\left[\left|\sum\_{i\leq n\_{w}}\hat{\mu}\_{L-1}^{{\mathfrak{m}}\_{L-1}(i)}-\mu^{{\mathfrak{m}}\_{L-1}(i)}\right|^{p}\right]^{\frac{1}{p}} | â‰¤âˆ‘iâ‰¤nsğ”¼â€‹[|Î¼^Lâˆ’1iâˆ’Î¼i|p]1p,absentsubscriptğ‘–subscriptğ‘›ğ‘ ğ”¼superscriptdelimited-[]superscriptsuperscriptsubscript^ğœ‡ğ¿1ğ‘–superscriptğœ‡ğ‘–ğ‘1ğ‘\displaystyle\leq\sum\_{i\leq n\_{s}}{\mathbb{E}}\left[\left|\hat{\mu}\_{L-1}^{i}-\mu^{i}\right|^{p}\right]^{\frac{1}{p}}, |  |

and

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[|âˆ‘iâ‰¤nwâˆ‘j=NLâˆ’1+1NLP^jğ”ªLâˆ’1â€‹(i)NLâˆ’NLâˆ’1âˆ’Î¼ğ”ªLâˆ’1â€‹(i)|p]1p=ğ”¼[ğ”¼[|âˆ‘iâ‰¤nwÎ´Î¼^Lğ”ªLâˆ’1â€‹(i)âˆ’Î¼ğ”ªLâˆ’1â€‹(i)|p|ğ”ªLâˆ’1]]1pâ‰¤(max1â‰¤i1<â€¦<inwâ‰¤nsâ€‹ğ”¼â€‹[|âˆ‘j=1nwÎ´â€‹Î¼^Lijâˆ’Î¼ij|p])1pâ‰¤max1â‰¤i1<â€¦<inwâ‰¤nsâ€‹âˆ‘j=1nwğ”¼â€‹[|Î´â€‹Î¼^Lijâˆ’Î¼ij|p]1p.\begin{split}{\mathbb{E}}\left[\left|\sum\_{i\leq n\_{w}}\frac{\sum\_{j=N\_{L-1}+1}^{N\_{L}}\hat{P}\_{j}^{{\mathfrak{m}}\_{L-1}(i)}}{N\_{L}-N\_{L-1}}-\mu^{{\mathfrak{m}}\_{L-1}(i)}\right|^{p}\right]^{\frac{1}{p}}&={\mathbb{E}}\left[{\mathbb{E}}\left[\left|\sum\_{i\leq n\_{w}}\delta\hat{\mu}\_{L}^{{\mathfrak{m}}\_{L-1}(i)}-\mu^{{\mathfrak{m}}\_{L-1}(i)}\right|^{p}\middle|{\mathfrak{m}}\_{L-1}\right]\right]^{\frac{1}{p}}\\ &\leq\left(\underset{1\leq i\_{1}<...<i\_{n\_{w}}\leq n\_{s}}{\max}{\mathbb{E}}\left[\left|\sum\_{j=1}^{n\_{w}}\delta\hat{\mu}\_{L}^{i\_{j}}-\mu^{i\_{j}}\right|^{p}\right]\right)^{\frac{1}{p}}\\ &\leq\underset{1\leq i\_{1}<...<i\_{n\_{w}}\leq n\_{s}}{\max}\sum\_{j=1}^{n\_{w}}{\mathbb{E}}\left[\left|\delta\hat{\mu}\_{L}^{i\_{j}}-\mu^{i\_{j}}\right|^{p}\right]^{\frac{1}{p}}.\end{split} |  |

To discuss the first term in the right-hand side of ([11](#S2.E11 "In Proof of Proposition 2.2. â€£ 2.2 General a-priori bound on the ğ•ƒ^ğ‘ error â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios")), the permutation error, let us first define Sqâ€‹[A]subscriptSğ‘delimited-[]ğ´{\rm S}\_{q}[A] as the collection of the qğ‘q smallest elements of a set AâŠ‚â„•ğ´â„•A\subset{\mathbb{N}}. If iâˆˆ[[1,nw]]âˆ©â„‘â„“âˆ’1âˆ–â„‘â„“ğ‘–delimited-[]1subscriptğ‘›ğ‘¤subscriptâ„‘â„“1subscriptâ„‘â„“i\in[\![1,n\_{w}]\!]\cap{\mathfrak{I}}\_{\ell-1}\setminus{\mathfrak{I}}\_{\ell}, then iâˆˆSqâ„“â€‹[â„‘â„“âˆ’1]âˆ–â„‘â„“ğ‘–subscriptSsubscriptğ‘â„“delimited-[]subscriptâ„‘â„“1subscriptâ„‘â„“i\in{\rm S}\_{q\_{\ell}}[{\mathfrak{I}}\_{\ell-1}]\setminus{\mathfrak{I}}\_{\ell} and therefore there exists kiâˆˆâ„›â„“:=â„‘â„“âˆ–Sqâ„“â€‹[â„‘â„“âˆ’1]subscriptğ‘˜ğ‘–subscriptâ„›â„“assignsubscriptâ„‘â„“subscriptSsubscriptğ‘â„“delimited-[]subscriptâ„‘â„“1k\_{i}\in{\mathcal{R}\_{\ell}}:={\mathfrak{I}}\_{\ell}\setminus{\rm S}\_{q\_{\ell}}[{\mathfrak{I}}\_{\ell-1}]. Thus, on the set {{i1,â€¦,iJ}=(â„‘â„“âˆ’1âˆ–â„‘â„“)âˆ©[[1,nw]]}subscriptğ‘–1â€¦subscriptğ‘–ğ½subscriptâ„‘â„“1subscriptâ„‘â„“delimited-[]1subscriptğ‘›ğ‘¤\{\{i\_{1},\ldots,i\_{J}\}=({\mathfrak{I}}\_{\ell-1}\setminus{\mathfrak{I}}\_{\ell})\cap[\![1,n\_{w}]\!]\}, one can define
ğ”¨â„“â€‹(i1):=maxâ¡â„›â„“assignsubscriptğ”¨â„“subscriptğ‘–1subscriptâ„›â„“{\mathfrak{k}}\_{\ell}(i\_{1}):=\max{\mathcal{R}\_{\ell}}
and ğ”¨â„“â€‹(ij+1):=maxâ¡{k<ğ”¨â„“â€‹(ij):kâˆˆâ„›â„“}assignsubscriptğ”¨â„“subscriptğ‘–ğ‘—1:ğ‘˜subscriptğ”¨â„“subscriptğ‘–ğ‘—ğ‘˜subscriptâ„›â„“{\mathfrak{k}}\_{\ell}(i\_{j+1}):=\max\{k<{\mathfrak{k}}\_{\ell}(i\_{j})\leavevmode\nobreak\ :\leavevmode\nobreak\ k\in{\mathcal{R}\_{\ell}}\} for j+1â‰¤Jğ‘—1ğ½j+1\leq J. Note that

|  |  |  |  |
| --- | --- | --- | --- |
|  | {iâˆˆâ„‘â„“âˆ’1âˆ–â„‘â„“}âŠ‚{Î¼^â„“ğ”¨â„“â€‹(i)>Î¼^â„“i}â€‹Â andÂ â€‹|â„›â„“|â‰¤qâ„“âˆ’1âˆ’qâ„“,ğ‘–subscriptâ„‘â„“1subscriptâ„‘â„“superscriptsubscript^ğœ‡â„“subscriptğ”¨â„“ğ‘–superscriptsubscript^ğœ‡â„“ğ‘–Â andÂ subscriptâ„›â„“subscriptğ‘â„“1subscriptğ‘â„“\displaystyle\{i\in{\mathfrak{I}}\_{\ell-1}\setminus{\mathfrak{I}}\_{\ell}\}\subset\{\hat{\mu}\_{\ell}^{{\mathfrak{k}}\_{\ell}(i)}>\hat{\mu}\_{\ell}^{i}\}\;\mbox{ and }\;|{\mathcal{R}\_{\ell}}|\leq q\_{\ell-1}-q\_{\ell}, |  | (12) |

since â„›â„“âŠ‚â„‘â„“âˆ’1âˆ–Sqâ„“â€‹[â„‘â„“âˆ’1]subscriptâ„›â„“subscriptâ„‘â„“1subscriptSsubscriptğ‘â„“delimited-[]subscriptâ„‘â„“1{\mathcal{R}\_{\ell}}\subset{\mathfrak{I}}\_{\ell-1}\setminus{\rm S}\_{q\_{\ell}}[{\mathfrak{I}}\_{\ell-1}] and |â„‘â„“âˆ’1|=qâ„“âˆ’1subscriptâ„‘â„“1subscriptğ‘â„“1|{\mathfrak{I}}\_{\ell-1}|=q\_{\ell-1}. Let ğ€q,qâ€²subscriptğ€

ğ‘superscriptğ‘â€²{\mathbf{A}}\_{q,q^{\prime}} denote the collection of subsets Ağ´A of [[q+1,ns]]delimited-[]ğ‘1subscriptğ‘›ğ‘ [\![q+1,n\_{s}]\!] such that |A|=qâ€²ğ´superscriptğ‘â€²|A|=q^{\prime}.
Then, it follows from ([4](#S1.E4 "In 1 Introduction â€£ Computation of Expected Shortfall by fast detection of worst scenarios")), HÃ¶lderâ€™s inequality and ([12](#S2.E12 "In Proof of Proposition 2.2. â€£ 2.2 General a-priori bound on the ğ•ƒ^ğ‘ error â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios")) that

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[|1nwâ€‹âˆ‘iâ‰¤nwÎ¼iâˆ’Î¼ğ”ªLâˆ’1â€‹(i)|p]1pâ‰¤1nwâ€‹âˆ‘iâ‰¤nwâˆ‘â„“=1Lâˆ’1ğ”¼â€‹[|(Î¼iâˆ’Î¼ğ”¨â„“â€‹(i))â€‹ğŸ{iâˆˆâ„‘â„“âˆ’1âˆ–â„‘â„“}|p]1pâ‰¤maxiâ‰¤nwâ€‹âˆ‘â„“=1Lâˆ’1ğ”¼â€‹[|(Î¼iâˆ’Î¼ğ”¨â„“â€‹(i))â€‹ğŸ{iâˆˆâ„‘â„“âˆ’1âˆ–â„‘â„“}|p]1pâ‰¤maxiâ‰¤nwâ€‹âˆ‘â„“=1Lâˆ’1(maxAâŠ‚ğ€qâ„“,Î´â€‹qâ„“â€‹âˆ‘kâˆˆAğ”¼â€‹[|(Î¼iâˆ’Î¼k)|pâ€‹ğŸ{iâˆˆâ„‘â„“âˆ’1âˆ–â„‘â„“,ğ”¨â„“â€‹(i)=k}])1pâ‰¤âˆ‘â„“=1Lâˆ’1(Î´â€‹qâ„“)1pâ€‹max(i,k)âˆˆ[[1,nw]]Ã—[[qâ„“+1,ns]]â¡(Î¼iâˆ’Î¼k)â€‹â„™â€‹[Î¼^â„“k>Î¼^â„“i]1p.ğ”¼superscriptdelimited-[]superscript1subscriptğ‘›ğ‘¤subscriptğ‘–subscriptğ‘›ğ‘¤superscriptğœ‡ğ‘–superscriptğœ‡subscriptğ”ªğ¿1ğ‘–ğ‘1ğ‘1subscriptğ‘›ğ‘¤subscriptğ‘–subscriptğ‘›ğ‘¤superscriptsubscriptâ„“1ğ¿1ğ”¼superscriptdelimited-[]superscriptsuperscriptğœ‡ğ‘–superscriptğœ‡subscriptğ”¨â„“ğ‘–subscript1ğ‘–subscriptâ„‘â„“1subscriptâ„‘â„“ğ‘1ğ‘ğ‘–subscriptğ‘›ğ‘¤superscriptsubscriptâ„“1ğ¿1ğ”¼superscriptdelimited-[]superscriptsuperscriptğœ‡ğ‘–superscriptğœ‡subscriptğ”¨â„“ğ‘–subscript1ğ‘–subscriptâ„‘â„“1subscriptâ„‘â„“ğ‘1ğ‘ğ‘–subscriptğ‘›ğ‘¤superscriptsubscriptâ„“1ğ¿1superscriptsubscriptğ´subscriptğ€  subscriptğ‘â„“ğ›¿subscriptğ‘â„“subscriptğ‘˜ğ´ğ”¼delimited-[]superscriptsuperscriptğœ‡ğ‘–superscriptğœ‡ğ‘˜ğ‘subscript1formulae-sequenceğ‘–subscriptâ„‘â„“1subscriptâ„‘â„“subscriptğ”¨â„“ğ‘–ğ‘˜1ğ‘superscriptsubscriptâ„“1ğ¿1superscriptğ›¿subscriptğ‘â„“1ğ‘subscriptğ‘–ğ‘˜delimited-[]1subscriptğ‘›ğ‘¤delimited-[]subscriptğ‘â„“1subscriptğ‘›ğ‘ superscriptğœ‡ğ‘–superscriptğœ‡ğ‘˜â„™superscriptdelimited-[]superscriptsubscript^ğœ‡â„“ğ‘˜superscriptsubscript^ğœ‡â„“ğ‘–1ğ‘\begin{split}{\mathbb{E}}\left[\left|\frac{1}{n\_{w}}\sum\_{i\leq n\_{w}}\mu^{i}-\mu^{{\mathfrak{m}}\_{L-1}(i)}\right|^{p}\right]^{\frac{1}{p}}&\leq\frac{1}{n\_{w}}\sum\_{i\leq n\_{w}}\sum\_{\ell=1}^{L-1}{\mathbb{E}}\left[\left|(\mu^{i}-\mu^{{\mathfrak{k}}\_{\ell}(i)}){\mathbf{1}}\_{\{i\in{\mathfrak{I}}\_{\ell-1}\setminus{\mathfrak{I}}\_{\ell}\}}\right|^{p}\right]^{\frac{1}{p}}\\ &\leq\underset{i\leq n\_{w}}{\max}\sum\_{\ell=1}^{L-1}{\mathbb{E}}\left[\left|(\mu^{i}-\mu^{{\mathfrak{k}}\_{\ell}(i)}){\mathbf{1}}\_{\{i\in{\mathfrak{I}}\_{\ell-1}\setminus{\mathfrak{I}}\_{\ell}\}}\right|^{p}\right]^{\frac{1}{p}}\\ &\leq\underset{i\leq n\_{w}}{\max}\sum\_{\ell=1}^{L-1}\left(\max\_{A\subset{\mathbf{A}}\_{q\_{\ell},\delta q\_{\ell}}}\sum\_{k\in A}{\mathbb{E}}\left[\left|(\mu^{i}-\mu^{k})\right|^{p}{\mathbf{1}}\_{\{i\in{\mathfrak{I}}\_{\ell-1}\setminus{\mathfrak{I}}\_{\ell},{\mathfrak{k}}\_{\ell}(i)=k\}}\right]\right)^{\frac{1}{p}}\\ &\leq\sum\_{\ell=1}^{L-1}\left(\delta q\_{\ell}\right)^{\frac{1}{p}}\max\_{(i,k)\in[\![1,{n\_{w}}]\!]\times[\![q\_{\ell}+1,{n\_{s}}]\!]}(\mu^{i}-\mu^{k}){\mathbb{P}}[\hat{\mu}\_{\ell}^{k}>\hat{\mu}\_{\ell}^{i}]^{\frac{1}{p}}.\end{split} |  |

âˆ

### 2.3 Error bound for Sub-Gamma distributions

To illustrate how the general error bound of Proposition [2.2](#S2.Thmtheorem2 "Proposition 2.2. â€£ 2.2 General a-priori bound on the ğ•ƒ^ğ‘ error â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios") can be used in practice to decide of the sequence (qâ„“,Nâ„“)â„“subscriptsubscriptğ‘â„“subscriptğ‘â„“â„“(q\_{\ell},N\_{\ell})\_{\ell}, we now consider the case where the components of P|sP\_{|{\rm s}} have sub-gamma distributions, and apply Bernsteinâ€™s inequality in ([10](#S2.E10 "In Proposition 2.2. â€£ 2.2 General a-priori bound on the ğ•ƒ^ğ‘ error â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios")), see e.g.Â [[7](#bib.bib7), Chapter 2]. This requires the following assumption.

###### Assumption 1.

There exists câˆˆâ„+ğ‘subscriptâ„c\in{\mathbb{R}}\_{+} such that the random variables Zâ€‹[i,k]:=(P|siâˆ’Î¼i)âˆ’(P|skâˆ’Î¼k)Z[i,k]:=(P\_{|{\rm s}^{i}}-\mu^{i})-(P\_{|{\rm s}^{k}}-\mu^{k}), i,kâ‰¤ns

ğ‘–ğ‘˜
subscriptğ‘›ğ‘ i,k\leq n\_{s}, satisfy Bernsteinâ€™s condition :

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[|Zâ€‹[i,k]|p]â‰¤p!â€‹cpâˆ’22â€‹ğ”¼â€‹[Zâ€‹[i,k]2],i,kâ‰¤ns,Â for allÂ â€‹pâ‰¥3.formulae-sequenceğ”¼delimited-[]superscriptğ‘ğ‘–ğ‘˜ğ‘  ğ‘superscriptğ‘ğ‘22ğ”¼delimited-[]ğ‘superscriptğ‘–ğ‘˜2ğ‘–formulae-sequenceğ‘˜subscriptğ‘›ğ‘ Â for allÂ ğ‘3\mathbb{E}\left[\left|Z[i,k]\right|^{p}\right]\leq\frac{p!\;c^{p-2}}{2}\mathbb{E}\left[Z[i,k]^{2}\right],\;i,k\leq n\_{s},\;\mbox{ for all }p\geq 3. |  |

From now on, we shall assume that the constant cğ‘c is known. It can usually be estimated in practice.

###### Corollary 2.3.

Assume that Assumption [1](#Thmassumption1 "Assumption 1. â€£ 2.3 Error bound for Sub-Gamma distributions â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios") holds. Then, for all pâ‰¥1ğ‘1p\geq 1,

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | ğ”¼â€‹[|ESâˆ’ES^|p]1pâ‰¤ğ”¼superscriptdelimited-[]superscriptES^ESğ‘1ğ‘absent\displaystyle\mathbb{E}\left[\left|{\rm ES}-\widehat{\rm ES}\right|^{p}\right]^{\frac{1}{p}}\leq | Fpâ€‹(q,N)subscriptFğ‘ğ‘ğ‘\displaystyle{\rm F}\_{p}(q,N) |  | (13) |

in which

|  |  |  |  |
| --- | --- | --- | --- |
|  | Fpâ€‹(q,N):=assignsubscriptFğ‘ğ‘ğ‘absent\displaystyle{\rm F}\_{p}(q,N):= | âˆ‘â„“=1Lâˆ’1(Î´â€‹qâ„“)1pâ€‹max(i,k)âˆˆ[[1,nw]]Ã—[[qâ„“+1,ns]]â¡(Î¼iâˆ’Î¼k)â€‹eâˆ’Nâ„“â€‹(Î¼iâˆ’Î¼k)22â€‹pâ€‹(Ïƒiâ€‹k2+câ€‹(Î¼iâˆ’Î¼k))superscriptsubscriptâ„“1ğ¿1superscriptğ›¿subscriptğ‘â„“1ğ‘subscriptğ‘–ğ‘˜delimited-[]1subscriptğ‘›ğ‘¤delimited-[]subscriptğ‘â„“1subscriptğ‘›ğ‘ superscriptğœ‡ğ‘–superscriptğœ‡ğ‘˜superscriptğ‘’subscriptğ‘â„“superscriptsuperscriptğœ‡ğ‘–superscriptğœ‡ğ‘˜22ğ‘superscriptsubscriptğœğ‘–ğ‘˜2ğ‘superscriptğœ‡ğ‘–superscriptğœ‡ğ‘˜\displaystyle\sum\_{\ell=1}^{L-1}(\delta q\_{\ell})^{\frac{1}{p}}\max\_{(i,k)\in[\![1,{n\_{w}}]\!]\times[\![q\_{\ell}+1,{n\_{s}}]\!]}(\mu^{i}-\mu^{k})e^{-\frac{N\_{\ell}(\mu^{i}-\mu^{k})^{2}}{2p(\sigma\_{ik}^{2}+c(\mu^{i}-\mu^{k}))}} |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | +1nwâ€‹Î´â€‹NLNLâ€‹max1â‰¤i1<â€¦<inwâ‰¤nsâ€‹âˆ‘j=1nw(Cp,Ïƒâ€‹pâ€‹Ïƒijp(Î´â€‹NL)p2+Cp,câ€‹pâ€‹cp(Î´â€‹NL)p)1p1subscriptğ‘›ğ‘¤ğ›¿subscriptğ‘ğ¿subscriptğ‘ğ¿1subscriptğ‘–1â€¦subscriptğ‘–subscriptğ‘›ğ‘¤subscriptğ‘›ğ‘ superscriptsubscriptğ‘—1subscriptğ‘›ğ‘¤superscriptsubscriptğ¶  ğ‘ğœğ‘superscriptsubscriptğœsubscriptğ‘–ğ‘—ğ‘superscriptğ›¿subscriptğ‘ğ¿ğ‘2subscriptğ¶  ğ‘ğ‘ğ‘superscriptğ‘ğ‘superscriptğ›¿subscriptğ‘ğ¿ğ‘1ğ‘\displaystyle+\frac{1}{n\_{w}}\frac{\delta N\_{L}}{N\_{L}}\underset{1\leq i\_{1}<...<i\_{{{n\_{w}}}}\leq{n\_{s}}}{\max}\sum\_{j=1}^{{n\_{w}}}\left(C\_{p,\sigma}\frac{p\sigma\_{i\_{j}}^{p}}{(\delta N\_{L})^{\frac{p}{2}}}+C\_{p,c}\frac{pc^{p}}{(\delta N\_{L})^{p}}\right)^{\frac{1}{p}} |  | (14) |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | +1nwâ€‹NLâˆ’1NLâ€‹âˆ‘i=1ns(Cp,Ïƒâ€‹pâ€‹Ïƒip(NLâˆ’1)p2+Cp,câ€‹pâ€‹cp(NLâˆ’1)p)1p1subscriptğ‘›ğ‘¤subscriptğ‘ğ¿1subscriptğ‘ğ¿superscriptsubscriptğ‘–1subscriptğ‘›ğ‘ superscriptsubscriptğ¶  ğ‘ğœğ‘superscriptsubscriptğœğ‘–ğ‘superscriptsubscriptğ‘ğ¿1ğ‘2subscriptğ¶  ğ‘ğ‘ğ‘superscriptğ‘ğ‘superscriptsubscriptğ‘ğ¿1ğ‘1ğ‘\displaystyle+\frac{1}{n\_{w}}\frac{N\_{L-1}}{N\_{L}}\sum\_{i=1}^{{n\_{s}}}\left(C\_{p,\sigma}\frac{p\sigma\_{i}^{p}}{(N\_{L-1})^{\frac{p}{2}}}+C\_{p,c}\frac{pc^{p}}{(N\_{L-1})^{p}}\right)^{\frac{1}{p}} |  |

with

|  |  |  |  |
| --- | --- | --- | --- |
|  | {Ïƒiâ€‹k2:=Varâ€‹[P|siâˆ’P|sk]â€‹Â andÂ â€‹Ïƒi2:=Varâ€‹[P|si],i,kâ‰¤nsCp,Ïƒ:=2pâˆ’1â€‹Î“â€‹(p2)â€‹Â andÂ â€‹Cp,c:=4pâ€‹Î“â€‹(p)\begin{split}\begin{cases}\sigma\_{ik}^{2}:={\rm Var}[P\_{|{\rm s}^{i}}-P\_{|{\rm s}^{k}}]\;\mbox{ and }\;\sigma\_{i}^{2}:={\rm Var}[P\_{|{\rm s}^{i}}],\;i,k\leq n\_{s}\\ C\_{p,\sigma}:=2^{p-1}\Gamma\left(\frac{p}{2}\right)\;\mbox{ and }\;C\_{p,c}:=4^{p}\Gamma\left(p\right)\end{cases}\end{split} |  | (15) |

where Î“Î“\Gamma is the Gamma function defined by

|  |  |  |
| --- | --- | --- |
|  | Î“â€‹(y)=âˆ«0+âˆxyâˆ’1â€‹eâˆ’xâ€‹ğ‘‘x,y>0.formulae-sequenceÎ“ğ‘¦superscriptsubscript0superscriptğ‘¥ğ‘¦1superscriptğ‘’ğ‘¥differential-dğ‘¥ğ‘¦0\Gamma\left(y\right)=\int\_{0}^{+\infty}x^{y-1}e^{-x}dx,\;y>0. |  |

The upper-bound of Corollary [2.3](#S2.Thmtheorem3 "Corollary 2.3. â€£ 2.3 Error bound for Sub-Gamma distributions â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios") has two advantages on Proposition [2.2](#S2.Thmtheorem2 "Proposition 2.2. â€£ 2.2 General a-priori bound on the ğ•ƒ^ğ‘ error â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios"). First, the dependence on (qâ„“,Nâ„“)â„“â‰¥0subscriptsubscriptğ‘â„“subscriptğ‘â„“â„“0(q\_{\ell},N\_{\ell})\_{\ell\geq 0} is more explicit. It depends on unknown quantities, but we can estimate (at least rough) confidence intervals for them, see e.g.Â Section [2.4](#S2.SS4 "2.4 Optimal a-priori allocation by deterministic dynamic programming based on fixed a-priori bounds â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios") below. Second, as we will see in the next section, it allows one to define a tractable deterministic optimal control problem satisfying a dynamic programming principle, or even simple heuristics (see Section [2.5](#S2.SS5 "2.5 Simplified 2-levels algorithm for a linear indifference zoneâ€™s size â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios")), to select an appropriate sequence (qâ„“,Nâ„“)â„“â‰¥0subscriptsubscriptğ‘â„“subscriptğ‘â„“â„“0(q\_{\ell},N\_{\ell})\_{\ell\geq 0}.

###### Proof of Corollary [2.3](#S2.Thmtheorem3 "Corollary 2.3. â€£ 2.3 Error bound for Sub-Gamma distributions â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios").

The first term in ([14](#S2.E14 "In Corollary 2.3. â€£ 2.3 Error bound for Sub-Gamma distributions â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios")) is an upper-bound for the first term in the right-hand side of ([10](#S2.E10 "In Proposition 2.2. â€£ 2.2 General a-priori bound on the ğ•ƒ^ğ‘ error â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios")), see [[7](#bib.bib7), Theorem 2.1]. As for the two other terms in ([10](#S2.E10 "In Proposition 2.2. â€£ 2.2 General a-priori bound on the ğ•ƒ^ğ‘ error â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios")), we use the usual argument, for iâ‰¤nsğ‘–subscriptğ‘›ğ‘ i\leq n\_{s},

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[|Î´â€‹Î¼^Liâˆ’Î¼i|p]=âˆ«0âˆpâ€‹xpâˆ’1â€‹â„™â€‹[|Î´â€‹Î¼^Liâˆ’Î¼i|â‰¥x]â€‹ğ‘‘xğ”¼delimited-[]superscriptğ›¿superscriptsubscript^ğœ‡ğ¿ğ‘–superscriptğœ‡ğ‘–ğ‘superscriptsubscript0ğ‘superscriptğ‘¥ğ‘1â„™delimited-[]ğ›¿superscriptsubscript^ğœ‡ğ¿ğ‘–superscriptğœ‡ğ‘–ğ‘¥differential-dğ‘¥{\mathbb{E}}\left[\left|\delta\hat{\mu}\_{L}^{i}-\mu^{i}\right|^{p}\right]=\int\_{0}^{\infty}px^{p-1}{\mathbb{P}}[|\delta\hat{\mu}\_{L}^{i}-\mu^{i}|\geq x]dx |  |

and

|  |  |  |
| --- | --- | --- |
|  | ğ”¼â€‹[|Î¼^Lâˆ’1iâˆ’Î¼i|p]=âˆ«0âˆpâ€‹xpâˆ’1â€‹â„™â€‹[|Î¼^Lâˆ’1iâˆ’Î¼i|â‰¥x]â€‹ğ‘‘x,ğ”¼delimited-[]superscriptsuperscriptsubscript^ğœ‡ğ¿1ğ‘–superscriptğœ‡ğ‘–ğ‘superscriptsubscript0ğ‘superscriptğ‘¥ğ‘1â„™delimited-[]superscriptsubscript^ğœ‡ğ¿1ğ‘–superscriptğœ‡ğ‘–ğ‘¥differential-dğ‘¥{\mathbb{E}}\left[\left|\hat{\mu}\_{L-1}^{i}-\mu^{i}\right|^{p}\right]=\int\_{0}^{\infty}px^{p-1}{\mathbb{P}}[|\hat{\mu}\_{L-1}^{i}-\mu^{i}|\geq x]dx, |  |

and then appeal to [[7](#bib.bib7), Theorem 2.1] again to deduce that

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â€‹[|Î´â€‹Î¼^Liâˆ’Î¼i|p]â‰¤ğ”¼delimited-[]superscriptğ›¿superscriptsubscript^ğœ‡ğ¿ğ‘–superscriptğœ‡ğ‘–ğ‘absent\displaystyle{\mathbb{E}}\left[\left|\delta\hat{\mu}\_{L}^{i}-\mu^{i}\right|^{p}\right]\leq | âˆ«0âˆpâ€‹xpâˆ’1â€‹eâˆ’Î´â€‹NLâ€‹x22â€‹(Ïƒi2+câ€‹x)â€‹ğ‘‘xsuperscriptsubscript0ğ‘superscriptğ‘¥ğ‘1superscriptğ‘’ğ›¿subscriptğ‘ğ¿superscriptğ‘¥22superscriptsubscriptğœğ‘–2ğ‘ğ‘¥differential-dğ‘¥\displaystyle\int\_{0}^{\infty}px^{p-1}e^{-\frac{\delta N\_{L}x^{2}}{2(\sigma\_{i}^{2}+cx)}}dx |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤\displaystyle\leq | âˆ«0âˆpâ€‹xpâˆ’1â€‹eâˆ’Î´â€‹NLâ€‹x24â€‹Ïƒi2â€‹ğŸ{xâ‰¤Ïƒi2c}â€‹ğ‘‘x+âˆ«0âˆpâ€‹xpâˆ’1â€‹eâˆ’Î´â€‹NLâ€‹x4â€‹câ€‹ğŸ{x>Ïƒi2c}â€‹ğ‘‘xsuperscriptsubscript0ğ‘superscriptğ‘¥ğ‘1superscriptğ‘’ğ›¿subscriptğ‘ğ¿superscriptğ‘¥24superscriptsubscriptğœğ‘–2subscript1ğ‘¥superscriptsubscriptğœğ‘–2ğ‘differential-dğ‘¥superscriptsubscript0ğ‘superscriptğ‘¥ğ‘1superscriptğ‘’ğ›¿subscriptğ‘ğ¿ğ‘¥4ğ‘subscript1ğ‘¥superscriptsubscriptğœğ‘–2ğ‘differential-dğ‘¥\displaystyle\int\_{0}^{\infty}px^{p-1}e^{-\frac{\delta N\_{L}x^{2}}{4\sigma\_{i}^{2}}}{\mathbf{1}}\_{\{x\leq\frac{\sigma\_{i}^{2}}{c}\}}dx+\int\_{0}^{\infty}px^{p-1}e^{-\frac{\delta N\_{L}x}{4c}}{\mathbf{1}}\_{\{x>\frac{\sigma\_{i}^{2}}{c}\}}dx |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤\displaystyle\leq | pâ€‹(Ïƒi2)p2(Î´â€‹NL)p2â€‹âˆ«0âˆypâˆ’1â€‹eâˆ’y24â€‹ğ‘‘y+pâ€‹cp(Î´â€‹NL)pâ€‹âˆ«0âˆypâˆ’1â€‹eâˆ’y4â€‹ğ‘‘y,ğ‘superscriptsubscriptsuperscriptğœ2ğ‘–ğ‘2superscriptğ›¿subscriptğ‘ğ¿ğ‘2superscriptsubscript0superscriptğ‘¦ğ‘1superscriptğ‘’superscriptğ‘¦24differential-dğ‘¦ğ‘superscriptğ‘ğ‘superscriptğ›¿subscriptğ‘ğ¿ğ‘superscriptsubscript0superscriptğ‘¦ğ‘1superscriptğ‘’ğ‘¦4differential-dğ‘¦\displaystyle\frac{p(\sigma^{2}\_{i})^{\frac{p}{2}}}{(\delta N\_{L})^{\frac{p}{2}}}\int\_{0}^{\infty}y^{p-1}e^{-\frac{y^{2}}{4}}dy+\frac{pc^{p}}{(\delta N\_{L})^{p}}\int\_{0}^{\infty}y^{p-1}e^{-\frac{y}{4}}dy, |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤\displaystyle\leq | pâ€‹Ïƒip(Î´â€‹NL)p2â€‹2pâˆ’1â€‹Î“â€‹(p2)+pâ€‹cp(Î´â€‹NL)pâ€‹4pâ€‹Î“â€‹(p),ğ‘superscriptsubscriptğœğ‘–ğ‘superscriptğ›¿subscriptğ‘ğ¿ğ‘2superscript2ğ‘1Î“ğ‘2ğ‘superscriptğ‘ğ‘superscriptğ›¿subscriptğ‘ğ¿ğ‘superscript4ğ‘Î“ğ‘\displaystyle\frac{p\sigma\_{i}^{p}}{(\delta N\_{L})^{\frac{p}{2}}}2^{p-1}\Gamma\left(\frac{p}{2}\right)+\frac{pc^{p}}{(\delta N\_{L})^{p}}4^{p}\Gamma(p), |  |

and

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼â€‹[|Î¼^Lâˆ’1iâˆ’Î¼i|p]â‰¤ğ”¼delimited-[]superscriptsuperscriptsubscript^ğœ‡ğ¿1ğ‘–superscriptğœ‡ğ‘–ğ‘absent\displaystyle{\mathbb{E}}\left[\left|\hat{\mu}\_{L-1}^{i}-\mu^{i}\right|^{p}\right]\leq | âˆ«0âˆpâ€‹xpâˆ’1â€‹eâˆ’NLâˆ’1â€‹x22â€‹(Ïƒi2+câ€‹x)â€‹ğ‘‘xsuperscriptsubscript0ğ‘superscriptğ‘¥ğ‘1superscriptğ‘’subscriptğ‘ğ¿1superscriptğ‘¥22superscriptsubscriptğœğ‘–2ğ‘ğ‘¥differential-dğ‘¥\displaystyle\int\_{0}^{\infty}px^{p-1}e^{-\frac{N\_{L-1}x^{2}}{2(\sigma\_{i}^{2}+cx)}}dx |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤\displaystyle\leq | âˆ«0âˆpâ€‹xpâˆ’1â€‹eâˆ’NLâˆ’1â€‹x24â€‹Ïƒi2â€‹ğŸ{xâ‰¤Ïƒi2c}â€‹ğ‘‘x+âˆ«0âˆpâ€‹xpâˆ’1â€‹eâˆ’NLâˆ’1â€‹x4â€‹câ€‹ğŸ{x>Ïƒi2c}â€‹ğ‘‘xsuperscriptsubscript0ğ‘superscriptğ‘¥ğ‘1superscriptğ‘’subscriptğ‘ğ¿1superscriptğ‘¥24superscriptsubscriptğœğ‘–2subscript1ğ‘¥superscriptsubscriptğœğ‘–2ğ‘differential-dğ‘¥superscriptsubscript0ğ‘superscriptğ‘¥ğ‘1superscriptğ‘’subscriptğ‘ğ¿1ğ‘¥4ğ‘subscript1ğ‘¥superscriptsubscriptğœğ‘–2ğ‘differential-dğ‘¥\displaystyle\int\_{0}^{\infty}px^{p-1}e^{-\frac{N\_{L-1}x^{2}}{4\sigma\_{i}^{2}}}{\mathbf{1}}\_{\{x\leq\frac{\sigma\_{i}^{2}}{c}\}}dx+\int\_{0}^{\infty}px^{p-1}e^{-\frac{N\_{L-1}x}{4c}}{\mathbf{1}}\_{\{x>\frac{\sigma\_{i}^{2}}{c}\}}dx |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤\displaystyle\leq | pâ€‹(Ïƒi2)p2(NLâˆ’1)p2â€‹âˆ«0âˆypâˆ’1â€‹eâˆ’y24â€‹ğ‘‘y+pâ€‹cp(NLâˆ’1)pâ€‹âˆ«0âˆypâˆ’1â€‹eâˆ’y4â€‹ğ‘‘y,ğ‘superscriptsubscriptsuperscriptğœ2ğ‘–ğ‘2superscriptsubscriptğ‘ğ¿1ğ‘2superscriptsubscript0superscriptğ‘¦ğ‘1superscriptğ‘’superscriptğ‘¦24differential-dğ‘¦ğ‘superscriptğ‘ğ‘superscriptsubscriptğ‘ğ¿1ğ‘superscriptsubscript0superscriptğ‘¦ğ‘1superscriptğ‘’ğ‘¦4differential-dğ‘¦\displaystyle\frac{p(\sigma^{2}\_{i})^{\frac{p}{2}}}{(N\_{L-1})^{\frac{p}{2}}}\int\_{0}^{\infty}y^{p-1}e^{-\frac{y^{2}}{4}}dy+\frac{pc^{p}}{(N\_{L-1})^{p}}\int\_{0}^{\infty}y^{p-1}e^{-\frac{y}{4}}dy, |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | â‰¤\displaystyle\leq | pâ€‹Ïƒip(NLâˆ’1)p2â€‹2pâˆ’1â€‹Î“â€‹(p2)+pâ€‹cp(NLâˆ’1)pâ€‹4pâ€‹Î“â€‹(p).ğ‘superscriptsubscriptğœğ‘–ğ‘superscriptsubscriptğ‘ğ¿1ğ‘2superscript2ğ‘1Î“ğ‘2ğ‘superscriptğ‘ğ‘superscriptsubscriptğ‘ğ¿1ğ‘superscript4ğ‘Î“ğ‘\displaystyle\frac{p\sigma\_{i}^{p}}{(N\_{L-1})^{\frac{p}{2}}}2^{p-1}\Gamma\left(\frac{p}{2}\right)+\frac{pc^{p}}{(N\_{L-1})^{p}}4^{p}\Gamma\left(p\right). |  |

âˆ

###### Remark 2.4.

If the (Î¼^â„“i)iâ‰¤nssubscriptsubscriptsuperscript^ğœ‡ğ‘–â„“ğ‘–subscriptğ‘›ğ‘ (\hat{\mu}^{i}\_{\ell})\_{i\leq n\_{s}} and (Î´â€‹Î¼^â„“i)iâ‰¤nssubscriptğ›¿subscriptsuperscript^ğœ‡ğ‘–â„“ğ‘–subscriptğ‘›ğ‘ (\delta\hat{\mu}^{i}\_{\ell})\_{i\leq n\_{s}} are Gaussian, which is the case asymptotically, then the bound of Corollary [2.3](#S2.Thmtheorem3 "Corollary 2.3. â€£ 2.3 Error bound for Sub-Gamma distributions â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios") remains valid with c=0ğ‘0c=0. This fact will be used later on for simplifying our numerical algorithms.

### 2.4 Optimal a-priori allocation by deterministic dynamic programming based on fixed a-priori bounds

Given N:=(Nâ„“)0â‰¤â„“â‰¤Lassignğ‘subscriptsubscriptğ‘â„“0â„“ğ¿N:=(N\_{\ell})\_{0\leq\ell\leq L} and q=(qâ„“)0â‰¤â„“â‰¤Lâˆ’1ğ‘subscriptsubscriptğ‘â„“0â„“ğ¿1q=(q\_{\ell})\_{0\leq\ell\leq L-1}, the total computation cost is

|  |  |  |
| --- | --- | --- |
|  | Câ€‹(q,N):=âˆ‘â„“=0Lâˆ’1qâ„“â€‹(Nâ„“+1âˆ’Nâ„“)assignCğ‘ğ‘superscriptsubscriptâ„“0ğ¿1subscriptğ‘â„“subscriptğ‘â„“1subscriptğ‘â„“{\rm C}(q,N):=\sum\_{\ell=0}^{L-1}q\_{\ell}(N\_{\ell+1}-N\_{\ell}) |  |

with the convention N0:=0assignsubscriptğ‘00N\_{0}:=0. Let ğ’©ğ’©{\mathcal{N}} denote the collection of non-decreasing sequences N:=(Nâ„“)0â‰¤â„“â‰¤Lassignğ‘subscriptsubscriptğ‘â„“0â„“ğ¿N:=(N\_{\ell})\_{0\leq\ell\leq L} with values in â„•â„•{\mathbb{N}} such that N0=0subscriptğ‘00N\_{0}=0, and let ğ’¬ğ’¬{\mathcal{Q}} denote the collections of non-increasing sequences888We write (qâ„“)0â‰¤â„“â‰¤Lsubscriptsubscriptğ‘â„“0â„“ğ¿(q\_{\ell})\_{0\leq\ell\leq L} for convenience also qLsubscriptğ‘ğ¿q\_{L} will never play any role. q=(qâ„“)0â‰¤â„“â‰¤Lğ‘subscriptsubscriptğ‘â„“0â„“ğ¿q=(q\_{\ell})\_{0\leq\ell\leq L} with values in [[nw,ns]]delimited-[]subscriptğ‘›ğ‘¤subscriptğ‘›ğ‘ [\![n\_{w},n\_{s}]\!] satisfying ([5](#S2.E5 "In 2.1 The algorithm â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios")). In this section, we fix a total effort K>0ğ¾0K>0 and recall how Fpâ€‹(q,N)subscriptFğ‘ğ‘ğ‘{\rm F}\_{p}(q,N), as defined in ([14](#S2.E14 "In Corollary 2.3. â€£ 2.3 Error bound for Sub-Gamma distributions â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios")), can be minimized
over the collection ğ’œğ’œ{\mathcal{A}} of sequences (N,q)âˆˆğ’©Ã—ğ’¬ğ‘ğ‘ğ’©ğ’¬(N,q)\in{\mathcal{N}}\times{\mathcal{Q}} satisfying Câ€‹(N,q)â‰¤KCğ‘ğ‘ğ¾{\rm C}(N,q)\leq K by using a standard dynamic programming approach.

Given (qÂ¯,NÂ¯)âˆˆğ’¬Ã—ğ’©Â¯ğ‘Â¯ğ‘ğ’¬ğ’©(\bar{q},\bar{N})\in{\mathcal{Q}}\times{\mathcal{N}} and 0â‰¤â„“â‰¤Lâˆ’10â„“ğ¿1{0\leq}\ell{\leq}{L-1}, we write

|  |  |  |  |
| --- | --- | --- | --- |
|  | Fpâ€‹(â„“,qÂ¯,NÂ¯):=assignsubscriptFğ‘â„“Â¯ğ‘Â¯ğ‘absent\displaystyle{\rm F}\_{p}(\ell,\bar{q},\bar{N}):= | 1nwâ€‹Î´â€‹NÂ¯LNÂ¯Lâ€‹max1â‰¤i1<â€¦<inwâ‰¤nsâ€‹âˆ‘j=1nw(Cp,Ïƒâ€‹pâ€‹Ïƒijp(Î´â€‹NÂ¯L)p2+Cp,câ€‹pâ€‹cp(Î´â€‹NÂ¯L)p)1p1subscriptğ‘›ğ‘¤ğ›¿subscriptÂ¯ğ‘ğ¿subscriptÂ¯ğ‘ğ¿1subscriptğ‘–1â€¦subscriptğ‘–subscriptğ‘›ğ‘¤subscriptğ‘›ğ‘ superscriptsubscriptğ‘—1subscriptğ‘›ğ‘¤superscriptsubscriptğ¶  ğ‘ğœğ‘subscriptsuperscriptğœğ‘subscriptğ‘–ğ‘—superscriptğ›¿subscriptÂ¯ğ‘ğ¿ğ‘2subscriptğ¶  ğ‘ğ‘ğ‘superscriptğ‘ğ‘superscriptğ›¿subscriptÂ¯ğ‘ğ¿ğ‘1ğ‘\displaystyle\frac{1}{n\_{w}}\frac{\delta\bar{N}\_{L}}{\bar{N}\_{L}}\underset{1\leq i\_{1}<...<i\_{{n\_{w}}}\leq{n\_{s}}}{\max}\sum\_{j=1}^{{n\_{w}}}\left(C\_{p,\sigma}\frac{p\sigma^{p}\_{i\_{j}}}{(\delta\bar{N}\_{L})^{\frac{p}{2}}}+C\_{p,c}\frac{pc^{p}}{(\delta\bar{N}\_{L})^{p}}\right)^{\frac{1}{p}} |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | +1nwâ€‹NÂ¯Lâˆ’1NÂ¯Lâ€‹âˆ‘i=1ns(Cp,Ïƒâ€‹pâ€‹Ïƒip(NÂ¯Lâˆ’1)p2+Cp,câ€‹pâ€‹cp(NÂ¯Lâˆ’1)p)1p+ğŸ{â„“<Lâˆ’1}â€‹âˆ‘â„“â€²=â„“+1Lâˆ’1fpâ€‹(qÂ¯â„“â€²,qÂ¯â„“â€²âˆ’1,NÂ¯â„“â€²),1subscriptğ‘›ğ‘¤subscriptÂ¯ğ‘ğ¿1subscriptÂ¯ğ‘ğ¿superscriptsubscriptğ‘–1subscriptğ‘›ğ‘ superscriptsubscriptğ¶  ğ‘ğœğ‘subscriptsuperscriptğœğ‘ğ‘–superscriptsubscriptÂ¯ğ‘ğ¿1ğ‘2subscriptğ¶  ğ‘ğ‘ğ‘superscriptğ‘ğ‘superscriptsubscriptÂ¯ğ‘ğ¿1ğ‘1ğ‘subscript1â„“ğ¿1superscriptsubscriptsuperscriptâ„“â€²â„“1ğ¿1subscriptğ‘“ğ‘subscriptÂ¯ğ‘superscriptâ„“â€²subscriptÂ¯ğ‘superscriptâ„“â€²1subscriptÂ¯ğ‘superscriptâ„“â€²\displaystyle+\frac{1}{n\_{w}}\frac{\bar{N}\_{L-1}}{\bar{N}\_{L}}\sum\_{i=1}^{{n\_{s}}}\left(C\_{p,\sigma}\frac{p\sigma^{p}\_{i}}{(\bar{N}\_{L-1})^{\frac{p}{2}}}+C\_{p,c}\frac{pc^{p}}{(\bar{N}\_{L-1})^{p}}\right)^{\frac{1}{p}}+{\mathbf{1}}\_{\{\ell<L-1\}}\sum\_{\ell^{\prime}=\ell+1}^{L-1}f\_{p}(\bar{q}\_{\ell^{\prime}},\bar{q}\_{\ell^{\prime}-1},\bar{N}\_{\ell^{\prime}}), |  | (16) |

where

|  |  |  |
| --- | --- | --- |
|  | fpâ€‹(qÂ¯â„“â€²,qÂ¯â„“â€²âˆ’1,NÂ¯â„“â€²):=(Î´â€‹qÂ¯â„“â€²)1pâ€‹max(i,k)âˆˆ[[1,nw]]Ã—[[qÂ¯â„“â€²+1,ns]]â¡(Î¼iâˆ’Î¼k)â€‹eâˆ’NÂ¯â„“â€²â€‹(Î¼iâˆ’Î¼k)22â€‹pâ€‹(Ïƒiâ€‹k2+câ€‹(Î¼iâˆ’Î¼k)),assignsubscriptğ‘“ğ‘subscriptÂ¯ğ‘superscriptâ„“â€²subscriptÂ¯ğ‘superscriptâ„“â€²1subscriptÂ¯ğ‘superscriptâ„“â€²superscriptğ›¿subscriptÂ¯ğ‘superscriptâ„“â€²1ğ‘subscriptğ‘–ğ‘˜delimited-[]1subscriptğ‘›ğ‘¤delimited-[]subscriptÂ¯ğ‘superscriptâ„“â€²1subscriptğ‘›ğ‘ superscriptğœ‡ğ‘–superscriptğœ‡ğ‘˜superscriptğ‘’subscriptÂ¯ğ‘superscriptâ„“â€²superscriptsuperscriptğœ‡ğ‘–superscriptğœ‡ğ‘˜22ğ‘superscriptsubscriptğœğ‘–ğ‘˜2ğ‘superscriptğœ‡ğ‘–superscriptğœ‡ğ‘˜f\_{p}(\bar{q}\_{\ell^{\prime}},\bar{q}\_{\ell^{\prime}-1},\bar{N}\_{\ell^{\prime}}):=(\delta\bar{q}\_{\ell^{\prime}})^{\frac{1}{p}}\max\_{(i,k)\in[\![1,n\_{w}]\!]\times[\![\bar{q}\_{\ell^{\prime}}+1,n\_{s}]\!]}(\mu^{i}-\mu^{k})e^{-\frac{\bar{N}\_{\ell^{\prime}}(\mu^{i}-\mu^{k})^{2}}{2p(\sigma\_{ik}^{2}+c(\mu^{i}-\mu^{k}))}}, |  |

and define

|  |  |  |
| --- | --- | --- |
|  | F^pâ€‹(â„“,qÂ¯,NÂ¯)=min(qÂ¯â€²,NÂ¯â€²)âˆˆğ’œâ€‹(â„“,qÂ¯,NÂ¯)â¡Fpâ€‹(â„“,qÂ¯â€²,NÂ¯â€²)subscript^Fğ‘â„“Â¯ğ‘Â¯ğ‘subscriptsuperscriptÂ¯ğ‘â€²superscriptÂ¯ğ‘â€²ğ’œâ„“Â¯ğ‘Â¯ğ‘subscriptFğ‘â„“superscriptÂ¯ğ‘â€²superscriptÂ¯ğ‘â€²\displaystyle\hat{\rm F}\_{p}(\ell,\bar{q},\bar{N})=\min\_{(\bar{q}^{\prime},\bar{N}^{\prime})\in{\mathcal{A}}(\ell,\bar{q},\bar{N})}{\rm F}\_{p}(\ell,\bar{q}^{\prime},\bar{N}^{\prime}) |  |

where999In the following, we only write ğ’œâ€‹(0)ğ’œ0{\mathcal{A}}(0) for â„“=0â„“0\ell=0 as it does not depend on (qÂ¯,NÂ¯)Â¯ğ‘Â¯ğ‘(\bar{q},\bar{N}).

|  |  |  |
| --- | --- | --- |
|  | ğ’œâ€‹(â„“,qÂ¯,NÂ¯):={(qÂ¯â€²,NÂ¯â€²)âˆˆğ’¬Ã—ğ’©:(qÂ¯lâ€²,NÂ¯lâ€²)0â‰¤lâ‰¤â„“=(qÂ¯l,NÂ¯l)0â‰¤lâ‰¤â„“â€‹Â andÂ â€‹Câ€‹(qÂ¯â€²,NÂ¯â€²)â‰¤K},â„“â‰¥0.formulae-sequenceassignğ’œâ„“Â¯ğ‘Â¯ğ‘conditional-setsuperscriptÂ¯ğ‘â€²superscriptÂ¯ğ‘â€²ğ’¬ğ’©subscriptsubscriptsuperscriptÂ¯ğ‘â€²ğ‘™subscriptsuperscriptÂ¯ğ‘â€²ğ‘™0ğ‘™â„“subscriptsubscriptÂ¯ğ‘ğ‘™subscriptÂ¯ğ‘ğ‘™0ğ‘™â„“Â andÂ CsuperscriptÂ¯ğ‘â€²superscriptÂ¯ğ‘â€²ğ¾â„“0{\mathcal{A}}(\ell,\bar{q},\bar{N}):=\{(\bar{q}^{\prime},\bar{N}^{\prime})\in{\mathcal{Q}}\times{\mathcal{N}}:(\bar{q}^{\prime}\_{l},\bar{N}^{\prime}\_{l})\_{{0}\leq l\leq\ell}=(\bar{q}\_{l},\bar{N}\_{l})\_{{0}\leq l\leq\ell}\mbox{ and }\;{\rm C}(\bar{q}^{\prime},\bar{N}^{\prime})\leq K\}\;,\;\ell\geq{0}. |  |

Then, the dynamic programming principle implies that

|  |  |  |  |
| --- | --- | --- | --- |
|  | F^pâ€‹(â„“,qÂ¯,NÂ¯)=subscript^Fğ‘â„“Â¯ğ‘Â¯ğ‘absent\displaystyle\hat{\rm F}\_{p}(\ell,\bar{q},\bar{N})= | min(qÂ¯â€²,NÂ¯â€²)âˆˆğ’œâ€‹(â„“,qÂ¯,NÂ¯)â¡[F^pâ€‹(â„“+1,qÂ¯â€²,NÂ¯â€²)+fpâ€‹(qÂ¯â„“+1â€²,qÂ¯â„“,NÂ¯â„“+1â€²)],Â forÂ â€‹0â‰¤â„“<Lâˆ’1.  subscriptsuperscriptÂ¯ğ‘â€²superscriptÂ¯ğ‘â€²ğ’œâ„“Â¯ğ‘Â¯ğ‘subscript^Fğ‘â„“1superscriptÂ¯ğ‘â€²superscriptÂ¯ğ‘â€²subscriptğ‘“ğ‘subscriptsuperscriptÂ¯ğ‘â€²â„“1subscriptÂ¯ğ‘â„“subscriptsuperscriptÂ¯ğ‘â€²â„“1Â forÂ 0 â„“ğ¿1\displaystyle\min\_{(\bar{q}^{\prime},\bar{N}^{\prime})\in{\mathcal{A}}(\ell,\bar{q},\bar{N})}\left[\hat{\rm F}\_{p}(\ell+1,\bar{q}^{\prime},\bar{N}^{\prime})+f\_{p}(\bar{q}^{\prime}\_{\ell+1},\bar{q}\_{\ell},\bar{N}^{\prime}\_{\ell+1})\right],\;\mbox{ for }0\leq\ell<{L-1}. |  |

This reduces the search for an optimal selection of (NÂ¯,qÂ¯)Â¯ğ‘Â¯ğ‘(\bar{N},\bar{q}) to Lâˆ’1ğ¿1L-1 one-step optimization problems, which is much simpler to solve than the optimization problem associated to the left-hand side of ([13](#S2.E13 "In Corollary 2.3. â€£ 2.3 Error bound for Sub-Gamma distributions â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios")).

In practice, the exact values of (Î¼i,Ïƒi2)iâ‰¤nssubscriptsuperscriptğœ‡ğ‘–subscriptsuperscriptğœ2ğ‘–ğ‘–subscriptğ‘›ğ‘ (\mu^{i},\sigma^{2}\_{i})\_{i\leq n\_{s}} and (Ïƒiâ€‹k2)i,kâ‰¤nssubscriptsubscriptsuperscriptğœ2ğ‘–ğ‘˜

ğ‘–ğ‘˜
subscriptğ‘›ğ‘ (\sigma^{2}\_{ik})\_{i,k\leq n\_{s}} are not known. However, one can consider robust versions of the above. For instance, if we know that there exists some (Î´qÂ¯,Î´qÂ¯)qâ‰¤nssubscriptÂ¯subscriptğ›¿ğ‘Â¯subscriptğ›¿ğ‘ğ‘subscriptğ‘›ğ‘ (\underline{\delta\_{q}},\overline{\delta\_{q}})\_{q\leq n\_{s}} and ÏƒÂ¯2superscriptÂ¯ğœ2\overline{\sigma}^{2} such that

|  |  |  |  |
| --- | --- | --- | --- |
|  | {0â‰¤Î´qÂ¯â‰¤Î¼iâˆ’Î¼kâ‰¤Î´qÂ¯,(i,k)âˆˆ[[1,nw]]Ã—[[q+1,ns]]Ïƒi2âˆ¨Ïƒk2âˆ¨Ïƒiâ€‹k2â‰¤ÏƒÂ¯2,(i,k)âˆˆ[[1,nw]]Ã—[[nw+1,ns]],casesformulae-sequence0Â¯subscriptğ›¿ğ‘superscriptğœ‡ğ‘–superscriptğœ‡ğ‘˜Â¯subscriptğ›¿ğ‘ğ‘–ğ‘˜delimited-[]1subscriptğ‘›ğ‘¤delimited-[]ğ‘1subscriptğ‘›ğ‘ formulae-sequencesuperscriptsubscriptğœğ‘–2superscriptsubscriptğœğ‘˜2superscriptsubscriptğœğ‘–ğ‘˜2superscriptÂ¯ğœ2ğ‘–ğ‘˜delimited-[]1subscriptğ‘›ğ‘¤delimited-[]subscriptğ‘›ğ‘¤1subscriptğ‘›ğ‘ \displaystyle\left\{\begin{array}[]{c}0\leq\underline{\delta\_{q}}\leq\mu^{i}-\mu^{k}\leq\overline{\delta\_{q}},\;(i,k)\in[\![1,n\_{w}]\!]\times[\![q+1,n\_{s}]\!]\\ \sigma\_{i}^{2}\vee\sigma\_{k}^{2}\vee\sigma\_{ik}^{2}\leq\overline{\sigma}^{2},\;(i,k)\in[\![1,n\_{w}]\!]\times[\![n\_{w}+1,n\_{s}]\!],\end{array}\right. |  | (19) |

then one can similarly minimize the upper-bound of FpsubscriptFğ‘{\rm F}\_{p} defined as

|  |  |  |
| --- | --- | --- |
|  | Î´â€‹NÂ¯LNÂ¯Lâ€‹(Cp,Ïƒâ€‹pâ€‹ÏƒÂ¯p(Î´â€‹NÂ¯L)p2+Cp,câ€‹pâ€‹cp(Î´â€‹NÂ¯L)p)+nsnwâ€‹NÂ¯Lâˆ’1NÂ¯Lâ€‹(Cp,Ïƒâ€‹pâ€‹ÏƒÂ¯p(NÂ¯Lâˆ’1)p2+Cp,câ€‹pâ€‹cp(NÂ¯Lâˆ’1)p)ğ›¿subscriptÂ¯ğ‘ğ¿subscriptÂ¯ğ‘ğ¿subscriptğ¶  ğ‘ğœğ‘superscriptÂ¯ğœğ‘superscriptğ›¿subscriptÂ¯ğ‘ğ¿ğ‘2subscriptğ¶  ğ‘ğ‘ğ‘superscriptğ‘ğ‘superscriptğ›¿subscriptÂ¯ğ‘ğ¿ğ‘subscriptğ‘›ğ‘ subscriptğ‘›ğ‘¤subscriptÂ¯ğ‘ğ¿1subscriptÂ¯ğ‘ğ¿subscriptğ¶  ğ‘ğœğ‘superscriptÂ¯ğœğ‘superscriptsubscriptÂ¯ğ‘ğ¿1ğ‘2subscriptğ¶  ğ‘ğ‘ğ‘superscriptğ‘ğ‘superscriptsubscriptÂ¯ğ‘ğ¿1ğ‘\displaystyle\frac{\delta\bar{N}\_{L}}{\bar{N}\_{L}}\left(C\_{p,\sigma}\frac{p\overline{\sigma}^{p}}{(\delta\bar{N}\_{L})^{\frac{p}{2}}}+C\_{p,c}\frac{pc^{p}}{(\delta\bar{N}\_{L})^{p}}\right)+\frac{n\_{s}}{n\_{w}}\frac{\bar{N}\_{L-1}}{\bar{N}\_{L}}\left(C\_{p,\sigma}\frac{p\overline{\sigma}^{p}}{(\bar{N}\_{L-1})^{\frac{p}{2}}}+C\_{p,c}\frac{pc^{p}}{(\bar{N}\_{L-1})^{p}}\right) |  |
|  |  |  |
| --- | --- | --- |
|  | +ğŸ{â„“<Lâˆ’1}â€‹âˆ‘â„“â€²=â„“Lâˆ’1f~pâ€‹(qÂ¯â„“â€²,qÂ¯â„“â€²âˆ’1,NÂ¯â„“â€²),subscript1â„“ğ¿1superscriptsubscriptsuperscriptâ„“â€²â„“ğ¿1subscript~ğ‘“ğ‘subscriptÂ¯ğ‘superscriptâ„“â€²subscriptÂ¯ğ‘superscriptâ„“â€²1subscriptÂ¯ğ‘superscriptâ„“â€²\displaystyle+{\mathbf{1}}\_{\{\ell<L-1\}}\sum\_{\ell^{\prime}=\ell}^{L-1}\tilde{f}\_{p}(\bar{q}\_{\ell^{\prime}},\bar{q}\_{\ell^{\prime}-1},\bar{N}\_{\ell^{\prime}}), |  |

with

|  |  |  |
| --- | --- | --- |
|  | f~pâ€‹(qÂ¯â„“â€²,qÂ¯â„“â€²âˆ’1,NÂ¯â„“â€²):=(Î´â€‹qÂ¯â„“â€²)1pâ€‹maxÎ´Â¯qÂ¯â„“â€²â‰¤Î´â‰¤Î´Â¯qÂ¯â„“â€²â¡Î´â€‹eâˆ’NÂ¯â„“â€²â€‹Î´22â€‹pâ€‹(ÏƒÂ¯2+câ€‹Î´).assignsubscript~ğ‘“ğ‘subscriptÂ¯ğ‘superscriptâ„“â€²subscriptÂ¯ğ‘superscriptâ„“â€²1subscriptÂ¯ğ‘superscriptâ„“â€²superscriptğ›¿subscriptÂ¯ğ‘superscriptâ„“â€²1ğ‘subscriptsubscriptÂ¯ğ›¿subscriptÂ¯ğ‘superscriptâ„“â€²ğ›¿subscriptÂ¯ğ›¿subscriptÂ¯ğ‘superscriptâ„“â€²ğ›¿superscriptğ‘’subscriptÂ¯ğ‘superscriptâ„“â€²superscriptğ›¿22ğ‘superscriptÂ¯ğœ2ğ‘ğ›¿\tilde{f}\_{p}(\bar{q}\_{\ell^{\prime}},\bar{q}\_{\ell^{\prime}-1},\bar{N}\_{\ell^{\prime}}):=(\delta\bar{q}\_{\ell^{\prime}})^{\frac{1}{p}}\max\_{\underline{\delta}\_{{\overline{q}\_{\ell^{\prime}}}}\leq\delta\leq\overline{\delta}\_{{\overline{q}}\_{\ell^{\prime}}}}\;\delta e^{-\frac{\bar{N}\_{\ell^{\prime}}\delta^{2}}{2p(\overline{\sigma}^{2}+c\delta)}}. |  |

This corresponds to a worst case scenario, when only the a priori bounds (Î´qÂ¯,Î´qÂ¯)qâ‰¤nssubscriptÂ¯subscriptğ›¿ğ‘Â¯subscriptğ›¿ğ‘ğ‘subscriptğ‘›ğ‘ (\underline{\delta\_{q}},\overline{\delta\_{q}})\_{q\leq n\_{s}} and ÏƒÂ¯2superscriptÂ¯ğœ2\overline{\sigma}^{2} are known.
In the above, one can also impose that qğ‘q takes values in a given subset of Qğ‘„Q of ğ’¬ğ’¬{\mathcal{Q}}. In this case, we will only need to know (Î´qÂ¯,Î´qÂ¯)qâˆˆQÂ¯subscriptÂ¯subscriptğ›¿ğ‘Â¯subscriptğ›¿ğ‘ğ‘Â¯ğ‘„(\underline{\delta\_{q}},\overline{\delta\_{q}})\_{q\in\bar{Q}}.

We refer to Section [4](#S4 "4 Numerical Experiments â€£ Computation of Expected Shortfall by fast detection of worst scenarios") below for numerical tests that show that such an algorithm seems to perform pretty well. Note that the optimization can be done off-line.

### 2.5 Simplified 2-levels algorithm for a linear indifference zoneâ€™s size

Inspired by [[3](#bib.bib3), [5](#bib.bib5), [6](#bib.bib6), [14](#bib.bib14)], we assume here that we know the value of a constant Î´0>0subscriptğ›¿00\delta\_{0}>0 such that the impacts of the nwsubscriptğ‘›ğ‘¤n\_{w} worst scenarios have values that are separated by at least (kâˆ’nw)â€‹Î´0ğ‘˜subscriptğ‘›ğ‘¤subscriptğ›¿0(k-n\_{w})\delta\_{0} from the kğ‘˜k-th worst scenario, for k>nwğ‘˜subscriptğ‘›ğ‘¤k>n\_{w}:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î¼nwâˆ’Î¼kâ‰¥(kâˆ’nw)â€‹Î´0,âˆ€kâˆˆ[[nw+1,ns]].formulae-sequencesuperscriptğœ‡subscriptğ‘›ğ‘¤superscriptğœ‡ğ‘˜ğ‘˜subscriptğ‘›ğ‘¤subscriptğ›¿0for-allğ‘˜delimited-[]subscriptğ‘›ğ‘¤1subscriptğ‘›ğ‘ \mu^{{n\_{w}}}-\mu^{{k}}\geq\left(k-n\_{w}\right)\delta\_{0},\;\forall\;k\in[\![n\_{w}+1,n\_{s}]\!]. |  | (20) |

To illustrate this, we plot on Figures [2](#S2.F2 "Figure 2 â€£ 2.5 Simplified 2-levels algorithm for a linear indifference zoneâ€™s size â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios")-[4](#S2.F4 "Figure 4 â€£ 2.5 Simplified 2-levels algorithm for a linear indifference zoneâ€™s size â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios") the curves kâ†¦|Î¼nwâˆ’Î¼k|maps-toğ‘˜superscriptğœ‡subscriptğ‘›ğ‘¤superscriptğœ‡ğ‘˜k\mapsto|\mu^{{n\_{w}}}-\mu^{{k}}| for different formerly used test books of Natixis.
We see that they are more flat on the interval [100,120]100120[100,120], so that a rather conservative value would be the minimum (over the different books) of (Î¼100âˆ’Î¼120)/20superscriptğœ‡100superscriptğœ‡12020(\mu^{100}-\mu^{120})/20. Another choice in practice could be to take the ratio (Î¼nwâˆ’Î¼100)/(100âˆ’nw)superscriptğœ‡subscriptğ‘›ğ‘¤superscriptğœ‡100100subscriptğ‘›ğ‘¤(\mu^{n\_{w}}-\mu^{100})/(100-n\_{w}) which amounts to considering only the first part of the curve, and neglecting points that are anyway far from the worst scenarios.

![Refer to caption](/html/2005.12593/assets/F2XX_mu_fig_bare.png)


Figure 1:  kâ†¦|Î¼nwâˆ’Î¼k|maps-toğ‘˜superscriptğœ‡subscriptğ‘›ğ‘¤superscriptğœ‡ğ‘˜k\mapsto|\mu^{{n\_{w}}}-\mu^{{k}}| for Book #1

![Refer to caption](/html/2005.12593/assets/F5C4_mu_fig_bare.png)


Figure 2:  kâ†¦|Î¼nwâˆ’Î¼k|maps-toğ‘˜superscriptğœ‡subscriptğ‘›ğ‘¤superscriptğœ‡ğ‘˜k\mapsto|\mu^{{n\_{w}}}-\mu^{{k}}| for Book #2



![Refer to caption](/html/2005.12593/assets/F785A_mu_fig_bare.png)


Figure 3:  kâ†¦|Î¼nwâˆ’Î¼k|maps-toğ‘˜superscriptğœ‡subscriptğ‘›ğ‘¤superscriptğœ‡ğ‘˜k\mapsto|\mu^{{n\_{w}}}-\mu^{{k}}| for Book #3

![Refer to caption](/html/2005.12593/assets/F785I_mu_fig_bare.png)


Figure 4:  kâ†¦|Î¼nwâˆ’Î¼k|maps-toğ‘˜superscriptğœ‡subscriptğ‘›ğ‘¤superscriptğœ‡ğ‘˜k\mapsto|\mu^{{n\_{w}}}-\mu^{{k}}| for Book #4

We now consider a simplified version of the algorithm of Section [2.1](#S2.SS1 "2.1 The algorithm â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios") where we only do one intermediate â€œfast pricingâ€ (meaning N1subscriptğ‘1N\_{1} rather small) and one final â€œfull pricingâ€ (meaning N2subscriptğ‘2N\_{2} large). In theory, this corresponds to L=3ğ¿3L=3 with q2=nwsubscriptğ‘2subscriptğ‘›ğ‘¤q\_{2}=n\_{w}, Î´â€‹N3=0ğ›¿subscriptğ‘30\delta N\_{3}=0 and Î´â€‹N2â†’âˆâ†’ğ›¿subscriptğ‘2\delta N\_{2}\to\infty. As Î´â€‹N2â†’âˆâ†’ğ›¿subscriptğ‘2\delta N\_{2}\to\infty, the second and third terms in ([14](#S2.E14 "In Corollary 2.3. â€£ 2.3 Error bound for Sub-Gamma distributions â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios")) vanish, as well as the component of the first term corresponding to â„“=2â„“2\ell=2. We therefore neglect them. In practice, we only take N2subscriptğ‘2N\_{2} large enough (and given) from the point of view of the bank, and minimize over (q1,N1)subscriptğ‘1subscriptğ‘1(q\_{1},N\_{1}) the remaining term in ([14](#S2.E14 "In Corollary 2.3. â€£ 2.3 Error bound for Sub-Gamma distributions â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios")):

|  |  |  |
| --- | --- | --- |
|  | F1âˆâ€‹(q1):=(nsâˆ’q1)1pâ€‹max(i,k)âˆˆ[[1,nw]]Ã—[[q1+1,ns]]â¡(Î¼iâˆ’Î¼k)â€‹eâˆ’N1â€‹(Î¼iâˆ’Î¼k)22â€‹pâ€‹(ÏƒÂ¯2+câ€‹(Î¼iâˆ’Î¼k)),assignsuperscriptsubscriptğ¹1subscriptğ‘1superscriptsubscriptğ‘›ğ‘ subscriptğ‘11ğ‘subscriptğ‘–ğ‘˜delimited-[]1subscriptğ‘›ğ‘¤delimited-[]subscriptğ‘11subscriptğ‘›ğ‘ superscriptğœ‡ğ‘–superscriptğœ‡ğ‘˜superscriptğ‘’subscriptğ‘1superscriptsuperscriptğœ‡ğ‘–superscriptğœ‡ğ‘˜22ğ‘superscriptÂ¯ğœ2ğ‘superscriptğœ‡ğ‘–superscriptğœ‡ğ‘˜\displaystyle F\_{1}^{\infty}(q\_{1}):=(n\_{s}-q\_{1})^{\frac{1}{p}}\max\_{(i,k)\in[\![1,n\_{w}]\!]\times[\![q\_{1}+1,n\_{s}]\!]}(\mu^{i}-\mu^{k})e^{-\frac{N\_{1}(\mu^{i}-\mu^{k})^{2}}{2p(\overline{\sigma}^{2}+c(\mu^{i}-\mu^{k}))}}, |  |

in which ÏƒÂ¯Â¯ğœ\bar{\sigma} is estimated to be as in ([19](#S2.E19 "In 2.4 Optimal a-priori allocation by deterministic dynamic programming based on fixed a-priori bounds â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios")),
under the computation cost constraint

|  |  |  |
| --- | --- | --- |
|  | Câ€‹(N1,q1)=q1â€‹(N2âˆ’N1)+nsâ€‹N1â‰¤KCsubscriptğ‘1subscriptğ‘1subscriptğ‘1subscriptğ‘2subscriptğ‘1subscriptğ‘›ğ‘ subscriptğ‘1ğ¾{\rm C}\left(N\_{1},q\_{1}\right)=q\_{1}(N\_{2}-N\_{1})+n\_{s}N\_{1}\leq K |  |

for some given maximal cost Kâˆˆâ„•âˆ—ğ¾superscriptâ„•K\in{\mathbb{N}}^{\*}.

For N1subscriptğ‘1N\_{1} (or Kğ¾K) large enough, the condition ([20](#S2.E20 "In 2.5 Simplified 2-levels algorithm for a linear indifference zoneâ€™s size â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios")) leads to minimizing over q1âˆˆ[[nw,ns]]âˆ©[1,K/N2]subscriptğ‘1delimited-[]subscriptğ‘›ğ‘¤subscriptğ‘›ğ‘ 1ğ¾subscriptğ‘2q\_{1}\in[\![n\_{w},n\_{s}]\!]\cap[1,K/N\_{2}] the upper-bound

|  |  |  |  |
| --- | --- | --- | --- |
|  | h0pâ€‹(q1):=(nsâˆ’q1)1pÃ—(q1+1âˆ’nw)â€‹Î´0â€‹expâ¡(âˆ’(Kâˆ’q1â€‹N2)â€‹(q1+1âˆ’nw)2â€‹Î´022â€‹pâ€‹(nsâˆ’q1)â€‹(ÏƒÂ¯2+câ€‹(q1+1âˆ’nw)â€‹Î´0)).assignsubscriptsuperscriptâ„ğ‘0subscriptğ‘1superscriptsubscriptğ‘›ğ‘ subscriptğ‘11ğ‘subscriptğ‘11subscriptğ‘›ğ‘¤subscriptğ›¿0ğ¾subscriptğ‘1subscriptğ‘2superscriptsubscriptğ‘11subscriptğ‘›ğ‘¤2superscriptsubscriptğ›¿022ğ‘subscriptğ‘›ğ‘ subscriptğ‘1superscriptÂ¯ğœ2ğ‘subscriptğ‘11subscriptğ‘›ğ‘¤subscriptğ›¿0h^{p}\_{0}(q\_{1}):=(n\_{s}-q\_{1})^{{\frac{1}{p}}}\times(q\_{1}+1-n\_{w})\delta\_{0}\exp\left(-\frac{\left(K-q\_{1}N\_{2}\right)(q\_{1}+1-n\_{w})^{2}\delta\_{0}^{2}}{2p(n\_{s}{-q\_{1}})\left(\overline{\sigma}^{2}+c\left(q\_{1}+1-n\_{w}\right)\delta\_{0}\right)}\right). |  | (21) |

The optimal q1âˆ—superscriptsubscriptğ‘1q\_{1}^{\*} can then be found easily by performing a one-dimensional numerical minimization. Upon replacing nsâˆ’q1subscriptğ‘›ğ‘ subscriptğ‘1n\_{s}{-q\_{1}} by nssubscriptğ‘›ğ‘ n\_{s} in the denominator of the exponential term, which provides a further upper-bound, the optimum can even be computed explicitly, see Appendix [A](#A1 "Appendix A Proxy of the optimal strategy for the heuristic (21) â€£ Computation of Expected Shortfall by fast detection of worst scenarios"). This provides a very easy to use algorithm.

Considering the case p=1ğ‘1p=1, let us now perform first numerical tests to see if the proxy based on h01superscriptsubscriptâ„01h\_{0}^{1} is far from F1âˆsubscriptsuperscriptğ¹1F^{\infty}\_{1}. We use the parameters of Tables [1](#S2.T1 "Table 1 â€£ 2.5 Simplified 2-levels algorithm for a linear indifference zoneâ€™s size â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios") and [2](#S2.T2 "Table 2 â€£ 2.5 Simplified 2-levels algorithm for a linear indifference zoneâ€™s size â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios") below and Î¼i=âˆ’iâ€‹Î´0superscriptğœ‡ğ‘–ğ‘–subscriptğ›¿0\mu^{i}=-i\delta\_{0}, iâ‰¤nsğ‘–subscriptğ‘›ğ‘ i\leq n\_{s}, where Î´0:=(Î¼nwâˆ’Î¼100)/(100âˆ’nw)assignsubscriptğ›¿0superscriptğœ‡subscriptğ‘›ğ‘¤superscriptğœ‡100100subscriptğ‘›ğ‘¤\delta\_{0}:=(\mu^{n\_{w}}-\mu^{100})/(100-n\_{w}) for the Î¼isuperscriptğœ‡ğ‘–\mu^{i}s of Figure [6](#S2.F6 "Figure 6 â€£ 2.5 Simplified 2-levels algorithm for a linear indifference zoneâ€™s size â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios"). In particular, we take c=0ğ‘0c=0, see Remark [2.4](#S2.Thmtheorem4 "Remark 2.4. â€£ 2.3 Error bound for Sub-Gamma distributions â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios").

In Figure [5](#S2.F5 "Figure 5 â€£ 2.5 Simplified 2-levels algorithm for a linear indifference zoneâ€™s size â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios"), the two increasing curves show the optimum q1âˆ—superscriptsubscriptğ‘1q\_{1}^{\*} (right axis) as found when applying the deterministic dynamic programming algorithm (dashed line) of Section [2.4](#S2.SS4 "2.4 Optimal a-priori allocation by deterministic dynamic programming based on fixed a-priori bounds â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios") associated to the real sample book curve of Figure [6](#S2.F6 "Figure 6 â€£ 2.5 Simplified 2-levels algorithm for a linear indifference zoneâ€™s size â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios"), and the heuristic (solid line) based on ([21](#S2.E21 "In 2.5 Simplified 2-levels algorithm for a linear indifference zoneâ€™s size â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios")). The two decreasing curves show the corresponding F1âˆâ€‹(q1âˆ—)subscriptsuperscriptğ¹1superscriptsubscriptğ‘1F^{\infty}\_{1}(q\_{1}^{\*}) (left axis) found when applying the deterministic dynamic programming algorithm (dotted line) and the heuristic (dashdot line). We see that the heuristic and the real minimizer are extremely close. The noise in the lines associated to the dynamic programming algorithm are due to grid effects.

![Refer to caption](/html/2005.12593/assets/q_star_vs_K_bare_real_mu_keep_final.png)


Figure 5: q1âˆ—superscriptsubscriptğ‘1q\_{1}^{\*} vs Kğ¾K for the distribution of Figure [6](#S2.F6 "Figure 6 â€£ 2.5 Simplified 2-levels algorithm for a linear indifference zoneâ€™s size â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios")



|  |  |
| --- | --- |
| Î´0subscriptğ›¿0\delta\_{0} | 2 766 |
| cğ‘c | 0 |
| ÏƒÂ¯Â¯ğœ\bar{\sigma} | 2â€‹(1âˆ’Ï)Ã—â€‰2â€‰200â€‰00021ğœŒ2200000\sqrt{2(1-\rho)}\times\,2\,200\,000 |
| ÏğœŒ\rho | 0.60.60.6 |
| nssubscriptğ‘›ğ‘ n\_{s} | 253 |
| nwsubscriptğ‘›ğ‘¤n\_{w} | 6 |

Table 1:  Sample Book Parameters



|  |  |
| --- | --- |
| Kğ¾K | 107superscript10710^{7} |
| N2subscriptğ‘2N\_{2} | 105superscript10510^{5} |

Table 2:  Computing Power

![Refer to caption](/html/2005.12593/assets/raw_mu_graph_bare.png)


Figure 6: Sample book distribution : iâ†¦Î¼imaps-toğ‘–superscriptğœ‡ğ‘–i\mapsto\mu^{i} for iâ‰¤nsğ‘–subscriptğ‘›ğ‘ i\leq n\_{s}

## 3 Adaptative algorithm

Although the true value Î¸âˆ˜=(Î¼âˆ˜,Î£âˆ˜)subscriptğœƒsubscriptğœ‡subscriptÎ£\theta\_{\circ}=(\mu\_{\circ},\Sigma\_{\circ}) of the vector of means and of the covariance matrix of P|sP\_{|\rm s} are unknown, we can set on it a prior distribution, e.g.Â based on previous Monte Carlo experiments, rather than just working on robust bounds as in the end of Section [2.4](#S2.SS4 "2.4 Optimal a-priori allocation by deterministic dynamic programming based on fixed a-priori bounds â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios"). Since the estimation of ESES{\rm ES} uses Monte Carlo simulations of P|sP\_{|\rm s}, the knowledge of these quantities can be improved along the different steps â„“â„“\ell of our estimation procedure. This suggests an adaptative algorithm for the optimization of the numerical effort allocation, in which we learn progressively the true value of these parameters, or part of them.
From now on, we therefore view the true value of the parameters as a random variable Î¸~:=(Î¼~,Î£~)assign~ğœƒ~ğœ‡~Î£\tilde{\theta}:=(\tilde{\mu},\tilde{\Sigma}) on which a prior law Î½0subscriptğœˆ0\nu\_{0} is set. At each step â„“â„“\ell, new Monte Carlo simulations will allow us to update this prior, and our strategy for the next steps accordingly.

### 3.1 Error bounds and convergence for predictable strategies

Let us first adapt the proof of Proposition [2.2](#S2.Thmtheorem2 "Proposition 2.2. â€£ 2.2 General a-priori bound on the ğ•ƒ^ğ‘ error â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios") and Corollary [2.3](#S2.Thmtheorem3 "Corollary 2.3. â€£ 2.3 Error bound for Sub-Gamma distributions â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios") to the case where controls are not deterministic but stochastic processes. Given a stochastic process Î±ğ›¼\alpha with values in ğ’¬Ã—ğ’©ğ’¬ğ’©{\mathcal{Q}}\times{\mathcal{N}}, we set (qÎ±,NÎ±):=Î±assignsuperscriptğ‘ğ›¼superscriptğ‘ğ›¼ğ›¼(q^{\alpha},N^{\alpha}):=\alpha where qÎ±superscriptğ‘ğ›¼q^{\alpha} and NÎ±superscriptğ‘ğ›¼N^{\alpha} are respectively ğ’¬ğ’¬{\mathcal{Q}} and ğ’©ğ’©{\mathcal{N}}-valued. We then define Î¼^Î±=(Î¼^â„“Î±)â„“â‰¤Lsuperscript^ğœ‡ğ›¼subscriptsubscriptsuperscript^ğœ‡ğ›¼â„“â„“ğ¿\hat{\mu}^{\alpha}=(\hat{\mu}^{\alpha}\_{\ell})\_{\ell\leq L}, (â„‘â„“Î±,ğ”ªâ„“Î±)â„“â‰¤Lsubscriptsubscriptsuperscriptâ„‘ğ›¼â„“subscriptsuperscriptğ”ªğ›¼â„“â„“ğ¿({\mathfrak{I}}^{\alpha}\_{\ell},{\mathfrak{m}}^{\alpha}\_{\ell})\_{\ell\leq L} as in Section [2.1](#S2.SS1 "2.1 The algorithm â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios") except that we see Î¼^â„“Î±subscriptsuperscript^ğœ‡ğ›¼â„“\hat{\mu}^{\alpha}\_{\ell} as a qâ„“Î±subscriptsuperscriptğ‘ğ›¼â„“q^{\alpha}\_{\ell}-dimensional random variables with entries given by (Î¼^â„“Î±,i)iâˆˆâ„‘â„“Î±subscriptsubscriptsuperscript^ğœ‡

ğ›¼ğ‘–â„“ğ‘–subscriptsuperscriptâ„‘ğ›¼â„“(\hat{\mu}^{\alpha,i}\_{\ell})\_{i\in{\mathfrak{I}}^{\alpha}\_{\ell}}. We use the same convention for Î´â€‹Î¼^â„“Î±ğ›¿subscriptsuperscript^ğœ‡ğ›¼â„“\delta\hat{\mu}^{\alpha}\_{\ell}, recall ([9](#S2.E9 "In 2.2 General a-priori bound on the ğ•ƒ^ğ‘ error â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios")). We say that Î±ğ›¼\alpha is admissible if it is predictable with respect to (â„±â„“Î±)â„“â‰¤Lsubscriptsubscriptsuperscriptâ„±ğ›¼â„“â„“ğ¿({\mathcal{F}}^{\alpha}\_{\ell})\_{\ell\leq L} in which â„±0Î±subscriptsuperscriptâ„±ğ›¼0{\mathcal{F}}^{\alpha}\_{0} is trivial and â„±â„“Î±=â„±â„“âˆ’1Î±âˆ¨Ïƒâ€‹(Pji,(i,j)âˆˆâ„‘â„“Î±Ã—[[1,Nâ„“Î±]])subscriptsuperscriptâ„±ğ›¼â„“subscriptsuperscriptâ„±ğ›¼â„“1ğœ

superscriptsubscriptğ‘ƒğ‘—ğ‘–ğ‘–ğ‘—
subscriptsuperscriptâ„‘ğ›¼â„“delimited-[]1subscriptsuperscriptğ‘ğ›¼â„“{\mathcal{F}}^{\alpha}\_{\ell}={\mathcal{F}}^{\alpha}\_{\ell-1}\vee\sigma(P\_{j}^{i},(i,j)\in{\mathfrak{I}}^{\alpha}\_{\ell}\times[\![1,N^{\alpha}\_{\ell}]\!]). We call ğ’œadsuperscriptğ’œad{\mathcal{A}}^{\rm ad} the collection of such processes. Then, one defines

|  |  |  |
| --- | --- | --- |
|  | ES^Î±:=1nwâ€‹âˆ‘i=1nwÎ¼^LÎ±,ğ”ªLâˆ’1Î±â€‹(i),Î±âˆˆğ’œad.formulae-sequenceassignsuperscript^ESğ›¼1subscriptğ‘›ğ‘¤superscriptsubscriptğ‘–1subscriptğ‘›ğ‘¤subscriptsuperscript^ğœ‡  ğ›¼superscriptsubscriptğ”ªğ¿1ğ›¼ğ‘–ğ¿ğ›¼superscriptğ’œad\widehat{\rm ES}^{\alpha}:=\frac{1}{n\_{w}}\sum\_{i=1}^{n\_{w}}\hat{\mu}^{\alpha,{\mathfrak{m}}\_{L-1}^{\alpha}(i)}\_{L},\;\alpha\in{\mathcal{A}}^{\rm ad}. |  |

The true value of the expected shortfall is now also written as a random variable

|  |  |  |
| --- | --- | --- |
|  | ES~:=1nwâ€‹âˆ‘i=1nwÎ¼~ğ”ª~â€‹(i),assign~ES1subscriptğ‘›ğ‘¤superscriptsubscriptğ‘–1subscriptğ‘›ğ‘¤superscript~ğœ‡~ğ”ªğ‘–\widetilde{\rm ES}:=\frac{1}{n\_{w}}\sum\_{i=1}^{n\_{w}}\tilde{\mu}^{\tilde{\mathfrak{m}}(i)}, |  |

in which ğ”ª~~ğ”ª\tilde{\mathfrak{m}} is the random permutation such that

|  |  |  |
| --- | --- | --- |
|  | {Î¼~ğ”ª~â€‹(1)â‰¥Î¼~ğ”ª~â€‹(2)â‰¥â€¦â‰¥Î¼~ğ”ª~â€‹(ns),ğ”ª~â€‹(i)<ğ”ª~â€‹(iâ€²)â€‹Â ifÂ â€‹Î¼~ğ”ª~â€‹(i)=Î¼~ğ”ª~â€‹(iâ€²)â€‹Â forÂ â€‹1â‰¤i<iâ€²â‰¤ns.casessuperscript~ğœ‡~ğ”ª1superscript~ğœ‡~ğ”ª2â€¦superscript~ğœ‡~ğ”ªsubscriptğ‘›ğ‘ ~ğ”ªğ‘–~ğ”ªsuperscriptğ‘–â€²Â ifÂ superscript~ğœ‡~ğ”ªğ‘–superscript~ğœ‡~ğ”ªsuperscriptğ‘–â€²Â forÂ 1ğ‘–superscriptğ‘–â€²subscriptğ‘›ğ‘ \left\{\begin{array}[]{l}\tilde{\mu}^{\tilde{\mathfrak{m}}(1)}\geq\tilde{\mu}^{\tilde{\mathfrak{m}}(2)}\geq\ldots\geq\tilde{\mu}^{\tilde{\mathfrak{m}}(n\_{s})},\\ \tilde{\mathfrak{m}}(i)<\tilde{\mathfrak{m}}(i^{\prime})\;\mbox{ if }\tilde{\mu}^{\tilde{\mathfrak{m}}(i)}=\tilde{\mu}^{\tilde{\mathfrak{m}}(i^{\prime})}\mbox{ for }1\leq i<i^{\prime}\leq n\_{s}.\end{array}\right. |  |

We let â„³â„³{\mathcal{M}} be a collection of laws on â„nsÃ—ğ•Šnssuperscriptâ„subscriptğ‘›ğ‘ superscriptğ•Šsubscriptğ‘›ğ‘ {\mathbb{R}}^{n\_{s}}\times{\mathbb{S}}^{n\_{s}}, where ğ•Šnssuperscriptğ•Šsubscriptğ‘›ğ‘ {\mathbb{S}}^{n\_{s}} denotes the collection of covariance matrices of size nssubscriptğ‘›ğ‘ n\_{s}. Given Î½âˆˆâ„³ğœˆâ„³\nu\in{\mathcal{M}}, we denote by ğ”¼Î½superscriptğ”¼ğœˆ{\mathbb{E}}^{\nu} the expectation operator given that Î¸~~ğœƒ\tilde{\theta} admits the law Î½ğœˆ\nu. When Î½ğœˆ\nu is a Dirac mass, we retrieve the situation of Section [2](#S2 "2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios") (up to re-ordering in a deterministic way the components of Î¼ğœ‡\mu).

We first provide a natural extension of Proposition [2.2](#S2.Thmtheorem2 "Proposition 2.2. â€£ 2.2 General a-priori bound on the ğ•ƒ^ğ‘ error â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios").

###### Proposition 3.1.

For all pâ‰¥1ğ‘1p\geq 1, Î½âˆˆâ„³ğœˆâ„³\nu\in{\mathcal{M}}, and Î±âˆˆğ’œadğ›¼superscriptğ’œad\alpha\in{\mathcal{A}}^{\rm ad},

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | ğ”¼Î½â€‹[|ES~âˆ’ES^Î±|p]1pâ‰¤superscriptğ”¼ğœˆsuperscriptdelimited-[]superscript~ESsuperscript^ESğ›¼ğ‘1ğ‘absent\displaystyle\mathbb{E}^{\nu}\left[\left|\widetilde{\rm ES}-\widehat{\rm ES}^{\alpha}\right|^{p}\right]^{\frac{1}{p}}\leq | 1nwâ€‹ğ”¼Î½â€‹[|âˆ‘iâˆˆâ„‘Lâˆ’1Î±Î¼^LÎ±,iâˆ’Î¼~i|p]1p1subscriptğ‘›ğ‘¤superscriptğ”¼ğœˆsuperscriptdelimited-[]superscriptsubscriptğ‘–superscriptsubscriptâ„‘ğ¿1ğ›¼superscriptsubscript^ğœ‡ğ¿  ğ›¼ğ‘–superscript~ğœ‡ğ‘–ğ‘1ğ‘\displaystyle\frac{1}{n\_{w}}{\mathbb{E}}^{\nu}\left[\left|\sum\_{i\in{\mathfrak{I}}\_{L-1}^{\alpha}}\hat{\mu}\_{L}^{\alpha,i}-\tilde{\mu}^{{i}}\right|^{p}\right]^{\frac{1}{p}} |  | (22) |
|  |  |  |  |
| --- | --- | --- | --- |
|  | +\displaystyle+ | âˆ‘â„“=1Lâˆ’1ğ”¼Î½[Î´qâ„“Î±max(i,k)âˆˆğ”ª~â„“âˆ’1Î±â€‹([[1,nw]]Ã—[[qâ„“Î±+1,qâ„“âˆ’1Î±]])(Î¼~iâˆ’Î¼~k)pâ„™Î½[Î¼^â„“Î±,k>Î¼^â„“Î±,i|â„±â„“âˆ’1Î±âˆ¨Ïƒ(Î¸~)]]1p,\displaystyle{\sum\_{\ell=1}^{L-1}{\mathbb{E}}^{\nu}\left[\delta q^{\alpha}\_{\ell}\max\_{(i,k)\in\tilde{\mathfrak{m}}^{\alpha}\_{\ell-1}([\![1,n\_{w}]\!]\times[\![q^{\alpha}\_{\ell}+1,q^{\alpha}\_{\ell-1}]\!])}(\tilde{\mu}^{i}-\tilde{\mu}^{k})^{p}{\mathbb{P}}^{\nu}[\hat{\mu}\_{\ell}^{\alpha,k}>\hat{\mu}\_{\ell}^{\alpha,i}|{\mathcal{F}}^{\alpha}\_{\ell-1}\vee\sigma(\tilde{\theta})]\right]^{\frac{1}{p}}}, |  |

with the convention maxâˆ…=0subscript0\max\_{\emptyset}=0 and in which ğ”ª~â„“âˆ’1Î±subscriptsuperscript~ğ”ªğ›¼â„“1\tilde{\mathfrak{m}}^{\alpha}\_{\ell-1} is defined as ğ”ª~~ğ”ª\tilde{\mathfrak{m}} but on the subset â„‘â„“âˆ’1Î±subscriptsuperscriptâ„‘ğ›¼â„“1{\mathfrak{I}}^{\alpha}\_{\ell-1} instead of â„‘0Î±=[[1,ns]]subscriptsuperscriptâ„‘ğ›¼0delimited-[]1subscriptğ‘›ğ‘ {\mathfrak{I}}^{\alpha}\_{0}=[\![1,n\_{s}]\!].

###### Proof.

We proceed as in the proof of Proposition [2.2](#S2.Thmtheorem2 "Proposition 2.2. â€£ 2.2 General a-priori bound on the ğ•ƒ^ğ‘ error â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios") to obtain that

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼Î½â€‹[|ES~âˆ’ES^Î±|p]1pâ‰¤superscriptğ”¼ğœˆsuperscriptdelimited-[]superscript~ESsuperscript^ESğ›¼ğ‘1ğ‘absent\displaystyle\mathbb{E}^{\nu}\left[\left|\widetilde{\rm ES}-\widehat{\rm ES}^{\alpha}\right|^{p}\right]^{\frac{1}{p}}\leq | ğ”¼Î½â€‹[|1nwâ€‹âˆ‘iâ‰¤nwÎ¼^LÎ±,ğ”ªLâˆ’1Î±â€‹(i)âˆ’Î¼~ğ”ªLâˆ’1Î±â€‹(i)|p]1p+ğ”¼Î½â€‹[|1nwâ€‹âˆ‘iâ‰¤nwÎ¼~ğ”ª~â€‹(i)âˆ’Î¼~ğ”ªLâˆ’1Î±â€‹(i)|p]1p,superscriptğ”¼ğœˆsuperscriptdelimited-[]superscript1subscriptğ‘›ğ‘¤subscriptğ‘–subscriptğ‘›ğ‘¤superscriptsubscript^ğœ‡ğ¿  ğ›¼subscriptsuperscriptğ”ªğ›¼ğ¿1ğ‘–superscript~ğœ‡subscriptsuperscriptğ”ªğ›¼ğ¿1ğ‘–ğ‘1ğ‘superscriptğ”¼ğœˆsuperscriptdelimited-[]superscript1subscriptğ‘›ğ‘¤subscriptğ‘–subscriptğ‘›ğ‘¤superscript~ğœ‡~ğ”ªğ‘–superscript~ğœ‡subscriptsuperscriptğ”ªğ›¼ğ¿1ğ‘–ğ‘1ğ‘\displaystyle{\mathbb{E}}^{\nu}\left[\left|\frac{1}{n\_{w}}\sum\_{i\leq n\_{w}}\hat{\mu}\_{L}^{\alpha,{\mathfrak{m}}^{\alpha}\_{L-1}(i)}-\tilde{\mu}^{{\mathfrak{m}}^{\alpha}\_{L-1}(i)}\right|^{p}\right]^{\frac{1}{p}}+{\mathbb{E}}^{\nu}\left[\left|\frac{1}{n\_{w}}\sum\_{i\leq n\_{w}}\tilde{\mu}^{\tilde{\mathfrak{m}}(i)}-\tilde{\mu}^{{\mathfrak{m}}^{\alpha}\_{L-1}(i)}\right|^{p}\right]^{\frac{1}{p}}, |  |

where

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼Î½â€‹[|1nwâ€‹âˆ‘iâ‰¤nwÎ¼^LÎ±,ğ”ªLâˆ’1Î±â€‹(i)âˆ’Î¼~ğ”ªLâˆ’1Î±â€‹(i)|p]1psuperscriptğ”¼ğœˆsuperscriptdelimited-[]superscript1subscriptğ‘›ğ‘¤subscriptğ‘–subscriptğ‘›ğ‘¤superscriptsubscript^ğœ‡ğ¿  ğ›¼subscriptsuperscriptğ”ªğ›¼ğ¿1ğ‘–superscript~ğœ‡subscriptsuperscriptğ”ªğ›¼ğ¿1ğ‘–ğ‘1ğ‘\displaystyle{\mathbb{E}}^{\nu}\left[\left|\frac{1}{n\_{w}}\sum\_{i\leq n\_{w}}\hat{\mu}\_{L}^{\alpha,{\mathfrak{m}}^{\alpha}\_{L-1}(i)}-\tilde{\mu}^{{\mathfrak{m}}^{\alpha}\_{L-1}(i)}\right|^{p}\right]^{\frac{1}{p}} | =1nwâ€‹ğ”¼Î½â€‹[|âˆ‘iâˆˆâ„‘Lâˆ’1Î±Î¼^LÎ±,iâˆ’Î¼~i|p]1p.absent1subscriptğ‘›ğ‘¤superscriptğ”¼ğœˆsuperscriptdelimited-[]superscriptsubscriptğ‘–superscriptsubscriptâ„‘ğ¿1ğ›¼superscriptsubscript^ğœ‡ğ¿  ğ›¼ğ‘–superscript~ğœ‡ğ‘–ğ‘1ğ‘\displaystyle=\frac{1}{n\_{w}}{\mathbb{E}}^{\nu}\left[\left|\sum\_{i\in{\mathfrak{I}}\_{L-1}^{\alpha}}\hat{\mu}\_{L}^{\alpha,i}-\tilde{\mu}^{i}\right|^{p}\right]^{\frac{1}{p}}. |  |

We define ğ”¨â„“Î±subscriptsuperscriptğ”¨ğ›¼â„“{\mathfrak{k}}^{\alpha}\_{\ell} as ğ”¨â„“subscriptğ”¨â„“{\mathfrak{k}}\_{\ell} in the proof of Proposition [2.2](#S2.Thmtheorem2 "Proposition 2.2. â€£ 2.2 General a-priori bound on the ğ•ƒ^ğ‘ error â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios") for the strategy Î±ğ›¼\alpha, with â„›â„“subscriptâ„›â„“\mathcal{R}\_{\ell} replaced by â„›â„“Î±subscriptsuperscriptâ„›ğ›¼â„“\mathcal{R}^{\alpha}\_{\ell} :=assign:= â„‘â„“Î±âˆ–ğ”ª~â€‹(Sqâ„“Î±â€‹[ğ”ª~âˆ’1â€‹(â„‘â„“âˆ’1)])subscriptsuperscriptâ„‘ğ›¼â„“~ğ”ªsubscriptSsubscriptsuperscriptğ‘ğ›¼â„“delimited-[]superscript~ğ”ª1subscriptâ„‘â„“1{\mathfrak{I}}^{\alpha}\_{\ell}\setminus\tilde{\mathfrak{m}}({\rm S}\_{q^{\alpha}\_{\ell}}[\tilde{\mathfrak{m}}^{-1}({\mathfrak{I}}\_{\ell-1})]). Then,

|  |  |  |
| --- | --- | --- |
|  | ğ”¼Î½â€‹[|1nwâ€‹âˆ‘iâ‰¤nwÎ¼~ğ”ª~â€‹(i)âˆ’Î¼~ğ”ªLâˆ’1Î±â€‹(i)|p]1psuperscriptğ”¼ğœˆsuperscriptdelimited-[]superscript1subscriptğ‘›ğ‘¤subscriptğ‘–subscriptğ‘›ğ‘¤superscript~ğœ‡~ğ”ªğ‘–superscript~ğœ‡subscriptsuperscriptğ”ªğ›¼ğ¿1ğ‘–ğ‘1ğ‘\displaystyle{\mathbb{E}}^{\nu}\left[\left|\frac{1}{n\_{w}}\sum\_{i\leq n\_{w}}\tilde{\mu}^{\tilde{\mathfrak{m}}(i)}-\tilde{\mu}^{{\mathfrak{m}}^{\alpha}\_{L-1}(i)}\right|^{p}\right]^{\frac{1}{p}} |  |
|  |  |  |
| --- | --- | --- |
|  | â‰¤ğ”¼Î½â€‹[|1nwâ€‹âˆ‘iâ‰¤nwâˆ‘â„“=1Lâˆ’1(Î¼~ğ”ª~â€‹(i)âˆ’Î¼~ğ”¨â„“Î±â€‹(ğ”ª~â€‹(i)))â€‹ğŸ{ğ”ª~â€‹(i)âˆˆâ„‘â„“âˆ’1Î±âˆ–â„‘â„“Î±}|p]1pabsentsuperscriptğ”¼ğœˆsuperscriptdelimited-[]superscript1subscriptğ‘›ğ‘¤subscriptğ‘–subscriptğ‘›ğ‘¤superscriptsubscriptâ„“1ğ¿1superscript~ğœ‡~ğ”ªğ‘–superscript~ğœ‡subscriptsuperscriptğ”¨ğ›¼â„“~ğ”ªğ‘–subscript1~ğ”ªğ‘–subscriptsuperscriptâ„‘ğ›¼â„“1subscriptsuperscriptâ„‘ğ›¼â„“ğ‘1ğ‘\displaystyle\leq{\mathbb{E}}^{\nu}\left[\left|\frac{1}{n\_{w}}\sum\_{i\leq n\_{w}}\sum\_{\ell=1}^{L-1}(\tilde{\mu}^{\tilde{\mathfrak{m}}(i)}-\tilde{\mu}^{{\mathfrak{k}}^{\alpha}\_{\ell}(\tilde{\mathfrak{m}}(i))}){\mathbf{1}}\_{\{\tilde{\mathfrak{m}}(i)\in{\mathfrak{I}}^{\alpha}\_{\ell-1}\setminus{\mathfrak{I}}^{\alpha}\_{\ell}\}}\right|^{p}\right]^{\frac{1}{p}} |  |
|  |  |  |
| --- | --- | --- |
|  | â‰¤1nwâ€‹âˆ‘â„“=1Lâˆ’1âˆ‘iâ‰¤nwğ”¼Î½â€‹[|(Î¼~ğ”ª~â€‹(i)âˆ’Î¼~ğ”¨â„“Î±â€‹(ğ”ª~â€‹(i)))|pâ€‹ğŸ{ğ”ª~â€‹(i)âˆˆâ„‘â„“âˆ’1Î±âˆ–â„‘â„“Î±}]1pabsent1subscriptğ‘›ğ‘¤superscriptsubscriptâ„“1ğ¿1subscriptğ‘–subscriptğ‘›ğ‘¤superscriptğ”¼ğœˆsuperscriptdelimited-[]superscriptsuperscript~ğœ‡~ğ”ªğ‘–superscript~ğœ‡subscriptsuperscriptğ”¨ğ›¼â„“~ğ”ªğ‘–ğ‘subscript1~ğ”ªğ‘–subscriptsuperscriptâ„‘ğ›¼â„“1subscriptsuperscriptâ„‘ğ›¼â„“1ğ‘\displaystyle\leq\frac{1}{n\_{w}}\sum\_{\ell=1}^{L-1}\sum\_{i\leq n\_{w}}{\mathbb{E}}^{\nu}\left[\left|(\tilde{\mu}^{\tilde{\mathfrak{m}}(i)}-\tilde{\mu}^{{\mathfrak{k}}^{\alpha}\_{\ell}(\tilde{\mathfrak{m}}(i))})\right|^{p}{\mathbf{1}}\_{\{\tilde{\mathfrak{m}}(i)\in{\mathfrak{I}}^{\alpha}\_{\ell-1}\setminus{\mathfrak{I}}^{\alpha}\_{\ell}\}}\right]^{\frac{1}{p}} |  |
|  |  |  |
| --- | --- | --- |
|  | â‰¤1nwâ€‹âˆ‘â„“=1Lâˆ’1âˆ‘iâ‰¤nwğ”¼Î½â€‹[âˆ‘kâˆˆğ”ª~â„“âˆ’1Î±â€‹([[qâ„“Î±+1,qâ„“âˆ’1Î±]])ğ”¼Î½â€‹[|(Î¼~ğ”ª~â€‹(i)âˆ’Î¼~k)|pâ€‹ğŸ{ğ”ª~â€‹(i)âˆˆâ„‘â„“âˆ’1Î±âˆ–â„‘â„“Î±,ğ”¨â„“Î±â€‹(ğ”ª~â€‹(i))=k}|â„±â„“âˆ’1Î±âˆ¨Ïƒâ€‹(Î¸~)]]1pabsent1subscriptğ‘›ğ‘¤superscriptsubscriptâ„“1ğ¿1subscriptğ‘–subscriptğ‘›ğ‘¤superscriptğ”¼ğœˆsuperscriptdelimited-[]subscriptğ‘˜subscriptsuperscript~ğ”ªğ›¼â„“1delimited-[]subscriptsuperscriptğ‘ğ›¼â„“1subscriptsuperscriptğ‘ğ›¼â„“1superscriptğ”¼ğœˆdelimited-[]conditionalsuperscriptsuperscript~ğœ‡~ğ”ªğ‘–superscript~ğœ‡ğ‘˜ğ‘subscript1formulae-sequence~ğ”ªğ‘–subscriptsuperscriptâ„‘ğ›¼â„“1subscriptsuperscriptâ„‘ğ›¼â„“subscriptsuperscriptğ”¨ğ›¼â„“~ğ”ªğ‘–ğ‘˜subscriptsuperscriptâ„±ğ›¼â„“1ğœ~ğœƒ1ğ‘\displaystyle\leq{\frac{1}{n\_{w}}\sum\_{\ell=1}^{L-1}\sum\_{i\leq n\_{w}}{\mathbb{E}}^{\nu}\left[\sum\_{k\in\tilde{\mathfrak{m}}^{\alpha}\_{\ell-1}([\![q^{\alpha}\_{\ell}+1,q^{\alpha}\_{\ell-1}]\!])}{\mathbb{E}}^{\nu}\left[\left|(\tilde{\mu}^{\tilde{\mathfrak{m}}(i)}-\tilde{\mu}^{k})\right|^{p}{\mathbf{1}}\_{\{\tilde{\mathfrak{m}}(i)\in{\mathfrak{I}}^{\alpha}\_{\ell-1}\setminus{\mathfrak{I}}^{\alpha}\_{\ell},{\mathfrak{k}}^{\alpha}\_{\ell}(\tilde{\mathfrak{m}}(i))=k\}}|{\mathcal{F}}^{\alpha}\_{\ell-1}\vee\sigma(\tilde{\theta})\right]\right]^{\frac{1}{p}}} |  |
|  |  |  |
| --- | --- | --- |
|  | â‰¤âˆ‘â„“=1Lâˆ’1ğ”¼Î½[Î´qâ„“Î±max(i,k)âˆˆğ”ª~â„“âˆ’1Î±â€‹([[1,nw]]Ã—[[qâ„“Î±+1,qâ„“âˆ’1Î±]])(Î¼~iâˆ’Î¼~k)pâ„™Î½[Î¼^â„“Î±,k>Î¼^â„“Î±,i|â„±â„“âˆ’1Î±âˆ¨Ïƒ(Î¸~)]]1p.\displaystyle{\leq\sum\_{\ell=1}^{L-1}{\mathbb{E}}^{\nu}\left[\delta q^{\alpha}\_{\ell}\max\_{(i,k)\in\tilde{\mathfrak{m}}^{\alpha}\_{\ell-1}([\![1,n\_{w}]\!]\times[\![q^{\alpha}\_{\ell}+1,q^{\alpha}\_{\ell-1}]\!])}(\tilde{\mu}^{i}-\tilde{\mu}^{k})^{p}{\mathbb{P}}^{\nu}[\hat{\mu}\_{\ell}^{\alpha,k}>\hat{\mu}\_{\ell}^{\alpha,i}|{\mathcal{F}}^{\alpha}\_{\ell-1}\vee\sigma(\tilde{\theta})]\right]^{\frac{1}{p}}.} |  |

âˆ

###### Remark 3.2.

Note that, when Î±ğ›¼\alpha is deterministic and Î½ğœˆ\nu is concentrated on a Dirac, the right-hand side of ([22](#S3.E22 "In Proposition 3.1. â€£ 3.1 Error bounds and convergence for predictable strategies â€£ 3 Adaptative algorithm â€£ Computation of Expected Shortfall by fast detection of worst scenarios")) is bounded from above by

|  |  |  |  |
| --- | --- | --- | --- |
|  |  | 1nwâ€‹Î´â€‹NLÎ±NLÎ±â€‹max1â‰¤ii<â€¦<inwâ‰¤nsâ€‹âˆ‘j=1nwğ”¼Î½â€‹[|Î´â€‹Î¼^LÎ±,ijâˆ’Î¼~ij|p]1p+1nwâ€‹NLâˆ’1Î±NLÎ±â€‹âˆ‘i=1nsğ”¼Î½â€‹[|Î¼^Lâˆ’1Î±,iâˆ’Î¼~i|p]1p1subscriptğ‘›ğ‘¤ğ›¿superscriptsubscriptğ‘ğ¿ğ›¼superscriptsubscriptğ‘ğ¿ğ›¼1subscriptğ‘–ğ‘–â€¦subscriptğ‘–subscriptğ‘›ğ‘¤subscriptğ‘›ğ‘ superscriptsubscriptğ‘—1subscriptğ‘›ğ‘¤superscriptğ”¼ğœˆsuperscriptdelimited-[]superscriptğ›¿subscriptsuperscript^ğœ‡  ğ›¼subscriptğ‘–ğ‘—ğ¿superscript~ğœ‡subscriptğ‘–ğ‘—ğ‘1ğ‘1subscriptğ‘›ğ‘¤superscriptsubscriptğ‘ğ¿1ğ›¼superscriptsubscriptğ‘ğ¿ğ›¼superscriptsubscriptğ‘–1subscriptğ‘›ğ‘ superscriptğ”¼ğœˆsuperscriptdelimited-[]superscriptsubscriptsuperscript^ğœ‡  ğ›¼ğ‘–ğ¿1superscript~ğœ‡ğ‘–ğ‘1ğ‘\displaystyle\frac{1}{n\_{w}}\frac{\delta N\_{L}^{\alpha}}{N\_{L}^{\alpha}}\underset{1\leq i\_{i}<...<i\_{n\_{w}}\leq n\_{s}}{\max}\sum\_{j=1}^{n\_{w}}{\mathbb{E}}^{\nu}\left[\left|\delta\hat{\mu}^{\alpha,i\_{j}}\_{L}-\tilde{\mu}^{i\_{j}}\right|^{p}\right]^{\frac{1}{p}}+\frac{1}{n\_{w}}\frac{N\_{L-1}^{\alpha}}{N\_{L}^{\alpha}}\sum\_{i=1}^{n\_{s}}{\mathbb{E}}^{\nu}\left[\left|\hat{\mu}^{\alpha,i}\_{L-1}-\tilde{\mu}^{i}\right|^{p}\right]^{\frac{1}{p}} |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | +\displaystyle+ | âˆ‘i=1nsâˆ‘â„“=1Lâˆ’1(Î´qâ„“Î±)1pğ”¼Î½[max(i,k)âˆˆğ”ª~â€‹[[1,nw]]Ã—[[qâ„“Î±+1,ns]](Î¼~iâˆ’Î¼~k)pâ„™Î½[Î¼^â„“Î±,k>Î¼^â„“Î±,i|â„±â„“âˆ’1Î±âˆ¨Ïƒ(Î¸~)]]1p,\displaystyle\sum\_{i=1}^{n\_{s}}\sum\_{\ell=1}^{L-1}\left(\delta q^{\alpha}\_{\ell}\right)^{\frac{1}{p}}{\mathbb{E}}^{\nu}\left[\max\_{(i,k)\in\tilde{\mathfrak{m}}[\![1,n\_{w}]\!]\times[\![q^{\alpha}\_{\ell}+1,n\_{s}]\!]}(\tilde{\mu}^{i}-\tilde{\mu}^{k})^{p}{\mathbb{P}}^{\nu}[\hat{\mu}\_{\ell}^{\alpha,k}>\hat{\mu}\_{\ell}^{\alpha,i}|{\mathcal{F}}^{\alpha}\_{\ell-1}\vee\sigma(\tilde{\theta})]\right]^{\frac{1}{p}}, |  |

which coincides with the bound of Proposition [2.2](#S2.Thmtheorem2 "Proposition 2.2. â€£ 2.2 General a-priori bound on the ğ•ƒ^ğ‘ error â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios")

The above guarantees the convergence of the algorithm.

###### Proposition 3.3.

Let (Kn)nâ‰¥1âŠ‚â„•âˆ—subscriptsuperscriptğ¾ğ‘›ğ‘›1superscriptâ„•(K^{n})\_{n\geq 1}\subset{\mathbb{N}}^{\*} be a sequence converging to infinity and let (Î±n)nâ‰¥1subscriptsuperscriptğ›¼ğ‘›ğ‘›1(\alpha^{n})\_{n\geq 1} be a sequence in ğ’œadsuperscriptğ’œad{\mathcal{A}}^{\rm ad} such that
Câ€‹(qÎ±n,NÎ±n)â‰¤KnCsuperscriptğ‘superscriptğ›¼ğ‘›superscriptğ‘superscriptğ›¼ğ‘›superscriptğ¾ğ‘›{\rm C}(q^{\alpha^{n}},N^{\alpha^{n}})\leq K^{n} for each nâ‰¥1ğ‘›1n\geq 1. Assume further that
min1â‰¤â„“â‰¤Lâ¡Nâ„“Î±nâ†’âˆâ†’subscript1â„“ğ¿subscriptsuperscriptğ‘superscriptğ›¼ğ‘›â„“\min\_{1\leq\ell\leq L}N^{\alpha^{n}}\_{\ell}\to\infty a.s.
Let Î½ğœˆ\nu be concentrated on the Dirac mass on Î¸âˆ˜subscriptğœƒ\theta\_{\circ}. Then,

|  |  |  |
| --- | --- | --- |
|  | ğ”¼Î½â€‹[|ES~âˆ’ES^Î±n|p]â†’0â€‹Â asÂ nâ†’âˆ.â†’superscriptğ”¼ğœˆdelimited-[]superscript~ESsuperscript^ESsuperscriptğ›¼ğ‘›ğ‘0Â asÂ nâ†’âˆ.\mathbb{E}^{\nu}\left[\left|\widetilde{\rm ES}-\widehat{\rm ES}^{\alpha^{n}}\right|^{p}\right]\to 0\;\;\mbox{ as $n\to\infty$.} |  |

###### Proof.

It suffices to use the fact that, for some Cp>0subscriptğ¶ğ‘0C\_{p}>0,

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼Î½â€‹[|Î¼^â„“Î±n,iâˆ’Î¼~i|p]superscriptğ”¼ğœˆdelimited-[]superscriptsuperscriptsubscript^ğœ‡â„“  superscriptğ›¼ğ‘›ğ‘–superscript~ğœ‡ğ‘–ğ‘\displaystyle{\mathbb{E}}^{\nu}\left[\left|\hat{\mu}\_{\ell}^{\alpha^{n},i}-\tilde{\mu}^{i}\right|^{p}\right] | â‰¤Cpâ€‹ğ”¼Î½â€‹[Î´â€‹Nâ„“Î±nNâ„“Î±nâ€‹ğ”¼Î½â€‹[|Î´â€‹Î¼^â„“Î±n,iâˆ’Î¼âˆ˜i|p|â„±â„“âˆ’1Î±n]]+Cpâ€‹ğ”¼Î½â€‹[Nâ„“âˆ’1Î±nNâ„“Î±nâ€‹|Î¼^â„“âˆ’1Î±n,iâˆ’Î¼âˆ˜i|p],absentsubscriptğ¶ğ‘superscriptğ”¼ğœˆdelimited-[]ğ›¿subscriptsuperscriptğ‘superscriptğ›¼ğ‘›â„“subscriptsuperscriptğ‘superscriptğ›¼ğ‘›â„“superscriptğ”¼ğœˆdelimited-[]conditionalsuperscriptğ›¿superscriptsubscript^ğœ‡â„“  superscriptğ›¼ğ‘›ğ‘–subscriptsuperscriptğœ‡ğ‘–ğ‘subscriptsuperscriptâ„±superscriptğ›¼ğ‘›â„“1subscriptğ¶ğ‘superscriptğ”¼ğœˆdelimited-[]subscriptsuperscriptğ‘superscriptğ›¼ğ‘›â„“1subscriptsuperscriptğ‘superscriptğ›¼ğ‘›â„“superscriptsuperscriptsubscript^ğœ‡â„“1  superscriptğ›¼ğ‘›ğ‘–subscriptsuperscriptğœ‡ğ‘–ğ‘\displaystyle\leq C\_{p}{\mathbb{E}}^{\nu}\left[\frac{\delta N^{\alpha^{n}}\_{\ell}}{N^{\alpha^{n}}\_{\ell}}{\mathbb{E}}^{\nu}\left[\left|\delta\hat{\mu}\_{\ell}^{\alpha^{n},i}-\mu^{i}\_{\circ}\right|^{p}|{\mathcal{F}}^{\alpha^{n}}\_{\ell-1}\right]\right]+C\_{p}{\mathbb{E}}^{\nu}\left[\frac{N^{\alpha^{n}}\_{\ell-1}}{N^{\alpha^{n}}\_{\ell}}\left|\hat{\mu}\_{\ell-1}^{\alpha^{n},i}-\mu^{i}\_{\circ}\right|^{p}\right], |  |

in which

|  |  |  |
| --- | --- | --- |
|  | Î´â€‹Nâ„“Î±nNâ„“Î±nâ€‹ğ”¼Î½â€‹[|Î´â€‹Î¼^â„“Î±n,iâˆ’Î¼âˆ˜i|p|â„±â„“âˆ’1Î±n]â†’0,Î½âˆ˜âˆ’a.s.,formulae-sequenceâ†’ğ›¿subscriptsuperscriptğ‘superscriptğ›¼ğ‘›â„“subscriptsuperscriptğ‘superscriptğ›¼ğ‘›â„“superscriptğ”¼ğœˆdelimited-[]conditionalsuperscriptğ›¿superscriptsubscript^ğœ‡â„“  superscriptğ›¼ğ‘›ğ‘–subscriptsuperscriptğœ‡ğ‘–ğ‘subscriptsuperscriptâ„±superscriptğ›¼ğ‘›â„“1  0subscriptğœˆas\frac{\delta N^{\alpha^{n}}\_{\ell}}{N^{\alpha^{n}}\_{\ell}}{\mathbb{E}}^{\nu}\left[\left|\delta\hat{\mu}\_{\ell}^{\alpha^{n},i}-\mu^{i}\_{\circ}\right|^{p}|{\mathcal{F}}^{\alpha^{n}}\_{\ell-1}\right]\to 0,\;\nu\_{\circ}-{\rm a.s.}, |  |

for all â„“>1â„“1\ell>1 and iâ‰¤nsğ‘–subscriptğ‘›ğ‘ i\leq n\_{s}. By induction, this implies that

|  |  |  |
| --- | --- | --- |
|  | ğ”¼Î½â€‹[|Î¼^â„“Î±n,iâˆ’Î¼~i|p]=ğ”¼Î½â€‹[|Î¼^â„“Î±n,iâˆ’Î¼âˆ˜i|p]â†’0superscriptğ”¼ğœˆdelimited-[]superscriptsuperscriptsubscript^ğœ‡â„“  superscriptğ›¼ğ‘›ğ‘–superscript~ğœ‡ğ‘–ğ‘superscriptğ”¼ğœˆdelimited-[]superscriptsuperscriptsubscript^ğœ‡â„“  superscriptğ›¼ğ‘›ğ‘–subscriptsuperscriptğœ‡ğ‘–ğ‘â†’0{\mathbb{E}}^{\nu}\left[\left|\hat{\mu}\_{\ell}^{\alpha^{n},i}-\tilde{\mu}^{i}\right|^{p}\right]={\mathbb{E}}^{\nu}\left[\left|\hat{\mu}\_{\ell}^{\alpha^{n},i}-\mu^{i}\_{\circ}\right|^{p}\right]\to 0 |  |

for all â„“â‰¤Lâ„“ğ¿\ell\leq L and iâ‰¤nsğ‘–subscriptğ‘›ğ‘ i\leq n\_{s}. Moreover, for some C>0ğ¶0C>0,

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼Î½â€‹[(Î¼~iâˆ’Î¼~k)pâ€‹â„™Î½â€‹[Î¼^â„“Î±n,k>Î¼^â„“Î±n,i|â„±â„“âˆ’1Î±âˆ¨Ïƒâ€‹(Î¸~)]]superscriptğ”¼ğœˆdelimited-[]superscriptsuperscript~ğœ‡ğ‘–superscript~ğœ‡ğ‘˜ğ‘superscriptâ„™ğœˆdelimited-[]superscriptsubscript^ğœ‡â„“  superscriptğ›¼ğ‘›ğ‘˜conditionalsuperscriptsubscript^ğœ‡â„“  superscriptğ›¼ğ‘›ğ‘–subscriptsuperscriptâ„±ğ›¼â„“1ğœ~ğœƒ\displaystyle{\mathbb{E}}^{\nu}\left[(\tilde{\mu}^{i}-\tilde{\mu}^{k})^{p}{\mathbb{P}}^{\nu}[\hat{\mu}\_{\ell}^{\alpha^{n},k}>\hat{\mu}\_{\ell}^{\alpha^{n},i}|{\mathcal{F}}^{\alpha}\_{\ell-1}\vee\sigma(\tilde{\theta})]\right] | â‰¤Câ€‹ğŸ{Î¼âˆ˜iâˆ’Î¼âˆ˜k>0}â€‹ğ”¼Î½â€‹[|Î¼^â„“Î±n,iâˆ’Î¼^â„“Î±n,kâˆ’(Î¼âˆ˜iâˆ’Î¼âˆ˜k)|]Î¼âˆ˜iâˆ’Î¼âˆ˜kâ†’0absentğ¶subscript1subscriptsuperscriptğœ‡ğ‘–subscriptsuperscriptğœ‡ğ‘˜0superscriptğ”¼ğœˆdelimited-[]superscriptsubscript^ğœ‡â„“  superscriptğ›¼ğ‘›ğ‘–superscriptsubscript^ğœ‡â„“  superscriptğ›¼ğ‘›ğ‘˜subscriptsuperscriptğœ‡ğ‘–subscriptsuperscriptğœ‡ğ‘˜subscriptsuperscriptğœ‡ğ‘–subscriptsuperscriptğœ‡ğ‘˜â†’0\displaystyle\leq C{\mathbf{1}}\_{\{\mu^{i}\_{\circ}-\mu^{k}\_{\circ}>0\}}\frac{{\mathbb{E}}^{\nu}[|\hat{\mu}\_{\ell}^{\alpha^{n},i}-\hat{\mu}\_{\ell}^{\alpha^{n},k}-(\mu^{i}\_{\circ}-\mu^{k}\_{\circ})|]}{\mu^{i}\_{\circ}-\mu^{k}\_{\circ}}\to 0 |  |

for all i<kğ‘–ğ‘˜i<k and â„“â‰¤Lâˆ’1â„“ğ¿1\ell\leq L-1.
âˆ

Using the fact that a control Î±âˆˆğ’œadğ›¼superscriptğ’œad\alpha\in{\mathcal{A}}^{\rm ad} is predictable, one can then proceed as in the proof of Corollary [2.3](#S2.Thmtheorem3 "Corollary 2.3. â€£ 2.3 Error bound for Sub-Gamma distributions â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios") to derive a more tractable upper-bound. It appeals to the following version of Assumption [1](#Thmassumption1 "Assumption 1. â€£ 2.3 Error bound for Sub-Gamma distributions â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios").

###### Assumption 2.

There exists c>0ğ‘0c>0 such that, for all Î½âˆˆâ„³ğœˆâ„³\nu\in{\mathcal{M}},

|  |  |  |
| --- | --- | --- |
|  | ğ”¼Î½[|Z[i,k]|p|Ïƒ(Î¸~)]â‰¤p!â€‹cpâˆ’22ğ”¼Î½[Z[i,k]2|Ïƒ(Î¸~)]Î½âˆ’a.s.,Â for allÂ i,kâ‰¤ns,pâ‰¥3.\mathbb{E}^{\nu}\left[\left|Z[i,k]\right|^{p}|\sigma(\tilde{\theta})\right]\leq\frac{p!\;c^{p-2}}{2}\mathbb{E}^{\nu}\left[Z[i,k]^{2}|\sigma(\tilde{\theta})\right]\;\nu-{\rm a.s.},\;\mbox{ for all }\;i,k\leq n\_{s},\;p\geq 3. |  |

###### Corollary 3.4.

Let Assumption [2](#Thmassumption2 "Assumption 2. â€£ 3.1 Error bounds and convergence for predictable strategies â€£ 3 Adaptative algorithm â€£ Computation of Expected Shortfall by fast detection of worst scenarios") holds. Then, for all pâ‰¥1ğ‘1p\geq 1, Î±âˆˆğ’œadğ›¼superscriptğ’œad\alpha\in{\mathcal{A}}^{\rm ad} and Î½âˆˆâ„³ğœˆâ„³\nu\in{\mathcal{M}},

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼Î½â€‹[|ES~âˆ’ES^Î±|p]1pâ‰¤superscriptğ”¼ğœˆsuperscriptdelimited-[]superscript~ESsuperscript^ESğ›¼ğ‘1ğ‘absent\displaystyle\mathbb{E}^{\nu}\left[\left|\widetilde{\rm ES}-\widehat{\rm ES}^{\alpha}\right|^{p}\right]^{\frac{1}{p}}\leq | Fpadâ€‹(Î±,Î½)subscriptsuperscriptFadğ‘ğ›¼ğœˆ\displaystyle{\rm F}^{\rm ad}\_{p}(\alpha,\nu) |  |

in which

|  |  |  |
| --- | --- | --- |
|  | Fpadâ€‹(Î±,Î½):=1nwâ€‹ğ”¼Î½â€‹[|âˆ‘iâˆˆâ„‘Lâˆ’1Î±Î¼^LÎ±,iâˆ’Î¼~i|p]1p+âˆ‘â„“=1Lâˆ’1ğ”¼Î½â€‹[fpadâ€‹(â„“,Î±,Î¸~)]1passignsubscriptsuperscriptFadğ‘ğ›¼ğœˆ1subscriptğ‘›ğ‘¤superscriptğ”¼ğœˆsuperscriptdelimited-[]superscriptsubscriptğ‘–superscriptsubscriptâ„‘ğ¿1ğ›¼superscriptsubscript^ğœ‡ğ¿  ğ›¼ğ‘–superscript~ğœ‡ğ‘–ğ‘1ğ‘superscriptsubscriptâ„“1ğ¿1superscriptğ”¼ğœˆsuperscriptdelimited-[]subscriptsuperscriptğ‘“adğ‘â„“ğ›¼~ğœƒ1ğ‘{\rm F}^{\rm ad}\_{p}(\alpha,\nu):=\frac{1}{n\_{w}}{\mathbb{E}}^{\nu}\left[\left|\sum\_{i\in{\mathfrak{I}}\_{L-1}^{\alpha}}\hat{\mu}\_{L}^{\alpha,i}-\tilde{\mu}^{i}\right|^{p}\right]^{\frac{1}{p}}+\sum\_{\ell=1}^{L-1}{\mathbb{E}}^{\nu}\left[f^{\rm ad}\_{p}(\ell,\alpha,\tilde{\theta})\right]^{\frac{1}{p}} |  |

where

|  |  |  |  |
| --- | --- | --- | --- |
|  | fpad(â„“,Î±,Î¸~):=Î´qâ„“Î±max(i,k)âˆˆğ”ª~â„“âˆ’1Î±â€‹([[1,nw]]Ã—[[qâ„“Î±+1,qâ„“âˆ’1Î±]])(Î¼~iâˆ’Î¼~k)p(eâˆ’Î´â€‹Nâ„“Î±â€‹(Ïâ„“Î±â€‹[i,k])22â€‹(Ïƒ~iâ€‹k2+câ€‹Ïâ„“Î±â€‹[i,k])ğŸ{Ïâ„“Î±â€‹[i,k]â‰¥0}+ğŸ{Ïâ„“Î±â€‹[i,k]<0})f^{\rm ad}\_{p}(\ell,\alpha,\tilde{\theta}):=\delta q^{\alpha}\_{\ell}\max\_{(i,k)\in{\tilde{\mathfrak{m}}^{\alpha}\_{\ell-1}([\![1,n\_{w}]\!]\times[\![q^{\alpha}\_{\ell}+1,q^{\alpha}\_{\ell-1}]\!])}}(\tilde{\mu}^{i}-\tilde{\mu}^{k})^{p}\left(e^{-\frac{\delta N^{\alpha}\_{\ell}(\rho^{\alpha}\_{\ell}[i,k])^{2}}{2(\tilde{\sigma}\_{ik}^{2}+c\rho^{\alpha}\_{\ell}[i,k])}}{\mathbf{1}}\_{\{\rho^{\alpha}\_{\ell}[i,k]\geq 0\}}+{\mathbf{1}}\_{\{\rho^{\alpha}\_{\ell}[i,k]<0\}}\right) |  | (23) |

with

|  |  |  |
| --- | --- | --- |
|  | Ïâ„“Î±â€‹[i,k]:=Î¼~iâˆ’Î¼~k+Nâ„“âˆ’1Î±Î´â€‹Nâ„“Î±â€‹(Î¼^â„“âˆ’1Î±,iâˆ’Î¼^â„“âˆ’1Î±,k)â€‹Â forÂ â„“â‰¥1Â andÂ i,kâ‰¤ns.assignsubscriptsuperscriptğœŒğ›¼â„“ğ‘–ğ‘˜superscript~ğœ‡ğ‘–superscript~ğœ‡ğ‘˜subscriptsuperscriptğ‘ğ›¼â„“1ğ›¿subscriptsuperscriptğ‘ğ›¼â„“superscriptsubscript^ğœ‡â„“1  ğ›¼ğ‘–superscriptsubscript^ğœ‡â„“1  ğ›¼ğ‘˜Â forÂ â„“â‰¥1Â andÂ i,kâ‰¤ns.\rho^{\alpha}\_{\ell}[i,k]:=\tilde{\mu}^{i}-\tilde{\mu}^{k}+\frac{N^{\alpha}\_{\ell-1}}{\delta N^{\alpha}\_{\ell}}(\hat{\mu}\_{\ell-1}^{\alpha,i}-\hat{\mu}\_{\ell-1}^{\alpha,k})\mbox{ for $\ell\geq 1$ and $i,k\leq n\_{s}$.} |  |

###### Proof.

We use Bernsteinâ€™s inequality, see [[7](#bib.bib7), Theorem 2.1], conditionally to â„±â„“âˆ’1Î±âˆ¨Ïƒâ€‹(Î¸~)subscriptsuperscriptâ„±ğ›¼â„“1ğœ~ğœƒ{\mathcal{F}}^{\alpha}\_{\ell-1}\vee\sigma(\tilde{\theta}), to deduce that

|  |  |  |
| --- | --- | --- |
|  | â„™Î½â€‹[Î¼^â„“Î±,k>Î¼^â„“Î±,i|â„±â„“âˆ’1Î±âˆ¨Ïƒâ€‹(Î¸~)]superscriptâ„™ğœˆdelimited-[]superscriptsubscript^ğœ‡â„“  ğ›¼ğ‘˜conditionalsuperscriptsubscript^ğœ‡â„“  ğ›¼ğ‘–subscriptsuperscriptâ„±ğ›¼â„“1ğœ~ğœƒ\displaystyle{\mathbb{P}}^{\nu}[\hat{\mu}\_{\ell}^{\alpha,k}>\hat{\mu}\_{\ell}^{\alpha,i}|{\mathcal{F}}^{\alpha}\_{\ell-1}\vee\sigma(\tilde{\theta})] |  |
|  |  |  |
| --- | --- | --- |
|  | =â„™Î½â€‹[Î´â€‹Î¼^â„“Î±,kâˆ’Î¼~kâˆ’(Î´â€‹Î¼^â„“Î±,iâˆ’Î¼~i)>Nâ„“âˆ’1Î±Î´â€‹Nâ„“Î±â€‹(Î¼^â„“âˆ’1Î±,iâˆ’Î¼^â„“âˆ’1Î±,k)âˆ’(Î¼~kâˆ’Î¼~i)|â„±â„“âˆ’1Î±âˆ¨Ïƒâ€‹(Î¸~)]absentsuperscriptâ„™ğœˆdelimited-[]ğ›¿superscriptsubscript^ğœ‡â„“  ğ›¼ğ‘˜superscript~ğœ‡ğ‘˜ğ›¿superscriptsubscript^ğœ‡â„“  ğ›¼ğ‘–superscript~ğœ‡ğ‘–subscriptsuperscriptğ‘ğ›¼â„“1ğ›¿subscriptsuperscriptğ‘ğ›¼â„“superscriptsubscript^ğœ‡â„“1  ğ›¼ğ‘–superscriptsubscript^ğœ‡â„“1  ğ›¼ğ‘˜conditionalsuperscript~ğœ‡ğ‘˜superscript~ğœ‡ğ‘–subscriptsuperscriptâ„±ğ›¼â„“1ğœ~ğœƒ\displaystyle={\mathbb{P}}^{\nu}[\delta\hat{\mu}\_{\ell}^{\alpha,k}-\tilde{\mu}^{k}-(\delta\hat{\mu}\_{\ell}^{\alpha,i}-\tilde{\mu}^{i})>\frac{N^{\alpha}\_{\ell-1}}{\delta N^{\alpha}\_{\ell}}(\hat{\mu}\_{\ell-1}^{\alpha,i}-\hat{\mu}\_{\ell-1}^{\alpha,k})-(\tilde{\mu}^{k}-\tilde{\mu}^{i})|{\mathcal{F}}^{\alpha}\_{\ell-1}\vee\sigma(\tilde{\theta})] |  |
|  |  |  |
| --- | --- | --- |
|  | â‰¤eâˆ’Î´â€‹Nâ„“Î±â€‹(Ïâ„“Î±â€‹[i,k])22â€‹(Ïƒ~iâ€‹k2+câ€‹Ïâ„“Î±â€‹[i,k])â€‹ğŸ{Ïâ„“Î±â€‹[i,k]â‰¥0}+ğŸ{Ïâ„“Î±â€‹[i,k]<0}.absentsuperscriptğ‘’ğ›¿subscriptsuperscriptğ‘ğ›¼â„“superscriptsubscriptsuperscriptğœŒğ›¼â„“ğ‘–ğ‘˜22superscriptsubscript~ğœğ‘–ğ‘˜2ğ‘subscriptsuperscriptğœŒğ›¼â„“ğ‘–ğ‘˜subscript1subscriptsuperscriptğœŒğ›¼â„“ğ‘–ğ‘˜0subscript1subscriptsuperscriptğœŒğ›¼â„“ğ‘–ğ‘˜0\displaystyle\leq e^{-\frac{\delta N^{\alpha}\_{\ell}(\rho^{\alpha}\_{\ell}[i,k])^{2}}{2(\tilde{\sigma}\_{ik}^{2}+c\rho^{\alpha}\_{\ell}[i,k])}}{\mathbf{1}}\_{\{\rho^{\alpha}\_{\ell}[i,k]\geq 0\}}+{\mathbf{1}}\_{\{\rho^{\alpha}\_{\ell}[i,k]<0\}}. |  |

âˆ

### 3.2 A generic progressive learning algorithm

Let us now describe how the result of Corollary [3.4](#S3.Thmtheorem4 "Corollary 3.4. â€£ 3.1 Error bounds and convergence for predictable strategies â€£ 3 Adaptative algorithm â€£ Computation of Expected Shortfall by fast detection of worst scenarios") can be turned into a (stochastic) dynamic programming algorithm, in the spirit of Section [2.4](#S2.SS4 "2.4 Optimal a-priori allocation by deterministic dynamic programming based on fixed a-priori bounds â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios"), that can be implemented in practice.

By Jensenâ€™s inequality, the upper-bound of Corollary [3.4](#S3.Thmtheorem4 "Corollary 3.4. â€£ 3.1 Error bounds and convergence for predictable strategies â€£ 3 Adaptative algorithm â€£ Computation of Expected Shortfall by fast detection of worst scenarios") can be rewritten as

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼Î½â€‹[|ES~âˆ’ES^Î±|p]â‰¤Fpadâ€‹(0,Î±,Î½)psuperscriptğ”¼ğœˆdelimited-[]superscript~ESsuperscript^ESğ›¼ğ‘subscriptsuperscriptFadğ‘superscript0ğ›¼ğœˆğ‘{\mathbb{E}}^{\nu}\left[\left|\widetilde{\rm ES}-\widehat{\rm ES}^{\alpha}\right|^{p}\right]\leq{\rm F}^{\rm ad}\_{p}(0,\alpha,\nu)^{p} |  | (24) |

where

|  |  |  |
| --- | --- | --- |
|  | Fpadâ€‹(0,Î±,Î½):=ğ”¼Î½â€‹[|1nwâ€‹âˆ‘iâˆˆâ„‘Lâˆ’1Î±Î¼^LÎ±,iâˆ’Î¼~i|p+âˆ‘â„“=1Lâˆ’1fpadâ€‹(â„“,Î±,Î¸~)],assignsubscriptsuperscriptFadğ‘0ğ›¼ğœˆsuperscriptğ”¼ğœˆdelimited-[]superscript1subscriptğ‘›ğ‘¤subscriptğ‘–superscriptsubscriptâ„‘ğ¿1ğ›¼superscriptsubscript^ğœ‡ğ¿  ğ›¼ğ‘–superscript~ğœ‡ğ‘–ğ‘superscriptsubscriptâ„“1ğ¿1subscriptsuperscriptğ‘“adğ‘â„“ğ›¼~ğœƒ{\rm F}^{\rm ad}\_{p}(0,\alpha,\nu):={\mathbb{E}}^{\nu}\left[\left|\frac{1}{n\_{w}}\sum\_{i\in{\mathfrak{I}}\_{L-1}^{\alpha}}\hat{\mu}\_{L}^{\alpha,i}-\tilde{\mu}^{i}\right|^{p}+\sum\_{\ell=1}^{L-1}f^{\rm ad}\_{p}(\ell,\alpha,\tilde{\theta})\right], |  |

to which we can associate the optimal control problem101010Only the conditional law given â„±â„“Î±subscriptsuperscriptâ„±ğ›¼â„“{\mathcal{F}}^{\alpha}\_{\ell} of the components of Î¸~~ğœƒ\tilde{\theta} corresponding to indexes in â„‘â„“Î±subscriptsuperscriptâ„‘ğ›¼â„“{\mathfrak{I}}^{\alpha}\_{\ell} play a role in the definition of F^padâ€‹(â„“,Î±,Î½)subscriptsuperscript^Fadğ‘â„“ğ›¼ğœˆ\hat{\rm F}^{\rm ad}\_{p}(\ell,\alpha,\nu) and Fpadâ€‹(â„“,Î±,Î½)subscriptsuperscriptFadğ‘â„“ğ›¼ğœˆ{\rm F}^{\rm ad}\_{p}(\ell,\alpha,\nu) below. To avoid introducing new complex notations, we shall indifferently take Î½ğœˆ\nu or only the conditional law of the corresponding components as an argument, depending on the context.

|  |  |  |
| --- | --- | --- |
|  | F^padâ€‹(â„“,Î±,Î½)=essâ€‹infÎ±â€²âˆˆğ’œadâ€‹(â„“,Î±)Fpadâ€‹(â„“,Î±â€²,Î½)â€‹Â forÂ 0â‰¤â„“â‰¤Lâˆ’1,Â Î½âˆˆâ„³Â andÂ Î±âˆˆğ’œad,subscriptsuperscript^Fadğ‘â„“ğ›¼ğœˆesssubscriptinfimumsuperscriptğ›¼â€²superscriptğ’œadâ„“ğ›¼subscriptsuperscriptFadğ‘â„“superscriptğ›¼â€²ğœˆÂ forÂ 0â‰¤â„“â‰¤Lâˆ’1,Â Î½âˆˆâ„³Â andÂ Î±âˆˆğ’œad,\displaystyle\hat{\rm F}^{\rm ad}\_{p}(\ell,\alpha,\nu)={\rm ess}\!\!\!\!\inf\_{\alpha^{\prime}\in{\mathcal{A}}^{\rm ad}(\ell,\alpha)}{\rm F}^{\rm ad}\_{p}(\ell,\alpha^{\prime},\nu)\;\;\mbox{ for $0\leq\ell\leq L-1$, $\nu\in{\mathcal{M}}$ and $\alpha\in{\mathcal{A}}^{\rm ad}$,} |  |

where

|  |  |  |
| --- | --- | --- |
|  | ğ’œadâ€‹(â„“,Î±):={Î±â€²=(qâ€²,Nâ€²)âˆˆğ’œad:(Î±lâ€²)0â‰¤lâ‰¤â„“=(Î±l)0â‰¤lâ‰¤â„“â€‹Â andâ€‹Câ€‹(qâ€²,Nâ€²)â‰¤K}assignsuperscriptğ’œadâ„“ğ›¼conditional-setsuperscriptğ›¼â€²superscriptğ‘â€²superscriptğ‘â€²superscriptğ’œadsubscriptsubscriptsuperscriptğ›¼â€²ğ‘™0ğ‘™â„“subscriptsubscriptğ›¼ğ‘™0ğ‘™â„“Â andCsuperscriptğ‘â€²superscriptğ‘â€²ğ¾{\mathcal{A}}^{\rm ad}(\ell,\alpha):=\{\alpha^{\prime}=(q^{\prime},N^{\prime})\in{\mathcal{A}}^{\rm ad}:(\alpha^{\prime}\_{l})\_{0\leq l\leq\ell}=(\alpha\_{l})\_{0\leq l\leq\ell}\mbox{ and}\;{\rm C}(q^{\prime},N^{\prime})\leq K\} |  |

and

|  |  |  |  |
| --- | --- | --- | --- |
|  | Fpadâ€‹(â„“,Î±â€²,Î½)subscriptsuperscriptFadğ‘â„“superscriptğ›¼â€²ğœˆ\displaystyle{\rm F}^{\rm ad}\_{p}(\ell,\alpha^{\prime},\nu) | :=ğ”¼Î½â€‹[|1nwâ€‹âˆ‘iâˆˆâ„‘Lâˆ’1Î±â€²Î¼^LÎ±â€²,iâˆ’Î¼~i|p+ğŸ{â„“<Lâˆ’1}â€‹âˆ‘l=â„“+1Lâˆ’1fpadâ€‹(l,Î±â€²,Î¸~)|â„±â„“Î±â€²].assignabsentsuperscriptğ”¼ğœˆdelimited-[]superscript1subscriptğ‘›ğ‘¤subscriptğ‘–superscriptsubscriptâ„‘ğ¿1superscriptğ›¼â€²superscriptsubscript^ğœ‡ğ¿  superscriptğ›¼â€²ğ‘–superscript~ğœ‡ğ‘–ğ‘conditionalsubscript1â„“ğ¿1superscriptsubscriptğ‘™â„“1ğ¿1subscriptsuperscriptğ‘“adğ‘ğ‘™superscriptğ›¼â€²~ğœƒsubscriptsuperscriptâ„±superscriptğ›¼â€²â„“\displaystyle:={\mathbb{E}}^{\nu}\left[\left|\frac{1}{n\_{w}}\sum\_{i\in{\mathfrak{I}}\_{L-1}^{\alpha^{\prime}}}\hat{\mu}\_{L}^{\alpha^{\prime},i}-\tilde{\mu}^{i}\right|^{p}+{{\mathbf{1}}\_{\{\ell<L-1\}}}\sum\_{l=\ell+1}^{L-1}f^{\rm ad}\_{p}(l,\alpha^{\prime},\tilde{\theta})\leavevmode\nobreak\ \Bigg{|}\leavevmode\nobreak\ {\mathcal{F}}^{\alpha^{\prime}}\_{\ell}\right]. |  |

It admits a dynamic programming principle that involves a Bayesian update of the prior law on Î¸~~ğœƒ\tilde{\theta} at each step of the algorithm, see e.g.Â [[10](#bib.bib10)].

Let us first observe that, from step â„“â„“\ell on, our bound only involves the components of Î¸~~ğœƒ\tilde{\theta} associated to the indexes in â„‘â„“Î±subscriptsuperscriptâ„‘ğ›¼â„“{\mathfrak{I}}^{\alpha}\_{\ell}. We therefore set

|  |  |  |
| --- | --- | --- |
|  | Î¸~â„“Î±=(Î¼~â„“Î±,Î£~â„“Î±):=ğ’¯â„‘â„“âˆ’1Î±â„‘â„“Î±â€‹(Î¸~â„“âˆ’1Î±),â„“â‰¥1,Â withÂ â€‹Î¸~0Î±:=Î¸~formulae-sequencesubscriptsuperscript~ğœƒğ›¼â„“subscriptsuperscript~ğœ‡ğ›¼â„“subscriptsuperscript~Î£ğ›¼â„“assignsubscriptsuperscriptğ’¯subscriptsuperscriptâ„‘ğ›¼â„“subscriptsuperscriptâ„‘ğ›¼â„“1subscriptsuperscript~ğœƒğ›¼â„“1formulae-sequenceâ„“1assignÂ withÂ subscriptsuperscript~ğœƒğ›¼0~ğœƒ\tilde{\theta}^{\alpha}\_{\ell}=(\tilde{\mu}^{\alpha}\_{\ell},\tilde{\Sigma}^{\alpha}\_{\ell}):={\cal T}^{{\mathfrak{I}}^{\alpha}\_{\ell}}\_{{\mathfrak{I}}^{\alpha}\_{\ell-1}}(\tilde{\theta}^{\alpha}\_{\ell-1}),\;\ell\geq 1,\;\mbox{ with }\tilde{\theta}^{\alpha}\_{0}:=\tilde{\theta} |  |

where, for two subsets Aâ€²âŠ‚AâŠ‚[[1,ns]]superscriptğ´â€²ğ´delimited-[]1subscriptğ‘›ğ‘ A^{\prime}\subset A\subset[\![1,n\_{s}]\!] and (Î¼,Î£)=((Î¼i)iâˆˆA,(Î£iâ€‹j)i,jâˆˆA)ğœ‡Î£subscriptsuperscriptğœ‡ğ‘–ğ‘–ğ´subscriptsuperscriptÎ£ğ‘–ğ‘—

ğ‘–ğ‘—
ğ´(\mu,\Sigma)=((\mu^{i})\_{i\in A},(\Sigma^{ij})\_{i,j\in A}), we define

|  |  |  |
| --- | --- | --- |
|  | ğ’¯AAâ€²â€‹(Î¼,Î£)=((Î¼i)iâˆˆAâ€²,(Î£iâ€‹j)i,jâˆˆAâ€²).subscriptsuperscriptğ’¯superscriptğ´â€²ğ´ğœ‡Î£subscriptsuperscriptğœ‡ğ‘–ğ‘–superscriptğ´â€²subscriptsuperscriptÎ£ğ‘–ğ‘—  ğ‘–ğ‘— superscriptğ´â€²{\cal T}^{A^{\prime}}\_{A}(\mu,\Sigma)=((\mu^{i})\_{i\in A^{\prime}},(\Sigma^{ij})\_{i,j\in A^{\prime}}). |  |

This means that the update of the prior can be restricted to a reduced number of components of Î¸~~ğœƒ\tilde{\theta}. This explains why we will concentrate on minimizing this upper-bound rather than directly the left-hand side of ([24](#S3.E24 "In 3.2 A generic progressive learning algorithm â€£ 3 Adaptative algorithm â€£ Computation of Expected Shortfall by fast detection of worst scenarios")), which would lead to a very high-dimensional optimal control problem, at each step â„“â„“\ell. This way, we expect to reduce very significantly the computation cost of the corresponding â€œoptimalâ€ strategy.

In order to make the updating rule explicit, we use the following assumption.

###### Assumption 3.

Given Î½0âˆˆâ„³subscriptğœˆ0â„³\nu\_{0}\in{\mathcal{M}}, there exists a measure mğ‘šm, such that, for all Î±âˆˆğ’œadğ›¼superscriptğ’œad\alpha\in{\mathcal{A}}^{\rm ad} and 1â‰¤â„“â‰¤L1â„“ğ¿{1}\leq\ell\leq L, the law of Iâ„“Î±:=(Pji,(i,j)âˆˆâ„‘â„“âˆ’1Î±Ã—[[Nâ„“âˆ’1Î±+1,Nâ„“Î±]])assignsubscriptsuperscriptğ¼ğ›¼â„“

superscriptsubscriptğ‘ƒğ‘—ğ‘–ğ‘–ğ‘—
subscriptsuperscriptâ„‘ğ›¼â„“1delimited-[]subscriptsuperscriptğ‘ğ›¼â„“11subscriptsuperscriptğ‘ğ›¼â„“I^{\alpha}\_{\ell}:=(P\_{j}^{i},(i,j)\in{\mathfrak{I}}^{\alpha}\_{{\ell-1}}\times[\![N^{\alpha}\_{\ell-1}+1,N^{\alpha}\_{\ell}]\!]) given â„±â„“âˆ’1Î±âˆ¨Ïƒâ€‹(Î¸~)subscriptsuperscriptâ„±ğ›¼â„“1ğœ~ğœƒ{\mathcal{F}}^{\alpha}\_{\ell-1}\vee\sigma(\tilde{\theta}) admits Î½0subscriptğœˆ0\nu\_{0}-a.s.Â the density
gâ„“Î±â€‹(â‹…,Î¸~â„“âˆ’1Î±):=gâ€‹(â‹…,â„‘â„“âˆ’1Î±,Nâ„“âˆ’1Î±,Nâ„“Î±,Î¸~â„“âˆ’1Î±)assignsubscriptsuperscriptğ‘”ğ›¼â„“â‹…subscriptsuperscript~ğœƒğ›¼â„“1ğ‘”â‹…subscriptsuperscriptâ„‘ğ›¼â„“1subscriptsuperscriptğ‘ğ›¼â„“1subscriptsuperscriptğ‘ğ›¼â„“subscriptsuperscript~ğœƒğ›¼â„“1g^{\alpha}\_{\ell}(\cdot,\tilde{\theta}^{\alpha}\_{{\ell-1}}):=g(\cdot,{\mathfrak{I}}^{\alpha}\_{{\ell-1}},N^{\alpha}\_{\ell-1},N^{\alpha}\_{\ell},\tilde{\theta}^{\alpha}\_{{\ell-1}}) with respect to mğ‘šm, in which gğ‘”g is a bounded measurable map111111As for measurability, we identify â„‘â„“âˆ’1Î±subscriptsuperscriptâ„‘ğ›¼â„“1{\mathfrak{I}}^{\alpha}\_{{\ell-1}} to the element of â„nssuperscriptâ„subscriptğ‘›ğ‘ {\mathbb{R}}^{n\_{s}} with iğ‘–i-th entry given by ğŸ{iâˆˆâ„‘â„“âˆ’1Î±}subscript1ğ‘–subscriptsuperscriptâ„‘ğ›¼â„“1{\mathbf{1}}\_{\{i\in{\mathfrak{I}}^{\alpha}\_{{\ell-1}}\}}., that is continuous in its first argument, uniformly in the other ones. Moreover, for all Î±âˆˆğ’œadğ›¼superscriptğ’œad\alpha\in{\mathcal{A}}^{\rm ad} and â„“â‰¤Lâ„“ğ¿\ell\leq L, the law of Î¸~~ğœƒ\tilde{\theta} given â„±â„“Î±subscriptsuperscriptâ„±ğ›¼â„“{\mathcal{F}}^{\alpha}\_{\ell} belongs to â„³â„³{\mathcal{M}} Î½0subscriptğœˆ0\nu\_{0}-a.s.

Under this assumption, we can compute the law Î½â„“Î±,â„“âˆ’1subscriptsuperscriptğœˆ

ğ›¼â„“1â„“{\nu^{\alpha,\ell-1}\_{\ell}} of Î¸~â„“âˆ’1Î±=ğ’¯â„“âˆ’1Î±â€‹(Î¸~)subscriptsuperscript~ğœƒğ›¼â„“1subscriptsuperscriptğ’¯ğ›¼â„“1~ğœƒ\tilde{\theta}^{\alpha}\_{{\ell-1}}={{\cal T}^{\alpha}\_{\ell-1}(\tilde{\theta})} given â„±â„“Î±subscriptsuperscriptâ„±ğ›¼â„“{\mathcal{F}}^{\alpha}\_{\ell} in terms of its counterpart Î½â„“âˆ’1Î±subscriptsuperscriptğœˆğ›¼â„“1\nu^{\alpha}\_{\ell-1} given â„±â„“âˆ’1Î±subscriptsuperscriptâ„±ğ›¼â„“1{\mathcal{F}}^{\alpha}\_{\ell-1},
in which
ğ’¯â„“âˆ’1Î±:=ğ’¯â„‘â„“âˆ’2Î±â„‘â„“âˆ’1Î±âˆ˜â‹¯âˆ˜ğ’¯â„‘0Î±â„‘1Î±.assignsubscriptsuperscriptğ’¯ğ›¼â„“1subscriptsuperscriptğ’¯subscriptsuperscriptâ„‘ğ›¼â„“1subscriptsuperscriptâ„‘ğ›¼â„“2â‹¯subscriptsuperscriptğ’¯subscriptsuperscriptâ„‘ğ›¼1subscriptsuperscriptâ„‘ğ›¼0{\cal T}^{\alpha}\_{\ell-1}:={\cal T}^{{\mathfrak{I}}^{\alpha}\_{\ell-1}}\_{{\mathfrak{I}}^{\alpha}\_{\ell-2}}\circ\cdots\circ{\cal T}^{{\mathfrak{I}}^{\alpha}\_{1}}\_{{\mathfrak{I}}^{\alpha}\_{0}}.
It is given by

|  |  |  |
| --- | --- | --- |
|  | Î½â„“Î±,â„“âˆ’1=ğ’°12â€‹(â„“,Î±,Î½â„“âˆ’1Î±)subscriptsuperscriptğœˆ  ğ›¼â„“1â„“superscriptğ’°12â„“ğ›¼subscriptsuperscriptğœˆğ›¼â„“1{\nu^{\alpha,\ell-1}\_{\ell}}={\mathcal{U}}^{\frac{1}{2}}(\ell,\alpha,\nu^{\alpha}\_{\ell-1}) |  |

with Î½0Î±=Î½subscriptsuperscriptğœˆğ›¼0ğœˆ\nu^{\alpha}\_{0}=\nu and

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ’°12â€‹(â„“,Î±,Î½â„“âˆ’1Î±)â€‹(A):=âˆ«ğ’Ÿâ„“âˆ’1Î±gâ„“Î±â€‹(Iâ„“Î±,Î¸)â€‹ğŸ{Î¸âˆˆA}â€‹Î½â„“âˆ’1Î±â€‹(dâ€‹Î¸)âˆ«ğ’Ÿâ„“âˆ’1Î±gâ„“Î±â€‹(Iâ„“Î±,Î¸)â€‹Î½â„“âˆ’1Î±â€‹(dâ€‹Î¸)assignsuperscriptğ’°12â„“ğ›¼subscriptsuperscriptğœˆğ›¼â„“1ğ´subscriptsubscriptsuperscriptğ’Ÿğ›¼â„“1subscriptsuperscriptğ‘”ğ›¼â„“subscriptsuperscriptğ¼ğ›¼â„“ğœƒsubscript1ğœƒğ´subscriptsuperscriptğœˆğ›¼â„“1ğ‘‘ğœƒsubscriptsubscriptsuperscriptğ’Ÿğ›¼â„“1subscriptsuperscriptğ‘”ğ›¼â„“subscriptsuperscriptğ¼ğ›¼â„“ğœƒsubscriptsuperscriptğœˆğ›¼â„“1ğ‘‘ğœƒ\displaystyle{\mathcal{U}}^{\frac{1}{2}}(\ell,\alpha,\nu^{\alpha}\_{\ell-1})(A):=\frac{\int\_{{\cal D}^{\alpha}\_{\ell-1}}g^{\alpha}\_{\ell}(I^{\alpha}\_{\ell},{\theta}){\mathbf{1}}\_{\{{\theta}\in A\}}{\nu^{\alpha}\_{\ell-1}}(d\theta)}{\int\_{{\cal D}^{\alpha}\_{\ell-1}}g^{\alpha}\_{\ell}(I^{\alpha}\_{\ell},{\theta})\nu^{\alpha}\_{\ell-1}(d\theta)} |  | (25) |

for a Borel set Ağ´A of
ğ’Ÿâ„“âˆ’1Î±:=ğ’¯â„“âˆ’1Î±â€‹(â„nsÃ—ğ•Šns).assignsubscriptsuperscriptğ’Ÿğ›¼â„“1subscriptsuperscriptğ’¯ğ›¼â„“1superscriptâ„subscriptğ‘›ğ‘ superscriptğ•Šsubscriptğ‘›ğ‘ {\cal D}^{\alpha}\_{\ell-1}:={\cal T}^{\alpha}\_{\ell-1}({\mathbb{R}}^{n\_{s}}\times{\mathbb{S}}^{n\_{s}}).
From this, one can deduce the law Î½â„“Î±subscriptsuperscriptğœˆğ›¼â„“{\nu^{\alpha}\_{\ell}} of Î¸~â„“Î±=ğ’¯â„“Î±â€‹(Î¸~)subscriptsuperscript~ğœƒğ›¼â„“subscriptsuperscriptğ’¯ğ›¼â„“~ğœƒ\tilde{\theta}^{\alpha}\_{{\ell}}={{\cal T}^{\alpha}\_{\ell}(\tilde{\theta})} given â„±â„“Î±subscriptsuperscriptâ„±ğ›¼â„“{\mathcal{F}}^{\alpha}\_{\ell}, in the form

|  |  |  |
| --- | --- | --- |
|  | Î½â„“Î±=ğ’°â€‹(â„“,Î±,Î½â„“âˆ’1Î±),subscriptsuperscriptğœˆğ›¼â„“ğ’°â„“ğ›¼subscriptsuperscriptğœˆğ›¼â„“1{\nu^{\alpha}\_{\ell}}={\mathcal{U}}(\ell,\alpha,\nu^{\alpha}\_{\ell-1}), |  |

by simply integrating on the components corresponding to indexes that are not in â„‘â„“Î±subscriptsuperscriptâ„‘ğ›¼â„“{\mathfrak{I}}^{\alpha}\_{\ell} (meaning that ğ’°ğ’°{\mathcal{U}} is explicit in terms of ğ’°12superscriptğ’°12{\mathcal{U}}^{\frac{1}{2}}).

We are now in position to state our dynamic programming principle, see e.g.Â [[10](#bib.bib10)]. Again, note that the law of fpadâ€‹(â„“+1,Î±â€²,Î¸~)subscriptsuperscriptğ‘“adğ‘â„“1superscriptğ›¼â€²~ğœƒf^{\rm ad}\_{p}(\ell{+1},\alpha^{\prime},\tilde{\theta}) given â„±â„“Î±â€²subscriptsuperscriptâ„±superscriptğ›¼â€²â„“{\mathcal{F}}^{\alpha^{\prime}}\_{\ell} depends on Î¸~~ğœƒ\tilde{\theta} only through Î¸~â„“Î±â€²subscriptsuperscript~ğœƒsuperscriptğ›¼â€²â„“\tilde{\theta}^{\alpha^{\prime}}\_{\ell}. For ease of notations, we identify all measures to an element of â„³â„³{\mathcal{M}} (even if it supported by a space smaller than â„nsÃ—ğ•Šnssuperscriptâ„subscriptğ‘›ğ‘ superscriptğ•Šsubscriptğ‘›ğ‘ {\mathbb{R}}^{n\_{s}}\times{\mathbb{S}}^{n\_{s}}).

###### Proposition 3.5.

Let Assumption [3](#Thmassumption3 "Assumption 3. â€£ 3.2 A generic progressive learning algorithm â€£ 3 Adaptative algorithm â€£ Computation of Expected Shortfall by fast detection of worst scenarios") hold. Then, for all Î±âˆˆğ’œadğ›¼superscriptğ’œad\alpha\in{\mathcal{A}}^{\rm ad}, 0â‰¤â„“â‰¤Lâˆ’20â„“ğ¿20\leq\ell\leq L-2 and Î½âˆˆâ„³ğœˆâ„³\nu\in{\mathcal{M}},

|  |  |  |
| --- | --- | --- |
|  | F^padâ€‹(â„“,Î±,Î½)=essâ€‹infÎ±â€²âˆˆğ’œadâ€‹(â„“,Î±)ğ”¼Î½â€‹[F^padâ€‹(â„“+1,Î±â€²,ğ’°â€‹(â„“+1,Î±â€²,Î½))+fpadâ€‹(â„“+1,Î±â€²,Î¸~)|â„±â„“Î±].subscriptsuperscript^Fadğ‘â„“ğ›¼ğœˆesssubscriptinfimumsuperscriptğ›¼â€²superscriptğ’œadâ„“ğ›¼superscriptğ”¼ğœˆdelimited-[]subscriptsuperscript^Fadğ‘â„“1superscriptğ›¼â€²ğ’°â„“1superscriptğ›¼â€²ğœˆconditionalsubscriptsuperscriptğ‘“adğ‘â„“1superscriptğ›¼â€²~ğœƒsubscriptsuperscriptâ„±ğ›¼â„“\displaystyle\hat{\rm F}^{\rm ad}\_{p}(\ell,\alpha,\nu)={\rm ess}\!\!\!\!\inf\_{\alpha^{\prime}\in{\mathcal{A}}^{\rm ad}(\ell,\alpha)}{\mathbb{E}}^{\nu}[\hat{\rm F}^{\rm ad}\_{p}(\ell+1,\alpha^{\prime},{\mathcal{U}}(\ell+1,\alpha^{\prime},\nu))+f^{\rm ad}\_{p}(\ell+1,\alpha^{\prime},\tilde{\theta})|{\mathcal{F}}^{\alpha}\_{\ell}]. |  |

In principle, this dynamic programming algorithm allows one to estimate numerically the optimal policy Î±â‹†superscriptğ›¼â‹†\alpha^{\star} in a feed-back form, off-line. Importantly, solving this problem given an initial prior Î½0subscriptğœˆ0\nu\_{0} is very different from first estimating the parameter Î¸~~ğœƒ\tilde{\theta} and then solving the control problem as if Î¸~~ğœƒ\tilde{\theta} was given. In the first case, we take into account the risk due to the uncertainly on the true value of Î¸~~ğœƒ\tilde{\theta}, not in the second one.

###### Remark 3.6.

In practice, the algorithm requires estimating and manipulating the law of a high-dimensional parameter, at least at the first steps. But the above can be modified by changing the filtration (â„±â„“Î±)â„“â‰¤Lsubscriptsubscriptsuperscriptâ„±ğ›¼â„“â„“ğ¿({\mathcal{F}}^{\alpha}\_{\ell})\_{\ell\leq L} in (â„±Â¯â„“Î±)â„“â‰¤LsubscriptsubscriptsuperscriptÂ¯â„±ğ›¼â„“â„“ğ¿(\bar{\mathcal{F}}^{\alpha}\_{\ell})\_{\ell\leq L} with â„±Â¯â„“Î±subscriptsuperscriptÂ¯â„±ğ›¼â„“\bar{\mathcal{F}}^{\alpha}\_{\ell} == Ïƒâ€‹(ğŸâ„“â‰¥Ï„Î±â€‹Pji,(i,j)âˆˆâ„‘â„“Î±Ã—[[1,Nâ„“Î±]])ğœ

subscript1â„“superscriptğœğ›¼superscriptsubscriptğ‘ƒğ‘—ğ‘–ğ‘–ğ‘—
subscriptsuperscriptâ„‘ğ›¼â„“delimited-[]1subscriptsuperscriptğ‘ğ›¼â„“\sigma({\mathbf{1}}\_{\ell\geq\tau^{\alpha}}P\_{j}^{i},(i,j)\in{\mathfrak{I}}^{\alpha}\_{\ell}\times[\![1,N^{\alpha}\_{\ell}]\!]) with Ï„Î±:=inf{lâ‰¤L:qlÎ±â‰¤Ï}assignsuperscriptğœğ›¼infimumconditional-setğ‘™ğ¿subscriptsuperscriptğ‘ğ›¼ğ‘™ğœŒ\tau^{\alpha}:=\inf\{l\leq L:q^{\alpha}\_{l}\leq\rho\} for some Ï>0ğœŒ0\rho>0. In this case, no additional information is considered up to step Ï„Î±superscriptğœğ›¼\tau^{\alpha}, the update of the prior only takes place from step Ï„Î±superscriptğœğ›¼\tau^{\alpha} on and it only concerns Î¸~Ï„Î±Î±subscriptsuperscript~ğœƒğ›¼superscriptğœğ›¼\tilde{\theta}^{\alpha}\_{\tau^{\alpha}} whose dimension is controlled by ÏğœŒ\rho. As for the first steps of the algorithm, namely before Ï„ÏsuperscriptğœğœŒ\tau^{\rho}, one can replace fpadsubscriptsuperscriptğ‘“adğ‘f^{\rm ad}\_{p} by a robust version in the spirit of Section [2.4](#S2.SS4 "2.4 Optimal a-priori allocation by deterministic dynamic programming based on fixed a-priori bounds â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios").

###### Remark 3.7.

The algorithm also requires knowing the conditional density gâ„“Î±subscriptsuperscriptğ‘”ğ›¼â„“g^{\alpha}\_{\ell}. Although, P|sP\_{|{\rm s}} can be simulated, its conditional density is not known in general. However, one can use a proxy and/or again modify the flow of information to reduce to a more explicit situation. Let us consider the situation in which (â„±â„“Î±)â„“â‰¤Lsubscriptsubscriptsuperscriptâ„±ğ›¼â„“â„“ğ¿({\mathcal{F}}^{\alpha}\_{\ell})\_{\ell\leq L} is replaced by (â„±Â¯â„“Î±)â„“â‰¤LsubscriptsubscriptsuperscriptÂ¯â„±ğ›¼â„“â„“ğ¿(\bar{\mathcal{F}}^{\alpha}\_{\ell})\_{\ell\leq L} with â„±Â¯â„“Î±subscriptsuperscriptÂ¯â„±ğ›¼â„“\bar{\mathcal{F}}^{\alpha}\_{\ell} == â„±Â¯â„“âˆ’1Î±âˆ¨Ïƒâ€‹(Î´â€‹Î¼^â„“Î±,i,iâˆˆâ„‘â„“Î±)subscriptsuperscriptÂ¯â„±ğ›¼â„“1ğœ

ğ›¿subscriptsuperscript^ğœ‡

ğ›¼ğ‘–â„“ğ‘–
subscriptsuperscriptâ„‘ğ›¼â„“\bar{\mathcal{F}}^{\alpha}\_{\ell-1}\vee\sigma(\delta\hat{\mu}^{{\alpha},i}\_{\ell},i\in{\mathfrak{I}}^{\alpha}\_{\ell}) and â„±Â¯0Î±subscriptsuperscriptÂ¯â„±ğ›¼0\bar{\mathcal{F}}^{\alpha}\_{0} is trivial. Then, conditionally to â„±Â¯â„“âˆ’1Î±âˆ¨Ïƒâ€‹(Î¸~â„“âˆ’1Î±)subscriptsuperscriptÂ¯â„±ğ›¼â„“1ğœsubscriptsuperscript~ğœƒğ›¼â„“1\bar{\mathcal{F}}^{\alpha}\_{\ell-1}\vee\sigma(\tilde{\theta}^{\alpha}\_{\ell-1}), Î´â€‹Nâ„“Î±â€‹(Î£~â„“Î±)âˆ’1â€‹(Î´â€‹Î¼^â„“Î±âˆ’Î¼~â„“Î±)ğ›¿subscriptsuperscriptğ‘ğ›¼â„“superscriptsubscriptsuperscript~Î£ğ›¼â„“1ğ›¿subscriptsuperscript^ğœ‡ğ›¼â„“subscriptsuperscript~ğœ‡ğ›¼â„“\sqrt{\delta N^{\alpha}\_{\ell}}(\tilde{\Sigma}^{\alpha}\_{\ell})^{-1}\left(\delta\hat{\mu}^{\alpha}\_{\ell}-\tilde{\mu}^{\alpha}\_{\ell}\right) is asymptotically Gaussian as Î´â€‹Nâ„“Î±ğ›¿subscriptsuperscriptğ‘ğ›¼â„“\delta N^{\alpha}\_{\ell} increases to infinity. In practice, we can do as if Î´â€‹Nâ„“Î±â€‹(Î£~â„“Î±)âˆ’1â€‹(Î´â€‹Î¼^â„“Î±âˆ’Î¼~â„“Î±)ğ›¿subscriptsuperscriptğ‘ğ›¼â„“superscriptsubscriptsuperscript~Î£ğ›¼â„“1ğ›¿subscriptsuperscript^ğœ‡ğ›¼â„“subscriptsuperscript~ğœ‡ğ›¼â„“\sqrt{\delta N^{\alpha}\_{\ell}}(\tilde{\Sigma}^{\alpha}\_{\ell})^{-1}\left(\delta\hat{\mu}^{\alpha}\_{\ell}-\tilde{\mu}^{\alpha}\_{\ell}\right) was actually following a standard Gaussian distribution, conditionally to Î¸~â„“Î±subscriptsuperscript~ğœƒğ›¼â„“\tilde{\theta}^{\alpha}\_{\ell} and â„±â„“âˆ’1Î±subscriptsuperscriptâ„±ğ›¼â„“1{\mathcal{F}}^{\alpha}\_{\ell-1}, which provides an explicit formula for the conditional density gÂ¯â„“Î±subscriptsuperscriptÂ¯ğ‘”ğ›¼â„“\bar{g}^{\alpha}\_{\ell} of Î´â€‹Î¼^â„“Î±ğ›¿subscriptsuperscript^ğœ‡ğ›¼â„“\delta\hat{\mu}^{\alpha}\_{\ell} given Î¸~â„“âˆ’1Î±subscriptsuperscript~ğœƒğ›¼â„“1\tilde{\theta}^{\alpha}\_{\ell-1} and â„±â„“âˆ’1Î±subscriptsuperscriptâ„±ğ›¼â„“1{\mathcal{F}}^{\alpha}\_{\ell-1}, to be plugged into ([25](#S3.E25 "In 3.2 A generic progressive learning algorithm â€£ 3 Adaptative algorithm â€£ Computation of Expected Shortfall by fast detection of worst scenarios")). Namely, the updating procedure takes the form

|  |  |  |
| --- | --- | --- |
|  | Î½â„“Î±=ğ’°Ë‡â€‹(â„“,Î±,Î½â„“âˆ’1Î±)subscriptsuperscriptğœˆğ›¼â„“Ë‡ğ’°â„“ğ›¼subscriptsuperscriptğœˆğ›¼â„“1\nu^{\alpha}\_{\ell}=\check{\mathcal{U}}(\ell,\alpha,\nu^{\alpha}\_{\ell-1}) |  |

where ğ’°Ë‡Ë‡ğ’°\check{\mathcal{U}} is explicit.

Then, if the initial prior Î½0subscriptğœˆ0\nu\_{0} is such that (Î¼~,Î£~)~ğœ‡~Î£(\tilde{\mu},\tilde{\Sigma}) is a Normal-inverse-Wishart distribution, all the posterior distribution Î½â„“Î±subscriptsuperscriptğœˆğ›¼â„“\nu^{\alpha}\_{\ell}, â„“â‰¤Lâ„“ğ¿\ell\leq L, are such that (Î¼~,Î£~)~ğœ‡~Î£(\tilde{\mu},\tilde{\Sigma}) remains in the class of Normal-inverse-Wishart distributions with parameters that can computed explicitly from our simulations. Namely, if, given â„±Â¯â„“Î±subscriptsuperscriptÂ¯â„±ğ›¼â„“\bar{\mathcal{F}}^{\alpha}\_{\ell}, Î£~~Î£\tilde{\Sigma} has the distribution121212Hereafter ğ’²ğš’âˆ’1â€‹(Î£)subscriptsuperscriptğ’²1ğš’Î£{\cal W}^{-1}\_{{\mathtt{i}}}(\Sigma) stands for the Inverse-Wishart distribution with degree of freedom ğš’ğš’{\mathtt{i}} and scale matrix Î£Î£\Sigma, while ğ’©â€‹(ğš–,Î£)ğ’©ğš–Î£{\cal N}({\mathtt{m}},\Sigma) is the Gaussian distribution with mean ğš–ğš–{\mathtt{m}} and covariance matrix Î£Î£\Sigma. ğ’²ğš’â„“Î±âˆ’1â€‹(Î£â„“Î±)superscriptsubscriptğ’²subscriptsuperscriptğš’ğ›¼â„“1subscriptsuperscriptÎ£ğ›¼â„“{\cal W}\_{{\mathtt{i}}^{\alpha}\_{\ell}}^{-1}(\Sigma^{\alpha}\_{\ell}) and Î¼~~ğœ‡\tilde{\mu} has the distribution ğ’©â€‹(ğš–â„“Î±,Î£~/ğš”â„“Î±)ğ’©subscriptsuperscriptğš–ğ›¼â„“~Î£subscriptsuperscriptğš”ğ›¼â„“{\cal N}({\mathtt{m}}^{\alpha}\_{\ell},\tilde{\Sigma}/{\mathtt{k}}^{\alpha}\_{\ell}) given Î£~~Î£\tilde{\Sigma}, then the coefficients corresponding to the law given â„±Â¯â„“+1Î±subscriptsuperscriptÂ¯â„±ğ›¼â„“1\bar{\mathcal{F}}^{\alpha}\_{\ell+1} are

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | {ğš’â„“+1Î±=ğš’â„“Î±+Î´â€‹Nâ„“+1Î±,ğš”â„“+1Î±=ğš”â„“Î±+Î´â€‹Nâ„“+1Î±,ğš–â„“+1Î±=1Îºâ„“Î±+Î´â€‹Nâ„“+1Î±â€‹[Îºâ„“Î±â€‹ğ’¯â„‘â„“Î±â„‘â„“+1Î±â€‹(ğš–â„“Î±)+Î´â€‹Nâ„“+1Î±â€‹Î´â€‹Î¼^â„“+1Î±]Î£â„“+1Î±=ğ’¯â„‘â„“Î±â„‘â„“+1Î±â€‹(Î£â„“Î±)+âˆ‘j=Nâ„“Î±+1Nâ„“+1Î±(ğ’¯â„“+1Î±â€‹(Pj)âˆ’Î´â€‹Î¼^â„“+1Î±)â€‹(ğ’¯â„“+1Î±â€‹(Pj)âˆ’Î´â€‹Î¼^â„“+1Î±)âŠ¤+Îºâ„“Î±â€‹Î´â€‹Nâ„“+1Î±Îºâ„“Î±+Î´â€‹Nâ„“+1Î±â€‹(ğ’¯â„‘â„“Î±â„‘â„“+1Î±â€‹(ğš–â„“Î±)âˆ’Î´â€‹Î¼^â„“+1Î±)â€‹(ğ’¯â„‘â„“Î±â„‘â„“+1Î±â€‹(ğš–â„“Î±)âˆ’Î´â€‹Î¼^â„“+1Î±)âŠ¤,casessubscriptsuperscriptğš’ğ›¼â„“1absentformulae-sequence  subscriptsuperscriptğš’ğ›¼â„“ğ›¿subscriptsuperscriptğ‘ğ›¼â„“1subscriptsuperscriptğš”ğ›¼â„“1 subscriptsuperscriptğš”ğ›¼â„“ğ›¿subscriptsuperscriptğ‘ğ›¼â„“1subscriptsuperscriptğš–ğ›¼â„“11subscriptsuperscriptğœ…ğ›¼â„“ğ›¿subscriptsuperscriptğ‘ğ›¼â„“1delimited-[]subscriptsuperscriptğœ…ğ›¼â„“subscriptsuperscriptğ’¯subscriptsuperscriptâ„‘ğ›¼â„“1subscriptsuperscriptâ„‘ğ›¼â„“subscriptsuperscriptğš–ğ›¼â„“ğ›¿subscriptsuperscriptğ‘ğ›¼â„“1ğ›¿subscriptsuperscript^ğœ‡ğ›¼â„“1subscriptsuperscriptÎ£ğ›¼â„“1absentsubscriptsuperscriptğ’¯subscriptsuperscriptâ„‘ğ›¼â„“1subscriptsuperscriptâ„‘ğ›¼â„“subscriptsuperscriptÎ£ğ›¼â„“superscriptsubscriptğ‘—subscriptsuperscriptğ‘ğ›¼â„“1subscriptsuperscriptğ‘ğ›¼â„“1subscriptsuperscriptğ’¯ğ›¼â„“1subscriptğ‘ƒğ‘—ğ›¿subscriptsuperscript^ğœ‡ğ›¼â„“1superscriptsubscriptsuperscriptğ’¯ğ›¼â„“1subscriptğ‘ƒğ‘—ğ›¿subscriptsuperscript^ğœ‡ğ›¼â„“1topmissing-subexpressionsubscriptsuperscriptğœ…ğ›¼â„“ğ›¿subscriptsuperscriptğ‘ğ›¼â„“1subscriptsuperscriptğœ…ğ›¼â„“ğ›¿subscriptsuperscriptğ‘ğ›¼â„“1subscriptsuperscriptğ’¯subscriptsuperscriptâ„‘ğ›¼â„“1subscriptsuperscriptâ„‘ğ›¼â„“subscriptsuperscriptğš–ğ›¼â„“ğ›¿subscriptsuperscript^ğœ‡ğ›¼â„“1superscriptsubscriptsuperscriptğ’¯subscriptsuperscriptâ„‘ğ›¼â„“1subscriptsuperscriptâ„‘ğ›¼â„“subscriptsuperscriptğš–ğ›¼â„“ğ›¿subscriptsuperscript^ğœ‡ğ›¼â„“1top\displaystyle\left\{\begin{array}[]{rl}{\mathtt{i}}^{\alpha}\_{\ell+1}=&{\mathtt{i}}^{\alpha}\_{\ell}+\delta N^{\alpha}\_{\ell+1},\;{\mathtt{k}}^{\alpha}\_{\ell+1}={\mathtt{k}}^{\alpha}\_{\ell}+\delta N^{\alpha}\_{\ell+1},\;{\mathtt{m}}^{\alpha}\_{\ell+1}=\frac{1}{\kappa^{\alpha}\_{\ell}+\delta N^{\alpha}\_{\ell+1}}\left[\kappa^{\alpha}\_{\ell}{\cal T}^{{\mathfrak{I}}^{\alpha}\_{\ell+1}}\_{{\mathfrak{I}}^{\alpha}\_{\ell}}({\mathtt{m}}^{\alpha}\_{\ell})+\delta N^{\alpha}\_{\ell+1}\delta\hat{\mu}^{\alpha}\_{\ell+1}\right]\\ \Sigma^{\alpha}\_{\ell+1}=&{\cal T}^{{\mathfrak{I}}^{\alpha}\_{\ell+1}}\_{{\mathfrak{I}}^{\alpha}\_{\ell}}(\Sigma^{\alpha}\_{\ell})+\sum\_{j=N^{\alpha}\_{\ell}+1}^{N^{\alpha}\_{\ell+1}}({\cal T}^{\alpha}\_{\ell+1}(P\_{j})-\delta\hat{\mu}^{\alpha}\_{\ell+1})({\cal T}^{\alpha}\_{\ell+1}(P\_{j})-\delta\hat{\mu}^{\alpha}\_{\ell+1})^{\top}\\ &+\frac{\kappa^{\alpha}\_{\ell}\delta N^{\alpha}\_{\ell+1}}{\kappa^{\alpha}\_{\ell}+\delta N^{\alpha}\_{\ell+1}}({\cal T}^{{\mathfrak{I}}^{\alpha}\_{\ell+1}}\_{{\mathfrak{I}}^{\alpha}\_{\ell}}({\mathtt{m}}^{\alpha}\_{\ell})-\delta\hat{\mu}^{\alpha}\_{\ell+1})({\cal T}^{{\mathfrak{I}}^{\alpha}\_{\ell+1}}\_{{\mathfrak{I}}^{\alpha}\_{\ell}}({\mathtt{m}}^{\alpha}\_{\ell})-\delta\hat{\mu}^{\alpha}\_{\ell+1})^{\top},\end{array}\right. |  | (29) |

see e.g.Â [[18](#bib.bib18), Section 9]. Later on, we shall write the corresponding law as ğ’©â€‹ğ’²âˆ’1â€‹(ğš™â„“+1Î±)ğ’©superscriptğ’²1subscriptsuperscriptğš™ğ›¼â„“1{\cal NW}^{-1}(\mathtt{p}^{\alpha}\_{\ell+1}) with

|  |  |  |
| --- | --- | --- |
|  | ğš™â„“+1Î±:=(ğš–â„“+1Î±,ğš”â„“+1Î±,ğš’â„“+1Î±,Î£â„“+1Î±).assignsubscriptsuperscriptğš™ğ›¼â„“1subscriptsuperscriptğš–ğ›¼â„“1subscriptsuperscriptğš”ğ›¼â„“1subscriptsuperscriptğš’ğ›¼â„“1subscriptsuperscriptÎ£ğ›¼â„“1\mathtt{p}^{\alpha}\_{\ell+1}:=({\mathtt{m}}^{\alpha}\_{\ell+1},{\mathtt{k}}^{\alpha}\_{\ell+1},{\mathtt{i}}^{\alpha}\_{\ell+1},\Sigma^{\alpha}\_{\ell+1}). |  |

### 3.3 Example of numerical implementation using neural networks

In this section, we aim at solving the version of the dynamic programming equation of Proposition [3.5](#S3.Thmtheorem5 "Proposition 3.5. â€£ 3.2 A generic progressive learning algorithm â€£ 3 Adaptative algorithm â€£ Computation of Expected Shortfall by fast detection of worst scenarios"), using an initial Normal-inverse-Wishart prior and the approximate updating procedure suggested in Remark [3.7](#S3.Thmtheorem7 "Remark 3.7. â€£ 3.2 A generic progressive learning algorithm â€£ 3 Adaptative algorithm â€£ Computation of Expected Shortfall by fast detection of worst scenarios"):

|  |  |  |
| --- | --- | --- |
|  | FË‡padâ€‹(â„“,Î±,Î½)=essâ€‹infÎ±â€²âˆˆğ’œadâ€‹(â„“,Î±)ğ”¼Î½â€‹[FË‡padâ€‹(â„“+1,Î±â€²,ğ’°Ë‡â€‹(â„“+1,Î±â€²,Î½))+fpadâ€‹(â„“+1,Î±â€²,Î¸~)|â„±Â¯â„“Î±],subscriptsuperscriptË‡Fadğ‘â„“ğ›¼ğœˆesssubscriptinfimumsuperscriptğ›¼â€²superscriptğ’œadâ„“ğ›¼superscriptğ”¼ğœˆdelimited-[]subscriptsuperscriptË‡Fadğ‘â„“1superscriptğ›¼â€²Ë‡ğ’°â„“1superscriptğ›¼â€²ğœˆconditionalsubscriptsuperscriptğ‘“adğ‘â„“1superscriptğ›¼â€²~ğœƒsubscriptsuperscriptÂ¯â„±ğ›¼â„“\displaystyle\check{\rm F}^{\rm ad}\_{p}(\ell,\alpha,\nu)={\rm ess}\!\!\!\!\inf\_{\alpha^{\prime}\in{\mathcal{A}}^{\rm ad}(\ell,\alpha)}{\mathbb{E}}^{\nu}[\check{\rm F}^{\rm ad}\_{p}(\ell+1,\alpha^{\prime},\check{\mathcal{U}}(\ell+1,\alpha^{\prime},\nu))+f^{\rm ad}\_{p}(\ell+1,\alpha^{\prime},\tilde{\theta})|\bar{\mathcal{F}}^{\alpha}\_{\ell}], |  |

with ğ’°Ë‡Ë‡ğ’°\check{\mathcal{U}} as in Remark [3.7](#S3.Thmtheorem7 "Remark 3.7. â€£ 3.2 A generic progressive learning algorithm â€£ 3 Adaptative algorithm â€£ Computation of Expected Shortfall by fast detection of worst scenarios") and

|  |  |  |
| --- | --- | --- |
|  | FË‡padâ€‹(Lâˆ’1,Î±,Î½):=ğ”¼Î½â€‹[|1nwâ€‹âˆ‘iâˆˆâ„‘Lâˆ’1Î±â€²Î¼^LÎ±â€²,iâˆ’Î¼~i|p|â„±Â¯Lâˆ’1Î±].assignsubscriptsuperscriptË‡Fadğ‘ğ¿1ğ›¼ğœˆsuperscriptğ”¼ğœˆdelimited-[]conditionalsuperscript1subscriptğ‘›ğ‘¤subscriptğ‘–superscriptsubscriptâ„‘ğ¿1superscriptğ›¼â€²superscriptsubscript^ğœ‡ğ¿  superscriptğ›¼â€²ğ‘–superscript~ğœ‡ğ‘–ğ‘subscriptsuperscriptÂ¯â„±ğ›¼ğ¿1\check{\rm F}^{\rm ad}\_{p}(L-1,\alpha,\nu):={\mathbb{E}}^{\nu}\left[\left|\frac{1}{n\_{w}}\sum\_{i\in{\mathfrak{I}}\_{L-1}^{\alpha^{\prime}}}\hat{\mu}\_{L}^{\alpha^{\prime},i}-\tilde{\mu}^{i}\right|^{p}\leavevmode\nobreak\ \Bigg{|}\leavevmode\nobreak\ \bar{\mathcal{F}}^{\alpha}\_{L-1}\right]. |  |

It would be tempting to use a standard grid-based approximation. However, to turn this problem in a Markovian one, one needs to let the value function at step â„“â„“\ell depend on qâ„“Î±subscriptsuperscriptğ‘ğ›¼â„“q^{\alpha}\_{\ell}, Nâ„“Î±subscriptsuperscriptğ‘ğ›¼â„“N^{\alpha}\_{\ell}, Câ„“Î±subscriptsuperscriptğ¶ğ›¼â„“C^{\alpha}\_{\ell}, Î¼^â„“Î±subscriptsuperscript^ğœ‡ğ›¼â„“\hat{\mu}^{\alpha}\_{\ell} and ğš™â„“Î±subscriptsuperscriptğš™ğ›¼â„“\mathtt{p}^{\alpha}\_{\ell}, where Câ„“Î±subscriptsuperscriptğ¶ğ›¼â„“C^{\alpha}\_{\ell} is the running cost of strategy Î±ğ›¼\alpha up to level â„“â„“\ell, defined for â„“â‰ 0â„“0\ell\neq 0 by Câ„“Î±=âˆ‘l=0â„“âˆ’1qlÎ±â€‹Î´â€‹Nl+1Î±subscriptsuperscriptğ¶ğ›¼â„“superscriptsubscriptğ‘™0â„“1subscriptsuperscriptğ‘ğ›¼ğ‘™ğ›¿subscriptsuperscriptğ‘ğ›¼ğ‘™1C^{\alpha}\_{\ell}=\sum\_{l=0}^{\ell-1}q^{\alpha}\_{l}\delta N^{\alpha}\_{l+1} and C0Î±=0subscriptsuperscriptğ¶ğ›¼00C^{\alpha}\_{0}=0. The dimension is then 1+1+1+qâ„“Î±+(1+qâ„“Î±+1+(qâ„“Î±)2)111subscriptsuperscriptğ‘ğ›¼â„“1subscriptsuperscriptğ‘ğ›¼â„“1superscriptsubscriptsuperscriptğ‘ğ›¼â„“21+1+1+q^{\alpha}\_{\ell}+(1+q^{\alpha}\_{\ell}+1+(q^{\alpha}\_{\ell})^{2}). Even for qâ„“Î±=20subscriptsuperscriptğ‘ğ›¼â„“20q^{\alpha}\_{\ell}=20, the corresponding space is already much too big to construct a reasonable grid on it. We therefore suggest using a neural network approximation.
Let us consider a family of bounded continuous functions {Ï•x,xâˆˆX}

subscriptitalic-Ï•xx
X\{\phi\_{{\rm x}},{\rm x}\in{\rm X}\}, XX{\rm X} being a compact subset of â„dXsuperscriptâ„subscriptğ‘‘X{\mathbb{R}}^{d\_{{\rm X}}} for some dXâ‰¥1subscriptğ‘‘X1d\_{{\rm X}}\geq 1, such that, for all q,Î´â€‹qâ‰¤ns

ğ‘ğ›¿ğ‘
subscriptğ‘›ğ‘ q,\delta q\leq n\_{s} and N,Î´â€‹Nâ‰¥1

ğ‘ğ›¿ğ‘
1N,\delta N\geq 1,

|  |  |  |
| --- | --- | --- |
|  | Ï•â‹…â€‹(Î´â€‹q,Î´â€‹N,q,N,C,â‹…):(x,Î¼,ğš™)âˆˆXÃ—â„qÃ—â„3+q+q2â†¦Ï•xâ€‹(Î´â€‹q,Î´â€‹N,q,N,C,Î¼,ğš™)âˆˆâ„â€‹Â is continuous.:subscriptitalic-Ï•â‹…ğ›¿ğ‘ğ›¿ğ‘ğ‘ğ‘ğ¶â‹…xğœ‡ğš™Xsuperscriptâ„ğ‘superscriptâ„3ğ‘superscriptğ‘2maps-tosubscriptitalic-Ï•xğ›¿ğ‘ğ›¿ğ‘ğ‘ğ‘ğ¶ğœ‡ğš™â„Â is continuous.\phi\_{\cdot}(\delta q,\delta N,q,N,C,\cdot):({\rm x},\mu,\mathtt{p})\in{\rm X}\times{\mathbb{R}}^{q}\times{\mathbb{R}}^{3+q+q^{2}}\mapsto\phi\_{{\rm x}}(\delta q,\delta N,q,N,C,\mu,\mathtt{p})\in{\mathbb{R}}\;\mbox{ is continuous.} |  |

We then fix a family {Î±k}kâ‰¤kÂ¯subscriptsuperscriptğ›¼ğ‘˜ğ‘˜Â¯ğ‘˜\{\alpha^{k}\}\_{k\leq\bar{k}} of deterministic paths of ğ’œâ€‹(0)ğ’œ0{\mathcal{A}}(0) and simulate independent copies {Î¸~j}jâ‰¤jÂ¯subscriptsuperscript~ğœƒğ‘—ğ‘—Â¯ğ‘—\{\tilde{\theta}^{j}\}\_{j\leq\bar{j}} of Î¸~~ğœƒ\tilde{\theta} according to Î½0subscriptğœˆ0\nu\_{0}, a Normal-inverse-Wishart distribution ğ’©â€‹ğ’²âˆ’1â€‹(ğš™0)ğ’©superscriptğ’²1subscriptğš™0{\cal NW}^{-1}(\mathtt{p}\_{0}). For each jğ‘—j, we consider an i.i.d.Â sequence (Pjâ€²j,1,â€¦,Pjâ€²j,ns)jâ€²â‰¥1subscriptsuperscriptsubscriptğ‘ƒsuperscriptğ‘—â€²

ğ‘—1â€¦superscriptsubscriptğ‘ƒsuperscriptğ‘—â€²

ğ‘—subscriptğ‘›ğ‘ superscriptğ‘—â€²1(P\_{j^{\prime}}^{j,1},\ldots,P\_{j^{\prime}}^{j,n\_{s}})\_{j^{\prime}\geq 1} in the law ğ’©â€‹(Î¼~j,Î£~j)ğ’©superscript~ğœ‡ğ‘—superscript~Î£ğ‘—{\mathcal{N}}(\tilde{\mu}^{j},\tilde{\Sigma}^{j}) with Î¸~j=:(Î¼~j,Î£~j)\tilde{\theta}^{j}=:(\tilde{\mu}^{j},\tilde{\Sigma}^{j}). We take these sequences independent and independent of Î¸~~ğœƒ\tilde{\theta}.
For each kğ‘˜k and jğ‘—j, we denote by (Î¼^â„“k,j)â„“â‰¤Lsubscriptsubscriptsuperscript^ğœ‡

ğ‘˜ğ‘—â„“â„“ğ¿(\hat{\mu}^{k,j}\_{\ell})\_{\ell\leq L}, (ğš™~â„“k,j)â„“â‰¤Lsubscriptsubscriptsuperscript~ğš™

ğ‘˜ğ‘—â„“â„“ğ¿(\tilde{\mathtt{p}}^{k,j}\_{\ell})\_{\ell\leq L} and (â„‘â„“k,j)â„“â‰¤Lsubscriptsuperscriptsubscriptâ„‘â„“

ğ‘˜ğ‘—â„“ğ¿({\mathfrak{I}}\_{\ell}^{k,j})\_{\ell\leq L} the paths (Î¼^â„“Î±k)â„“â‰¤Lsubscriptsubscriptsuperscript^ğœ‡superscriptğ›¼ğ‘˜â„“â„“ğ¿(\hat{\mu}^{\alpha^{k}}\_{\ell})\_{\ell\leq L}, (ğš™â„“Î±k)â„“â‰¤Lsubscriptsubscriptsuperscriptğš™superscriptğ›¼ğ‘˜â„“â„“ğ¿(\mathtt{p}^{\alpha^{k}}\_{\ell})\_{\ell\leq L} and (â„‘â„“Î±k)â„“â‰¤Lsubscriptsuperscriptsubscriptâ„‘â„“superscriptğ›¼ğ‘˜â„“ğ¿({\mathfrak{I}}\_{\ell}^{\alpha^{k}})\_{\ell\leq L} associated to the jğ‘—j-th sequence (Pjâ€²j,1,â€¦,Pjâ€²j,ns)jâ€²â‰¥1subscriptsuperscriptsubscriptğ‘ƒsuperscriptğ‘—â€²

ğ‘—1â€¦superscriptsubscriptğ‘ƒsuperscriptğ‘—â€²

ğ‘—subscriptğ‘›ğ‘ superscriptğ‘—â€²1(P\_{j^{\prime}}^{j,1},\ldots,P\_{j^{\prime}}^{j,n\_{s}})\_{j^{\prime}\geq 1} and the control Î±ksuperscriptğ›¼ğ‘˜\alpha^{k}. Similarly, we write fpad,k,jâ€‹(â„“,â‹…)superscriptsubscriptğ‘“ğ‘

adğ‘˜ğ‘—â„“â‹…f\_{p}^{{\rm ad},k,j}(\ell,\cdot) to denote the function fpadâ€‹(â„“,â‹…)superscriptsubscriptğ‘“ğ‘adâ„“â‹…f\_{p}^{\rm ad}(\ell,\cdot) defined as in ([23](#S3.E23 "In Corollary 3.4. â€£ 3.1 Error bounds and convergence for predictable strategies â€£ 3 Adaptative algorithm â€£ Computation of Expected Shortfall by fast detection of worst scenarios")) but in terms of â„‘â„“âˆ’1k,jsuperscriptsubscriptâ„‘â„“1

ğ‘˜ğ‘—{\mathfrak{I}}\_{\ell-1}^{k,j} in place of â„‘â„“âˆ’1Î±superscriptsubscriptâ„‘â„“1ğ›¼{\mathfrak{I}}\_{\ell-1}^{\alpha}.
Given an integer râ‰¥1ğ‘Ÿ1r\geq 1, we first compute xË‡Lâˆ’1subscriptË‡xğ¿1\check{\rm x}\_{L-1} as the argmin over xâˆˆXxX{\rm x}\in{\rm X} of

|  |  |  |
| --- | --- | --- |
|  | âˆ‘k=1kÂ¯âˆ‘j=1jÂ¯|ğ”¼Lâˆ’1Î½Lâˆ’1k,jâ€‹[|1nwâ€‹âˆ‘iâˆˆâ„‘Lâˆ’1k,j(Î¼^Lk,j)iâˆ’Î¼~i|p]âˆ’Ï•xâ€‹(0,Î´â€‹NLÎ±k,qLâˆ’1Î±k,NLâˆ’1Î±k,CLâˆ’1Î±k,Î¼^Lâˆ’1k,j,ğš™Lâˆ’1k,j)|rsuperscriptsubscriptğ‘˜1Â¯ğ‘˜superscriptsubscriptğ‘—1Â¯ğ‘—superscriptsubscriptsuperscriptğ”¼subscriptsuperscriptğœˆ  ğ‘˜ğ‘—ğ¿1ğ¿1delimited-[]superscript1subscriptğ‘›ğ‘¤subscriptğ‘–superscriptsubscriptâ„‘ğ¿1  ğ‘˜ğ‘—superscriptsuperscriptsubscript^ğœ‡ğ¿  ğ‘˜ğ‘—ğ‘–superscript~ğœ‡ğ‘–ğ‘subscriptitalic-Ï•x0ğ›¿subscriptsuperscriptğ‘superscriptğ›¼ğ‘˜ğ¿subscriptsuperscriptğ‘superscriptğ›¼ğ‘˜ğ¿1subscriptsuperscriptğ‘superscriptğ›¼ğ‘˜ğ¿1subscriptsuperscriptğ¶superscriptğ›¼ğ‘˜ğ¿1subscriptsuperscript^ğœ‡  ğ‘˜ğ‘—ğ¿1subscriptsuperscriptğš™  ğ‘˜ğ‘—ğ¿1ğ‘Ÿ\sum\_{k=1}^{\bar{k}}\sum\_{j=1}^{\bar{j}}\left|{\mathbb{E}}^{\nu^{k,j}\_{L-1}}\_{L-1}\left[\left|\frac{1}{n\_{w}}\sum\_{i\in{\mathfrak{I}}\_{L-1}^{k,j}}(\hat{\mu}\_{L}^{{k},j})^{i}-\tilde{\mu}^{i}\right|^{p}\right]-\phi\_{{\rm x}}(0,{\delta N^{\alpha^{k}}\_{L}},q^{\alpha^{k}}\_{L-1},N^{\alpha^{k}}\_{L-1},C^{\alpha^{k}}\_{L-1},\hat{\mu}^{k,j}\_{L-1},\mathtt{p}^{k,j}\_{L-1})\right|^{r} |  |

in which ğ”¼Lâˆ’1Î½Lâˆ’1k,jsubscriptsuperscriptğ”¼subscriptsuperscriptğœˆ

ğ‘˜ğ‘—ğ¿1ğ¿1{\mathbb{E}}^{\nu^{k,j}\_{L-1}}\_{L-1} means that the expectation is taken only over Î¼~~ğœ‡\tilde{\mu} according to the law Î½Lâˆ’1k,jsubscriptsuperscriptğœˆ

ğ‘˜ğ‘—ğ¿1\nu^{k,j}\_{L-1}, i.e.Â ğ’©â€‹ğ’²âˆ’1â€‹(ğš™Lâˆ’1k,j)ğ’©superscriptğ’²1subscriptsuperscriptğš™

ğ‘˜ğ‘—ğ¿1{\cal NW}^{-1}(\mathtt{p}^{k,j}\_{{L-1}}), and (â‹…)isuperscriptâ‹…ğ‘–(\cdot)^{i} means that we take the iğ‘–i-th component of the vector in the brackets.
Then, for any Î±âˆˆğ’œadğ›¼superscriptğ’œad\alpha\in{\mathcal{A}}^{\rm ad},
we set

|  |  |  |
| --- | --- | --- |
|  | Ï•Ë‡Lâˆ’1â€‹(qLâˆ’1Î±,NLâˆ’1Î±,CLâˆ’1Î±,â‹…):=min(0,Î´â€‹N)âˆˆAâ€‹(Lâˆ’1,Î±)â¡Ï•xË‡Lâˆ’1â€‹(0,Î´â€‹N,qLâˆ’1Î±,NLâˆ’1Î±,CLâˆ’1Î±,â‹…),assignsubscriptË‡italic-Ï•ğ¿1subscriptsuperscriptğ‘ğ›¼ğ¿1subscriptsuperscriptğ‘ğ›¼ğ¿1subscriptsuperscriptğ¶ğ›¼ğ¿1â‹…subscript0ğ›¿ğ‘Ağ¿1ğ›¼subscriptitalic-Ï•subscriptË‡xğ¿10ğ›¿ğ‘subscriptsuperscriptğ‘ğ›¼ğ¿1subscriptsuperscriptğ‘ğ›¼ğ¿1subscriptsuperscriptğ¶ğ›¼ğ¿1â‹…\check{\phi}\_{L-1}(q^{\alpha}\_{L-1},N^{\alpha}\_{L-1},C^{\alpha}\_{L-1},\cdot):=\min\_{{\left(0,\delta N\right)}\in{\rm A}(L-1,\alpha)}\phi\_{\check{\rm x}\_{L-1}}(0,\delta N,q^{\alpha}\_{L-1},N^{\alpha}\_{L-1},{C^{\alpha}\_{L-1}},\cdot), |  |

where

|  |  |  |
| --- | --- | --- |
|  | Aâ€‹(Lâˆ’1,Î±):={(Î´â€‹q,Î´â€‹N)âˆˆ{0}Ã—â„•:CLâˆ’1Î±+nwâ€‹Î´â€‹Nâ‰¤K}.assignAğ¿1ğ›¼conditional-setğ›¿ğ‘ğ›¿ğ‘0â„•subscriptsuperscriptğ¶ğ›¼ğ¿1subscriptğ‘›ğ‘¤ğ›¿ğ‘ğ¾{\rm A}(L-1,\alpha):=\{(\delta q,\delta N)\in\{0\}\times{\mathbb{N}}:C^{\alpha}\_{L-1}+n\_{w}\delta N\leq K\}. |  |

Given Ï•Ë‡â„“+1subscriptË‡italic-Ï•â„“1\check{\phi}\_{\ell+1} for some â„“â‰¤Lâˆ’2â„“ğ¿2\ell\leq L-2, we then compute a minimizer xË‡â„“âˆˆXsubscriptË‡xâ„“X\check{\rm x}\_{\ell}\in{\rm X} of

|  |  |  |
| --- | --- | --- |
|  | âˆ‘k=1kÂ¯âˆ‘j=1jÂ¯|ğ”¼â„“Î½â„“k,j[Ï•Ë‡â„“+1(qâ„“+1Î±k,Nâ„“+1Î±k,Câ„“+1Î±k,Î¼^â„“+1k,ğš™â„“+1k)+fpad,k,j(â„“+1,Î±k,Î¸~)]âˆ’Ï•x(Î´qâ„“+1Î±k,Î´Nâ„“+1Î±k,qâ„“Î±k,Nâ„“Î±k,Câ„“Î±k,Î¼^â„“k,j,ğš™â„“k,j)|r,superscriptsubscriptğ‘˜1Â¯ğ‘˜superscriptsubscriptğ‘—1Â¯ğ‘—superscriptsubscriptsuperscriptğ”¼subscriptsuperscriptğœˆ  ğ‘˜ğ‘—â„“â„“delimited-[]subscriptË‡italic-Ï•â„“1subscriptsuperscriptğ‘superscriptğ›¼ğ‘˜â„“1subscriptsuperscriptğ‘superscriptğ›¼ğ‘˜â„“1subscriptsuperscriptğ¶superscriptğ›¼ğ‘˜â„“1subscriptsuperscript^ğœ‡ğ‘˜â„“1subscriptsuperscriptğš™ğ‘˜â„“1subscriptsuperscriptğ‘“  adğ‘˜ğ‘—ğ‘â„“1superscriptğ›¼ğ‘˜~ğœƒsubscriptitalic-Ï•xğ›¿subscriptsuperscriptğ‘superscriptğ›¼ğ‘˜â„“1ğ›¿subscriptsuperscriptğ‘superscriptğ›¼ğ‘˜â„“1subscriptsuperscriptğ‘superscriptğ›¼ğ‘˜â„“subscriptsuperscriptğ‘superscriptğ›¼ğ‘˜â„“subscriptsuperscriptğ¶superscriptğ›¼ğ‘˜â„“subscriptsuperscript^ğœ‡  ğ‘˜ğ‘—â„“subscriptsuperscriptğš™  ğ‘˜ğ‘—â„“ğ‘Ÿ\begin{split}\sum\_{k=1}^{\bar{k}}\sum\_{j=1}^{\bar{j}}&\bigg{|}{\mathbb{E}}^{\nu^{k,j}\_{\ell}}\_{\ell}\left[\check{\phi}\_{\ell+1}(q^{\alpha^{k}}\_{\ell+1},N^{\alpha^{k}}\_{\ell+1},C^{\alpha^{k}}\_{\ell+1},\hat{\mu}^{k}\_{\ell+1},\mathtt{p}^{k}\_{\ell+1})+f^{{\rm ad},k,j}\_{p}(\ell+1,\alpha^{k},\tilde{\theta})\right]\\ &-\phi\_{{\rm x}}(\delta q^{\alpha^{k}}\_{\ell+1},\delta N^{\alpha^{k}}\_{\ell+1},q^{\alpha^{k}}\_{\ell},N^{\alpha^{k}}\_{\ell},C^{\alpha^{k}}\_{\ell},\hat{\mu}^{k,j}\_{\ell},\mathtt{p}^{k,j}\_{\ell})\bigg{|}^{r},\end{split} |  |

where ğ”¼â„“Î½â„“k,jsubscriptsuperscriptğ”¼subscriptsuperscriptğœˆ

ğ‘˜ğ‘—â„“â„“{\mathbb{E}}^{\nu^{k,j}\_{\ell}}\_{\ell} means that the expectation is computed over (Î¼^â„“+1Î±k,ğš™â„“+1Î±k,Î¸~,ğ”ª~â„“Î±k)subscriptsuperscript^ğœ‡superscriptğ›¼ğ‘˜â„“1subscriptsuperscriptğš™superscriptğ›¼ğ‘˜â„“1~ğœƒsubscriptsuperscript~ğ”ªsuperscriptğ›¼ğ‘˜â„“(\hat{\mu}^{\alpha^{k}}\_{\ell+1},\mathtt{p}^{\alpha^{k}}\_{\ell+1},\tilde{\theta},\tilde{\mathfrak{m}}^{\alpha^{k}}\_{\ell}) given (Î¼^â„“k,ğš™â„“k)=(Î¼^â„“k,j,ğš™â„“k,j)subscriptsuperscript^ğœ‡ğ‘˜â„“subscriptsuperscriptğš™ğ‘˜â„“subscriptsuperscript^ğœ‡

ğ‘˜ğ‘—â„“subscriptsuperscriptğš™

ğ‘˜ğ‘—â„“(\hat{\mu}^{k}\_{\ell},\mathtt{p}^{k}\_{\ell})=(\hat{\mu}^{k,j}\_{\ell},\mathtt{p}^{k,j}\_{\ell}) and using the prior Î½â„“k,jsubscriptsuperscriptğœˆ

ğ‘˜ğ‘—â„“\nu^{k,j}\_{\ell} on Î¸~~ğœƒ\tilde{\theta} associated to ğš™â„“k,jsubscriptsuperscriptğš™

ğ‘˜ğ‘—â„“\mathtt{p}^{k,j}\_{\ell}. Then, we set

|  |  |  |
| --- | --- | --- |
|  | Ï•Ë‡â„“â€‹(qâ„“Î±,Nâ„“Î±,Câ„“Î±,â‹…):=min(Î´â€‹q,Î´â€‹N)âˆˆAâ€‹(â„“,Î±)â¡Ï•xË‡â„“â€‹(Î´â€‹q,Î´â€‹N,qâ„“Î±,Nâ„“Î±,Câ„“Î±,â‹…),assignsubscriptË‡italic-Ï•â„“subscriptsuperscriptğ‘ğ›¼â„“subscriptsuperscriptğ‘ğ›¼â„“subscriptsuperscriptğ¶ğ›¼â„“â‹…subscriptğ›¿ğ‘ğ›¿ğ‘Aâ„“ğ›¼subscriptitalic-Ï•subscriptË‡xâ„“ğ›¿ğ‘ğ›¿ğ‘subscriptsuperscriptğ‘ğ›¼â„“subscriptsuperscriptğ‘ğ›¼â„“subscriptsuperscriptğ¶ğ›¼â„“â‹…\check{\phi}\_{\ell}(q^{\alpha}\_{\ell},N^{\alpha}\_{\ell},C^{\alpha}\_{\ell},\cdot):=\min\_{{\left(\delta q,\delta N\right)}\in{\rm A}(\ell,\alpha)}\phi\_{\check{\rm x}\_{\ell}}(\delta q,\delta N,q^{\alpha}\_{\ell},N^{\alpha}\_{\ell},{C^{\alpha}\_{\ell}},\cdot), |  |

where

|  |  |  |  |
| --- | --- | --- | --- |
|  | Aâ€‹(â„“,Î±)Aâ„“ğ›¼\displaystyle{\rm A}(\ell,\alpha) | :={(Î´â€‹q,Î´â€‹N)âˆˆ[[0,qâ„“Î±âˆ’nw]]Ã—â„•:Câ„“Î±+(qâ„“Î±âˆ’Î´â€‹q)â€‹Î´â€‹Nâ‰¤K},â„“<Lâˆ’2,formulae-sequenceassignabsentconditional-setğ›¿ğ‘ğ›¿ğ‘delimited-[]0subscriptsuperscriptğ‘ğ›¼â„“subscriptğ‘›ğ‘¤â„•subscriptsuperscriptğ¶ğ›¼â„“subscriptsuperscriptğ‘ğ›¼â„“ğ›¿ğ‘ğ›¿ğ‘ğ¾â„“ğ¿2\displaystyle:=\{(\delta q,\delta N)\in[\![{0},q^{\alpha}\_{\ell}-{n\_{w}}]\!]\times{\mathbb{N}}:{C^{\alpha}\_{\ell}}+{(q^{\alpha}\_{\ell}-\delta q)}\delta N\leq K\},\;\ell<L-2, |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | Aâ€‹(Lâˆ’2,Î±)Ağ¿2ğ›¼\displaystyle{\rm A}(L-2,\alpha) | :={(Î´â€‹q,Î´â€‹N)âˆˆ{qâ„“Î±âˆ’nw}Ã—â„•:CLâˆ’1Î±+(qLâˆ’2Î±âˆ’Î´â€‹q)â€‹Î´â€‹Nâ‰¤K},assignabsentconditional-setğ›¿ğ‘ğ›¿ğ‘subscriptsuperscriptğ‘ğ›¼â„“subscriptğ‘›ğ‘¤â„•subscriptsuperscriptğ¶ğ›¼ğ¿1subscriptsuperscriptğ‘ğ›¼ğ¿2ğ›¿ğ‘ğ›¿ğ‘ğ¾\displaystyle:=\{(\delta q,\delta N)\in\{q^{\alpha}\_{\ell}-n\_{w}\}\times{\mathbb{N}}:{C^{\alpha}\_{L-1}}+{(q^{\alpha}\_{L-2}-\delta q)}\delta N\leq K\}, |  |

and so on until obtaining Ï•0â€‹(ns,0,0,0,ğš™0)subscriptitalic-Ï•0subscriptğ‘›ğ‘ 000subscriptğš™0\phi\_{{0}}({n\_{s},0},0,0,\mathtt{p}\_{0}). By continuity of Ï•â‹…â€‹(â‹…)subscriptitalic-Ï•â‹…â‹…\phi\_{\cdot}(\cdot) and compactness of XX{\rm X} and Aâ€‹(â„“,Î±)Aâ„“ğ›¼{\rm A}(\ell,\alpha) for Î±ğ›¼\alpha given, the minimum is achieved in the above, possibly not unique, and one can choose a measurable map aâ„“â‹†subscriptsuperscriptaâ‹†â„“{\rm a}^{\star}\_{{\ell}} such that

|  |  |  |
| --- | --- | --- |
|  | aâ„“â‹†â€‹(qâ„“Î±,Nâ„“Î±,Câ„“Î±,â‹…)âˆˆargâ€‹min(Î´â€‹q,Î´â€‹N)âˆˆAâ€‹(â„“,Î±)â¡Ï•xË‡â„“Nâ€‹(Î´â€‹q,Î´â€‹N,qâ„“Î±,Nâ„“Î±,Câ„“Î±,â‹…)subscriptsuperscriptaâ‹†â„“subscriptsuperscriptğ‘ğ›¼â„“subscriptsuperscriptğ‘ğ›¼â„“subscriptsuperscriptğ¶ğ›¼â„“â‹…argsubscriptğ›¿ğ‘ğ›¿ğ‘Aâ„“ğ›¼subscriptitalic-Ï•subscriptsuperscriptË‡ğ‘¥ğ‘â„“ğ›¿ğ‘ğ›¿ğ‘subscriptsuperscriptğ‘ğ›¼â„“subscriptsuperscriptğ‘ğ›¼â„“subscriptsuperscriptğ¶ğ›¼â„“â‹…{\rm a}^{\star}\_{{\ell}}(q^{\alpha}\_{\ell},N^{\alpha}\_{\ell},C^{\alpha}\_{\ell},\cdot)\in\mbox{arg}\min\_{{\left(\delta q,\delta N\right)}\in{\rm A}(\ell,\alpha)}\phi\_{\check{x}^{N}\_{\ell}}(\delta q,\delta N,q^{\alpha}\_{\ell},N^{\alpha}\_{\ell},{C^{\alpha}\_{\ell}},\cdot) |  |

for all Î±âˆˆğ’œadğ›¼superscriptğ’œad\alpha\in{\mathcal{A}}^{\rm ad}. Then, given the parameter ğš™0subscriptğš™0\mathtt{p}\_{0} of our initial prior Î½0subscriptğœˆ0\nu\_{0}, our estimator of the optimal policy is given by Î±â‹†=(qâ‹†,Nâ‹†)superscriptğ›¼â‹†superscriptğ‘â‹†superscriptğ‘â‹†\alpha^{\star}=(q^{\star},N^{\star}) defined by induction by

|  |  |  |  |
| --- | --- | --- | --- |
|  | (Î´â€‹q1â‹†,Î´â€‹N1â‹†)ğ›¿subscriptsuperscriptğ‘â‹†1ğ›¿subscriptsuperscriptğ‘â‹†1\displaystyle(\delta q^{\star}\_{1},\delta N^{\star}\_{1}) | =a0â‹†â€‹(ns,0,0,0,ğš™0)â€‹Â andÂ â€‹(Î´â€‹qâ„“+1â‹†,Î´â€‹Nâ„“+1â‹†)=aâ„“â‹†â€‹(qâ„“â‹†,Nâ„“â‹†,Câ„“â‹†,Î¼^â„“Î±â‹†,ğš™â„“Î±â‹†)â€‹Â forÂ â€‹0<â„“<L.absentsubscriptsuperscriptaâ‹†0subscriptğ‘›ğ‘ 000subscriptğš™0Â andÂ ğ›¿subscriptsuperscriptğ‘â‹†â„“1ğ›¿subscriptsuperscriptğ‘â‹†â„“1subscriptsuperscriptaâ‹†â„“subscriptsuperscriptğ‘â‹†â„“subscriptsuperscriptğ‘â‹†â„“subscriptsuperscriptğ¶â‹†â„“subscriptsuperscript^ğœ‡superscriptğ›¼â‹†â„“subscriptsuperscriptğš™superscriptğ›¼â‹†â„“Â forÂ 0â„“ğ¿\displaystyle={\rm a}^{\star}\_{{0}}(n\_{s},0,0,0,\mathtt{p}\_{0})\;\mbox{ and }\;(\delta q^{\star}\_{\ell+1},\delta N^{\star}\_{\ell+1})={\rm a}^{\star}\_{{\ell}}(q^{\star}\_{\ell},N^{\star}\_{\ell},C^{\star}\_{\ell},\hat{\mu}^{\alpha^{\star}}\_{\ell},\mathtt{p}^{\alpha^{\star}}\_{\ell})\;\mbox{ for }0<\ell<L. |  |

Note that the above algorithm for the estimation of the optimal control only requires off-line simulations according to the initial prior Î½0subscriptğœˆ0\nu\_{0}. It is certainly costly but does not require to evaluate the real financial book, it can be trained on a proxy, and can be done off-line. It can be combined with the approach of Remark [3.6](#S3.Thmtheorem6 "Remark 3.6. â€£ 3.2 A generic progressive learning algorithm â€£ 3 Adaptative algorithm â€£ Computation of Expected Shortfall by fast detection of worst scenarios") to reduce the computation time. In order to prepare for the use of a different initial prior, one can also slightly adapt the above algorithm by considering different initial values of ğš™0subscriptğš™0\mathtt{p}\_{0} (e.g.Â drawn from another distribution around ğš™0subscriptğš™0\mathtt{p}\_{0}), so as to estimate Ï•Ë‡0subscriptË‡italic-Ï•0\check{\phi}\_{0} not only at the point ğš™0subscriptğš™0\mathtt{p}\_{0}. When applied to the real book, the update of the prior according to ([29](#S3.E29 "In Remark 3.7. â€£ 3.2 A generic progressive learning algorithm â€£ 3 Adaptative algorithm â€£ Computation of Expected Shortfall by fast detection of worst scenarios")) leads to an additional cost that is negligible with respect to the simulation of the book. It leads to the computation of new priors associated to the financial book at hand, that can be used for a new estimation of the optimal policy or simply as a new initial prior for the next computation of the ESES{\rm ES}.

An example of a simple practical implementation is detailed in Appendix [B](#A2 "Appendix B Precise implementation of the neural network algorithm â€£ Computation of Expected Shortfall by fast detection of worst scenarios"), while numerical tests are performed in Section [4](#S4 "4 Numerical Experiments â€£ Computation of Expected Shortfall by fast detection of worst scenarios").

## 4 Numerical Experiments

This section is dedicated to first numerical tests of the different algorithms presented in the previous sections. The settings of the experiments are as follows. We first choose a Normal-inverse-Wishart prior distribution Î½0subscriptğœˆ0\nu\_{0} with parameters ğš™0:=(ğš–0,ğš”0,ğš’0,Î£0)assignsubscriptğš™0subscriptğš–0subscriptğš”0subscriptğš’0subscriptÎ£0\mathtt{p}\_{0}:=({\mathtt{m}}\_{0},{\mathtt{k}}\_{0},{\mathtt{i}}\_{0},\Sigma\_{0}). The vector ğš–0subscriptğš–0{\mathtt{m}}\_{0} is represented on Figure [6](#S2.F6 "Figure 6 â€£ 2.5 Simplified 2-levels algorithm for a linear indifference zoneâ€™s size â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios") with ğš–0i=Î¼isuperscriptsubscriptğš–0ğ‘–superscriptğœ‡ğ‘–{\mathtt{m}}\_{0}^{i}=\mu^{i}, iâ‰¤nsğ‘–subscriptğ‘›ğ‘ i\leq n\_{s}, and Î£0=(ğš’0âˆ’nsâˆ’1)â€‹Î£subscriptÎ£0subscriptğš’0subscriptğ‘›ğ‘ 1Î£\Sigma\_{0}=({\mathtt{i}}\_{0}-n\_{s}-1)\Sigma where Î£Î£\Sigma has entries

|  |  |  |  |
| --- | --- | --- | --- |
|  | {Î£iâ€‹i=4.84Ã—1012â€‹Â ifÂ â€‹i=jÎ£iâ€‹j=ÏÃ—4.84Ã—1012â€‹Â ifÂ â€‹iâ‰ j,casessuperscriptÎ£ğ‘–ğ‘–4.84superscript1012Â ifÂ ğ‘–ğ‘—otherwisesuperscriptÎ£ğ‘–ğ‘—ğœŒ4.84superscript1012Â ifÂ ğ‘–ğ‘—otherwise\begin{cases}\Sigma^{ii}=4.84\times 10^{12}\textnormal{ if }i=j\\ \Sigma^{ij}=\rho\times 4.84\times 10^{12}\textnormal{ if }i\neq j,\end{cases} |  | (30) |

with Ï=0.6ğœŒ0.6\rho=0.6 or Ï=0ğœŒ0\rho=0 depending on the experiments below.
As for ğš”0subscriptğš”0{\mathtt{k}}\_{0} and ğš’0subscriptğš’0{\mathtt{i}}\_{0}, they are chosen equal to 300, meaning that we have a low confidence in our prior. The computing power is K=107ğ¾superscript107K=10^{7}.

We apply the four different algorithms on 5â€‰000 runs (i.e.Â 5â€‰000 independent implementations of each algorithm). For each run, we

* â€¢

  first simulate a value for the real scenarios and covariance matrices (Î¼~,Î£~)âˆ¼ğ’©â€‹ğ’²âˆ’1â€‹(ğš™0)similar-to~ğœ‡~Î£ğ’©superscriptğ’²1subscriptğš™0\left(\tilde{\mu},\tilde{\Sigma}\right)\sim\mathcal{NW}^{-1}(\mathtt{p}\_{0}),
* â€¢

  apply each of the four algorithms, with simulated prices following P|sâˆ¼ğ’©â€‹(Î¼~,Î£~)P\_{{|{\rm s}}}\sim{\mathcal{N}}\left(\tilde{\mu},{\tilde{\Sigma}}\right),
* â€¢

  for each algorithm, we measure the relative error Eâ€‹S^âˆ’ES~ES~^ğ¸ğ‘†~ES~ES\frac{\widehat{ES}-\widetilde{\rm ES}}{\widetilde{\rm ES}} and the error ES^âˆ’ES~^ES~ES\widehat{\rm ES}-\widetilde{\rm ES}, where ES~=1nwâ€‹âˆ‘i=1nwÎ¼~ğ”ª~â€‹(i)~ES1subscriptğ‘›ğ‘¤superscriptsubscriptğ‘–1subscriptğ‘›ğ‘¤superscript~ğœ‡~ğ”ªğ‘–\widetilde{\rm ES}=\frac{1}{n\_{w}}\sum\_{i=1}^{n\_{w}}\tilde{\mu}^{\tilde{\mathfrak{m}}(i)}.

The four algorithms that we compare are:

* â€¢

  A Uniform Pricing Algorithm: All the scenarios are priced with K/nsğ¾subscriptğ‘›ğ‘ K/n\_{s} Monte Carlo simulations, and the estimator ES^^ES\widehat{\rm ES} is the average of the nw=6subscriptğ‘›ğ‘¤6n\_{w}=6 worst scenarios. This is the most naive method, with only one step and where all scenarios are priced with an equal number of Monte Carlo simulations. It serves as a benchmark.
* â€¢

  The Heuristic Algorithm: We use the 2-levels strategy described in Section [2.5](#S2.SS5 "2.5 Simplified 2-levels algorithm for a linear indifference zoneâ€™s size â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios") with the book sample parameters of Table [1](#S2.T1 "Table 1 â€£ 2.5 Simplified 2-levels algorithm for a linear indifference zoneâ€™s size â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios") and the computation parameters of Table [2](#S2.T2 "Table 2 â€£ 2.5 Simplified 2-levels algorithm for a linear indifference zoneâ€™s size â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios"). We do not evaluate the constant cğ‘c of Assumption [1](#Thmassumption1 "Assumption 1. â€£ 2.3 Error bound for Sub-Gamma distributions â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios") but simply set it to 00, see Remark [2.4](#S2.Thmtheorem4 "Remark 2.4. â€£ 2.3 Error bound for Sub-Gamma distributions â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios"). The optimal strategy is given by (q0,q1,N1,N2)=(253,68,17â€‰297,100â€‰000)subscriptğ‘0subscriptğ‘1subscriptğ‘1subscriptğ‘22536817297100000\left(q\_{0},q\_{1},N\_{1},N\_{2}\right)=(253,68,17\,297,100\,000).
* â€¢

  The Deterministic Algorithm: We run the deterministic algorithm of Section [2.4](#S2.SS4 "2.4 Optimal a-priori allocation by deterministic dynamic programming based on fixed a-priori bounds â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios") optimized with Î¼=ğš–0ğœ‡subscriptğš–0\mu={\mathtt{m}}\_{0} as the values of the scenarios, Î£Î£\Sigma with Ï=0.6ğœŒ0.6\rho=0.6 as the covariance matrix and L=4ğ¿4L=4. Note that using the real mean parameter as an entry for optimization is quite favorable for this algorithm, although the â€œtrueâ€ parameter of each run will actually deviate from this mean value. This gives us the strategy (q0,q1,q2,q3,N0,N1,N2,N3,N4)=(253,35,10,6,0,6â€‰000,44â€‰000,44â€‰000,1â€‰235â€‰666)subscriptğ‘0subscriptğ‘1subscriptğ‘2subscriptğ‘3subscriptğ‘0subscriptğ‘1subscriptğ‘2subscriptğ‘3subscriptğ‘4253351060600044000440001235666\left(q\_{0},q\_{1},q\_{2},q\_{3},N\_{0},N\_{1},N\_{2},N\_{3},N\_{4}\right)=\left(253,35,10,6,0,6\,000,44\,000,44\,000,1\,235\,666\right), which we apply to each run.
* â€¢

  The Adaptative Algorithm: We do the training part of the adaptative algorithm using our prior ğš™0:=(ğš–0,ğš”0,ğš’0,Î£0)assignsubscriptğš™0subscriptğš–0subscriptğš”0subscriptğš’0subscriptÎ£0\mathtt{p}\_{0}:=({\mathtt{m}}\_{0},{\mathtt{k}}\_{0},{\mathtt{i}}\_{0},\Sigma\_{0}), with Ï=0.6ğœŒ0.6\rho=0.6, as parameters and L=4ğ¿4L=4. We use a very simple one hidden-layer neural network. It could certainly be improved by using a more sophisticated multi-layers neural network, but this version will be enough for our discussion. Details on the implementation are given in the Appendix [B](#A2 "Appendix B Precise implementation of the neural network algorithm â€£ Computation of Expected Shortfall by fast detection of worst scenarios").
  Once this is done, we apply the optimal adaptative strategy on each run.

### 4.1 Positively correlated scenarios Ï=0.6ğœŒ0.6\rho=0.6

In this first experiment, the simulated runs use the values Ï=0.6ğœŒ0.6\rho=0.6 and ğš’0=ğš”0=300subscriptğš’0subscriptğš”0300{\mathtt{i}}\_{0}={\mathtt{k}}\_{0}=300.

To get an idea of how much noise is added to the average scenario values in our simulations, we plot in Figure [7](#S4.F7 "Figure 7 â€£ 4.1 Positively correlated scenarios ğœŒ=0.6 â€£ 4 Numerical Experiments â€£ Computation of Expected Shortfall by fast detection of worst scenarios") the prior value ğš–0isuperscriptsubscriptğš–0ğ‘–{\mathtt{m}}\_{0}^{i} for each scenario of index iâ‰¤nsğ‘–subscriptğ‘›ğ‘ i\leq n\_{s} (this is the line) and the first 202020 Î¼~jisubscriptsuperscript~ğœ‡ğ‘–ğ‘—\tilde{\mu}^{i}\_{j} out of the 5â€‰00050005\,000 runs for each scenario (these are the points).

![Refer to caption](/html/2005.12593/assets/scenario_values_graph.png)


Figure 7: True value of Î¼âˆ˜subscriptğœ‡\mu\_{\circ} and simulations of Î¼~~ğœ‡\tilde{\mu}

For the adaptative algorithm, the three mostly used strategies are:

* â€¢

  (q0,q1,q2,q3,N1,N2,N3,N4)=(253,40,25,6,8â€‰399,97â€‰995,172â€‰504,577â€‰252)subscriptğ‘0subscriptğ‘1subscriptğ‘2subscriptğ‘3subscriptğ‘1subscriptğ‘2subscriptğ‘3subscriptğ‘425340256839997995172504577252\left(q\_{0},q\_{1},q\_{2},q\_{3},N\_{1},N\_{2},N\_{3},N\_{4}\right)=\left(253,40,25,6,8\,399,97\,995,172\,504,577\,252\right)
* â€¢

  (q0,q1,q2,q3,N1,N2,N3,N4)=(253,40,30,6,8â€‰399,99â€‰733,148â€‰560,608â€‰040)subscriptğ‘0subscriptğ‘1subscriptğ‘2subscriptğ‘3subscriptğ‘1subscriptğ‘2subscriptğ‘3subscriptğ‘425340306839999733148560608040\left(q\_{0},q\_{1},q\_{2},q\_{3},N\_{1},N\_{2},N\_{3},N\_{4}\right)=\left(253,40,30,6,8\,399,99\,733,148\,560,608\,040\right)
* â€¢

  (q0,q1,q2,q3,N1,N2,N3,N4)=(253,40,30,6,8â€‰399,75â€‰033,123â€‰860,748â€‰007)subscriptğ‘0subscriptğ‘1subscriptğ‘2subscriptğ‘3subscriptğ‘1subscriptğ‘2subscriptğ‘3subscriptğ‘425340306839975033123860748007\left(q\_{0},q\_{1},q\_{2},q\_{3},N\_{1},N\_{2},N\_{3},N\_{4}\right)=\left(253,40,30,6,8\,399,75\,033,123\,860,748\,007\right)

Compared to the deterministic algorithm, we see that the adaptative one uses much less Monte Carlo simulations at the final steps and focuses more on the intermediate steps to select the worst scenarios. The deterministic algorithm is also more aggressive in the choice of q1subscriptğ‘1q\_{1} and q2subscriptğ‘2q\_{2}. This can be easily explained by the fact that the latter believes that the real distribution is not far from the solid curve on Figure [7](#S4.F7 "Figure 7 â€£ 4.1 Positively correlated scenarios ğœŒ=0.6 â€£ 4 Numerical Experiments â€£ Computation of Expected Shortfall by fast detection of worst scenarios") (up to standard deviation) while the adaptative one only knows a much more diffuse distribution corresponding to the cloud of points of Figure [7](#S4.F7 "Figure 7 â€£ 4.1 Positively correlated scenarios ğœŒ=0.6 â€£ 4 Numerical Experiments â€£ Computation of Expected Shortfall by fast detection of worst scenarios") since his level of uncertainty is quite high for our choice ğš’0=ğš”0=300subscriptğš’0subscriptğš”0300{\mathtt{i}}\_{0}={\mathtt{k}}\_{0}=300.

On Figures [11](#S4.F11 "Figure 11 â€£ 4.1 Positively correlated scenarios ğœŒ=0.6 â€£ 4 Numerical Experiments â€£ Computation of Expected Shortfall by fast detection of worst scenarios")-[11](#S4.F11 "Figure 11 â€£ 4.1 Positively correlated scenarios ğœŒ=0.6 â€£ 4 Numerical Experiments â€£ Computation of Expected Shortfall by fast detection of worst scenarios"), we plot the histograms of the relative errors. We see that the distribution is tightest for the deterministic algorithm, followed quite closely by the adaptative algorithm. Both of them perform very well. As expected, the uniform algorithm is very poor. Note that the heuristic one already very significantly improves the uniform algorithm, although it does not reach the precision of the two most sophisticated algorithms (without surprise). Because of the huge uncertainty mentioned above, the adaptative algorithm is rather conservative while the deterministic algorithm makes full profit of essentially knowing the correct distribution, and performs better. We will see in our second experiment that things will change when we will deviate from the parameters used for optimizing the deterministic algorithm (by simply passing from Ï=0.6ğœŒ0.6\rho=0.6 to Ï=0ğœŒ0\rho=0 in the simulated runs).

![Refer to caption](/html/2005.12593/assets/er_dist_ind_ad_rho_0-6.png)


Figure 8: Relative Error for Adaptative Algorithm

![Refer to caption](/html/2005.12593/assets/er_dist_ind_det_rho_0-6.png)


Figure 9: Relative Error for Determinist Algorithm

![Refer to caption](/html/2005.12593/assets/er_dist_ind_heur_1_rho_0-6.png)


Figure 10: Relative Error for Heuristic Algorithm

![Refer to caption](/html/2005.12593/assets/er_dist_ind_uni_rho_0-6.png)


Figure 11: Relative Error for Uniform Algorithm

In Table [3](#S4.T3 "Table 3 â€£ 4.1 Positively correlated scenarios ğœŒ=0.6 â€£ 4 Numerical Experiments â€£ Computation of Expected Shortfall by fast detection of worst scenarios"), we provide the ğ•ƒ1superscriptğ•ƒ1{\mathbb{L}}^{1} and relative errors (with standard deviations), the ğ•ƒ2superscriptğ•ƒ2{\mathbb{L}}^{2} error and the number of correct selections, that is the number of runs for which a given algorithm has chosen the correct worst 6 scenarios. In terms of ğ•ƒ1superscriptğ•ƒ1{\mathbb{L}}^{1} or ğ•ƒ2superscriptğ•ƒ2{\mathbb{L}}^{2} error, the relative performance of the algorithms is as above. However, if we look at the number of correct selections, we see that the adaptive algorithm performs better than the other 3 algorithms. Again, by comparing the strategies of the deterministic and the adaptive algorithms, we see that those of the adaptative algorithm are more conservative on the ranking and filtering part versus the final pricing as it puts relatively more Monte Carlo simulations to detect the correct scenarios and relatively less for their estimation.

| Algorithm | ğ•ƒ1superscriptğ•ƒ1{\mathbb{L}}^{1} Err. | ğ•ƒ1superscriptğ•ƒ1{\mathbb{L}}^{1} Err. Std | Rel. Err. (%) | Rel. Err. Std (%) | ğ•ƒ2superscriptğ•ƒ2{\mathbb{L}}^{2} Err. | Correct Selections |
| --- | --- | --- | --- | --- | --- | --- |
| Ad. Alg. | 1 891 | 20.4 | 0.623 | 0.00886 | 2377 | 4247 |
| Det. Alg. | 1 411 | 16.1 | 0.465 | 0.00693 | 1813 | 3499 |
| Heur.Â Alg. | 4 562 | 50.2 | 1.49 | 0.0234 | 5779 | 4054 |
| Unif. Alg. | 7 269 | 81.6 | 2.38 | 0.0348 | 9279 | 3500 |

Table 3: Errors for Ï=0.6ğœŒ0.6\rho=0.6

In Figures [12](#S4.F12.1 "Figure 12 â€£ 4.1 Positively correlated scenarios ğœŒ=0.6 â€£ 4 Numerical Experiments â€£ Computation of Expected Shortfall by fast detection of worst scenarios"), we plot the function xâ†¦â„™â€‹[X>5000âˆ’x]maps-toğ‘¥â„™delimited-[]ğ‘‹5000ğ‘¥x\mapsto\mathbb{P}[X>5000-x] where Xğ‘‹X is the absolute error of the algorithm on a run.

![Refer to caption](/html/2005.12593/assets/L1_all_cdf_06.png)


Figure 12: Tail Distribution of the errors. First top lines: Uniform and Heuristic algorithms, respectively. Solid line: Adaptative algorithm. Dotted line: Deterministic algorithm.

In Figure [13](#S4.F13 "Figure 13 â€£ 4.1 Positively correlated scenarios ğœŒ=0.6 â€£ 4 Numerical Experiments â€£ Computation of Expected Shortfall by fast detection of worst scenarios"), we provide, for the first 4 runs, the values and real ranks of the 6 worst scenarios selected by each algorithm. The numbers displayed are the true ranks of the selected scenarios given by Î¼~~ğœ‡\tilde{\mu} and their y-coordinate is the value obtained when running the algorithm. â€œRealâ€ is the real values as sampled.

![Refer to caption](/html/2005.12593/assets/numbers_graph.png)


Figure 13: Worst Scenarios Ranks and Values

### 4.2 Uncorrelated scenarios Ï=0ğœŒ0\rho=0

We now do the numerical test with Ï=0ğœŒ0\rho=0 as the true correlation. The deterministic and adaptative algorithm are still trained with Ï=0.6ğœŒ0.6\rho=0.6, but P|sP\_{|{\rm s}} is simulated using Ï=0ğœŒ0\rho=0.

On Figures [17](#S4.F17 "Figure 17 â€£ 4.2 Uncorrelated scenarios ğœŒ=0 â€£ 4 Numerical Experiments â€£ Computation of Expected Shortfall by fast detection of worst scenarios")-[17](#S4.F17 "Figure 17 â€£ 4.2 Uncorrelated scenarios ğœŒ=0 â€£ 4 Numerical Experiments â€£ Computation of Expected Shortfall by fast detection of worst scenarios"), we show the histograms of the relative errors. We see that the distribution of the relative errors is now tightest for the adaptative method, followed by the deterministic method, then by the heuristic and the uniform methods. Furthermore, we see that the distribution corresponding to the deterministic method is significantly biased to the left. This is actually true for all algorithms, but at a less significant level. This suggests that we now have a large part of the error that does not come from the final pricing error, but from errors in the selection of scenarios.

![Refer to caption](/html/2005.12593/assets/er_dist_ind_ad_rho_0.png)


Figure 14: Relative Error for Adaptative Algorithm

![Refer to caption](/html/2005.12593/assets/er_dist_ind_det_rho_0.png)


Figure 15: Relative Error for Determinist Algorithm

![Refer to caption](/html/2005.12593/assets/er_dist_ind_heur_1_rho_0.png)


Figure 16: Relative Error for Heuristic Algorithm

![Refer to caption](/html/2005.12593/assets/er_dist_ind_uni_rho_0.png)


Figure 17: Relative Error for Uniform Algorithm

In Table [4](#S4.T4 "Table 4 â€£ 4.2 Uncorrelated scenarios ğœŒ=0 â€£ 4 Numerical Experiments â€£ Computation of Expected Shortfall by fast detection of worst scenarios"), we provide the ğ•ƒ1superscriptğ•ƒ1{\mathbb{L}}^{1} and relative errors (with standard deviations), the ğ•ƒ2superscriptğ•ƒ2{\mathbb{L}}^{2} error and the number of correct selections for the 4 algorithms. For all algorithms, compared to the case Ï=0.6ğœŒ0.6\rho=0.6, we see that we have simultaneously a lower number of correct selections of scenarios (which we could expect to increase the errors) and a lower ğ•ƒ1superscriptğ•ƒ1{\mathbb{L}}^{1} error. This surprising result is explained by the fact that lowering the correlation has two effects. The filtering and ranking part of the algorithm becomes harder, as can be seen from Corollary [2.3](#S2.Thmtheorem3 "Corollary 2.3. â€£ 2.3 Error bound for Sub-Gamma distributions â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios"). This explains why the number of correct selections becomes lower. However, we compute at the end an average over the nwsubscriptğ‘›ğ‘¤n\_{w} worst scenarios and the error on this average is lower when the pricings are uncorrelated compared to the case where they exhibit a positive correlation.

The adaptative algorithm has now simultaneously the lowest ğ•ƒ1superscriptğ•ƒ1{\mathbb{L}}^{1} and ğ•ƒ2superscriptğ•ƒ2{\mathbb{L}}^{2} errors, as well as the highest number of correct selections. We see that it is especially good in ğ•ƒ2superscriptğ•ƒ2{\mathbb{L}}^{2} error, so we expect it to present a very low number of large errors. As, by construction, it has been trained to detect misspecifications of the parameters, it now has a clear advantage on the deterministic algorithm which does not see it. This results in an improvement of almost 20% of the ğ•ƒ2superscriptğ•ƒ2{\mathbb{L}}^{2} error.

Following the above reasoning, we understand that, compared to the previous experiment, the final pricing error now plays a smaller role and the ranking and selection error a bigger role, which explains why the histogram of the errors for the determinist algorithm is strongly biased to the left, as it now incorrectly selects scenarios more often.

| Algorithm | ğ•ƒ1superscriptğ•ƒ1{\mathbb{L}}^{1} Err. | ğ•ƒ1superscriptğ•ƒ1{\mathbb{L}}^{1} Err. Std | Rel. Err. (%) | Rel. Err. Std (%) | ğ•ƒ2superscriptğ•ƒ2{\mathbb{L}}^{2} Err. | Correct Selections |
| --- | --- | --- | --- | --- | --- | --- |
| Ad. Alg. | 1 083 | 11.8 | 0.27 | 0.00294 | 1 366 | 3 930 |
| Det. Alg. | 1 175 | 17.5 | 0.293 | 0.00448 | 1 705 | 3 202 |
| Heur. Alg. | 2 547 | 28.33 | 0.628 | 0.00700 | 3 240 | 3 753 |
| Unif. Alg. | 4 062 | 44.7 | 1.00 | 0.0111 | 5 147 | 3 102 |

Table 4: Errors for Ï=0ğœŒ0\rho=0

In Figures [18](#S4.F18.1 "Figure 18 â€£ 4.2 Uncorrelated scenarios ğœŒ=0 â€£ 4 Numerical Experiments â€£ Computation of Expected Shortfall by fast detection of worst scenarios"), we plot the function xâ†¦â„™â€‹[X>5000âˆ’x]maps-toğ‘¥â„™delimited-[]ğ‘‹5000ğ‘¥x\mapsto\mathbb{P}[X>5000-x] where Xğ‘‹X is the absolute error of the algorithm on a run. As was suggested by the ğ•ƒ2superscriptğ•ƒ2{\mathbb{L}}^{2} errors of Table [4](#S4.T4 "Table 4 â€£ 4.2 Uncorrelated scenarios ğœŒ=0 â€£ 4 Numerical Experiments â€£ Computation of Expected Shortfall by fast detection of worst scenarios"), we see that the tail distribution of errors is lowest for the adaptative algorithm, followed by the deterministic algorithm (for big errors), and then by the heuristic and uniform algorithms.

![Refer to caption](/html/2005.12593/assets/L1_all_cdf_0.png)


Figure 18: Tail Distribution of the errors. First top lines: Uniform and Heuristic algorithms, respectively. Solid line: Adaptative algorithm. Dotted line: Determinist algorithm

## 5 Conclusion

We propose in this paper different algorithms for the computation of the expected shortfall based on given historical scenarios. All are multi-steps algorithms that use Monte Carlo simulations to reduce the number of historical scenarios that potentially belong to the set of worst scenarios. We provide explicit error bounds and we test them on simulated data deviating from the true values of the historical impacts used for computing the associated optimal strategies. The first algorithm is a very easy to implement 222-steps algorithm that already provides relatively small errors on our numerical tests. A four step deterministic dynamic programming algorithm performs very well when real datas are not far from the parameters used in the optimization procedure. It seems even to be quite robust, as shown by our numerical test in the case where the true correlation parameter is not the one used for computing the optimal policy. Finally, we propose an adaptative algorithm that aims at learning the true value of the parameters at the different steps of the algorithm. Our first numerical tests suggest that it is more conservative than the deterministic one, but probably more robust to parameters misspecifications, as expected. The version we use is built on a very simple one hidden layer neural network and can certainly be considerably improved for industrial purposes.

## References

* [1]

  Carlo Acerbi and Dirk Tasche.
  On the coherence of expected shortfall.
  Journal of Banking & Finance, 26(7):1487â€“1503, 2002.
* [2]

  Philippe Artzner, Freddy Delbaen, Jean-Marc Eber, and David Heath.
  Coherent measures of risk.
  Mathematical finance, 9(3):203â€“228, 1999.
* [3]

  RaghuÂ Raj Bahadur and Herbert Robbins.
  The problem of the greater mean.
  The Annals of Mathematical Statistics, pages 469â€“487, 1950.
* [4]

  BaselÂ Committee onÂ BankingÂ Supervision.
  Minimum capital requirements for market risk.
  2016.
* [5]

  RobertÂ E Bechhofer.
  A single-sample multiple decision procedure for ranking means of
  normal populations with known variances.
  The Annals of Mathematical Statistics, pages 16â€“39, 1954.
* [6]

  RobertÂ E Bechhofer, CharlesÂ W Dunnett, and Milton Sobel.
  A tow-sample multiple decision procedure for ranking means of normal
  populations with a common unknown variance.
  Biometrika, 41(1-2):170â€“176, 1954.
* [7]

  Bernard Bercu, Bernard Delyon, and Emmanuel Rio.
  Concentration inequalities for sums and martingales.
  Springer, 2015.
* [8]

  Mark Broadie, Yiping Du, and CiamacÂ C Moallemi.
  Efficient risk estimation via nested sequential simulation.
  Management Science, 57(6):1172â€“1194, 2011.
* [9]

  SimonÂ A Broda, Jochen Krause, and MarcÂ S Paolella.
  Approximating expected shortfall for heavy-tailed distributions.
  Econometrics and statistics, 8:184â€“203, 2018.
* [10]

  David Easley and NicholasÂ M Kiefer.
  Controlling a stochastic process with unknown parameters.
  Econometrica: Journal of the Econometric Society, pages
  1045â€“1064, 1988.
* [11]

  RobertÂ J Elliott and Hong Miao.
  Var and expected shortfall: a non-normal regime switching framework.
  Quantitative Finance, 9(6):747â€“755, 2009.
* [12]

  Christian Francq and Jean-Michel ZakoÃ¯an.
  Multi-level conditional var estimation in dynamic models.
  In Modeling Dependence in Econometrics, pages 3â€“19. Springer,
  2014.
* [13]

  MichaelÂ B Gordy and Sandeep Juneja.
  Nested simulation in portfolio risk measurement.
  Management Science, 56(10):1833â€“1848, 2010.
* [14]

  ShantiÂ S Gupta and SÂ Panchapakesan.
  Sequential ranking and selection procedures.
  Handbook of sequential analysis, pages 363â€“380, 1991.
* [15]

  Lennart Hoogerheide and HermanÂ K van Dijk.
  Bayesian forecasting of value at risk and expected shortfall using
  adaptive importance sampling.
  International Journal of Forecasting, 26(2):231â€“247, 2010.
* [16]

  Jochen Krause and MarcÂ S Paolella.
  A fast, accurate method for value-at-risk and expected shortfall.
  Econometrics, 2(2):98â€“122, 2014.
* [17]

  Ming Liu and Jeremy Staum.
  Stochastic kriging for efficient nested simulation of expected
  shortfall.
  Journal of Risk, 12(3):3, 2010.
* [18]

  KevinÂ P Murphy.
  Conjugate bayesian analysis of the gaussian distribution.
  cs.ubc.ca/âˆ¼similar-to\simmurphyk/Papers/bayesGauss.pdf.
* [19]

  Saralees Nadarajah, BoÂ Zhang, and Stephen Chan.
  Estimation methods for expected shortfall.
  Quantitative Finance, 14(2):271â€“291, 2014.
* [20]

  Luis Ortiz-Gracia and CornelisÂ W Oosterlee.
  Efficient var and expected shortfall computations for nonlinear
  portfolios within the delta-gamma approach.
  Applied Mathematics and Computation, 244:16â€“31, 2014.
* [21]

  Franco Peracchi and AndreiÂ V Tanase.
  On estimating the conditional expected shortfall.
  Applied Stochastic Models in Business and Industry,
  24(5):471â€“493, 2008.
* [22]

  Jimmy Risk and Michael Ludkovski.
  Sequential design and spatial modeling for portfolio tail risk
  measurement.
  SIAM Journal on Financial Mathematics, 9(4):1137â€“1174, 2018.
* [23]

  RÂ Tyrrell Rockafellar and Stanislav Uryasev.
  Conditional value-at-risk for general loss distributions.
  Journal of banking & finance, 26(7):1443â€“1471, 2002.
* [24]

  JulesÂ Sadefo Kamdem.
  Value-at-risk and expected shortfall for linear portfolios with
  elliptically distributed risk factors.
  International Journal of Theoretical and Applied Finance,
  8(05):537â€“551, 2005.
* [25]

  Jean-Guy Simonato.
  The performance of johnson distributions for computingvalue at risk
  and expected shortfall.
  The Journal of Derivatives, 19(1):7â€“24, 2011.
* [26]

  Keming Yu, AÂ Allay, Shanchao Yang, and DÂ Hand.
  Kernel quantile-based estimation of expected shortfall.
  2010.
* [27]

  Meng-Lan Yueh and MarkÂ CW Wong.
  Analytical var and expected shortfall for quadratic portfolios.
  The Journal of Derivatives, 17(3):33â€“44, 2010.

## Appendix A Proxy of the optimal strategy for the heuristic ([21](#S2.E21 "In 2.5 Simplified 2-levels algorithm for a linear indifference zoneâ€™s size â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios"))

In the case p=1ğ‘1p=1, ([21](#S2.E21 "In 2.5 Simplified 2-levels algorithm for a linear indifference zoneâ€™s size â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios")) can even be further simplified by using the upper-bound

|  |  |  |  |
| --- | --- | --- | --- |
|  | h~01â€‹(q1)â‰¤maxâ¡{h~1â€‹(q1);h~2â€‹(q1)}subscriptsuperscript~â„10subscriptğ‘1subscript~â„1subscriptğ‘1subscript~â„2subscriptğ‘1\displaystyle\tilde{h}^{1}\_{0}(q\_{1})\leq\max\{\tilde{h}\_{1}(q\_{1});\tilde{h}\_{2}(q\_{1})\} |  | (31) |

where

|  |  |  |  |
| --- | --- | --- | --- |
|  | h~1â€‹(q1)subscript~â„1subscriptğ‘1\displaystyle\tilde{h}\_{1}(q\_{1}) | :=nsâ€‹(q1+1âˆ’nw)â€‹Î´0â€‹expâ¡(âˆ’(Kâˆ’q1â€‹N2)â€‹(q1+1âˆ’nw)â€‹Î´04â€‹nsâ€‹c)assignabsentsubscriptğ‘›ğ‘ subscriptğ‘11subscriptğ‘›ğ‘¤subscriptğ›¿0ğ¾subscriptğ‘1subscriptğ‘2subscriptğ‘11subscriptğ‘›ğ‘¤subscriptğ›¿04subscriptğ‘›ğ‘ ğ‘\displaystyle:=n\_{s}\left(q\_{1}+1-n\_{w}\right)\delta\_{0}\exp\left(-\frac{\left(K-q\_{1}N\_{2}\right)(q\_{1}+1-n\_{w})\delta\_{0}}{4n\_{s}c}\right) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | h~2â€‹(q1)subscript~â„2subscriptğ‘1\displaystyle\tilde{h}\_{2}(q\_{1}) | :=nsâ€‹(nsâˆ’nw)â€‹Î´0â€‹expâ¡(âˆ’(Kâˆ’q1â€‹N2)â€‹((q1+1âˆ’nw)â€‹Î´0)24â€‹nsâ€‹ÏƒÂ¯2).assignabsentsubscriptğ‘›ğ‘ subscriptğ‘›ğ‘ subscriptğ‘›ğ‘¤subscriptğ›¿0ğ¾subscriptğ‘1subscriptğ‘2superscriptsubscriptğ‘11subscriptğ‘›ğ‘¤subscriptğ›¿024subscriptğ‘›ğ‘ superscriptÂ¯ğœ2\displaystyle:=n\_{s}\left(n\_{s}-n\_{w}\right)\delta\_{0}\exp\left(-\frac{\left(K-q\_{1}N\_{2}\right)((q\_{1}+1-n\_{w})\delta\_{0})^{2}}{4n\_{s}\overline{\sigma}^{2}}\right). |  |

The right-hand side of ([31](#A1.E31 "In Appendix A Proxy of the optimal strategy for the heuristic (21) â€£ Computation of Expected Shortfall by fast detection of worst scenarios")) is now tractable for minimization.
Given,

|  |  |  |  |
| --- | --- | --- | --- |
|  | {Î”:=(Kâˆ’(nwâˆ’1)â€‹N2)2âˆ’32â€‹nsâ€‹N2â€‹cÎ´0B:=ÏƒÂ¯2câ€‹Î´0+nwâˆ’1q12,âˆ—:=maxâ¡(nwâˆ’13+2â€‹K3â€‹N2,nw)q11,1,âˆ—:=maxâ¡(3â€‹(nwâˆ’1)4+Kâˆ’Î”4â€‹N2,nw)q11,2,âˆ—:=maxâ¡(3â€‹(nwâˆ’1)4+K+Î”4â€‹N2,nw)casesassignÎ”superscriptğ¾subscriptğ‘›ğ‘¤1subscriptğ‘2232subscriptğ‘›ğ‘ subscriptğ‘2ğ‘subscriptğ›¿0otherwiseassignğµsuperscriptÂ¯ğœ2ğ‘subscriptğ›¿0subscriptğ‘›ğ‘¤1otherwiseassignsuperscriptsubscriptğ‘1  2subscriptğ‘›ğ‘¤132ğ¾3subscriptğ‘2subscriptğ‘›ğ‘¤otherwiseassignsuperscriptsubscriptğ‘1  113subscriptğ‘›ğ‘¤14ğ¾Î”4subscriptğ‘2subscriptğ‘›ğ‘¤otherwiseassignsuperscriptsubscriptğ‘1  123subscriptğ‘›ğ‘¤14ğ¾Î”4subscriptğ‘2subscriptğ‘›ğ‘¤otherwise\begin{cases}\Delta:=\left(K-\left(n\_{w}-1\right)N\_{2}\right)^{2}-\frac{32n\_{s}N\_{2}c}{\delta\_{0}}\\ B:=\frac{\bar{\sigma}^{2}}{c\delta\_{0}}+n\_{w}-1\\ q\_{1}^{2,\*}:=\max\left(\frac{n\_{w}-1}{3}+\frac{2K}{3N\_{2}},n\_{w}\right)\\ q\_{1}^{1,1,\*}:=\max\left(\frac{3\left(n\_{w}-1\right)}{4}+\frac{K-\sqrt{\Delta}}{4N\_{2}},n\_{w}\right)\\ q\_{1}^{1,2,\*}:=\max\left(\frac{3\left(n\_{w}-1\right)}{4}+\frac{K+\sqrt{\Delta}}{4N\_{2}},n\_{w}\right)\end{cases} |  | (32) |

the optimal policy q1hsubscriptsuperscriptğ‘â„1q^{h}\_{1} is defined by the following table131313We optimize here over real positive numbers.:

|  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- |
| Cond. on BğµB | Cond. Î”Î”\Delta | Cond. q12,âˆ—superscriptsubscriptğ‘1  2q\_{1}^{2,\*} | Cond. q11,1,âˆ—superscriptsubscriptğ‘1  11q\_{1}^{1,1,\*} | Cond. q11,2,âˆ—superscriptsubscriptğ‘1  12q\_{1}^{1,2,\*} | Choice of q1hsuperscriptsubscriptğ‘1â„q\_{1}^{h} |
| â‰¥nsabsentsubscriptğ‘›ğ‘ \geq n\_{s} |  |  |  |  | q1h:=q12,âˆ—assignsuperscriptsubscriptğ‘1â„superscriptsubscriptğ‘1  2q\_{1}^{h}:=q\_{1}^{2,\*} |
| â‰¤nwabsentsubscriptğ‘›ğ‘¤\leq n\_{w} | >0absent0>0 |  |  |  | q1h:=argminq1âˆˆ{nw,q11,2,âˆ—}â€‹h01â€‹(q1)assignsuperscriptsubscriptğ‘1â„subscriptğ‘1subscriptğ‘›ğ‘¤superscriptsubscriptğ‘1  12argminsuperscriptsubscriptâ„01subscriptğ‘1q\_{1}^{h}:=\underset{q\_{1}\in\{n\_{w},q\_{1}^{1,2,\*}\}}{\textnormal{argmin}}h\_{0}^{1}(q\_{1}) |
| â‰¤nwabsentsubscriptğ‘›ğ‘¤\leq n\_{w} | â‰¤0absent0\leq 0 |  |  |  | q1h:=nwassignsuperscriptsubscriptğ‘1â„subscriptğ‘›ğ‘¤q\_{1}^{h}:=n\_{w} |
| nw<â‹…<nsn\_{w}<\cdot<n\_{s} | >0absent0>0 | â‰¤Babsentğµ\leq B | â‰¤Babsentğµ\leq B | â‰¤Babsentğµ\leq B | q1h:=argminq1âˆˆ{q12,âˆ—,B}â€‹h01â€‹(q1)assignsuperscriptsubscriptğ‘1â„subscriptğ‘1superscriptsubscriptğ‘1  2ğµargminsuperscriptsubscriptâ„01subscriptğ‘1q\_{1}^{h}:=\underset{q\_{1}\in\{q\_{1}^{2,\*},B\}}{\textnormal{argmin}}h\_{0}^{1}(q\_{1}) |
| nw<â‹…<nsn\_{w}<\cdot<n\_{s} | >0absent0>0 | â‰¤Babsentğµ\leq B | â‰¤Babsentğµ\leq B | â‰¥Babsentğµ\geq B | q1h:=argminq1âˆˆ{q12,âˆ—,q11,2,âˆ—}â€‹h01â€‹(q1)assignsuperscriptsubscriptğ‘1â„subscriptğ‘1superscriptsubscriptğ‘1  2superscriptsubscriptğ‘1  12argminsuperscriptsubscriptâ„01subscriptğ‘1q\_{1}^{h}:=\underset{q\_{1}\in\{q\_{1}^{2,\*},q\_{1}^{1,2,\*}\}}{\textnormal{argmin}}h\_{0}^{1}(q\_{1}) |
| nw<â‹…<nsn\_{w}<\cdot<n\_{s} | >0absent0>0 | â‰¤Babsentğµ\leq B | â‰¥Babsentğµ\geq B | â‰¥Babsentğµ\geq B | q1h:=argminq1âˆˆ{q12,âˆ—,B,q11,2,âˆ—}â€‹h01â€‹(q1)assignsuperscriptsubscriptğ‘1â„subscriptğ‘1superscriptsubscriptğ‘1  2ğµsuperscriptsubscriptğ‘1  12argminsuperscriptsubscriptâ„01subscriptğ‘1q\_{1}^{h}:=\underset{q\_{1}\in\{q\_{1}^{2,\*},B,q\_{1}^{1,2,\*}\}}{\textnormal{argmin}}h\_{0}^{1}(q\_{1}) |
| nw<â‹…<nsn\_{w}<\cdot<n\_{s} | >0absent0>0 | â‰¥Babsentğµ\geq B | â‰¤Babsentğµ\leq B | â‰¤Babsentğµ\leq B | q1h:=Bassignsuperscriptsubscriptğ‘1â„ğµq\_{1}^{h}:=B |
| nw<â‹…<nsn\_{w}<\cdot<n\_{s} | >0absent0>0 | â‰¥Babsentğµ\geq B |  | â‰¥Babsentğµ\geq B | q1h:=argminq1âˆˆ{B,q11,2,âˆ—}â€‹h01â€‹(q1)assignsuperscriptsubscriptğ‘1â„subscriptğ‘1ğµsuperscriptsubscriptğ‘1  12argminsuperscriptsubscriptâ„01subscriptğ‘1q\_{1}^{h}:=\underset{q\_{1}\in\{B,q\_{1}^{1,2,\*}\}}{\textnormal{argmin}}h\_{0}^{1}(q\_{1}) |
| nw<â‹…<nsn\_{w}<\cdot<n\_{s} | â‰¤0absent0\leq 0 | â‰¤Babsentğµ\leq B |  |  | q1h:=argminq1âˆˆ{q12,âˆ—,B}â€‹h01â€‹(q1)assignsuperscriptsubscriptğ‘1â„subscriptğ‘1superscriptsubscriptğ‘1  2ğµargminsuperscriptsubscriptâ„01subscriptğ‘1q\_{1}^{h}:=\underset{q\_{1}\in\{q\_{1}^{2,\*},B\}}{\textnormal{argmin}}h\_{0}^{1}(q\_{1}) |
| nw<â‹…<nsn\_{w}<\cdot<n\_{s} | â‰¤0absent0\leq 0 | â‰¥Babsentğµ\geq B |  |  | q1h:=Bassignsuperscriptsubscriptğ‘1â„ğµq\_{1}^{h}:=B |

Table 5: Optimal q1hsuperscriptsubscriptğ‘1â„q\_{1}^{h} for h~01subscriptsuperscript~â„10\tilde{h}^{1}\_{0}.

For simplicity, let us consider the case c=0ğ‘0c=0, see Remark [2.4](#S2.Thmtheorem4 "Remark 2.4. â€£ 2.3 Error bound for Sub-Gamma distributions â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios").

On Figure [19](#A1.F19 "Figure 19 â€£ Appendix A Proxy of the optimal strategy for the heuristic (21) â€£ Computation of Expected Shortfall by fast detection of worst scenarios"), the square is q11,2,âˆ—=52.41superscriptsubscriptğ‘1

1252.41q\_{1}^{1,2,\*}=52.41, the circle is q12,âˆ—=68.33superscriptsubscriptğ‘1

268.33q\_{1}^{2,\*}=68.33 and the cross is the real optimum q1âˆ—=71superscriptsubscriptğ‘171q\_{1}^{\*}=71 of h01subscriptsuperscriptâ„10h^{1}\_{0}, for the parameters of Tables [1](#S2.T1 "Table 1 â€£ 2.5 Simplified 2-levels algorithm for a linear indifference zoneâ€™s size â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios") and [2](#S2.T2 "Table 2 â€£ 2.5 Simplified 2-levels algorithm for a linear indifference zoneâ€™s size â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios"). We see that we actually almost reach the correct minimum. It corresponds (up to rounding) to N11,âˆ—=23723superscriptsubscriptğ‘1

123723N\_{1}^{1,\*}=23723, N12,âˆ—=17148superscriptsubscriptğ‘1

217148N\_{1}^{2,\*}=17148, N1âˆ—=15934superscriptsubscriptğ‘115934N\_{1}^{\*}=15934.

![Refer to caption](/html/2005.12593/assets/S215A_compromise_between_q_and_N_mc_bare.png)


Figure 19: Square: h01â€‹(q11,2,âˆ—)superscriptsubscriptâ„01superscriptsubscriptğ‘1

12h\_{0}^{1}(q\_{1}^{1,2,\*}). Circle: h01â€‹(q12,âˆ—)superscriptsubscriptâ„01superscriptsubscriptğ‘1

2h\_{0}^{1}(q\_{1}^{2,\*}). Cross: h01â€‹(q1âˆ—)superscriptsubscriptâ„01superscriptsubscriptğ‘1h\_{0}^{1}(q\_{1}^{\*}).

Using the same set of parameters, we plot on Figure [20](#A1.F20 "Figure 20 â€£ Appendix A Proxy of the optimal strategy for the heuristic (21) â€£ Computation of Expected Shortfall by fast detection of worst scenarios") the two functions h01superscriptsubscriptâ„01h\_{0}^{1} and h~1subscript~â„1\tilde{h}\_{1}. Although these two functions have values of different orders of magnitude, their shapes are quite close, which explains why we manage to obtain a relatively good approximation for the minimizer.

![Refer to caption](/html/2005.12593/assets/S215A_h_0_max_h_1_h_2_bias_bare.png)


Figure 20: Solid line : h01superscriptsubscriptâ„01h\_{0}^{1}. Dashed line : h~1subscript~â„1\tilde{h}\_{1}.

## Appendix B Precise implementation of the neural network algorithm

In this Appendix, we describe in more details how the neural network approximation of the optimal policy of the adaptative algorithm is constructed. All the parameters values are given in Tables [6](#A2.T6 "Table 6 â€£ B.12 Normalization constants, implementation parameters, and learning rates â€£ Appendix B Precise implementation of the neural network algorithm â€£ Computation of Expected Shortfall by fast detection of worst scenarios"), [7](#A2.T7 "Table 7 â€£ B.12 Normalization constants, implementation parameters, and learning rates â€£ Appendix B Precise implementation of the neural network algorithm â€£ Computation of Expected Shortfall by fast detection of worst scenarios") and [8](#A2.T8 "Table 8 â€£ B.12 Normalization constants, implementation parameters, and learning rates â€£ Appendix B Precise implementation of the neural network algorithm â€£ Computation of Expected Shortfall by fast detection of worst scenarios") below.

### B.1 Initialization

* â€¢

  In practice, the neural network inputâ€™s size depends on the window size qğ‘q. Therefore, we need to train different neural networks for each window size. In order to get enough points to train each of these neural networks, we have chosen the grid

  |  |  |  |
  | --- | --- | --- |
  |  | qg=[6,10,15,20,25,30,35,40,45,50,60,70,80,90,100,150,200,253]subscriptğ‘ğ‘” 610152025303540455060708090100150200253q\_{g}=[6,10,15,20,25,30,35,40,45,50,60,70,80,90,100,150,200,253] |  |

  of possible values for qğ‘q.
* â€¢

  We simulate independent copies {Î¸~j}jâ‰¤jâ€‹\_â€‹bar={(Î¼~j,Î£~j)}jâ‰¤jâ€‹\_â€‹barsubscriptsuperscript~ğœƒğ‘—ğ‘—j\_barsubscriptsuperscript~ğœ‡ğ‘—superscript~Î£ğ‘—ğ‘—j\_bar\{\tilde{\theta}^{j}\}\_{j\leq{\rm j\\_bar}}=\{(\tilde{\mu}^{j},\tilde{\Sigma}^{j})\}\_{j\leq{\rm j\\_bar}} of Î¸~~ğœƒ\tilde{\theta}, where j\_bar is given in Table [8](#A2.T8 "Table 8 â€£ B.12 Normalization constants, implementation parameters, and learning rates â€£ Appendix B Precise implementation of the neural network algorithm â€£ Computation of Expected Shortfall by fast detection of worst scenarios"). For each 1â‰¤jâ‰¤1ğ‘—absent1\leq j\leq j\_bar, Î£~jsuperscript~Î£ğ‘—\tilde{\Sigma}^{j} is an inverse-Wishart of parameters ğš’0,Î£0
  subscriptğš’0subscriptÎ£0{\mathtt{i}}\_{0},\Sigma\_{0}, and Î¼~jsuperscript~ğœ‡ğ‘—\tilde{\mu}^{j} is a Gaussian random vector of mean ğš–0subscriptğš–0{\mathtt{m}}\_{0} and covariance matrix Î£~j/ğš”0superscript~Î£ğ‘—subscriptğš”0\tilde{\Sigma}^{j}/{\mathtt{k}}\_{0}. The parameters ğš’0,ğš”0
  subscriptğš’0subscriptğš”0{\mathtt{i}}\_{0},{\mathtt{k}}\_{0} and Î£0subscriptÎ£0\Sigma\_{0} are defined in Table [8](#A2.T8 "Table 8 â€£ B.12 Normalization constants, implementation parameters, and learning rates â€£ Appendix B Precise implementation of the neural network algorithm â€£ Computation of Expected Shortfall by fast detection of worst scenarios") and ([30](#S4.E30 "In 4 Numerical Experiments â€£ Computation of Expected Shortfall by fast detection of worst scenarios")), while ğš–0i=Î¼isubscriptsuperscriptğš–ğ‘–0superscriptğœ‡ğ‘–{\mathtt{m}}^{i}\_{0}=\mu^{i}, iâ‰¤nsğ‘–subscriptğ‘›ğ‘ i\leq n\_{s}, with the Î¼isuperscriptğœ‡ğ‘–\mu^{i}â€™s of Figure [6](#S2.F6 "Figure 6 â€£ 2.5 Simplified 2-levels algorithm for a linear indifference zoneâ€™s size â€£ 2 Algorithm with a deterministic effort allocation â€£ Computation of Expected Shortfall by fast detection of worst scenarios").

### B.2 Strategy Generation

To generate the deterministic strategies (Î±k)kâ‰¤kâ€‹\_â€‹barsubscriptsuperscriptğ›¼ğ‘˜ğ‘˜k\_bar(\alpha^{k})\_{k\leq{\rm k\\_bar}}, where k\_bar is given in Table [8](#A2.T8 "Table 8 â€£ B.12 Normalization constants, implementation parameters, and learning rates â€£ Appendix B Precise implementation of the neural network algorithm â€£ Computation of Expected Shortfall by fast detection of worst scenarios"), we proceed as follows.

* â€¢

  For each 1â‰¤kâ‰¤1ğ‘˜absent1\leq k\leqk\_bar, we simulate L+1ğ¿1L+1 uniform random variables (Un)n=0Lsuperscriptsubscriptsubscriptğ‘ˆğ‘›ğ‘›0ğ¿\left(U\_{n}\right)\_{n=0}^{L} between 00 and 111. We sort them in increasing order (Usâ€‹(n))n=0Lsuperscriptsubscriptsubscriptğ‘ˆğ‘ ğ‘›ğ‘›0ğ¿\left(U\_{s(n)}\right)\_{n=0}^{L} and define a cost Kâ„“:=Kâ€‹(Usâ€‹(â„“)âˆ’Usâ€‹(â„“âˆ’1))assignsubscriptğ¾â„“ğ¾subscriptğ‘ˆğ‘ â„“subscriptğ‘ˆğ‘ â„“1K\_{\ell}:=K(U\_{s(\ell)}-U\_{s(\ell-1)}) when 1â‰¤â„“â‰¤Lâˆ’11â„“ğ¿11\leq\ell\leq L-1, and KL=Kâ€‹(Usâ€‹(0)+1âˆ’Usâ€‹(Lâˆ’1))subscriptğ¾ğ¿ğ¾subscriptğ‘ˆğ‘ 01subscriptğ‘ˆğ‘ ğ¿1K\_{L}=K(U\_{s(0)}+1-U\_{s(L-1)}). The idea is that we select L+1ğ¿1L+1 points randomly on a cercle of total length Kğ¾K: we choose one of these points, and starting from it, the computational power that we will use at each level 1â‰¤â„“â‰¤Lâˆ’11â„“ğ¿11\leq\ell\leq L-1 is the length of the arc between the previous and the next point. For the last step, we take Kğ¾K times the length between the points Lâˆ’1ğ¿1L-1 and 00, so as to put, in average, twice more computational power on this last step.
* â€¢

  Once we have the computational cost for each step, we can choose the qâ„“subscriptğ‘â„“q\_{\ell} for each strategy, so that we can deduce Î´â€‹Nâ„“+1:=Kâ„“/qâ„“assignğ›¿subscriptğ‘â„“1subscriptğ¾â„“subscriptğ‘â„“\delta N\_{\ell+1}{:=K\_{\ell}/q\_{\ell}}. For â„“=0â„“0\ell=0, we choose q\_index0=18subscriptq\_index018\textnormal{q\\_index}\_{0}={18}, where 181818 is the number of terms in the grid qgsubscriptğ‘ğ‘”q\_{g}, which therefore gives q0=qgâ€‹[q\_index0]=nssubscriptğ‘0subscriptğ‘ğ‘”delimited-[]subscriptq\_index0subscriptğ‘›ğ‘ q\_{0}=q\_{g}[\textnormal{q\\_index}\_{0}]=n\_{s}. For â„“=Lâˆ’1â„“ğ¿1\ell=L-1, we choose q\_indexLâˆ’1=0subscriptq\_indexğ¿10\textnormal{q\\_index}\_{L-1}=0, that is, qLâˆ’1=nwsubscriptğ‘ğ¿1subscriptğ‘›ğ‘¤q\_{L-1}=n\_{w}. For 1â‰¤â„“â‰¤Lâˆ’21â„“ğ¿21\leq\ell\leq L-2, we choose q\_indexâ„“subscriptq\_indexâ„“\textnormal{q\\_index}\_{\ell} as a random integer between [Lâˆ’â„“,q\_indexâ„“âˆ’1âˆ’1]ğ¿â„“subscriptq\_indexâ„“11[L-\ell,\textnormal{q\\_index}\_{\ell-1}-1]. The choice of qâ„“subscriptğ‘â„“q\_{\ell} is then qâ„“=qgâ€‹[q\_indexâ„“]subscriptğ‘â„“subscriptğ‘ğ‘”delimited-[]subscriptq\_indexâ„“q\_{\ell}=q\_{g}[\textnormal{q\\_index}\_{\ell}]. We check that the sequence (Nâ„“)1â‰¤â„“â‰¤Lsubscriptsubscriptğ‘â„“1â„“ğ¿(N\_{\ell})\_{1\leq\ell\leq L} is non-decreasing. If this is the case, we keep it, if not, we reject it and do another run.

### B.3 Forward Pass

The next step is to generate all prices and execute for each kğ‘˜k and each jğ‘—j the strategy kğ‘˜k.

* â€¢

  For 1â‰¤jâ‰¤j\_bar1ğ‘—j\_bar1\leq j\leq\textnormal{j\\_bar}, 1â‰¤kâ‰¤k\_bar1ğ‘˜k\_bar1\leq k\leq\textnormal{k\\_bar} and 1â‰¤â„“â‰¤L1â„“ğ¿1\leq\ell\leq L, we simulate Î´â€‹Nâ„“kğ›¿superscriptsubscriptğ‘â„“ğ‘˜\delta N\_{\ell}^{k} Gaussian variables (Pjâ€²j,1,â€¦,Pjâ€²j,ns)jâ€²=Nâ„“âˆ’1kNâ„“ksuperscriptsubscriptsuperscriptsubscriptğ‘ƒsuperscriptğ‘—â€²
  ğ‘—1â€¦superscriptsubscriptğ‘ƒsuperscriptğ‘—â€²
  ğ‘—subscriptğ‘›ğ‘ superscriptğ‘—â€²superscriptsubscriptğ‘â„“1ğ‘˜superscriptsubscriptğ‘â„“ğ‘˜\left(P\_{j^{\prime}}^{j,1},\ldots,P\_{j^{\prime}}^{j,n\_{s}}\right)\_{j^{\prime}=N\_{\ell-1}^{k}}^{N\_{\ell}^{k}} of mean Î¼~jsuperscript~ğœ‡ğ‘—\tilde{\mu}^{j} and covariance matrix Î£~jsuperscript~Î£ğ‘—\tilde{\Sigma}^{j} (independently across jğ‘—j and kğ‘˜k).
* â€¢

  We then update Î¼^â„“k,j,ğš–â„“k,j,ğš’â„“k,j,ğš”â„“k,j,Î£â„“k,j
  superscriptsubscript^ğœ‡â„“
  ğ‘˜ğ‘—superscriptsubscriptğš–â„“
  ğ‘˜ğ‘—superscriptsubscriptğš’â„“
  ğ‘˜ğ‘—superscriptsubscriptğš”â„“
  ğ‘˜ğ‘—superscriptsubscriptÎ£â„“
  ğ‘˜ğ‘—\hat{\mu}\_{\ell}^{k,j},{\mathtt{m}}\_{\ell}^{k,j},{\mathtt{i}}\_{\ell}^{k,j},{\mathtt{k}}\_{\ell}^{k,j},\Sigma\_{\ell}^{k,j} accordingly, recall ([29](#S3.E29 "In Remark 3.7. â€£ 3.2 A generic progressive learning algorithm â€£ 3 Adaptative algorithm â€£ Computation of Expected Shortfall by fast detection of worst scenarios")).
* â€¢

  Updating Î£â„“k,jsuperscriptsubscriptÎ£â„“
  ğ‘˜ğ‘—\Sigma\_{\ell}^{k,j} from level â„“âˆ’1â„“1\ell-1 to level â„“â„“\ell can use a lot of memory. Indeed, âˆ‘j=Nâ„“Î±+1Nâ„“+1Î±(ğ’¯â„“+1Î±â€‹(Pj)âˆ’Î´â€‹Î¼^â„“+1Î±)â€‹(ğ’¯â„“+1Î±â€‹(Pj)âˆ’Î´â€‹Î¼^â„“+1Î±)âŠ¤superscriptsubscriptğ‘—subscriptsuperscriptğ‘ğ›¼â„“1subscriptsuperscriptğ‘ğ›¼â„“1subscriptsuperscriptğ’¯ğ›¼â„“1subscriptğ‘ƒğ‘—ğ›¿subscriptsuperscript^ğœ‡ğ›¼â„“1superscriptsubscriptsuperscriptğ’¯ğ›¼â„“1subscriptğ‘ƒğ‘—ğ›¿subscriptsuperscript^ğœ‡ğ›¼â„“1top\sum\_{j=N^{\alpha}\_{\ell}+1}^{N^{\alpha}\_{\ell+1}}({\cal T}^{\alpha}\_{\ell+1}(P\_{j})-\delta\hat{\mu}^{\alpha}\_{\ell+1})({\cal T}^{\alpha}\_{\ell+1}(P\_{j})-\delta\hat{\mu}^{\alpha}\_{\ell+1})^{\top} consists in Î´â€‹Nâ„“+1Î±Ã—|qâ„“+1|2ğ›¿superscriptsubscriptğ‘â„“1ğ›¼superscriptsubscriptğ‘â„“12\delta N\_{\ell+1}^{\alpha}\times|q\_{\ell+1}|^{2} terms, which can quickly exceed memory limits. Therefore, we do the sum with only N\_memory\_new\_pricings\_opt terms at a time, see Table [8](#A2.T8 "Table 8 â€£ B.12 Normalization constants, implementation parameters, and learning rates â€£ Appendix B Precise implementation of the neural network algorithm â€£ Computation of Expected Shortfall by fast detection of worst scenarios") below.

### B.4 Computation of f\_precompute, running\_costs and admissible\_sets

* â€¢

  In order to speed up the computation time, we now precompute several values that will be used many times afterwards. First, we compute f\_precompute(â„“,k,j)â„“ğ‘˜ğ‘—(\ell,k,j) as f1adâ€‹(â„“,â‹…)subscriptsuperscriptğ‘“ad1â„“â‹…f^{\rm ad}\_{1}(\ell,\cdot) at the point corresponding to (k,j)ğ‘˜ğ‘—(k,j) except that, in the definition of f1adâ€‹(â„“,â‹…)subscriptsuperscriptğ‘“ad1â„“â‹…f^{\rm ad}\_{1}(\ell,\cdot), we replace the random permutation ğ”ª~~ğ”ª\tilde{\mathfrak{m}} by its estimation from the previous step, Î¼~~ğœ‡\tilde{\mu} by its average under the posterior distribution at â„“+1â„“1\ell+1, and Ïƒ~~ğœ\tilde{\sigma} by its estimation at step â„“+1â„“1\ell+1.
* â€¢

  We compute running\_costâ€‹(â„“,k):=Câ„“Î±kassignrunning\_costâ„“ğ‘˜subscriptsuperscriptğ¶superscriptğ›¼ğ‘˜â„“\textnormal{running\\_cost}(\ell,k):=C^{\alpha^{k}}\_{\ell} of each kğ‘˜k at step â„“â„“\ell.
* â€¢

  We restrict the set of possible actions at step â„“â„“\ell, given that we have followed the strategy kğ‘˜k so far, to admissible\_setsâ€‹(â„“,k)admissible\_setsâ„“ğ‘˜\textnormal{admissible\\_sets}(\ell,k) defined as the collection of {(Î´qâ„“+1kâ€²,Î´Nâ„“+1kâ€²)\{(\delta q\_{\ell+1}^{k^{\prime}},\delta N\_{\ell+1}^{k^{\prime}}), kâ€²â‰¤k\_bar}k^{\prime}\leq{\rm k\\_bar}\}, such that

  |  |  |  |
  | --- | --- | --- |
  |  | qâ„“k+Î´â€‹qâ„“+1kâ€²âˆˆqg,Nâ„“+1kâ€²>Nâ„“k,running\_costâ€‹(â„“,k)+qâ„“kâ€‹Î´â€‹Nâ„“+1kâ€²â‰¤max1â‰¤kâ€²â€²â‰¤kÂ¯â€‹running\_costâ€‹(â„“+1,kâ€²â€²).formulae-sequencesuperscriptsubscriptğ‘â„“ğ‘˜ğ›¿superscriptsubscriptğ‘â„“1superscriptğ‘˜â€²subscriptğ‘ğ‘”formulae-sequencesuperscriptsubscriptğ‘â„“1superscriptğ‘˜â€²superscriptsubscriptğ‘â„“ğ‘˜running\_costâ„“ğ‘˜superscriptsubscriptğ‘â„“ğ‘˜ğ›¿superscriptsubscriptğ‘â„“1superscriptğ‘˜â€²1superscriptğ‘˜â€²â€²Â¯ğ‘˜running\_costâ„“1superscriptğ‘˜â€²â€²q\_{\ell}^{k}+\delta q\_{\ell+1}^{k^{\prime}}\in q\_{g},\;N\_{\ell+1}^{k^{\prime}}>N\_{\ell}^{k},\;\textnormal{running\\_cost}(\ell,k)+q\_{\ell}^{k}\delta N\_{\ell+1}^{k^{\prime}}\leq\underset{1\leq k^{\prime\prime}\leq\bar{k}}{\max}\textnormal{running\\_cost}(\ell+1,k^{\prime\prime}). |  |

  The last condition avoids inducing a strategy with a running cost that is not present in our data set, when doing the one step optimization.

### B.5 Computation of the final expectations

We first pre-compute the quantities

|  |  |  |
| --- | --- | --- |
|  | ğ”¼LÎ½Lk,jâ€‹[|1nwâ€‹âˆ‘iâˆˆâ„‘Lâˆ’1k,j(Î¼^Lk,j)iâˆ’Î¼~i|]subscriptsuperscriptğ”¼subscriptsuperscriptğœˆ  ğ‘˜ğ‘—ğ¿ğ¿delimited-[]1subscriptğ‘›ğ‘¤subscriptğ‘–superscriptsubscriptâ„‘ğ¿1  ğ‘˜ğ‘—superscriptsuperscriptsubscript^ğœ‡ğ¿  ğ‘˜ğ‘—ğ‘–superscript~ğœ‡ğ‘–{\mathbb{E}}^{\nu^{k,j}\_{L}}\_{L}\left[\left|\frac{1}{n\_{w}}\sum\_{i\in{\mathfrak{I}}\_{L-1}^{k,j}}(\hat{\mu}\_{L}^{{k},j})^{i}-\tilde{\mu}^{i}\right|\right] |  |

by Monte Carlo using Nesubscriptğ‘ğ‘’N\_{e} simulations. As the simulation of an inverse-Wishart random variable is significantly slower than the simulation of a Gaussian random variable, we only simulate 1 inverse-Wishart for Npsubscriptğ‘ğ‘N\_{p} Gaussians. The values of Nesubscriptğ‘ğ‘’N\_{e} and Npsubscriptğ‘ğ‘N\_{p} are given by N\_mu\_tildes\_simulated and N\_wishart\_proportion of Table [8](#A2.T8 "Table 8 â€£ B.12 Normalization constants, implementation parameters, and learning rates â€£ Appendix B Precise implementation of the neural network algorithm â€£ Computation of Expected Shortfall by fast detection of worst scenarios"). The estimation is called expectationLk,jsuperscriptsubscriptexpectationğ¿

ğ‘˜ğ‘—\textnormal{expectation}\_{L}^{k,j}.

### B.6 Training of the neural network at level Lğ¿L

* â€¢

  We use a neural network with one inner layer with 256 neurons and 1 output layer with 1 neuron to fit (expectationLk,j)jâ‰¤jâ€‹\_â€‹bar,kâ‰¤kâ€‹\_â€‹barsubscriptsuperscriptsubscriptexpectationğ¿
  ğ‘˜ğ‘—formulae-sequenceğ‘—j\_barğ‘˜k\_bar(\textnormal{expectation}\_{L}^{k,j})\_{j\leq{\rm j\\_bar},k\leq{\rm k\\_bar}}. The neurons of the inner layer consist of the composition of the softplus function with an affine transformation of the inputs.
* â€¢

  We initialize the neural network parameters using a Xavier initialization. We then train the neural network by selecting a random new batch every N\_batch\_change\_proportion. This random new batch is composed of the samples indexed by 1â‰¤ğ”ªaâ€‹(j)â‰¤j\_batch1subscriptğ”ªğ‘ğ‘—j\_batch1\leq{\mathfrak{m}}\_{a}(j)\leq\textnormal{j\\_batch} and strategies indexed by 1â‰¤ğ”ªbâ€‹(k)â‰¤k\_batch1subscriptğ”ªğ‘ğ‘˜k\_batch1\leq{\mathfrak{m}}\_{b}(k)\leq\textnormal{k\\_batch}, where ğ”ªasubscriptğ”ªğ‘{\mathfrak{m}}\_{a} and ğ”ªbsubscriptğ”ªğ‘{\mathfrak{m}}\_{b} are uniform random permutations of [[1,jâ€‹\_â€‹bar]]delimited-[]1j\_bar[\![1,{\rm j\\_bar}]\!] and [[1,kâ€‹\_â€‹bar]]delimited-[]1k\_bar[\![1,{\rm k\\_bar}]\!]. For each batch, the algorithm used for the training is the standard gradient descent of Tensorflow. We do N\_Iter training steps in total. The learning rate used is given in Table [7](#A2.T7 "Table 7 â€£ B.12 Normalization constants, implementation parameters, and learning rates â€£ Appendix B Precise implementation of the neural network algorithm â€£ Computation of Expected Shortfall by fast detection of worst scenarios"). In order to bring the input values of the parameters close to 00 and 111, we renormalize them according to the values in Table [6](#A2.T6 "Table 6 â€£ B.12 Normalization constants, implementation parameters, and learning rates â€£ Appendix B Precise implementation of the neural network algorithm â€£ Computation of Expected Shortfall by fast detection of worst scenarios").

### B.7 Computation of the expectations at level Lâˆ’1ğ¿1L-1

We now estimate

|  |  |  |
| --- | --- | --- |
|  | ğ”¼Lâˆ’1Î½Lâˆ’1k,jâ€‹[Ï•Ë‡Lâ€‹(qLÎ±k,NLÎ±k,CLÎ±k,Î¼^Lk,ğš™Lk)]subscriptsuperscriptğ”¼subscriptsuperscriptğœˆ  ğ‘˜ğ‘—ğ¿1ğ¿1delimited-[]subscriptË‡italic-Ï•ğ¿subscriptsuperscriptğ‘superscriptğ›¼ğ‘˜ğ¿subscriptsuperscriptğ‘superscriptğ›¼ğ‘˜ğ¿subscriptsuperscriptğ¶superscriptğ›¼ğ‘˜ğ¿subscriptsuperscript^ğœ‡ğ‘˜ğ¿subscriptsuperscriptğš™ğ‘˜ğ¿{\mathbb{E}}^{\nu^{k,j}\_{L-1}}\_{L-1}\left[\check{\phi}\_{L}(q^{\alpha^{k}}\_{L},N^{\alpha^{k}}\_{L},C^{\alpha^{k}}\_{L},\hat{\mu}^{k}\_{L},\mathtt{p}^{k}\_{L})\right] |  |

where Ï•Ë‡LsubscriptË‡italic-Ï•ğ¿\check{\phi}\_{L} is the fit of (expectationLk,j)jâ‰¤jâ€‹\_â€‹bar,kâ‰¤kâ€‹\_â€‹barsubscriptsuperscriptsubscriptexpectationğ¿

ğ‘˜ğ‘—formulae-sequenceğ‘—j\_barğ‘˜k\_bar(\textnormal{expectation}\_{L}^{k,j})\_{j\leq{\rm j\\_bar},k\leq{\rm k\\_bar}} from the previous step. The most cpu demanding part is no more the simulation of the inverse-Wisharts, but the updates of the parameters of the inverse-Wishart. Therefore, we simulate as many Gaussian random variables as inverse-Wishart random variables, with Nesubscriptğ‘ğ‘’N\_{e} given by N\_mu\_tildes\_simulated\_non\_final\_level of Table [8](#A2.T8 "Table 8 â€£ B.12 Normalization constants, implementation parameters, and learning rates â€£ Appendix B Precise implementation of the neural network algorithm â€£ Computation of Expected Shortfall by fast detection of worst scenarios").

For our computations, we need to update Î£Lâˆ’1k,jsubscriptsuperscriptÎ£

ğ‘˜ğ‘—ğ¿1\Sigma^{k,j}\_{L-1} to the corresponding posterior parameter according to ([29](#S3.E29 "In Remark 3.7. â€£ 3.2 A generic progressive learning algorithm â€£ 3 Adaptative algorithm â€£ Computation of Expected Shortfall by fast detection of worst scenarios")). This can however lead to an enormous amount of multiplications and additions. Therefore, instead of updating the whole matrix, we only update the diagonal terms according to ([29](#S3.E29 "In Remark 3.7. â€£ 3.2 A generic progressive learning algorithm â€£ 3 Adaptative algorithm â€£ Computation of Expected Shortfall by fast detection of worst scenarios")) and estimate non diagonal terms by keeping the correlation terms equal to the ones of Î£Lâˆ’1k,jsubscriptsuperscriptÎ£

ğ‘˜ğ‘—ğ¿1\Sigma^{k,j}\_{L-1}.
This enables us to approximately gain a factor of qLksuperscriptsubscriptğ‘ğ¿ğ‘˜q\_{L}^{k} in speed in this critical step.

### B.8 Training of the neural network at level Lâˆ’1ğ¿1L-1

* â€¢

  To fit the expectation of the previous step, we use a neural network with the same structure as in level Lğ¿L, with the same cost function.
* â€¢

  The initialization, choice of batches, and training of the neural network are the same as for the level Lğ¿L. The number of iteration, learning rate, and renormalization constants are given in Tables [6](#A2.T6 "Table 6 â€£ B.12 Normalization constants, implementation parameters, and learning rates â€£ Appendix B Precise implementation of the neural network algorithm â€£ Computation of Expected Shortfall by fast detection of worst scenarios"), [8](#A2.T8 "Table 8 â€£ B.12 Normalization constants, implementation parameters, and learning rates â€£ Appendix B Precise implementation of the neural network algorithm â€£ Computation of Expected Shortfall by fast detection of worst scenarios") and [7](#A2.T7 "Table 7 â€£ B.12 Normalization constants, implementation parameters, and learning rates â€£ Appendix B Precise implementation of the neural network algorithm â€£ Computation of Expected Shortfall by fast detection of worst scenarios").
* â€¢

  We take j\_batch=minâ¡(j\_batch\_size,j\_bar)j\_batchj\_batch\_sizej\_bar\textnormal{j\\_batch}=\min\left(\textnormal{j\\_batch\\_size},\textnormal{j\\_bar}\right) and k\_batch=minâ¡(k\_batch\_size,k\_bar)k\_batchk\_batch\_sizek\_bar\textnormal{k\\_batch}=\min\left(\textnormal{k\\_batch\\_size},\textnormal{k\\_bar}\right), where j\_batch\_size and k\_batch\_size are defined in Table [8](#A2.T8 "Table 8 â€£ B.12 Normalization constants, implementation parameters, and learning rates â€£ Appendix B Precise implementation of the neural network algorithm â€£ Computation of Expected Shortfall by fast detection of worst scenarios").

### B.9 Computation of the expectations at levels 0â‰¤â„“â‰¤Lâˆ’20â„“ğ¿20\leq\ell\leq L-2

* â€¢

  The expectations at step â„“â„“\ell are computed by Monte Carlo after replacing the value function at step â„“+1â„“1\ell+1 by its neural network approximation, and f1adâ€‹(â„“,â‹…)subscriptsuperscriptğ‘“ad1â„“â‹…f^{\rm ad}\_{1}(\ell,\cdot) by f\_precompute(â„“,â‹…)â„“â‹…(\ell,\cdot).
* â€¢

  We simulate as many Gaussian random variables as inverse-Wishart random variables, with Nesubscriptğ‘ğ‘’N\_{e} given by N\_mu\_tildes\_simulated\_non\_final\_level of Table [8](#A2.T8 "Table 8 â€£ B.12 Normalization constants, implementation parameters, and learning rates â€£ Appendix B Precise implementation of the neural network algorithm â€£ Computation of Expected Shortfall by fast detection of worst scenarios").
* â€¢

  We not not fully update Î£â„“k,jsubscriptsuperscriptÎ£
  ğ‘˜ğ‘—â„“\Sigma^{k,j}\_{\ell} to the corresponding posterior parameter but proceed as in level Lâˆ’1ğ¿1L-1.

### B.10 Training of neural networks at levels 0â‰¤â„“â‰¤Lâˆ’20â„“ğ¿20\leq\ell\leq L-2

* â€¢

  We now have to optimize over qâ„“kâˆˆqgsuperscriptsubscriptğ‘â„“ğ‘˜subscriptğ‘ğ‘”q\_{\ell}^{k}\in q\_{g}. Therefore, we must now train up to |qg|subscriptğ‘ğ‘”|q\_{g}| different neural networks (with different inputsâ€™ sizes). In practice, we only train neural networks indexed by qâˆˆ(qâ„“k)1â‰¤kâ‰¤k\_barâŠ‚qgğ‘subscriptsuperscriptsubscriptğ‘â„“ğ‘˜1ğ‘˜k\_barsubscriptğ‘ğ‘”q\in(q\_{\ell}^{k})\_{1\leq k\leq\textnormal{k\\_bar}}\subset q\_{g}, that is, for all the choices of qğ‘q that are obtained by at least one strategy at level â„“â„“\ell.
* â€¢

  We must also choose a Î´â€‹Nğ›¿ğ‘\delta N that should be added as an entry of the neural network before optimizing. Furthermore, to help the neural networks converge, we decided to add f\_precomputeâ€‹(â„“,j,k)f\_precomputeâ„“ğ‘—ğ‘˜\textnormal{f\\_precompute}(\ell,j,k) as an input.
* â€¢

  The loss function and the structure of the neural network is as above, and we still use Xavier initialization, and bring the inputs of the neural networks to reasonable values close to 0 and 1 by renormalizing them using the constants of Table [6](#A2.T6 "Table 6 â€£ B.12 Normalization constants, implementation parameters, and learning rates â€£ Appendix B Precise implementation of the neural network algorithm â€£ Computation of Expected Shortfall by fast detection of worst scenarios").
* â€¢

  Compared to levels Lğ¿L and Lâˆ’1ğ¿1L-1, the choice of batches is slightly different. Indeed, to train a neural network associated to qâˆˆqgğ‘subscriptğ‘ğ‘”q\in q\_{g}, we only use strategies such that qâ„“k=qsuperscriptsubscriptğ‘â„“ğ‘˜ğ‘q\_{\ell}^{k}=q. To do so, we first define Sq={kâˆˆ[[1,kâ€‹\_â€‹bar]]:qâ„“k=q}subscriptğ‘†ğ‘conditional-setğ‘˜delimited-[]1k\_barsuperscriptsubscriptğ‘â„“ğ‘˜ğ‘S\_{q}=\{k\in[\![1,{\rm k\\_bar}]\!]:q\_{\ell}^{k}=q\}. We then define k\_batch=minâ¡(k\_batch\_size,|Sq|)k\_batchk\_batch\_sizesubscriptğ‘†ğ‘\textnormal{k\\_batch}=\min\left(\textnormal{k\\_batch\\_size},|S\_{q}|\right) and j\_batch=minâ¡(j\_batch\_size,jâ€‹\_â€‹bar)j\_batchj\_batch\_sizej\_bar\textnormal{j\\_batch}=\min\left(\textnormal{j\\_batch\\_size},{\rm j\\_bar}\right). We then proceed nearly identically as for levels Lğ¿L and Lâˆ’1ğ¿1L-1. We select a new batch every N\_batch\_change\_proportion, composed of indices 1â‰¤ğ”ªaâ€‹(j)â‰¤j\_batch1subscriptğ”ªğ‘ğ‘—j\_batch1\leq{\mathfrak{m}}\_{a}(j)\leq\textnormal{j\\_batch}, 1â‰¤ğ”ªbâ€‹(k)â‰¤k\_batch1subscriptğ”ªğ‘ğ‘˜k\_batch1\leq{\mathfrak{m}}\_{b}(k)\leq\textnormal{k\\_batch}, where ğ”ªasubscriptğ”ªğ‘{\mathfrak{m}}\_{a} and ğ”ªbsubscriptğ”ªğ‘{\mathfrak{m}}\_{b} are uniform random permutations of [[1,jâ€‹\_â€‹bar]]delimited-[]1j\_bar[\![1,{\rm j\\_bar}]\!] and Sqsubscriptğ‘†ğ‘S\_{q}. For each batch, the algorithm used for the training is again the standard gradient descent of Tensorflow.
* â€¢

  Compared to levels Lğ¿L and Lâˆ’1ğ¿1L-1, we found that making the neural networks converge was much harder. In particular, the learning rate had to be really fine tuned. In order to automatize the process, for each qğ‘q, we proceed as follows. We do not intanciate one, but
    
  number\_of\_neural\_networks\_for\_learning\_rate\_test neural networks. For each of these neural networks, we do N\_Iter\_learning\_rate\_test training steps, but use different learning rates for each. For the first neural network, we use base\_learning\_rate as the learning rate, for the second,
    
  base\_learning\_rate/10, and for the kğ‘˜k-th, base\_learning\_rate/10kâˆ’1base\_learning\_ratesuperscript10ğ‘˜1\textnormal{base\\_learning\\_rate}/10^{k-1}. For each of these neural networks, we store at each iteration step the log error. Once the N\_Iter\_learning\_rate\_test training steps have been done for each of these neural networks, we keep the neural network instance that has the lowest average log error. If it is the kğ‘˜k-th neural network, we then train it again for N\_Iter training steps, using as learning rate base\_learning\_rate/10kbase\_learning\_ratesuperscript10ğ‘˜\textnormal{base\\_learning\\_rate}/10^{k}.

### B.11 Parallelization

In practice, we parallelize the forward pass according to the strategy indices kğ‘˜k. We run thread\_batch\_size processes in parallel, where thread\_batch\_size is defined in Table [8](#A2.T8 "Table 8 â€£ B.12 Normalization constants, implementation parameters, and learning rates â€£ Appendix B Precise implementation of the neural network algorithm â€£ Computation of Expected Shortfall by fast detection of worst scenarios").

At a given level â„“â„“\ell, the computation of expectationâ„“k,jsuperscriptsubscriptexpectationâ„“

ğ‘˜ğ‘—\textnormal{expectation}\_{\ell}^{k,j} can be parallelized according to the sample indices jğ‘—j. In practice, we run number\_of\_threads\_for\_level\_expectations number of processes in parallel, where number\_of\_threads\_for\_level\_expectations is defined in Table [8](#A2.T8 "Table 8 â€£ B.12 Normalization constants, implementation parameters, and learning rates â€£ Appendix B Precise implementation of the neural network algorithm â€£ Computation of Expected Shortfall by fast detection of worst scenarios").

For a given level, the training of each neural network corresponding to a given qâˆˆqgğ‘subscriptğ‘ğ‘”q\in q\_{g} can be done independently. Therefore, at a given level, we multiprocessed our code in order to train all the neural networks in parallel.

### B.12 Normalization constants, implementation parameters, and learning rates

| Level | q | m | Î£Î£\Sigma | N | running\_cost | f\_precompute |
| --- | --- | --- | --- | --- | --- | --- |
| 1 | 6 | 106superscript10610^{6} | 1012superscript101210^{12} | 104superscript10410^{4} | 107superscript10710^{7} | 105superscript10510^{5} |
| 2 | 6 | 106superscript10610^{6} | 1012superscript101210^{12} | 104superscript10410^{4} | 107superscript10710^{7} | 105superscript10510^{5} |
| 3 | 6 | 106superscript10610^{6} | 1012superscript101210^{12} | 104superscript10410^{4} | 105superscript10510^{5} | 106superscript10610^{6} |
| 4 | 6 | 106superscript10610^{6} | 1011superscript101110^{11} | 104superscript10410^{4} | 105superscript10510^{5} | 106superscript10610^{6} |

Table 6: Inputsâ€™ renormalization constants by Level



| Level | q | base\_learning\_rate | Level | q | base\_learning\_rate |
| --- | --- | --- | --- | --- | --- |
| 1 | 6 | 10âˆ’9superscript10910^{-9} | 2 | 6 | 10âˆ’9superscript10910^{-9} |
| 1 | 10 | 10âˆ’9superscript10910^{-9} | 2 | 10 | 10âˆ’9superscript10910^{-9} |
| 1 | 15 | 10âˆ’9superscript10910^{-9} | 2 | 15 | 10âˆ’9superscript10910^{-9} |
| 1 | 20 | 10âˆ’9superscript10910^{-9} | 2 | 20 | 10âˆ’9superscript10910^{-9} |
| 1 | 25 | 10âˆ’9superscript10910^{-9} | 2 | 25 | 10âˆ’9superscript10910^{-9} |
| 1 | 30 | 10âˆ’9superscript10910^{-9} | 2 | 30 | 10âˆ’9superscript10910^{-9} |
| 1 | 35 | 10âˆ’9superscript10910^{-9} | 2 | 35 | 10âˆ’9superscript10910^{-9} |
| 1 | 40 | 10âˆ’9superscript10910^{-9} | 2 | 40 | 10âˆ’9superscript10910^{-9} |
| 1 | 45 | 10âˆ’9superscript10910^{-9} | 2 | 45 | 10âˆ’9superscript10910^{-9} |
| 1 | 50 | 10âˆ’9superscript10910^{-9} | 2 | 50 | 10âˆ’9superscript10910^{-9} |
| 1 | 60 | 10âˆ’9superscript10910^{-9} | 2 | 60 | 10âˆ’9superscript10910^{-9} |
| 1 | 70 | 10âˆ’9superscript10910^{-9} | 2 | 70 | 10âˆ’9superscript10910^{-9} |
| 1 | 80 | 10âˆ’9superscript10910^{-9} | 2 | 80 | 10âˆ’9superscript10910^{-9} |
| 1 | 90 | 10âˆ’9superscript10910^{-9} | 2 | 90 | 10âˆ’9superscript10910^{-9} |
| 1 | 100 | 10âˆ’9superscript10910^{-9} | 2 | 100 | 10âˆ’9superscript10910^{-9} |
| 1 | 150 | 10âˆ’9superscript10910^{-9} | 2 | 150 | 10âˆ’9superscript10910^{-9} |
| 1 | 200 | 10âˆ’10superscript101010^{-10} | 2 | 200 | 10âˆ’10superscript101010^{-10} |
| 1 | 253 | 10âˆ’10superscript101010^{-10} | 2 | 253 | 10âˆ’10superscript101010^{-10} |
| 3 | 6 | 10âˆ’7superscript10710^{-7} | 4 | 6 | 10âˆ’7superscript10710^{-7} |

Table 7: Neural network base learning rates

|  |  |
| --- | --- |
| Parameter | Value |
| j\_batch\_size | 4 |
| k\_batch\_size | 4 |
| N\_batch\_change\_proportion | 1 000 |
| N\_iter\_show\_proportion | 100 |
| smaller\_learning\_rate\_proportion | 10 |
| N\_Iter\_smaller\_learning\_rate | 10 000 |
| L | 4 |
| n\_s | 253 |
| n\_w | 6 |
| k\_bar | 200 |
| j\_bar | 40 |
| i\_0 | 300 |
| k\_0 | 300 |
| Î£0subscriptÎ£0\Sigma\_{0} | (300âˆ’253âˆ’1)â€‹Î£3002531Î£(300-253-1)\Sigma |
| N\_wishart\_proportion | 1 000 |
| N\_mu\_tildes\_simulated | 1 000 000 |
| thread\_batch\_size | 4 |
| number\_of\_threads\_for\_level\_expectations | 4 |
| thread\_batch\_size\_for\_level\_expectations | 4 |
| p | 1 |
| r | 2 |
| c | 0 |
| N\_Iter | 1 000 000 |
| N\_Iter\_learning\_rate\_test | 100 000 |
| number\_of\_neural\_networks\_for\_learning\_rate\_test | 4 |
| K | 10 000 000 |
| N\_mu\_tildes\_simulated\_non\_final\_level | 1 000 |
| N\_memory\_new\_pricings\_opt | 100 |

Table 8: Implementation parameters

[â—„](javascript: void(0))
[![ar5iv homepage](/assets/ar5iv.png)](/)
[Feeling  
lucky?](/feeling_lucky)

[Conversion  
report](/log/2005.12593)
[Report  
an issue](https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2005.12593)
[ViewÂ original  
onÂ arXiv](https://arxiv.org/abs/2005.12593)[â–º](javascript: void(0))